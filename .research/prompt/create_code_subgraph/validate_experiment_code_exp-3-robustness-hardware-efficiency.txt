
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "1. Existing forward-only TTA (NGFAT) still updates only BN γ,β and therefore cannot adapt models that use Group/Layer/Instance Norm or no normalisation at all (e.g. transformers, self-normalising nets).\n2. Forward-only natural-gradient steps are always executed although many test batches are easy – unnecessary parameter noise slows convergence and may accumulate drift in recurring scenarios.\n3. There is no mechanism to detect harmful updates on-device; collapsing on rare, high-entropy inputs remains possible and cannot be rolled back without labelled data.\n4. Current Fisher blocks ignore cross-sample curvature; mini-batches <8 (typical on MCU) give noisy statistics.",
    "Methods": "Method name: ZORRO – Zero-backward Online Risk-aware RObust adaptation.\nKey novelties:\na) Universal forward-Fisher: derive closed-form 2×2 (or 1×1) natural-gradient for *any* affine layer that is a linear map followed by element-wise normalisation f(x)=α(x−μ)/σ+β. This covers BN, GN, LN, IN and the RMS norm of ViT. Statistics (μ,σ) are read from the same forward pass; memory overhead 2×feature-dim.\n\nb) Cross-batch shrinkage: maintain a running Fisher F̂_t and apply James–Stein shrinkage with factor τ_t= n/(n+λ) where n is effective sample count; this drastically reduces noise for tiny batches (n≤4) without extra FLOPs.\n\nc) Accuracy-estimation gate: integrate AETTA-mini (dropout-free variant using Monte-Carlo weight masking) to obtain a label-free error estimate ĥ_t after each batch. Update is *committed* only if ĥ_t − ĥ_{t−1}>ϵ or entropy drop >δ; otherwise parameters are left unchanged (pure inference). This halves update frequency on slow streams and prevents drift.\n\nd) Rollback buffer: store last K=3 accepted parameter states + their ĥ. If two consecutive batches yield worse ĥ than all K checkpoints, revert to best state (cost O(K·P) copy, P tiny because we save only γ,β,α). Fully unsupervised.\n\ne) µC-friendly maths: all matrix inverses are 2×2; no sqrt/exp outside LUT; fits in CMSIS-NN.\n",
    "Experimental Setup": "Datasets & streams:\n• TinyImageNet-C (severity 3-5) with η_r∈{1,1/2,1/8}.\n• CIFAR-10-C and CIFAR-10.1 natural shift.\n• Google Speech Commands v2 with background noise (audio, shows non-vision generality).\n• Real on-device capture: 5-fps webcam stream on STM32H7 (640 KB SRAM).\n\nModels: ResNet-20-GN, MobileNet-V2-GN, ViT-Tiny with RMS Norm, and a 1-D CNN for speech.\n\nBaselines: Source, TENT, FATENT, NGFAT, RoTTA, AETTA-reset, BN adaptive.\n\nMetrics:\n1. Error after {1,2,4} batches.\n2. Area-under-error-curve under Realistic Online Protocol (ROP, η_r).\n3. Avg. wall-clock latency & energy (Arm-Cortex-M vs Jetson-Nano).\n4. False-update rate (updates skipped although target batch lowered error <0.1 %).\n5. Collapse-recovery success (% runs where rollback prevents >5 % error spike).\n",
    "Experimental Code": "# zorro.py  (core update-gate, 60 LOC)\n@torch.no_grad()\ndef zorro_step(model, batch, state):\n    out = model(batch)\n    H = entropy(out)\n    ## 1. Unsup. accuracy proxy via softmax disagreement\n    q = out.softmax(1)\n    p = (q * (1-q)).sum(1)             # variance proxy\n    acc_hat = 1 - p.mean()\n    ## 2. Decide whether to adapt\n    if (acc_hat < state.last_acc-1e-3) or (H.mean()>state.last_H*0.9):\n        for mod in state.affine:       # BN, GN, LN, RMSNorm\n            y = mod.output             # cached activation\n            g = state.dH_dz @ y.mean(0)  # ∂H/∂α like NGFAT\n            F = y.var(0)+1e-5\n            tau = state.n/(state.n+state.lambda_)\n            step = -tau * g / F\n            mod.weight += step\n        state.save_checkpoint(acc_hat)\n    state.update_stats(acc_hat, H)\n    return out",
    "Expected Result": "• Matches NGFAT accuracy on CIFAR-C but with 35 % fewer parameter updates.\n• On GN/LN models where BN-only methods fail, ZORRO reduces error from 38 % (source) to 24 % within 2 batches, beating TENT-GN (27 %).\n• MCU demo: 5 fps stream maintained at 3.7 mJ/frame vs 6.2 mJ for TENT.\n• Rollback prevents >90 % of catastrophic error spikes observed in NGFAT under recurring shift.\n• Audio CNN: word error 8.1 %→6.4 % after first noisy batch, showing modality-agnosticism.",
    "Expected Conclusion": "ZORRO generalises forward-only natural-gradient TTA to any normalisation layer, autonomously decides *when* to adapt via built-in label-free risk estimates, and adds a lightweight rollback against collapse. It retains single-pass speed, extends applicability to transformers and micro-controllers, and raises the bar for robust, energy-aware test-time learning in real-world streams."
}

# Experimental Design
## Experiment Strategy
Overall experimental strategy for demonstrating the effectiveness of ZORRO

1. Core validation axes
   a. Adaptation benefit – Does ZORRO lower on-line error faster and further than all competitors?
   b. Efficiency – Does it achieve that benefit with fewer parameter updates, lower latency, and lower energy?
   c. Robustness – Does it remain stable under severe or recurring shifts and avoid catastrophic drift?
   d. Generality – Does it work across normalisation types, model families, modalities, batch sizes, and hardware classes?
   e. Safety – Can the gate + rollback reject harmful updates better than prior art?

2. Comparative framework
   • Methods: (i) frozen source model, (ii) classic BN adaptation, (iii) forward-only SOTA (TENT, NGFAT, FATENT), (iv) risk-aware SOTA (RoTTA, AETTA-reset), (v) ZORRO ablations (−Fisher, −shrinkage, −gate, −rollback, −all) and full ZORRO.
   • Datasets/shifts: vision, audio, synthetic recurrences, real device streams. Each experiment will use an identical set of comparison methods so curves are directly overlayable.
   • Hardware profiles: A100 (heavy), Jetson-Nano (edge GPU), STM32H7 (MCU) – same software harness, differing batch-size budgets.

3. Multi-angle evidence collection
   • Quantitative accuracy: instantaneous error at fixed adaptation horizons {1,2,4,∞} batches; area-under-error-curve (AUEC) over the whole stream; Wilcoxon signed-rank test over 5 seeds.
   • Adaptation cost: (i) number of parameter update steps, (ii) wall-clock latency per frame, (iii) mean energy per frame (on-board power logger for Jetson / MCU).
   • Stability metrics: variance of error, maximum error spike, collapse rate, rollback recovery rate, false-update rate.
   • Qualitative analysis: histogram of gate decisions, t-SNE of feature drift, visual audio/vision examples where ZORRO reverted.
   • Computational analysis: FLOPs, memory, additional parameters; measured with Torch-Profiler and MCU on-device counters.

4. Experimental phases (common skeleton for every task)
   Phase-0: Baseline replication to calibrate code, fix seeds, and verify that results reproduce published numbers on clean data.
   Phase-1: Standard corruptions – run single-pass streams under ROP; log all core metrics.
   Phase-2: Recurring/easy-hard cycles – stress test the gate/rollback; identical shift schedule for all methods.
   Phase-3: Tiny-batch regime – constrain batch ≤4 to expose Fisher shrinkage; repeat Phase-1 metrics.
   Phase-4: Hardware deployment – run frozen inference & on-line adaptation in real-time on Jetson/MCU; capture energy & fps.
   Phase-5: Ablation sweep – repeat Phases 1–3 with each component disabled to pinpoint contribution.
   Phase-6: Cross-modality – apply to Speech Commands; same phases 1–3.

5. Success criteria (must hold in ≥4/5 seeds)
   • AUEC improvement ≥10% over best non-ZORRO competitor on every dataset-model pair.
   • Parameter-update count ≤70% of NGFAT average while maintaining equal or better final error.
   • Catastrophic collapse rate <1%; rollback recovers ≥80% of induced collapses.
   • Latency overhead ≤5% vs source inference on A100; energy per frame ≤65% of TENT on MCU.
   • No ablation variant may outperform full ZORRO on more than 10% of runs.

6. Statistical protocol
   • 5 random seeds × 5 shuffled test streams per condition.
   • Report median and 95% CI; conduct paired Wilcoxon tests with Holm-Bonferroni correction.
   • Release code, logs, and weights for full transparency.

7. Resource allocation
   • Distributed runner uses 6 A100 GPUs for parallel seeds, leaving 2 GPUs for hyper-parameter sweeps.
   • RAM headroom (2 TB) holds all datasets and checkpoints in a RAM-disk, eliminating I/O variance.
   • Edge hardware tests run nightly; results auto-synced to central dashboard.

This unified strategy ensures every experiment, regardless of dataset or model, produces directly comparable evidence on adaptation benefit, cost, robustness, generality, and safety, thereby establishing a comprehensive case for ZORRO’s effectiveness.

# Base Code (Reference Foundation)
{"evaluate_py": "#!/usr/bin/env python\n\"\"\"Aggregate results across run variations, compute comparison metrics \u0026\npublication-ready figures.  This script is triggered *once* by src.main after\nall individual experiment runs are finished.\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nmatplotlib.use(\"Agg\")\n\nFIG_TOPICS = [\n    (\"training_loss\", \"Cross-entropy loss\"),\n    (\"accuracy\", \"Accuracy (%)\"),\n    (\"final_accuracy_bar\", \"Final validation accuracy\")\n]\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Root directory containing sub-dirs for each run\")\n    return p.parse_args()\n\n\ndef collect_results(results_dir: Path) -\u003e pd.DataFrame:\n    records: List[Dict] = []\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        res_file = sub / \"results.json\"\n        if res_file.exists():\n            rec = json.loads(res_file.read_text())\n            records.append(rec)\n    if not records:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n    return pd.DataFrame.from_records(records)\n\n\ndef plot_comparisons(df: pd.DataFrame, results_dir: Path):\n    images = results_dir / \"images\"\n    images.mkdir(exist_ok=True, parents=True)\n\n    # 1) Line curves \u2013 loss \u0026 acc\n    for metric_key, ylabel in [(\"loss\", \"Cross-entropy loss\"),\n                               (\"acc\", \"Accuracy (%)\")]:\n        plt.figure()\n        for _, row in df.iterrows():\n            y = [h[metric_key] for h in row[\"train_history\"]]\n            x = [h[\"epoch\"] for h in row[\"train_history\"]]\n            plt.plot(x, y, marker=\"o\", label=row[\"run_id\"])\n            plt.annotate(f\"{y[-1]:.2f}\" if metric_key == \"acc\" else f\"{y[-1]:.3f}\",\n                         (x[-1], y[-1]), textcoords=\"offset points\",\n                         xytext=(0, 5), ha=\u0027center\u0027)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(ylabel)\n        plt.legend()\n        plt.title(ylabel + \" comparison\")\n        plt.tight_layout()\n        fname = \"training_loss.pdf\" if metric_key == \"loss\" else \"accuracy.pdf\"\n        plt.savefig(images / fname, bbox_inches=\"tight\")\n        plt.close()\n\n    # 2) Bar \u2013 final validation accuracy\n    plt.figure()\n    sns.barplot(x=\"run_id\", y=\"val_acc\", data=df)\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"val_acc\"] + 0.2, f\"{row[\u0027val_acc\u0027]:.2f}%\", ha=\u0027center\u0027)\n    plt.ylabel(\"Validation accuracy (%)\")\n    plt.xlabel(\"Run\")\n    plt.title(\"Final validation accuracy across runs\")\n    plt.tight_layout()\n    plt.savefig(images / \"final_accuracy_bar.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir).expanduser()\n\n    df = collect_results(results_dir)\n    plot_comparisons(df, results_dir)\n\n    # Output comparison results in JSON (stdout)\n    comp = df[[\"run_id\", \"val_acc\", \"val_loss\" if \"val_loss\" in df.columns else None]].to_dict(\n        orient=\"records\")\n    print(json.dumps({\"comparison\": comp}))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Template for the *full* experiment suite. Only placeholders are provided here.\n# The derive_specific step will populate this with concrete datasets, models and\n# hyper-parameters.\n\nexperiments:\n  # --------------------------------------------------------------------- #\n  # PLACEHOLDER: Add comprehensive run variations (baseline, proposed,     #\n  #              ablations, hardware-specific) here.                      #\n  #                                                                       #\n  # Example structure:                                                     #\n  # - run_id: tinyimagenet_resnet20_gn_source                              #\n  #   dataset:                                                             #\n  #     name: TINYIMAGENET_CORRUPT_C_PLACEHOLDER                           #\n  #   model:                                                               #\n  #     name: RESNET20_GN_PLACEHOLDER                                      #\n  #     pretrained: /path/to/source_ckpt.pt                                #\n  #   training:                                                            #\n  #     method: source                                                     #\n  #     batch_size: 32                                                     #\n  #     epochs: 0                                                          #\n  # --------------------------------------------------------------------- #\n  - run_id: FULL_EXPERIMENT_PLACEHOLDER\n    dataset: {DATASET_PLACEHOLDER}\n    model: {MODEL_PLACEHOLDER}\n    training: {SPECIFIC_CONFIG_PLACEHOLDER}\n", "main_py": "#!/usr/bin/env python\n\"\"\"Main orchestrator script.\n\nUsage examples:\n\n(1) Smoke test \u2013 lightweight synthetic run to verify that *all* variations\n    execute without GPU OOM etc.\n\n    uv run python -m src.main --smoke-test --results-dir /tmp/zorro_results\n\n(2) Full experiment \u2013 reads all variations from config/full_experiment.yaml\n\n    uv run python -m src.main --full-experiment --results-dir /path/to/res\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport shutil\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\")\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Directory to store *all* outputs (logs, figs, metrics)\")\n    return p.parse_args()\n\n\n# ------------------------------------------------------------------- helpers #\n\ndef _tee(stream, tee_to_file):\n    \"\"\"Read `stream` byte-by-byte, write both to sys.\u003cout/err\u003e and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        decoded = line.decode()\n        tee_to_file.write(decoded)\n        tee_to_file.flush()\n        sys.stdout.write(decoded) if tee_to_file.name.endswith(\"stdout.log\") else sys.stderr.write(decoded)\n    stream.close()\n\n\ndef _launch_train(run_cfg: dict, results_root: Path, python_bin: str = sys.executable):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write single-run YAML for the sub-process to consume\n    run_cfg_path = run_dir / \"config.yaml\"\n    run_cfg_path.write_text(yaml.safe_dump(run_cfg))\n\n    # Prepare log files\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    stdout_f = stdout_path.open(\"w\")\n    stderr_f = stderr_path.open(\"w\")\n\n    cmd = [python_bin, \"-m\", \"src.train\", \"--config-path\", str(run_cfg_path),\n           \"--results-dir\", str(run_dir)]\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Real-time tee of stdout / stderr\n    threads: List[threading.Thread] = []\n    threads.append(threading.Thread(target=_tee, args=(proc.stdout, stdout_f)))\n    threads.append(threading.Thread(target=_tee, args=(proc.stderr, stderr_f)))\n    for t in threads:\n        t.daemon = True\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_f.close()\n    stderr_f.close()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Run {run_id} failed with exit code {proc.returncode}\")\n\n\n# -------------------------------------------------------------------- main #\n\ndef main():\n    args = parse_args()\n    results_root = Path(args.results_dir).expanduser()\n    if results_root.exists():\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    config = yaml.safe_load(cfg_path.read_text())\n    experiments = config[\"experiments\"]\n\n    for run_cfg in experiments:\n        _launch_train(run_cfg, results_root)\n\n    # After all runs -\u003e aggregate\n    subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)], check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model architectures \u0026 factory.\nOnly *core* logic is provided here. Dataset / modality specific models (e.g.\nResNet-20-GN, MobileNet-V2-GN, ViT-Tiny) will be injected later via the\nplaceholder mechanism.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\"build_model\"]\n\n\n# ------------------------------------------------------------------ helpers #\n\ndef _make_norm(norm_type: str, num_features: int):\n    if norm_type == \"batch\":\n        return nn.BatchNorm2d(num_features, affine=True)\n    if norm_type == \"group\":\n        return nn.GroupNorm(num_groups=4, num_channels=num_features, affine=True)\n    if norm_type == \"layer\":\n        return nn.GroupNorm(num_groups=1, num_channels=num_features, affine=True)\n    raise ValueError(f\"Unsupported norm type: {norm_type}\")\n\n\n# --------------------------------------------------------------  SIMPLE CNN #\nclass SimpleCNN(nn.Module):\n    \"\"\"Light-weight CNN with GroupNorm. Suitable for smoke tests and synthetic\n    datasets. Real research models will be plugged-in later.\"\"\"\n\n    def __init__(self, in_channels: int = 3, num_classes: int = 10,\n                 norm: str = \"group\"):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 32, 3, padding=1),\n            _make_norm(norm, 32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, padding=1),\n            _make_norm(norm, 64),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n\n# ------------------------------------------------------------------ factory #\n\ndef build_model(model_cfg: Dict):\n    name = model_cfg.get(\"name\", \"simple_cnn\").lower()\n\n    if name in {\"simple_cnn\", \"simple_cnn_placeholder\"}:\n        return SimpleCNN(num_classes=model_cfg.get(\"num_classes\", 10))\n\n    # ------------------------------------------------------------------ #\n    # PLACEHOLDER: Additional models (ResNet-20-GN, ViT-Tiny, etc.) will be\n    # registered here in subsequent experiment-specific steps.            #\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f\"Model \u0027{name}\u0027 not yet implemented.\")", "preprocess_py": "\"\"\"Common data pipeline with dataset placeholders.\nThe *core* logic (splitting, DataLoader creation, seeding) is fully\nimplemented. Dataset-specific loading is isolated behind clear placeholders so\nthat future steps can swap-in real datasets without touching any other code.\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n__all__ = [\"get_dataloaders\", \"set_seed\"]\n\n\n# ------------------------------------------------------------------ utils --- #\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n# ------------------------------------------------------------ core pipeline #\n\ndef _load_placeholder_dataset(cfg: dict) -\u003e Tuple[torch.utils.data.Dataset, int]:\n    \"\"\"Generate a *synthetic* classification dataset. Useful for smoke tests and\n    retaining end-to-end functionality before real datasets are injected.\"\"\"\n    num_samples: int = cfg.get(\"num_samples\", 1024)\n    num_classes: int = cfg.get(\"num_classes\", 10)\n    input_shape = cfg.get(\"input_shape\", [3, 32, 32])\n\n    data = torch.randn(num_samples, *input_shape)\n    targets = torch.randint(0, num_classes, (num_samples,))\n    return TensorDataset(data, targets), num_classes\n\n\ndef _dataset_factory(cfg: dict):\n    name = cfg[\"name\"]\n    if name == \"SYNTHETIC_CLASSIFICATION_PLACEHOLDER\":\n        return _load_placeholder_dataset(cfg)\n    # ---------------------------------------------------------------------- #\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic.     #\n    # Insert custom dataset returns (dataset, num_classes) below this line.  #\n    # ---------------------------------------------------------------------- #\n    raise NotImplementedError(f\"Dataset \u0027{name}\u0027 is not implemented yet. \")\n\n\ndef get_dataloaders(dataset_cfg: dict, training_cfg: dict):\n    \"\"\"Return (train_loader, val_loader, num_classes).\"\"\"\n    dataset, num_classes = _dataset_factory(dataset_cfg)\n\n    # ------------------------- split into train / val -------------------- #\n    val_fraction = training_cfg.get(\"val_fraction\", 0.2)\n    val_size = int(len(dataset) * val_fraction)\n    train_size = len(dataset) - val_size\n    train_set, val_set = random_split(dataset, [train_size, val_size])\n\n    batch_size = training_cfg.get(\"batch_size\", 32)\n    num_workers = training_cfg.get(\"num_workers\", 0)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader, num_classes", "pyproject_toml": "[project]\nname = \"zorro-experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for ZORRO adaptation experiments\"\nrequires-python = \"\u003e=3.9\"\nauthors = [\n    { name = \"Research Automation\" }\n]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\ntorch = \"^2.0\"\ntorchvision = \"^0.15\"\nmatplotlib = \"^3.7\"\nseaborn = \"^0.12\"\npyyaml = \"^6.0\"\npandas = \"^2.0\"\nscikit-learn = \"^1.2\"\nrich = \"^13.0\"\ntqdm = \"^4.0\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "# Lightweight configuration used by CI smoke tests.  \n# Uses synthetic data so that full pipeline can run under \u003c30 sec.\n\nexperiments:\n  - run_id: smoke_source\n    seed: 123\n    dataset:\n      name: SYNTHETIC_CLASSIFICATION_PLACEHOLDER  # PLACEHOLDER: Will be replaced\n      num_samples: 512\n      num_classes: 10\n      input_shape: [3, 32, 32]\n    model:\n      name: SIMPLE_CNN_PLACEHOLDER               # PLACEHOLDER: Will be replaced\n    training:\n      epochs: 1\n      batch_size: 64\n      learning_rate: 0.001\n      method: source\n\n  - run_id: smoke_zorro\n    seed: 123\n    dataset:\n      name: SYNTHETIC_CLASSIFICATION_PLACEHOLDER  # PLACEHOLDER\n      num_samples: 512\n      num_classes: 10\n      input_shape: [3, 32, 32]\n    model:\n      name: SIMPLE_CNN_PLACEHOLDER               # PLACEHOLDER\n    training:\n      epochs: 1\n      batch_size: 64\n      learning_rate: 0.001\n      method: zorro", "train_py": "#!/usr/bin/env python\n\"\"\"\nTraining / adaptation runner for a SINGLE experiment variation.  \nThis file *must not* be called directly by users \u2013 it is launched as a\nsub-process by src.main so that logs can be tee\u2019ed and captured per run.  \nNevertheless, it can be invoked stand-alone for debugging:\n\n    python -m src.train --config-path path/to/run_cfg.yaml --results-dir /tmp/res\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport json\nimport os\nimport time\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport yaml\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders, set_seed\nfrom .model import build_model\n\nmatplotlib.use(\"Agg\")  # mandatory for CLI / CI environments\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config-path\", type=str, required=True,\n                   help=\"YAML file describing *one* experiment run (written by main.py)\")\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Directory where all outputs of this run are saved\")\n    return p.parse_args()\n\n\n################################################################################\n# ------------------------------  ZORRO ADAPTER  ------------------------------ #\n################################################################################\n\nclass ZorroState:\n    \"\"\"Holds adaptation statistics, checkpoints \u0026 hyper-params.\"\"\"\n\n    def __init__(self, model: nn.Module, lambda_: float = 1.0,\n                 eps: float = 1e-3, k_ckpt: int = 3):\n        self.lambda_ = lambda_\n        self.eps = eps\n        self.n = 0  # effective sample count for shrinkage\n        self.last_acc_hat = 1.0  # optimistic starting point\n        self.last_entropy = 0.0\n        self.checkpoints: deque = deque(maxlen=k_ckpt)\n\n        self.affine: List[nn.Module] = []\n        self.activation_cache: Dict[int, torch.Tensor] = {}\n        self._register_hooks(model)\n\n    # ---------------------------------------------------------------------\n    def _register_hooks(self, model: nn.Module):\n        \"\"\"Register forward hooks on affine normalisation layers to cache outputs.\"\"\"\n        for m in model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d,\n                              nn.GroupNorm, nn.LayerNorm)):\n                self.affine.append(m)\n                m.register_forward_hook(self._make_hook(m))\n\n    def _make_hook(self, module):\n        def _hook(_, __, output):\n            # Detach to avoid autograd bookkeeping \u2013 adaptation is forward-only.\n            self.activation_cache[id(module)] = output.detach()\n        return _hook\n\n    # ---------------------------------------------------------------------\n    @torch.no_grad()\n    def zorro_step(self, logits: torch.Tensor):\n        \"\"\"Perform *one* ZORRO adaptation step given the model logits.\"\"\"\n        probs = torch.softmax(logits, dim=1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        entropy = -(probs * log_probs).sum(1)           # per-sample entropy\n        var_proxy = (probs * (1 - probs)).sum(1)        # as in original paper\n        acc_hat = 1.0 - var_proxy.mean()               # label-free accuracy proxy\n\n        # ---------------- decision gate ---------------- #\n        should_update = (\n            (acc_hat \u003c self.last_acc_hat - self.eps) or\n            (entropy.mean() \u003e self.last_entropy * 0.9)\n        )\n\n        tau = self.n / (self.n + self.lambda_) if (self.n + self.lambda_) \u003e 0 else 0.0\n\n        if should_update:\n            for mod in self.affine:\n                y = self.activation_cache[id(mod)]      # activation from last forward\n                if y.ndim \u003e 2:                          # e.g. Conv feature-maps\n                    g = y.mean(dim=(0, 2, 3))          # gradient proxy \u2202H/\u2202\u03b1\n                    F_diag = y.var(dim=(0, 2, 3)) + 1e-5\n                else:                                   # e.g. LayerNorm over features\n                    g = y.mean(dim=0)\n                    F_diag = y.var(dim=0) + 1e-5\n                step = -tau * g / F_diag\n                step = step.view_as(mod.weight.data)\n                mod.weight.data.add_(step)\n            # ---------------- checkpointing ---------------- #\n            self.checkpoints.append((self._snapshot(), acc_hat.item()))\n        else:\n            # If two consecutive bad batches \u21d2 rollback to best ckpt.\n            if len(self.checkpoints) == self.checkpoints.maxlen and \\\n               acc_hat.item() \u003e max(a for _, a in self.checkpoints):\n                best_state, _ = max(self.checkpoints, key=lambda x: x[1])\n                self._restore(best_state)\n\n        # Update running statistics\n        self.last_acc_hat = acc_hat.item()\n        self.last_entropy = entropy.mean().item()\n        self.n += 1\n\n    # ------------------------------------------------------------------ utils #\n    def _snapshot(self):\n        \"\"\"Return *only* the affine parameters weight/bias for lightweight ckpt.\"\"\"\n        return {\n            id(m): {\n                \"weight\": m.weight.data.clone(),\n                \"bias\": None if m.bias is None else m.bias.data.clone(),\n            } for m in self.affine\n        }\n\n    def _restore(self, state_dict):\n        for m in self.affine:\n            buf = state_dict[id(m)]\n            m.weight.data.copy_(buf[\"weight\"])\n            if m.bias is not None and buf[\"bias\"] is not None:\n                m.bias.data.copy_(buf[\"bias\"])\n\n################################################################################\n# ------------------------------  MAIN TRAINING  ----------------------------- #\n################################################################################\n\ndef main():\n    args = parse_args()\n    cfg = yaml.safe_load(Path(args.config_path).read_text())\n    run_id = cfg[\"run_id\"]\n\n    results_root = Path(args.results_dir).expanduser()\n    results_root.mkdir(parents=True, exist_ok=True)\n    images_dir = results_root / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    set_seed(cfg.get(\"seed\", 42))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------- dataset #\n    train_loader, val_loader, num_classes = get_dataloaders(cfg[\"dataset\"],\n                                                           cfg[\"training\"])\n\n    # --------------------------------------------------------------   model #\n    model_cfg = cfg[\"model\"]\n    model_cfg[\"num_classes\"] = num_classes  # ensure consistency\n    model = build_model(model_cfg).to(device)\n\n    # Optionally load a pre-trained source checkpoint\n    if \"pretrained\" in model_cfg and model_cfg[\"pretrained\"]:\n        model.load_state_dict(torch.load(model_cfg[\"pretrained\"], map_location=device))\n\n    # -------------------------------------------------------------  optimiser #\n    optim_cfg = cfg[\"training\"]\n    optimiser = optim.Adam(model.parameters(), lr=optim_cfg.get(\"learning_rate\", 1e-3))\n    criterion = nn.CrossEntropyLoss()\n\n    method = optim_cfg.get(\"method\", \"source\").lower()\n    use_zorro = method == \"zorro\"\n    z_state = ZorroState(model) if use_zorro else None\n\n    epochs = optim_cfg.get(\"epochs\", 1)\n    history: List[Dict[str, float]] = []\n\n    for epoch in range(1, epochs + 1):\n        model.train(not use_zorro)  # keep BN stats frozen for adaptation\n        epoch_loss = 0.0\n        correct, total = 0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            inputs, targets = (b.to(device) for b in batch)\n\n            if use_zorro:\n                # ---------------- inference + forward NG update ---------------- #\n                with torch.no_grad():\n                    logits = model(inputs)\n                    z_state.zorro_step(logits)\n                loss = criterion(logits, targets)\n            else:\n                optimiser.zero_grad()\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n                loss.backward()\n                optimiser.step()\n\n            # ---------------- stats ---------------- #\n            epoch_loss += loss.item() * inputs.size(0)\n            _, preds = logits.max(1)\n            correct += preds.eq(targets).sum().item()\n            total += inputs.size(0)\n            pbar.set_postfix({\"loss\": loss.item(),\n                              \"acc\": 100.0 * correct / total})\n\n        epoch_loss /= total\n        epoch_acc = 100.0 * correct / total\n        history.append({\"epoch\": epoch, \"loss\": epoch_loss, \"acc\": epoch_acc})\n\n    # ----------------------------------------------------------------- eval #\n    model.eval()\n    val_correct, val_total, val_loss = 0, 0, 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            val_loss += loss.item() * inputs.size(0)\n            _, preds = logits.max(1)\n            val_correct += preds.eq(targets).sum().item()\n            val_total += inputs.size(0)\n    val_loss /= val_total\n    val_acc = 100.0 * val_correct / val_total\n\n    ############################################################################\n    # ---------------------------  SAVE ARTIFACTS  --------------------------- #\n    ############################################################################\n    ckpt_path = results_root / \"model.pt\"\n    torch.save(model.state_dict(), ckpt_path)\n\n    # ------------------------ metrics JSON ------------------------ #\n    metrics = {\n        \"run_id\": run_id,\n        \"method\": method,\n        \"epochs\": epochs,\n        \"train_history\": history,\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc,\n    }\n    (results_root / \"results.json\").write_text(json.dumps(metrics, indent=2))\n\n    # ---------------------------  FIGURES  ------------------------- #\n    epochs_axis = [h[\"epoch\"] for h in history]\n    train_loss_axis = [h[\"loss\"] for h in history]\n    train_acc_axis = [h[\"acc\"] for h in history]\n\n    # line \u2013 loss\n    plt.figure()\n    plt.plot(epochs_axis, train_loss_axis, marker=\"o\", label=\"train_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-entropy loss\")\n    plt.title(f\"Training loss \u2013 {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{train_loss_axis[-1]:.3f}\",\n                 (epochs_axis[-1], train_loss_axis[-1]),\n                 textcoords=\"offset points\", xytext=(0, 5), ha=\u0027center\u0027)\n    plt.tight_layout()\n    plt.savefig(images_dir / \"training_loss.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # line \u2013 accuracy\n    plt.figure()\n    plt.plot(epochs_axis, train_acc_axis, marker=\"o\", label=\"train_acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"Training accuracy \u2013 {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{train_acc_axis[-1]:.2f}%\", (epochs_axis[-1], train_acc_axis[-1]),\n                 textcoords=\"offset points\", xytext=(0, 5), ha=\u0027center\u0027)\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # -------------------- stdout summary (required) -------------------- #\n    print(json.dumps(metrics))\n\n\nif __name__ == \"__main__\":\n    main()"}

# Current Experiment (To be validated)
- Experiment ID: exp-3-robustness-hardware-efficiency
- Description: Objective / hypothesis: Demonstrate that ZORRO maintains robustness under recurring shifts and tiny micro-controller batches while offering superior energy & latency efficiency on resource-constrained hardware.

Models
• ResNet-20-GN (for STM32 MCU deployment)
• 1-D CNN-Speech (5-layer, GN)

Datasets / streams
• TinyImageNet-C with easy↔hard cycles (as in exp-2) streamed at 5 FPS.
• Google Speech Commands v2 with additive café/traffic noise (SNR 0–20 dB) streamed at real-time 16 kHz windows.
• Live STM32H7 webcam stream (640×480 @5 FPS, indoor office lighting) captured via serial link.

Dataset preprocessing
Vision: bilinear down-scale to 64×64 for MCU RAM budget, uint8→float32 normalise on device.
Audio: 40-channel log-melspectrogram, 1×128 frame window, no overlap.

Data splitting
• Train/val as official.
• Online test: continuous real-time capture; first 1 min discarded for warm-up.

Repetitions
• 3 physical devices × 3 seeds per stream.

Evaluation metrics
Primary  (edge): energy/frame (mJ) via INA260 sensor, average latency/frame, sustained FPS.
Secondary: AUEC, collapse rate, rollback recovery success.

Comparisons / adjustments
• NGFAT & RoTTA re-implemented in CMSIS-NN fixed-point; learning-rate retuned (η=5e-4) for stable MCU convergence.
• ZORRO-lite = Fisher+shrinkage but without rollback (for memory ablation to fit 448 KB).

Hyper-parameter analysis
• Measure sensitivity to gate entropy threshold δ ∈ {0.05,0.1,0.2} at MCU batch-size 1.

Robustness tests
• Noise-injection: live brightness oscillation (vision) and pink-noise burst (audio).
• OOD domain transfer: suddenly switch webcam lighting to infrared illumination; evaluate gate reaction and rollback.

Compute profiling
• FLOPs/inference: measured by CubeMX cycle counter; memory footprint via map file.
• Wall-clock cost on Jetson-Nano (GPU) vs STM32 (CPU) recorded.

Selection / averaging
• Report median over devices; 95% CI with bootstrap (1 k resamples).

Example MCU code (fragment, C++)
```c
for(;;){
  capture_frame(buf);
  preprocess(buf, scratch);
  zorro_step_int8(model, scratch, &state);
  transmit_logits(model.out);
}
```
Expected outcome: On STM32, ZORRO-full keeps error <25% with 3.7 mJ/frame (-40% vs TENT) and zero collapses; ZORRO-lite fits RAM but shows higher collapse rate; NGFAT fails to converge on batch-1 streams. On Jetson-Nano, ZORRO saves ≥30% updates and 20% energy vs NGFAT while matching accuracy.
- Run Variations: ['source-frozen', 'NGFAT', 'RoTTA', 'ZORRO-lite', 'ZORRO-full']

# Derived Experiment Code (To be validated)

{"evaluate_py": "#!/usr/bin/env python\n\"\"\"Aggregate results across run variations, compute comparison metrics \u0026\npublication-ready figures.  This script is triggered *once* by src.main after\nall individual experiment runs are finished.\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nmatplotlib.use(\"Agg\")\n\nFIG_TOPICS = [\n    (\"training_loss\", \"Cross-entropy loss\"),\n    (\"accuracy\", \"Accuracy (%)\"),\n    (\"final_accuracy_bar\", \"Final validation accuracy\")\n]\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Root directory containing sub-dirs for each run\")\n    return p.parse_args()\n\n\ndef collect_results(results_dir: Path) -\u003e pd.DataFrame:\n    records: List[Dict] = []\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        res_file = sub / \"results.json\"\n        if res_file.exists():\n            rec = json.loads(res_file.read_text())\n            records.append(rec)\n    if not records:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n    return pd.DataFrame.from_records(records)\n\n\ndef plot_comparisons(df: pd.DataFrame, results_dir: Path):\n    images = results_dir / \"images\"\n    images.mkdir(exist_ok=True, parents=True)\n\n    # 1) Line curves \u2013 loss \u0026 acc\n    for metric_key, ylabel in [(\"loss\", \"Cross-entropy loss\"),\n                               (\"acc\", \"Accuracy (%)\")]:\n        plt.figure()\n        for _, row in df.iterrows():\n            y = [h[metric_key] for h in row[\"train_history\"]]\n            x = [h[\"epoch\"] for h in row[\"train_history\"]]\n            plt.plot(x, y, marker=\"o\", label=row[\"run_id\"])\n            plt.annotate(f\"{y[-1]:.2f}\" if metric_key == \"acc\" else f\"{y[-1]:.3f}\",\n                         (x[-1], y[-1]), textcoords=\"offset points\",\n                         xytext=(0, 5), ha=\u0027center\u0027)\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(ylabel)\n        plt.legend()\n        plt.title(ylabel + \" comparison\")\n        plt.tight_layout()\n        fname = \"training_loss.pdf\" if metric_key == \"loss\" else \"accuracy.pdf\"\n        plt.savefig(images / fname, bbox_inches=\"tight\")\n        plt.close()\n\n    # 2) Bar \u2013 final validation accuracy\n    plt.figure()\n    sns.barplot(x=\"run_id\", y=\"val_acc\", data=df)\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"val_acc\"] + 0.2, f\"{row[\u0027val_acc\u0027]:.2f}%\", ha=\u0027center\u0027)\n    plt.ylabel(\"Validation accuracy (%)\")\n    plt.xlabel(\"Run\")\n    plt.title(\"Final validation accuracy across runs\")\n    plt.tight_layout()\n    plt.savefig(images / \"final_accuracy_bar.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir).expanduser()\n\n    df = collect_results(results_dir)\n    plot_comparisons(df, results_dir)\n\n    # Output comparison results in JSON (stdout)\n    comp = df[[\"run_id\", \"val_acc\", \"val_loss\" if \"val_loss\" in df.columns else None]].to_dict(\n        orient=\"records\")\n    print(json.dumps({\"comparison\": comp}))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "experiments:\n  # ---------------- Tiny-ImageNet-C (vision) ----------------\n  - run_id: tinyimagenet_source_frozen\n    seed: 42\n    dataset:\n      name: tinyimagenet_c\n      split: test\n      image_size: 64\n    model:\n      name: resnet20_gn\n    training:\n      method: source\n      epochs: 0  # frozen inference only\n      batch_size: 32\n      val_fraction: 0.2\n\n  - run_id: tinyimagenet_ngfat\n    seed: 42\n    dataset:\n      name: tinyimagenet_c\n      split: test\n      image_size: 64\n    model:\n      name: resnet20_gn\n    training:\n      method: ngfat\n      epochs: 1\n      batch_size: 32\n      val_fraction: 0.2\n\n  - run_id: tinyimagenet_rotta\n    seed: 42\n    dataset:\n      name: tinyimagenet_c\n      split: test\n      image_size: 64\n    model:\n      name: resnet20_gn\n    training:\n      method: rotta\n      epochs: 1\n      batch_size: 32\n      val_fraction: 0.2\n      entropy_threshold: 1.5\n\n  - run_id: tinyimagenet_zorro_lite\n    seed: 42\n    dataset:\n      name: tinyimagenet_c\n      split: test\n      image_size: 64\n    model:\n      name: resnet20_gn\n    training:\n      method: zorro-lite\n      epochs: 1\n      batch_size: 32\n      val_fraction: 0.2\n\n  - run_id: tinyimagenet_zorro_full\n    seed: 42\n    dataset:\n      name: tinyimagenet_c\n      split: test\n      image_size: 64\n    model:\n      name: resnet20_gn\n    training:\n      method: zorro-full\n      epochs: 1\n      batch_size: 32\n      val_fraction: 0.2\n\n  # ---------------- Speech Commands v2 (audio) --------------\n  - run_id: speech_source_frozen\n    seed: 42\n    dataset:\n      name: speechcommands_v2\n      subset: training\n      sample_rate: 16000\n      n_mels: 40\n    model:\n      name: cnn_speech_gn\n    training:\n      method: source\n      epochs: 0\n      batch_size: 64\n      val_fraction: 0.2\n\n  - run_id: speech_ngfat\n    seed: 42\n    dataset:\n      name: speechcommands_v2\n      subset: training\n      sample_rate: 16000\n      n_mels: 40\n    model:\n      name: cnn_speech_gn\n    training:\n      method: ngfat\n      epochs: 1\n      batch_size: 64\n      val_fraction: 0.2\n\n  - run_id: speech_rotta\n    seed: 42\n    dataset:\n      name: speechcommands_v2\n      subset: training\n      sample_rate: 16000\n      n_mels: 40\n    model:\n      name: cnn_speech_gn\n    training:\n      method: rotta\n      epochs: 1\n      batch_size: 64\n      val_fraction: 0.2\n      entropy_threshold: 1.5\n\n  - run_id: speech_zorro_lite\n    seed: 42\n    dataset:\n      name: speechcommands_v2\n      subset: training\n      sample_rate: 16000\n      n_mels: 40\n    model:\n      name: cnn_speech_gn\n    training:\n      method: zorro-lite\n      epochs: 1\n      batch_size: 64\n      val_fraction: 0.2\n\n  - run_id: speech_zorro_full\n    seed: 42\n    dataset:\n      name: speechcommands_v2\n      subset: training\n      sample_rate: 16000\n      n_mels: 40\n    model:\n      name: cnn_speech_gn\n    training:\n      method: zorro-full\n      epochs: 1\n      batch_size: 64\n      val_fraction: 0.2\n      # All other hyper-params rely on defaults in src.train\n  \n  # Note: All random seeds fixed to 42 for comparability.  Hardware-specific\n  # deployment parameters (energy logging, MCU batch size =1) are controlled by\n  # the low-level C harness and are therefore *not* part of this YAML.\n  \n  # Total experiments: 10  (5 methods \u00d7 2 modality/model pairs)", "main_py": "#!/usr/bin/env python\n\"\"\"Main orchestrator script.\n\nUsage examples:\n\n(1) Smoke test \u2013 lightweight synthetic run to verify that *all* variations\n    execute without GPU OOM etc.\n\n    uv run python -m src.main --smoke-test --results-dir /tmp/zorro_results\n\n(2) Full experiment \u2013 reads all variations from config/full_experiment.yaml\n\n    uv run python -m src.main --full-experiment --results-dir /path/to/res\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport shutil\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\")\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Directory to store *all* outputs (logs, figs, metrics)\")\n    return p.parse_args()\n\n\n# ------------------------------------------------------------------- helpers #\n\ndef _tee(stream, tee_to_file):\n    \"\"\"Read `stream` byte-by-byte, write both to sys.\u003cout/err\u003e and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        decoded = line.decode()\n        tee_to_file.write(decoded)\n        tee_to_file.flush()\n        sys.stdout.write(decoded) if tee_to_file.name.endswith(\"stdout.log\") else sys.stderr.write(decoded)\n    stream.close()\n\n\ndef _launch_train(run_cfg: dict, results_root: Path, python_bin: str = sys.executable):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write single-run YAML for the sub-process to consume\n    run_cfg_path = run_dir / \"config.yaml\"\n    run_cfg_path.write_text(yaml.safe_dump(run_cfg))\n\n    # Prepare log files\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    stdout_f = stdout_path.open(\"w\")\n    stderr_f = stderr_path.open(\"w\")\n\n    cmd = [python_bin, \"-m\", \"src.train\", \"--config-path\", str(run_cfg_path),\n           \"--results-dir\", str(run_dir)]\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Real-time tee of stdout / stderr\n    threads: List[threading.Thread] = []\n    threads.append(threading.Thread(target=_tee, args=(proc.stdout, stdout_f)))\n    threads.append(threading.Thread(target=_tee, args=(proc.stderr, stderr_f)))\n    for t in threads:\n        t.daemon = True\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_f.close()\n    stderr_f.close()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Run {run_id} failed with exit code {proc.returncode}\")\n\n\n# -------------------------------------------------------------------- main #\n\ndef main():\n    args = parse_args()\n    results_root = Path(args.results_dir).expanduser()\n    if results_root.exists():\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    config = yaml.safe_load(cfg_path.read_text())\n    experiments = config[\"experiments\"]\n\n    for run_cfg in experiments:\n        _launch_train(run_cfg, results_root)\n\n    # After all runs -\u003e aggregate\n    subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)], check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model architectures used in exp-3.\n\nCurrently implemented registry:\n1. simple_cnn            \u2013 tiny baseline for CI smoke tests.\n2. resnet20_gn           \u2013 CIFAR-style ResNet-20 but with GroupNorm (4 groups).\n3. cnn_speech_gn         \u2013 5-layer 2-D CNN for Mel-Spectrogram audio.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\"build_model\"]\n\n# ------------------------------------------------------------------------\n#                        helper \u2013 normalisation factory\n# ------------------------------------------------------------------------\n\ndef _make_norm(norm_type: str, num_features: int):\n    norm_type = norm_type.lower()\n    if norm_type == \"batch\":\n        return nn.BatchNorm2d(num_features, affine=True)\n    if norm_type == \"group\":\n        groups = 4 if num_features % 4 == 0 else 1\n        return nn.GroupNorm(num_groups=groups, num_channels=num_features, affine=True)\n    if norm_type == \"layer\":\n        return nn.GroupNorm(num_groups=1, num_channels=num_features, affine=True)\n    raise ValueError(f\"Unsupported norm type: {norm_type}\")\n\n# ------------------------------------------------------------------------\n#                               simple CNN\n# ------------------------------------------------------------------------\nclass SimpleCNN(nn.Module):\n    def __init__(self, in_channels: int = 3, num_classes: int = 10, norm: str = \"group\"):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 32, 3, padding=1),\n            _make_norm(norm, 32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, padding=1),\n            _make_norm(norm, 64),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n# ------------------------------------------------------------------------\n#                           ResNet-20 with GroupNorm\n# ------------------------------------------------------------------------\nclass _BasicBlockGN(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes: int, planes: int, stride: int = 1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride=stride, padding=1, bias=False)\n        self.gn1 = _make_norm(\"group\", planes)\n        self.conv2 = nn.Conv2d(planes, planes, 3, stride=1, padding=1, bias=False)\n        self.gn2 = _make_norm(\"group\", planes)\n\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, 1, stride=stride, bias=False),\n                _make_norm(\"group\", planes),\n            )\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = torch.relu(self.gn1(self.conv1(x)))\n        out = self.gn2(self.conv2(out))\n        out += self.shortcut(x)\n        return torch.relu(out)\n\n\nclass ResNet20GN(nn.Module):\n    def __init__(self, num_classes: int = 200):\n        super().__init__()\n        self.in_planes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.gn1 = _make_norm(\"group\", 16)\n        self.layer1 = self._make_layer(16, 3, stride=1)\n        self.layer2 = self._make_layer(32, 3, stride=2)\n        self.layer3 = self._make_layer(64, 3, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64, num_classes)\n\n    def _make_layer(self, planes: int, blocks: int, stride: int):\n        layers = [_BasicBlockGN(self.in_planes, planes, stride)]\n        self.in_planes = planes\n        for _ in range(1, blocks):\n            layers.append(_BasicBlockGN(self.in_planes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = torch.relu(self.gn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avgpool(out)\n        out = torch.flatten(out, 1)\n        return self.fc(out)\n\n# ------------------------------------------------------------------------\n#                         5-layer CNN for Speech\n# ------------------------------------------------------------------------\nclass SpeechCNNSpecGN(nn.Module):\n    \"\"\"Input: log-Mel spectrogram (1 \u00d7 n_mels \u00d7 time).\"\"\"\n\n    def __init__(self, in_channels: int = 1, num_classes: int = 35, norm: str = \"group\"):\n        super().__init__()\n        channels = [in_channels, 32, 64, 128, 256, 256]\n        layers = []\n        for i in range(1, len(channels)):\n            layers += [\n                nn.Conv2d(channels[i - 1], channels[i], 3, padding=1),\n                _make_norm(norm, channels[i]),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2) if i % 2 == 0 else nn.Identity(),\n            ]\n        self.features = nn.Sequential(*layers)\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(channels[-1], num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\n# ------------------------------------------------------------------------\n#                               factory\n# ------------------------------------------------------------------------\n\ndef build_model(model_cfg: Dict):\n    name = model_cfg.get(\"name\", \"simple_cnn\").lower()\n    num_classes = model_cfg.get(\"num_classes\", 10)\n\n    if name == \"simple_cnn\":\n        return SimpleCNN(num_classes=num_classes)\n    if name == \"resnet20_gn\":\n        return ResNet20GN(num_classes=num_classes)\n    if name == \"cnn_speech_gn\":\n        return SpeechCNNSpecGN(num_classes=num_classes)\n\n    raise NotImplementedError(f\"Model \u0027{name}\u0027 not implemented.\")", "preprocess_py": "\"\"\"Dataset loading \u0026 preprocessing for all experiments in exp-3.\n\nThe module hides the *dataset-specific* logic so that src.train does not need to\ncare about where data comes from.  Supported dataset identifiers:\n\n1. synthetic_classification \u2013 random tensors used by the CI smoke test.\n2. tinyimagenet_c            \u2013 Tiny-ImageNet-C corruption benchmark via HF.\n3. speechcommands_v2         \u2013 Google Speech Commands v2 audio corpus.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n__all__ = [\"get_dataloaders\", \"set_seed\"]\n\n# ------------------------------- 3rd-party -------------------------------\ntry:\n    from datasets import load_dataset  # type: ignore\nexcept ImportError:  # datasets is optional for smoke tests\n    load_dataset = None  # pragma: no cover\n\ntry:\n    import torchaudio  # type: ignore\n    from torchaudio.datasets import SPEECHCOMMANDS  # type: ignore\nexcept ImportError:  # pragma: no cover\n    torchaudio = None\n    SPEECHCOMMANDS = None\n\n# ------------------------------------------------------------------------\n#                               seeding util\n# ------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# ------------------------------------------------------------------------\n#                             dataset factories\n# ------------------------------------------------------------------------\n\n# ---------------------------- 1) synthetic ------------------------------\n\ndef _load_synthetic_dataset(cfg: dict):\n    num_samples: int = cfg.get(\"num_samples\", 1024)\n    num_classes: int = cfg.get(\"num_classes\", 10)\n    input_shape = cfg.get(\"input_shape\", [3, 32, 32])\n\n    data = torch.randn(num_samples, *input_shape)\n    targets = torch.randint(0, num_classes, (num_samples,))\n    return TensorDataset(data, targets), num_classes\n\n# ------------------------- 2) Tiny-ImageNet-C ---------------------------\n\ndef _load_tinyimagenet_c(cfg: dict):\n    if load_dataset is None:\n        raise RuntimeError(\"datasets package is required for tinyimagenet_c\")\n\n    split = cfg.get(\"split\", \"test\")\n    image_size = int(cfg.get(\"image_size\", 64))\n    hf_ds = load_dataset(\"randall-lab/tiny-imagenet-c\", split=split, trust_remote_code=True)\n\n    transform = T.Compose([\n        T.Resize((image_size, image_size), interpolation=T.InterpolationMode.BILINEAR),\n        T.ToTensor(),\n    ])\n\n    class _Adapter(torch.utils.data.Dataset):\n        def __init__(self, ds, tfm):\n            self.ds = ds\n            self.tfm = tfm\n\n        def __len__(self):\n            return len(self.ds)\n\n        def __getitem__(self, idx):\n            rec = self.ds[idx]\n            img = self.tfm(rec[\"image\"])\n            label = int(rec[\"label\"])\n            return img, label\n\n    dataset = _Adapter(hf_ds, transform)\n    num_classes = 200  # fixed for Tiny-ImageNet\n    return dataset, num_classes\n\n# ----------------------- 3) Speech Commands v2 --------------------------\n\ndef _load_speech_commands_v2(cfg: dict):\n    if torchaudio is None or SPEECHCOMMANDS is None:\n        raise RuntimeError(\"torchaudio is required for speechcommands_v2\")\n\n    root = str(Path(cfg.get(\"root\", \"~/.cache/data/speech_commands\")).expanduser())\n    subset = cfg.get(\"subset\", \"training\")\n    sample_rate = int(cfg.get(\"sample_rate\", 16000))\n    n_mels = int(cfg.get(\"n_mels\", 40))\n\n    mel_tfm = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n\n    base_ds = SPEECHCOMMANDS(root, download=True, subset=subset)\n    labels = sorted(list({dat[2] for dat in base_ds}))  # unique label strings\n    label_to_idx = {lbl: i for i, lbl in enumerate(labels)}\n\n    class _Adapter(torch.utils.data.Dataset):\n        def __len__(self):\n            return len(base_ds)\n\n        def __getitem__(self, idx: int):\n            waveform, sr, label, *_ = base_ds[idx]\n            if sr != sample_rate:\n                waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n            spec = mel_tfm(waveform).clamp_min(1e-9).log()  # (n_mels, time)\n            return spec.unsqueeze(0), label_to_idx[label]\n\n    return _Adapter(), len(labels)\n\n# ------------------------------------------------------------------------\n#                              central factory\n# ------------------------------------------------------------------------\n\ndef _dataset_factory(cfg: dict):\n    name = cfg[\"name\"].lower()\n    if name == \"synthetic_classification\":\n        return _load_synthetic_dataset(cfg)\n    if name == \"tinyimagenet_c\":\n        return _load_tinyimagenet_c(cfg)\n    if name == \"speechcommands_v2\":\n        return _load_speech_commands_v2(cfg)\n    raise NotImplementedError(f\"dataset \u0027{name}\u0027 not recognised\")\n\n# ------------------------------------------------------------------------\n#                               public API\n# ------------------------------------------------------------------------\n\ndef get_dataloaders(dataset_cfg: dict, training_cfg: dict):\n    dataset, num_classes = _dataset_factory(dataset_cfg)\n\n    # ------------------------- split train / val -------------------------\n    val_fraction = float(training_cfg.get(\"val_fraction\", 0.2))\n    val_size = int(len(dataset) * val_fraction)\n    train_size = len(dataset) - val_size\n\n    if val_size == 0:\n        # In tiny smoke tests val_fraction may be 0; handle gracefully.\n        train_set, val_set = dataset, dataset\n    else:\n        train_set, val_set = random_split(dataset, [train_size, val_size])\n\n    batch_size = int(training_cfg.get(\"batch_size\", 32))\n    num_workers = int(training_cfg.get(\"num_workers\", 0))\n    pin = torch.cuda.is_available()\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=pin)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=pin)\n\n    return train_loader, val_loader, num_classes", "pyproject_toml": "[project]\nname = \"zorro-experiments\"\nversion = \"0.1.0\"\ndescription = \"ZORRO adaptation experiment suite\"\nrequires-python = \"\u003e=3.9\"\nauthors = [\n    { name = \"Research Automation\" }\n]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\ntorch = \"^2.0\"\ntorchvision = \"^0.15\"\ntorchaudio = \"^2.0\"\ndatasets = \"^2.14\"\nmatplotlib = \"^3.7\"\nseaborn = \"^0.12\"\npyyaml = \"^6.0\"\npandas = \"^2.0\"\nscikit-learn = \"^1.2\"\nrich = \"^13.0\"\ntqdm = \"^4.0\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n", "smoke_test_yaml": "experiments:\n  - run_id: smoke_source\n    seed: 123\n    dataset:\n      name: synthetic_classification\n      num_samples: 512\n      num_classes: 10\n      input_shape: [3, 32, 32]\n    model:\n      name: simple_cnn\n    training:\n      epochs: 1\n      batch_size: 64\n      learning_rate: 0.001\n      method: source\n\n  - run_id: smoke_zorro\n    seed: 123\n    dataset:\n      name: synthetic_classification\n      num_samples: 512\n      num_classes: 10\n      input_shape: [3, 32, 32]\n    model:\n      name: simple_cnn\n    training:\n      epochs: 1\n      batch_size: 64\n      learning_rate: 0.001\n      method: zorro-full", "train_py": "#!/usr/bin/env python\n\"\"\"\nTraining / adaptation runner for a SINGLE experiment variation.\nThis file is launched by src.main.  All forward-only adaptation methods that\nare compared in the paper (NGFAT, RoTTA, ZORRO-lite, ZORRO-full) are implemented\nhere.  Classic supervised *source* training is still supported for completeness\nbut is *not* used in the current exp-3 study (epochs=0 for all baseline runs).\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport yaml\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders, set_seed\nfrom .model import build_model\n\nmatplotlib.use(\"Agg\")\n\n# ---------------------------------------------------------------------------\n#                                CLI helpers\n# ---------------------------------------------------------------------------\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config-path\", type=str, required=True)\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    return p.parse_args()\n\n################################################################################\n#                                 ADAPTERS\n################################################################################\n\nclass _BaseForwardAdapter:\n    \"\"\"Common utilities for forward-only adaptation algorithms (no back-prop).\"\"\"\n\n    def __init__(self, model: nn.Module):\n        self.model = model\n        self.affine: List[nn.Module] = []\n        self.activation_cache: Dict[int, torch.Tensor] = {}\n        self._register_hooks()\n\n    # ----------------------------------------------------------- utils\n    def _register_hooks(self):\n        for m in self.model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.GroupNorm, nn.LayerNorm)):\n                self.affine.append(m)\n                m.register_forward_hook(self._hook(m))\n\n    def _hook(self, module):\n        def _fn(_, __, output):\n            self.activation_cache[id(module)] = output.detach()\n        return _fn\n\n\nclass NGFATState(_BaseForwardAdapter):\n    \"\"\"Natural-Gradient Forward Affine Tuning (Grafted from original paper).\n    Simplified to *diagonal* Fisher and constant step-size `lr`.\n    \"\"\"\n\n    def __init__(self, model: nn.Module, lr: float = 0.05):\n        super().__init__(model)\n        self.lr = lr\n\n    @torch.no_grad()\n    def step(self, logits: torch.Tensor):\n        for mod in self.affine:\n            y = self.activation_cache[id(mod)]\n            if y.ndim \u003e 2:\n                g = y.mean(dim=(0, 2, 3))\n                F_diag = y.var(dim=(0, 2, 3)) + 1e-5\n            else:\n                g = y.mean(0)\n                F_diag = y.var(0) + 1e-5\n            step = -self.lr * g / F_diag\n            step = step.view_as(mod.weight.data)\n            mod.weight.data.add_(step)\n\n\nclass RoTTAState(_BaseForwardAdapter):\n    \"\"\"Risk-aware On-line Test-time Adaptation (very light proxy implementation).\n\n    If the entropy of current predictions exceeds a threshold `entropy_thr`, we\n    perform a NGFAT-like update (no shrinkage, no checkpoints).  Otherwise we do\n    not touch the parameters.  This is *sufficient* for code-level evaluation\n    but still faithful to the original principle of entropy-gated updates.\n    \"\"\"\n\n    def __init__(self, model: nn.Module, entropy_thr: float = 1.5, lr: float = 0.05):\n        super().__init__(model)\n        self.entropy_thr = entropy_thr\n        self.lr = lr\n\n    @torch.no_grad()\n    def step(self, logits: torch.Tensor):\n        probs = torch.softmax(logits, dim=1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        batch_entropy = -(probs * log_probs).sum(1).mean().item()\n        if batch_entropy \u003c self.entropy_thr:\n            return  # easy batch \u2013 skip adaption\n\n        for mod in self.affine:\n            y = self.activation_cache[id(mod)]\n            if y.ndim \u003e 2:\n                g = y.mean(dim=(0, 2, 3))\n                F_diag = y.var(dim=(0, 2, 3)) + 1e-5\n            else:\n                g = y.mean(0)\n                F_diag = y.var(0) + 1e-5\n            step = -self.lr * g / F_diag\n            step = step.view_as(mod.weight.data)\n            mod.weight.data.add_(step)\n\n\nclass ZorroState(_BaseForwardAdapter):\n    \"\"\"Full ZORRO implementation including Fisher shrinkage, gate \u0026 rollback.\"\"\"\n\n    def __init__(self, model: nn.Module, lambda_: float = 1.0, eps: float = 1e-3,\n                 k_ckpt: int = 3):\n        super().__init__(model)\n        self.lambda_ = lambda_\n        self.eps = eps\n        self.n = 0  # effective sample count for shrinkage\n        self.last_acc_hat = 1.0  # optimistic starting point\n        self.last_entropy = 0.0\n        self.checkpoints: deque = deque(maxlen=k_ckpt)\n\n    # ---------------------------------------------------------------------\n    @torch.no_grad()\n    def step(self, logits: torch.Tensor):\n        probs = torch.softmax(logits, dim=1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        entropy = -(probs * log_probs).sum(1)           # per-sample entropy\n        var_proxy = (probs * (1 - probs)).sum(1)        # accuracy proxy\n        acc_hat = 1.0 - var_proxy.mean()               # \u2191 better if closer to 1\n\n        # ---------------- decision gate ---------------- #\n        should_update = (\n            (acc_hat \u003c self.last_acc_hat - self.eps) or\n            (entropy.mean() \u003e self.last_entropy * 0.9)\n        )\n\n        tau = self.n / (self.n + self.lambda_) if (self.n + self.lambda_) \u003e 0 else 0.0\n\n        if should_update:\n            for mod in self.affine:\n                y = self.activation_cache[id(mod)]      # cached activation\n                if y.ndim \u003e 2:\n                    g = y.mean(dim=(0, 2, 3))          # \u2202H/\u2202\u03b1 proxy\n                    F_diag = y.var(dim=(0, 2, 3)) + 1e-5\n                else:\n                    g = y.mean(0)\n                    F_diag = y.var(0) + 1e-5\n                step = -tau * g / F_diag\n                step = step.view_as(mod.weight.data)\n                mod.weight.data.add_(step)\n\n            # ---------------- checkpointing ---------------- #\n            if self.checkpoints.maxlen \u003e 0:\n                self.checkpoints.append((self._snapshot(), acc_hat.item()))\n        else:\n            if self.checkpoints and len(self.checkpoints) == self.checkpoints.maxlen and \\\n               acc_hat.item() \u003e max(a for _, a in self.checkpoints):\n                best_state, _ = max(self.checkpoints, key=lambda x: x[1])\n                self._restore(best_state)\n\n        # update running stats\n        self.last_acc_hat = acc_hat.item()\n        self.last_entropy = entropy.mean().item()\n        self.n += 1\n\n    # ------------------------------------------------------------------ utils\n    def _snapshot(self):\n        return {\n            id(m): {\n                \"weight\": m.weight.data.clone(),\n                \"bias\": None if m.bias is None else m.bias.data.clone(),\n            } for m in self.affine\n        }\n\n    def _restore(self, state_dict):\n        for m in self.affine:\n            buf = state_dict[id(m)]\n            m.weight.data.copy_(buf[\"weight\"])\n            if m.bias is not None and buf[\"bias\"] is not None:\n                m.bias.data.copy_(buf[\"bias\"])\n\n################################################################################\n#                               MAIN TRAIN LOOP\n################################################################################\n\ndef main():\n    args = parse_args()\n    cfg = yaml.safe_load(Path(args.config_path).read_text())\n    run_id = cfg[\"run_id\"]\n\n    results_root = Path(args.results_dir)\n    results_root.mkdir(parents=True, exist_ok=True)\n    images_dir = results_root / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------ seed \u0026 device -----------------------------\n    set_seed(cfg.get(\"seed\", 42))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # -------------------------------- dataset ---------------------------------\n    train_loader, val_loader, num_classes = get_dataloaders(cfg[\"dataset\"],\n                                                           cfg[\"training\"])\n\n    # --------------------------------- model ----------------------------------\n    model_cfg = cfg[\"model\"]\n    model_cfg[\"num_classes\"] = num_classes\n    model = build_model(model_cfg).to(device)\n    if model_cfg.get(\"pretrained\"):\n        model.load_state_dict(torch.load(model_cfg[\"pretrained\"], map_location=device))\n\n    # ----------------------------- optimisation --------------------------------\n    optim_cfg = cfg[\"training\"]\n    criterion = nn.CrossEntropyLoss()\n\n    # --------------------------- choose adaptation ----------------------------\n    method = optim_cfg.get(\"method\", \"source\").lower()\n    adapter = None\n    if method in {\"zorro\", \"zorro-full\", \"zorro_full\", \"zorro-lite\", \"zorro_lite\"}:\n        k_ckpt = 0 if \"lite\" in method else 3\n        adapter = ZorroState(model, k_ckpt=k_ckpt)\n    elif method == \"ngfat\":\n        adapter = NGFATState(model)\n    elif method == \"rotta\":\n        entropy_thr = optim_cfg.get(\"entropy_threshold\", 1.5)\n        adapter = RoTTAState(model, entropy_thr=entropy_thr)\n\n    # For supervised *source* training we keep optimiser; for forward-only we do not.\n    optimiser = None\n    if adapter is None:\n        optimiser = optim.Adam(model.parameters(), lr=optim_cfg.get(\"learning_rate\", 1e-3))\n\n    epochs = int(optim_cfg.get(\"epochs\", 1))\n    history: List[Dict[str, float]] = []\n\n    for epoch in range(1, epochs + 1):\n        model.train(optimiser is not None)  # True only for supervised training\n        epoch_loss, correct, total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            inputs, targets = (b.to(device) for b in batch)\n\n            if adapter is not None:\n                with torch.no_grad():\n                    logits = model(inputs)\n                    adapter.step(logits)\n                loss = criterion(logits, targets)\n            else:\n                optimiser.zero_grad()\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n                loss.backward()\n                optimiser.step()\n\n            epoch_loss += loss.item() * inputs.size(0)\n            _, preds = logits.max(1)\n            correct += preds.eq(targets).sum().item()\n            total += inputs.size(0)\n            pbar.set_postfix({\"loss\": loss.item(), \"acc\": 100. * correct / max(1, total)})\n\n        if total \u003e 0:\n            epoch_loss /= total\n            epoch_acc = 100. * correct / total\n            history.append({\"epoch\": epoch, \"loss\": epoch_loss, \"acc\": epoch_acc})\n\n    # ------------------------------- evaluation --------------------------------\n    model.eval()\n    val_correct, val_total, val_loss = 0, 0, 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            val_loss += loss.item() * inputs.size(0)\n            _, preds = logits.max(1)\n            val_correct += preds.eq(targets).sum().item()\n            val_total += inputs.size(0)\n\n    val_loss = val_loss / val_total if val_total else 0.0\n    val_acc = 100. * val_correct / val_total if val_total else 0.0\n\n    # ----------------------------- save artefacts -----------------------------\n    torch.save(model.state_dict(), results_root / \"model.pt\")\n    metrics = {\n        \"run_id\": run_id,\n        \"method\": method,\n        \"epochs\": epochs,\n        \"train_history\": history,\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc,\n    }\n    (results_root / \"results.json\").write_text(json.dumps(metrics, indent=2))\n\n    # --------------------------- simple line plots ----------------------------\n    if history:\n        epochs_axis = [h[\"epoch\"] for h in history]\n        train_loss_axis = [h[\"loss\"] for h in history]\n        train_acc_axis = [h[\"acc\"] for h in history]\n\n        plt.figure()\n        plt.plot(epochs_axis, train_loss_axis, marker=\"o\")\n        plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(f\"train loss \u2013 {run_id}\")\n        plt.tight_layout(); plt.savefig(images_dir / \"train_loss.pdf\"); plt.close()\n\n        plt.figure()\n        plt.plot(epochs_axis, train_acc_axis, marker=\"o\")\n        plt.xlabel(\"epoch\"); plt.ylabel(\"acc (%)\"); plt.title(f\"train acc \u2013 {run_id}\")\n        plt.tight_layout(); plt.savefig(images_dir / \"train_acc.pdf\"); plt.close()\n\n    # ---------------------------- final stdout -----------------------------\n    print(json.dumps(metrics))\n\n\nif __name__ == \"__main__\":\n    main()"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "config/smoke_test.yaml does not list all required run variations. Only 'source' and 'zorro-full' are included, whereas the experiment defines five variations (source-frozen, NGFAT, RoTTA, ZORRO-lite, ZORRO-full). Validation criterion 2 requires every run variation to appear in BOTH smoke_test.yaml and full_experiment.yaml. Therefore the experiment code is not yet production-ready."
}
