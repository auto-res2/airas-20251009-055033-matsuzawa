
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning Konstantin Sozykin ∗† Andrei Chertkov ∗† Roman Schutski † Anh-Huy Phan † Andrzej Cichocki †‡§ Ivan Oseledets †¶ Abstract We present a novel procedure for optimization based on the combination of efﬁcient quantized tensor train representation and a generalized maximum matrix volume principle. We demonstrate the applicability of the new Tensor Train Optimizer (TTOpt) method for various tasks, ranging from minimization of multidimensional functions to reinforcement learning. Our algorithm compares favorably to popular gradient-free methods and outperforms them by the number of function evaluations or execution time, often by a signiﬁcant margin. 1 Introduction In recent years learning-based algorithms achieved impressive results in various applications, ranging from image and text analysis and generation [ 55] to sequential decision making and control [ 43] and even quantum physics simulations [ 51]. The vital part of every learning-based algorithm is an optimization procedure, e.g., Stochastic Gradient Descent. In many situations, however, the problem-speciﬁc target function is not differentiable, too complex, or its gradients are not helpful due to the non-convex nature of the problem [32, 2]. The examples include hyper-parameter selection during the training of neural models, policy optimization in reinforcement learning (RL), training neural networks with discrete (quantized) weights [60] or with non-differentiable loss functions [ 21]. In all these contexts, efﬁcient direct gradient-free optimization procedures are highly needed. Recently, [56] showed that an essential class of gradient-free methods, namely the evolutionary strategies (ES) [23, 27, 59], are competitive in reinforcement learning problems. In RL, the goal is to ﬁnd the agent’s action distributionπ(the policy), maximizing some cumulative reward functionJ. The policy is usually parameterized with a set of parameters θ. It follows that the reward is a function of the parameters of the policy: J(π(θ)) = J(θ). The idea of [56] and similar works [39, 15, 35, 10, 11] is to directly optimize the cumulative reward function J(θ) = J(θ1,θ2,...,θ d) with respect to the parameters of the policy. We pursued a similar approach to transform a traditional Markov Decision Process into an optimization problem (we provide the details in Appendix B.3). Although the agents trained with ES often demonstrate more rich behavior and better generalization compared to traditional gradient-based policy optimization, the convergence of ES is often slow [11]. ∗Equal Contribution, corresponding emails {konstantin.sozykin,a.chertkov}@skoltech.ru †Center of Artiﬁcial Intelligence Technology, Skolkovo Institute of Science and Technology (Skoltech) Moscow, Russia ‡RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan §Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland ¶Artiﬁcial Intelligence Research Institute (AIRI), Moscow, Russia 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2205.00293v2  [cs.LG]  28 Sep 2022As an alternative to previous works, we present a tensor-based6 gradient-free optimization approach and apply it to advanced continuous control RL benchmarks. The algorithm that we called Tensor- Train (TT) Optimizer ( TTOpt), works for multivariable functions with discrete parameters by reformulating the optimization problem in terms of tensor networks. Consider a function J(θ) : Nd −→Rof a d-dimensional argument θ, where each entry θk of the vector θtakes a value in the discrete set {ωi}N i=1. The function J may be viewed as an implicitly deﬁned d-dimensional tensor J. Each entry in Jis a value of J for some argument. Maximizing J is equivalent to ﬁnding the sets of indices {θ(m) max}M m=1 of maximal entries {j(m) max}M m=1 of J. By using only a tiny fraction of adaptively selected tensor elements (e.g., a small number of function evaluations), our method builds a representation of Jin the TT-format and ﬁnds a set of the largest elements. Although the algorithm works only with functions of discrete arguments, the grid size for each parameter can be huge thanks to the efﬁciency of the TT-format. Fine discretization makes it possible to almost reach a continuous limit and obtain large precision with TTOpt. The strength of our approach is, however, the direct handling of discrete (quantized) parameters. Contributions. We propose an efﬁcient gradient-free optimization algorithm for multivariable functions based on the low-rank TT-format and the generalized maximum matrix volume principle7. We demonstrate that our approach is competitive with a set of popular gradient-free methods for optimizing benchmark functions and neural network-based agents in RL problems. We empirically show that agents with discrete (heavily quantized) weights perform well in continuous control tasks. Our algorithm can directly train quantized neural networks, producing policies suitable for low-power devices. 2 Optimization with tensor train In this section, we introduce a novel optimization algorithm. We show how to represent the optimiza- tion problems in the discrete domain efﬁciently and then formulate optimization as a sampling of the objective function guided by the maximum volume principle. 2.1 Discrete formulation of optimization problems We ﬁrst need to transfer the problem to the discrete domain to apply our method. It may seem that discretizing an optimization problem will make it harder. However, it will allow us to use powerful techniques for tensor network representation to motivate the algorithm. For each continuous parameter θk (k= 1,2,...,d ) of the objective function J(θ) we introduce a grid {θ(nk) k }Nk nk=1. At each point (θ(n1) 1 ,θ(n2) 2 ,...,θ (nd) d ) of this grid with index (n1,n2,...,n d) the objective function takes a value J(θ(n1) 1 ,θ(n2) 2 ,...,θ (nd) d ) ≡J [n1,n2,...,n d]. We thus can regard the objective function as an implicit d-dimensional tensor Jwith sizes of the modes N1,N2,...,N d. Finding the maximum of the function J(θ) translates into ﬁnding the maximal element of the tensor Jin the discrete setting. Notice that the number of elements of Jequals: |J| = N1 ·N2 ·... ·Nd ∼(max1≤k≤dNk)d. The size of Jis exponential in the number of dimensions d. This tensor can not be evaluated or stored for sufﬁciently large d. Fortunately, efﬁcient approximations were developed to work with multidimensional arrays in recent years. Notable formats include Tensor Train (TT) [ 49, 48, 52], Tensor Chain/Tensor Ring [ 31, 67] and Hierarchical Tucker [ 22]. We use the most studied TT- format [13], but the extensions of our method to other tensor decompositions are possible. 6By tensors we mean multidimensional arrays with a number of dimensions d(d≥1). A two-dimensional tensor (d= 2) is a matrix, and when d= 1 it is a vector. For scalars we use normal font, we denote vectors with bold letters and we use upper case calligraphic letters (A,B,C,... ) for tensors with d> 2. Curly braces deﬁne sets. We highlight discrete and continuous scalar functions of multidimensional argument in the appropriate font, e.x., J(·), in this case, the maximum and minimum values of the function are denoted as Jmax and Jmin, respectively. 7We implemented the proposed algorithm within the framework of the publicly available software product: https://github.com/AndreiChertkov/ttopt. 22.2 Tensor Train decomposition Deﬁnition 2.1. A tensor J∈ RN1×N2×···×Nd is said to be in the TT-format [48] if its elements are represented by the following expression J[n1,n2,...,n d] = R0∑ r0=1 R1∑ r1=1 ··· Rd∑ rd=1 G1[r0,n1,r1]G2[r1,n2,r2] ... Gd[rd−1,nd,rd], (1) where nk = 1,2,...,N k for k= 1,2,...,d . In TT-format the d-dimensional tensor Jis approximated as a product of three-dimensional tensors Gk ∈ RRk−1×Nk×Rk , called TT-cores. The sizes of the internal indices R0,R1,··· ,Rd (with convention R0 = Rd = 1 ) are known as TT-ranks. These ranks control the accuracy of the approximation. The storage of the TT-cores, G1,G2,..., Gd, requires at most d·max1≤k≤dNk ·(max0≤k≤dRk)2 memory cells, and hence the TT-approximation is free from the curse of dimensionality 8 if the TT-ranks are bounded. The basic linear algebra operations (such as ﬁnding a norm, differentiation, integration, and others) can also be implemented in the TT-format with polynomial complexity in dimensionality and mode size. Building TT-approximation. Several efﬁcient schemes were proposed to ﬁnd TT-approximation if all or some of the elements of the initial tensor are known or may be generated by the function’s call. Examples include TT-SVD [49, 48], TT-ALS [28] and TT-CAM [50] (Cross Approximation Method in the TT-format). We build upon the TT-CAM but modify it not to compute the approximation for theentire tensor, but rather to ﬁnd a small subset of its maximal entries. The original algorithm builds a TT-approximation by adaptively requesting elements of the input tensor. As we will show below, these elements with high probability will have large absolute values. Based on this observation, we formulate a robust optimization algorithm for multivariate functions (either discrete or continuous). To simplify the understanding, we outline the approach for the two-dimensional case, and after that, we describe our gradient-free optimization method for the multidimensional case. 2.3 Maximal element in a matrix The Cross Approximation Method (CAM) for matrices [19, 9, 1] is a well-established algorithm for building a rank-Rapproximation ˜Jof an implicitly given matrix J: J≃ ˜J, ˜J= JC ˆJ−1JR, (2) where JC consists of Rcolumns of J, JR is composed of Rrows of J, and ˆJ is a submatrix at their intersection. Such approximation (also called cross or skeleton decomposition) may be built iteratively using a well known alternating directions method and a maximum volume ( maxvol) algorithm9 [19], as we will sketch below. Intuition behind TTOpt. The main interest in optimization problems is not the approximation (2) itself, but the following property of the resulting maximum volume submatrix ˆJ. [19] proved that if ˆJis an R×Rsubmatrix of maximal volume (in selected rows and columns) then the maximal (by modulus) element ˆJmax ∈ ˆJbounds the absolute maximal element Jmax in the full matrix J: ˆJmax ·R2 ≥Jmax. (3) 8The number of elements of an uncompressed tensor (hence, the memory required to store it) and the number of elementary operations required to perform computations with such a tensor grow exponentially in dimensionality. This problem is called the curse of dimensionality. 9The maxvol algorithm ﬁnds Rrows in an arbitrary non-degenerate matrix A∈RN×R (N >R) which span a maximal-volume R×Rsubmatrix ˆA. The matrix ˆA∈Ahas maximal value of the modulus of the determinant on the set of all nondegenerate square submatrices of the sizeR×R. We describe the implementation of maxvol in Appendix A.1. The algorithm greedily rearranges rows of Ato maximize submatrix volume. Its computational complexity is O(NR2 + KNR), where Kis a number of iterations. 3Figure 1: The scheme of the cross approximation algorithm for matrices using the alternating direc- tion and maximal-volume principle. Green bars represent generated rows/columns; purple bars are rows/columns selected for generation in the next step by the maxvol algorithm. The method allows to ﬁnd the optimum of the two dimensional func- tion J(θ1,θ2). Figure 2: Conceptual scheme of TTOpt al- gorithm based on the alternating direction and maximal-volume approaches for tensors. Only a small part of the tensor is explicitly generated during this procedure, as shown here with green columns. For the simplicity of presentation, the rows and columns se- lected at iterations are drawn as continuous blocks (they are not in practice). This statement is evident for R= 1, and for the case R> 1 it gives an upper bound for the element. By using elementwise transformations of J, this upper bound can be used to obtain a sequence that converges to the global optimum. The main idea of the maxvol-based methods is that it is easier to ﬁnd a submatrix with a large volume rather than the element with the largest absolute value. Moreover, our numerical experiments show that this bound is pessimistic, and in practice, the maximal-volume submatrix contains the element which is very close to the optimal one. TTOpt algorithm for matrices. The idea of the TTOpt algorithm for matrices is to iteratively search for the maximal volume submatrices in the column and row space of the implicitly given10 input matrix J∈RN1×N2 . After T iterations a series of “intersection” matrices {ˆJ(t)}T t=1 ∈RR×R is produced. The maximal element is searched in these small submatrices. We schematically represent the TTOpt algorithm in Figure 1, and a description is given below: 1. At the initial stage, we set the expected rank of the approximation, R, and select Rran- dom columns I(C,1). We then generate the corresponding column submatrix J(1) C = J[: ,I(C,1)] ∈RN1×R. Using the maxvol algorithm, we ﬁnd the maximal-volume submatrix ˆJ(1) ∈RR×R in J(1) C and store its row indices in the list I(R,1). 2. The indices I(R,1) are used to generate a row submatrix J(2) R = J[I(R,1),:] ∈RR×N2 . Then, using the maxvol algorithm, we ﬁnd the maximal-volume submatrix ˆJ(2) in the matrix J(2) R and store the corresponding column indices in the list I(C,2). 10The matrix is speciﬁed as a function J(·) that allows to calculate the value of an arbitrary requested element (n1,n2), where 1 ≤n1 ≤N1 and 1 ≤n2 ≤N2. We present the approach to approximate the value of the maximum modulus element of such a matrix. The method of ﬁnding the minimal or maximal elements within the framework of this algorithm will be described in Section 2.6. 43. We generate the related columns J(3) C = J[:,I(C,2)], apply again the maxvol algorithm to the column submatrix, and iterate the process until convergence. 4. The approximate value of the maximum modulus element of the matrix Jis found as ˆJmax = max ( max ( ˆJ(1)), max ( ˆJ(2)), ..., max ( ˆJ(T)) ) . (4) 2.4 Optimization in the multidimensional case As we explained previously, the target tensor,J, is deﬁned implicitly, e.g., by a multivariable function J. We propose a novel method to ﬁnd the optimum in this implicit tensor. We outline the approach below and provide detailed algorithms in Appendix A.2. As shown in Figure 2, we begin by considering the ﬁrst unfolding11 J1 ∈RN1×N2...Nd of the tensor Jand select R1 random columns I(C) 1 . Precisely, I(C) 1 here is a list of R1 random multi-indices of size d−1, which specify positions along modes k = 2 to k = d. We then generate the submatrix J(C) 1 ∈RN1×R1 for all positions along the ﬁrst mode (shown in green in Figure 2). Like in matrix case, we apply the maxvol algorithm to ﬁnd the maximal-volume submatrix ˆJ1 ∈RR1×R1 and store the corresponding indices of R1 rows in the list I(R) 1 . In contrast with matrix case, we cannot generate the row submatrix J(R) 1 ∈RR1×N2N3...Nd for the selected row indices I(R) 1 , since it contains an exponential number of elements. The following trick is used instead. We consider the implicit matrix J(R) 1 and reshape it to a new matrix J2 ∈ RR1N2×N3...Nd . We sample R2 random columns I(C) 2 in the matrix J2 and generate the entire small submatrix J(C) 2 ∈RR1N2×R2 . Next we ﬁnd the maximal-volume submatrix ˆJ2 ∈RR2×R2 in J(C) 2 and store the corresponding R2 row multi-indices in the list I(R) 2 . The resulting submatrix J(R) 2 ∈RR2×N3N4...Nd is then transformed (without explicitly evaluating its elements) into the matrix J3 ∈RR2N3×N4...Nd . We continue the described operations, called sweeps, until the last mode of the initial tensor,J, is reached. After that, we repeat the process in the opposite direction, sampling now the row indices instead of the column indices. These sequences of forward and backward sweeps are continued until the algorithm converges to some row and column indices for all unfolding matrices 12 or until the user-speciﬁed limit on the number of requests to the objective function, J, is exceeded. Finally, after T sweeps, the approximate value of the maximum modulo element can be found by the formula (4), as in the two-dimensional case. Note that currently there is no analog of Eq. (3) in the multidimensional case, and hence there are no formal guarantees of the convergence of the sweeps to the global minimum, nor the rate of this convergence. The only guarantee is that the result will monotonically improve with iterations. 2.5 Complexity of the algorithm It can be easily shown that the described algorithm requires to evaluate only O ( d·max1≤k≤d ( NkR2 k )) elements of the implicit tensor in one sweep. Thus, with a total number of sweeps T, we will have O ( T ·d· max 1≤k≤d ( NkR2 k )) , (5) calls to the objective function J. In practice, it turns out to be more convenient to limit the maximum number of function calls, M, according to the computational budget. 11The k-th unfolding Jk for the d-dimensional tensor J ∈ RN1×N2×···×Nd is the matrix Jk ∈ RN1...Nk×Nk+1...Nd ,with elements Jk[ n1,...,n k,nk+1,...,n d] ≡J[n1,n2,...,n d] for all indices. 12The TT-approximation(1) of the tensor Jmay be recovered from the generated columnsJ(C) k and maximal- volume submatrices ˆJk (k= 1,2,...,d ) as follows: Gk = J(C) k ˆJ−1 k ∈RRk−1Nk×Rk , where Gk is the 2-th unfolding of the k-th TT-core. However, we do not consider this point in more detail, since in our work, the main task is to ﬁnd the minimum or maximum value of the tensor, not to construct its low-rank approximation. 5If the time of a single call to J is signiﬁcant, then the effort spent on the algorithm’s operation will be negligible. Otherwise, the bottleneck will be the calculation of the maximal-volume submatrices by the maxvol algorithm. Taking into account the estimate of the maxvol complexity, given in Appendix A.1, it can be shown that in this case the complexity of our algorithm is O ( T ·d· max 1≤k≤d ( NkR3 k )) . (6) 2.6 Implementation details For the effective implementation of the TTOpt algorithm, the following important points should be taken into account (see also the detailed pseudocode in Appendix A.2). Stability. Submatrices J(C) k (or J(k) C and J(k) R for the two-dimensional case) that arise during the iterations may degenerate, and in this case it is impossible to apply the maxvol algorithm. To solve this problem, we ﬁrst calculate the QR decomposition for these matrices and then apply the maxvol to the corresponding Qfactors13. Rank selection. We do not know in advance the exact ranks R1,R2,...,R d of the unfolding matrices (or rank Rof the matrix Jfor the two-dimensional case). Therefore, instead of the maxvol algorithm, we use its modiﬁcation, i.e., the rect_maxvol algorithm14 [42], within the framework of which several (“most important”) rows are added to the maximal-volume submatrix. In this case, we have ˆJ(C) 1 ∈R(R1+∆R1)×R1 , ˆJ(C) 2 ∈R(R2+∆R2)×R2 , etc. Note that the ﬁnal approximation obtained in this case may have an overestimated rank, which, if necessary, can be reduced by appropriate rounding, for example, by truncated SVD decomposition. Mapping function. Maximal-volume submatrices contain the maximum modulus element but not the minimum or maximum element of the tensor (i.e., the sign is not taken into account). We introduce a dynamic mapping function to ﬁnd the global minimum (or maximum). Instead of J at each step of the algorithm, we evaluate15: g(x) = π 2 −atan (J(x) −Jmin) , (7) where Jmin is the current best approximation for the minimum element of the tensor. Quantization. To reach high accuracy, we often need ﬁne grids. In the case when the sizes of the tensor modes N1,N2,...,N d are large, the sizes of unfolding matrices become large, which leads to a signiﬁcant increase in computational complexity of the maxvol algorithm. To solve this problem, we apply additional compression based on quantization of the tensor modes [47]. Assume without the loss of generality that the size of each mode is Nk = Pq (k= 1,2,...,d ; P ≥2; q≥2). Then we can reshape the original d-dimensional tensor J∈ RN1×N2×···×Nd into the tensor ˜J∈ RP×P×···×P of a higher dimension d·q, but with smaller modes of size P. The TTOpt algorithm can be applied for this “long” tensor instead of the original one16. We found that this idea signiﬁcantly boosts the accuracy of our algorithm and reduces the complexity and execution time. Typically,P is taken as small as possible, e.g. P = 2. 3 Experiments To demonstrate the advantage of the proposed optimization method, we tested TTOpt on several numerical problems. First, we consider analytical benchmark functions and then the practically 13It can be shown that this operation does not increase the complexity estimate (6) of the algorithm. 14The rect_maxvol algorithm allows to ﬁnd R+ ∆Rrows in an arbitrary nondegenerate matrix A∈RN×R (N >R, N ≥R+ ∆R) which form an approximation to the rectangular maximal-volume submatrix of the given matrix A. Details about rect_maxvol are provided in Appendix A.1. 15The mapping function should be continuous, smooth, and strictly monotone. There are various ways to choose such a function. However, during test runs, it turned out that the proposed function (7) is most suitable. 16If the maximal rank R>P , we select all indices for ﬁrst kmodes until Pk >R. We note however that this detail does not change the global behavior of the TTOpt algorithm. 6Table 1: Comparison of the TTOpt optimizer versus baselines in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for various benchmark functions. See Table 1 in Appendix B.1 with the list of functions and their properties. The reported values are averaged over 10 independent runs. A upper half of the table presents gradient free (zeroth order) methods, and lower half is for ﬁrst and second order methods. F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 TTOPT ϵ 3.9 E-06 2.9 E-07 1.8 E-12 4.4 E-15 2.8 E-02 1.1 E-01 5.5 E-09 4.6 E-11 1.8 E-01 1.3 E-04 τ 2.61 2.44 2.45 2.52 2.40 2.48 2.39 2.60 2.32 2.44 GA ϵ 9.7 E-02 8.4 E-03 5.8 E-03 2.0 E+00 3.9 E-04 1.0 E+01 1.2 E-01 7.9 E-01 6.2 E-03 4.2 E+03 τ 6.21 4.56 5.09 4.87 5.85 5.69 5.05 5.04 5.04 4.70 OPEN ES ϵ 1.8 E-01 1.2 E-02 1.7 E-02 2.0 E+00 1.2 E-03 9.7 E+00 3.8 E+00 2.1 E+00 1.8 E-02 4.2 E+03 τ 2.62 1.08 1.62 1.08 2.41 2.04 1.30 1.39 1.62 1.12 CMA ES ϵ 5.1 E+287 3.1 E-01 9.3 E-77 2.0 E+00 7.6 E+289 1.9 E+01 5.5 E-02 9.3 E+01 5.3 E+289 1.7 E+282 τ 10.36 8.50 9.40 9.13 12.86 9.76 9.04 9.13 11.15 8.86 DE ϵ 1.1 E+00 4.3 E-02 3.3 E-02 9.0 E-05 1.8 E-01 2.6 E-01 2.0 E-01 6.2 E+00 6.6 E-01 3.8 E+02 τ 38.91 38.06 51.05 39.48 41.35 41.40 41.34 41.31 37.97 38.64 NB ϵ 1.5 E+01 6.5 E+00 3.9 E+01 1.2 E-01 2.4 E+01 6.6 E+00 2.6 E+10 6.3 E+01 3.4 E+00 3.2 E+03 τ 45.23 46.98 37.50 45.91 48.03 37.16 40.05 44.95 44.06 46.91 PSO ϵ 1.2 E+01 5.3 E+00 3.5 E+01 9.8 E-02 2.0 E+01 2.5 E-01 2.0 E+10 2.3 E+01 5.1 E-01 2.9 E+03 τ 47.19 47.04 45.50 43.39 46.80 44.97 46.46 42.78 43.15 47.13 BFGS ϵ 1.9 E+01 2.1 E+00 1.9 E+01 4.3 E-13 1.2 E-02 6.4 E+00 2.4 E-05 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.02 0.03 0.00 0.02 0.01 0.04 0.01 0.01 0.00 L-BFGS ϵ 1.9 E+01 1.9 E+00 4.0 E-10 4.3 E-13 1.2 E-02 4.5 E+00 4.5 E-10 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.06 0.05 0.00 0.01 0.02 0.02 0.01 0.01 0.01 CG ϵ 1.9 E+01 3.4 E+00 N/A 0.0 E+00 2.0 E-02 4.4 E+00 2.9 E-12 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.07 N/A 0.00 0.01 0.05 0.02 0.01 0.03 0.00 NCG ϵ 1.9 E+01 3.4 E+00 1.7 E-19 0.0 E+00 7.4 E-02 6.4 E+00 2.1 E-12 7.0 E+01 4.2 E+00 2.2 E+03 τ 0.01 0.06 0.04 0.00 0.01 0.06 0.01 0.00 0.02 0.01 NEWTON ϵ 1.9 E+01 2.2 E+00 1.1 E+11 0.0 E+00 3.2 E-02 6.4 E+00 2.8 E-24 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.02 0.01 0.00 0.01 0.10 0.01 0.01 0.01 0.00 TR NCG ϵ 1.9 E+01 6.5 E+00 2.4 E-10 4.0 E-12 4.8 E+00 4.4 E+00 1.2 E-14 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.12 0.04 0.00 0.02 0.03 0.01 0.00 0.02 0.01 TR ϵ 1.9 E+01 7.0 E+00 3.2 E-13 4.0 E-12 6.1 E+01 2.6 E+00 5.3 E-11 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.02 12.97 0.06 0.00 0.06 0.05 0.02 0.01 0.01 0.01 Table 2: The result of the TTOpt optimizer in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for benchmark F1 (Ackley function) for various dimension numbers. DIMENSION d = 10 d = 50 d = 100 d = 500 ERROR , ϵ 3.9 E-06 3.9 E-06 3.9 E-06 3.9 E-06 TIME , τ 3.1 40.1 143.9 3385.3 signiﬁcant problem of optimizing the parameters of the RL agent. We select GA (Genetic Algo- rithm [27, 62]), openES (basic version of OpenAI Evolution Strategies [ 56]) and cmaES (the Covariance Matrix Adaptation Evolution Strategy [23]) as baselines for both experiments. Addi- tionally we used DE (Differential Evolution [61]), NB (NoisyBandit method from Nevergrad [5]) and PSO (Particle Swarm Optimization [25, 36]) for benchmark functions17. We also compared the proposed approach with gradient-based methods applied for all benchmark functions18: BFGS (Broyden–Fletcher–Goldfarb–Shanno algorithm), L-BFGS (Limited-memory BFGS), CG (Conju- gate Gradient algorithm), NCG (Newton CG algorithm), Newton (Newton Exact algorithm), TR NCG (Trust-Region NCG algorithm) and TR (Trust-Region Exact algorithm). According to our approach, the TTOpt solver has the following conﬁgurable parameters: a and b are lower and upper grid bounds (for simplicity, we use the same value for all dimensions); R is a rank (for simplicity, we use the same value for all unfolding matrices); P is a submode size (mode size of the quantized tensor; for simplicity, we use the same value for all dimensions);q is the number of submodes in the quantized tensor (each mode of the original tensor has size N = Pq); M is a limit on the number of requests to the objective function. 17We used implementations of the methods from available packages estool (https://github.com/ hardmaru/estool), pycma (https://github.com/CMA-ES/pycma, and nevergrad (https://github. com/facebookresearch/nevergrad). 18We used implementations from the package https://github.com/rfeinman/pytorch-minimize). We carried out computations with all methods from this library, except for Trust-Region GLTR (Krylov) and Dogleg methods, for which the calculation ended with an error for most benchmarks. 7Table 3: Mean E and standard deviation σ of the ﬁnal cumulative reward. The environments are encoded using capital letters Swimmer-v3, LunarLanderContinuous-v2, InvertedPendulum-v2 and HalfCheetah-v3. The left sub-table is for mode size N = 3, another one is for mode size N = 28. All runs are averaged over seven random seeds. S(31) L(31) I(31) H(31) S(28) L(28) I(28) H(28) TTOPT E σ 357.50 6.59 290.29 24.40 1000.00 0.00 4211.02 211.94 311.82 29.61 286.87 21.65 1000.00 0.00 2935.90 544.11 GA E σ 349.91 10.04 283.05 16.28 893.00 283.10 2495.37 185.11 359.79 4.21 213.75 99.67 222.86 342.79 3085.80 842.76 CMA ES E σ 342.31 36.07 214.55 93.79 721.00 335.37 2549.83 501.08 340.54 78.90 221.95 133.80 621.00 472.81 2879.46 929.55 OPEN ES E σ 318.39 44.61 114.97 113.48 651.86 436.37 2423.16 602.43 109.39 40.11 73.08 163.33 224.71 217.51 1691.22 976.96 3.1 Benchmark functions minimization To analyze the effectiveness of theTTOpt, we applied it to 10-dimensional benchmark functions with known global minimums; see Table 1 in Appendix B.1 with the list of functions and their properties (note that some of the considered benchmarks are multimodal non-separable functions). Also, in Appendix B.1, we present a more detailed study of the TTOpt solver and the dependence of the accuracy on the value of its parameters (R, qand M). In all experiments with baselines (GA, openES, cmaES, DE, NB, PSO, BFGS, L-BFGS, CG, NCG, Newton, TR NCGand TR), we used default parameter values. In Appendix B.2 we also present the additional experiments with Bayesian Optimization [40]. For TTOpt we selected rank19 R= 4, submode size P = 2 and the number of submodes q = 25. For all methods, a limit on the number of requests to the objective function is chosen as M = 105. All calculations are performed on a standard laptop. The results are demonstrated in Table 1. For each method we list the absolute deviation of the result ˆJmin from the global minimum Jmin, i.e., ϵ = |ˆJmin −Jmin|. We also present the total running time, τ. Compared to other benchmarks, TTOpt is consistently fast, accurate and avoids random failures to converge seen in other algorithms. Additionally, TTOpt turns out one of the fastest gradient-free algorithms (GA, openES, cmaES, DE, NB, PSO), despite a simple Python implementation. One of the advantages of the proposed TTOpt approach is the possibility of its application to essentially multidimensional functions. In Table 2 we present the result of TTOpt for the F1 benchmark function of various dimensionality (results for other benchmarks are in Appendix B.1). Note that as a limit on the number of requests to the objective function we chose M = 104 ·d, and the values of the remaining parameters were chosen the same as above. As can be seen, even for 500-dimensional functions, the TTOpt method gives a fairly accurate result. 3.2 Application of TTOpt to Reinforcement Learning We used several continuous RL tasks implemented in Mujoco [63] and OpenAI-GYM [8]: Swimmer- v3 [16], LunarLanderContinuous-v2, InvertedPendulum-v2 and HalfCheetah-v3 [65]. In all experi- ments, the policy πis represented by a neural network with three hidden layers and with tanh and ReLU activations. Each layer is a convolution layer. See additional details about hyperparameters in Table 6 in Appendix B. We discretize (quantize) the values of agent’s weights. TheTTOpt method is used to optimize discrete agent’s weights in order to maximize the cumulative reward of the episode. This corresponds to on-policy learning. 19We chose rank Rusing the following heuristic. The minimal number of sweeps is ﬁxed as T = 5. It follows that the algorithm will need 2 ·T ·(dq) ·P ·R2 function calls. With a given limit on the number of function requests M, the rank can be estimated as R≤ √ M 2·T·d·q·P. 80 2 4 6 Number of function queries 104 0 1 2 3 4average rewards 103 HalfCheetah-v3 TTopt cmaES openES GA target reward 0 2500 5000 7500 10000 12500 15000 seconds 0 1 2 3 4average rewards 103 HalfCheetah-v3 TTopt cmaES openES GA target reward Figure 3: Training curves of TTOpt and baselines for HalfCheetah-v3 (N = 3). The upper plot is the average cumulative reward versus the number of interactions with the environment. The lower plot is the same versus execution time. The reward is averaged for seven seeds. The shaded area shows the difference of one standard deviation around the mean. See similar plots for other environments in Appendix B.5. To properly compare TTOpt with other methods, we propose modiﬁed evolutionary baselines that enforce constrained parameter domain. We adapt penalty term and projection techniques from [33, 6] to introduce constraints, see Appendix B.4 for details. First, we run benchmarks with small mode size N = 3 1 with lower and upper grid bounds ±1. Another series of experiments was done with ﬁner mode of sizeN = 2q with the same bounds. These experiments model the case of neural networks with discrete (quantized) weights which use q-bits quantization. Finally, we provide the results of using TTOpt as a ﬁne-tuning procedure for linear policies from [39]. We present characteristic training curves based on the number of environment interactions and execution time for the HalfCheetah-v3 experiment (N = 3) in Figure 3. Training curves for other environments can be found in Appendix B.5, in Figure 3 for N = 3 and in Figure 4 for N = 256. TTOpt consistently outperforms all other baselines on the coarse grid with mode size N = 3 . Our method is still best for ﬁner grids with mode size N = 256 on InvertedPendulum-v2 and LunarLanderContinuous-v2, and second-best on HalfCheetah-v3. Moreover, TTOpt has signiﬁcantly lower execution time compared to evolutionary baselines. Another interesting observation is that the training curves of TTOpt have low dispersion, e.g., the algorithm performs more consistently than the baselines (see Appendix B.5). Table 3 summarizes the experiments for the coarse and ﬁne grids. Results for ﬁne-tuning of linear policies are presented in Table 5 in Appendix B.5. We also did rank and reward dependency study in Appendix B. 4 Related work In the case of high dimensional optimization, evolutionary strategies (ES) [ 59, 45] are one of the most advanced methods of black-box optimization. This approach aims to optimize the parameters of the search distribution, typically a multidimensional Gaussian, to maximize the objective function. Finite difference schemes are commonly used to approximate gradients of the parameters of the search distribution. Numerous works proposed techniques to improve the convergence of ES [45]. [66] proposed to use second-order natural gradient of [12] to generate updates, while [ 23] suggested to include the history of recent updates to generate next ones. [ 38] presented the concept of surrogate gradients for faster convergence. Another series of works aimed to reduce the high sampling complexity of ES. In [ 11] the au- thors described how to use active subspaces [ 14] to reduce the number of objective function calls dynamically. Plenty of the already mentioned works in gradient-free optimization speciﬁcally applied these methods to RL tasks [ 56, 10, 11, 39, 15, 35, 24]. Overall, the performance of ES-based methods 9is comparable to conventional policy gradients, especially if the number of model parameters is small [57, 58]. Another advantage of ES over policy gradient is that it produces more robust and diverse policies [34, 35] by eliminating the problem of delayed rewards and short length time horizons. Finally, evolutionary approaches are suitable for the problems with non-Markovian properties [24]. Other metaheuristic [26, 41] and classical optimization [53] techniques are also studied within RL scope. The examples include simulated annealing [4], particle swarm optimization [25, 36] and even classical Nelder-Mead algorithm [44, 46]. These methods, however, are not tested on common RL task sets. Several other works combined evolutionary methods with RL to achieve better performance in complex scenarios [30, 18], e.g. AlphaStar [64, 3]. Finally, low-rank tensor approximations have been applied to RL problems in settings different from ours, including multi-agent scenarios [ 37], and improving dynamic programming approaches [20, 7]. The idea of our method is quite different from the presented works, especially in the RL area. 5 Conclusion We proposed a new discrete optimization method based on quantized tensor-train representation and maximal volume principle. We demonstrate its performance for analytical benchmark functions and reinforcement learning problems. Our algorithm is more efﬁcient under a ﬁxed computational budget than baselines, especially on discrete domains. Moreover, the execution time for TTOpt is lower by a signiﬁcant margin compared with other baselines. Finally, we show that the agents with discrete parameters in RL can be as efﬁcient as their continuous parameter versions. This observation supports the broad adoption of quantization in machine learning. We hope that our approach will serve as a bridge between continuous and discrete optimization methods. Acknowledgments and Disclosure of Funding The work was supported by Ministry of Science and Higher Education grant No. 075-10-2021-068. References [1] Salman Ahmadi-Asl, Cesar F Caiafa, Andrzej Cichocki, Anh Huy Phan, Toshihisa Tanaka, Ivan Oseledets, and Jun Wang. Cross tensor approximation methods for compression and dimensionality reduction. IEEE Access, 9:150809–150838, 2021. [2] Stéphane Alarie, Charles Audet, Aïmen E. Gheribi, Michael Kokkolaras, and Sébastien Le Digabel. Two decades of blackbox optimization applications. EURO Journal on Computational Optimization, 9:100011, 2021. [3] Kai Arulkumaran, Antoine Cully, and Julian Togelius. Alphastar: An evolutionary computation perspective. In Proceedings of the Genetic and Evolutionary Computation Conference Com- panion, GECCO ’19, page 314–315, New York, NY , USA, 2019. Association for Computing Machinery. [4] A.F. Atiya, A.G. Parlos, and L. Ingber. A reinforcement learning method based on adaptive simulated annealing. In 2003 46th Midwest Symposium on Circuits and Systems , volume 1, pages 121–124 V ol. 1, 2003. [5] Pauline Bennet, Carola Doerr, Antoine Moreau, Jeremy Rapin, Fabien Teytaud, and Olivier Teytaud. Nevergrad: Black-box optimization platform. SIGEVOlution, 14(1):8–15, apr 2021. [6] Rafał Biedrzycki. Handling bound constraints in cma-es: An experimental study. Swarm and Evolutionary Computation, 52:100627, 2020. [7] AI Boyko, IV Oseledets, and G Ferrer. Tt-qi: Faster value iteration in tensor train format for stochastic optimal control. Computational Mathematics and Mathematical Physics, 61(5):836– 846, 2021. [8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. ArXiv, abs/1606.01540, 2016. [9] Cesar F. Caiafa and Andrzej Cichocki. Generalizing the column–row matrix decomposition to multi-way arrays. Linear Algebra and its Applications, 433(3):557–573, 2010. 10[10] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 970–978. PMLR, 10–15 Jul 2018. [11] Krzysztof M Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sind- hwani. From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [12] Andrzej Cichocki and Shun-ichi Amari. Adaptive Blind Signal and Image Processing: Learning Algorithms and Applications. John Wiley & Sons, Inc., USA, 2002. [13] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, and Danilo P. Mandic. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low- rank tensor decompositions. Foundations and Trends® in Machine Learning, 9(4-5):249–429, 2016. [14] Paul G. Constantine. Active subspaces - emerging ideas for dimension reduction in parameter studies. In SIAM spotlights, 2015. [15] Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, page 5032–5043, Red Hook, NY , USA, 2018. Curran Associates Inc. [16] Rémi Coulom. Reinforcement Learning Using Neural Networks, with Applications to Motor Control. PhD thesis, Institut National Polytechnique de Grenoble, 2002. [17] Tom Erez, Yuval Tassa, and Emanuel Todorov. Inﬁnite-horizon model predictive control for periodic tasks with contacts. In Hugh F. Durrant-Whyte, Nicholas Roy, and Pieter Abbeel, editors, Robotics: Science and Systems VII, University of Southern California, Los Angeles, CA, USA, June 27-30, 2011, 2011. [18] Aleksandra Faust, Anthony G. Francis, and Dar Mehta. Evolving rewards to automate reinforce- ment learning. In 6th ICML Workshop on Automated Machine Learning, 2019. [19] Sergei A Goreinov, Ivan V Oseledets, Dimitry V Savostyanov, Eugene E Tyrtyshnikov, and Nikolay L Zamarashkin. How to ﬁnd a good submatrix. In Matrix Methods: Theory, Algorithms And Applications: Dedicated to the Memory of Gene Golub, pages 247–256. World Scientiﬁc, 2010. [20] Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk. High-dimensional stochastic optimal control using continuous tensor decompositions.The International Journal of Robotics Research, 37(2-3):340–377, 2018. [21] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [22] Wolfgang Hackbusch and Stefan Kühn. A new scheme for the tensor representation. Journal of Fourier analysis and applications, 15(5):706–722, 2009. [23] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review, pages 75–102. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006. [24] Verena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforce- ment learning. Journal of Algorithms, 64(4):152–168, 2009. Special Issue: Reinforcement Learning. [25] Daniel Hein, Alexander Hentschel, Thomas Runkler, and Steffen Udluft. Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies. Engineering Applications of Artiﬁcial Intelligence, 65:87–98, 2017. [26] J. Michael Herrmann, Adam Price, and Thomas Joyce. 3. Ant colony optimization and rein- forcement learning, pages 45–62. De Gruyter, 2020. [27] John H. Holland. Genetic algorithms. Scientiﬁc American, 267(1):66–73, 1992. 11[28] Sebastian Holtz, Thorsten Rohwedder, and Reinhold Schneider. The alternating linear scheme for tensor optimization in the tensor train format. SIAM Journal on Scientiﬁc Computing , 34(2):A683–A713, 2012. [29] Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2):150–194, 2013. [30] Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, page 1196–1208, Red Hook, NY , USA, 2018. Curran Associates Inc. [31] Boris N. Khoromskij. O(dlog n)-quantics approximation of n-d tensors in high-dimensional numerical modeling. Constructive Approximation, 34(2):257–280, Oct 2011. [32] Tamara G. Kolda, Robert Michael Lewis, and Virginia Torczon. Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review, 45(3):385–482, 2003. [33] Oliver Kramer. A review of constraint-handling techniques for evolution strategies.Appl. Comp. Intell. Soft Comput., 2010, January 2010. [34] Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O. Stanley. Es is more than just a traditional ﬁnite-difference approximator. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’18, page 450–457, New York, NY , USA, 2018. Association for Computing Machinery. [35] Fei-Yu Liu, Zi-Niu Li, and Chao Qian. Self-guided evolution strategies with historical estimated gradients. In Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI’20, 2021. [36] Tundong Liu, Liduan Li, Guifang Shao, Xiaomin Wu, and Meng Huang. A novel policy gradient algorithm with pso-based parameter exploration for continuous control. Eng. Appl. Artif. Intell., 90(C), apr 2020. [37] Anuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean Kossaiﬁ, Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for multi-agent reinforcement learning. InInternational Conference on Machine Learning (ICML), volume 139, pages 7301–7312, 2021. [38] Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided evolutionary strategies: augmenting random search with surrogate gradients. In Ka- malika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 4264–4273. PMLR, 09–15 Jun 2019. [39] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [40] Erich Merrill, Alan Fern, Xiaoli Fern, and Nima Dolatnia. An empirical study of bayesian optimization: Acquisition versus partition. Journal of Machine Learning Research, 22(4):1–25, 2021. [41] Laurent Meunier, Herilalaina Rakotoarison, Pak Kan Wong, Baptiste Roziere, Jérémy Rapin, Olivier Teytaud, Antoine Moreau, and Carola Doerr. Black-box optimization revisited: Im- proving algorithm selection wizards through massive benchmarking. IEEE Transactions on Evolutionary Computation, pages 1–1, 2021. [42] Aleksandr Mikhalev and Ivan V Oseledets. Rectangular maximum-volume submatrices and their applications. Linear Algebra and its Applications, 538:187–211, 2018. [43] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe- tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, Feb 2015. [44] John A. Nelder and Roger Mead. A simplex method for function minimization. Computer Journal, 7:308–313, 1965. 12[45] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527–566, Apr 2017. [46] Barry D. Nichols. Continuous action-space reinforcement learning methods applied to the minimum-time swing-up of the acrobot. In 2015 IEEE International Conference on Systems, Man, and Cybernetics, pages 2084–2089, 2015. [47] I. V . Oseledets. Approximation of2d×2d matrices using tensor decomposition. SIAM J. Matrix Anal. Appl., 31(4):2130–2145, 2010. [48] I. V . Oseledets. Tensor-train decomposition.SIAM Journal on Scientiﬁc Computing, 33(5):2295– 2317, 2011. [49] Ivan V Oseledets and Eugene E Tyrtyshnikov. Breaking the curse of dimensionality, or how to use svd in many dimensions. SIAM Journal on Scientiﬁc Computing, 31(5):3744–3759, 2009. [50] Ivan V Oseledets and Eugene E Tyrtyshnikov. TT-cross approximation for multidimensional arrays. Linear Algebra and its Applications, 432(1):70–88, 2010. [51] David Pfau, James S. Spencer, Alexander G. D. G. Matthews, and W. M. C. Foulkes. Ab initio solution of the many-electron schrödinger equation with deep neural networks. Phys. Rev. Research, 2:033429, Sep 2020. [52] Anh-Huy Phan, Andrzej Cichocki, André Uschmajew, Petr Tichavský, George Luta, and Danilo P. Mandic. Tensor networks for latent variable analysis: Novel algorithms for tensor train approximation. IEEE Transactions on Neural Networks and Learning Systems, 31(11):4622– 4636, 2020. [53] Zhiwei Qin, Weichang Li, and Firdaus Janoos. Sparse reinforcement learning via convex optimization. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 424–432, Bejing, China, 22–24 Jun 2014. PMLR. [54] Marc H. Raibert, Jr H. Benjamin Brown, and Michael Chepponis. Experiments in balance with a 3d one-legged hopping machine. The International Journal of Robotics Research, 3(2):75–92, 1984. [55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. [56] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. ArXiv, abs/1703.03864, 2017. [57] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 ofProceedings of Machine Learning Research, pages 1889–1897, Lille, France, 07–09 Jul 2015. PMLR. [58] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017. [59] Hans-Paul Schwefel. Evolutionsstrategien für die numerische Optimierung, pages 123–176. Birkhäuser Basel, Basel, 1977. [60] Artur M. Schweidtmann and Alexander Mitsos. Deterministic global optimization with artiﬁcial neural networks embedded. Journal of Optimization Theory and Applications, 180(3):925–948, Oct 2018. [61] Rainer Storn and Kenneth Price. Differential evolution – a simple and efﬁcient heuristic for global optimization over continuous spaces. Journal of Global Optimization, 11(4):341–359, Dec 1997. [62] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. ArXiv, abs/1712.06567, 2017. [63] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033, 2012. 13[64] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McK- inney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, Nov 2019. [65] Pawel Wawrzynski. Learning to control a 6-degree-of-freedom walking robot. In EUROCON 2007 - The International Conference on "Computer as a Tool", pages 698–705, 2007. [66] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen Schmidhuber. Natural evolution strategies. Journal of Machine Learning Research, 15(27):949–980, 2014. [67] Qibin Zhao, Masashi Sugiyama, Longhao Yuan, and Andrzej Cichocki. Learning efﬁcient tensor representations with ring-structured networks. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8608–8612, 2019. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] The work has no any societal impacts, since its about optimization methods. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] The code will be provided in supplemental material and as github repository after decision (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running exper- iments multiple times)? [Yes] We used a typical shade(mean-std) plots that used in reinforcement learning experiments (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] . We mentioned all references and source-codes for gradient-free baselines. (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... 14(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] A Description of MaxVol and TTOpt algorithms A.1 Maxvol Algoritm The search for the maximal element in a large matrix can be signiﬁcantly simpliﬁed if one can obtain a "good" submatrix, for example, a maximal-volume submatrix. Finding the maximal-volume submatrix of a nondegenerate matrix A∈RN×R,N > Ris an NP-hard problem. In this paper, we adapted the maxvol algorithm [19] which can ﬁnd a submatrix C∈RR×R of A, such that its determinant is close to maximum in absolute value. The algorithm selects a set of Rrows denoted by I⊂[1,...,N ] which form the matrix C= A[I,:]. For the given tolerance threshold ϵ(ϵ≥1 and close to one), we ﬁnd the set Ias follows: 1. Compute the LU-decomposition A= PLU and store the permutation of the ﬁrst Rrows (according to the matrix P) in the list I. 2. Generate the matrix Q∈RR×N as a solution to the linear system with an upper triangular matrix U: UTQ= AT. 3. Compute the matrix B∈RN×R as a solution to the linear system with the lower triangular matrix L: (L[: R,:])TBT = Q. 4. Find the maximum modulo element b= Bi,j of the matrix B. If |b|≤ ϵ, then terminate the algorithm20 by returning the current list I and matrix B. 5. Update the matrix B ←B−B[:,j] (B[i,:] −eT j ) b−1, where ej is the j-th unit basis vector. 6. Update the list Ias I[j] ←i. 7. Return to step 4. It can be shown that at each step kof the algorithm the volume of the submatrix Cincreases by a factor not less than ϵ. Therefore, the estimate for the number of iterations, K, is the following: K = log (|det ( ˆC)|) −log (|det (C(0))|) log ϵ , (8) where ˆCis the exact maximal-volume submatrix and C(0) is an initial approximation obtained from the LU-decomposition (see step 1 above). The computational complexity of the algorithm is the sum of the initialization complexity and the complexities of Kiterations. The complexity equals O ( NR2 + KNR ) . (9) In the context of the maximal matrix element search problem, other deﬁnitions of "good" submatrices are possible. For example, one can search for a rectangular submatrix containing rows or columns which span the largest volume. The rect_maxvol algorithm can be used in this case. We employ rect_maxvol [42] to choose rectangular submatrices within the framework of the TTOpt algorithm to adaptively increase the rank. Consider an arbitrary nondegenerate matrix A∈RN×R (N >R). We search for a rectangular sub- matrix ˆC∈R(R+∆R)×R,R + ∆R<N , which maximizes the rectangular volume √ det ( ˆCT ˆC) objective. An approximation Cto ˆCcan be found as follows. The ﬁrst Rrows of Care obtained by the maxvol algorithm. The following rect_maxvol algorithm will ﬁnd additional ∆Rrows: 20In addition to the indices of the rows Ithat form the maximal-volume submatrix C ∈RR×R, we also obtain the matrix of coefﬁcients B∈RN×R such that A= BC. 151. First, generate Rindices of rows I ∈NR and the coefﬁcient matrix B ∈RN×R of the initial approximation using the maxvol algorithm. 2. Compute the vector l∈RN containing the norms of the rows: l[j] = (B[j,:])TB[j,:] for j = 1,2,...,N . 3. Find the maximum modulo element of the vector l, i.e. i= argmax(l). 4. If the current lengthR+∆Rof the vector Iis greater than or equal toR(max) or if l[i] ≤τ2, then terminate the algorithm21 by returning the current list Iand matrix B. 5. Update the coefﬁcient matrix as B:= [ B−B(B[i,:])T B[i,:] 1+B[i,:](B[i,:])T B(B[i,:])T 1+B[i,:](B[i,:])T ] . 6. Update the vector of row norms l[j] := l[j] − |B[j,:](B[i,:])T|2 1 + B[i,:](B[i,:])T , j = 1,2,...,N. 7. Add current row index ito the list I. 8. Return to step 3. The approximate computational complexity of this algorithm according to the work [42] is O(NR2), and the expected number of rows in the resulting submatrix is 2R−1 for the case τ = 1. A.2 TTOpt Algorithm In Algorithm A.1 we present the details of the TTOpt implementation. The TTOpt algorithm builds the TT proxy of the minimized function which is iteratively updated. The procedure of updating the TT proxy is outlined in Algorithm A.2 (function update_left, that updates core tensors of the network from right to left ) and in Algorithm A.3 (function update_right, updates core tensors of the network from left to right ). The requests to the objective function and the transformation of resulting values are presented in Algorithm A.4 (function eval). The procedure begins by building one-dimensional uniform gridsxi (i= 1,2,...,d ) for the function argument along each mode, using the speciﬁed bounds of the rectangular search domain Ω. Note that, if necessary, arbitrary nonuniform grids can be used, taking into account the speciﬁc features of the function under consideration. 21As a criterion for stopping the algorithm, we consider either the achievement of the maximum number of rows in the maximal-volume submatrix (R(max)), or the sufﬁciently small norm of the remaining rows in the matrix of coefﬁcients B. 16Algorithm A.1: Multivariable function minimizer TTOpt. Data: function J(θ), where θ∈Ω ⊂Rd; boundary points of the rectangular domain Ω = [a1,b1] ×[a2,b2] ×···× [ad,bd]; the number of grid points for every dimension N1,N2,...,N d; number of inner iterations (sweeps) kmax; maximum TT-rank rmax. Result: approximation of the spatial point θmin ∈Rd at which the function J reaches its minimum in the region Ω and a corresponding function value Jmin ∈R. 1 // Construct a uniform grid x1,x2,..., xd: 2 Set xi[m] = ai + (bi −ai) ·m−1 Ni−1 3 for all i= 1,...,d and m= 1,2,...,N i. 4 // Initialize the TT-ranks R0,R1,...,R d: 5 Set R0 = 1. 6 for i= 1 to (d−1) do 7 Set Ri = min (Ri−1 ·Ni,Ri ·Ni,rmax). 8 end 9 Set Rd = 1. 10 // Initialize set of points of interest X0,X1,..., Xd: 11 Set X0 = None. 12 for i= 0 to (d−2) do 13 Set Gi = random(Ri ·Ni,Ri+1) from the standard normal distribution. 14 Compute QR-decomposition Q,R= QR(Gi). 15 Compute indices mopt = maxvol(Q). 16 Set Xi+1 = update_right(Xi,xi,Ni,Ri,mopt). 17 end 18 Set Xd = None. 19 // Iterate in a loop to ﬁnd optimal θmin and Jmin: 20 Set θmin = None and Jmin = +∞. 21 for k= 1 to kmax do 22 // Traverse the TT-cores from right to left: 23 for i= dto 1 do 24 Compute z,θmin,Jmin = eval(∗). 25 Reshape zto matrix Z∈RRi×ni·Ri+1 . 26 Compute Q,R= QR(ZT). 27 Compute indices mopt = rect_maxvol(Q[:,0 : R[i]]). 28 Set Xi = update_left(Xi+1,xi,Ni,Ri+1,mopt). 29 end 30 // Traverse the TT-cores from left to right: 31 for i= 1 to ddo 32 Compute z,θmin,Jmin = eval(∗). 33 Reshape zto matrix z∈RRi·ni×Ri+1 . 34 Compute Q,R= QR(Z). 35 Compute indices mopt = rect_maxvol(Q). 36 Set Xi = update_right(Xi+1,xi,Ni,Ri+1,mopt). 37 end 38 end 39 return (θmin, Jmin). Algorithm A.2: Function update_left to update points of interest when traverse the tensor modes from right to left. Data: current set of points for the (i+ 1)-th mode Xi+1; grid points xi; number of grid points Ni; TT-rank Ri+1; list of indices to be selected mopt. Result: new set of points Xi. 1 Set W1 = ones(Ri+1) ⊗xi and W2 = Xi+1 ⊗ones(Ni) 2 Set Xi = [W1,W2]. 3 Select subset of rows Xi = Xi[mopt,:]. 4 return Xi. 17Algorithm A.3: Function update_right to update points of interest when traverse the tensor modes from left to right. Data: current set of points for the i-th mode Xi; grid points xi; number of grid points Ni; TT-rank Ri; list of indices to be selected mopt. Result: new set of points Xi+1. 1 Set W1 = ones(Ni) ⊗Xi and W2 = xi ⊗ones(Ri). 2 Set Xi+1 = [W1,W2]. 3 Select subset of rows Xi+1 = Xi+1[mopt,:]. 4 return Xi+1. Algorithm A.4: Function eval to compute the target function in points of interest. Data: Xi; Xi+1; Ri; Ni; Ri+1; J; θmin, Jmin (see Algorithm A.1 for details). Result: transformed function values z∈RRi·Ni·Ri+1 , updated θmin and Jmin. 1 Set W1 = ones(Ni ·Ri+1) ⊗Xi. 2 Set W2 = ones(Ri+1) ⊗xi ⊗ones(Ri). 3 Set W3 = Xi+1 ⊗ones(Ri ·Ni). 4 Set Xcurr = [W1,W2,W3] ∈RRi·Ni·Ri+1×d. 5 // Compute function for each point in Xcurr: 6 Set ycurr = J(Xcurr). 7 if min (ycurr) <Jmin then 8 Set mmin = argmin(ycurr). 9 Set θmin = Xcurr[mmin,:]. 10 Set Jmin = ycurr[mmin]. 11 end 12 // Compute smooth function for each value in ycurr: 13 Set z= π 2 −arctan (ycurr −Jmin). 14 return (z,θmin,Jmin). We then randomly initialize the TT proxy tensor with input ranks rmax. If necessary, we reduce some of the ranks to satisfy the condition Ri−1Ni ≥Ri (i= 1,2,...,d −1). Note that in this case, for all TT-cores Gi ∈RRi−1×Ni×Ri (i= 1,2,...,d ), the right unfolding matrices G(2) i ∈RRi−1·Ni×Ri will turn out to be “tall” matrices, that is, their number of rows is not less than the number of columns, and hence we can apply maxvol and rect_maxvol algorithms to these matrices. Next, we iteratively traverse all tensor modes (using corresponding TT-cores) in the direction from right to left and vice versa. We evaluate (and transform) the objective function to reﬁne the selected rows and columns. For each k-th mode of the tensor we evaluate the submatrixJ(C) k ∈RRk−1·Nk×Rk of the corresponding unfolding matrix, compute its QR decomposition, ﬁnd the row indices of the rectangular maximal-volume submatrix ˆJk ∈R(Rk+∆Rk)×Rk of the Q factor and add resulting indices of the original tensor to the index set Xk. The arguments for target function evaluation in Algorithm A.4 are selected as merged left and right index sets, constructed from previous rect_maxvol computations. After each request to the objective function, we update the current optimal value Jmin and then transform the calculated values by the mapping (7) described in the main text. B Additional Experiments B.1 Experiments with benchmark functions In Section 3.1 we compared theTTOpt solver22 with baseline methods, applied to various model func- tions [29]. The list of functions is presented in Table 4. For each function, we provide the lower/upper 22We implemented the TTOpt algorithm in a python package with detailed documentation, demos, and reproducible scripts for all benchmark calculations. 18Table 4: Benchmark functions for comparison of the considered optimization algorithms and perfor- mance evaluation of the TTOpt approach. For each function, we present the lower grid bound (a), the upper grid bound (b), the global minimum (Jmin) and the analytical formula. Note that Jmin for the F6 function is given for the 10-dimensional case. Function a b Jmin Formula F1 (Ackley) −32.768 32.768 0. f(x) = −Ae−B √ 1 d ∑d i=1 x2 i − e 1 d ∑d i=1 cos (Cxi) + A+ e1, where A = 20 , B = 0.2 and C = 2π F2 (Alpine) −10 10 0. f(x) = ∑d i=1 |xisin xi + 0.1xi| F3 (Brown) −1 4 0. f(x) = ∑d−1 i=1 ( x2 i )(x2 i+1+1) + ( x2 i+1 )(x2 i +1) F4 (Exponential) −1 1 −1. f(x) = −e−1 2 ∑d i=1 x2 i F5 (Griewank) −600 600 0. f(x) = ∑d i=1 x2 i 4000 −∏d i=1 cos ( xi√ i ) + 1 F6 (Michalewicz) 0 π −9.66015 f(x) = −∑d i=1 sin (xi) sin2m ( ix2 i π ) F7 (Qing) 0 500 0. f(x) = ∑d i=1 ( x2 i −i )2 F8 (Rastrigin) −5.12 5.12 0. f(x) = A·d+∑d i=1 ( x2 i −A·cos (2π·xi) ) , where A= 10 F9 (Schaffer) −100 100 0 f(x) = ∑d−1 i=1 (0.5 + sin2 (√ x2 i +x2 i+1)−0.5 (1+0.001(x2 i +x2 i+1)) 2 ) F10 (Schwefel) −500 500 0. f(x) = 418.9829 ·d−∑d i=1 xi ·sin ( √ |xi|) grid bounds (aand b) and global minimum (Jmin). Note that many benchmarks is multimodal (have two or more local optima), introducing additional complications into the optimization problem. The main conﬁgurable parameters of our solver are the mode size ( N; N = 2 q in the case of quantization, where q is the number of submodes in the quantized tensor); the rank ( R), and the limit on the number of requests to the objective function (M). The choice of these parameters can affect the ﬁnal accuracy of the optimization process. Below we present the results of the studies of parameter importance. In all calculations, we ﬁxed the non-varying parameters at the values provided in Section 3.1. Mode size inﬂuence. To reach high accuracy, we need ﬁne grids. As we indicated in Section 2.6, in this case, the quantization of the tensor modes seems attractive. We reshape the originald-dimensional tensor J ∈RN1×N2×···×Nd into the tensor ˜J ∈R2×2×···×2 of a higher dimension d·q, but with smaller modes of size 2, and apply the TTOpt algorithm to this “long” tensor instead of the original one. 19Table 5: Comparison of the “direct” (TT) and “quantized” (QTT) TTOpt solvers in terms of the ﬁnal error (absolute deviation of the obtained value from the exact minimum) for various benchmark functions. The reported values are averaged over ten independent runs. MODE SIZE F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 256 TT 1.2 E+00 2.0 E-02 0.0 E+00 7.7 E-05 1.0 E+00 9.8 E-02 9.4 E+01 8.0 E-01 4.2 E-01 4.5 E-01 QTT 1.2 E+00 2.3 E-02 0.0 E+00 7.7 E-05 1.0 E+00 1.6 E-01 9.4 E+01 8.0 E-01 3.9 E-01 4.5 E-01 1024 TT 1.6 E+01 4.2 E+00 3.0 E+01 2.6 E-01 5.7 E+01 2.0 E+00 1.9 E+10 4.7 E+01 1.5 E+00 1.1 E+03 QTT 1.8 E-01 8.2 E-03 6.9 E-05 4.8 E-06 2.1 E-01 7.1 E-02 5.2 E+00 5.0 E-02 1.2 E-01 2.8 E-02 4096 TT 1.9 E+01 1.5 E+01 5.0 E+08 5.1 E-01 1.4 E+02 5.7 E+00 5.1 E+10 9.8 E+01 3.5 E+00 2.6 E+03 QTT 3.5 E-02 1.8 E-03 0.0 E+00 3.0 E-07 3.9 E-02 4.3 E-02 2.6 E-01 3.1 E-03 8.7 E-02 1.0 E-02 16384 TT 2.0 E+01 1.9 E+01 9.9 E+17 5.9 E-01 1.7 E+02 5.9 E+00 6.3 E+10 1.2 E+02 3.8 E+00 3.2 E+03 QTT 8.2 E-03 7.8 E-04 2.7 E-07 1.9 E-08 2.6 E-02 8.9 E-02 2.2 E-02 1.9 E-04 1.2 E-01 3.8 E-04 65536 TT 2.0 E+01 1.9 E+01 1.2 E+10 6.7 E-01 2.2 E+02 7.8 E+00 7.1 E+10 1.6 E+02 4.4 E+00 3.6 E+03 QTT 2.0 E-03 1.3 E-04 1.2 E-09 1.2 E-09 2.2 E-02 7.1 E-02 9.4 E-04 1.2 E-05 1.4 E-01 1.6 E-04 262144 TT 2.1 E+01 2.3 E+01 1.7 E+16 8.0 E-01 3.0 E+02 8.4 E+00 8.9 E+10 1.8 E+02 4.5 E+00 3.7 E+03 QTT 5.0 E-04 3.3 E-05 1.1 E-09 7.3 E-11 3.0 E-02 3.8 E-02 3.7 E-05 7.6 E-07 1.4 E-01 1.3 E-04 1048576 TT 2.1 E+01 2.8 E+01 5.5 E+16 8.3 E-01 3.3 E+02 8.4 E+00 1.2 E+11 1.9 E+02 4.4 E+00 3.7 E+03 QTT 1.3 E-04 1.2 E-05 0.0 E+00 4.5 E-12 1.7 E-02 6.8 E-02 5.9 E-06 4.7 E-08 1.1 E-01 1.3 E-04 In Table 5 we present the comparison of optimization results for the basic algorithm without quan- tization (“TT”) and for the improved algorithm with quantization (“QTT”). For each value N of the mode size, we choose the number of submodes in the quantized tensor as q = log 2 N. The QTT-solver gives several orders of magnitude more accurate results than the TT-solver. At the same time, for the QTT-solver, a regular decrease in the error is observed with an increase in the mode size. Thus, for the stable operation of gradient-free optimization methods based on the low-rank tensor approximations, it is necessary to quantize the modes of the original tensor. Rank inﬂuence. The rank (the size of the maximal-volume submatrices) determines how many points are queried at each iteration of theTTOpt algorithm, and this parameter is similar to population size in evolutionary algorithms. Small maximal-volume submatrices may give a better bound for maximal elements (see Eq. (3) from the main text), but ﬁnding small submatrices may be more challenging for the algorithm and may lead to numerical instabilities. At the same time, when choosing rank R, we should take into account that the algorithm will need2·T·(dq)·P·R2 function calls, where T is the number of sweeps (it should be at least 1, however, for better convergence, it is worth taking values of4−5) and P = 2 is a submode size. Hence we have inequalityR≤ √ M 4·T·d·q·, where M is a given limit on the number of function requests. In Figure 4 we demonstrate the dependence of the TTOpt’s accuracy on the rank. As can be seen, with small ranks (1 or 2), we have too low accuracy for most benchmarks. At the same time, the accuracy begins to drop at too high-rank values (7 or more), which is due to the insufﬁcient number of sweeps taken by the algorithm for convergence. Number of function queries inﬂuence. The number of requests to the objective function can be determined automatically based on algorithm iterations. Thus, with a total number of sweeps T, we will have O ( T ·d·max1≤k≤d ( NkR2 k )) calls to the objective function. However, in practice, it turns out to be more convenient to limit the maximum number of function calls, M, according to the computational budget. In Figure 5 the dependence of the accuracy on the total number of requests, M, to the objective function is presented. Predictably, as M increases, the accuracy also increases. The plateau for benchmarks are associated with the dependence of the result on the remaining parameters (R, q) of the TTOpt solver. 20Figure 4: The dependence of the ﬁnal error (absolute deviation of the obtained value from the exact minimum) on the rank for various benchmark functions. The reported values are averaged over ten independent runs. Figure 5: The dependence of the ﬁnal error (absolute deviation of the obtained value from the exact minimum) on the number of target functions calls for various benchmark functions. The reported values are averaged over ten independent runs. Function dimensionality inﬂuence. One of the advantages of the proposed approach is the possi- bility of its application to essentially multidimensional functions. In Table 6 we present the results of TTOpt for functions of various dimensions (we removed the F6 function from benchmarks, since its optima are known only for 2, 5 and 10-dimensional cases). Note that as a limit on the number of requests of the objective function, we choose 104 ·d, and the values of the remaining parameters were chosen the same as above. As can be seen, even for 500-dimensional functions, the TTOpt method results in fairly accurate solutions for most benchmarks. However, for benchmarks F4, F7 and F9 the errors are larger than for lower dimensions. We suspect that our heuristic of the number of objective function evaluations is not accurate in these cases. 21Table 6: The result of the TTOpt optimizer in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for various benchmark functions and various dimension numbers (d). FUNCTION d = 10 d = 50 d = 100 d = 500 F1 ϵ 3.9 E-06 3.9 E-06 3.9 E-06 3.9 E-06 τ 3.1 37.8 131.0 3153.5 F2 ϵ 2.9 E-07 3.7 E-06 5.2 E-06 2.1 E-05 τ 2.5 36.3 129.5 3153.1 F3 ϵ 2.3 E-12 4.9 E-10 1.1 E-09 4.7 E-09 τ 2.6 36.8 132.6 3205.4 F4 ϵ 4.4 E-15 2.2 E-14 4.4 E-14 1.0 E+00 τ 2.5 35.6 129.2 3131.1 F5 ϵ 2.5 E-02 3.7 E-02 3.7 E-02 3.7 E-02 τ 2.5 36.1 130.4 3132.2 F7 ϵ 5.5 E-09 8.9 E-08 3.4 E-07 5.6 E+02 τ 2.5 35.6 130.2 3123.7 F8 ϵ 4.6 E-11 2.3 E-10 4.6 E-10 2.3 E-09 τ 2.5 35.6 130.1 3124.0 F9 ϵ 3.4 E-01 9.3 E-01 2.2 E+00 1.0 E+01 τ 2.5 36.0 130.6 3157.4 F10 ϵ 1.3 E-04 6.4 E-04 1.3 E-03 6.4 E-03 τ 2.6 35.7 130.0 3140.7 Table 7: Comparison of the TTOpt optimizer with Bayesian optimization [40] baselines in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for various 10-dimensional benchmark functions. Note that τ values for Simultaneous Optimistic Optimization ( SOO), Direct Simultaneous Optimistic Optimization (dSOO), Locally Oriented Global Optimization (LOGO) and Random Optimization (RANDOM) refers to the time measured for a complied C-code, while our TTOpt optimizer is implemented in python, and will be more time-efﬁcient if written in C. ACKLEY RASTRIGIN ROSENBROCK SCHWEFEL TTO PT ϵ 3.9 E-06 4.6 E-11 3.9 E-01 8.4 E-02 τ 1.23 1.21 1.18 1.21 DSOO ϵ 4.0 E-10 2.0 E+00 8.1 E+00 5.3 E+02 τ 8.10 7.20 7.75 7.01 SOO ϵ 9.0 E-10 2.29 E+00 7.0 E+02 5.2 E+02 τ 7.44 7.57 0.66 7.31 LOGO ϵ 1.2 E-09 3.44 E+01 7.9 E+00 5.3 E+02 τ 6.80 0.77 7.06 7.51 RANDOM ϵ 1.1 E+01 2.29 E+00 1.5 E+00 1.2 E+03 τ 0.78 7.34 7.25 0.77 22B.2 Comparison with Bayesian optimization In Table 7 we present the results of TTOpt and several Bayesian methods for 10-dimensional benchmarks. We selected functions supported by the Bayesian optimization package from 23 [40]. Note that in all cases we chose 105 as the limit on the number of requests to the objective function and the values of the remaining parameters were chosen the same as above. TTOpt outperforms all tested Bayesian algorithms for Rastrigin, Rosenbrock, and Schwefel functions. For the Ackley function, the difference in accuracy is not signiﬁcant. On average, TTOpt is faster than Bayesian methods, despite they are implemented in C language. We stress that standard Bayesian methods are not applicable in higher-dimensional problems. B.3 Formulation of reinforcement learning problem as black-box optimization task Here we describe a typical reinforcement learning setting within Markov decision process formalism. The agent acts in the environment that has a set of states S. In each state s∈S the agent takes an action from a set of actions a∈A. Upon taking this action, the agent receives a local reward r(s,a) and reaches a new state s′, determined by the transition probability distribution T(s′|s,a). The policy π(a|s) speciﬁes which action the agent will take depending on its current state. Upon taking T (T is also called horizon) actions, the agent receives a cumulative reward, deﬁned as J = T−1∑ t=0 γtr(st,at), (10) where γ ∈[0,1] the is discounting factor, speciﬁes the relevance of historic rewards for the current step. The goal of the agent is to ﬁnd the policy π∗(a|s) that maximizes the expected cumulative reward J over the agent’s lifetime. In policy-based approaches, the policy is approximated by a function π(a|s,θ) (for example, a neural network), which depends on a vector of parameters θ. It follows then that the cumulative reward is a function of the parameters of the agent: J(θ) = E(st,at)∼T,π(θ) [T−1∑ t=0 γtr(st,at)) ] , (11) where r(st,at) ∼r(st,π(st−1 |θ). In case of episodic tasks we can assume γ = 1. Finding an optimal policy can be done by maximizing the cumulative reward J with respect to parameters θ: π∗(a|s) = π(a|s,θ∗), (12) where θ∗≃argmax J(θ). Notice that J may be non-differentiable due to the stochastic nature of T or the deﬁnition of r, depending on a particular problem formulation. However, this does not pose a problem for direct optimization algorithms. To summarize, the RL problem can be transformed into a simple optimization problem for the cumulative reward J(θ). The parameters of this function are the weights of the agent. Optimization of the cumulative reward with direct optimization algorithms is an on-policy learning in RL algorithm classiﬁcation. B.4 Rank dependence study Since rank is an important parameter of our method, we studied its inﬂuence on the rewards in RL, see Figure 5. Note that the rank determines how many points are queried at each iteration, and this parameter is similar to population size in evolutionary algorithms. We found almost no dependency of the ﬁnal reward on rank after R> 3 (on average). The Eq. (3) from the main text states that small maximal-volume submatrices should give a better bound for the maximal element. However, ﬁnding small submatrices may be more challenging for the algorithm. It turns out that reward functions in considered RL tasks are "good" for the maximum volume heuristic, e.g., even with small ranks, the algorithm produces high-quality solutions. 23The source code is available at https://github.com/Eiii/opt_cmp 23Table 8: The mean and standard deviation ( E ±σ) of ﬁnal cumulative reward before and after ﬁne-tuning with TTOpt. The policy’s weights are from the original repository of ARS [39]. ARS [39] ARS TTO PT(28) ANT-V3 4972.48 ±21.58 5039.90 ±57.00 HALF CHEETAH -V3 6527.89 ±82.70 6840.39 ±87.41 HOPPER -V3 3764.74 ±355.08 3296.49 ±11.81 HUMANOID -V3 11439.79 ±51.44 11560.01 ±54.08 SWIMMER -V3 354.43 ±2.32 361.87 ±1.76 WALKER 2D-V3 11519.77 ±112.55 11216.25 ±88.32 Table 9: The number of hidden units in each layer of convolutional policy h, the total number of parameters d, the sizes of the state and action spaces Aand S, the rank Rand the activation function between the layers (Act.). The average number of function quires per iteration (population size) in the case of TTOpt and ES baselines, respectively, is denoted by Q(the values separated by a comma). The number of seeds is Sd. H D S A R Act. Q Sd S 8 55 8 2 3 TANH 55,64 7 L 8 55 8 2 3 RELU 53,64 7 I 4 26 4 1 3 TANH 57,64 7 H 4 44 17 6 5 TANH 120,128 7 B.5 Constraint Handling in Evolutionary Algorithms There are two options to satisfy constraints in evolutionary computation called projection and penalization. These steps can be represented as two functions, θp = fproj(θ), and fpen(θp,θ) with a regularization term: Jp(θ) = J(θp) −λfpen(θp,θ). (13) In this work, we use the constraint functions described below. CDF projection is applied in exper- iments with mode size N = 3 (see Table 3 from the main text). The idea is to use the cumulative density function to map normally distributed parameters of the policies to {-1,0,1} set: θ=    −1 CDF(θ) ≤1 3 , 0 1 3 <CDF(θ) < 2 3 , 1 CDF(θ) ≥2 3 . (14) Uniform projection is applied when N = 256 in experiments shown in Table (3) from the main text. In this case, the idea is to keep the value if it satisﬁes the bounds, otherwise, we draw a new sample uniformly from a grid deﬁned in Algorithm A.1: θi p = {θi, if L≤θi ≤U, xi[k] otherwise. (15) Quadratic penalty is applied in all experiments. If Land U are the bounds, then fpen(θp,θ) =∑ i:θi<L(L−θi)2 + ∑ i:θi>U(θi −U)2. We set λ= 0.1 in all experiments. B.6 Reinforcement Learning Experiments Figure 6 and Figure 7 show training curves for all test environments which were not included in the main text. Fine-tuning of Linear Policies. We use TTOpt to ﬁne-tune Augmented Random Search (ARS) [39] linear policies obtained from the original paper. The cost function is the average of seven independent episodes with ﬁxed random seeds. The upper and lower grid bounds are estimated using statistics of pre-trained linear policies: bi = θi ±α·σ(θ) with α= 0.1. For Ant, Humanoid [ 17], Walker [17] and HalfCheetah [65] we select α= 0.5, and for Swimmer [16] and Hopper [54] we set α= 1. 240.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Number of function queries 104 0.0 0.1 0.2 0.3 average rewards 103 Swimmer-v3 0 1000 2000 3000 4000 seconds 0.0 0.1 0.2 0.3 average rewards 103 Swimmer-v3 0.0 0.2 0.4 0.6 0.8 1.0 Number of function queries 104 0.00 0.25 0.50 0.75 1.00 average rewards 103 InvertedPendulum-v2 0 50 100 150 200 250 300 seconds 0.0 0.2 0.4 0.6 0.8 1.0 average rewards 103 InvertedPendulum-v2 0 1 2 3 4 5 6 7 Number of function queries 104 0 1 2 3 4average rewards 103 HalfCheetah-v3 0 2000 4000 6000 8000 10000 12000 14000 seconds 0 1 2 3 4average rewards 103 HalfCheetah-v3 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Number of function queries 104 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 TTOpt cmaES openES GA target reward 0 500 1000 1500 2000 2500 3000 seconds 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 Figure 6: Training curves of TTOpt and baselines for N = 3 possible weight values: (−1,0,1). Left is the dependence of the average cumulative reward on the number of interactions with the environment (episodes). Right is the same reward depending on the execution time. The reward is averaged for seven seeds. The shaded area shows the difference of one standard deviation around the mean. 250 1 2 3 4 5 6 7 Number of function queries 104 0 1 2 3 4average rewards 103 HalfCheetah-v3 0 2000 4000 6000 8000 10000 12000 14000 seconds 0 1 2 3average rewards 103 HalfCheetah-v3 0 1 2 3 4 5 6 7 Number of function queries 104 0.0 0.1 0.2 0.3 0.4 average rewards 103 Swimmer-v3 0 5000 10000 15000 20000 seconds 0.0 0.1 0.2 0.3 average rewards 103 Swimmer-v3 0 2 4 6 8 10 Number of function queries 104 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 0 1000 2000 3000 4000 5000 6000 7000 seconds 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 0 2 4 6 8 10 Number of function queries 104 0.00 0.25 0.50 0.75 1.00 average rewards 103 InvertedPendulum-v2 TTOpt cmaES openES GA target reward 0 500 1000 1500 2000 2500 3000 seconds 0.0 0.2 0.4 0.6 0.8 1.0 average rewards 103 InvertedPendulum-v2 Figure 7: Training curves of TTOpt and baselines for N = 256 possible weight values. Left is the dependence of the average cumulative reward on the number of interactions with the environment (episodes). Right is the same reward depending on the execution time. The reward is averaged for seven seeds. The shaded area shows the difference of one standard deviation around the mean. 262.5 5.0 7.5 10.0 12.5 15.0 max ranks 0.0 0.2 0.4 0.6 0.8 1.0 ﬁnal rewards 103 InvertedPendulum-v2 max mean min 2.5 5.0 7.5 10.0 12.5 15.0 max ranks 0.15 0.20 0.25 0.30 0.35 ﬁnal rewards 103 Swimmer-v3 max mean min 2.5 5.0 7.5 10.0 12.5 15.0 max ranks 0.10 0.15 0.20 0.25 0.30 ﬁnal rewards 103 LunarLanderContinuous-v2 max mean min 2.5 5.0 7.5 10.0 12.5 15.0 max ranks 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ﬁnal rewards 103 HalfCheetah-v3 max mean min Figure 8: The dependency of the ﬁnal cumulative reward on rank. The mean, the minimum, and the maximum over seven random seeds are presented. The mode size is N = 3. 27
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Cross tensor approximation methods for compression and dimensionality reduction",
        "Two decades of blackbox optimization applications",
        "Alphastar: An evolutionary computation perspective",
        "A reinforcement learning method based on adaptive simulated annealing",
        "Nevergrad: Black-box optimization platform",
        "Handling bound constraints in cma-es: An experimental study",
        "Tt-qi: Faster value iteration in tensor train format for stochastic optimal control",
        "Openai gym",
        "Generalizing the column–row matrix decomposition to multi-way arrays",
        "Structured evolution with compact architectures for scalable policy optimization",
        "From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization",
        "Adaptive Blind Signal and Image Processing: Learning Algorithms and Applications",
        "Active subspaces - emerging ideas for dimension reduction in parameter studies",
        "Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents",
        "Reinforcement Learning Using Neural Networks, with Applications to Motor Control",
        "Infinite-horizon model predictive control for periodic tasks with contacts",
        "Evolving rewards to automate reinforcement learning",
        "How to find a good submatrix",
        "High-dimensional stochastic optimal control using continuous tensor decompositions",
        "Recurrent world models facilitate policy evolution",
        "A new scheme for the tensor representation",
        "The CMA Evolution Strategy: A Comparing Review",
        "Neuroevolution strategies for episodic reinforcement learning",
        "Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies",
        "Ant colony optimization and reinforcement learning",
        "Genetic algorithms",
        "The alternating linear scheme for tensor optimization in the tensor train format",
        "A literature survey of benchmark functions for global optimisation problems",
        "Evolution-guided policy gradient in reinforcement learning",
        "O(dlog n)-quantics approximation of n-d tensors in high-dimensional numerical modeling",
        "Optimization by direct search: New perspectives on some classical and modern methods",
        "A review of constraint-handling techniques for evolution strategies",
        "Es is more than just a traditional finite-difference approximator",
        "Self-guided evolution strategies with historical estimated gradients",
        "A novel policy gradient algorithm with pso-based parameter exploration for continuous control",
        "Tesseract: Tensorised actors for multi-agent reinforcement learning",
        "Guided evolutionary strategies: augmenting random search with surrogate gradients",
        "Simple random search of static linear policies is competitive for reinforcement learning",
        "An empirical study of bayesian optimization: Acquisition versus partition",
        "Black-box optimization revisited: Improving algorithm selection wizards through massive benchmarking",
        "Rectangular maximum-volume submatrices and their applications",
        "Human-level control through deep reinforcement learning",
        "A simplex method for function minimization",
        "Random gradient-free minimization of convex functions",
        "Continuous action-space reinforcement learning methods applied to the minimum-time swing-up of the acrobot",
        "Approximation of 2d×2d matrices using tensor decomposition",
        "Tensor-train decomposition",
        "Breaking the curse of dimensionality, or how to use svd in many dimensions",
        "TT-cross approximation for multidimensional arrays",
        "Ab initio solution of the many-electron schrödinger equation with deep neural networks",
        "Tensor networks for latent variable analysis: Novel algorithms for tensor train approximation",
        "Sparse reinforcement learning via convex optimization",
        "Experiments in balance with a 3d one-legged hopping machine",
        "Zero-shot text-to-image generation",
        "Evolution strategies as a scalable alternative to reinforcement learning",
        "Trust region policy optimization",
        "Proximal policy optimization algorithms",
        "Evolutionsstrategien für die numerische Optimierung",
        "Deterministic global optimization with artificial neural networks embedded",
        "Differential evolution – a simple and efficient heuristic for global optimization over continuous spaces",
        "Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning",
        "Mujoco: A physics engine for model-based control",
        "Grandmaster level in starcraft ii using multi-agent reinforcement learning",
        "Learning to control a 6-degree-of-freedom walking robot",
        "Natural evolution strategies",
        "Learning efficient tensor representations with ring-structured networks"
    ]
}
