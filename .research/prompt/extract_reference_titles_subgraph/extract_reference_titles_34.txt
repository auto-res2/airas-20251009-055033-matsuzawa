
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
Analytical Study of Momentum-Based Acceleration Methods in Paradigmatic High-Dimensional Non-Convex Problems Stefano Sarao Mannelli Department of Experimental Psychology University of Oxford Oxford, United Kingdom stefano.saraomannelli@psy.ox.ac.uk Pierfrancesco Urbani Université Paris-Saclay, CNRS, CEA Institut de physique théorique Gif-sur-Yvette, France pierfrancesco.urbani@ipht.fr Abstract The optimization step in many machine learning problems rarely relies on vanilla gradient descent but it is common practice to use momentum-based accelerated methods. Despite these algorithms being widely applied to arbitrary loss functions, their behaviour in generically non-convex, high dimensional landscapes is poorly understood. In this work, we use dynamical mean ﬁeld theory techniques to describe analytically the average dynamics of these methods in a prototypical non-convex model: the (spiked) matrix-tensor model. We derive a closed set, of equations that describe the behaviour of heavy-ball momentum and Nesterov acceleration in the inﬁnite dimensional limit. By numerical integration of these equations we observe that these methods speed up the dynamics but do not improve the algorithmic threshold with respect to gradient descent in the spiked model. 1 Introduction In many computer science applications one of the critical steps is the minimization of a cost function. Apart from very few exceptions, the simplest way to approach the problem is by running local algorithms that move down in the cost landscape and hopefully approach a minimum at a small cost. The simplest algorithm of this kind is gradient descent, that has been used since the XIX century to address optimization problems Cauchy (1847). Later on, faster and more stable algorithms have been developed: second order methods Levenberg (1944); Marquardt (1963); Broyden (1970); Fletcher (1970); Goldfarb (1970); Shanno (1970) where information from the Hessian is used to adapt the descent to the local geometry of the cost landscape, and ﬁrst order methods based on momentum Polyak (1964); Nesterov (1983); Cyrus et al. (2018); An et al. (2018); Ma and Yarats (2019) that introduce inertia in the algorithm and provably speed up convergence in a variety of convex problems. In the era of deep-learning and large datasets, the research has pushed towards memory efﬁcient algorithms, in particular stochastic gradient descent that trades off computational and statistical efﬁciency Robbins and Monro (1951); Sutskever et al. (2013), and momentum-based methods are very used in practice Lessard et al. (2016). Which algorithm is the best in practice seems not to have a simple answer and there are instances where a class of algorithms outperforms the other and vice-versa Kidambi et al. (2018). Most of the theoretical literature on momentum-based methods concerns convex problems Ghadimi et al. (2015); Flammarion and Bach (2015); Gitman et al. (2019); Sun et al. (2019); Loizou and Richtárik (2020) and, despite these methods have been successfully applied to a variety of problems, only recently high dimensional non-convex settings have been considered Yang et al. (2016); Gadat et al. (2018); Wang and Abernethy (2020). Furthermore, with few exceptions Scieur and Pedregosa (2020), the majority of these studies focus on worst-case analysis while empirically one could also be interested in the behaviour of such algorithms on typical Preprint. Under review. arXiv:2102.11755v4  [cond-mat.dis-nn]  27 Oct 2021instances of the optimization problem, formulated in terms of a generative model extracted from a probability distribution. The main contribution of this paper is the analytical description of the average evolution of momentum- based methods in two simple non-convex, high-dimensional, optimization problems. First we consider the mixed p-spin model Barrat et al. (1997); Folena et al. (2020), a paradigmatic random high-dimensional optimization problem. Furthermore we consider its spiked version, the spiked matrix-tensor Richard and Montanari (2014); Sarao Mannelli et al. (2020b) which is a prototype high-dimensional non-convex inference problem in which one wants to recover a signal hidden in the landscape. The second main result of the paper is the characterization of the algorithmic threshold for accelerated-methods in the inference setting and the ﬁnding that this seems to coincide with the threshold for gradient descent. The deﬁnition of the model and the algorithms used are reported in section 2. In section 3 and 4 we use dynamical mean ﬁeld theory Martin et al. (1973); De Dominicis (1978); Crisanti and Sommers (1992) to derive a set of equations that describes the average behaviour of these algorithms starting from random initialization in the high dimensional limit and in a fully non-convex setting. We apply our equations to the spiked matrix-tensor model Sarao Mannelli et al. (2020b, 2019b,a), which displays a similar phenomenology as the one described in Wang and Abernethy (2020); Sarao Mannelli et al. (2020a) for the phase retrieval problem: all algorithms have two dynamical regimes. First, they navigate in the non-convex landscape and, second, if the signal to noise ratio is strong enough, the dynamics eventually enters in the basin of attraction of the signal and rapidly reaches the bottom of the cost function. We use the derived state evolution of the algorithms to determine their algorithmic threshold for signal recovery. Finally, in Sec. 5 we show that in the analysed models, momentum-based methods only have an advantage in terms of speed but they do not outperform vanilla gradient descent in terms of the algorithmic recovery threshold. 2 Model deﬁnition We consider two paradigmatic non-convex models: the mixedp-spin model Crisanti and Sommers (1992); Cugliandolo and Kurchan (1993), and the spiked matrix-tensor model Richard and Montanari (2014); Sarao Mannelli et al. (2020b). Given a tensor TTT ∈(RN)⊗p and a matrix YYY ∈RN×N, the goal is to ﬁnd a common low-rank representation xxxthat minimizes the loss L= − 1 ∆p √ (p−1)! Np−1 N∑ i1,...,ip=1 Ti1,...,ip xi1 ...x ip − 1 ∆2 1√ N N∑ i,j=1 Yijxixj, (1) with xxxin the N-dimensional sphere of radius √ N. The two problems differ by the deﬁnition of the variables TTT and YYY. Call ξξξ(p) and ξξξ(2) order ptensor and a matrix having i.i.d. Gaussian elements, with zero mean and variances ∆p and ∆2 respectively. In the mixed p-spin model, tensor and matrix are completely random TTT = ξξξ(p) and YYY = ξξξ(2). While in the spiked matrix-tensor model there is a low-rank representation given by xxx∗∈SN−1( √ N) embedded in the problem as follows: Ti1...ip = √ (p−1)! Np−1 x∗ i1 ...x ∗ ip + ξ(p) i1...ip , Y ij = x∗ ix∗ j√ N + ξ(2) ij . (2) These problems have been studied both in physics, and computer science. In the physics literature, research has focused on the relationship of gradient descent and Langevin dynamics and the corre- sponding topology of the complex landscape Crisanti and Sommers (1992); Crisanti et al. (1993); Crisanti and Leuzzi (2006); Cugliandolo and Kurchan (1993); Aufﬁnger et al. (2013); Folena et al. (2020, 2021). The state evolution of the gradient descent dynamics for the mixed spiked matrix- tensor model has been studied only more recently Sarao Mannelli et al. (2019b,a). All these works considered simple gradient descent dynamics and its noisy (Langevin) dressing. In this work we focus on accelerated methods and provide an analytical characterization of the average performance of these algorithms for the models introduced above. In order to simplify the analysis we relax the hard constraint on the norm of the vector xxxand consider xxx∈RN while adding a penalty term to Lto enforce a soft constraint µ 4N (∑ ix2 i −N )2 , so that the total cost function is 2H= L+ µ 4N (∑ ix2 i −N )2 . Using the techniques described in detail in the next section we write the state evolution for the following algorithms: • Nesterov acceleration Nesterov (1983) starting from yyy[0] = xxx[0] ∈SN−1 (√ N ) xxx[t+ 1] = yyy[t] −α∇H(yyy[t]), (3) yyy[t+ 1] = xxx[t+ 1] + t t+ 3 (xxx[t+ 1] −xxx[t]) . (4) given αthe learning rate of the algorithm. • Polyak’s or heavy ball momentum(HB) Polyak (1964) starting from yyy[0] = 000 and xxx[0] ∈ SN−1 (√ N ) , given the parameters α, β yyy[t+ 1] = βyyy[t] + ∇H(xxx[t]), (5) xxx[t+ 1] = xxx[t] −αyyy[t+ 1]; (6) • gradient descent (GD) starting from xxx[0] ∈SN−1 (√ N ) xxx[t+ 1] = xxx[t] −α∇H(xxx[t]). (7) This case has been considered in Folena et al. (2020); Sarao Mannelli et al. (2019b) with the constraint ∑ ix2 i = N. The generalization to the present case in which constraint is soft is a straightforward small extension of these previous works. We will not compare the performance of these accelerated gradient methods to algorithms of different nature (such as for example message passing ones) in the same settings. Our goal will be the derivation of a set of dynamical equations describing the average evolution of such algorithms in the high dimensional limit N →∞. 3 Dynamical mean ﬁeld theory 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss H B =0.9 Nesterov GD 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius Figure 1: Simulation and DMFT comparison in mixed p-spin model. The simulations in the ﬁgures have parameters p= 3, ∆3 = 2/p, ∆2 = 1, ridge parameter µ= 10 and input dimension N = 1024. In all our simulations we use the dilution technique Semerjian et al. (2004); Krzakala and Zdeborová (2013) to reduce the computational cost. We consider: Nesterov acceleration in pink; heavy ball momentum in blue with α = 0.01 and β = 0.9; and gradient descent in grey. We run 100 simulations (in transparency) and draw the average. The parameters for heavy ball are the best parameters found in our simulations, see also Fig. 2 for a comparison. The results from the DMFT equations are drawn with dotted lines. We use dynamical mean ﬁeld theory (DMFT) techniques to derive a set of equation describing the evolution of the algorithms in the high-dimensional limit. The method has its origin in statistical physics and can be applied to the study of Langevin dynamics of disordered systems Martin et al. (1973); De Dominicis (1978); Mézard et al. (1987). More recently it was proved to be rigorous in the 3case of the mixed p-spin model Ben Arous et al. (2006); Dembo and Subag (2020). The application to the inference version of the optimization problem is in Sarao Mannelli et al. (2020b, 2019b). The same techniques have also been applied to study the stochastic gradient descent dynamics in single layer networks Mignacco et al. (2020) and in the analysis of recurrent neural networks Sompolinsky et al. (1988); Mastrogiuseppe and Ostojic (2017); Can et al. (2020). The derivation presented in the rest of the section is heuristic and, as such, it is not fully rigorous. Making our results rigorous would be an extension of the works Ben Arous et al. (2006); Dembo and Subag (2020) where path-integral methods are used to prove a large deviation principle for the inﬁnite-dimensional limit. Our non-rigorous results are checked against extensive numerical simulations. The idea behind DMFT is that, if the input dimension N is sufﬁciently large, one can obtain a description of the dynamics in terms of the typical evolution of a representative entry of the vector xxx (and vector yyywhen it applies). The representative element evolves according to a non-Markovian stochastic process whose memory term and noise source encode, in a self-consistent way, the interaction with all the other components of vector xxx(and yyy). The memory terms as well as the statistical properties of the noise are described by dynamical order parameters which, in the present model, are given by the dynamical two-time correlation and response functions. In this ﬁrst step of the analysis we obtain an effective dynamics for a representative entry xi (and yi). The next step consists in using such equations to compute self-consistently the properties of the corresponding stochastic processes, namely the memory kernel and the statistical correlation of the noise. In Fig. 1 we anticipate the results by comparing numerical simulations with the integration of the DMFT equations for the different algorithms: on the left we observe the evolution of the loss, on the right we observe the evolution of the radius of the vector xxx, deﬁned as the L2 norm of the vector ||xxx||2. We ﬁnd a good agreement between the DMFT state evolution and the numerical simulations. We compare Nesterov acceleration with the heavy ball momentum in the mixed p-spin model Fig. 1, and in the spiked model Fig. 3. Nesterov acceleration allows for a fast convergence to the asymptotic energy without need of parameter tuning. In Fig. 2 we compare the numerical simulations for the HB algorithm and the DMFT description of the corresponding massive momentum version for several control parameters. DMFT equations In the following we describe the resulting DMFT equations for the correlation and response functions. The details of their derivation for the case of the Nesterov acceleration are provided in the following section, while we leave the other cases to the supplementary material (SM). The dynamical order parameters appearing in the DMFT equations are one-time or two-time correlations, e.g. Cxy[t,t′] = ∑ ixi[t]yi[t′]/N, and response to instantaneous perturbation of the dynamics, e.g. Rx[t,t′] = (∑ iδxi[t]/δHi[t′] ) /Nby a local ﬁeld HHH[t′] ∈RN where the symbol δ denotes the functional derivative. In this section we show only the equations for the mixed p-spin model and we discuss the difference and the derivation of the equations for the spiked tensor in the SM. From the order parameters we can evaluate useful quantities that describe the evolution of the algorithms. In particular in Figs. 1,2,3 we show the loss, the radius, and the overlap with the solution in the spiked case (Fig. 3): • Average loss L[t] = − α ∆pCx[t,t] p 2 t∑ t′′=0 Rx[t,t′]Cx[t,t′]p−1 − α ∆2Cx[t,t] t∑ t′′=0 Rx[t,t′]Cx[t,t′]; (8) • Radius √ Cx[t,t]; • Deﬁne mx[t] = 1 N ∑ ixi[t]x∗ i an additional order parameter for the spiked matrix-tensor model (more details are given in the SM), the overlap with ground truth is xxx[t] ·xxx∗ ||xxx|| = mx[t]√ Cx[t,t] 4Nesterov acceleration. It has been shown that this algorithm has a quadratic convergence rate to the minimum in convex optimization problems under Lipschitz loss functions Nesterov (1983); Su et al. (2014), thus it outperforms standard gradient descent whose convergence is linear in the number of iterations. The analysis of the algorithm is described by the ﬂow of the following dynamical correlation functions Cx[t,t′] = 1 N ∑ i xi[t]xi[t′], (9) Cy[t,t′] = 1 N ∑ i yi[t]yi[t′], (10) Cxy[t,t′] = 1 N ∑ i xi[t]yi[t′], (11) Rx[t,t′] = 1 N ∑ i δxi[t] δHi[t′], (12) Ry[t,t′] = 1 N ∑ i δyi[t] δHi[t′]. (13) The dynamical equations are obtained following the procedure detailed in section 4. Call Q(x) = x2/(2∆2) + xp/(p∆p), Cx[t+ 1,t′] = Cxy[t,t′] −αµ(Cy[t,t] −1) Cy[t,t′] + α2 t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cy[t,t′′]) + + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cxy[t′,t′′]; (14) Cxy[t+ 1,t′] = Cy[t,t′] −αµ(Cy[t,t] −1) Cxy[t,t′] + α2 t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cy[t,t′′]) + + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cy[t′,t′′]; (15) Cxy[t′,t + 1] = 2t+ 3 t+ 3 Cx[t+ 1,t′] − t t+ 3Cx[t,t′]; (16) Cy[t′,t + 1] = 2t+ 3 t+ 3 Cxy[t+ 1,t′] − t t+ 3Cxy[t,t′]; (17) Rx[t+ 1,t′] = Ry[t,t′] + δt,t′ −αµ(Cy[t,t] −1) Ry[t,t′] + α2 t∑ t′′=t′ Ry[t,t′′]Ry[t′′,t′]Q′′(Cy[t,t′′]) ; (18) Ry[t′,t + 1] = 2t+ 3 t+ 3 Rx[t+ 1,t′] − t t+ 3Rx[t,t′]. (19) The initial conditions are: Cx[0,0] = 1, Cy[0,0] = 1, Cxy[0,0] = 1, Rx[t+ 1,t] = 1, Ry[t+ 1,t] = 2t+3 t+3 . The equations show a discretized version of the typical structure of DMFT equations. We can observe: terms immediately ascribable to the dynamical equations (3,4) and summations whose interpretation is less trivial without looking into the derivation. They represent memory kernels that take into account linear response theory for small perturbations to the dynamics (e.g. the last term of Eq. equation 14) and a noise whose statistical properties encode the effect of all the degrees of freedom on a representative one (e.g. the second last term of Eq. equation 14). 5Heavy ball momentum. The DMFT equations are obtained analogously to previous ones, Cy[t+ 1,t′] = βCy[t,t′] + µ(Cx[t,t] −1) Cxy[t,t′] + α t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cxy[t′′,t′]; (20) Cxy[t′,t + 1] = βCxy[t′,t] + µ(Cx[t,t] −1) Cx[t,t′] + α t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cx[t′,t′′]; (21) Cxy[t+ 1,t′] = Cxy[t,t′] −αCy[t+ 1,t′]; (22) Cx[t+ 1,t′] = Cx[t,t′] −αCxy[t′,t + 1]; (23) Ry[t+ 1,t′] = βRy[t,t′] + 1 αδt,t′ + µ(Cx[t,t] −1) Rx[t,t′] + α t∑ t′′=0 Rx[t,t′′]Rx[t′′,t′]Q′′(Cx[t,t′′]) ; (24) Rx[t+ 1,t′] = Rx[t,t′] −αRy[t+ 1,t′]. (25) with initial conditions: Cx[0,0] = 1, Cy[0,0] = 0, Cxy[0,0] = 0, Ry[t+ 1,t] = 1/α, Rx[t+ 1,t] = −1. Fig. 2 shows the consistency of theory and simulations. Mappings between discrete update equation and continuous ﬂow for both heavy ball momentum and Nesterov acceleration have been proposed in the literature. In the SM we considered the work Qian (1999) that maps HB to second order ODEs in some regimes of αand β. This mapping establishes the equivalence of the algorithm to the physics problem of a massive particle moving under the action of a potential. This problem has been studied in Cugliandolo et al. (2017) but the result is limited to the fully under-damped regime where there is no ﬁrst order derivative term, corresponding therefore to a dynamics that is fully inertial and which never stops due to energy conservation. In the SM we obtain the dynamical equations for arbitrary damping regimes, and we recover the equivalence established in Qian (1999) comparing the results from the two DMFTs formulations. 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss =0.9 =0.7 =0.5 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius Figure 2: DMFT for HB. Simulations of HB momentum in the mixed p-spin model with p = 3, ∆3 = 2/p, ∆2 = 1, ridge parameter µ= 10 and input dimension N = 1024. The parameters are α= 0.01 for all the simulations and β ∈{0.5,0.7,0.9}. We use solid lines to represent the result from the simulation, the dotted lines for the DMFT of HB. 6Gradient descent. A simple way to obtain the gradient descent DMFT is by taking the limitm→0 in the DMFT of the massive momentum description of HB. We get Cx[t+ 1,t′] = Cx[t,t′] −αµ(Cx[t,t] −1) Cx[t,t′] + α2 t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cx[t,t′′]) + α2 t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cx[t′,t′′]; (26) Rx[t+ 1,t′] = Rx[t,t′] + δt,t′ + α2 t∑ t′′=0 Rx[t,t′′]Rx[t′′,t′]Q′′(Cx[t,t′′]) −αµ(Cx[t,t] −1) Rx[t,t′]. (27) with initial conditions: Cx[0,0] = 1, and Rx[t+ 1,t] = 1. Apart from the µ-dependent term, these equations are a particular case of the ones that appear in Cugliandolo and Kurchan (1993); Crisanti et al. (1993) and we point to these previous references for details. 4 Derivation of DMFT for Nesterov acceleration 100 101 102 103 104 iteration 2.0 1.5 1.0 0.5 0.0 loss GD HB =0.9 Nesterov 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20 1.25radius 100 101 102 103 104 iteration 0.0 0.2 0.4 0.6 0.8 1.0overlap with solution Figure 3: DMFT in the spiked matrix-tensor model. Performance of heavy ball and Nesterov in the spiked matrix-tensor model with p= 3, 1/∆2 = 2.7, ∆3 = 1.0, and µ= 10. The parameters in the simulations are: α = 0 .01 and β = 0 .9 for HB. The different solid lines correspond to simulations with input dimension N = 8192, while the dotted lines are obtained from the DMFT that, by deﬁnition, is in the inﬁnite dimension limit. In the spiked version of the model the ﬁnite size effects are stronger and larger simulation sizes are needed. The approach for the DMFT proposed in this section is based on the dynamical cavity method Mézard et al. (1987). Consider the problem having dimension N + 1 and denote the additional entry of the vectors xxxand yyywith the subscript 0, x0 and y0. The idea behind cavity method is to evaluate how this additional dimension changes the dynamics of all degrees of freedom. If the dimension is sufﬁciently large the dynamics is only slightly modiﬁed by the additional dimension, and the effect of the additional degree of freedom can be tracked in perturbation theory. The framework described in this section might be extended to more other momentum-based algorithms (such as PID An et al. (2018) and quasi-hyperbolic momentum Ma and Yarats (2019)) with some minor adaptations. The steps to follow Mézard et al. (1987) can be summarised in: • Writing the equation of motion isolating the contributions of an additional degree of freedom, leading to Eqs. (28-30; • Treating the effect of the terms containing the new degree of freedom in perturbation theory, Eqs. (32-34); • Identifying the order dynamical order parameters, namely dynamical correlation and re- sponse functions, Eqs. (37,38). 7Consider the Nesterov update algorithm and isolate the effect of the additional degree of freedom xi[t+ 1] = yi[t] + α ∑ j̸=0 Jijyj[t] + α ∑ (i,i2,...,ip) Ji,i2,...,ip yi2 [t] ...y ip [t] −αµ  ∑ j̸=0 y2 j[t] N −1  yi[t] (28) + α ∑ (i,0,i3,...,ip) Ji,0,i3,...,ip y0[t]yi3 [t] ...y ip [t] + αJi0y0[t] + µ Ny2 0[t]yi[t], (29) yi[t+ 1] = xi[t+ 1] + t t+ 3 (xi[t+ 1] −xi[t]) . (30) We identify the term in line (29) as a perturbation, denoted by Hi[t]. We will assume that the perturbation is sufﬁciently small and the effective dynamics is well approximated by a ﬁrst order expansion around the original updates, so-called linear response regime. Therefore, the perturbed entries can be written as xi[t] ≈x0 i + α t∑ t′′=0 δxi[t] δHi[t′′]Hi[t′′], y i[t] ≈y0 i + α t∑ t′′=0 δyi[t] δHi[t′′]Hi[t′′]. (31) The dynamics of the 0th degree of freedom to the leading order in the perturbation is x0[t+ 1] = y0[t] −αµ (1 N ∑ j y2 j[t] −1 ) y0[t] + Ξ[t] + α2 ∑ j J0j t∑ t′′=0 δyj[t] δHj[t′′]Hj[t′′] (32) + α2 ∑ (0,i2,...,ip) J0,i2,...,ip ( t∑ t′′=0 δyi2 [t] δHi2 [t′′]Hi2 [t′′]yi3 [t] ...y ip [t] + perm. ) + O (1 N ) , (33) yi[t+ 1] = xi[t+ 1] + t t+ 3 (xi[t+ 1] −xi[t]) , (34) with Ξ = α∑ jJ0jyj[t] + α∑ (0,i2,...,ip) J0,i2,...,ip yi2 [t] ...y ip [t] a Gaussian noise with moments: E[Ξ[t]] = 0, E[Ξ[t]Ξ[t′]] = 1 ∆2 Cy[t,t′] + 1 ∆p Cp−1 y [t,t′] = Q′(Cy[t,t′]) ˙ =K[t,t′]. The terms in Eqs. (32,33) can be simpliﬁed. Consider the last term in Eq. equation 32: after substituting the Hi, J0jJ0j and J0jJ(j,0,...,ip) can be approximated by their expected values with a difference that is subleading in 1/N α2 ∑ j J0j t∑ t′′=0 δyj[t] δHj[t′′]J0jy0[t′′] ≈ α2 ∆2N t∑ t′′=0 δyj[t] δHj[t′′]y0[t′′] = α2 ∆2 t∑ t′′=0 Ry[t,t′′]y0[t′′], (35) where the last equality follows from the deﬁnition of response function in y. The same approximation is applied to Eq. (33), taking carefully into account the permutations, obtaining α2(p−1) ∆p t∑ t′′=0 Ry[t,t′′] (Cy[t,t′′])p−2 y0[t′′]. (36) Finally, collecting all terms, the effective dynamics of the additional dimension is given by x0[t+ 1] = y0[t] + αΞ[t] −αµ(Cy[t,t] −1) y0[t] + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) y0[t′′]; (37) y0[t+ 1] = x0[t+ 1] + t t+ 3 (x0[t+ 1] −x0[t]) . (38) 8In order to derive the updates of the order parameters, we need the expected values of ⟨Ξ[t]x0[t′]⟩ and ⟨Ξ[t]y0[t′]⟩with respect to the stochastic process. These are obtained using Girsanov theorem ⟨Ξ[t]x0[t′]⟩= α ∑ t′′ Rx[t′,t′′]Q′(Cy[t,t′′]) , ⟨Ξ[t]y0[t′]⟩= α ∑ t′′ Ry[t′,t′′]Q′(Cy[t,t′′]) . The ﬁnal step consists in substituting the Eqs. (37,38) into the equations of the order parameters Eqs. (14-19). Then we identify the order parameters in the equations and use the results of Girsanov theorem to obtain the dynamical equations reported in section 3. 5 Algorithmic threshold 0 1 2 3 4 5 p 0.5 1.0 1.5 2.0 2.5 3.0 3.51/ 2 GD extrapolated th. HB extrapolated th. Nesterov extrapolated th. Figure 4: Phase diagram of the spiked matrix-tensor model. The horizontal and vertical axis represent the parameters of the model ∆p and 1/∆2. We identify two regions in the diagram: where Nesterov, heavy ball and gradient descent algorithms lead to the hidden solution (upper region), and where they fail (lower region). The grey square connected by a solid line represents the threshold of gradient descent estimated numerically as detailed in the text. We use points to indicate the threshold extrapolated from the DMFT: pink circles for Nesterov acceleration and blue diamonds for heavy ball momentum with β = 0.9 and α= 0.01. Finally we investigate the performance of accelerated methods in recovering a signal in a complex non-convex landscape. The dynamics of the gradient descent has been studied in the spiked matrix- tensor model in Sarao Mannelli et al. (2019b). Using DMFT it was possible to compute the phase diagram for signal recovery in terms of the noise levels ∆2 and ∆p. This phase diagram was later conﬁrmed theoretically Sarao Mannelli et al. (2019a). Given the DMFT equations derived in the previous sections we can apply the analysis used in Sarao Mannelli et al. (2019b) to accelerated gradient methods. Given order of the tensor pand ∆p, increasing ∆2 the problem becomes harder and moves from the easy phase - where the signal can be partially recovered - to an algorithmically impossible phase - where the algorithm remains stuck at vanishingly small overlap with the signal. The goal of the analysis is to characterize the algorithmic threshold that separates the two phases. Using the DMFT we estimate the relaxation time – the time the accelerated methods need to ﬁnd the signal. Since this time diverges approaching the algorithmic threshold, the ﬁt of the divergence point gives an estimation of the threshold. More precisely, for each value of ∆p as the noise to signal ratio (∆2) increases the simulation time required to arrive close to the signal 1 increases like a power law ∼a |∆2 −∆al. 2 (∆p)|−θ. The algorithmic threshold ∆al. 2 (∆p) is obtained by ﬁtting the parameters of the power law (a,θ, ∆al. 2 ). In the SM we show an example of the extrapolation of a single point where many initial conditions mx(0) are considered in order to correctly characterize the limits N →∞ and mx(0) →0+. Finally the ﬁts obtained for the three algorithms and for several ∆p are shown in the phase diagram of Fig. 4 for p= 3. We observe that all the algorithms give very close thresholds. DMFT allows to obtain a good estimation of the threshold, free from ﬁnite size effects and stochastic ﬂuctuations that are present in the direct estimation from the simulations. 1Since the best possible overlap for maximum a posteriori estimator mMAP can be computed explicitly, "close" means the time that the algorithms takes to arrive at 0.9mMAP 9Conclusions and broader impact In this work we analysed momentum-accelerated methods in two paradigmatic high-dimensional non-convex problems: the mixed p-spin model and the spiked matrix-tensor model. Our analysis is based on dynamical mean ﬁeld theory and provides a set of equations that characterize the average evolution of the dynamics. We have focused on Polyak’s heavy ball and Nesterov acceleration, but the same techniques may be applied to more recent methods such as quasi-hyperbolic momentum Ma and Yarats (2019) and proportional integral-derivative control algorithm An et al. (2018). Momentum-based methods are techniques commonly used in practice but poorly understood at the theoretical level. This work analysed the dynamics of momentum-based algorithms in a very con- trolled setting of a high-dimensional non-convex inference problem which allowed us to establish that accelerated methods have a recovery threshold which is – within the limits of numerical integration – the same of vanilla gradient descent. Our analysis can be easily extended to 1-layer neural networks – combining our technical results with the techniques of Mignacco et al. (2020) – and to simple inference problem seen from the learning point of view, such as the phase retrieval problem Mignacco et al. (2021). The same questions can also be analysed in the context of recurrent networks Mastrogiuseppe and Ostojic (2017); Can et al. (2020) where DMFT approaches have already been applied to gradient-based methods. Our study is theoretical in nature and we do not foresee any societal impact. Acknowledgments The authors thank Andrew Saxe for precious discussions. This work was supported by the Wellcome Trust and Royal Society (grant number 216386/Z/19/Z), and by "Investissements d’Avenir" LabEx- PALM (ANR-10-LABX-0039-PALM). References Elisabeth Agoritsas, Giulio Biroli, Pierfrancesco Urbani, and Francesco Zamponi. Out-of-equilibrium dynamical mean-ﬁeld equations for the perceptron model. Journal of Physics A: Mathematical and Theoretical, 51(8):085002, 2018. Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller approach for stochastic optimization of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8522–8531, 2018. Antonio Aufﬁnger, Gérard Ben Arous, and Ji ˇrí ˇCern`y. Random matrices and complexity of spin glasses. Communications on Pure and Applied Mathematics, 66(2):165–201, 2013. Alain Barrat, Silvio Franz, and Giorgio Parisi. Temperature evolution and bifurcations of metastable states in mean-ﬁeld spin glasses, with connections with structural glasses. Journal of Physics A: Mathematical and General, 30(16):5593–5612, aug 1997. doi: 10.1088/0305-4470/30/16/006. URL https://doi.org/10.1088/0305-4470/30/16/006. Gérard Ben Arous, Amir Dembo, and Alice Guionnet. Cugliandolo-Kurchan equations for dynamics of spin-glasses. Probability theory and related ﬁelds, 136(4):619–660, 2006. Charles G Broyden. The convergence of a class of double-rank minimization algorithms: 2. the new algorithm. IMA journal of applied mathematics, 6(3):222–231, 1970. Tankut Can, Kamesh Krishnamurthy, and David J Schwab. Gating creates slow modes and controls phase-space complexity in grus and lstms. In Mathematical and Scientiﬁc Machine Learning , pages 476–511. PMLR, 2020. Tommaso Castellani and Andrea Cavagna. Spin-glass theory for pedestrians. Journal of Statistical Mechanics: Theory and Experiment, 2005(05):P05012, 2005. Augustin Cauchy. Méthode générale pour la résolution des systemes d’équations simultanées. Comp. Rend. Sci. Paris, 25(1847):536–538, 1847. 10Andrea Crisanti and Luca Leuzzi. Spherical 2+ p spin-glass model: An analytically solvable model with a glass-to-glass transition. Physical Review B, 73(1):014412, 2006. Andrea Crisanti and H-J Sommers. The sphericalp-spin interaction spin glass model: the statics. Zeitschrift für Physik B Condensed Matter, 87(3):341–354, 1992. Andrea Crisanti, Heinz Horner, and H-J Sommers. The sphericalp-spin interaction spin-glass model. Zeitschrift für Physik B Condensed Matter, 92(2):257–271, 1993. Leticia F Cugliandolo and Jorge Kurchan. Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model. Physical Review Letters, 71(1):173, 1993. Leticia F Cugliandolo, Gustavo S Lozano, and Emilio N Nessi. Non equilibrium dynamics of isolated disordered systems: the classical hamiltonian p-spin model. Journal of Statistical Mechanics: Theory and Experiment, 2017(8):083301, 2017. Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization algorithm for strongly convex functions. In 2018 Annual American Control Conference (ACC), pages 1376–1381. IEEE, 2018. C De Dominicis. Dynamics as a substitute for replicas in systems with quenched random impurities. Physical Review B, 18(9):4913, 1978. Amir Dembo and Eliran Subag. Dynamics for spherical spin glasses: disorder dependent initial conditions. Journal of Statistical Physics, pages 1–50, 2020. Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In Conference on Learning Theory, pages 658–695. PMLR, 2015. Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):317–322, 1970. Giampaolo Folena, Silvio Franz, and Federico Ricci-Tersenghi. Rethinking mean-ﬁeld glassy dynamics and its relation with the energy landscape: The surprising case of the spherical mixed p-spin model. Physical Review X, 10(3):031045, 2020. Giampaolo Folena, Silvio Franz, and Federico Ricci-Tersenghi. Gradient descent dynamics in the mixed p-spin spherical model: ﬁnite-size simulations and comparison with mean-ﬁeld integration. Journal of Statistical Mechanics: Theory and Experiment, 2021(3):033302, 2021. Sébastien Gadat, Fabien Panloup, Soﬁane Saadane, et al. Stochastic heavy ball. Electronic Journal of Statistics, 12(1):461–529, 2018. Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the heavy-ball method for convex optimization. In 2015 European control conference (ECC), pages 310–315. IEEE, 2015. Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum in stochastic gradient methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 9633–9643. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper/2019/file/4eff0720836a198b6174eecf02cbfdbf-Paper.pdf. Donald Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26, 1970. Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufﬁciency of existing momentum schemes for stochastic optimization. In 2018 Information Theory and Applications Workshop (ITA), pages 1–9. IEEE, 2018. Florent Krzakala and Lenka Zdeborová. Performance of simulated annealing in p-spin glasses. In Journal of Physics: Conference Series, volume 473, page 012022. IOP Publishing, 2013. Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57–95, 2016. 11Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of applied mathematics, 2(2):164–168, 1944. Nicolas Loizou and Peter Richtárik. Momentum and stochastic momentum for stochastic gradi- ent, newton, proximal point and subspace descent methods. Computational Optimization and Applications, 77(3):653–710, 2020. Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for deep learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=S1fUpoR5FQ. Donald W Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the society for Industrial and Applied Mathematics, 11(2):431–441, 1963. Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. Physical Review A, 8(1):423, 1973. Francesca Mastrogiuseppe and Srdjan Ostojic. Intrinsically-generated ﬂuctuating activity in excitatory-inhibitory networks. PLoS computational biology, 13(4):e1005498, 2017. Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientiﬁc Publishing Company, 1987. Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Dynamical mean-ﬁeld theory for stochastic gradient descent in gaussian mixture classiﬁcation. In 2020 Conference on Neural Information Processing Systems-NeurIPS 2020, 2020. Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborová. Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem. Machine Learning: Science and Technology, 2021. Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983. Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1): 145–151, 1999. Emile Richard and Andrea Montanari. A statistical model for tensor pca. In Z. Ghahra- mani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Ad- vances in Neural Information Processing Systems , volume 27, pages 2897–2905. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/ b5488aeff42889188d03c9895255cecc-Paper.pdf. Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951. Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, and Lenka Zdeborová. Who is afraid of big bad minima? analysis of gradient-ﬂow in spiked matrix-tensor models. In Advances in Neural Information Processing Systems, pages 8679–8689, 2019a. Stefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Passed & spurious: Descent algorithms and local minima in spiked matrix-tensor models. In International Conference on Machine Learning, pages 4333–4342, 2019b. Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Complex dynamics in simple neural networks: Understanding gradient ﬂow in phase retrieval. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 3265–3274. Cur- ran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/ 2172fde49301047270b2897085e4319d-Paper.pdf. 12Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Marvels and pitfalls of the langevin algorithm in noisy high-dimensional inference. Physical Review X, 10(1):011057, 2020b. Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8565–8572. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/scieur20a.html. Guilhem Semerjian, Leticia F Cugliandolo, and Andrea Montanari. On the stochastic dynamics of disordered spin models. Journal of statistical physics, 115(1):493–530, 2004. David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of computation, 24(111):647–656, 1970. Haim Sompolinsky, Andrea Crisanti, and Hans-Jurgen Sommers. Chaos in random neural networks. Physical review letters, 61(3):259, 1988. Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s accelerated gradient method: Theory and insights. Advances in neural information processing systems, 27:2510–2518, 2014. Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang. Non-ergodic convergence analysis of heavy-ball algorithms. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 5033–5040, 2019. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139– 1147. PMLR, 2013. Jun-Kun Wang and Jacob Abernethy. Quickly ﬁnding a benign region via heavy ball momentum in non-convex optimization. arXiv preprint arXiv:2010.01449, 2020. Tianbao Yang, Qihang Lin, and Zhe Li. Uniﬁed convergence analysis of stochastic momentum methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016. 13Supplemental Material A Spiked matrix-tensor model In this section we discuss how the DMFT equations of the spiked matrix-tensor model differ from the mixed p-spin model. The main difference is that the hidden solution deforms locally the loss function H= − 1 ∆p √ (p−1)! Np−1 N∑ i1,...,ip=1 Ti1,...,ip xi1 ...x ip − 1 ∆2 1√ N N∑ i,j=1 Yijxixj+ − 1 p∆p  1 N ∑ j xjx∗ j   p − 1 2∆2  1 N ∑ j xjx∗ j   2 + µ 4N (N∑ i=1 x2 i −N )2 . (39) As it clearly appears from the equation of the loss, the overlap of the hidden solution with the estimator plays an important role. This leads to two additional order parameters mx[t] = 1 N ∑ jxj[t]x∗ j and my[t] = 1 N ∑ jyj[t]x∗ j (or mv(t) = 1 N ∑ jvj(t)x∗ j for massive gradient ﬂow). Since the stochastic part of the loss is unchanged, the derivation follows same steps shown in section 4 of the main text. They lead to modiﬁed dynamical equations where overlap with the hidden solution is present, for instance in Nesterov they are x0[t+ 1] = y0[t] + αΞ[t] −αµ(Cy[t,t] −1) y0[t]+ + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) y0[t′′] + Q′(my[t])xxx∗, (40) y0[t+ 1] = x0[t+ 1] + t t+ 3 (x0[t+ 1] −x0[t]) . (41) Finally, substituting the effective dynamics into the deﬁnition of the order parameters we obtain: • for Nesterov acceleration Cx[t+ 1,t′] = Cxy[t,t′] + α2 t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cy[t,t′′]) + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cxy[t′,t′′]+ −αµ(Cy[t,t] −1) Cy[t,t′] −Q′(my[t])mx[t′], Cxy[t+ 1,t′] = Cy[t,t′] + α2 t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cy[t,t′′]) + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cy[t′,t′′]+ −αµ(Cy[t,t] −1) Cxy[t,t′] −Q′(my[t])my[t′], Cxy[t′,t + 1] = Cx[t+ 1,t′] + t t+ 3 (Cx[t+ 1,t′] −Cx[t,t′]) , Cy[t′,t + 1] = Cxy[t+ 1,t′] + t t+ 3 (Cxy[t+ 1,t′] −Cxy[t,t′]) , Rx[t+ 1,t′] = Ry[t,t′] + δt,t′ + α2 t∑ t′′=t′ Ry[t,t′′]Ry[t′′,t′]Q′′(Cy[t,t′′]) −αµ(Cy[t,t] −1) Ry[t,t′], Ry[t′,t + 1] = Rx[t+ 1,t′] + t t+ 3 (Rx[t+ 1,t′] −Rx[t,t′]) , 14mx[t+ 1] = my[t] −αµ(Cy[t,t] −1) my[t] + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′)) my[t′′] + Q′(my[t]) , my[t+ 1] = mx[t+ 1] + t t+ 3 (mx[t+ 1] −mx[t]) , with initial conditions Cx[0,0] = 1 , Cy[0,0] = 1 , Cxy[0,0] = 1 , Rx[t + 1,t] = 1 , Ry[t+ 1,t] = 2t+3 t+3 , mx[0] = 0+, my[0] = 0+; • for heavy ball momentum. Cy[t+ 1,t′] = βCy[t,t′] + µ(Cx[t,t] −1) Cxy[t,t′] + α t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cxy[t′′,t′] −Q′(mx[t])my[t′]; Cxy[t′,t + 1] = βCxy[t′,t] + µ(Cx[t,t] −1) Cx[t,t′] + α t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cx[t′,t′′] −Q′(mx[t])mx[t′]; Cxy[t+ 1,t′] = Cxy[t,t′] −αCy[t+ 1,t′]; Cx[t+ 1,t′] = Cx[t,t′] −αCxy[t′,t + 1]; Ry[t+ 1,t′] = βRy[t,t′] + 1 αδt,t′ + µ(Cx[t,t] −1) Rx[t,t′] + α t∑ t′′=0 Rx[t,t′′]Rx[t′′,t′]Q′′(Cx[t,t′′]) ; Rx[t+ 1,t′] = Rx[t,t′] −αRy[t+ 1,t′] my[t+ 1] = βmy[t] −µ(Cx[t,t] −1) mx[t]+ + t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′)) mx[t′′] −Q′(mx[t]) , mx[t+ 1] = mx[t] −αmy[t+ 1]. with initial conditions: Cx[0,0] = 1 , Cy[0,0] = 0 , Cxy[0,0] = 0 , Ry[t+ 1,t] = 1 /α, Rx[t+ 1,t] = −1, my[0] = O+, mx[0] = O+. • for massive gradient ﬂow (see Sec. C) ∂tCx(t,t′) = Cxv(t′,t) , m∂tCv(t,t′) = −Cv(t,t′) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]Cxv(t′′,t′)+ + ∫ t′ 0 Q′[Cx(t,t′′)]Rv(t′,t′′) −µCx(t.t′) (Cx(t,t) −1) + Q′[mx(t′)]mx(t) , ∂tCxv(t,t′) = Cv(t,t′) , m∂t′ Cxv(t,t′) = −Cxv(t,t′) + ∫ t′ 0 dt′′Rx|v(t′,t′′)Q′′[Cx(t′,t′′)]Cx(t,t′′)+ + ∫ t 0 Q′[Cx(t′,t′′)]Rx|v(t,t′′) −µCxv(t,t′) (Cx(t,t) −1) + Q′[mx(t)]mv(t′) , m∂tRv(t,t′) = δ(t−t′) −Rv(t,t′) + ∫ t t′ dt′′Q′′[C(t,t′′)]Rx|v(t,t′′)Rx|v(t′′,t′)+ −µRx|v(t,t′) (Cx(t,t) −1) , 15∂tRx|v(t,t′) = Rv(t,t′) , ∂tmx(t) = mv(t) , m∂tmv(t) = −mv(t) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]mx(t′′) + Q′[mx(t)]+ −µmx(t) (Cx(t,t) −1) , with initial conditions are : Cx(0,0) = 1; Cv(0,0) = 0; Cxv(0,0) = 0; Rv(t+,t) = 1/m; Rx|v(t,t) = 0; mx(0) = 0+. my(0) = 0+. Finally the equation to compute the loss in time is L[t] = − α ∆pCx[t,t] p 2 t∑ t′′=0 Rx[t,t′]Cx[t,t′]p−1 − α ∆2Cx[t,t] t∑ t′′=0 Rx[t,t′]Cx[t,t′] −Q(mx[t]) . (42) B Extracting the recovery threshold 1.5 2.0 2.5 3.0 1/ 2 0 5000 10000 15000 20000 25000 30000 tjump 1.5 2.0 2.5 3.0 1/ 2 3 4 5 6 7 8 ln tjump rescaled estrapolation 1.38 mx(0)=1.0e-10 mx(0)=1.0e-15 mx(0)=1.0e-20 mx(0)=1.0e-25 mx(0)=1.0e-30 mx(0)=1.0e-35 mx(0)=1.0e-40 Figure 5: Algorithmic threshold extrapolated using Nesterov acceleration. The dots are the divergence time obtained for the different values of ∆2 with ∆3 = 4.0 ﬁxed. On the right panel we show that these lines collapse to a single line once rescaled by a factor aln mx[0] with a≈1.089. The extrapolation procedure for ∆3 = 4.0 is shown in Fig. 5. The threshold obtained by ﬁtting with a power law and observing the divergent ∆2. On the left panel we plot the number of iteration before the algorithm jumps to the solution tjump as a function of the signal to noise ratio 1/∆2. The ﬁgure also show remarkable effects of the initial conditions for mx and my. These effects where already described and understood in Sarao Mannelli et al. (2019b). 16C Correspondence with continuous HB equations 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss = 0.9 = 0.7 = 0.5 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius DMFT massive DMFT HB Figure 6: Comparison of HB and massive with mapping. The ﬁgure reproduce the same setting of Fig. 2 with the additional dashed line for the DMFT of massive gradient ﬂow using the mapping. An alternative way to analyze the HB dynamics is by using the results of Qian (1999) to map it to the massive momentum described by the ﬂow equation m¨xi(t) + ˙xi(t) = −δL[xxx(t)] δxi(t) . (43) The natural discretization of this equation is m h2 (x[k+ 1]−2x[k]−x[k−1])+ 1 h(x[k+ 1] −x[k]) = −∇L(x[k]) (44) being hthe time discretization step (analogous to the learning rate in gradient descent). Using the mapping of Qian (1999) we can identify m= βα (1 −β)2 (45) h= α 1 −β (46) Observe that in order to be consistent with a continuous dynamics we need the following scaling β = O(1), α= O[(1 −β)2]. We empirically observe in the simulations a good agreement between massive and HB even forβ = 0.999 and α= 0.01. In the following, when discussing the comparison between simulation and DMFT, we mean that we run HB algorithm and superimpose on its massive momentum description. The massive momentum dynamics was also considered in Cugliandolo et al. (2017) without the damping term ( ˙xi(t)) and for the model with a hard spherical constraint ∑ ixi[t]2 = N. While the DMFT derived in Cugliandolo et al. (2017) completely describe the aforementioned particular case, the way in which it is written uses the fact that without damping the dynamics is conservative and the spherical constraint can be enforced using that. In our case we are not in this regime and therefore we resort to a different computation, that will lead us to quite different equations. Indeed if one wants to transform massive momentum in a practical algorithm one needs to transform the second order ODEs into ﬁrst order by deﬁning velocity variables vi(t) = ˙xi(t). Then the discrete version of Eq. equation 43 is xi[t+ 1] = xi[t] + hvi[t] vi[t+ 1] = vi[t] −h m vi[t] −h m δH δxi[t] (47) Analysing these equations through DMFT one gets a set of ﬂow equations for the following dy- namical order parameters Cx[t,t′] = ∑ ixi[t]xi[t′]/N, Cv[t,t′] = ∑ ivi[t]vi[t′]/N, Cxv[t,t′] =∑ ixi[t]vi[t]/N, Rv[t,t′] = 1 N ∑ i δvi[t] δHi[t′] , and Rx|v[t,t′] = 1 N ∑ i δxi[t] δHi[t′] ; where HHH is an instanta- neous perturbation acting on the velocity. The result of the computation gives: Cx[t+ 1,t′] = Cx[t,t′] + hCxv(t′,t) ; (48) 17Cv[t+ 1,t′] = Cv[t,t′] −h m Cv(t,t′) −µ h m Cxv(t,t′) (Cx(t,t) −1) + h2 m t∑ t′′=0 Rx|v[t,t′′]Q′′(Cx[t,t′′]) Cxv[t′′,t′] + h2 m t′ ∑ t′′=0 Q′(Cx[t,t′′]) Rv[t′,t′′]; (49) Cxv[t+ 1,t′] = Cxv[t,t′] + hCv[t,t′] ; (50) Cxv[t,t′+ 1] = Cxv[t,t′] −h m Cxv[t,t′] −µ h m Cx[t,t′] (Cx[t′,t′] −1) + h2 m t′ ∑ t′′=0 Rx|v[t′,t′′]Q′′(Cx[t′,t′′]) Cx[t,t′′] + h2 m t∑ t′′=0 Q′(Cx[t′,t′′]) Rx|v[t,t′′] (51) Rv[t+ 1,t′] = Rv[t,t′] + h m δt,t′ −µ h m Rx|v[t,t′] (Cx(t,t) −1) −h m Rv[t,t′] + h2 m t∑ t′′=0 Q′′(C[t,t′′]) Rx|v[t,t′′]Rx|v[t′′,t′] (52) Rx|v[t+ 1,t′] = Rx|v[t,t′] + hRv[t,t′]. (53) and initial conditions : Cx[0,0] = 1, Cv[0,0] = 0, Cxv[0,0] = 0, Rv[t+1,t] = 1/m, Rx|v[t+1,t] = 0. 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss =0.9 =0.7 =0.5 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius DMFT massive DMFT HB Figure 7: Comparison of HB and with DMFT. Simulations of HB momentum in the mixed p-spin model with p= 3, ∆3 = 2/p, ∆2 = 1, ridge parameter µ= 10 and input dimension N = 1024. The parameters are α= 0.01 for all the simulations and β ∈{0.5,0.7,0.9,0.99,0.999}. We use solid line to represent the result from the simulation, the dotted line for the DMFT of massive gradient ﬂow with the mapping. We empirically observe that in this problem the value of βthat gives the best speed up is β = 0.9. In order the integrate the DMFT of massive gradient we matched the mass as described in Eq. equation 45 and consider time steps h∈{0.005,0.005,0.0125,0.25,0.25}. Derivation of the DMFT equations for massive gradient ﬂow In this section we derive the dynamical mean ﬁeld theory (DMFT) equations of massive gradient ﬂow in the mixed p-spin. We use the generating functional approach described in Castellani and Cavagna (2005); Agoritsas et al. (2018) to obtain the effective dynamical equations. First we rewrite de massive dynamics Eq. equation 43 as two ODEs m˙vvv(t) = −vvv(t) −∇H[xxx(t)], (54) ˙xxx(t) = vvv(t). (55) We use the following simple identity that takes the name of generating functional 1 = Z= ∫ D[xxx,vvv] δ(m˙vvv(t) +vvv(t) + ∇H[xxx(t)]) δ( ˙xxx(t) −vvv(t)) (56) 18= ∫ D[xxx,˜xxx,vvv,˜vvv] N∏ i=1 exp { i ∫ ˜vi(t) [m˙vi(t) + vi(t) + ∇iH[xxx(t)]] dt } exp { i ∫ ˜xi(t) [ ˙xi(t) −vi(t)] dt } (57) where in the ﬁrst line we integrate over all possible trajectories of vvvand xxx, and we impose them to match the massive gradient ﬂow equations using Dirac’s deltas. In the second line we use the Fourier representation of the delta and we absorb the normalization constants in the term D[xxx,˜xxx,vvv,˜vvv]. We can now average over the stochasticity of the problem, let us indicate with an overline the average. 1 = Z= ∫ D[xxx,˜xxx,vvv,˜vvv] N∏ i=1 exp { i ∫ ˜vi(t) [m˙vi(t) + vi(t)] dt } exp { i ∫ ˜xi(t) [ ˙xi(t) −vi(t)] dt } × (58) × N∏ i=1 exp   i ∫ ˜vi(t)   √ (p−1)! Np−1 ∑ (i,i2,...,ip) ξ(p) i,i2,...,ip xi2 (t) ...x ip (t) + 1√ N ∑ j ξ(2) i,j xj(t)  dt   × (59) × N∏ i=1 exp   i ∫ ˜vi(t)µ  1 N ∑ j x2 j(t) −1  xi(t)dt    (60) We can proceed integrating the second line over the noise. Importantly we must group all the element that multiply a given ξ(p). Considering only second line and neglecting constant multiplicative factors we obtain exp { − N 2p∆p ∫ [ p (∑ i ˜vi(t)˜vi(t′) N )(∑ i xi(t)xi(t′) N )p−1 + + p(p−1) (∑ i ˜vi(t)xi(t′) N )(∑ i xi(t)˜vi(t′) N )(∑ i xi(t)xi(t′) N )p−2 ] dt′dt } × ×exp { − N 2∆2 ∫ [(∑ i ˜vi(t)˜vi(t′) N )(∑ i xi(t)xi(t′) N ) + (∑ i ˜vi(t)xi(t′) N )(∑ i xi(t)˜vi(t′) N )] dt′dt } We deﬁne Q(x) = xp/(p∆p) + x2/(2∆2) and the order parameters Cx[t,t′] = ∑ i xi[t]xi[t′]/N, (61) Cv[t,t′] = ∑ i vi[t]vi[t′]/N, (62) Cxv[t,t′] = ∑ i xi[t]vi[t]/N, (63) Rv[t,t′] = 1 N ∑ i δvi[t] δHi[t′], (64) Rx|v[t,t′] = 1 N ∑ i δxi[t] δHi[t′]; (65) and enforce some of them using Dirac’s deltas ∫ D[Cx,C˜v,Cx˜v,C˜vx]δ  NC˜v(t,t′) − ∑ j i˜vj(t) i˜vj(t′)  δ  NCx(t,t′) − ∑ j xj(t) xj(t′)  × ×δ  NCx˜v(t,t′) − ∑ j xj(t) i˜vj(t′)  δ  NC˜vx(t,t′) − ∑ j i˜vj(t) xj(t′)  × 19×exp { − N 2∆p ∫ [ C˜v(t,t′)Q′[Cx(t,t′)] + C˜vx(t,t′)Cx˜v(t,t′)Q′′[Cx(t,t′)] ] dt′dt } . Using again the Fourier representation of the deltas and considering N large, the auxiliary variables introduced with the transform concentrate to their saddle point according to Laplace approximation. Furthermore it is easy to show Castellani and Cavagna (2005) that Cx˜v(t,t′) = C˜vx(t′,t) and Cx˜v(t,t′) = Rx|v(t,t′), with Rx|v. Under this considerations, we rewrite the average generating functional 1 = Z= ∫ D[xxx,˜xxx,vvv,˜vvv] N∏ i=1 exp { i ∫ ˜vi(t) [ m˙vi(t) + vi(t) + µ (∑ jx2 j(t) N −1 ) xi(t) ] dt } × (66) × N∏ i=1 exp { i ∫ ˜xi(t) [ ˙xi(t) −vi(t)] dt } × (67) ×exp   − ∫ ∑ j [1 2Q′[Cx(t,t′)]i˜vj(t) i˜vj(t′) −Q′′[Cx(t,t′)]Rx|v(t,t′) i˜vj(t′) xj(t) ] dt′dt   . (68) Finally we can take a Hubbard-Stratonovich transform on the ﬁrst term of the second line and identify a stochastic process Ξ(t) with zero mean at all times and covariance E[Ξ(t)Ξ(t′)] = Q′[Cx(t,t′)]. The resulting generating functional now represent the dynamics in xxxand vvvwhere the cross-element interactions are replaced by the stochastic process. The resulting effective equations are m˙vvv(t) = −vvv(t) + ∫ t 0 dt′′Rx|v(t,t′′)Q′[Cx(t,t′′)]xxx(t′′) + ΞΞΞ(t) −µ[Cx(t,t) −1]xxx(t), (69) ˙xxx(t) = vvv(t). (70) Finally we introduce the order parameters Eqs. (61-65) and compute their equation explicitly by substituting the equations of the effective dynamics. In order to do that, we use Girsanov theorem and evaluate the following expected values ⟨v(t)Ξ(t′)⟩= ∫ t′ 0 dt′′Rv(t,t′′)Q′[Cx(t,t′′)], (71) ⟨x(t)Ξ(t′)⟩= ∫ t′ 0 dt′′Rx|v(t,t′′)Q′[Cx(t,t′′)]. (72) The dynamical equations are ∂tCx(t,t′) = Cxv(t′,t) ; (73) m∂tCv(t,t′) = −Cv(t,t′) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]Cxv(t′′,t′)+ + ∫ t′ 0 Q′[Cx(t,t′′)]Rv(t′,t′′) −µCx(t,t′) (Cx(t,t′) −1) ; (74) ∂tCxv(t,t′) = Cv(t,t′) ; (75) m∂t′ Cxv(t,t′) = −Cxv(t,t′) + ∫ t′ 0 dt′′Rx|v(t′,t′′)Q′′[Cx(t′,t′′)]Cx(t,t′′)+ + ∫ t 0 Q′[Cx(t′,t′′)]Rx|v(t,t′′) −µCxv(t,t′) (Cx(t,t′) −1) ; (76) m∂tRv(t,t′) = δ(t−t′) −Rv(t,t′) + ∫ t t′ dt′′Q′′[C(t,t′′)]Rx|v(t,t′′)Rx|v(t′′,t′)+ −µRx|v(t,t′) (Cx(t,t′) −1) ; (77) 20∂tRx|v(t,t′) = Rv(t,t′). (78) and µ(t) = Cxv(t,t). The initial conditions are : Cx(t,t) = 1 ; (79) Cv(t= 0,t = 0) = 0 ; (80) Cxv(t= 0,t = 0) = 0 ; (81) Rv(t+,t+) = 1/m; (82) Rx|v(t,t) = 0 . (83) Where Eq. equation 79 comes from the spherical constraint; Eqs. (80,81) come from the initialization with no kinetic energy vvv(0) = 000; Eqs. equation 82 and equation 83 come from Eqs. equation 69 and equation 70 (respectively) after deriving by Ξ(t′), integrating on tin [t−h; t+ h] (with h→0) and taking t′→t. The equations shown in the main text are the discrete equivalent of the ones just obtained. D Hard spherical constraint It is also possible to consider a hard spherical constraint, which is the situation typically considered in the physics literature Crisanti and Sommers (1992); Cugliandolo and Kurchan (1993). A massive dynamics was already considered in Cugliandolo et al. (2017) but their derivation was in the under- damped regime where the total energy is conserved. Using that approach the conservation of the energy was key. Unfortunately the energy is not conserved in general, and in particular in the case of optimization where we aim to go down in energy in order to ﬁnd a minimum. For reference sake we consider massive gradient ﬂow, the same considerations apply straightforwardly to Nesterov acceleration. Let us write the dynamics in this case splitting the system in two ODEs m˙vvv(t) = −vvv(t) −∇L[xxx(t)], (84) ˙xxx(t) = vvv(t) −µ(t)xxx(t). (85) The last term in the second line constraints the dynamics to move in the sphere by removing the terms of the velocity that move orthogonally to the sphere. Therefore the term µ(t) is given by the projection of the velocity in the direction that is tangent to the sphere ∑ jxj(t)vj(t)/N. We can then follow the usual techniques, e.g. section C, and obtain ∂tCx(t,t′) = −µ(t)Cx(t,t′) + Cxv(t′,t) , (86) m∂tCv(t,t′) = −Cv(t,t′) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]Cxv(t′′,t′) + ∫ t′ 0 Q′[Cx(t,t′′)]Rv(t′,t′′) , (87) ∂tCxv(t,t′) = −µ(t)Cxv(t,t′) + Cv(t,t′) , (88) m∂t′ Cxv(t,t′) = −Cxv(t,t′) + ∫ t′ 0 dt′′Rx|v(t′,t′′)Q′′[Cx(t′,t′′)]Cx(t,t′′) + ∫ t 0 Q′[Cx(t′,t′′)]Rx|v(t,t′′) , (89) m∂tRv(t,t′) = δ(t−t′) −Rv(t,t′) + ∫ t t′ dt′′Q′′[C(t,t′′)]Rx|v(t,t′′)Rx|v(t′′,t′) , (90) ∂tRx|v(t,t′) = −µ(t)Rx|v(t,t′) + Rv(t,t′); (91) with µ(t) = Cxv(t,t) and initial conditions Cx(t,t) = 1, Cv(0,0) = 0, Cxv(0,0) = 0, Rv(t+,t) = 1 m, Rx|v(t,t) = 0. 21
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Out-of-equilibrium dynamical mean-field equations for the perceptron model",
        "A pid controller approach for stochastic optimization of deep networks",
        "Random matrices and complexity of spin glasses",
        "Temperature evolution and bifurcations of metastable states in mean-field spin glasses, with connections with structural glasses",
        "The convergence of a class of double-rank minimization algorithms: 2. the new algorithm",
        "Gating creates slow modes and controls phase-space complexity in grus and lstms",
        "Spin-glass theory for pedestrians",
        "Méthode générale pour la résolution des systemes d’équations simultanées",
        "Spherical 2+ p spin-glass model: An analytically solvable model with a glass-to-glass transition",
        "The sphericalp-spin interaction spin glass model: the statics",
        "The sphericalp-spin interaction spin-glass model",
        "Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model",
        "Non equilibrium dynamics of isolated disordered systems: the classical hamiltonian p-spin model",
        "A method for solving the convex programming problem with convergence rate o (1/kˆ 2)",
        "A numerical method for solving linear least squares problems. Communications on Pure and Applied Mathematics",
        "From averaging to acceleration, there is only a step-size",
        "Stochastic heavy ball",
        "Dynamics for spherical spin glasses: disorder dependent initial conditions",
        "A new approach to variable metric algorithms",
        "Performance of simulated annealing in p-spin glasses",
        "Analysis and design of optimization algorithms via integral quadratic constraints",
        "Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods",
        "Quasi-hyperbolic momentum and adam for deep learning",
        "An algorithm for least-squares estimation of nonlinear parameters",
        "A method for the solution of certain non-linear problems in least squares",
        "Statistical dynamics of classical systems",
        "A method for solving the convex programming problem with convergence rate O (1/k2)",
        "Non equilibrium dynamics of isolated disordered systems: the classical hamiltonian p-spin model",
        "Understanding the role of momentum in stochastic gradient methods",
        "A statistical model for tensor pca",
        "Passed & spurious: Descent algorithms and local minima in spiked matrix-tensor models",
        "Complex dynamics in simple neural networks: Understanding gradient flow in phase retrieval",
        "Marvels and pitfalls of the langevin algorithm in noisy high-dimensional inference",
        "Universal average-case optimality of polyak momentum",
        "Gradient descent dynamics in the mixed p-spin spherical model: finite-size simulations and comparison with mean-field integration",
        "On the stochastic dynamics of disordered spin models",
        "On the importance of initialization and momentum in deep learning",
        "Quickly ﬁnding a benign region via heavy ball momentum in non-convex optimization",
        "A differential equation for modeling nesterov’s accelerated gradient method: Theory and insights",
        "Uniﬁed convergence analysis of stochastic momentum methods for convex and non-convex optimization",
        "Who is afraid of big bad minima? analysis of gradient-flow in spiked matrix-tensor models"
    ]
}
