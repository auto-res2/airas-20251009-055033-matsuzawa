
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
Predicting Training Time Without Training Luca Zancato1,2 Alessandro Achille2 Avinash Ravichandran2 Rahul Bhotika2 Stefano Soatto2 University of Padova1 Amazon Web Services2 luca.zancato@phd.unipd.it {aachille,ravinash,bhotikar,soattos}@amazon.com Abstract We tackle the problem of predicting the number of optimization steps that a pre- trained deep network needs to converge to a given value of the loss function. To do so, we leverage the fact that the training dynamics of a deep network during ﬁne-tuning are well approximated by those of a linearized model. This allows us to approximate the training loss and accuracy at any point during training by solving a low-dimensional Stochastic Differential Equation (SDE) in function space. Using this result, we are able to predict the time it takes for Stochastic Gradient Descent (SGD) to ﬁne-tune a model to a given loss without having to perform any training. In our experiments, we are able to predict training time of a ResNet within a 20% error margin on a variety of datasets and hyper-parameters, at a 30 to 45-fold reduction in cost compared to actual training. We also discuss how to further reduce the computational and memory cost of our method, and in particular we show that by exploiting the spectral properties of the gradients’ matrix it is possible predict training time on a large dataset while processing only a subset of the samples. 1 Introduction Say you are a researcher with many more ideas than available time and compute resources to test them. You are pondering to launch thousands of experiments but, as the deadline approaches, you wonder whether they will ﬁnish in time, and before your computational budget is exhausted. Could you predict the time it takes for a network to converge, before even starting to train it? We look to efﬁciently estimate the number of training steps a Deep Neural Network (DNN) needs to converge to a given value of the loss function, without actually having to train the network. This problem has received little attention thus far, possibly due to the fact that the initial training dynamics of a randomly initialized DNN are highly non-trivial to characterize and analyze. However, in most practical applications, it is common to not start from scratch, but from a pre-trained model. This may simplify the analysis, since the ﬁnal solution obtained by ﬁne-tuning is typically not too far from the initial solution obtained after pre-training. In fact, it is known that the dynamics of overparametrized DNNs [9, 31, 2] during ﬁne-tuning tends to be more predictable and close to convex [24]. We therefore characterize the training dynamics of a pre-trained network and provide a computation- ally efﬁcient procedure to estimate the expected proﬁle of the loss curve over time. In particular, we provide qualitative interpretation and quantitative prediction of the convergence speed of a DNN as a function of the network pre-training, the target task, and the optimization hyper-parameters. We use a linearized version of the DNN model around pre-trained weights to study its actual dynamics. In [20] a similar technique is used to describe the learning trajectories of randomly initialized wide neural networks. Such an approach is inspired by the Neural Tangent Kernel (NTK) for inﬁnitely wide networks [14]. While we note that NTK theory may not correctly predict the dynamics of real (ﬁnite size) randomly initialized networks [12], we show that our linearized approach can be extended to ﬁne-tuning of real networks in a similar vein to [24]. In order to predict ﬁne-tuning Training Time (TT) without training we introduce a Stochastic Differential Equation (SDE) (similar to [ 13]) to Preprint. Under review. arXiv:2008.12478v1  [cs.LG]  28 Aug 2020approximate the behavior of SGD: we do so for a linearized DNN and in function space rather than in weight space. That is, rather than trying to predict the evolution of the weights of the network (a D-dimensional vector), we aim to predict the evolution of the outputs of the network on the training set (a N ×C-dimensional vector, where N is the size of the dataset and Cthe number of network’s outputs). This drastically reduces the dimensionality of the problem for over-parametrized networks (that is, when NC ≪D). A possible limiting factor of our approach is that the memory requirement to predict the dynamics scales as O(DC2N2). This would rapidly become infeasible for datasets of moderate size and for real architectures ( D is in the order of millions). To mitigate this, we show that we can use random projections to restrict to a much smaller D0-dimensional subspace with only minimal loss in prediction accuracy. We also show how to estimate Training Time using a small subset of N0 samples, which reduces the total complexity to O(D0 C2N2 0 ). We do this by exploiting the spectral properties of the Gram matrix of the gradients. Under mild assumptions the same tools can be used to estimate Training Time on a larger dataset without actually seeing the data. To summarize, our main contributions are: (i) We present both a qualitative and quantitative analysis of the ﬁne-tuning Training Time as a function of the Gram-Matrix Θ of the gradients at initialization (empirical NTK matrix). (ii) We show how to reduce the cost of estimating the matrix Θ using random projections of the gradients, which makes the method efﬁcient for common architectures and large datasets. (iii) We introduce a method to estimate how much longer a network will need to train if we increase the size of the dataset without actually having to see the data (under the hypothesis that new data is sampled from the same distribution). (iv) We test the accuracy of our predictions on off-the-shelf state-of-the-art models trained on real datasets. We are able to predict the correct training time within a 20% error with 95% conﬁdence over several different datasets and hyperparameters at only a small fraction of the time it would require to actually run the training (30-45x faster in our experiments). 2 Related Work Predicting the training time of a state-of-the-art architecture on large scale datasets is a relatively understudied topic. In this direction, Justus et al. [ 15] try to estimate the wall-clock time required for a forward and backward pass on given hardware. We focus instead on a complementary aspect: estimating the number of ﬁne-tuning steps necessary for the loss to converge below a given threshold. Once this has been estimated we can combine it with the average time for the forward and backward pass to get a ﬁnal estimate of the wall clock time to ﬁne-tune a DNN model without training it. Hence, we are interested in predicting the learning dynamics of a pre-trained DNN trained with either Gradient Descent (GD) or Stochastic Gradient Descent (SGD). While different results are known to describe training dynamics under a variety of assumptions (e.g. [16, 28, 26, 6]), in the following we are mainly interested on recent developments which describe the optimization dynamics of a DNN using a linearization approach. Several works [14, 19, 10] suggest that in the over-parametrized regime wide DNNs behave similar to linear models, and in particular they are fully characterized by the Gram-Matrix of the gradients, also known as empirical Neural Tangent Kernel (NTK). Under these assumptions, [ 14, 3] derive a simple connection between training time and spectral decomposition of the NTK matrix. However, their results are limited to Gradient Descend dynamics and to simple architectures which are not directly applicable to real scenarios. In particular, their arguments hinge on the assumption of using a randomly initialized very wide two-layer or inﬁnitely wide neural network [3, 11, 22]. We take this direction a step further, providing a uniﬁed framework which allows us to describe training time for both SGD and GD on common architectures. Again, we rely on a linear approximation of the model, but while the practical validity of such linear approximation for randomly initialized state-of-the-art architectures (such as ResNets) is still discussed [12], we follow Mu et al. [24] and argue that the ﬁne-tuning dynamics of over-parametrized DNNs can be closely described by a linearization. We expect such an approximation to hold true since the network does not move much in parameters space during ﬁne-tuning and over-parametrization leads to smooth and regular loss function around the pre-trained weights [9, 31, 2, 21]. Under this 20 20 40 60 80 100 120 140 160 Real Training Time 0 25 50 75 100 125 150 175 Predicted Training Time Perfect prediction +13% error -13% error CIFAR10    slope 0.93 CIFAR100   slope 0.94 CUB200     slope 0.97 Aircrafts  slope 1.09 Mit67      slope 0.89 Surfaces   slope 0.90 Cars       slope 1.00 (a) Training with Gradient Descent. 0 20 40 60 80 100 120 140 Real Training Time 0 25 50 75 100 125 150 175 Predicted Training Time Perfect prediction +20% error -20% error CIFAR10    slope 0.87 CIFAR100   slope 0.78 CUB200     slope 0.91 Aircrafts  slope 0.78 Mit67      slope 0.87 Surfaces   slope 0.78 Cars       slope 0.90 (b) Training with SGD. Figure 1: Training time prediction (# iterations) for several ﬁne-tuning tasks. Scatter plots of the predicted time vs the actual training time when ﬁne-tuning a ResNet-18 pre-trained on ImageNet on several tasks. Each task is obtained by randomly sampling a subset of ﬁve classes with 150 images (when possible) each from one popular dataset with different hyperparameters (batch size, learning rate). The closer the scatter plots to the bisector the better the TT estimate. Our prediction is (a) within 13% of the real training time 95% of the times when using GD and (b) within 20% of the real training time when using SGD. premise, we tackle both GD and SGD in an uniﬁed framework and build on [13] to model training of a linear model using a Stochastic Differential Equation in function space. We show that, as also hypothesized by [24], linearization can provide an accurate approximation of ﬁne-tuning dynamics and therefore can be used for training time prediction. 3 Predicting training time In this section we look at how to efﬁciently approximate the training time of a DNN without actual training. By Training Time (TT) we mean the number of optimization steps – of either Gradient Descent (GD) or Stochastic Gradient Descent (SGD) – needed to bring the loss on the training set below a certain threshold. We start by introducing our main tool. Let fw(x) denote the output of the network, where wdenotes the weights of the network and x ∈Rd denotes its input (e.g., an image). Let w0 be the weight conﬁguration after pre-training. We assume that when ﬁne-tuning a pre-trained network the solution remains close to pre-trained weights w0 [24, 9, 31, 2]. Under this assumption – which we discuss further in Section 6 – we can faithfully approximate the network with its Taylor expansion around w0 [20]. Let wt be the ﬁne-tuned weights at time t. Using big-O notation and ft ≡fwt, we have: ft(x) = f0(x) + ∇wf0(x)|w=w0 (wt −w0) + O(∥wt −w0∥2) We now want to use this approximation to characterize the training dynamics of the network during ﬁne-tuning to estimate TT. In such theoretical analyses [14, 20, 3] it is common to assume that the network is trained with Gradient Descent (GD) rather than Stochastic Gradient Descent, and in the limit of a small learning rate. In this limit, the dynamics are approximated by the gradient ﬂow differential equation ˙wt = −η∇wtL[14, 20] where ηdenotes the learning rate and L(w) denotes the loss function L(w) = ∑ N i=1 ℓ(yi,fw(xi))., where ℓis the per-sample loss function (e.g. Cross- Entropy). This approach however has two main drawbacks. First, it does not properly approximate Stochastic Gradient Descent, as it ignores the effect of the gradient noise on the dynamics, which affects both training time and generalization. Second, the differential equation involves the weights of the model, which live in a very high dimensional space thus making ﬁnding numerical solutions to the equation not tractable. To address both problems, building on top of [13] in the Supplementary we prove the following result. Proposition 1 In the limit of small learning rate η, the output on the training set of a linearized network flin t trained with SGD evolves according to the following Stochastic Differential Equation (SDE): dflin t (X) = −ηΘ∇flin t (X)Ltdt    deterministic part + η√ |B| ∇wflin 0 (X)Σ 1 2 (flin t (X))dn    stochastic part , (1) 30 20 40 60 80 100 Iterations 0 20 40 60 80 Accuracy ODE vs SDE approximation SGD SDE ODE 0 500 1000 1500 2000 2500 Iterations 20 40 60 80 100 Train Err ELR effects ELR:0.001 ELR:0.005 ELR:0.010 ELR:0.050 ELR:0.100 Figure 2: (Left) ODE vs. SDE. ODE approximation may not be well suited to describe the actual non-linear SGD dynamics (high learning rates regime). (Right) Fine-tuning with the same ELR have similar curves . We ﬁne-tune an ImageNet pre-trained network on MIT-67 with different combinations of learning rates and momentum coefﬁcients. We note that as long as the effective learning rate is the same, the loss curves are also similar. where Xis the set of training images,|B|the batch-size and dnis a D-dimensional Brownian motion. We have deﬁned the Gram gradients matrix Θ [14, 27] (i.e., the empirical Neural Tangent Kernel matrix) and the covariance matrix Σ of the gradients as follows: Θ := ∇wf0(X)∇wf0(X)T, (2) Σ(flin t (X)) := E [ (gi∇flin t (xi)L) ⊗(gi∇flin t (xi)L) ] −E [ gi∇flin t (xi)L ] ⊗E [ gi∇flin t (xi)L ] . (3) where gi ≡∇wf0(xi). Note both Θ and Σ only require gradients w.r.t. parameters computed at initialization. The ﬁrst term of eq. (1) is an ordinary differential equation (ODE) describing the deterministic part of the optimization, while the second stochastic term accounts for the noise. In Figure 2 (left) we show the qualitative different behaviour of the solution to the deterministic part of eq. (1) and the complete SDE eq. (1). While several related results are known in the literature for the dynamics of the network in weight space [7], note that eq. (1) completely characterizes the training dynamics of the linearized model by looking at the evolution of the output flin t (X) of the model on the training samples – a N×C-dimensional vector – rather than looking at the evolution of the weightswt – a D-dimensional vector. When the number of data points is much smaller than the number of weights (which are in the order of millions for ResNets), this can result in a drastic dimensionality reduction, which allows easy estimation of the solution to eq. (1). Solving eq. (1) still comes with some challenges, particularly in computing Θ efﬁciently on large datasets and architectures. We tackle these in Section 4. Before that, we take a look at how different hyper-parameters and different pre-trainings affect the training time of a DNN on a given task. 3.1 Effect of hyper-parameters on training time Effective learning rate. From Proposition 1 we can gauge how hyper-parameters will affect the optimization process of the linearized model and, by proxy, of the original model it approximates. One thing that should be noted is that Proposition 1 assumes the network is trained with momentum m= 0. Using a non-zero momentum leads to a second order differential equation in weight space, that is not captured by Proposition 1. We can however, introduce heuristics to handle the effect of momentum: Smith et al. [28] note that the momentum acts on the stochastic part shrinking it by a factor √ 1/(1 −m). Meanwhile, under the assumptions we used in Proposition 1 (small learning rate), we can show (see Supplementary Material) the main effect of momentum on the deterministic part is to re-scale the learning rates by a factor1/(1 −m). Given these results, we deﬁne the effective learning rate (ELR) ˆη= η/(1 −m) and claim that, in ﬁrst approximation, we can simulate the effect of momentum by using ˆηinstead of ηin eq. (1). In particular, models with different learning rates and momentum coefﬁcients will have similar (up to noise) dynamics (and hence training time) as long as the effective learning rate ˆηremains the same. In Figure 2 we show empirically that indeed same effective learning rate implies similar loss curve. That similar effective learning rate gives similar test performance has also been observed in [21, 28]. Batch size. The batch size appears only in the stochastic part of the equation, its main effect is to decrease the scale of the SDE noise term. In particular, when the batch size goes to inﬁnity |B|→∞ 4(a) Features and Gradients clustering.  (b) Trajectory clustering. Figure 3: Are gradients good descriptors to cluster data by semantics and training time? (a) Features vs Gradients clustering. (Right) t-SNE plot of the ﬁrst ﬁve principal components of the gradients of each sample in a subset of CIFAR-10 with 3 classes. Colors correspond to the sample class. We observe that the ﬁrst 5 principal components are enough to separate the data by class. By Proposition 2 this implies faster training time. (Left) In the same setting as before, t-SNE plot of the features using the ﬁrst 5 components of PCA. We observe that gradients separate the classes better than the features. (b) t-SNE on predicted trajectories To see if gradients are good descriptors of both semantics and training time we use gradients to predict linearized trajectories: we cluster the trajectories using t-SNE and we color each point by (left) class and (right) training time. We observe that: clusters split trajectories according both to labels (left) and training time (right). Interestingly inside each class there are clusters of points that may converge at different speed. we recover the deterministic gradient ﬂow also studied by [20]. Note that we need the batch size |B| to go to inﬁnity, rather than being as large as the dataset since we assumed random batch sampling with replacement. If we assume extraction without replacement the stochasticity is annihilated as soon as |B|= N (see [7] for a more in depth discussion). 3.2 Effect of pre-training on training time We now use the SDE in eq. (1) to analyze how the combination of different pre-trainings of the model – that is, different w0’s – and different tasks affect the training time. In particular, we show that a necessary condition for fast convergence is that the gradients after pre-training cluster well with respect to the labels. We conduct this analysis for a binary classiﬁcation task with yi = ±1, but the extension is straightforward for multi-class classiﬁcation, under the simplifying assumptions that we are operating in the limit of large batch size (GD) so that only the deterministic part of eq. (1) remains. Under these assumptions, eq. (1) can be solved analitically and the loss of the linearized model at time tcan be written in closed form as (see Supplementary Material): Lt = (Y− f0(X))Te−2ηΘt(Y− f0(X)) (4) The following characterization can easily be obtained using an eigen-decomposition of the matrix Θ. Proposition 2 Let S = ∇wfw(X)T∇wfw(X) be the second moment matrix of the gradients and let S = UΣUT be the uncentered PCA of the gradients, where Σ = diag(λ1,...,λ n,0,..., 0) is a D×Ddiagonal matrix, n ≤min(N,D) is the rank of S and λi are the eigenvalues sorted in descending order. Then we have Lt = D∑ k=1 e−2ηλkt(δy ·vk)2, (5) where λkvk = (gi ·uk)N i=1 is the N-dimensional vector containing the value of the k-th principal component of gradients gi and δy := Y− f0(X). Training speed and gradient clustering.We can give the following intuitive interpretation: consider the gradient vector gi as a representation of the sample xi. If the ﬁrst principal components of gi are sufﬁcient to separate the classes (i.e., cluster them), then convergence is faster (see Figure 3). Conversely, if we need to use the higher components (associated to small λk) to separate the data, then convergence will be exponentially slower. Arora et al. [3] also use the eigen-decomposition of Θ 5100 101 Iterations Train Loss Trajectory approximations Full kernel Reduced kernel Actual dynamics 0 50 100 150 200 250 300 350 400 N 0.000 0.002 0.004 0.006 0.008 0.010 ||Θ − ˆΘ ||F ||Θ ||F 0 10 20 30 40 50 60 70 80 Computational Time (s) Approximated vs True kernel True Approximated 100 101 102 103 Eigenvalues index 10 1 100 101 102 103 Eigenvalues Power law for different dataset sizes N 30 N 60 N 150 N 300 N 600 N 1500 N 2100 Figure 4: (Left) Actual ﬁne-tuning of a DNN with GD compared to the numerical solution of eq. (1) and the solution using an approximated Θ. The approximated Θ can faithfully describe ﬁne-tuning dynamics while being twice as fast to compute and 100 times smaller to be stored. (Center) Relative difference in Frobenius norm of the real and approximated Θ as the dataset size varies (red), and their computational time (blue). Right: Eigen-spectrum of Θ computed on subsets of MIT-67 of increasing size. Note the convergence to a common power law (i.e., a line in log-log scale). to explain the slower convergence observed for a randomly initialized two-layer network trained with random labels. This is straightforward since the projection of a random vector will be uniform on all eigenvectors, rather than concentrated on the ﬁrst few, leading to slower convergence. However, we note that the exponential dynamics predicted by [3] do not hold for more general networks trained from scratch [30] (see Section 6). In particular, eq. (5) mandates that the loss curve is always convex (it is sum of convex functions), which may not be the case for deep networks trained from scratch. 4 Efﬁcient numerical estimation of training time In Proposition 2 we have shown a closed form solution to the SDE in eq. (1) in the limit of large batch size, and for the MSE loss. Unfortunately, in general eq. (1) does not have a closed form expression when using the cross-entropy loss [20]. A numerical solution is however possible, enabled by the fact that we describe the network training in function space, which is much smaller than weight space for over-parametrized models. The main computational cost is to create the matrix Θ in eq. (1) – which has cost O(DC2N2) – and to compute the noise in the stochastic term. Here we show how to reduce the cost of Θ to O(D0C2N2) for D0 ≪Dusing a random projection approximation. Then, we propose a fast approximation for the stochastic part. Finally, we describe how to reduce the cost in N by using only a subset N′<N of samples to predict training time. Random projection. To keep the notation uncluttered, here we assume w.l.o.g. C = 1. In this case the matrix Θ contains N2 pairwise dot-products of the gradients (a D-dimensional vector) for each of the N training samples (see eq. 2). Since Dcan be very large (in the order of millions) storing and multiplying all gradients can be expensive as N grows. Hence, we look at a dimensionality reduction technique. The optimal dimensionality reduction that preserves the dot-product is obtained by projecting on the ﬁrst principal components of SVD, which however are themselves expensive to obtain. A simpler technique is to project the gradients on a set ofD′standard Gaussian random vectors: it is known that such random projections preserve (in expectation) pairwise product [5, 1] between vectors, and hence allow us to reconstruct the Gram matrix while storing only D′-dimensional vector, with D′ ≪D. We further increase computational efﬁciency using multinomial random vectors {-1,0,+1} as proposed in [1] which further reduce the computational cost by avoiding ﬂoating point multiplications. In Figure 4 we show that the entries of Θ and its spectrum are well approximated using this method, while the computational time becomes much smaller. Computing the noise. The noise covariance matrix Σ is a D×D-matrix that changes over time. Both computing it at each step and storing it is prohibitive. Estimating Σ correctly is important to describe the dynamics of SGD [8], however we claim that a simple approximation may sufﬁce to describe the simpler dynamic in function space. We approximate ∇wflin 0 (X)Σ1/2 approximating Σ with its diagonal (so that the we only need to store a D-dimensional vector). Rather than computing the whole Σ at each step, we estimate the value of the diagonal at the beginning of the training. Then, by exploiting eq. (3), we see that the only change to Σ is due to ∇flin t L, whose norm decreases over time. Therefore we use the easy-to-compute ∇flin t Lto re-scale our initial estimate of Σ. Larger datasets. In the MSE case from eq. (4), knowing the eigenvalues λk and the corresponding residual projections pk = (δy ·vk)2 we can predict in closed form the whole training curve. Is it possible to predict λk and pk using only a subset of the dataset? It is known [27] that the eigenvalues 6of the Gram matrix of Gaussian data follow a power-law distribution of the form λk = ck−s. Moreover, by standard concentration argument, one can prove that the eigenvalues should converge to a given limit as the number of datapoints increases. We verify that a similar power-law and convergence result also holds for real data (see Figure 4). Exploiting this result, we can estimate c and sfrom the spectrum computed on a subset of the data, and then predict the remaining eigenvalues. A similar argument holds for the projections pk, which also follow a power-law (albeit with slower convergence). We describe the complete estimation in the Supplementary Material. 5 Results We now empirically validate the accuracy of proposition 1 in approximating the loss curve of an actual deep neural network ﬁne-tuned on a large scale dataset. We also validate the goodness of the numerical approximations described in Section 4. Due to the lack of a standard and well established benchmark to test Training Time estimation algorithms we developed one with the main goal to closely resemble ﬁne-tuning common practice for a wide spectrum of different tasks. Experimental setup. We deﬁne training time as the ﬁrst time the (smoothed) loss is below a given threshold. However, since different datasets converge at different speeds, the same threshold can be too high (it is hit immediately) for some datasets, and too low for others (it may take hundreds of epochs to be reached). To solve this, and have cleaner readings, we deﬁne a ‘normalized’ threshold as follows: we ﬁx the total number of ﬁne-tuning steps T, and measure instead the ﬁrst time the loss is within ϵfrom the ﬁnal value at time T. This measure takes into account the ‘asymptotic’ loss reached by the DNN within the computational budget (which may not be close to zero if the budget is low), and naturally adapts the threshold to the difﬁculty of the dataset. We compute both the real loss curve and the predicted training curve using Proposition 1 and compare the ϵ-training-time measured on both. We report the absolute prediction error, that is |tpredicted −treal|. For all the experiments we extract 5 random classes from each dataset (Table 1) and sample 150 images (or the maximum available for the speciﬁc dataset). Then we ﬁne-tuned ResNet18/34 using either GD or SGD. Accuracy of the prediction. In Figure 1 we show TT estimates errors (for different ϵ∈{1,..., 40}) under a plethora of different conditions ranging from different learning rates, batch sizes, datasets and optimization methods. For all the experiments we choose a multi-class classiﬁcation problem with Cross Entropy (CE) Loss unless speciﬁed otherwise, and ﬁxed computational budget of T = 150 steps both for GD and SGD. We note that our estimates are consistently within respectively a 13% and 20% relative error around the actual training time 95% of the times. In Table 1 we describe the sensitivity of our estimates to different thresholds ϵ both when our assumptions do and do not hold (high and low learning rates regimes). Note that a larger thresholdϵis hit during the initial convergence phase of the network, when a small number of iterations corresponds a large change in the loss. Correspondingly, the hitting time can be measured more accurately and our errors are lower. A smaller ϵdepends more on correct prediction of the slower asymptotic phase, for which exact hitting time is more difﬁcult to estimate. TT error (# of steps) ϵ = 1% ϵ = 10% ϵ= 40% Lr low high low high low high Cars [17] 9 18 7 8 1 0 Surfaces [4] 6 13 6 7 6 3 Mit67 [25] 8 10 6 8 3 1 Aircrafts [23] 5 21 5 4 9 7 CUB200 [29] 6 6 5 8 1 1 CIFAR100 [18] 10 15 6 7 2 3 CIFAR10 [18] 9 14 8 9 3 3 Table 1: Training Time estimation error for CE loss using GD for T = 150 epochs at different thresholds ϵ. We compare TT estimates when ODE assumptions do and do not hold: high {0.005} and small LR {0.001, 0.0001}. (S) Figure 5: Wall clock time (in seconds) to com- pute TT estimate vs ﬁne-tuning running time. We run the methods described in Section 4 both on GPU and CPU. Training is done on GPU. Wall-clock run-time. In Figure 5 we show the wall-clock runtime of our training time prediction method compared to the time to actually train the network for T steps. Our method is 30-40 times faster. Moreover, we note that it can be run completely on CPU without a drastic drop in performance. This allows to cheaply estimate TT and allocate/manage resources even without access to a GPU. 70.0001 0.001 0.005 LR 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 TT error Surfaces CIFAR100 Aircrafts Mit67 CUB200 CIFAR10 Cars 50 100 Batch Size 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 TT error Surfaces CIFAR100 Aircrafts Mit67 CUB200 CIFAR10 50 250 625 Dataset Size 0 10 20 30 40 50 TT error Surfaces CIFAR100 Aircrafts Mit67 CUB200 CIFAR10 Cars Figure 6: Average and 95% conﬁdence intervals of TT estimate error for: Left: GD using different learning rates. Center: SGD using different batch sizes. Right: SGD using different dataset sizes. The average is taken w.r.t. random classes with different number of samples: {10, 50, 125} Effect of dataset distance. We note that the average error for Surfaces (Figure 6) is uniformily higher than the other datasets. This may be due to the texture classiﬁcation task being quite different from ImageNet, on which the network is pretrained. In this case we can expect that the linearization assumption is partially violated since the features must adjust more during ﬁne-tuning. Effect of hyper-parameters on prediction accuracy. We derived Proposition 1 under several assumptions, importantly: small learning rate and wt close to w0. In Figure 6 (left) we show that indeed increasing the learning rate decreases the accuracy of our prediction, albeit the accuracy remains good even at larger learning rates. Fine-tuning on larger dataset makes the weights move farther away from the initialization w0. In Figure 6 (right) we show that this slightly increases the prediction error. Finally, we observe in Figure 6 (center) that using a smaller batch size, which makes the stochastic part of Proposition 1 larger also slightly increases the error. This can be ascribed to the approximation of the noise term (Section 4). On the other hand, in Figure 2 (right) we see that the effect of momentum on a ﬁne-tuned network is very well captured by the effective learning rate (Section 3.1), as long as the learning rate is reasonably small, which is the case for ﬁne-tuning. Hence the SDE approximation is robust to different values of the momentum. In general, we note that even when our assumptions are not fully met training time can still be approximated with only a slightly higher error. This suggest that point-wise proximity of the training trajectory of linear and real models is not necessary as long as their behavior (decay-rate) is similar (see also Supplementary Material). 6 Discussion and conclusions We have shown that we can predict with a 13-20% accuracy the time that it will take for a pre-trained network to reach a given loss, in only a small fraction of the time that it would require to actually train the model. We do this by studying the training dynamics of a linearized version of the model – using the SDE in eq. (1) – which, being in the smaller function space compared to parameters space, can be solved numerically. We have also studied the dependency of training time from pre-training and hyper-parameters (Section 3.1), and how to make the computation feasible for larger datasets and architectures (Section 4). While we do not necessarily expect a linear approximation around a random initialization to hold during training of a real (non wide) network, we exploit the fact that when using a pre-trained network the weights are more likely to remain close to initialization [ 24], improving the quality of the approximation. However, in the Supplementary Material we show that even when using a pre-trained network, the trajectories of the weights of linearized model and of the real model can differ substantially. On the other hand, we also show that the linearized model correctly predicts the outputs (not the weights) of the real model throughout the training, which is enough to compute the loss. We hypothesise that this is the reason why eq. (1) can accurately predict the training time using a linear approximation. The procedure described so far can be considered as an open loop procedure meaning that, since we are estimating training time before any ﬁne-tuning step is performed, we are not gaining any feedback from the actual training. How to perform training time prediction during the actual training, and use training feedback (e.g., gradients updates) to improve the prediction in real time, is an interesting future direction of research. 8References [1] Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary coins. J. Comput. Syst. Sci., 66(4):671–687, June 2003. [2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. arXiv preprint arXiv:1811.03962, 2018. [3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op- timization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322–332, 2019. [4] Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala. Material recognition in the wild with the materials in context database. Computer Vision and Pattern Recognition (CVPR), 2015. [5] Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245–250, 2001. [6] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-parameterized networks that provably generalize on linearly separable data. CoRR, abs/1710.10174, 2017. [7] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. CoRR, abs/1710.11029, 2017. [8] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer- ence, converges to limit cycles for deep networks. In International Conference on Learning Representations, 2018. [9] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1675–1685, Long Beach, California, USA, 09–15 Jun 2019. PMLR. [10] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018. [11] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018. [12] Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom Goldstein. Truth or backpropaganda? an empirical investigation of deep learning theory, 2019. [13] Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-ﬁeld behaviour of neural tangent kernel for deep neural networks. arXiv preprint arXiv:1905.13654, 2019. [14] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571–8580, 2018. [15] Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. Predicting the computational cost of deep learning models. CoRR, abs/1811.11880, 2018. [16] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016. [17] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. [18] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Computer Science Department, University of Toronto, 2009. [19] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017. 9[20] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent, 2019. [21] Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for ﬁne-tuning, 2020. [22] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. CoRR, abs/1705.09886, 2017. [23] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft. Technical report, 2013. [24] Fangzhou Mu, Yingyu Liang, and Yin Li. Gradients as features for deep representation learning. In International Conference on Learning Representations, 2020. [25] Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In CVPR, pages 413–420. IEEE Computer Society, 2009. [26] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. [27] J. Shawe-Taylor, C. K. I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory, 51(7):2510–2522, 2005. [28] Samuel L. Smith and Quoc V . Le. A bayesian perspective on generalization and stochastic gradient descent. ArXiv, abs/1710.06451, 2018. [29] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010. [30] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [31] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu networks. CoRR, abs/1811.08888, 2018. 10Predicting Training Time Without Training: Supplementary Material In the Supplementary Material we give the pseudo-code for the training time prediction algorithm (Appendix A) together with implementation details, show additional results including prediction of training time using only a subset of samples, and comparison of real and predicted loss curves in a variety of conditions (Appendix C). Finally, we give proofs of all statements. A Algorithm Algorithm 1: Estimate the Training Time on a given target dataset and hyper-parameters. 1: Data: Number of steps T to simulate, threshold ϵto determine convergence, pre-trained weights w0 of the model, a target dataset with images X= {xi}N i=1 and labels Y= {yi}N i=1, batch size B, learning rate η, momentum m∈[0,1). 2: Result: An estimate ˆTϵ of the number of steps necessary to converge within ϵto the ﬁnal value Tϵ := min{t: |Lt −LT|<ϵ}. 3: Initialization: Compute initial network predictions f0(X), estimate Θ using random projections (Section 4), compute the ELR ˜η= η/(1 −m) to use in eq. (1) instead of η; 4: if B = N then 5: Get flin t (X) solving the ODE in eq. (1) (only the deterministic part) for T steps; 6: else 7: Get flin t (X) solving the SDE in eq. (1) for T steps (see approximation in Section 4); 8: end if 9: Using flin t (X) and Ycompute linearized loss Llin t ∀t∈{1,...,T } 10: return ˆTϵ := min{t: |Llin t −Llin T |<ϵ}; We can compute the estimate on training time based also on the accuracy of the model: we straight- forwardly modify the above algorithm and use the predictions flin t (X) to compute the error instead of the loss (e.g. ﬁg. 10). We now brieﬂy describe some implementations details regarding the numerical solution of ODE and SDE. Both of them can be solved by means of standard algorithms: in the ODE case we used LSODA (which is the default integrator in scipy.integrate.odeint), in the SDE case we used Euler-Maruyama algorithm for Ito equations. We observe removing batch normalization (preventing the statistics to be updated) and removing data augmentation improve linearization approximation both in the case of GD and SGD. Interestingly data augmentation only marginally alters the spectrum of the Gram matrix Θ and has little impact on the linearization approximation w.r.t. batch normalization. [ 12] observed similar effects but, differently from us, their analysis has been carried out using randomly initialized ResNets. B Target datasets Dataset Number of images Classes Mean samples per class Imbalance factor cifar10 [18] 50000 10 5000 1 cifar100 [18] 50000 100 500 1 cub200 [29] 5994 200 29.97 1.03 fgvc-aircrafts [23] 6667 100 66.67 1.02 mit67 [25] 5360 67 80 1.08 opensurfacesminc2500 [4] 48875 23 2125 1.03 stanfordcars [17] 8144 196 41.6 2.83 Table 2: Target datasets. 11C Additional Experiments Prediction of training time using a subset of samples. In Section 4 we suggest that in the case of MSE loss, it is possible to predict the training time on a large dataset using a smaller subset of samples (we discuss the details in Appendix D). In Figure 7 we show the result of predicting the loss curve on a dataset of N = 4000 samples using a subset of N = 1000 samples. Similarly, in Figure 11 (top row) we show the more difﬁcult example of predicting the loss curve on N = 1000 samples using a very small subset of N0 = 100 samples. In both cases we correctly predict that training on a larger dataset is slower, in particular we correctly predict the asymptotic convergence phase. Note in the case N0 = 100 the prediction is less accurate, this is in part due to the eigenspectrum of Θ being still far from its limiting behaviour achieved for large number of data (see Appendix D). Comparison of predicted and real error curve. In Figure 8 we compare the error curve predicted by our method and the actual train error of the model as a function of the number of optimization steps. The model is trained on a subset of 2 classes of CIFAR-10 with 150 samples. We run the comparison for both gradient descent (left) and SGD (right), using learning rate η= 0.001, momentum m= 0 and (in the case of SGD) batch size 100. In both cases we observe that the predicted curve is reasonably close to the actual curve, more so at the beginning of the training (which is expected, since the linear approximation is more likely to hold). We also perform an ablation study to see the effect of different approximation of SGD noise in the SDE in eq. (1). In Figure 8 (center) we estimate the variance of the noise of SGD at the beginning of the training, and then assume it is constant to solve the SDE. Notice that this predicts the wrong asymptotic behavior, in particular the predicted error does not converge to zero as SGD does. In Figure 8 (right) we rescale the noise as we suggest in Section 4: once the noise is rescaled the SDE is able to predict the right asymptotic behavior of SGD. Prediction accuracy in weight space and function space. In Section 3 and Section 6 we argue that using a differential equation to predict the dynamics in function space rather than weight space is not only faster (in the over-parametrized case), but also more accurate. In Figure 9 we show empirically that solving the corresponding ODE in weight space leads to a substantially larger prediction error. Effective learning rate. In Section 3.1 we note that as long as the effective learning rate ˜η = η/(1 −m) remains constant, runs with different learning rate ηand momentum mwill have similar learning curve. We show a formal derivation in Appendix E. In Figure 12 we show additional experiments, similar to Figure 2, on several other datasets to further conﬁrm this point. Point-wise similarity of predicted and observed loss curve. In some cases, we observe that the predicted and observed loss curves can differ. This is especially the case when using cross-entropy loss (Figure 10). We hypothesize that this may be due to improper prediction of the dynamics when the softmax output saturates, as the dynamic becomes less linear [20]. However, the train error curve (which only depends on the relative order of the outputs) remains relatively correct. We should also notice that prediction of the ϵ-training-time ˆTϵ can be accurate even if the curves are not point-wise close. The ϵ-training-time seeks to ﬁnd the ﬁrst time after which the loss or the error is within an ϵ threshold. Hence, as long as the real and predicted loss curves have a similar asymptotic slope the prediction will be correct, as we indeed verify in Figure 10 (bottom). 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.1 0.2 0.3 0.4 0.5 Train Loss size 1000 size 4000 predicted 0 5 10 15 20 25 30 35 40 Threshold 0 25 50 75 100 125 150 175 TT Real TT Estimated TT Figure 7: Training-time prediction using a subset of the data. (Left) Using the method described in Appendix D, we predict (green) the loss curve on a large dataset of N = 4000 samples (orange) using a subset of N0 = 1000 samples (blue). In Figure 11 we show a similar result using a much smaller subset of N0 = 100 samples. (Right) Corresponding estimated training time on the larger dataset at different thresholds ϵcompared to the real training time on the larger dataset. 12Figure 8: (Left) Comparison of the real error curve on CIFAR10 using gradient descent and the predicted curve. (Center) Same as before, but this time we train using SGD and compare it with the prediction using the technique described in Section 4 to approximate the covariance of the SGD noise that appears in the SDE in eq. (1). (Right) Same as (center), but using constant noise instead of rescaling the noise using the value of the loss function as described in Section 4. Note that in this case we do not capture the right asymptotic behavior of SGD. D Prediction of training time on larger datasets In Section 4 we suggest that, in the case of MSE loss, it is possible to predict the training time on a large dataset using a subset of the samples. To do so we leverage the fact that the eigenvalues of Θ follows a power-law which is independent on the size of the dataset for large enough sizes (see Figure 4, right). More precisely, from Proposition 2, we know that given the eigenvalues λk of Θ and the projections pk = δy ·vk it is possible to predict the loss curve using Lt = ∑ k pke−2ηλkt. Let Θ0 be the Gram-matrix of the gradients computed on the small subset of N0 samples, and let Θ be the Gram-matrix of the whole dataset of size N. Using the fact that, as we increase the number of samples, the eigenvalues (once normalized by the dataset size) converge to a ﬁxed limit (Figure 4, right), we estimate the eigenvalues λk of Θ as follow: we ﬁt the coefﬁcients sand cof a power law λk = ck−s to the eigenvalues of Θ0, and use the same coefﬁcients to predict the eigenvalues of Θ. However, we notice that the coefﬁcient s(slope of the power law) estimated using a small subset of the data is often smaller than the slope observed on larger datase (note in Figure 4 (right) that the curves for smaller datasets are more ﬂat). We found that using the following corrected power law increases the precision of the prediction: ˆλk = ck−s+α ( N0 N −1 ) . Empirically, we determined α∈[0.1,0.2] to give a good ﬁt over different combinations ofN and N0. In Figure 11 (center) we compare the predicted eigenspectrum of Θ with the actual eigenspectrum of Θ . The projections pk follow a similar power-law – albeit more noisy (see Figure 11, right) – so directly ﬁtting the data may give an incorrect result. However, notice that in this case we can exploit an 0 25 50 75 100 125 150 175 Iterations 0.0 0.2 0.4 0.6 0.8 1.0 Relative Error Weights relative approx error Output relative approx error Figure 9: Comparison of prediction accuracy in weight space vs. function space. We compare the result of using the deterministic part of eq. (1) to predict the weights wt at time tand the outputs ft(X) of the networks under GD. The relative error in predicting the outputs is much smaller than the relative error of predicting the weights at all times. This, together with the computational advantage, motivates the decision of using eq. (1) to predict the behavior in function space. 130 20 40 60 80 100 120 140 Iterations 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Train Loss Loss 0 20 40 60 80 100 120 140 Iterations 0 10 20 30 40 50 60 70 80 Train Err Err True Linearized 0 5 10 15 20 25 30 35 Threshold 40 60 80 100 120 140 TT 0 5 10 15 20 25 30 35 Threshold 20 40 60 80 100 120 140 TT Mit67 0 20 40 60 80 100 120 140 Iterations 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Train Loss Loss 0 20 40 60 80 100 120 140 Iterations 10 20 30 40 50 60 70 80 Train Err Err True Linearized 0 5 10 15 20 25 30 35 Threshold 40 60 80 100 120 140 TT 0 5 10 15 20 25 30 35 Threshold 20 40 60 80 100 120 140 TT CIFAR10 Figure 10: Training time prediction is accurate even if loss curve prediction is not. (Top row) Loss curve and error curve prediction on MIT-67 (left) and CIFAR-10 (right).(Bottom row)Predicted time to reach a given threshold (orange) vs real training time (blue). We note that on some datasets our loss curve prediction differs from the real curve near convergence. However, since our training time deﬁnition measures the time to reach the asymptotic value (which is what is most useful in practice) rather than the time reach an absolute threshold, this does not affect the accuracy of the prediction (see Appendix C). 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.1 0.2 0.3 0.4 0.5 Train Loss size 100 size 1000 predicted 100 101 102 103 Ordered eigenvalues 10 1 100 101 102 103 Spectrum Eig small dataset Eig large dataset Eig predicted 0 25 50 75 100 125 150 175 200 Ordered eigenvalues 10 6 10 5 10 4 10 3 10 2 10 1 100 101 102 y f0( ) Projections Large dataset projection Small dataset projection Prediction Figure 11: Training time prediction using a subset of the data. (Left) We predict the loss curve on a large dataset of N = 1000 samples using a subset of N0 = 100 samples on CIFAR10 (similar results hold for other datasets presented so far). (Center) Eigenspectrum of Θ computed using N0 = 100 samples (orange), N = 1000 samples (green) and predicted spectrum using our method (blue). (Right) Value of the projectionspk of δy on the eigenvectors of Θ, computed at N0 = 100 (orange) and N = 1000 (blue). Note that while they approximatively follow a power-law on average, it is much more ‘noisy’ than that of the eigenvalues. In green we show the predicted trend using our method. additional constraint, namely that ∑ kpk = ∥δy∥2 (∥δy∥2 is a known quantity: labels and initial model predictions on the large dataset). Let pk = δy ·vk and let p′ k = δy ·v′ k where vk and v′ k are the eigenvectors of Θ and Θ0 respectively. Fix a small k0 (in our experiments, k0 = 100). By convergence laws [27], we have that p′ k ≃pk when k < k0. The remaining tail of pk for k > k0 must now follow a power-law and also be such that ∑ kpk = ∥δy∥2. This uniquely identify the coefﬁcients of a power law. Hence, we use the following prediction rule for pk: ˆpk = {p′ k if k<k 0 ak−b if k≥k0 where aand bare such that ˆpk0 = p′ k0 and ∑ k ˆpk = ∥δy∥2. In Figure 11 (left), we use the approximated ˆλk and ˆpk to predict the loss curve on a dataset of N = 1000 samples using a smaller subset of N0 = 100 samples. Notice that we correctly predict that the convergence is slower on the larger dataset. Moreover, while training on the smaller dataset quickly reaches zero, we correctly estimate the much slower asymptotic phase on the larger dataset. Increasing both N and N0 increases the accuracy of the estimate, since the eigenspectrum of Θ is closer to convergence: In Figure 7 we show the same experiment as Figure 11 with N0 = 1000 and N = 4000. Note the increase in accuracy on the predicted curve. 14Figure 12: Additional experiments on the effective learning rate. We show additional plots showing the error curves obtained on different datasets using different values of the effective learning rate ˜η= η/(1 −m), where ηis the learning rate and mis the momentum. Each line is the observed error curve of a model trained with a different learning rate ηand momentum m. Lines with the same color have the same ELR ˜η, but each has a different ηand m. As we note in Section 3.1, as long as ˜η remains the same, training dynamics with different hyper-parameters will have similar error curves. E Effective learning rate We now show that having a momentum term has the effect of increasing the effective learning rate in the deterministic part of eq. (1). A similar treatment of the momentum term is also in [28, Appendix D]. Consider the update rule of SGD with momentum: at+1 = mat + gt+1, wt+1 = wt −ηat+1, If ηis small, the weights wt will change slowly and we can consider gt to be approximately constant on short time periods, that is gt+1 = g. Under these assumptions, the gradient accumulator at satisﬁes the following recursive equation: at+1 = mat + g, which is solved by (assuming a0 = 0 as common in most implementations): at = (1 −mt) g 1 −m. In particular, at converges exponentially fast to the asymptotic value a∗= g/(1 −m). Replacing this asymptotic value in the weight update equation above gives: wt+1 = wt −ηa∗= wt − η 1 −mg= wt −˜ηg, that is, once at reaches its asymptotic value, the weights are updated with an higher effective learning rate ˜η= η 1−m. Note that this approximation remains true as long as the gradient gt does not change much in the time that it takes at to reach its asymptotic value. This happens whenever the momentum mis small (since at will converge faster), or when η is small (gt will change more slowly). For larger momentum and learning rate, the effective learning rate may not properly capture the effect of momentum. F Proof of theorems F.1 Proposition 1: SDE in function space for linearized networks trained with SGD We now prove our Proposition 1 and show how we can approximate the SGD evolution in function space rather than in parameters space. We follow the standard method used in [13] to derive a general SDE for a DNN, then we speciaize it to the case of linearized deep networks. Our notation follows [20], we deﬁne fθt(X) = vec([ft(x)]x∈X) ∈RCN the stacked vector of model output logits for all examples, where Cis the number of classes and N the number of samples in the training set. 15To describe SGD dynamics in function space we start from deriving the SDE in parameter space. In order to derive the SDE required to model SGD we will start describing the discrete update of SGD as done in [13]. θt+1 = θt −η∇θLB(θt) (6) where LB(θt) = L(fθt(XB),YB) is the average loss on a mini-batch B(for simplicity, we assume that Bis a set of indexes sampled with replacement). The mini-batch gradient ∇θLB(θt) is an unbiased estimator of the full gradient, in particular the following holds: E[∇θLB(θt)] = 0 cov[ ∇θLB(θt)] = Σ(θt) |B| (7) Where we deﬁned the covariance of the gradients as: Σ(θt) := E [ (gi∇ft(xi)L) ⊗(gi∇ft(xi)L) ] −E [ gi∇ft(xi)L ] ⊗E [ gi∇ft(xi)L ] and gi := ∇wf0(xi). The ﬁrst term in the covariance is the second order moment matrix while the second term is the outer product of the average gradient. Following standard approximation arguments (see [7] and references there in) in the limit of small learning rate ηwe can approximate the discrete stochastic equation eq. (6) with the SDE: dθt = −η∇θL(θt)dt+ η√ |B| Σ(θt) 1 2 dn (8) where n(t) is a Brownian motion. Given this result, we are going now to describe how to derive the SDE for the outputft(X) of the network on the train set X. Using Ito’s lemma (see [13] and references there in), given a random variable θthat evolves according to an SDE, we can obtain a corresponding SDE that describes the evolution of a function of θ. Applying the lemma to fθ(X) we obtain: dft(X) = [−ηΘt∇ftL(ft(X),Y) + 1 2vec(A)]dt+ η√ |B| ∇θf(X)Σ(θt) 1 2 dn (9) where ∇θf(X) ∈RCN×D is the jacobian matrix and Dis the number of parameters. Note Ais a N ×Cmatrix which, denoting by f(j) θ (x) the j-th output of the model on a sample x, is given by: Aij = tr[Σ(θt)∇2 θf(j) θ (xi)]. Using the fact that in our case the model is linearized, so fθ(x) is a linear function of θ, we have that ∇2 θf(j)(x) = 0 and hence A= 0. This leaves us with the SDE: dft(X) = −ηΘt∇ftLdt+ η√ |B| ∇θf(X)Σ(θt) 1 2 dn (10) as we wanted. F.2 Proposition 2: Loss decomposition Let ∇wfw(X) = VΛU be the singular value decomposition of ∇wfw(X) where Λ is a rectangular matrix (of the same size of ∇wfw(X)) containing the singular values {σ1,...,σ N}on the diagonal. Both U and V are orthogonal matrices. Note that we have S = ∇wfw(X)T∇wfw(X) = UTΛTΛU, Θ = ∇wfw(X)∇wfw(X)T = VΛΛTVT. We now use the singular value decomposition to derive an expression for Lt in case of gradient descent and MSE loss (which we call Lt). In this case, the differential equation eq. (1) reduces to: ˙flin t (X) = −ηΘ(Y− flin t (X)), which is a linear ordinary differential equation that can be solved in closed form. In particular, we have: flin t (X) = (I−e−ηΘt)Y+ e−ηΘtf0(X). 16Replacing this in the expression for the MSE loss at time twe have: Lt = ∑ i (yi −flin t (xi))2 = (Y− flin t (X))T(Y− flin t (X)) = (Y− f0(X))Te−2ηΘt(Y− f0(X)). Now recall that, by the properties of the matrix exponential, we have: e−2ηΘt = e−2ηVΛΛT VT t = Ve−2ηΛΛT tVT, where e−2ΛΛT t = diag(e−2ηλ1t,e−2ηλ2t,... ) with λk := σ2 k. Then, deﬁning δy = Y− f0(X) and denoting with vk the k-th column of V we have: Lt = δyTVe−2ηΛΛT tVTδy = N∑ k=1 e−2ηλkt(δy ·vk). Now let uk denote the k-th column of UT and gi the i-th column of ∇wfw(X)T (that is, the gradient of the i-th sample). To conclude the proof we only need to show that λkvk = (gi ·uk)N i=1. But this follows directly from the SVD decompostion ∇wfw(X) = VΛU, since then VΛ = ∇wfw(X)UT. 17
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Database-friendly random projections: Johnson-lindenstrauss with binary coins",
        "A convergence theory for deep learning via over-parameterization",
        "Fine-grained analysis of op- timization and generalization for overparameterized two-layer neural networks",
        "Material recognition in the wild with the materials in context database",
        "Random projection in dimensionality reduction: applications to image and text data",
        "SGD learns over-parameterized networks that provably generalize on linearly separable data",
        "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
        "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
        "Gradient descent finds global minima of deep neural networks",
        "Gradient descent finds global minima of deep neural networks",
        "Gradient descent provably optimizes over-parameterized neural networks",
        "Truth or backpropaganda? an empirical investigation of deep learning theory",
        "Mean-ﬁeld behaviour of neural tangent kernel for deep neural networks",
        "Neural tangent kernel: Convergence and generalization in neural networks",
        "Predicting the computational cost of deep learning models",
        "On large-batch training for deep learning: Generalization gap and sharp minima",
        "3d object representations for ﬁne-grained categorization",
        "Learning multiple layers of features from tiny images",
        "Wide neural networks of any depth evolve as linear models under gradient descent, 2019",
        "Rethinking the hyperparameters for ﬁne-tuning",
        "Convergence analysis of two-layer neural networks with relu activation",
        "Fine-grained visual classiﬁcation of aircraft",
        "Gradients as features for deep representation learning",
        "Recognizing indoor scenes",
        "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "On the eigenspectrum of the gram matrix and the generalization error of kernel-pca",
        "A bayesian perspective on generalization and stochastic gradient descent",
        "Caltech-UCSD Birds 200",
        "Understanding deep learning requires rethinking generalization",
        "Stochastic gradient descent optimizes over-parameterized deep relu networks"
    ]
}
