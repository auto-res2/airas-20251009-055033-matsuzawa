
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
Continuous Meta-Learning without Tasks James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone Stanford University, Stanford, CA {jharrison, apoorva, cbfinn, pavone}@stanford.edu Abstract Meta-learning is a promising strategy for learning to efﬁciently learn using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, ofﬂine data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with unsegmented time series data. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian change- point detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on three nonlinear meta-regression benchmarks as well as two meta-image-classiﬁcation benchmarks. 1 Introduction Meta-learning methods have recently shown promise as an effective strategy for enabling efﬁcient few-shot learning in complex domains from image classiﬁcation to nonlinear regression [ 10, 40]. These methods leverage an ofﬂine meta-learning phase, in which data from a collection of learning tasks is used to learn priors and update rules for more efﬁcient learning on new related tasks. Meta-learning algorithms have thus far solely focused on settings with task segmentation, where the learning agent knows when the latent task changes. At meta-train time, these algorithms assume access to a meta-dataset of datasets from individual tasks, and at meta-test time, the learner is evaluated on a single task. However, there are many applications where task segmentation is unavailable, which have been under-addressed in the meta-learning literature. For example, environmental factors may change during a robot’s deployment, and these changes may not be directly observed. Furthermore, crafting a meta-dataset from an existing stream of experience may require a difﬁcult or expensive process of detecting switches in the task. In this work, we aim to enable meta-learning in task-unsegmented settings, operating directly on time series data in which the latent task undergoes discrete, unobserved switches, rather than requiring a pre-segmented meta-dataset. Equivalently, from the perspective of online learning, we wish to optimize an online learning algorithm using past data sequences to perform well in a sequential prediction setting wherein the underlying data generating process (i.e. the task) may vary with time. Contributions. Our primary contribution is an algorithmic framework for task unsegmented meta- learning which we refer to as meta-learning via online changepoint analysis (MOCA). MOCA wraps arbitrary meta-learning algorithms in a differentiable Bayesian changepoint estimation scheme, enabling their application to problems that require continual learning on time series data. By backpropagating through the changepoint estimation framework, MOCA learns both a rapidly adaptive underlying predictive model (the meta-learning model), as well as an effective changepoint detection algorithm, optimized to work together. MOCA is a generic framework which works with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1912.08866v2  [cs.LG]  21 Oct 2020many existing meta-learning algorithms. We demonstrate MOCA on both regression and classiﬁcation settings with unobserved task switches. 2 Problem Statement Our goal is to enable meta-learning in the general setting of sequential prediction, in which we observe a sequence of inputs xt and their corresponding labels yt. In this setting, the learning agent makes probabilistic predictions over the labels, leveraging past observations: pθ(ˆyt |x1:t,y1:t−1), where θare the parameters of the learning agent. We assume the data are drawn from an underlying generative model; thus, given a training sequence from this model Dtrain = (x1:N,y1:N), we can optimize θto perform well on another sample sequence from the same model at test time. We assume data is drawn according to a latent (unobserved) task Tt, that is xt,yt ∼p(x,y|Tt). Further, we assume that every so often, the task switches to a new task sampled from some distribution p(T). At each timestep, the task changes with probability λ, which we refer to as the hazard rate. We evaluate the learning algorithm in terms of a log likelihood, leading to the following objective: min θ E [∞∑ t=1 −log pθ(yt |x1:t,y1:t−1) ] (1) subj. to xt,yt ∼p(x,y|Tt), Tt = {Tt−1 w.p. 1 −λ Tt,new w.p. λ , T1 ∼p(T), Tt,new ∼p(T) Given Dtrain, we can approximate this expectation and thus learn θat train time. Note that just as in standard meta-learning, we leverage data drawn from a diverse collection of tasks in order to optimize a learning agent to do well on new tasks at test time. However, there are three key differences from standard meta-learning: • The learning agent continually adapts as it is evaluated on its predictions, rather than only adapting on klabeled examples, as is common in few-shot learning. • At train time, data is unsegmented, i.e. not grouped by the latent task T. • Similarly, at test time, the task changes with time, so the agent must infer which past data are drawn from the current task when making predictions. Thus, the setting we consider here can be considered a generalization of the standard meta-learning setting, relaxing the requirement of task segmentation at train and test time. Both our problem setting and an illustration of the MOCA algorithm are presented in Fig. 1. 3 Preliminaries Meta-Learning. The core idea of meta-learning is to directly optimize the few-shot learning performance of a machine learning model over a distribution of learning tasks, such that this learning performance generalizes to other tasks from this distribution. A meta-learning method consists of two phases: meta-training and online adaptation. Let θbe the parameters of this model learned in meta-training. During online adaptation, the model uses context data Dt = (x1:t,y1:t) from within one task to compute statistics ηt = fθ(Dt), where f is a function parameterized by θ. For example, in MAML [10], the statistics are the neural network weights after gradient updates computed using Dt. For recurrent network-based meta-learning algorithms, these statistics correspond to the hidden state of the network. For a simple nearest-neighbors model, η may simply be the context data. The model then performs predictions by using these statistics to deﬁne a conditional distribution on ygiven new inputs x, which we write y|x,Dt ∼pθ(y|x,ηt). Adopting a Bayesian perspective, we refer to pθ(y|x,ηt) as the posterior predictive distribution. The performance of this model on this task can be evaluated through the log-likelihood of task data under this posterior predictive distribution L(Dt,θ) = Ex,y∼p(·,·|Ti)[−log pθ(y|x,fθ(Dt))]. Meta-learning algorithms, broadly, aim to optimize the parameters θsuch that the model performs well across a distribution of tasks, minθ ETi∼p(T) [EDt∼Ti [L(Dt,θ)]] .Across most meta-learning algorithms, both the update rule fθ(·) and the prediction function are chosen to be differentiable operations, such that the parameters can be optimized via stochastic gradient descent. Given a dataset 2Figure 1: An illustration of a simpliﬁed version of our problem setting and of the MOCA algorithm. An agent sequentially observes an input x(e.g, an image), makes a probabilistic prediction, and receives the true label y (here, class 1 or 2). An unobserved change in the task (a “changepoint”) results in a change in the generative model of xand/or y. In the above image, the images corresponding to label 1 switch from sailboats to school buses, while the images corresponding to label 2 switch from sloths to geese. MOCA recursively estimates the time since the last changepoint, and conditions an underlying meta-learning model only on data that is relevant to the current task to optimize its predictions. pre-segmented into groups of data from individual tasks, standard meta-learning algorithms can estimate this expectation by ﬁrst sampling a group for which T is ﬁxed, then treating one part as context data Dt, and sampling from the remainder to obtain test points from the same task. While this strategy is effective for few-shot learning, it fails for settings like sequential prediction, where the latent task may change over time and segmenting data by task is difﬁcult. Our goal is to bring meta-learning tools to such settings. Bayesian Online Changepoint Detection. To enable meta-learning without task segmentation, we build upon Bayesian online changepoint detection [1], an approach for detecting discrete changes in a data stream (i.e. task switches), originally presented in an unconditional density estimation context. BOCPD operates by maintaining a belief distribution over run lengths, i.e. how many past data points were generated under the current task. A run length rt = 0 implies that the task has switched at time t, and so the current datapoint yt was drawn from a new task T′∼p(T). We denote this belief distribution at time tas bt(rt) = p(rt |y1:t−1). We can reason about the overall posterior predictive by marginalizing over the run length rt according to bt(rt), p(yt |y1:t−1) = ∑t−1 τ=0 p(yt | y1:t−1,rt = τ)bt(τ), Given rt = τ, we know the past τ data points all correspond to the current task, so p(yt |y1:t−1,rt = τ) can be computed as the posterior predictive of an underlying predictive model (UPM), conditioning on the past τ data points. BOCPD recursively computes posterior predictive densities using this UPM for each value of rt ∈{0,...,t −1}, and then evaluates new datapointsyt+1 under these posterior predictive densities to update the belief distribution b(rt). In this work, we extend these techniques to conditional density estimation, deriving update rules which use meta-learning models as the UPM. 4 Meta-Learning via Online Changepoint Analysis We now present MOCA1, which enables meta-learning in settings without task segmentation, both at train and test time. In the following subsections, we ﬁrst extend BOCPD to derive a recursive Bayesian ﬁltering algorithm for run length, leveraging a base meta-learning algorithm as the underlying predictive model (UPM). We then outline how the full framework allows both training and evaluating meta-learning models on time series without task segmentation. 4.1 Bayesian Task Duration Estimation As in BOCPD, MOCA maintains a belief over possible run lengths rt. Throughout this paper, we use bt to refer to the belief before observing data at that timestep, (xt,yt). Note that bt is a discrete distribution with support over rt ∈{0,...,t −1}. MOCA also maintains a version of the base meta-learning algorithm’s posterior parametersηfor every possible run length. We write ηt[r] to refer to the posterior parameters produced by the meta-learning algorithm after adapting to the past r 1Code is available at https://github.com/StanfordASL/moca 3Algorithm 1 Meta-Learning via Online Changepoint Analysis Require: Training data x1:n, y1:n, number of training iterations N, initial model parameters θ 1: for i = 1to N do 2: Sample training batch x1:T , y1:T from the full timeseries. 3: Initialize run length belief b1(r1 = 0) = 1, posterior statistics η0[r = 0]according to θ 4: for t = 1to T do 5: Observe xt, compute bt(rt |xt) via (2) 6: Predict pθ(ˆyt |x1:t, y1:t−1) via (5) 7: Observe yt and incur NLL loss ℓt = −log pθ(yt |x1:t, y1:t−1) 8: Compute updated posteriors ηt[rt] for all rt via (6) 9: Compute bt(rt |xt, yt) via (3) 10: Compute updated belief over run length bt+1 via (4) 11: end for 12: Compute ∇θ ∑k+T t=k ℓt and take gradient descent step to update θ 13: end for datapoints, (xt−r+1:t,yt−r+1:t). Given this collection of posteriors, we can compute the likelihood of observing data given the run length r. This allows us to apply rules from Bayesian ﬁltering to update the run length belief in closed form. These updates involve three steps: If the base meta-learning algorithm maintains a posterior distribution of inputs pθ(xt |ηt−1), then MOCA can update the belief bt directly after observing xt, as follows bt(rt |xt) := p(rt |x1:t,y1:t−1) ∝pθ(xt |ηt−1[rt])bt(rt) (2) which can be normalized by summing over the ﬁnite support of bt. This step relies on maintaining a generative model of the input variable, which is atypical for most regression models and is not done for discriminative classiﬁcation models. While this ﬁltering step is optional, it allows MOCA to detect task switches based on a changes in the input distribution when possible. Next, upon observing the labelyt, we can use the base meta-learning algorithm’s conditional posterior predictive pθ(yt |xt,ηt−1) to again update the belief over run length: bt(rt |xt,yt) := p(rt |x1:t,y1:t) ∝pθ(yt |xt,ηt−1[rt])bt(rt |xt), (3) which can similarly be normalized. Finally, to push the run length belief forward in time, we note that we assume that the task switches with probability λat every timestep, and so the task remains ﬁxed with probability 1 −λ. This yields the update bt+1(rt+1 = k) = {λ if k= 0 (1 −λ)bt(rt = k−1 |xt,yt) if k> 0 . (4) For more details on the derivation of these updates, we refer the reader to Appendix A. 4.2 Meta Learning without Task Segmentation By taking a Bayesian ﬁltering approach to changepoint detection, we avoid hard assignments of changepoints and instead perform a soft selection over run lengths. In this way, MOCA is able to backpropagate through the changepoint detection and directly optimize the underlying predictive model, which may be any meta-learning model that admits a probabilistic interpretation. MOCA processes a time series sequentially. We initialize b1(r1 = 0) = 1, and initialize the posterior statistics for η0[r1 = 0] as speciﬁed by the parameters θof the meta learning algorithm. Then, at timestep t, we ﬁrst observe inputs xt and compute bt(rt |xt) according to (2). Next, we marginalize to make a probabilistic prediction for the label, pθ(ˆyt |x1:t,y1:t−1) equal to t−1∑ rt=0 bt(rt |xt)pθ(ˆyt |xt,ηt−1[rt]) (5) We then observe the label yt and incur the corresponding loss. We can also use the label both to compute bt(rt |xt,yt) according to (3), as well as to update the posterior statistics for all the run 4lengths using the labeled example. Many meta-learning algorithms admit a recursive update rule which allows these parameters to be computed efﬁciently using the past values of η, ηt[r] = h(xt,yt,ηt−1[r−1]) ∀r= 1,...,t. (6) While MOCA could work without such a recursive update rule, this would require storing data online and running the non-recursive posterior computation ηt = fθ((xt−rt+1:t,yt−rt+1:t)) for every rt, which involves toperations using datasets of sizes from 0 to t, and thus can be an O(t2) operation. In contrast, the recursive updates involve toperations involving just the latest datapoint, yielding O(t) complexity. Finally, we propagate the belief over run length forward in time to obtain bt(rt+1) to be ready to process the next data point in the timeseries. Since all these operations are differentiable, given a training time series in which there are task switches Dtrain, we can run this procedure, sum the negative log likelihood (NLL) losses incurred at each step, and use backpropagation within a standard automatic differentiation framework to optimize the parameters of the base learning algorithm, θ. Algorithm 1 outlines this training procedure. In practice, we sample shorter time series of length T from the training data to ease computational requirements during training; we discuss implications of this in Appendix D. If available, a user can input various levels of knowledge on task segmentation by manually updating b(rt) at any time; further details and empirical validation of this task semi-segmented use case are also provided in Appendix D 4.3 Making your MOCA: Model Instantiations Thus far, we have presented MOCA at an abstract level, highlighting the fact that it can be used with any meta-learning model that admits the probabilistic interpretation as the UPM. Practically, as MOCA maintains several copies of the posterior statistics η, meta-learning algorithms with lower-dimensional posterior statistics which admit recursive updates yield better computational efﬁciency. With this in mind, for our experiments we implemented MOCA using a variety of base meta-learners: an LSTM-based meta-learning approach [ 21], as well as meta-learning algorithms based on Bayesian modeling which exploit conjugate prior/likelihood models allowing for closed- form recursive posterior updates, speciﬁcally ALPaCA [ 16] for regression and a novel algorithm in a similar vein which we call PCOC, for probabilistic clustering for online classiﬁcation, for classiﬁcation. Further details on all methods are provided in Appendix B. LSTM Meta-learner. The LSTM meta-learning approach encodes the information in the observed samples using hidden state ht of an LSTM [20], and subsequently uses this hidden state to make predictions. Speciﬁcally, we follow the architecture proposed in [21], wherein an encoding of the current input zt = φ(xt,w) as well as the previous label yt−1 are fed as input to the LSTM cell to update the hidden state ht and cell state ct. For regression, the mean and variance of a Gaussian posterior predictive distribution are output as a function of the hidden state and encoded input [µ,Σ] = f(ht,zt; wf). The function f is a feedforward network in both cases, with weights wf. Within the MOCA framework, the posterior statistics for this model are ηt = {ht,ct,yt}. ALPaCA: Bayesian Meta-Learning for Regression. ALPaCA is a meta-learning approach which performs Bayesian linear regression in a learned feature space, such thaty|x∼N(KTφ(x,w),Σϵ) where φ(x,w) is a feed-forward neural network with weights w mapping inputs x to a nφ- dimensional feature space. ALPaCA maintains a matrix-normal distribution over K, and thus results in a matrix-normal posterior distribution over K. This posterior inference may be performed exactly, and computed recursively. The matrix-normal distribution on the last layer results in a Gaussian posterior predictive density. Note that, as is typical in regression, ALPaCA only models the conditional density p(y|x), and assumes that p(x) is independent of the underlying task. The algorithm parameters θare the prior on the last layer, as well as the weights wof the neural network feature network φ. The posterior statistics ηencode the mean and variance of the Gaussian posterior distribution on the last layer weights. PCOC: Bayesian Meta-Learning for Classiﬁcation. In the classiﬁcation setting, one can obtain a similar Bayesian meta-learning algorithm by performing Gaussian discriminant analysis in a learned feature space. We refer to this novel approach to meta-learning for classiﬁcation as probabilistic clustering for online classiﬁcation (PCOC). Labeled input/class pairs (xt,yt) are processed by encoding the input through an embedding network zt = φ(xt; w), and performing Bayesian density estimation in this feature space for every class. Speciﬁcally, we assume a Categorical-Gaussian generative model in this embedding space, and impose the conjugate Dirichlet prior over the class 50 20 40 60 80 Time step 0 20 40Run length t=20 t=67 Figure 2: MOCA with ALPaCA on the sinusoid regression problem. Left: The belief over run length versus time. The intensity of each point in the plot corresponds to the belief in run length at the associated time. The red lines show the true changepoints. Middle, Right: Visualizations of the posterior predictive density at the times marked by blue lines in the left ﬁgure. The red line denotes the current function (task), and red points denote data from the current task. Green points denote data from previous tasks, where more faint points are older. By reasoning about task run-length, MOCA ﬁts the current sinusoid while avoiding negative transfer from past data, and resets to prior predictions when tasks switch. probabilities and a Gaussian prior over the mean for each class. This ensures the posterior remains Dirichlet-Gaussian, whose parameters can be updated recursively. The posterior parameters ηfor this algorithm are the mean and covariance of the posterior distribution on each class mean, as well as the counts of observations per class. The learner parameters θare the weights of the encoding network w, the prior parameters, and the covariance assumed for the observation noise. PCOC can be thought of a Bayesian analogue of prototypical networks [40]. 5 Related Work Online Learning, Continuous Learning, and Concept Drift Adaptation. A substantial literature exists on online, continual and lifelong learning [ 18, 6]. These ﬁelds all consider learning within a streaming series of tasks, wherein it is desirable to re-use information from previous tasks while avoiding negative transfer [12, 42]. Typically, continual learning assumes access to task segmentation information, whereas online learning does not [3]. Regularization approaches [26, 18, 28] have been shown to be an effective method for avoiding forgetting in continual learning. By augmenting the loss function for a new task with a penalty for deviation from the parameters learned for previous tasks, the regularizing effects of a prior are mimicked; in contrast we explicitly learn a prior over task weights that is meta-trained to be rapidly adaptive. Thus, MOCA is capable of avoiding substantial negative transfer by detecting task change, and rapidly adapting to new tasks. [3] loosen the assumption of task segmentation in continual learning and operate in a similar setting to that addressed herein, but they aim to optimize one model for all tasks simultaneously; in contrast, our work takes a meta-learning approach and aims to optimize a learning algorithm to quickly adapt to changing tasks. Meta-Learning for Continuous and Online Learning. In response to the slow adaption of contin- ual learning algorithms, there has been substantial interest in applying ideas from meta-learning to continual learning to enable rapid adaptation to new tasks. To handle streaming data, several works [31, 19] use a sliding window approach, wherein a ﬁxed amount of past data is used to condition the meta-learned model. As this window length is not reactive to task change, these models risk suffering from negative transfer. Indeed, MOCA may be interpreted as sliding window model, that actively infers the optimal window length. [ 32] and [24] aim to detect task changes online by combining mean estimation of the labels with MAML. However, these models are less expressive than MOCA (which maintains a full Bayesian posterior), and require task segmentation as test time. [36] employ gradient-based meta-learning to improve transfer between tasks in continual learning; in contrast MOCA works with any meta-learning algorithm. Empirical Bayes for Changepoint Models. Follow-on work to BOCPD [1] and the similar simulta- neous work of [9] has considered applying empirical Bayes to optimize the underlying predictive model, a similar problem to that addressed herein. In particular, [33] develop a forward-backward algorithm that allows closed-form max likelihood estimation of the prior for simple distributions via EM. [43] derive general-purpose gradients for hyperparameter optimization within the BOCPD model. MOCA pairs these ideas with neural network meta-learning models, and thus can leverage recent advances in automatic differentiation for gradient computation. 6Figure 3: Performance of MOCA with ALPaCA versus baselines in sinusoid regression (left) and the switching wheel contextual bandit problem (right). In the bandit problem, we evaluate performance as the regret of the model (compared to an optimal decision maker with perfect knowledge of switch times) as a percentage of the regret of the random agent, following previous work [37]. In both problems, lower is better. Conﬁdence intervals in this ﬁgure and throughout are 95%. Model Test NLL TOE 0.889 ±0.073 SW5 −3.032 ±0.058 SW10 −3.049 ±0.054 SW50 −3.061 ±0.054 COE −3.044 ±0.059 MOCA −3.291 ±0.074 0 20 40 60 80 x (ft) 0 10 20 30 40 50 y (ft) 25 50 75 x position (ft)x position (ft)x position (ft)x position (ft)x position (ft)x position (ft)x position (ft) 10 20 y position (ft)y position (ft)y position (ft)y position (ft)y position (ft)y position (ft)y position (ft) 0 50 100 150 Time step (dt = 0.04 s) 0 50 run length belief Figure 4: Left: Test NLL of MOCA + LSTM against baselines. Middle: Visualization of sample trajectory, segmented by color according to predicted task changes. We see that task changes visually correspond to different plays. Right: Trajectories plotted against time, together with MOCA’s belief over run length. Task switches (dashed gray) were placed where the MAP run length drops to a value less than 5. 6 Experimental Results We investigate the performance of MOCA in ﬁve problem settings: three in regression and two in classiﬁcation. Our primary goal is to characterize how effectively MOCA can enable meta-learning algorithms to perform without access to task segmentation. We compare against baseline sliding window models, which again use the same meta-learning algorithm, but always condition on the last ndata points, for n ∈{5,10,50}. These baselines are a competitive approach to learning in time-varying data streams [13] and have been applied meta-learning in time-varying settings [31]. We also compare to a “train on everything” model, which only learns a prior and does not adapt online, corresponding to a standard supervised learning approach. Finally, where possible, we compare MOCA against an “oracle” model that uses the same base meta-learning algorithm, but has access to exact task segmentation at train and test time, to explicitly characterize the utility of task segmentation. Due to space constraints, this section contains only core numerical results for each problem setting; further experiments and ablations are presented in the appendix. We ﬁnd by explicitly reasoning about task run-length, MOCA is able to outperform baselines across all the domains with a variety of base meta-learning algorithms and provide interpretable estimates of task-switches at test time. Sinusoid Regression. To characterize MOCA in the regression setting, we investigate the perfor- mance on a switching sinusoid problem adapted from [10], in which a task change corresponds to a re-sampled sinusoid phase and amplitude. Qualitative results are visualized for the sinusoid in Fig. 2. In this problem we pair MOCA with ALPaCA as it outperforms LSTM-based meta-learners. MOCA is capable of accurate and calibrated posterior inference with only a handful of data points, and is capable of rapidly identifying task change. Typically, it identiﬁes task change in one timestep, unless the datapoint happens to have high likelihood under the previous task as in Fig. 2d. Performance of MOCA against baselines is presented in Fig. 3 for all problem domains. For sinusoid (left), MOCA 7Figure 5: Performance of MOCA with PCOC on rainbow MNIST ( left) and miniImageNet (right). In both problems, higher is better. achieves performance close to the oracle model and substantially outperforms the sliding window approaches for all hazard rates. Wheel Bandit . Bandit problems have seen recent highly fruitful application of meta-learning algorithms [4, 45, 15]. We investigate the performance of MOCA (paired with ALPaCA) in the switching bandit problem, in which the reward function of the bandit undergoes discrete changes [14, 17, 30]. We extend the wheel bandit problem [ 37], a common benchmark for meta-learning algorithms [15, 34]. Details of the full bandit problem are provided in the appendix. In this problem, changepoint identiﬁcation is difﬁcult, as only a small subset of states contains information about whether the reward function has changes. Following [30], we use Thompson sampling for action selection. We use the notion of regret deﬁned in [14], in which the chosen action is compared to the action with the best mean reward at each time, with perfect knowledge of switches. As shown in [ 14], the sliding window baselines have strong theoretical guarantees on regret, as well as good empirical performance. Performance is plotted in Fig. 3. MOCA outperforms baselines for lower hazard rates. Detecting task switches requires observing a state close to the (changing) high-reward boundary, and at high hazard rates, the rapid task changes make identiﬁcation of changepoints difﬁcult, and we see that MOCA performance matches all the sliding windows in this regime. NBA Player Movement. To test MOCA on a real-world data with an unobserved switching latent task, we test it on predicting the movement of NBA players, whose intent may switch over time, from, e.g., running towards a position on the three-point line, to moving inside the key to recover a rebound. This changing latent state has made it a common benchmark for recurrent predictive models [22, 29]. In our experiments, the input xis an individual player’s current position on the court(xt,yt), and the label yt = xt+1 −xt is the step the player takes at that time. For this problem, we pair MOCA with the LSTM meta-learner, since recurrent models are well suited to this task and we saw better performance relative to ALPaCA. We add a “condition on everything” (COE) baseline which updates a single set of posterior statistics ηusing all available data, as the LSTM can theoretically learn to only consider relevant data. Nevertheless, we ﬁnd that that MOCA’s explicit reasoning over task length yields better performance over COE and other baselines, as shown in Fig. 4. While true task segmentation is unavailable for this data, we see in the ﬁgure that MOCA’s predictions of task changes correspond intuitively to changes in the player’s intent. Rainbow MNIST. In the classiﬁcation setting, we apply MOCA with PCOC to the Rainbow MNIST dataset of [11]. In this dataset, MNIST digits have been perturbed via a color change, rotation, and scaling; each task corresponds to a unique combination of these transformations. Relative to baselines, MOCA approaches oracle performance for low hazard rates, due in part to the fact that task change can usually be detected prior to prediction via a change in digit color. Seven colors were used, so with probability 6/7, MOCA has a strong indicator of task change before observing the image class. miniImageNet. Finally, we investigate the performance of MOCA with PCOC on the miniImageNet benchmark task [44]. This dataset consists of 100 ImageNet categories [ 7], each with 600 RGB images of resolution 84×84. In our continual learning setting, we associate each class with a semantic label that is consistent between tasks. As ﬁve-way classiﬁcation is standard for miniImageNet [44, 40], we split the miniImageNet dataset in to ﬁve approximately balanced “super-classes." For example, 8one super-class is dog breeds, while another is food, kitchen and clothing items; details are provided in the appendix. Each new task corresponds to resampling a particular class from each super-class from which to draw inputs x; the labels yremain the ﬁve super-classes, enabling knowledge re- use between classes. This corresponds to a continual learning scenario in which each super-class experiences distributional shift over time. Fig. 5 shows that MOCA outperforms baselines for all hazard rates. 7 Discussion and Conclusions Future Work. In this work, we address the case in which tasks are sampled i.i.d. from a (typically continuous) distribution, and thus knowledge re-use adds marginal value. However, many domains may have tasks that can reoccur, or temporal dynamics to task evolution and thus data efﬁciency may be improved by re-using information for previous tasks. Previous work [32, 24, 27] has addressed the case in which tasks reoccur in both meta-learning and the BOCPD framework, and thus knowledge (in the form of a posterior estimate) may be re-used. Broadly, moving beyond the assumption of i.i.d. tasks to tasks having associated dynamics [2] represents a promising future direction. Conclusions. MOCA enables the application of existing meta-learning algorithms to problems without task segmentation, such as the problem setting of continual learning. We ﬁnd that by leveraging a Bayesian perspective on meta-learning algorithms and augmenting these algorithms with a Bayesian changepoint detection scheme to automatically detect task switches within time series, we can achieve similar predictive performance when compared to the standard task-segmented meta-learning setting, without the often prohibitive requirement of supervised task segmentation. Funding Disclosure and Acknowledgments James Harrison was supported in part by the Stanford Graduate Fellowship and the National Sciences and Engineering Research Council of Canada (NSERC). The authors were partially supported by an Early Stage Innovations grant from NASA’s Space Technology Research Grants Program, and by DARPA, Assured Autonomy program. The authors wish to thank Matteo Zallio for help in the design of ﬁgures. Broader Impact Our work provides a method to extend meta-learning algorithms beyond the task-segmented case, to the time series series domain. Equivalently, our work extends core methods in changepoint detection, enabling the use of highly expressive predictive models via empirical Bayes. This work has the potential to extend the domain of applicability of both of these methods. Standard meta-learning relies on a collection of datasets, each corresponding to discrete tasks. A natural question is how such datasets are constructed; in many cases, these datasets rely on segmentation of time series data by experts. Thus, our work has the potential to make meta-learning algorithms applicable to problems that, previously, would have been too expensive or impossible to segment. Moreover, our work has the potential to improve the applicability of changepoint detection methods to difﬁcult time series forecasting problems. While MOCA has the potential to expand the domain of problems addressable via meta-learning, this has the effect of amplifying the risks associated with these methods. Meta-learning enables efﬁcient learning for individual members of a population via leveraging empirical priors. There are clear risks in few-shot learning generally: for example, efﬁcient facial recognition from a handful of images has clear negative implications for privacy. Moreover, while there is promising initial work on fairness for meta-learning [39], we believe considerable future research is required to understand the degree to which meta-learning algorithms increase undesirable bias or decrease fairness. While it is plausible that ﬁne-tuning to the individual results in reduced bias, there are potential unforeseen risks associated with the adaptation process, and future research should address how bias is potentially introduced in this process. Relative to decision making rules that are ﬁxed across a population, algorithms which ﬁne-tune decision making to the individual present unique challenges in analyzing fairness. Further research is required to ensure that the adaptive learning enabled by algorithms such as MOCA do not lead to unfair outcomes. 9References [1] Ryan Prescott Adams and David JC MacKay. Bayesian online changepoint detection. arXiv:0710.3742, 2007. [2] Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. International Conference on Learning Representations (ICLR), 2018. [3] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. Com- puter Vision and Pattern Recognition (CVPR), 2019. [4] Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits. arXiv:2005.08531, 2020. [5] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁcation. International Conference on Learning Representations (ICLR), 2019. [6] Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, 2016. [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. Computer Vision and Pattern Recognition (CVPR), 2009. [8] Bradley Efron and Carl Morris. Stein’s estimation rule and its competitors—an empirical Bayes approach. Journal of the American Statistical Association, 1973. [9] Paul Fearnhead and Zhen Liu. On-line inference for multiple changepoint problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2007. [10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta- tion of deep networks. International Conference on Machine Learning (ICML), 2017. [11] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. International Conference on Machine Learning (ICML), 2019. [12] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 1999. [13] João Gama, Indr˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept drift adaptation. ACM computing surveys (CSUR), 2014. [14] Aurélien Garivier and Eric Moulines. On upper-conﬁdence bound policies for switching bandit problems. International Conference on Algorithmic Learning Theory, 2011. [15] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S.M. Ali Eslami, and Yee Whye Teh. Neural processes. International Conference on Machine Learning (ICML), 2018. [16] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efﬁcient online Bayesian regression. Workshop on the Algorithmic Foundations of Robotics (WAFR), 2018. [17] Cédric Hartland, Nicolas Baskiotis, Sylvain Gelly, Michèle Sebag, and Olivier Teytaud. Change point detection and meta-bandits for online learning in dynamic environments. 2007. [18] Elad Hazan. Introduction to online convex optimization. Foundations and Trends® in Opti- mization, 2016. [19] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan Pascanu. Task agnostic continual learning via meta learning. arXiv preprint arXiv:1906.05201, 2019. [20] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997. [21] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. International Conference on Artiﬁcial Neural Networks (ICANN, 2001. [22] Boris Ivanovic, Edward Schmerling, Karen Leung, and Marco Pavone. Generative modeling of multimodal multi-human behavior. IEEE International Conference on Intelligent Robots and Systems (IROS), 2018. [23] Khurram Javed and Martha White. Meta-learning representations for continual learning. Neural Information Processing Systems (NeurIPS), 2019. 10[24] Ghassen Jerfel, Erin Grant, Thomas L Grifﬁths, and Katherine Heller. Online gradient-based mixtures for transfer modulation in meta-learning. Neural Information Processing Systems (NeurIPS), 2019. [25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015. [26] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS), 2017. [27] Jeremias Knoblauch and Theodoros Damoulas. Spatio-temporal bayesian on-line changepoint detection with model selection. International Conference on Machine Learning (ICML), 2018. [28] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017. [29] Scott W Linderman, Andrew C Miller, Ryan P Adams, David M Blei, Liam Paninski, and Matthew J Johnson. Recurrent switching linear dynamical systems. arXiv:1610.08466, 2016. [30] Joseph Mellor and Jonathan Shapiro. Thompson sampling in switching environments with bayesian online change detection. Artiﬁcial Intelligence and Statistics (AISTATS), 2013. [31] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta- reinforcement learning. International Conference on Learning Representations (ICLR), 2019. [32] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual adaptation for model-based RL. International Conference on Learning Representa- tions (ICLR), 2019. [33] Ulrich Paquet. Empirical Bayesian change point detection. Graphical Models, 2007. [34] Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. International Conference on Learning Representations (ICLR), 2018. [35] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenen- baum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. International Conference on Learning Representations (ICLR), 2018. [36] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Ger- ald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. International Conference on Learning Representations (ICLR), 2019. [37] Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An em- pirical comparison of bayesian deep networks for thompson sampling. International Conference on Learning Representations (ICLR), 2018. [38] Y Saatci, R Turner, and CE Rasmussen. Gaussian process change point models. International Conference on Machine Learning (ICML), 2010. [39] Dylan Slack, Sorelle Friedler, and Emile Givental. Fair meta-learning: Learning how to learn fairly. arXiv:1911.04336, 2019. [40] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Neural Information Processing Systems (NeurIPS), 2017. [41] Charles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. Third Berkeley symposium on Mathematical statistics and Probability, 1956. [42] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer, 2012. [43] Ryan Turner, Yunus Saatci, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. NeurIPS Workshop on Nonparametric Bayes, 2009. [44] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Neural Information Processing Systems (NeurIPS), 2016. [45] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763, 2016. [46] Robert C Wilson, Matthew R Nassar, and Joshua I Gold. Bayesian online learning of the hazard rate in change-point problems. Neural Computation, 2010. 11A MOCA Algorithmic Details In this section, we derive the Bayesian belief updates used in MOCA. As in the paper, we will use bt to refer to the belief before observing data at that timestep, (xt,yt). Note that bt is a discrete distribution with support over rt ∈{0,...,t −1}. We write ηt[r] to refer to the posterior parameters of the meta-learning algorithm conditioned on the past rdata points, (xt−r+1:t,yt−r+1:t). At time t, the agent ﬁrst observes the input xt, then makes a prediction p(yt |x1:t,y1:t−1), and subsequently observes yt. Generally, the latent task can inﬂuence both the marginal distribution of the input, p(xt |x1:t−1,y1:t−1) as well as the conditional distribution p(yt |x1:t,y1:t−1). Thus, the agent can update its belief over run lengths once after observing the input xt, and again after observing the label yt. We will use bt(rt |xt) = p(rt |x1:t,y1:t−1) to represent the updated belief over run length after observing only xt, and bt(rt |xt,yt) = p(rt |x1:t,y1:t) to represent the fully updated belief over rt after observing yt. Finally, we will propagate this forward in time according to our assumptions on task dynamics to compute bt+1(rt+1), which is used in the subsequent timestep. To derive the Bayesian update rules, we start by noting that the updated posterior is proportional to the joint density, bt(rt |xt) = p(rt |x1:t,y1:t−1) (7) = Z−1p(rt,xt |x1:t−1,y1:t−1) = Z−1p(xt |x1:t−1,y1:t−1,rt)bt(rt) (8) where the normalization constant Zcan be computed by summing over the ﬁnite support of bt−1(rt). Importantly, this update requires pθ(xt |ηt−1[rt]), the base meta-learning algorithm’s posterior predictive density over the inputs. Within classiﬁcation, this density is available for generative models, and thus a generative approach is favorable to a discriminative approach within MOCA. In regression, it is uncommon to estimate the distribution of the independent variable. We take the same approach in this work and assume that xt is independent of the task for regression problems, in which case bt(rt |xt) = bt(rt). Next, upon observing yt, we can similarly factor the belief over run lengths for the next timestep, bt(rt |xt,yt) ∝pθ(yt |xt,ηt−1[rt])bt(rt |xt), (9) which can again easily be normalized. Finally, we must propagate this belief forward in time: bt+1(rt+1) = p(rt+1 |x1:t,y1:t) = ∑ rt p(rt+1,rt |x1:t,y1:t) = ∑ rt p(rt+1 |rt)bt(rt |xt,yt). where we have exploited the assumption that the changes in task, and hence the evolution of run length rt, happen independently of the data generation process. The conditional run-length distribution p(rt+1 |rt) is deﬁned by our model of task evolution. Recall that we assume that the task switches with ﬁxed probability λ, the hazard rate. Thus, p(rt+1 = 0 |rt) = λfor all rt, implying bt+1(rt+1 = 0) = λ. Conditioned on the task remaining the same, rt+1 = k> 0 and rt = k−1. Thus, p(rt+1 = k|rt) = (1 −λ)1{rt = k−1}implying bt+1(rt+1 = k) = (1 −λ)bt(rt = k−1 |xt,yt). (10) This gives the time-propagation update step, as in equation (4), used by MOCA. B Base Meta-Learning Algorithm Details In the following subsections, we describe how each of the base meta-learning algorithms we use for the experiments ﬁt into the MOCA framework. Speciﬁcally, we highlight (1) which parameters θare optimized, (2) the statistics ηfor each algorithm, (3) how these statistics deﬁne a posterior predictive distribution pθ(ˆyt+1 |x1:t+1,y1:t), and ﬁnally (4) the recursive update rule ηt = h(ηt−1,xt,yt) used to incorporate a new labeled example. 12B.1 LSTM Meta-Learner For our LSTM meta-learner, we follow the architecture of [21]. The LSTM input is the concatenation of the current encoded input zt = φ(xt,w) and the label from the past timestep yt−1. In this way, through the LSTM update process, the hidden state can process a sequence of input/label pairs and encode statistics of the posterior distribution in the hidden state. Thus, the necessary statistics to make predictions after observing x1:t and y1:t are ηt = [ht,ct,yt]. Given a new example x,y, and the posterior at time t, the updated posterior can be computed recursively ht+1,ct+1 = LSTM([x,yt],ht,ct) (11) yt+1 = y (12) where LSTM([x,yt],h,c) carries out the LSTM update rules for hidden and cell states given input [x,yt]. We depart from the architecture proposed in [21] and include both the hidden state ht and the current encoded input zt as input to the decoder f which outputs the statistics of the posterior predictive distribution ˆyt ∼N(µt,Σt). µt,st = f(ht,zt,wf) (13) Σ = diag(exp(st)) (14) where f is a single hidden layer feed-forward network with weightswf. This functional form ensures that the covariance matrix of the posterior predictive remains positive deﬁnite. By including zt as input to the decoder, we lessen the information that needs to be stored in the hidden state, as it no longer needs to also encode the posterior predictive density for y|xt, just the posterior on the latent task. This was found to substantially improve performance and learning stability. The parameters that are optimized during meta-training are the weights of the encoder and decoder w,wf, as well as the parameters of the LSTM gates. The LSTM meta-learner makes few assumptions on the structure of the probabilistic model of the unobserved task parameter. For example, it does not by design satisfy the exchangeability criterion ensuring that the order of the context data does not change the posterior. This makes it a ﬂexible algorithm that, e.g., can handle unobserved latent states that have dynamics (both slow varying and switching behavior, in theory). However, empirically we ﬁnd the lack of this structure can make these models harder to train. Indeed, the more structured algorithms introduced in the following sections outperformed the LSTM meta-learner on many of our experiments. B.2 ALPaCA ALPaCA [16] is a meta-learning approach for which the base learning model is Bayesian linear regression in a learned feature space, such that y|x∼N(KTφ(x,w),Σϵ). We ﬁx the prior K ∼MN( ¯K0,Σϵ,Λ−1 0 ). In this matrix-normal prior, ¯K0 ∈Rnφ×ny is the prior mean and Λ0 is a nφ ×nφ precision matrix (inverse of the covariance). Given this prior and data model, the posterior may be recursively computed as follows. First, we deﬁne Qt = Λ−1 t ¯Kt. Then, the one step posterior update is Λ−1 t+1 = Λ−1 t −(Λ−1 t φ(xt+1))(Λ−1 t φ(xt+1))T 1 + φT(xt+1)Λ−1 t φ(xt+1) , (15) Qt+1 = yt+1φT(xt+1) + Qt (16) and the posterior predictive distribution is pθ(ˆyt+1 |x1:t+1,y1:t) = N(µ(xt+1),Σ(xt+1)), (17) where µ(xt+1) = (Λ−1 t Qt)Tφ(xt+1) and Σ(xt+1) = (1 + φT(xt+1)Λ−1 t φ(xt+1))Σϵ. To summarize, ALPaCA is a meta learning model for which the posterior statistics are ηt = {Qt,Λ−1 t }, and the recursive update rule h(x,y,η) is given by (16). The parameters that are meta-learned are the prior statistics, the feature network weights, and the noise covariance: θ = {¯K0,Λ0,w,Σϵ}. Note that, as is typical in regression, ALPaCA only models the conditional density p(y|x), assuming that p(x) is independent of the underlying task. 13B.3 PCOC In PCOC we process labeled input/class pairs (xt,yt) by encoding the input through an embedding network zt = φ(xt; w), and performing Bayesian density estimation for every class. Speciﬁcally, we assume a Categorical-Gaussian generative model in this embedding space, and impose the conjugate Dirichlet prior over the class probabilities and a Gaussian prior over the mean for each class, yt ∼Cat(p1,...,p ny), p 1,...,p ny ∼Dir(α0), zt |yt ∼N(¯zyt,Σϵ,yt), ¯zyt ∼N(µyt,0,Λ−1 yt,0). Given labeled context data (xt,yt), the algorithm updates its belief over the Gaussian mean for the corresponding class, as well as its belief over the probability of each class. As with ALPaCA, these posterior computations can be performed through closed form recursive updates. Deﬁning qi,t = Λi,tµi,t, we have αt = αt−1 + 1yt, qyt,t = qyt,t−1 + Σ−1 ϵ,ytφ(xt), Λyt,t = Λyt,t−1 + Σ−1 ϵ,yt (18) where 1i denotes a one-hot vector with a one at index i. Terms not related to class yt are left unchanged in this recursive update. Given this set of posterior parameters ηt = {αt,q1:J,t,Λ1:J,t}, the posterior predictive density in the embedding space can be computed as p(y|ηt) = αy,t/(∑J i=1αi,t) p(z,y |ηt) = p(y|ηt)N(z; Λ−1 y,tqy,t,Λ−1 y,t + Σϵ,y) where N(z; µ,Σ) denotes the Gaussian pdf with mean µand covariance Σ evaluated at z. Applying Bayes rule, the posterior predictive on yt+1 given zt+1 is p(ˆy|x1:t+1,y1:t) = p(zt+1,ˆy|ηt)∑ y′ p(zt+1,y′|ηt), (19) where zt+1 = φ(xt+1). This generative modeling approach also allows computing p(zt+1 |ηt) by simply marginalizing out yfrom the joint density of p(z,y), p(zt+1 |ηt) = J∑ y=1 p(y)N(zt+1; µt,Λ−1 y,t + Σϵ,y) As this only depends on the input x, we can use this likelihood within MOCA to update the run length belief upon seeing xt and before predicting ˆyt. In summary, PCOC leverages Bayesian Gaussian discriminant analysis, meta-learning the parameters θ= {α0,q1:J,0,Λ1:J,0,w,Σϵ,1:J}for efﬁcient few-shot online classiﬁcation. In practice, we assume that all the covariances are diagonal to limit memory footprint of the posterior parameters. Discussion. PCOC extends a line of work on meta-classiﬁcation based on prototypical networks [40]. This framework maps the context data to an embedding space, after which it computes the centroid for each class. For a new data point, it models the probability of belonging to each class as the softmax of the distances between the embedded point and the class centroids, for some distance metric. For Euclidean distances (which the authors focus on), this corresponds to performing frequentist estimation of class means, under the assumption that the variance matrix for each class is the identity matrix2. Indeed, this corresponds to the cheapest-to-evaluate simpliﬁcation of PCOC. [35] propose adding a class-dependent length scale (which is a scalar), which corresponds to meta- learning a frequentist estimate of the variance for each class. Moreover, it corresponds to assuming a variance that takes the form of a scaled identity matrix. Indeed, assuming diagonality of the covariance matrix results in substantial performance improvement as the matrix inverse may be performed element-wise. This reduces the numerical complexity of this operation in the (frequently high-dimensional) embedding space from cubic to linear. In our implementation of MOCA, we assume diagonal covariances throughout, resulting in comparable computational complexity to the different ﬂavors of prototypical networks. If one were to use dense covariances, the computational performance decreases substantially (due to the necessity of expensive matrix inversions), especially in high dimensional embedding spaces. 2[40] discuss this correspondence, as they outline how the choice of metric corresponds to a different assumptions on the distributions in the embedding space. 14In contrast to this previous work, PCOC has several desirable features. First, both [ 40] and [35] make the implicit assumption that the classes are balanced, whereas we perform online estimation of class probabilities via Dirichlet posterior inference. Beyond this, our approach is explicitly Bayesian, and we maintain priors over the parameters that we estimate online. This is critical for utilization in the MOCA framework. Existence of these priors allows “zero-shot” learning—it enables a model to classify incoming data to a certain class, even if no data belonging to that class has been observed within the current task. Finally, because the posteriors concentrate (the predictive variance decreases as more data is observed), we may better estimate when a change in the task has occurred. We also note that maximum likelihood estimation of Gaussian means is dominated by the James-Stein estimator [41], which shrinks the least squares estimator toward some prior. Moreover, the James-Stein estimator paired with empirical Bayesian estimation of the prior—which is the basis for Bayesian meta-learning approaches such as ALPaCA and PCOC—has been shown to be a very effective estimator in this problem setting [8]. C Experimental Details C.1 Problem Settings Sinusoid. To test the performance of the MOCA framework combined with ALPaCA for the regression setting, we investigate a switching sinusoid regression problem. The standard sinusoid regression problem, in which randomly sampled phase and amplitude constitute a task, is a standard benchmark in meta-learning [10]. Moreover, a switching sinusoid problem is a popular benchmark in continuous learning [19, 23]. Each task consists of a randomly sampled phase in the range [0,π] and amplitude in [0.1,5]. This task was investigated for varying hazard rates. For the experiments in this paper, samples from the sinusoid had additive zero-mean Gaussian noise of variance 0.05. Wheel Bandit. As a second, more practical regression example, we investigate a modiﬁed version of the wheel bandit presented in [37]. This bandit has been used to evaluate several Bayesian meta- learning algorithms [15, 34], due to the fact that the problem requires effective exploration (which itself relies on an accurate model of the posterior). We will outline the standard problem, and then discuss our modiﬁed version. The wheel problem is a contextual bandit problem in which a state x = [ x1,x2]T is sampled uniformly from the unit ball. The unit ball is split into two regions according to a radius δ∈[0,1], and into four quadrants (for details, see [37]). There are ﬁve actions, a0,...,a 4. The ﬁrst, a0 always results in reward rm. The other four actions each have one associated quadrant. For state xin quadrant 1, with ∥x∥>δ, a1 returns rh, and all other actions return reward rl. Actions a2,a3,a4 all return rl. If ∥x∥≤ δ, a1 also returns rl. In quadratic 2, a2 returns rh for x>δ, and so on. Critically, E[rl] <E[rm] <E[rh]. In summary, a0 always returns a medium reward, whereas actionsa1,...,a 4 return high reward for the correct quadrant outside of the (unknown) radius δ, and otherwise return low reward. We make several modiﬁcations to the setting to be better suited to the switching bandit setting. The standard wheel bandit problem is focused on exploration over long horizons. In the standard problem, the radius of the wheel is ﬁxed, and an algorithm must both learn the structure of the problem and infer the radius. In meta-learning-based investigations of the problem, a collection of wheel bandit problems with different radii are provided for training. Then, at test time, a new problem with a previously unseen radius is provided, an the decision-making agent must correctly infer the radius. In our switching setting, the radius of the wheel changes sharply, randomly in time. The radius was sampled δ∼U[0,1] in previous work [15, 34]. In our setting, with probability λat each time step (the hazard), the radius is re-sampled from this uniform distribution. Thus, the agent must constantly be inferring the current radius. Note that in this problem, only a small subset of states allow for meaningful exploration. Indeed, if the problem switches from radius δ1 to δ2, only xsuch that ∥x∥∈ [δ1,δ2] provides information about the switch. Thus, this problem provides an interesting domain in which changepoint detection is difﬁcult and necessarily temporally delayed. In addition to changing the sampling of the radius, we also change the reward function. As in [37], the rewards are deﬁned as ri ∼N (µi,σ2) for i = l,m,h . In [ 37, 15, 34], µl = 1.0, µm = 1.2, and µ3 = 50.0; σ = 0.01. This reward design results in agents necessarily needing to accurately identifying the radius of the problem, as for states outside of this value they may take the high reward action, and otherwise the agent takes action a0, resulting in reward of (approximately) 1.2. While this results in an interesting exploration versus exploitaion problems in the long horizon, the relatively 15greedy strategy of always choosing the action corresponding the quadrant of the state (potentially yielding high reward) performs well over short horizons. Thus, we modiﬁed the reward structure to make the shorter horizon problem associated with the switching bandit more interesting. In particular, we set µl = 0 .0, µm = 1 .0, µ3 = 2 .0 and σ = 0 .5. Thus, while the long horizon exploration problem is less interesting, a greedy agent performs worse over the short horizon. Moreover, the substantially higher noise variance increases the difﬁculty of the radius inference problem as well as the changepoint inference problem. NBA Player Movement. The behavior of basketball players is well described as a sequence of distinct plays ("tasks"), e.g. running across the court or driving in towards the basket. As such, predicting a player’s movement requires To generate data, we extracted 8 second trajectories of player movement sampled at 12.5 Hz from games from the 2015-2016 NBA season3. For the training data, we used trajectories from two games randomly sampled from the dataset: the November 11th, 2015 game between the Orlando Magic and the Chicago Bulls, and the December 12, 2015 game between the New Orleans Pelicans and the Chicago Bulls. The validation data was extracted from the November 7th, 2015 game between the New Orleans Pelicans and the Dallas Mavericks. The test set was trajectories from the November 6th game between the Milwaukee Bucks and the New York Knicks. The input xt was the player’s(x,y) position at time t, scaled down by a factor of 50. The labels were the unscaled changes in position, yt = 50(xt+1 −xt). The scaling was performed to convert the inputs, with units of feet and taking on values ranging from 0-100, to values that are more amenable for training with standard network initialization. Rainbow MNIST . The Rainbow MNIST dataset (introduced in [ 11]) contains 56 different color/scale/rotation transformations of the MNIST dataset, where one transformation constitutes a task. We split this dataset into a train set of 49 transformations and a test set of 7. For hyperparameter optimization, we split the train set into a training set of 42 transformations and a validation of 7. However, because the dataset represents a fairly small amount of tasks (relative to the sinusoid problem, which has inﬁnite), after hyperparameters were set we trained on all 49 tasks. We found this notably improved performance. Note that the same approach was used in [40]. miniImageNet. We use the miniImageNet dataset of [44], a standard benchmark in few-shot learning. However, the standard few-shot learning problem does not require data points to be assigned to a certain class label. Instead, given context data, the goal is to associated the test data with the correct context data. We argue that this problem setting is implausible for the continual learning setting: while observing a data stream, you are also inferring the set of possible labels. Moreover, after a task change, there is no context data to associate a new point with. Therefore we instead assume a known set of classes. We group the 100 classes of miniImageNet in to ﬁve super-classes, and perform ﬁve-way classiﬁcation given these. These super-classes vary in intra-class diversity of sub-classes: for example, one of the super-class is entirely composed of sub-classes that are breeds of dogs, while another corresponds to buildings, furniture, and household objects. Thus, the strength of the prior information for each super-class varies. Moreover, the intra-class similarities are quite weak, and thus generalization from the train set to the test set is difﬁcult and few-shot learning is still necessary and beneﬁcial. The super-classes are detailed in table 1. The super-classes are roughly balanced in terms of number of classes contained. Each task correspond to sampling a class from within each super-class, which was ﬁxed for the duration of that task. Each super-class was sampled with equal probability. C.2 Baselines Four baselines were used, described below: • Train on Everything: This baseline consists of ignoring task variation and treating the train- ing timeseries as one dataset. Note that many datasets contain latent temporal information that is ignored, and so this approach is effectively common practice. • Condition on Everything: This baseline maintains only one set of posterior statistics and continuously updates them with all past data, ηt = f(x1:t,y1:t). For recurrent network based meta-learning algorithms like the LSTM meta-learner, it is possible that the LSTM can learn to detect a task switch and reset automatically. Thus, we use this baseline only in experiments with the LSTM meta-learner to highlight how MOCA’s principled Bayesian 3The data was accessed and processed using the scripts provided here: https://github.com/ sealneaward/nba-movement-data 16Class Description Train/Val/Test Synsets 1 Non-dog animals Train n01532829, n01558993, n01704323, n01749939, n01770081, n01843383, n01910747, n02074367, n02165456, n02457408, n02606052, n04275548 Validation n01855672, n02138441, n02174001 Test n01930112, n01981276, n02129165, n02219486, n02443484 2 Dogs, foxes, wolves Train n02089867, n02091831, n02101006, n02105505, n02108089, n02108551, n02108915, n02111277, n02113712, n02120079 Validation n02091244, n02114548 Test n02099601, n02110063, n02110341, n02116738 3 Vehicles, musical instruments, nature/outdoors Train n02687172, n02966193, n03017168, n03838899, n03854065, n04251144, n04389033, n04509417, n04515003, n04612504, n09246464, n13054560 Validation n02950826, n02981792, n03417042, n03584254, n03773504, n09256479 Test n03272010, n04146614 4 Food, kitchen equipment, clothing Train n02747177, n02795169, n02823428, n03047690, n03062245, n03207743, n03337140, n03400231, n03476684, n03527444, n03676483, n04596742, n07584110, n07697537, n07747607, n13133613 Validation n03770439, n03980874 Test n03146219, n03775546, n04522168, n07613480 5 Building, furniture, household items Train n03220513, n03347037, n03888605, n03908618, n03924679, n03998194, n04067472, n04243546, n04258138, n04296562, n04435653, n04443257, n04604644, n06794110 Validation n02971356, n03075370, n03535780 Test n02871525, n03127925, n03544143, n04149813, n04418357 Table 1: Our super-class groupings for miniImageNet experiments. runlength estimation serves to add a useful inductive bias in settings with switching tasks, and leads to improved performance even in models that may theoretically learn the same behavior. • Oracle: In this baseline, the same ALPaCA and PCOC models were used as in MOCA, but with exact knowledge of the task switch times. Note that within a regret setting, one typically compares to the best achievable performance. The oracle actually outperforms the best achieveable performance in this problem setting, as it takes at least one data point (and the associated prediction, on which loss is incurred) to become aware of the task variation. • Sliding Window: The sliding window approach is commonly used within problems that exhibit time variation, both within meta-learning [31] and continual learning [19, 13]. In this approach, the last ndata points are used for conditioning, under the expectation that the most recent data is the most predictive of the observations in the near future. Typically, some form of validation is used to choose the window length, n. As MOCA is performing a form of adaptive windowing, it should ideally outperform any ﬁxed window length. We compare to three window lengths (n= 5,10,50), each of which are well-suited to part of the range of hazard rates that we consider. C.3 Training Details The training details are described below for each problem. For all problems, we used the Adam [25] optimizer. Sinusoid. A standard feedforward network consisting of two hidden layers of 128 units was used with ReLU nonlinearities. These layers were followed by a 32 units layer and another tanh nonlinearity. Finally, the output layer (for which we learn a prior) was of size 32 ×1. The same architecture was used for all baselines. This is the same architecture for sinusoid regression as was used in [16] (with the exception of using ReLU nonlinearities instead of all tanh nonlinearities). The following parameters were used for training: • Learning rate: 0.02 17• Batch size: 50 • Batch length: 100 • Train iterations: 7500 Batch length here corresponds to the number of timesteps in each training batch. Note that longer batch lengths are necessary to achieve good performance on low hazard rates, as short batch lengths artiﬁcially increase the hazard rate as a result of the assumption that each batch begins with a new task. The learning rate was decayed every 1000 training iterations. We allowed the noise variance to be learned by the model. This, counter-intuitively, resulted in a substantial performance improvement over a ﬁxed (accurate) noise variance. This is due to a curriculum effect, where the model early one increases the noise variance and learns roughly accurate features, followed by slowly decreasing the noise variance to the correct value. Wheel Bandit. For all models, a feedforward network consisting of four hidden layers with ReLU nonlinearities was used. Each of these layers had 64 units, and the output dimension of the network was 100. There was no activation used on the last layer of the network. The actions were encoded as one-hot and passed in with the two dimensional state as the input to the network (seven dimensional input in total). The following parameters were used for training: • Learning rate: 0.005 • Batch size: 15 • Batch length: 100 • Train iterations: 2000 and the learning rate was decayed every 500 training iterations. We allow the noise variance to be learned by the model. We use the same amount of training data as was used in [15]: 64 ×562 samples. In [15], this was 64 different bandits, each with 562 data points. We use the same amount of data, but generated as one continuous stream with the bandit switching according to the hazard rate. We use a validation set of size 16 ×562, also generated as one trajectory, but did not use any form of early termination based on the validation set. In [37, 15] data was collected by random action sampling. To generate a dataset that matches the test conditions slightly better, we instead sample a random action with probability 0.5, and otherwise sample the action correspond to the quadrant in which the state was sampled. This results in more training data in which high rewards are achieved. This primarily resulted in smoother training. The combined MOCA and ALPaCA models provide a posterior belief over the reward. This posterior must be mapped to an action selection at each time that sufﬁciently trades off greedy exploitation (maximizing reward) and exploration (information gathering actions). A common and effective heuristic in the bandit literature is Thompson sampling, in which a reward function is sampled from the posterior distribution at each time, and this sampled function is optimized over actions. This approach was applied in the changing bandit setting by [30]. Other common approaches to action selection typically rely on some form of optimism, in which the agent aims to explore possible reward functions that may perform better than the expectation of the posterior. These methods typically use concentration inequalities to derive an upper bound on the reward function. These methods have been applied in switching bandits in [14] and others. We follow [30] and use Thompson sampling the main experimental results, primarily due to its simplicity (and thus ease of reproduction, for the sake of comparison). However, because the switching rate between reward functions is relatively high, it is likely that optimistic methods (which typically have a short-term bias) would outperform Thompson sampling. As the action sampling is not a core contribution of the paper, we use Thompson sampling for simplicity. Moreover, this approach meshes well with the Gaussian mixture posterior predictive (which is easily sampled from). For completeness, we present experiments in section D in which we investigate optimistic action selection methods. NBA Player Movement. For this experiment, we used the LSTM meta-learner, with the encoder φ(x,w) deﬁned as a 3 hidden layer feedforward network with a hidden layer size of 128, and a feature dimension nφ = 32. The LSTM had a dimension of 64, and used a single hidden layer feedforward network as the decoder. ALPaCA did not perform as well as the LSTM model here; we hypothesize that this is due to the LSTM model being able to account for unobserved state variables that change with time, in contrast to ALPaCA, which assumes all unobserved state variables are task parameters and hence static for the duration of a task. 18Figure 6: The performance of MOCA with ALPaCA on the sinusoid regression problem. Bottom: The belief over run length versus time. The intensity of each point in the plot corresponds to the belief in run length at the associated time. The red lines show the true changepoints. Top: Visualizations of the posterior predictive density at the times marked by blue dotted lines in the bottom ﬁgure. The red line denotes the current function (task), and red points denote data from the current task. Green points denote data from previous tasks, where more faint points are older. a) A visualization of the posterior at an arbitrary time. b) The posterior for a case in which MOCA did not successfully detect the changepoint. In this case, it is because the pre- and post-change tasks (corresponding to ﬁgure a and b) are very similar. c) An instance of a multimodal posterior. d) The changepoint is initially missed due to the data generated from the new task having high likelihood under the previous posterior. e) After an unlikely data point, the model increases its uncertainty as the changepoint is detected. The following parameters were used for training: • Learning rate: 0.01 • Batch size: 25 • Batch length: 150 • Train iterations: 5000 The learning rate was decayed every 1000 training iterations. Rainbow MNIST. In our experiments, we used the same architecture as was used as in [40, 44]. It is often unclear in recent work on few-shot learning whether performance improvements are due to improvements in the meta-learning scheme or the network architecture used (although these things are not easily disentangled). As such, the architecture we use in this experiment provides fair comparison to previous few-shot learning work. This architecture consists of four blocks of 64 3 ×3 convolution ﬁlters, followed by a batchnorm, ReLU nonlinearity and 2 ×2 max pool. On the last conv black, we removed the batchnorm and the nonlinearity. For the 28 ×28 Rainbow MNIST dataset, this encoder leads to a 64 dimensional embedding space. For the “train on everything” baseline, we used the same architecture followed by a fully connected layer and a softmax. This architecture is standard for image classiﬁcation and has a comparable number of parameters to our model. We used a diagonal covariance factorization within PCOC, substantially reducing the number of terms in the covariance matrix for each class and improving the performance of the model (due to the necessary inversion of the posterior predictive covariance). We learned a prior mean and variance for each class, as well as a noise covariance for each class (again, diagonal). We also ﬁxed the Dirichlet priors to be large, effectively imbuing the model with the knowledge that the classes were balanced. The following parameters were used for training: 19Figure 7: Left: A visualization of samples from the reward function for randomly sampled states and action a1. Middle: The mean of the reward function posterior predictive distribution at time t = 135in an evaluation run (hazard 0.02). Right: The run length belief for the same evaluation run. Red lines denote the true changepoints. • Learning rate: 0.02 • Batch size: 10 • Batch length: 100 • Train iterations: 5000 The learning rate was decayed every 1500 training iterations. miniImageNet. Finally, for miniImageNet, we used six convolution blocks, each as previously described. This resulted in a 64 dimensional embedding space. We initially attempted to use the same four-conv backbone as for Rainbow MNIST, but the resulting 1600 dimensional embedding space had unreasonable memory requirements for batches lengths of 100. Again, for the “train on everything” baseline, we used the same architectures with one fully connected layer followed by a softmax. The following parameters were used for training: • Learning rate: 0.002 • Batch size: 10 • Batch length: 100 • Train iterations: 3000 The learning rate was decayed every 1000 training iterations. We used the validation set to monitor performance, and as in [ 5], we used the highest validation accuracy iteration for test. We also performed data augmentation as in [5] by adding random reﬂections and color jitter to the training data. C.4 Test details. For sinusoid, rainbow MNIST, and miniImageNet, a test horizon of 400 was used. Again, the longest possible test horizon was used to avoid artiﬁcial distortion of the test hazard rate. For these problems, a batch of 200 evaluations was performed. For the bandit, we evaluated on 10 trials of length 1000. For the NBA dataset, we obtained quantitative results by evaluated on 200 sequences of horizon 150. We chose a sequence of length 200 for qualitative visualization. D Further Experimental Results In this section we present a collection of experimental results investigating task and computational performance of MOCA, as well as hyperparameters of the algorithm and modiﬁed problem settings. D.1 Visualizing MOCA Posteriors Posteriors for the sinusoid and the bandit problem are provided in Fig. 6 and Fig. 7. These are visualized as they represents two ends of the spectrum; identifying changes in the sinusoid model is extremely easy, as a large amount of information is provided on possible changes for every datapoint. On the other hand, as discussed previously, only a small subset of points in the bandit problem are informative about the possible occurance of a changepoint. Accordingly, the run length belief in Fig. 6 is nearly exactly correct are concentrated on a particular run length. In contrast to this, the run length belief in Fig. 7 is less concentrated. Indeed, highly multimodal beliefs can be seen as well as the model placing a non-trivial amount of weight on many hypotheses. Finally, while some 20Figure 8: Regret compared to optimal action selection for optimistic action selection with three samples (left) and ﬁve (right) samples. Figure 9: Performance change from augmenting a model trained with MOCA with task supervision at test time (violet) and from using changepoint estimation at test time for a model trained with task-supervision (teal), for sinusoid (left), Rainbow MNIST (middle), and miniImageNet (right). changepoints are detected near immediately in the bandit problem, some take a handful of timesteps passing before the changepoint is detected. Interestingly, because MOCA maintains a belief over all possible run lengths, changepoints which are initially missed may be retrospectively identiﬁed, as can partially be seen starting around time 65 in Fig. 7. D.2 Action Selection Schemes in the Wheel Bandit In the body of the paper, we used Thompson sampling for action selection due to the simplicity of the method, as well as favorable perforamance in previous work on switching bandits [30]. However, optimism-based methods have also been effective in the switching bandit problem [14]. The MOCA posterior is a mixture of Gaussians, and thus many existing optimism-based bandit methods are not directly applicable. To investigate optimism-based action selection methods, we investigate a method in which we sample a collection of reward functions from the posterior, and choose the best action across all sampled reward models. Fig. 8 shows regret versus hazard for sampling three and ﬁve reward functions, respectively. The performance difference between MOCA and sliding window methods at low hazards is similar for Thompson sampling and for optimistic methods, as is the reversion of near-identical performance at high hazards. Compared to a standard (non-switching) bandit problem, the posterior will not concentrate to a point in the limit of inﬁnite timesteps as there is always some weight on the prior (as the problem could switch at any timestep). This impacts optimism-based exploration methods: in the limit of a large number of samples, the prior will dominate for all states. Efﬁcient exploration methods in the switching bandit remain an active research topic, especially paired with changepoint detection methods [30, 14, 17]. D.3 MOCA with Differing Train/Test Task Supervision To more closely analyze the difference between MOCA performance, which must infer task switches both at train-time and at test-time, and the oracle model, which has task segmentation information in 21Figure 10: Test negative log likelihood of MOCA on the sinusoid problem with partial task segmentation. The partial segmentation during training results in negligible performance increase, while partial supervision at test time uniformly improves performance. Note that each column corresponds to one trained model, and thus the randomly varying performance across train supervision rates may be explained by simply results of minor differences in individual models. both phases, we also compared against performance when task segmentation was provided at only one of these phases. We discuss the results of these comparisons for each of the experiments for which oracle task supervision was available below. Sinusoid. Fig. 9 shows the performance of MOCA when augmented with task segmentation at test time (violet), compared to unsegmented (blue), as well as the oracle model without test segmentation (teal) compared to with test segmentation (gray). We ﬁnd that as the hazard rate increases, the value of both train-time and test-time segmentation increases steadily. Because our regression version of MOCA only models the conditional density, it is not able to detect a changepoint before incurring the loss associated with an incorrect prediction. Thus, for high hazard rates with many changepoints, the beneﬁts of test-time task segmentation are increased. Interestingly and counter-intuitively, the model trained with MOCA outperforms the model trained with task segmentation when both are given task segmentation at test time. We hypothesize that this is due to MOCA having improved training dynamics. Early in training, an oracle model may produce posteriors that are highly concentrated but incorrect, yielding very large losses that can destabilize training. In contrast, MOCA always places a non-zero weight on the prior, mitigating these effects. We ﬁnd that we can match MOCA’s performance by artiﬁcially augmenting to the oracle model’s loss with a small weight (down to 10−16) on the prior likelihood, supporting this hypothesis. Rainbow MNIST. In Fig. 9, the relative effect of the train and test segmentation is visible. Looking at the effect of train-time segmentation in isolation, comparing blue to teal and violet to gray, we see that the beneﬁt of train-time segmentation is most pronounced at higher hazard rates. The effect of test segmentation (comparing blue to violet and teal to gray) is minimal, indicating MOCA is effectively able to detect task switches prior to making predictions. miniImageNet. Fig. 9 shows that, in contrast to the Rainbow MNIST experiment, there is a large and constant (with respect to hazard rate) performance decrease moving from oracle to MOCA at test time. Interestingly, while one would expect the performance decrease with increasing hazard rate to be attributable primarily to lack of test-time segmentation, this trend is primarily a consequence of MOCA training, consistent with the Rainbow MNIST experiments. This is likely a consequence of the limited amount of data, as the trend is not apparent for the sinusoid experiment. D.4 MOCA with Partial Task Segmentation Since MOCA explicitly reasons about a belief over run-lengths, it can operate anywhere in the spectrum of the task-unsegmented case as presented so far, to the fully task-segmented setting of standard meta-learning. At every time step t, the user can override the belief bt(rt) to provide a degree of supervision. At known changepoints, for example, the user can override bt(rt) to have 22Figure 11: Time per iteration versus iteration number at test time. Note that the right hand side of the curve shows the expected linear complexity expected of MOCA. Note that for these experiments, no hypothesis pruning was performed, and thus at test time performance could be constant time as opposed to linear. This ﬁgure shows 95% conﬁdence intervals for 10 trials, but the repeatability of the computation time is consistent enough that they are not visible. all its mass on rt = 0. If the task is known not to change at the given time, the user can set the hazard probability to 0 when updating the belief for the next timestep. If a user applies both of these overrides, it amounts to effectively sidestepping the Bayesian reasoning over changepoints and revealing this information to the meta-learning algorithm. If the user only applies the former, the user effectively indicates to the algorithm when known changepoints occur, but the algorithm is free to propagate this belief forward in time according to the update rules, and detect further changepoints that were not known to the user. Finally, the Bayesian framework allows a supervisor to provide their belief over a changepoint, which may not have probability mass entirely at rt = 0. Thus, MOCA ﬂexibly incorporates any type of task supervision available to a system designer. Fig. 10 shows the performance of partial task segmentation at both train and test for the sinusoid problem, for the hazard rate 0.2. This problem was chosen as the results were highly repeatable and thus the trend is more readily observed. Here, we label a changepoint with some probability, which we refer to as the supervision rate. We do not provide supervision for any non-changepoint timesteps, and thus a supervision rate of 1 corresponds to labeling every changepoint but is not equivalent to the oracle. Speciﬁcally, the model may still have false positive changepoints, but is incapable of false negatives. This ﬁgure shows that the performance monotonically improves with increasing train supervision rate, but is largely invariant under varying train supervision. This performance improvement agrees with Fig. 9, which shows that for the sinusoid problem, performance is improved by full online segmentation. Indeed, these results show that training with MOCA results in models with comparable test performance to those with supervised changepoints, and thus there is little marginal value to task segmentation during training. D.5 Computational Performance Fig. 11 shows the computational performance at test time on the sinusoid problem. Note that the right hand side of the curve shows a linear trend that is expected from the growing run length belief vector. However, even for 25000 iterations, the execution time is approximately 7ms for one iteration. These experiments were performed on an Nvidia Titan Xp GPU. Interestingly, on the left hand side of the curve, the time per iteration is effectively constant until the number of iterations approaches approximately 4500. Based on our code proﬁling, we hypothesize that this is an artifact of overhead in matrix multiplication computations done on the GPU. D.6 Batch Training MOCA In practice, we sample batches of length T from the full training time series, and train on these com- ponents. While this artiﬁcially increases the observed hazard rate (as a result of the initial belief over 23Figure 12: Performance versus the training horizon (T) for the sinusoid with hazard 0.01. The lowest hazard was used to increase the effects of the short training horizon. A minor decrease in performance is visible for very small training horizons (around 20), but ﬂattens off around 100 and above. It is expected that these diminishing marginal returns will occur for all systems and hazard rates. run length being 0 with probability 1), it substantially reduces the computational burden of training. Because MOCA maintains a posterior for each possible run length, computational requirements grow linearly with T. Iterating over the whole training time series without any hypothesis pruning can be prohibitively expensive. While a variety of different pruning methods within BOCPD have been proposed [46, 38], we require a pruning method which does not break model differentiability. Note that at test-time, we no longer require differentiability and so previously developed pruning methods may be applied. Empirically, we observe diminishing marginal returns when training on longer sequences. Fig. 12 shows the performance of MOCA for varying training sequence lengths ( T). In all experiments presented in the body of the paper, we use T = 100. As discussed, small T values artiﬁcially inﬂate the observed hazard rate, so we expect to see performance improve with larger T values. Fig. 12 shows that this effect results in diminishing marginal returns, with little performance improvement beyond T = 100. Longer training sequences lead to increased computation per iteration (as MOCA is linear in the runlength), as well as an increased memory burden (especially during training, when the computation graph must be retained by automatic differentiation frameworks). Thus, we believe it is best to train on the shortest possible sequences, and propose T = 1/λ(where λis the hazard rate) as a rough rule of thumb. 24
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Bayesian online changepoint detection",
        "Continuous adaptation via meta-learning in nonstationary and competitive environments",
        "Task-free continual learning",
        "Meta-learning with stochastic linear bandits",
        "A closer look at few-shot classification",
        "Lifelong machine learning",
        "ImageNet: A Large-Scale Hierarchical Image Database",
        "Stein’s estimation rule and its competitors—an empirical Bayes approach",
        "On-line inference for multiple changepoint problems",
        "Model-agnostic meta-learning for fast adaptation of deep networks",
        "Online meta-learning",
        "Catastrophic forgetting in connectionist networks",
        "A survey on concept drift adaptation",
        "On upper-conﬁdence bound policies for switching bandit problems",
        "Neural processes",
        "Meta-learning priors for efﬁcient online Bayesian regression",
        "Change point detection and meta-bandits for online learning in dynamic environments",
        "Introduction to online convex optimization",
        "Task agnostic continual learning via meta learning",
        "Long short-term memory",
        "Learning to learn using gradient descent",
        "Generative modeling of multimodal multi-human behavior",
        "Meta-learning representations for continual learning",
        "Online gradient-based mixtures for transfer modulation in meta-learning",
        "Adam: A method for stochastic optimization",
        "Overcoming catastrophic forgetting in neural networks",
        "Spatio-temporal bayesian on-line changepoint detection with model selection",
        "Learning without forgetting",
        "Recurrent switching linear dynamical systems",
        "Thompson sampling in switching environments with bayesian online change detection",
        "Learning to adapt in dynamic, real-world environments through meta- reinforcement learning",
        "Deep online learning via meta-learning: Continual adaptation for model-based RL",
        "Empirical Bayesian change point detection",
        "Amortized bayesian meta-learning",
        "Meta-learning for semi-supervised few-shot classification",
        "Learning to learn without forgetting by maximizing transfer and minimizing interference",
        "Deep bayesian bandits showdown: An em- pirical comparison of bayesian deep networks for thompson sampling",
        "Gaussian process change point models",
        "Fair meta-learning: Learning how to learn fairly",
        "Prototypical networks for few-shot learning",
        "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution",
        "Learning to learn",
        "Adaptive sequential bayesian change point detection",
        "Matching networks for one shot learning",
        "Learning to reinforcement learn",
        "Bayesian online learning of the hazard rate in change-point problems"
    ]
}
