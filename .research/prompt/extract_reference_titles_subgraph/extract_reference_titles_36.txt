
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective Yu-Bang Zheng1 Xi-Le Zhao2,* Junhua Zeng3,4 Chao Li4 Qibin Zhao4 Heng-Chao Li1 Ting-Zhu Huang2 1School of Information Science and Technology, Southwest Jiaotong University, China 2School of Mathematical Sciences, University of Electronic Science and Technology of China, China 3School of Automation, Guangdong University of Technology, China 4Tensor Learning Team, RIKEN Center for Advanced Intelligence Project (AIP), Japan zhengyubang@163.com, xlzhao122003@163.com, jh.zenggdut@gmail.com, chao.li@riken.jp qibin.zhao@riken.jp, hcli@home.swjtu.edu.cn, tingzhuhuang@126.com Abstract Tensor network (TN) representation is a powerful tech- nique for computer vision and machine learning. TN struc- ture search (TN-SS) aims to search for a customized struc- ture to achieve a compact representation, which is a chal- lenging NP-hard problem. Recent “sampling-evaluation”- based methods require sampling an extensive collection of structures and evaluating them one by one, resulting in pro- hibitively high computational costs. To address this issue, we propose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN), which allows us to efficiently solve the TN-SS problem from a regularized modeling per- spective, eliminating the repeated structure evaluations. To be specific, by inserting a diagonal factor for each edge of the fully-connected TN, SVDinsTN allows us to calculate TN cores and diagonal factors simultaneously, with the fac- tor sparsity revealing a compact TN structure. In theory, we prove a convergence guarantee for the proposed method. Experimental results demonstrate that the proposed method achieves approximately100∼1000 times acceleration com- pared to the state-of-the-art TN-SS methods while maintain- ing a comparable level of representation ability. 1. Introduction Tensor network (TN) representation, which aims to express higher-order data with small-sized tensors (called TN cores) by a specific operation among them, has gained significant attention in various areas of data analysis [1, 11, 25, 37], machine learning [5, 9, 27], computer vision [21, 30, 35, 36, 39], etc. By regarding TN cores as nodes and oper- *Corresponding author. ations as edges, a TN corresponds to a graph (called TN topology). The vector composed of the weights of all edges in the topology is defined as the TN rank. TN structure (in- cluding topology and rank) search (TN-SS) aims to search for a suitable TN structure to achieve a compact represen- tation for a given tensor, which is known as a challenging NP-hard problem [13, 18]. The selection of TN structure dramatically impacts the performance of TN representation in practical applications [12, 15, 17, 18]. Recently, there have been several notable efforts to tackle the TN-SS problem [12, 15, 17, 18]. But most of them adopt the “sampling-evaluation” framework, which requires sam- pling a large number of structures as candidates and con- ducting numerous repeated structure evaluations. For in- stance, for a tensor of size40 ×60 ×3 ×9 ×9 (used in Sec- tion 4.2), TNGA in [15] requires thousands of evaluations and TNALE in [18] requireshundreds of evaluations, where each evaluation entails solving an optimization problem to compute TN cores iteratively. Consequently, the computa- tional cost becomes exceedingly high. A meaningful ques- tion is whether we can optimize the TN structure simultane- ously during the computation of TN cores, thus escaping the “sampling-evaluation” framework and fundamentally ad- dressing the computationally consuming issue. In this paper, we introduce for the first time a regularized modeling perspective on solving the TN-SS problem. This perspective enables us to optimize the TN structure simul- taneously during the computation of TN cores, effectively eliminating the need for repetitive structure evaluations. To be specific, we propose a novel TN paradigm, termed as SVD-inspired TN decomposition (SVDinsTN) , by inserting diagonal factors between any two TN cores in the “fully- connected” topology (see Figure 1(b)). The intuition behind SVDinsTN is to leverage the sparsity of the inserted diago- arXiv:2305.14912v6  [cs.LG]  9 Apr 2024Run time (s) Compression Ratio  (5.64%, 104s)  (7.55%, 1366s)  (4.52%, 25050s)  (20.9%, 619s)  (26.9%, 3835s)  0%  30% 5×101 5×10 35×10 2 5×10 4 5×10 5 10% 20% A D G E F B (5.01%, 140200s)  (4.73%, 75510s)  C Ours A TNGreedy E TRALS G TNGA D FCTNALS F TNALE B TNLS C (c) Performance comparison of different methods (b) SVD-inspired TN decomposition X I1 I2 I3 I5 I4 G1 I1 G2 I2 G3 I3 G4 I4 G5 I5 S12 S23 S3đ4 S15 S4đ5 S13 S25 S24 4 S 1  4 S 3  5 S t , l G k Diagonal factors  TN cores  X I 1 I 2 G 1 I 1 G 2 I 2 S 1,2  (a) SVD Figure 1. (a) A graphical illustration of SVD. (b) A graphical illustration of SVD-inspired TN decomposition on a fifth-order tensor. (c) Comparison of the compression ratio ( ↓) and run time ( ↓) of different methods on a fifth-order light field image Knights, where the reconstruction error bound is set to 0.05, TRALS [38] and FCTNALS [41] are methods with pre-defined topologies, and TNGreedy [12], TNGA [15], TNLS [17], and TNALE [18] are TN-SS methods (please see more results in Table 3). nal factors to reveal a compact TN structure and utilize the TN cores (merged with the diagonal factors) to represent a given tensor. Based on SVDinsTN, we establish a regular- ized model, which updates the TN cores and diagonal fac- tors iteratively and imposes a sparse operator to induce the sparsity of the diagonal factors. In theory, we prove a con- vergence guarantee for the proposed method and establish an upper bound for the TN rank. In particular, we design a novel initialization scheme for the proposed method based on the upper bound. This initialization scheme enables the proposed method to overcome the high computational cost in the first several iterations, which is caused by the utiliza- tion of a “fully-connected” topology as the starting point. As a result, SVDinsTN is capable of capturing a customized TN structure and providing a compact representation for a given tensor in an efficient manner. In summary, we make the following three contributions. • We propose SVDinsTN, a new TN paradigm, that enables us to optimize the TN structure during the computation of TN cores, greatly reducing the computational cost. • In theory, we prove a convergence guarantee for the pro- posed method and establish an upper bound for the TN rank involved in SVDinsTN. The upper bound serves as a guide for designing an efficient initialization scheme. • Experimental results verify numerically that the proposed method achieves100∼1000 times acceleration compared to the state-of-the-art TN-SS methods with a comparable representation ability (see Figure 1(c)). 1.1. Related Works TN representation1 aims to find a set of small-sized TN cores to express a large-sized tensor under a given TN struc- 1We focus on TN representation in scientific computing and machine learning, while acknowledging its history of research in physics [7, 23, 31]. ture (including topology and rank) [4, 5, 31]. In the past decades, many works focused on TN representation with a fixed TN topology, such as tensor train (TT) decomposition with a “chain” topology [24], tensor ring (TR) decompo- sition with a “ring” topology [38], fully-connected tensor network (FCTN) decomposition with a “fully-connected” topology [41], etc. In addition, these works also presented various methods to optimize the TN cores, such as alternat- ing least square (ALS) [38], gradient descent (GD) [32, 34], proximal alternating minimization (PAM) [41, 42], etc. In contrast, SVDinsTN can reveal a compact TN structure for a given tensor, surpassing methods with pre-defined topolo- gies in terms of representation ability. TN structure search (TN-SS) aims to search for a suit- able or optimal TN structure, including both topology and rank, to achieve a compact representation for a given ten- sor [8, 12, 15, 17–20, 22, 26]. However, the majority of existing TN-SS methods follow the “sampling-evaluation” framework, which necessitates the use of heuristic search algorithms like the greedy algorithm [12], genetic algo- rithm [15], and alternating local enumeration algorithm [18] to sample candidate structures and subsequently evaluate them individually. Therefore, these methods inevitably suf- fer from prohibitively high computational costs due to the numerous repeated evaluations, each involving the iterative calculation of TN cores within an optimization problem. In contrast, SVDinsTN addresses the TN-SS problem from a regularized modeling perspective, thereby avoiding the re- peated structure evaluations and significantly reducing com- putational costs. 2. Notations and Preliminaries A tensor is a multi-dimensional array, and the number of dimensions (also called modes) of which is referred to asTable 1. Several operations and their interpretations. Operation Interpretation diag diag( X) returns a column vector formed from the elements on the main diagonal of X when the input variable is a diagonal matrix, and diag(x) returns a diagonal matrix whose main diagonal is formed from the elements of x when the input variable is a column vector. ones ones( I1, I2, ··· , IN ) returns an I1 × I2 × ··· ×IN tensor, whose elements are all equal to 1. zeros zeros( I1, I2, ··· , IN ) returns an I1 × I2 × ··· ×IN tensor, whose elements are all equal to 0. vec vec( X) returns a column vector by lexicographical reordering of the elements of X. the tensor order . In the paper, first-order tensors (vec- tors), second-order tensors (matrices), and Nth-order ten- sors are denoted by x ∈ RI1 , X ∈ RI1×I2 , and X ∈ RI1×I2×···×IN , respectively. We use ∥X∥F and ∥X∥1 to denote the Frobenius norm and ℓ1-norm of X, respectively. To simplify the explanation, we let x1:d denote the ordered set {x1, x2, ··· , xd}, KN denote the set{1, 2, ··· , N}, and TLN denote the set {(t, l)|1 ≤ t < l≤ N; t, l∈ N}. We next review several operations on tensors [41]. The generalized tensor transposition[41] is an operation that rearranges tensor modes. For example, an I1 × I2 × I3 × I4 tensor can be transposed into an I3 × I2 × I1 × I4 tensor, denoted by ⃗Xn with n = (3 , 2, 1, 4). We use ⃗Xn = permute(X, n) and X = ipermute( ⃗Xn, n) to de- note the corresponding transposition operation and its in- verse operation, respectively. The generalized tensor unfolding [41] is an operation that converts a tensor into a matrix by merging a group of tensor modes into the rows of the matrix and merging the remaining modes into the columns. For example, an I1 ×I2 ×I3 ×I4 tensor can be unfolded into anI1I3 ×I2I4 matrix. We use X[1,3;2,4] = GUnfold( X, (1, 3; 2, 4)) and X = GFold( X[1,3;2,4], (1, 3; 2, 4)) to denote the corre- sponding unfolding operation and its inverse operation, re- spectively. We also use X(2) to simply denote X[2;1,3,4] ∈ RI2×I1I2I4 , which is also called mode-2 unfolding. We use X(2) = Unfold( X, 2) and X = Fold(X(2), 2) to denote the corresponding mode-2 unfolding operation and its in- verse operation, respectively [14]. The tensor contraction [41] is an operation that obtains a new tensor by pairing, multiplying, and summing indices of certain modes of two tensors. For example, if a fourth- order tensor X ∈ RI1×I2×I3×I4 and a third-order tensor Y ∈RJ1×J2×J3 satisfy I2 = J1 and I4 = J2, then the tensor contraction between the 2nd and 4th modes ofX and the 1st and 2nd modes ofY yields a tensor Z = X ×1,2 2,4 Y ∈ RI1×I3×J3 . The elements of Z are calculated as follows: Z(i1, i3, j3)= XI2 i2=1 XI4 i4=1 X(i1, i2, i3, i4)Y(i2, i4, j3). In Table 1, we give the interpretations of the operations “diag”, “ones”, “zeros”, and “vec”. 2.1. Tensor Network In general, atensor network (TN)is defined as a set of small- sized tensors, known as TN cores, in which some or all modes are contracted according to specific operations [5]. The primary purpose of a TN is to represent higher-order data using these TN cores. By considering TN cores as nodes and operations between cores as edges, we define the graph formed by these nodes and edges as the TN topology. Additionally, we assign a non-negative integer weight to each edge to indicate the size of the corresponding mode of TN cores, and call the vector composed of these weights the TN rank. Consequently, a TN structure refers to a weighted graph, encompassing both the TN topology and TN rank. This paper focuses on only a class of TNs that employs tensor contraction as the operation among TN cores and adopts a simple graph as the TN topology. More particu- larly, when representing an Nth-order tensor X, this class of TNs comprises precisely N TN cores, each correspond- ing to one mode of X. A notable method is FCTN de- composition, which represents an Nth-order tensor X ∈ RI1×I2×···×IN by N small-sized Nth-order cores denoted by Gk ∈ RR1,k×R2,k×···×Rk−1,k×Ik×Rk,k+1×···×Rk,N for k ∈ KN [41]. In this decomposition, any two cores Gl and Gt for (t, l) ∈ TLN share an equal-sized mode Rt,l used for tensor contraction. We denote the above FCTN de- composition by X = FCTN(G1:N ) and define the FCTN rank as the vector (R1,2, R1,3, ··· , R1,N , R2,3, ··· , R2,N , ··· , RN−1,N ) ∈ RN(N−1)/2. According to the concept of tensor contraction, removing rank-one edges in the TN topology does not change the expression of the TN. This means that if any element in the FCTN rank is equal to one, the corresponding edge can be harmlessly eliminated from the “fully-connected” topology. For instance, a “fully- connected” topology with the rank (R1,2, 1, ··· , 1, R2,3, 1, ··· , 1, RN−2,N−1, RN−1,N ) can be converted into a “chain” topology with rank (R1,2, R2,3, ··· , RN−1,N ) in this manner. This fact can be formally stated as follows. Property 1 [16] There exists a one-to-one correspondence between the TN structure and FCTN rank. According to Property 1, we can search for a compact TN structure by optimizing the FCTN rank.3. An Efficient Method for TN-SS We propose an efficient method to solve the TN-SS problem from a regularized modeling perspective. Unlike the exist- ing “sampling-evaluation” framework, the main idea of the proposed method is to optimize the TN structure (the FCTN rank) simultaneously during the computation of TN cores, thereby eliminating the repetitive structure evaluations and greatly decreasing the computational cost. 3.1. SVDinsTN We start with the definition of the following SVDinsTN. Definition 1 (SVDinsTN) Let X ∈RI1×I2×···×IN be an Nth-order tensor such that X(i1, i2, ··· , iN ) = R1,2X r1,2=1 R1,3X r1,3=1 ··· R1,NX r1,N=1 R2,3X r2,3=1 ··· R2,NX r2,N=1 ··· RN−1,NX rN−1,N=1 S1,2(r1,2, r1,2)S1,3(r1,3, r1,3) ··· S1,N (r1,N , r1,N ) S2,3(r2,3, r2,3) ··· S2,N (r2,N , r2,N ) ··· SN−1,N (rN−1,N , rN−1,N ) G1(i1, r1,2, r1,3, ··· , r1,N ) G2(r1,2, i2, r2,3, ··· , r2,N ) ··· Gk(r1,k, r2,k, ··· , rk−1,k, ik, rk,k+1, ··· , rk,N ) ··· GN (r1,N , r2,N , ··· , rN−1,N , iN ), (1) where Gk ∈ RR1,k×R2,k×···×Rk−1,k×Ik×Rk,k+1×···×Rk,N for ∀k ∈ KN are Nth-order tensors and called TN cores, and St,l ∈ RRt,l×Rt,l for ∀(t, l) ∈ TLN are diagonal ma- trices. Then we call (1) an SVD-inspired TN decomposition (SVDinsTN) of X, denoted by X = STN(G, S), where G denotes {Gk|k ∈ KN } and S denotes {St,l|(t, l) ∈ TLN }. As shown in Figure 1(b), SVDinsTN includes both TN cores and diagonal factors, and can use the sparsity of diag- onal factors to reveal a compact TN structure and utilize TN cores (merged with diagonal factors) to represent a tensor. Remark 1 (SVDinsTN & SVD) As shown in Figure 1(a)- (b), SVDinsTN extends the “core & diagonal factor & core” form of SVD to higher-order cases, incorporating the idea of determining rank through non-zero elements in the diag- onal factor. In particular, SVDinsTN can degrade into SVD in second-order cases when TN cores satisfy orthogonality. Remark 2 (SVDinsTN & FCTN) SVDinsTN builds upon FCTN decomposition [41] but can reveal the FCTN rank. It achieves this by inserting diagonal factors between any two TN cores in FCTN decomposition and leveraging the number of non-zero elements in the diagonal factors to de- termine the FCTN rank. In particular, SVDinsTN can trans- form into a TN decomposition by merging the diagonal fac- tors into TN cores through the tensor contraction operation. 3.2. A Regularized Method for TN-SS We present an SVDinsTN-based regularized method, which updates TN cores and diagonal factors alternately, and im- poses a sparse operator to induce the sparsity of diagonal factors to reveal a compact TN structure. We consider anℓ1-norm-based operator for diagonal fac- tors S and Tikhonov regularization [10] for TN coresG. The ℓ1-norm-based operator is used to promote the sparsity ofS, and the Tikhonov regularization is used to constrict the fea- sible range of G. Mathematically, the proposed model can be formulated as follows: min G,S 1 2∥X −STN(G, S)∥2 F + µ 2 X k∈KN ∥Gk∥2 F + X (t,l)∈TLN λt,l∥St,l∥1, (2) where λt,l > 0 and µ >0 are regularization parameters. We use the PAM-based algorithm [2] to solve (2), whose solution is obtained by alternately updating    Gk =argmin Gk 1 2∥X −STN(G, S)∥2 F + µ 2 ∥Gk∥2 F + ρ 2∥Gk − ˆGk∥2 F , ∀k ∈ KN , St,l =argmin St,l 1 2∥X −STN(G, S)∥2 F +λt,l∥St,l∥1 + ρ 2∥St,l−ˆSt,l∥2 F , ∀(t, l) ∈ TLN , (3) where ρ >0 is a proximal parameter (we fix ρ = 0.001), and ˆGk and ˆSt,l are the solutions of the Gk-subproblem and St,l-subproblem at the previous iteration, respectively. 1) Update Gk for ∀k ∈ KN : Solving the Gk-subproblem requires fixing the other TN cores and diagonal factors. To address this, we use Mk to denote the matrix obtained by performing tensor contraction and unfolding operations on all diagonal factors and TN cores except Gk. Algorithm 1 presents a way to compute Mk. We can obtain X(k) = Gk(k)Mk. In this way, the Gk-subproblem can be rewritten as follows: min Gk(k) 1 2∥X(k) − Gk(k)Mk∥2 F + µ 2 ∥Gk(k)∥2 F + ρ 2∥Gk(k) − ˆGk(k)∥2 F . (4) The objective function of (4) is differentiable, and thus its solution can be obtained by Gk(k) =   X(k)MT k +ρ ˆGk(k)   MkMT k +(µ+ρ)I −1 . (5) 2) Update St,l for ∀(t, l) ∈ TLN : Solving the St,l- subproblem requires fixing the other diagonal factors and TN cores. In a similar fashion, we use Ht,l to denote theAlgorithm 1 Mk = STN   {Gq}N q=1, {St,l}t,l∈N 1≤t<l≤N , /Gk  . Input: Gq ∈ RR1,q×R2,q×···×Rq−1,q×Iq×Rq,q+1×···×Rq,N for ∀q ∈ KN and q ̸= k; St,l ∈ RRt,l×Rt,l for ∀(t, l) ∈ TLN ; and an index k ∈ KN . Initialization: a = (k + 1 :N, 1 :k). 1: for i = 1to k − 1 and i = k + 1to N − 1 do 2: for j = i + 1to N do 3: Let Gi = Gi ×1 i+1 Si,j. 4: end for 5: if i > kthen 6: Let Gi = Gi ×1 k Sk,i. 7: Let Gi = permute(Gi, (1 :k − 1, N, k: N − 1)). 8: end if 9: Let Gi = permute(Gi, a). 10: end for 11: Let Mk = Ga(1), m1 = 1, and n1 = 2. 12: for i = 1to N − 2 do 13: Let Mk = Mk ×m1,m2,···,mi n1,n2,···,ni Ga(i+1). 14: Let mj = j for j = 1, 2, ··· , i+ 1. 15: Let nj = 2 + (j − 1)(N − i) for j = 1, 2, ··· , i+ 1. 16: end for 17: Let Mk = permute(Mk, (2(N −k) + 1 : 2(N − 1), 1 : 2(N − k))). 18: Let c = zeros(1, N− 1) and d = zeros(1, N− 1). 19: for i = i to N − 1 do 20: Let c(i) = 2i and d(i) = 2i − 1 21: end for 22: Let Mk = GUnfold(Mk, c; d). Output: Matrix Mk ∈ R Qk−1 i=1 Ri,k QN i=k+1 Rk,i×QN i=1,i̸=k Ii. matrix obtained by performing tensor contraction and un- folding operations on all TN cores and diagonal factors ex- cept St,l. Algorithm 2 presents a way to compute Ht,l. We can obtain x = Ht,lst,l, where x = vec( X) and st,l = diag(St,l). Then, the St,l-subproblem can be rewrit- ten as follows: min st,l 1 2∥x−Ht,lst,l∥2 F +λt,l∥st,l∥1+ ρ 2∥st,l−ˆst,l∥2 F . (6) We use an alternating direction method of multipliers (ADMM) [6] to solve the St,l-subproblem, which can be rewritten as follows: min st,l,qt,l 1 2∥x−Ht,lqt,l∥2 F +λt,l∥st,l∥1+ ρ 2∥st,l−ˆst,l∥2 F s.t. st,l−qt,l =0, (7) where qt,l is an auxiliary variable. The augmented La- grangian function of (7) can be expressed as the following concise form: Lβt,l(st,l, qt,l, pt,l)= 1 2∥x−Ht,lqt,l∥2 F +λt,l∥st,l∥1 + ρ 2∥st,l−ˆst,l∥2 F + βt,l 2 st,l−qt,l+ pt,l βt,l  2 F , (8) Algorithm 2 Ht,l = STN   {Gk}N k=1, {Sp,q}p,q∈N 1≤p<q≤N , /St,l  . Input: Gk ∈ RR1,k×R2,k×···×Rk−1,k×Ik×Rk,k+1×···×Rk,N for ∀k ∈ KN ; Sp,q ∈ RRp,q×Rp,q for ∀(p, q) ∈ TLN , and (p, q) ̸= (t, l); and an index (t, l) ∈ TLN . 1: for i = 1to t − 1 and i = t + 1to N − 1 do 2: for j = i + 1to N do 3: Let Gi = Gi ×1 i+1 Si,j. 4: end for 5: end for 6: for j = t + 1to l − 1 do 7: Let Gt = Gt ×1 t+1 St,j. 8: end for 9: for j = l + 1to N do 10: Let Gt = Gt ×1 t+2 St,j. 11: end for 12: Let Gt = permute(Gt, (1 :t, t+ 2 :l, t+ 1, l+ 1 :N)). 13: Let Gt = Unfold(Gt, l) and Gl = Unfold(Gl, t). 14: Let Ht,l = zeros(QN k=1 Ik, Rt,l). 15: for i = 1to Rt,l do 16: Let Gt = Fold(Gt(i, :), l) and Gl = Fold(Gl(i, :), t). 17: Let Ht,l(:, i) = vec(FCTN({Gk}N k=1)). 18: end for Output: Matrix Ht,l ∈ R QN k=1 Ik×Rt,l. where pt,l is the Lagrangian multiplier and βt,l > 0 is the penalty parameter. Within the ADMM framework,qt,l, st,l, and pt,l can be solved by alternately updating    qt,l = argmin qt,l Lβt,l(st,l, qt,l, pt,l), st,l = argmin st,l Lβt,l(st,l, qt,l, pt,l), pt,l = pt,l + βt,l(st,l − qt,l). (9) That is,    qt,l =  HT t,lHt,l+βt,lI −1 HT t,lx+βt,lst,l+pt,l  , st,l =shrink ρˆst,l+βt,lqt,l−pt,l ρ+βt,l , λt,l ρ+βt,l  , pt,l =pt,l+βt,l(st,l−qt,l), (10) where shrink(a, b) = max(a − b, 0) + min(a + b, 0). We describe the pseudocode to optimize model (2) in Al- gorithm 3. Below, we present a brief analysis of the compu- tational complexity and provide a theoretical convergence guarantee for the developed algorithm. Computational complexity. For simplicity, we let the size of the Nth-order tensor X be I × I × ··· ×I and the initial rank be (R, R,··· , R) satisfied R ≤ I. The compu- tational cost involves updatingG and S, resulting in costs of O   N PN k=2 IkRk(N−k)+k−1+NI N−1R2(N−1)+N3IRN  and O   N2 PN k=2 IkRk(N−k)+k−1+N4IRN +N2IN R2 , respectively. Hence, the computational cost at each iteration is O   N2 PN k=2 IkRk(N−k)+k−1+N4IRN +N2IN R2 .Algorithm 3 PAM-based algorithm to optimize model (2). Input: A tensor X ∈RI1×I2×···×IN and a parameter γ. Initialization: Initialize St,l and Rt,l by the initialization scheme in Section 3.3 and let βt,l = 1for ∀(t, l) ∈ TLN ; let Gk = 1/√Ik ones(R1,k, R2,k, ··· , Rk−1,k, Ik, Rk,k+1, ··· , Rk,N ) for ∀k ∈ KN and µ = 1. 1: while not converged do 2: Let ˆX = X and λt,l = γ max(St,l)(ρ + βt,l). 3: Update Gk(k) by (5) and let Gk = Fold(Gk(k), k). 4: for i = 1to 5 do 5: Update qt,l, st,l, and pt,l by (10). 6: end for 7: Delete zero elements in st,l, let St,l = diag(st,l), and define the size of st,l as Rt,l. 8: Delete the corresponding dimensions of Gk and let X = STN(G, S). 9: Check the convergence condition: ∥X− ˆX∥F ∥ ˆX∥F < 10−5. 10: end while Output: Gk for ∀k ∈ KN , and St,l and Rt,l for ∀(t, l) ∈ TLN . Theorem 1 (Convergence guarantee) The sequence gen- erated by Algorithm 3, denoted by {G(s), S(s)}s∈N, con- verges to a critical point of the optimization problem (2). 3.3. Initialization Scheme SVDinsTN encounters high computational cost in the first several iterations if the TN rank Rt,l for ∀(t, l) ∈ TLN are initialized with large values. This is due to the adoption of a “fully-connected” topology as a starting point. To solve this challenge, we design a novel initialization scheme aimed at effectively reducing the initial values of the TN rank. We first give an upper bound for the TN rank, by which we then design an initialization scheme for the TN rankRt,l and diagonal factors St,l ∈ RRt,l×Rt,l for ∀(t, l) ∈ TLN . Theorem 2 Let X ∈RI1×I2×···×IN be an Nth-order ten- sor, then there exists an SVDinsTN (1) with the TN rank Rt,l ≤ min(rank(X(t)), rank(X(l))) for ∀(t, l) ∈ TLN . Theorem 2 indicates that min(rank(X(t)), rank(X(l))) can be the initial value of the TN rank Rt,l. For real-world data, this value is usually embodied by the rank of mode- (t, l) slices2 of X. Therefore, we initialize Rt,l and St,l by virtue of truncated SVD of mode- (t, l) slices of X, which consists of the following two steps. Step 1: We first calculate the mean of all mode- (t, l) slices of X and denote it by Xt,l. Then we perform SVD on Xt,l to obtain st,l ∈ Rmin(It,Il), whose elements are singu- lar values of Xt,l. Step 2: We first let st,l = shrink   st,l, γ max(st,l) |st,l|+10−16  and delete zero elements in st,l. Then we let St,l = diag(st,l) 2Mode-(t, l) slices are obtained by fixing all but the mode- t and the mode-l indexes of a tensor [40]. and define the size of st,l as Rt,l. In practical applications, the shrink operation in Step 2 effectively reduces the initial value of Rt,l by projecting very small singular values in st,l to zero. As a result, the challenge of high computational costs in the first several it- erations of SVDinsTN can be effectively addressed(see Fig- ure 2 for a numerical illustration). 4. Numerical Experiments In this section, we present numerical experiments on both synthetic and real-world data to evaluate the performance of the proposed SVDinsTN. The primary objective is to vali- date the following three Claims: A: SVDinsTN can reveal a customized TN structure that aligns with the unique structure of a given tensor. B: SVDinsTN can greatly reduce time costs while achiev- ing a comparable representation ability to state-of-the- art TN-SS methods. Moreover, SVDinsTN can also sur- pass existing tensor decomposition methods with pre- defined topologies regarding representation ability. C: SVDinsTN can outperform existing tensor decomposi- tion methods in the tensor completion task, highlighting its effectiveness as a valuable tool in applications. 4.1. Experiments for Validating Claim A We conduct experiments to validate Claim A. Since real- world data lacks a true TN structure, we consider only syn- thetic data in this experiment. Data generation. We first randomly generate Gk for ∀k ∈ KN and St,l for ∀(t, l) ∈ TLN , whose elements are taken from a uniform distribution between 0 and 1. Then we obtain the synthetic tensor by X = STN(G, S). Experiment setting. We test both fourth-order tensors of size 16 × 18 × 20 × 22 and fifth-order tensors of size 14 ×16 ×18 ×20 ×22, and consider different kinds of TN structures. For each structure, we conduct 100 independent tests and regenerate the synthetic data to ensure reliable and unbiased results. The ability of the proposed SVDinsTN to reveal TN structure is measured by the success rate of the output structures, defined as ST/T ×100%, where T = 100 is the total number of tests and ST is the number of tests that accurately output the true TN structure. In all tests, the parameter γ is set to 0.0015. Table 2 presents the success rate of the output TN struc- tures obtained by the proposed SVDinsTN in 100 indepen- dent tests on fourth-order and fifth-order tensors. It can be observed that the proposed SVDinsTN consistently yields high success rates of over 95% in all test cases. Notably, in approximately half of the test cases, the success rates reach a perfect score of 100%. Moreover, it is also worth mentioning that in the test on fifth-order tensors, we con- sider two isomorphic topologies: the “ring” topology andTable 2. Performance of SVDinsTN on TN structure revealing under 100 independent tests. True structure (4th-order) G1 G2 G3 G4 I1 I3 I4 I2 4 3 2 2 2 3 G1 G2 G3 G4 I1 I3 I4 I2 3 4 4 3 2 G1 G2 G3 G4 I1 I3 I4 I2 4 3 2 3 G1 G2 G3 G4 I1 I3 I4 I2 3 4 4 G1 G2 G3 G4 I1 I3 I4 I2 3 4 2 Success rate 100% 100% 96% 95% 99% True structure (5th-order) G1 G2 G3 I1 G5 G4 I4I3 I2 I5 4 2 3 2 2 3 3 2 2 2 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 3 34 2 3 2 2 3 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 4 2 3 2 3 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 3 2 3 4 3 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 Success rate 100% 98% 96% 97% 100% the “five-star” topology. These two topologies are both the “ring” topology (TR decomposition), but with differ- ent permutations: G1 → G2 → G3 → G4 → G5 → G1 and G1 →G3 →G5 →G2 →G4 →G1, respectively. It can be seen that despite the isomorphism, the proposed SVDinsTN can identify the correct permutation for each topology. 4.2. Experiments for Validating Claim B We conduct experiments to validate Claim B. We consider both real-world data and synthetic data, and use different methods to represent it in this experiment. Experiment setting. We test three light field data 3, named Bunny, Knights, and Truck, which are fifth-order ten- sors of size 40 × 60 × 3 × 9 × 9 (spatial height ×spatial width×color channel×vertical grid×horizontal grid). We employ six representative methods as the compared base- lines, including two methods with pre-defined topology: TRALS [38] and FCTNALS [41], and four TN-SS methods: TNGreedy [12], TNGA [15], TNLS [17], and TNALE [18]. We represent the test light field data by different methods and calculate the corresponding compression ratio (CR) to achieve a certain reconstruction error (RE) bound. The CR is defined as FG/FX × 100%, where FG is the number of elements of TN cores used to represent a tensor and FX is the number of total elements of the original tensor. The RE is defined as ∥X −˜X∥F /∥X∥F , where X is the original data and ˜X is the reconstructed data. In all tests, we select the parameter γ from the interval [10−7, 10−3]. Result analysis. Table 3 reports CR and run time of dif- ferent methods on fifth-order light field data. The results show that the proposed SVDinsTN achieves significantly lower CRs than TRALS and FCTNALS, which are methods with pre-defined topology. This indicates that SVDinsTN can obtain a more compact structure than the pre-defined one. Furthermore, while SVDinsTN requires the determi- nation of diagonal factors alongside TN cores, its iterative process generates progressively simpler structures, enhanc- 3http://lightfield.stanford.edu/lfs.html Table 3. Comparison of CR ( ↓) and run time ( ×1000s, ↓) of dif- ferent methods on light field data. Method RE bound: 0.01 RE bound: 0.05 RE bound: 0.1 CR Time CR Time CR Time Bunny TRALS [38] 60.5% 13.54 17.4% 0.471 5.31% 0.118 FCTNALS [41] 65.1% 13.08 20.9% 0.473 3.93% 0.041 TNGreedy [12] 26.1% 11.02 6.32% 1.021 2.34% 0.362 TNGA [15] 27.9% 1014 5.01% 180.3 2.25% 12.52 TNLS [17] 24.3% 1402 4.26% 63.70 2.16% 24.53 TNALE [18] 26.3% 144.5 4.52% 18.36 2.26% 3.064 SVDinsTN 22.4% 0.745 6.92% 0.029 2.66% 0.005 Knights TRALS [38] 74.7% 10.31 26.9% 3.835 9.15% 0.423 FCTNALS [41] 73.5% 12.35 20.9% 0.619 3.93% 0.014 TNGreedy [12] 32.1% 12.53 7.55% 1.366 3.50% 0.481 TNGA [15] 38.7% 912.9 5.01% 140.2 2.44% 12.52 TNLS [17] 27.3% 1286 4.73% 75.51 2.15% 5.320 TNALE [18] 27.6% 266.4 4.52% 25.05 2.10% 3.386 SVDinsTN 32.0% 1.548 5.64% 0.104 2.76% 0.019 Truck TRALS [38] 62.8% 17.62 22.6% 1.738 6.00% 0.090 FCTNALS [41] 69.3% 7.735 20.9% 2.953 3.93% 0.159 TNGreedy [12] 26.9% 6.676 7.26% 1.259 3.35% 0.488 TNGA [15] 27.9% 1029 5.01% 170.3 2.85% 14.83 TNLS [17] 26.4% 992.6 4.99% 119.8 2.57% 19.35 TNALE [18] 24.7% 239.3 5.77% 19.54 2.90% 5.160 SVDinsTN 23.5% 1.051 6.42% 0.152 2.83% 0.023 ing the computational efficiency. Consequently, SVDin- sTN demonstrates faster performance compared to TRALS and FCTNALS. Compared to the TN-SS methods, the pro- posed SVDinsTN achieves a substantial speed improve- ment while maintaining a comparable level ofCR. Remark- ably, SVDinsTN achieves an acceleration of approximately 100∼1000 times over TNGA, TNLS, and TNALE. This isTable 4. Comparison of CR ( ↓) and run time ( ×1000s, ↓) of SVDinsTN with different initializations on light field data Truck. Initialization RE bound: 0.01 RE bound: 0.05 RE bound: 0.1 CR Time CR Time CR Time Random 30.7% 1.203 8.17% 0.474 4.14% 0.208 Ours 23.5% 1.051 6.42% 0.152 2.83% 0.023 181s 149s 151s 3.35s 2.18s0 50 100 150 200 RE: 0.01 RE: 0.05 RE: 0.1 Truck  Run time  include “shrink”  exclude “shrink”  37.9s Figure 2. Comparison of the runtime in the first five iterations of SVDinsTN on light field dataTruck when including and excluding the shrink operation in our initialization scheme. because TNGreedy, TNGA, TNLS, and TNALE adopt the “sampling-evaluation” framework, necessitating a signifi- cant number of repeated structure evaluations. In contrast, SVDinsTN introduces a regularized modeling framework, requiring only a single evaluation. Impact of the initialization scheme. We analyze the impact of the initialization scheme. In Table 4, we report CR and run time of SVDinsTN with different initializations on light field data Truck. As observed, our initialization scheme achieves lower CRs compared to random initializa- tion, while maintaining higher efficiency. This corroborates that our initialization scheme can provide a favorable start- ing point and enhance computational efficiency. In partic- ular, even with random initialization, our method achieves significant acceleration compared to other TN-SS methods. We further analyze the impact of theshrink operation in our initialization scheme. In Figure 2, we present the run time comparison of the first five iterations of our method when including and excluding the shrink operation in our initial- ization scheme. As observed, the shrink operation in our initialization scheme enables our method to greatly reduce the computational costs in the first several iterations. Higher-order cases. We analyze whether the proposed SVDinsTN still performs well on higher-order tensors. We randomly generate 6th-, 8th-, and 10th-order tensors by us- ing the same procedure in Section 4.1. The size of each ten- sor mode is randomly selected from {5, 6, 7, 8}, the edge number of each TN is randomly selected from {6, 8, 10}, and the rank of each edge is randomly selected from{2, 3}. For each tensor order, we randomly generate 5 tensors. We compare SVDinsTN and baseline methods in terms of CR Table 5. Comparison of the CR ( ↓) and run time ( ×1000s, ↓) of different methods when reaching the RE bound of 0.01. The result is the average value of 5 independent experiments and “–” indicates “out of memory”. Method 6th-order 8th-order 10th-order CR Time CR Time CR Time TRALS [38] 1.35% 0.006 0.064% 0.034 – – FCTNALS [41] 2.13% 0.002 – – – – TNGreedy [12] 0.88% 0.167 0.016% 2.625 0.0008% 45.39 TNGA [15] 0.94% 3.825 0.024% 51.40 – – TNLS [17] 1.11% 0.673 0.038% 59.83 – – TNALE [18] 1.65% 0.201 0.047% 19.96 – – SVDinsTN 1.13% 0.002 0.016% 0.017 0.0007% 0.608 and run time when reaching the RE bound of 0.01, and show the results in Table 5. As observed, SVDinsTN is appli- cable to higher orders beyond 5, and even up to 10. The behind rational is the truncated SVD used in initialization restricts the initial values of the rank for each edge to a rel- atively small range, thus improving computational and stor- age efficiency (see Figure 2). As the iterations progress, the sparsity regularization in the model leads to progressively simpler learned structures, further boosting efficiency. 4.3. Experiments for Validating Claim C We conduct experiments to validate Claim C. We employ the proposed SVDinsTN to a fundamental application, i.e., tensor completion (TC), and compare it with the state-of- the-art tensor decomposition-based TC methods. Given an incomplete observation tensor F ∈RI1×I2×···×IN of X ∈ RI1×I2×···×IN , the proposed TC method first updatesG and S by Algorithm 3, and then updates the target tensor X as follows: X = PΩc((STN(G, S) +ρ ˆX)/(1 +ρ)) +PΩ(F), where Ω is the index set of the known elements, PΩ(X) is a projection operator that projects the elements in Ω to themselves and all others to zeros, ˆX is the result at the previous iteration, and the initial X is F. Experiment setting. We test four color videos4, named Bunny, News, Salesman, and Silent, which are fourth-order tensors of size 144 × 176 × 3 × 50 (spatial height×spatial width×color channel×frame). We employ six methods for comparison, named FBCP [37], TMac [29], TMacTT [3], TRLRF [33], TW [28], and TNLS 5 [17], respectively. We set the missing ratio (MR) to 90%, which is defined as the ratio of the number of missing elements to the total number of elements. We evaluate the reconstructed quality by the mean peak signal-to-noise ratio (MPSNR) computed across all frames. In all tests, the parameter γ is set to 0.0003. Result analysis. Table 6 reports MPSNR and run time 4http://trace.eas.asu.edu/yuv/ 5TNLS excels in the compression task; therefore, we use it as a repre- sentative TN-SS method for comparison.Table 6. Comparison of MPSNR (↑) and run time (in seconds, ↓) of different TC methods on color videos. Video FBCP [37] TMac [29] TMacTT [3] TRLRF [33] TW [28] TNLS [17] SVDinsTN MPSNR Time MPSNR Time MPSNR Time MPSNR Time MPSNR Time MPSNR Time MPSNR Time Bunny 28.402 1731.2 28.211 1203.5 29.523 453.76 29.163 486.76 30.729 1497.4 28.787 99438 32.401 691.33 News 28.234 1720.4 27.882 340.46 28.714 535.97 28.857 978.12 30.027 1426.3 29.761 37675 31.643 932.42 Salesman 29.077 1783.2 28.469 353.63 29.534 656.45 28.288 689.35 30.621 1148.7 30.685 76053 31.684 769.54 Silent 30.126 1453.9 30.599 316.21 30.647 1305.6 31.081 453.24 31.731 1232.0 28.830 98502 32.706 532.31 FBCP [37] TMac [29] TMacTT [3] TRLRF [33] TW [28] TNLS [17] SVDinsTN Ground truth 0 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 0 0.40.3 0.70.5 0.80.2 0.60.1 10.9 Figure 3. Reconstructed images and residual images obtained by different methods on the 25th frame of News. Here the residual image is the average absolute difference between the reconstructed image and the ground truth over R, G, and B channels. 0 200 400 600 800 1000 Iteration 0 0.05 0.1 0.15 0.2 0.25 0.3 Relative change Bunny 0 200 400 600 800 1000 Iteration 0 0.05 0.1 0.15 0.2 0.25 0.3 Relative change Silent Figure 4. Relative change curves with respect to the iteration num- ber on test color videosBunny and Silent. Here the relative change is defined as ∥X −ˆX∥F /∥ ˆX∥F , and X and ˆX are the results of the current iteration and its previous iteration. obtained by different TC methods. As observed, the pro- posed SVDinsTN consistently achieves the highestMPSNR values among all utilized TC methods across all test color videos. In Figure 3, we present the reconstructed images and their corresponding residual images at the 25th frame of News. We observe that the proposed SVDinsTN outper- forms the baseline methods in terms of visual quality, par- ticularly with respect to background cleanliness and local details (e.g. “dancer”) recovery. Numerical convergence. In Theorem 1, we provide a theoretical convergence guarantee for the proposed method. Here, we select color videos Bunny and Silent as examples to numerically verify the convergence. Figure 4 presents the relative change in the reconstructed color videos at each it- eration compared to their respective previous iterations. We observe that the values of the relative change achieved by the proposed method decrease and gradually tend to zero as the number of iterations increases. This justifies the numer- ical convergence of the proposed method. 5. Conclusion We propose a novel TN paradigm, called SVDinsTN, which enables us to solve the challenging TN-SS problem from a regularized modeling perspective. This perspective renders our model highly amenable to easy solutions, allowing us to leverage well-established optimization algorithms to solve the regularized model. As a result, the proposed method achieves about 100 ∼ 1000 times acceleration compared to the state-of-the-art TN-SS methods with a comparable rep- resentation ability. Besides, SVDinsTN demonstrates its ef- fectiveness as a valuable tool in practical applications. Limitations. In existing research on TN-SS, two chal- lenging issues remain open. One is the computationally consuming issue, and the other is the theoretical guaran- tee of the optimal TN structure. SVDinsTN addresses the computationally consuming issue. But the theoretical guar- antee of the optimal TN structure is still an open problem. Solving this issue will be the direction of our future work. Acknowledgements We would like to express our gratitude to Prof. Guillaume Rabusseau for his valuable assistance in correcting the ex- perimental results of “TNGreedy”.References [1] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research , 15(1): 2773–2832, 2014. 1 [2] H ´edy Attouch, J´erˆome Bolte, Patrick Redont, and An- toine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequal- ity. Mathematics of Operations Research, 35(2):438– 457, 2010. 4 [3] Johann A Bengua, Ho N Phien, Hoang Duong Tuan, and Minh N Do. Efficient tensor completion for color image and video recovery: Low-rank tensor train. IEEE Transactions on Image Processing, 26(5):2466– 2479, 2017. 8, 9 [4] Andrzej Cichocki, Danilo P. Mandic, Anh Huy Phan, Cesar F. Caiafa, Guoxu Zhou, Qibin Zhao, and Lieven De Lathauwer. Tensor decompositions for signal pro- cessing applications: From two-way to multiway com- ponent analysis. IEEE Signal Processing Magazine , 32(2):145–163, 2015. 2 [5] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh Huy Phan, Qibin Zhao, and Danilo P. Mandic. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor de- compositions. Foundations and Trends® in Machine Learning, 9(4-5):249–429, 2016. 1, 2, 3 [6] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers and Mathe- matics with Applications, 2(1):17–40, 1976. 5 [7] Silvano Garnerone, Thiago R. de Oliveira, and Paolo Zanardi. Typicality in random matrix product states. Physical Review A, 81:032336, 2010. 2 [8] Mehrdad Ghadiri, Matthew Fahrbach, Gang Fu, and Vahab Mirrokni. Approximately optimal core shapes for tensor decompositions. arXiv preprint arXiv:2302.03886, 2023. 2 [9] Ivan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eis- ert, and J. Ignacio Cirac. Expressive power of tensor- network factorizations for probabilistic modeling. In Advances in Neural Information Processing Systems , 2019. 1 [10] Gene H Golub, Per Christian Hansen, and Dianne P O’Leary. Tikhonov regularization and total least squares. SIAM journal on matrix analysis and appli- cations, 21(1):185–194, 1999. 4 [11] Kang Han and Wei Xiang. Multiscale tensor decom- position and rendering equation encoding for view synthesis. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR), pages 4232–4241, 2023. 1 [12] Meraj Hashemizadeh, Michelle Liu, Jacob Miller, and Guillaume Rabusseau. Adaptive learning of tensor network structures. arXiv preprint arXiv:2008.05437, 2020. 1, 2, 7, 8 [13] Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are NP-hard. Journal of the ACM , 60(6): 1–39, 2013. 1 [14] Tamara G Kolda and Brett W Bader. Tensor decom- positions and applications. SIAM Review, 51(3):455– 500, 2009. 3 [15] Chao Li and Zhun Sun. Evolutionary topology search for tensor network decomposition. In Proceedings of the 37th International Conference on Machine Learn- ing, pages 5947–5957, 2020. 1, 2, 7, 8 [16] Chao Li and Qibin Zhao. Is rank minimization of the essence to learn tensor network structure? In Second Workshop on Quantum Tensor Networks in Machine Learning (QTNML), Neurips, 2021. 3 [17] Chao Li, Junhua Zeng, Zerui Tao, and Qibin Zhao. Permutation search of tensor network structures via local sampling. In Proceedings of the 39th Inter- national Conference on Machine Learning , pages 13106–13124, 2022. 1, 2, 7, 8, 9 [18] Chao Li, Junhua Zeng, Chunmei Li, Cesar Caiafa, and Qibin Zhao. Alternating local enumeration (tnale): Solving tensor network structure search with fewer evaluations. In Proceedings of the 40th International Conference on Machine Learning, 2023. 1, 2, 7, 8 [19] Nannan Li, Yu Pan, Yaran Chen, Zixiang Ding, Dong- bin Zhao, and Zenglin Xu. Heuristic rank selec- tion with progressively searching tensor ring network. Complex & Intelligent Systems, 8(2):771–785, 2022. [20] Yipeng Liu, Yingcong Lu, Weiting Ou, Zhen Long, and Ce Zhu. Adaptively topological tensor network for multi-view subspace clustering. arXiv preprint arXiv:2305.00716, 2023. 2 [21] Yisi Luo, Xi-Le Zhao, Deyu Meng, and Tai-Xiang Jiang. Hlrtf: Hierarchical low-rank tensor factoriza- tion for inverse problems in multi-dimensional imag- ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19281–19290, 2022. 1 [22] Chang Nie, Huan Wang, and Le Tian. Adaptive tensor networks decomposition. In British Machine Vision Conference, 2021. 2 [23] Rom ´an Or ´us. A practical introduction to tensor net- works: Matrix product states and projected entangled pair states. Annals of Physics, 349:117–158, 2014. 2 [24] Ivan Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing , 33(5):2295–2317, 2011. 2[25] Piyush Rai, Yingjian Wang, Shengbo Guo, Gary Chen, David Dunson, and Lawrence Carin. Scalable Bayesian low-rank decomposition of incomplete mul- tiway tensors. In Proceedings of the 31st International Conference on International Conference on Machine Learning, page II–1800–II–1808, 2014. 1 [26] Farnaz Sedighin, Andrzej Cichocki, and Anh Huy Phan. Adaptive rank selection for tensor ring decom- position. IEEE Journal of Selected Topics in Signal Processing, 15(3):454–463, 2021. 2 [27] Moein Shakeri and Hong Zhang. Moving object detec- tion under discontinuous change in illumination using tensor low-rank and invariant sparse decomposition. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 7221–7230, 2019. 1 [28] Zhong-Cheng Wu, Ting-Zhu Huang, Liang-Jian Deng, Hong-Xia Dou, and Deyu Meng. Tensor wheel decomposition and its tensor completion application. In Advances in Neural Information Processing Sys- tems, pages 27008–27020, 2022. 8, 9 [29] Yangyang Xu, Ruru Hao, Wotao Yin, and Zhixun Su. Parallel matrix factorization for low-rank tensor com- pletion. Inverse Problems and Imaging, 9(2):601–624, 2015. 8, 9 [30] Ryuki Yamamoto, Hidekata Hontani, Akira Imakura, and Tatsuya Yokota. Fast algorithm for low-rank ten- sor completion in delay-embedded space. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 2048–2056, 2022. 1 [31] Ke Ye and Lek-Heng Lim. Tensor network ranks. arXiv preprint arXiv:1801.02662, 2018. 2 [32] Longhao Yuan, Jianting Cao, Xuyang Zhao, Qiang Wu, and Qibin Zhao. Higher-dimension tensor com- pletion via low-rank tensor ring decomposition. In Asia-Pacific Signal and Information Processing Asso- ciation Annual Summit and Conference , pages 1071– 1076, 2018. 2 [33] Longhao Yuan, Chao Li, Danilo Mandic, Jianting Cao, and Qibin Zhao. Tensor ring decomposition with rank minimization on latent space: An efficient approach for tensor completion. In Proceedings of the AAAI Conference on Artificial Intelligence , pages 9151–9158, 2019. 8, 9 [34] Longhao Yuan, Qibin Zhao, Lihua Gui, and Jianting Cao. High-order tensor completion via gradient-based optimization under tensor train format. Signal Pro- cessing: Image Communication, 73:53–61, 2019. 2 [35] Shipeng Zhang, Lizhi Wang, Lei Zhang, and Hua Huang. Learning tensor low-rank prior for hyper- spectral image reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 12001–12010, 2021. 1 [36] Xinyuan Zhang, Xin Yuan, and Lawrence Carin. Non- local low-rank tensor factor analysis for image restora- tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8232–8241, 2018. 1 [37] Qibin Zhao, Liqing Zhang, and Andrzej Cichocki. Bayesian CP factorization of incomplete tensors with automatic rank determination. IEEE Transactions on Pattern Analysis and Machine Intelligence , 37(9): 1751–1763, 2015. 1, 8, 9 [38] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring decomposition. arXiv preprint arXiv:1606.05535, 2016. 2, 7, 8 [39] Wen-Jie Zheng, Xi-Le Zhao, Yu-Bang Zheng, Jie Lin, Lina Zhuang, and Ting-Zhu Huang. Spatial-spectral- temporal connective tensor network decomposition for thick cloud removal.ISPRS Journal of Photogram- metry and Remote Sensing, 199:182–194, 2023. 1 [40] Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, Tai- Xiang Jiang, Teng-Yu Ji, and Tian-Hui Ma. Tensor N-tubal rank and its convex relaxation for low-rank tensor recovery. Information Sciences, 532:170–189, 2020. 6 [41] Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, Qibin Zhao, and Tai-Xiang Jiang. Fully-connected tensor network decomposition and its application to higher- order tensor completion. In Proceedings of the AAAI Conference on Artificial Intelligence , pages 11071– 11078, 2021. 2, 3, 4, 7, 8 [42] Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, and Qibin Zhao. Tensor completion via fully-connected tensor network decomposition with regularized fac- tors. Journal of Scientific Computing , 92(8):1–35, 2022. 2
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Tensor decompositions for learning latent variable models",
        "Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequal- ity",
        "Efficient tensor completion for color image and video recovery: Low-rank tensor train",
        "Tensor decompositions for signal pro- cessing applications: From two-way to multiway com- ponent analysis",
        "Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor de- compositions",
        "A dual algorithm for the solution of nonlinear variational problems via finite element approximation",
        "Typicality in random matrix product states",
        "Approximately optimal core shapes for tensor decompositions",
        "Expressive power of tensor net- work factorizations for probabilistic modeling",
        "Tikhonov regularization and total least squares",
        "Multiscale tensor decom- position and rendering equation encoding for view synthesis",
        "Adaptive learning of tensor network structures",
        "Most tensor problems are NP-hard",
        "Tensor decom- positions and applications",
        "Evolutionary topology search for tensor network decomposition",
        "Is rank minimization of the essence to learn tensor network structure?",
        "Permutation search of tensor network structures via local sampling",
        "Alternating local enumeration (tnale): Solving tensor network structure search with fewer evaluations",
        "Heuristic rank selec- tion with progressively searching tensor ring network",
        "Adaptive tensor networks decomposition",
        "A practical introduction to tensor net- works:Matrix product states and projected entangled pair states",
        "Tensor-train decomposition",
        "Scalable Bayesian low-rank decomposition of incomplete mul- tiway tensors",
        "Adaptive rank selection for tensor ring decom- position",
        "Learning tensor low-rank prior for hyper- spectral image reconstruction",
        "Non-local low-rank tensor factor analysis for image restora- tion",
        "Bayesian CP factorization of incomplete tensors with automatic rank determination",
        "Tensor ring decomposition",
        "Spatial-spectral-temporal connective tensor network decomposition for thick cloud removal",
        "Tensor N-tubal rank and its convex relaxation for low-rank tensor recovery",
        "Fully-connected tensor network decomposition and its application to higher-order tensor completion",
        "Tensor completion via fully-connected tensor network decomposition with regularized fac- tors"
    ]
}
