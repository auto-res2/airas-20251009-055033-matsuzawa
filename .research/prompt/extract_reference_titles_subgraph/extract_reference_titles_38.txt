
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
Published as a conference paper at ICLR 2023 ACCELERATING HAMILTONIAN MONTE CARLO VIA CHEBYSHEV INTEGRATION TIME Jun-Kun Wang and Andre Wibisono Department of Computer Science, Yale University {jun-kun.wang,andre.wibisono}@yale.edu ABSTRACT Hamiltonian Monte Carlo (HMC) is a popular method in sampling. While there are quite a few works of studying this method on various aspects, an interesting question is how to choose its integration time to achieve acceleration. In this work, we consider accelerating the process of sampling from a distribution π(x) ∝ exp(−f(x)) via HMC via time-varying integration time. When the potential f is L-smooth and m-strongly convex, i.e. for sampling from a log-smooth and strongly log-concave target distribution π, it is known that under a constant integration time, the number of iterations that ideal HMC takes to get an ϵ Wasserstein-2 distance to the target πis O(κlog 1 ϵ), where κ:= L m is the condition number. We propose a scheme of time-varying integration time based on the roots of Chebyshev polynomials. We show that in the case of quadratic potential f, i.e. when the target πis a Gaussian distribution, ideal HMC with this choice of integration time only takes O(√κlog 1 ϵ) number of iterations to reach Wasserstein-2 distance less thanϵ; this improvement on the dependence on condition number is akin to acceleration in optimization. The design and analysis of HMC with the proposed integration time is built on the tools of Chebyshev polynomials. Experiments ﬁnd the advantage of adopting our scheme of time-varying integration time even for sampling from distributions with smooth strongly convex potentials that are not quadratic. 1 I NTRODUCTION Markov chain Monte Carlo (MCMC) algorithms are fundamental techniques for sampling from probability distributions, which is a task that naturally arises in statistics (Duane et al., 1987; Girolami & Calderhead, 2011), optimization (Flaxman et al., 2005; Duchi et al., 2012; Jin et al., 2017), machine learning and others (Wenzel et al., 2020; Salakhutdinov & Mnih, 2008; Koller & Friedman, 2009; Welling & Teh, 2011). Among all the MCMC algorithms, the most popular ones perhaps are Langevin methods (Li et al., 2022; Dalalyan, 2017; Durmus et al., 2019; Vempala & Wibisono, 2019; Lee et al., 2021b; Chewi et al., 2020) and Hamiltonian Monte Carlo (HMC) (Neal, 2012; Betancourt, 2017; Hoffman & Gelman, 2014; Levy et al., 2018). For the former, recently there have been a sequence of works leveraging some techniques in optimization to design Langevin methods, which include borrowing the idea of momentum methods like Nesterov acceleration (Nesterov, 2013) to design fast methods, e.g., (Ma et al., 2021; Dalalyan & Riou-Durand, 2020). Speciﬁcally, Ma et al. (2021) show that for sampling from distributions satisfying the log-Sobolev inequality, under-damped Langevin improves the iteration complexity of over-damped Langevin from O(d ϵ) to O( √ d ϵ), where dis the dimension and ϵis the error in KL divergence, though whether their result has an optimal dependency on the condition number is not clear. On the other hand, compared to Langevin methods, the connection between HMCs and techniques in optimization seems rather loose. Moreover, to our knowledge, little is known about how to accelerate HMCs with a provable acceleration guarantee for converging to a target distribution. Speciﬁcally, Chen & Vempala (2019) show that for sampling from strongly log-concave distributions, the iteration complexity of ideal HMC is O(κlog 1 ϵ), and Vishnoi (2021) shows the same rate of ideal HMC when the potential is strongly convex quadratic in a nice tutorial. In contrast, there are a few methods that exhibit acceleration when minimizing strongly convex quadratic functions in optimization. For example, while Heavy Ball (Polyak, 1964) does not have an accelerated linear rate globally for minimizing general smooth strongly convex functions, it does show acceleration when minimizing strongly convex quadratic functions (Wang et al., 2020; 1 arXiv:2207.02189v2  [cs.LG]  14 Feb 2023Published as a conference paper at ICLR 2023 Algorithm 1:IDEAL HMC 1: Require: an initial point x0 ∈Rd, number of iterations K, and a scheme of integration time {η(K) k }. 2: for k= 1 to Kdo 3: Sample velocity ξ∼N(0,Id). 4: Set (xk,vk) = HMCη(K) k (xk−1,ξ). 5: end for 2021; 2022). This observation makes us wonder whether one can get an accelerated linear rate of ideal HMC for sampling, i.e., O(√κlog 1 ϵ), akin to acceleration in optimization. We answer this question afﬁrmatively, at least in the Gaussian case. We propose a time-varying integration time for HMC, and we show that ideal HMC with this time-varying integration time exhibits acceleration when the potential is a strongly convex quadratic (i.e. the targetπis a Gaussian), compared to what is established in Chen & Vempala (2019) and Vishnoi (2021) for using a constant integration time. Our proposed time-varying integration time at each iteration of HMC depends on the total number of iterations K, the current iteration index k, the strong convexity constant m, and the smoothness constant Lof the potential; therefore, the integration time at each iteration is simple to compute and is set before executing HMC. Our proposed integration time is based on the roots of Chebysev polynomials, which we will describe in details in the next section. In optimization, Chebyshev polynomials have been used to help design accelerated algorithms for minimizing strongly convex quadratic functions, i.e., Chebyshev iteration (see e.g., Section 2.3 in d’Aspremont et al. (2021)). Our result of accelerating HMC via using the proposed Chebyshev integration time can be viewed as the sampling counterpart of acceleration from optimization. Interestingly, for minimizing strongly convex quadratic functions, acceleration of vanilla gradient descent can be achieved via a scheme of step sizes that is based on a Chebyshev polynomial, see e.g., Agarwal et al. (2021), and our work is inspired by a nice blog article by Pedregosa (2021). Hence, our acceleration result of HMC can also be viewed as a counterpart in this sense. In addition to our theoretical ﬁndings, we conduct experiments of sampling from a Gaussian as well as sampling from distributions whose potentials are not quadratics, which include sampling from a mixture of two Gaussians, Bayesian logistic regression, and sampling from a hard distribution that was proposed in Lee et al. (2021a) for establishing some lower-bound results of certain Metropolized sampling methods. Experimental results show that our proposed time-varying integration time also leads to a better performance compared to using the constant integration time of Chen & Vempala (2019) and Vishnoi (2021) for sampling from the distributions whose potential functions are not quadratic. We conjecture that our proposed time-varying integration time also helps accelerate HMC for sampling from log-smooth and strongly log-concave distributions, and we leave the analysis of such cases for future work. 2 P RELIMINARIES 2.1 H AMILTONIAN MONTE CARLO (HMC) Suppose we want to sample from a target probability distribution ν(x) ∝exp(−f(x)) on Rd, where f: Rd →R is a continuous function which we refer to as the potential. Denote x ∈Rd the position and v ∈Rd the velocity of a particle. In this paper, we consider the standard Hamiltonian of the particle (Chen & Vempala, 2019; Neal, 2012), which is deﬁned as H(x,v) := f(x) + 1 2 ∥v∥2, (1) while we refer the readers to Girolami & Calderhead (2011); Hirt et al. (2021); Brofos & Lederman (2021) and the references therein for other notions of the Hamiltonian. The Hamiltonian ﬂow generated by H is the ﬂow of the particle which evolves according to the following differential equations: dx dt = ∂H ∂v and dv dt = −∂H ∂x . For the standard Hamiltonian deﬁned in (1), the Hamiltonian ﬂow becomes dx dt = v and dv dt = −∇f(x). (2) 2Published as a conference paper at ICLR 2023 We will write (xt,vt) = HMCt(x0,v0) as the position xand the velocity vof the Hamiltonian ﬂow after integration time tstarting from (x0,v0). There are many important properties of the Hamiltonian ﬂow including that the Hamiltonian is conserved along the ﬂow, the vector ﬁeld associated with the ﬂow is divergence free, and the Hamiltonian dynamic is time reversible, see e.g., Section 3 in Vishnoi (2021). The Ideal HMCalgorithm (see Algorithm 1) proceeds as follows: in each iteration k, sample an initial velocity from the normal distribution, and then ﬂow following the Hamiltonian ﬂow with a pre-speciﬁed integration time ηk. It is well-known that ideal HMC preserves the target density π(x) ∝exp(−f(x)); see e.g., Theorem 5.1 in Vishnoi (2021). Furthermore, in each iteration, HMC brings the density of the iterates xk ∼ρk closer to the target π. However, the Hamiltonian ﬂow HMCt(x0,v0) is in general difﬁcult to simulate exactly, except for some special potentials. In practice, the Verlet integrator is commonly used to approximate the ﬂow and a Metropolis-Hastings ﬁlter is applied to correct the induced bias arises from the use of the integrator (Tripuraneni et al., 2017; Brofos & Lederman, 2021; Hoffman et al., 2021; Lee et al., 2021a; Chen et al., 2020). In recent years, there have been some progress on showing some rigorous theoretical guarantees of HMCs for converging to a target distribution, e.g., Chen et al. (2020); Durmus et al. (2017); Bou-Rabee & Eberle (2021); Mangoubi & Smith (2019; 2021); Mangoubi & Vishnoi (2018). There are also other variants of HMCs proposed in the literature, e.g., Riou-Durand & V ogrinc (2022); Bou-Rabee & Sanz-Serna (2017); Zou & Gu (2021); Steeg & Galstyan (2021); Hoffman & Gelman (2014); Tripuraneni et al. (2017); Chen et al. (2014), to name just a few. Recall that the 2-Wasserstein distance between probability distributions ν1 and ν2 is W2(ν1,ν2) := inf x,y∈Γ(ν1,ν2) E [ ∥x−y∥2]1/2 where Γ(ν1,ν2) represents the set of all couplings of ν1 and ν2. 2.2 A NALYSIS OF HMC IN QUADRATIC CASE WITH CONSTANT INTEGRATION TIME In the following, we replicate the analysis of ideal HMC with a constant integration time for quadratic potentials (Vishnoi, 2021), which provides the necessary ingredients for introducing our method in the next section. Speciﬁcally, we consider the following quadratic potential: f(x) := ∑d j=1 λjx2 j, where 0 <m ≤λj ≤L, (3) which means the target density is the Gaussian distribution π= N(0,Λ−1), where Λ the diagonal matrix whose jth diagonal entry is λj. We note for a general Gaussian target N(µ,Σ) for some µ∈Rd and Σ ≻0, we can shift and rotate the coordinates to make µ= 0 and Σ a diagonal matrix, and our analysis below applies. So without loss of generality, we may assume the quadratic potential is separable, as in (3). In this quadratic case, the Hamiltonian ﬂow (2) becomes a linear system of differential equations, and we have an exact solution given by sinusoidal functions, which are xt[j] = cos (√ 2λjt ) x0[j] + 1√ 2λj sin (√ 2λjt ) v0[j], vt[j] = − √ 2λjsin (√ 2λjt ) x0[j] + cos (√ 2λjt ) v0[j]. (4) In particular, we recall the following result on the deviation between two co-evolving particles with the same initial velocity. Lemma 1. (Vishnoi, 2021) Let x0,y0 ∈ Rd. Consider the following coupling: (xt,vt) = HMCt(x0,ξ) and (yt,ut) = HMC t(y0,ξ) for some ξ ∈ Rd. Then for all t ≥ 0 and for all j ∈[d], it holds that xt[j] −yt[j] = cos (√ 2λjt ) ×(x0[j] −y0[j]). Using Lemma 1, we can derive the convergence rate of ideal HMC for the quadratic potential as follows. 3Published as a conference paper at ICLR 2023 Lemma 2. (Vishnoi, 2021) Let π∝exp(−f) = N(0,Λ−1) be the target distribution, where f(x) is deﬁned on (3). Let ρK be the distribution of xK generated by Algorithm 1 at the ﬁnal iteration K. Then for any ρ0 and any K ≥1, we have W2(ρK,π) ≤maxj∈[d] ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐W2(ρ0,π). We replicate the proof of Lemma 1 and Lemma 2 in Appendix B for the reader’s convenience. Vishnoi (2021) shows that by choosing (Constant integration time) η(K) k = π 2 1√ 2L , (5) one has that cos (√ 2λjη(K) k ) ≤1 −Θ (m L ) for all the iterations k∈[K] and dimensions j ∈[d]. Hence, by Lemma 2, the distance satisﬁes W2(ρK,π) = O (( 1 −Θ (m L ))K) W2(ρ0,π) after Kiterations of ideal HMC with the constant integration time. On the other hand, for general smooth strongly convex potentials f(·), Chen & Vempala (2019) show the same convergence rate 1 −Θ (m L ) of HMC using a constant integration time η(K) k = c√ L, where c >0 is a universal constant. Therefore, under the constant integration time, HMC needs O(κlog 1 ϵ) iterations to reach error W2(ρK,π) ≤ϵ, where κ = L m is condition number. Furthermore, they also show that the relaxation time of ideal HMC with a constant integration time is Ω(κ) for the Gaussian case. 2.3 C HEBYSHEV POLYNOMIALS We denote ΦK(·) the degree-KChebyshev polynomial of the ﬁrst kind, which is deﬁned by: ΦK(x) =    cos(Karccos(x)) if x∈[−1,1], cosh(Karccosh(x)) if x> 1, (−1)Kcosh(Karccosh(x)) if x< 1. (6) Our proposed integration time is built on a scaled-and-shifted Chebyshev polynomial, deﬁned as: ¯ΦK(λ) := ΦK(h(λ)) ΦK(h(0)) , (7) where h(·) is the mapping h(λ) := L+m−2λ L−m . Observe that the mapping h(·) maps all λ∈[m,L] into the interval [−1,1]. The roots of the degree-Kscaled-and-shifted Chebyshev polynomial ¯ΦK(λ) are (Chebyshev roots) r(K) k := L+ m 2 −L−m 2 cos ((k−1 2 )π K ) , (8) where k = 1,2,...,K , i.e., ¯ΦK(r(K) k ) = 0. We now recall the following key result regarding the scaled-and-shifted Chebyshev polynomial ¯ΦK. Lemma 3. (e.g., Section 2.3 in d’Aspremont et al. (2021)) For any positive integerK, we have maxλ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐≤2 ( 1 −2 √m√ L+√m )K = O (( 1 −Θ (√m L ))K) . (9) The proof of Lemma 3 is in Appendix B. 3 C HEBYSHEV INTEGRATION TIME We are now ready to introduce our scheme of time-varying integration time. LetKbe the pre-speciﬁed total number of iterations of HMC. Our proposed method will ﬁrst permute the array [1,2,...,K ] before executing HMC for Kiterations. Denote σ(k) the kth element of the array [1,2,...,K ] after an arbitrary permutation σ. Then, we propose to set the integration time of HMC at iteration k, i.e., set η(K) k , as follows: 4Published as a conference paper at ICLR 2023 Figure 1: Left: Set K = 400 , m = 1 and L = 100 . The green solid line (Cheby- shev integration time (10)) on the subﬁgure represents maxλ∈{m,m+0.1,...,L} ⏐⏐⏐Πk s=1cos (√ 2λη(K) s )⏐⏐⏐ =⏐⏐⏐⏐⏐Πk s=1cos ( π 2 √ λ r(K) σ(s) )⏐⏐⏐⏐⏐ v.s. k, while the blue dash line (Constant integration time (5)) represents maxλ∈{m,m+0.1,...,L} ⏐⏐⏐Πk s=1cos (√ 2λη(K) s )⏐⏐⏐= ⏐⏐⏐⏐Πk s=1cos ( π 2 √ λ L )⏐⏐⏐⏐v.s. k. Since the cosine product con- trols the convergence rate of the W2 distance by Lemma 2, this conﬁrms the acceleration via using the proposed scheme of Chebyshev integration over the constant integration time (Chen & Vempala, 2019; Vishnoi, 2021). Right: ψ(x) = cos(π 2 √x) 1−x v.s. x. (Chebyshev integration time) η(K) k = π 2 1√ 2r(K) σ(k) . (10) We note the usage of the permutation σis not needed in our analysis below; however, it seems to help improve performance in practice. Speciﬁcally, though the guarantees of HMC at the ﬁnal iteration K provided in Theorem 1 and Lemma 4 below is the same regardless of the permutation, the progress of HMC varies under different permutations of the integration time, which is why we recommend an arbitrary permutation of the integration time in practice. Our main result is the following improved convergence rate of HMC under the Chebyshev integration time, for quadratic potentials. Theorem 1. Denote the target distribution π∝exp(−f(x)) = N(0,Λ−1), where f(x) is deﬁned on (3), and denote the condition number κ := L m. Let ρK be the distribution of xK generated by Algorithm 1 at the ﬁnal iteration K. Then, we have W2(ρK,π) ≤2 ( 1 −2 √m√ L+ √m )K W2(ρ0,π) = O (( 1 −Θ ( 1√κ ))K) W2(ρ0,π). Consequently, the total number of iterations K such that the Wasserstein-2 distance satisﬁes W2(ρK,π) ≤ϵis O (√κlog 1 ϵ ) . Theorem 1 shows an accelerated linear rate 1 −Θ ( 1√κ ) using Chebyshev integration time, and hence improves the previous result of 1 −Θ (1 κ ) as discussed above. The proof of Theorem 1 relies on the following lemma, which upper-bounds the cosine products that appear in the bound of the W2 distance in Lemma 2 by the scaled-and-shifted Chebyshev polynomial ¯ΦK(λ) on (7). Lemma 4. Denote |PCos K (λ)|:= ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λ r(K) σ(k) )⏐⏐⏐⏐⏐. Suppose λ∈[m,L]. Then, we have for any positive integer K, |PCos K (λ)|≤ ⏐⏐¯ΦK(λ) ⏐⏐. (11) The proof of Lemma 4 is available in Appendix C. Figure 1 compares the cosine product maxλ∈[m,L] ⏐⏐⏐Πk s=1cos (√ 2λη(K) s )⏐⏐⏐in Lemma 2 of using the proposed integration time and that 5Published as a conference paper at ICLR 2023 Algorithm 2:HMC WITH CHEBYSHEV INTEGRATION TIME 1: Given: a potential f(·), where π(x) ∝exp(−f(x)) and f(·) is L-smooth and m-strongly convex. 2: Require: number of iterations Kand the step size of the leapfrog steps θ. 3: Deﬁne r(K) k := L+m 2 −L−m 2 cos ((k−1 2 )π K ) ,for k= 1,...,K. 4: Arbitrarily permute the array [1,2,...,K ]. Denote σ(k) the kth element of the array after permutation. 5: for k= 1,2,...,K do 6: Sample velocity ξk ∼N(0,Id). 7: Set integration time η(K) k ←π 2 1√ 2r(K) σ(k) . 8: Set the number of leapfrog steps Sk ←⌊ η(K) k θ ⌋. 9: (¯x0,¯v0) ←(xk−1,ξk) % Leapfrog steps 10: for s= 0,2,...,S k −1 do 11: ¯vs+ 1 2 = ¯vs −θ 2 ∇f(¯xs); ¯ xs+1 = ¯xs + θ¯vs+ 1 2 ; ¯ vs+1 = ¯vs+ 1 2 −θ 2 ∇f(¯xs+1); 12: end for % Metropolis ﬁlter 13: Compute the acceptance ratio αk = min ( 1, exp(−H(¯xSk,¯vSk)) exp(−H(¯x0,¯v0)) ) . 14: Draw ζ ∼Uniform[0,1]. 15: If ζ <αk then 16: xk ←¯xSk 17: Else 18: xk ←xk−1. 19: end for of using the constant integration time, which illustrates acceleration via the proposed Chebyshev integration time. We now provide the proof of Theorem 1. Proof. (of Theorem 1) From Lemma 2, we have W2(ρK,π) ≤maxj∈[d] ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐·W2(ρ0,π). (12) We can upper-bound the cosine product of any j ∈[d] as, ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐ (a) = ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λj r(K) σ(k) )⏐⏐⏐⏐⏐ (b) ≤ ⏐⏐¯ΦK(λj) ⏐⏐ (c) ≤2 ( 1 −2 √m√ L+√m )K , (13) where (a) is due to the use of Chebyshev integration time (10), (b) is by Lemma 4, and (c) is by Lemma 3. Combining (12) and (13) leads to the result. HMC with Chebyshev Integration Time for General DistributionsTo sample from general strongly log-concave distributions, we propose Algorithm 2, which adopts the Verlet integrator (a.k.a. the leapfrog integrator) to simulate the Hamiltonian ﬂow HMCη(·,ξ) and uses Metropolis ﬁlter to correct the bias. It is noted that the number of leapfrog steps Sk in each iteration kis equal to the integration time η(K) k divided by the step size θused in the leapfrog steps. More precisely, we have Sk = ⌊η(K) k θ ⌋in iteration kof HMC. 4 E XPERIMENTS We now evaluate HMC with the proposed Chebyshev integration time (Algorithm 2) and HMC with the constant integration time (Algorithm 2 with line 7 replaced by the constant integration time (5)) in several tasks. For all the tasks in the experiments, the total number of iterations of HMCs is set to be K = 10,000, and hence we collect K = 10,000 samples along the trajectory. For the step size θ in the leapfrog steps, we let θ ∈{0.001,0.005,0.01,0.05}. To evaluate the methods, we 6Published as a conference paper at ICLR 2023 Table 1: Ideal HMC with K = 10,000 iterations for sampling from a Gaussian N(µ,Σ), where µ= [ 0 0 ] and Σ = [ 1 0 0 100 ] . Here, Cheby. (W/) is ideal HMC with a arbitrary permutation of the Chebyshev integration time, while Cheby. (W/O) is ideal HMC without a permutation; and Const. refers to using the constant integration time (5). Method Mean ESS Min ESS Cheby. (W/) 10399.00811 ±347.25021 7172 .50338 ±257.21244 Cheby. (W/O) 10197.09964 ±276.94894 7043 .55293 ±284.78037 Const. 7692.00382 ±207.19628 5533 .26519 ±213.31943 compute effective sample size (ESS), which is a common performance metric of HMCs (Girolami & Calderhead, 2011; Brofos & Lederman, 2021; Hirt et al., 2021; Riou-Durand & V ogrinc, 2022; Hoffman et al., 2021; Hoffman & Gelman, 2014; Steeg & Galstyan, 2021), by using the toolkit ArViz (Kumar et al., 2019). The ESS of a sequence of N dependent samples is computed based on the autocorrelations within the sequence at different lags: ESS := N/(1 + 2∑ kγ(k)), where γ(k) is an estimate of the autocorrelation at lag k. We consider 4 metrics, which are (1) Mean ESS:the average of ESS of all variables. That is, ESS is computed for each variable/dimension, and Mean ESS is the average of them. (2) Min ESS:the lowest value of ESS among the ESSs of all variables; (3) Mean ESS/Sec.:Mean ESS normalized by the CPU time in seconds; (4) Min ESS/Sec.:Minimum ESS normalized by the CPU time in seconds. In the following tables, we denote “Cheby.” as our proposed method, and “Const.” as HMC with the the constant integration time (Vishnoi, 2021; Chen & Vempala, 2019). Each of the conﬁgurations is repeated 10 times, and we report the average and the standard deviation of the results. We also report the acceptance rate of the Metropolis ﬁlter (Acc. Prob) on the tables. Our implementation of the experiments is done by modifying a publicly available code of HMCs by Brofos & Lederman (2021). Code for our experiments can be found in the supplementary. 4.1 I DEAL HMC FLOW FOR SAMPLING FROM A GUSSIAN WITH A DIAGONAL COVARIANCE Before evaluating the empirical performance of Algorithm 2 in the following subsections, here we discuss and compare the use of a arbitrary permutation of the Chebyshev integration time and that without permutation (as well as that of using a constant integration time). We simulate ideal HMC for sampling from a Gaussian N(µ,Σ), where µ= [ 0 0 ] and Σ = [ 1 0 0 100 ] . It is noted that ideal HMC ﬂow for this case has a closed-form solution as (4) shows. The result are reported on Table 1. From the table, the use of a Chebyshev integration time allows to obtain a larger ESS than that from using a constant integration time, and a arbitrary permutation helps get a better result. An explanation is that the ESS is a quantity that is computed along the trajectory of a chain, and therefore a permutation of the integration time could make a difference. We remark that the observation here (a arbitrary permutation of time generates a larger ESS) does not contradict to Theorem 1, since Theorem 1 is about the guarantee in W2 distance at the last iteration K. 4.2 S AMPLING FROM A GAUSSIAN We sample N(µ,Σ), where µ= [ 0 1 ] and Σ = [ 1 0 .5 0.5 100 ] . Therefore, the strong convexity constant mis approximately 0.01 and the smoothness constant Lis approximately 1. Table 2 shows the results. HMC with Chebyshev integration time consistently outperforms that of using the constant integration time in terms of all the metrics: Mean ESS, Min ESS, Mean ESS/Sec, and Min ESS/Sec. We also plot two quantities throughout the iterations of HMCs on Figure 2. Speciﬁcally, Sub-ﬁgure (a) on Figure 2 plots the size of the difference between the targeted covarianceΣ and an estimated covariance ˆΣk at each iteration kof HMC, where ˆΣk is the sample covariance of 10,000 samples collected from a number of 10,000 HMC chains at their kth iteration. Sub-ﬁgure (b) plots a discrete TV distance that is computed as follows. We use a built-in function of Numpy to sample 10,000 samples from the target distribution, while we also have 10,000 samples collected from a number 7Published as a conference paper at ICLR 2023 Table 2: Sampling from a Gaussian distribution. We report 4 metrics regarding ESS (the higher the better), please see the main text for their deﬁnitions. Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 5187.28 ±261.13 307 .09 ±21.92 20 .28 ±1.74 1 .20 ±0.11 1 .00 ±0.00 0.001 Const. 1912.76 ±72.10 39 .87 ±13.77 15 .87 ±0.89 0 .33 ±0.11 1 .00 ±0.00 0.005 Cheby. 5146.71 ±257.65 304 .126 ±19.09 97 .84 ±9.23 5 .79 ±0.68 1 .00 ±0.00 0.005 Const. 1926.71 ±136.53 32 .83 ±9.57 80 .31 ±4.39 1 .37 ±0.39 1 .00 ±0.00 0.01 Cheby. 5127.90 ±211.46 279 .59 ±38.09 184 .26 ±20.99 10 .01 ±1.52 1 .00 ±0.00 0.01 Const. 1832.87 ±77.47 35 .71 ±11.74 147 .53 ±12.59 2 .85 ±0.95 1 .00 ±0.00 0.05 Cheby. 5133.67 ±195.07 316 .87 ±36.27 871 .72 ±88.73 53 .54 ±6.22 0 .99 ±0.00 0.05 Const. 1849.15 ±92.75 34 .98 ±14.70 615 .73 ±30.16 11 .70 ±5.07 0 .99 ±0.00 0.1 Cheby. 4948.46 ±144.03 281 .66 ±44.79 1492 .96 ±166.21 84 .39 ±13.04 0 .99 ±0.00 0.1 Const. 1852.79 ±132.95 38 .17 ±16.35 1035 .54 ±82.34 21 .44 ±9.51 0 .99 ±0.00 (a) ∥Σ −ˆΣk∥F v.s. iteration k  (b) discrete TV(ˆπ,ˆρk) v.s. iteration k Figure 2: Sampling from a Gaussian distribution. Both lines correspond to HMCs with the same step size h= 0.05 used in the leapfrog steps (but with different schemes of the integration time). Please see the main text for the precise deﬁnitions of the quantities and the details of how we compute them. of 10,000 HMC chains at each iteration k. Using these two sets of samples, we construct two histograms with 30 number of bins for each dimension, we denote them as ˆπand ˆρk. The discrete TV(ˆπ,ˆρk) at iteration k is 0.5 times the sum of the absolute value of the difference between the number of counts of all the pairs of the bins divided by 10,000, which serves as a surrogate of the Wasserstein-2 distance between the true target πand ρk from HMC, since computing or estimating the true Wasserstein distance is challenging. 4.3 S AMPLING FROM A MIXTURE OF TWO GAUSSIANS For a vector a ∈ Rd and a positive deﬁnite matrix Σ ∈ Rd×d, we consider sampling from a mixture of two Gaussians N(a,Σ) and N(−a,Σ) with equal weights. Denote b := Σ −1a and Λ := Σ −1. The potential is f(x) = 1 2 ∥x−a∥2 Λ −log(1 + exp(−2x⊤b)), and its gradient is ∇f(x) = Λ x−b+ 2b(1 + exp(−2x⊤b))−1.For each dimension i ∈[d], we set a[i] = √ i 2d and set the covariance Σ = diag1≤i≤d( i d). The potential is strongly convex if a⊤Σ−1a <1, see e.g., Riou-Durand & V ogrinc (2022). We setd= 10 in the experiment, and simply use the smallest and the largest eigenvalue of Λ to approximate the strong convexity constant mand the smoothness constant Lof the potential, which are ˆm= 1 and ˆL= 10 in this case. Table 3 shows that the proposed method generates a larger effective sample size than the baseline. 4.4 B AYESIAN LOGISTIC REGRESSION We also consider Bayesian logistic regression to evaluate the methods. Given an observation (zi,yi), where zi ∈Rd and yi ∈{0,1}, the likelihood function is modeled as p(yi|zi,w) = 1 1+exp(−yiz⊤ i w) . Moreover, the prior on the model parameter wis assumed to follow a Gaussian distribution, p(w) = N(0,α−1Id), where α >0 is a parameter. The goal is to sample w ∈Rd from the posterior, p(w|{zi,yi}n i=1) = p(w)Πn i=1p(yi|zi,w), where nis the number of data points in a dataset. The potential function f(w) can be written as f(w) = ∑n i=1 fi(w), where fi(w) = log ( 1 + exp(−yiw⊤zi) ) + α∥w∥2 2n . (14) 8Published as a conference paper at ICLR 2023 Table 3: Sampling from a mixture of two Gaussians Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 2439.86 ±71.83 815 .20 ±83.82 22 .68 ±0.93 7 .57 ±0.81 0 .89 ±0.00 0.001 Const. 845.44 ±31.42 261 .14 ±34.34 12 .90 ±0.52 3 .98 ±0.53 0 .91 ±0.00 0.005 Cheby. 2399.50 ±100.12 784 .06 ±82.07 105 .97 ±8.78 34 .58 ±4.12 0 .89 ±0.00 0.005 Const. 876.61 ±25.62 277 .72 ±30.62 63 .80 ±4.67 20 .22 ±2.62 0 .91 ±0.00 0.01 Cheby. 2341.35 ±89.99 794 .27 ±48.75 194 .81 ±23.51 66 .30 ±9.89 0 .88 ±0.00 0.01 Const. 860.61 ±20.39 235 .33 ±33.73 110 .62 ±14.09 30 .40 ±6.34 0 .91 ±0.00 0.05 Cheby. 2214.19 ±87.27 748 .66 ±46.18 761 .59 ±68.88 256 .51 ±13.76 0 .89 ±0.00 0.05 Const. 853.40 ±41.05 265 .70 ±37.41 376 .54 ±67.83 116 .45 ±22.23 0 .91 ±0.00 0.1 Cheby. 2064.42 ±67.44 657 .45 ±60.44 1162 .67 ±84.19 370 .07 ±41.11 0 .90 ±0.00 0.1 Const. 632.70 ±22.78 182 .88 ±37.10 450 .53 ±93.60 132 .58 ±43.91 0 .92 ±0.00 We set α= 1 in the experiments. We consider three datasets: Heart, Breast Cancer, and Diabetes binary classiﬁcation datasets, which are all publicly available online. To approximate the strong convexity constant mand the smoothness constant Lof the potential f(w), we compute the smallest eigenvalue and the largest eigenvalue of the Hessian∇2f(w) at the maximizer of the posterior, and we use them as estimates of mand Lrespectively. We apply Newton’s method to approximately ﬁnd the maximizer of the posterior. The experimental results are reported on Table 4 in Appendix E.1 due to the space limit, which show that our method consistently outperforms the baseline. 4.5 S AMPLING FROM A hard DISTRIBUTION We also consider sampling from a step-size-dependent distribution π(x) ∝exp(−fh(x)), where the potential fh(·) is κ-smooth and 1-strongly convex. The distribution is considered in Lee et al. (2021a) for showing a lower bound regarding certain Metropolized sampling methods using a constant integration time and a constant step size hof the leapfrog integrator. More concretely, the potential is fh(x) := ∑d i=1 f(h) i (xi), where f(h) i (xi) = { 1 2 x2 i, i = 1 κ 3 x2 i −κh 3 cos ( xi√ h ) , 2 ≤i≤d. (15) In the experiment, we set κ= 50 and d= 10. The results are reported on Table 5 in Appendix E.2. The scheme of the Chebyshev integration time is still better than the constant integration time for this task. 5 D ISCUSSION AND OUTLOOK The Chebyshev integration time shows promising empirical results for sampling from a various of strongly log-concave distributions. On the other hand, the theoretical guarantee of acceleration that we provide in this work is only for strongly convex quadratic potentials. Therefore, a direction left open by our work is establishing some provable acceleration guarantees for general strongly log-concave distributions. However, unlike quadratic potentials, the output (position, velocity) of a HMC ﬂow does not have a closed-form solution in general, which makes the analysis much more challenging. A starting point might be improving the analysis of Chen & Vempala (2019), where a contraction bound of two HMC chains under a small integration time η= O( 1√ L) is shown. Since the scheme of the Chebyshev integration time requires a large integration time η= Θ ( 1√m ) at some iterations of HMC, a natural question is whether a variant of the result of Chen & Vempala (2019) can be extended to a large integration time η= Θ ( 1√m ) . We state as an open question: can ideal HMC with a scheme of time-varying integration time achieve an accelerated rateO(√κlog 1 ϵ) for general smooth strongly log-concave distributions? The topic of accelerating HMC with provable guarantees is underexplored, and we hope our work can facilitate the progress in this ﬁeld. After the preprint of this work was available on arXiv, Jiang (2022) proposes a randomized integration time with partial velocity refreshment and provably shows that ideal HMC with the proposed machinery has the accelerated rate for sampling from a Gaussian distribution. Exploring any connections between the scheme of Jiang (2022) and ours can be an interesting direction. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGMENTS We thank the reviewers for constructive feedback, which helps improve the presentation of this paper. REFERENCES Naman Agarwal, Surbhi Goel, and Cyril Zhang. Acceleration via fractal learning rate schedules. ICML, 2021. Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv:1701.02434, 2017. Nawaf Bou-Rabee and Andreas Eberle. Mixing time guarantees for unadjusted Hamiltonian Monte Carlo. arXiv:2105.00887, 2021. Nawaf Bou-Rabee and Jesus Maria Sanz-Serna. Randomized Hamiltonian Monte Carlo. Annals of Applied Probability, 2017. James A. Brofos and Roy R. Lederman. Evaluating the implicit midpoint integrator for Riemannian manifold Hamiltonian Monte Carlo. ICML, 2021. Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. ICML, 2014. Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast mixing of Metropolized Hamiltonian Monte Carlo: Beneﬁts of multi-step gradients. JMLR, 2020. Zongchen Chen and Santosh S Vempala. Optimal convergence rate of Hamiltonian Monte Carlo for strongly logconcave distributions. International Conference on Randomization and Computation (RANDOM), 2019. Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin J. Stromme. Exponential ergodicity of mirror-langevin diffusions. NeurIPS, 2020. Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from a smooth and log-concave density. Journal of the Royal Statistical Society: Series B, 2017. Arnak S. Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using kinetic Langevin diffusions. Bernoulli, 2020. Alexandre d’Aspremont, Damien Scieur, and Adrien Taylor. Acceleration methods. Foundations and Trends in Optimization, 2021. Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics Letters B, 1987. John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 2012. Alain Durmus, Eric Moulines, and Eero Saksman. On the convergence of Hamiltonian Monte Carlo. arXiv:1705.00166, 2017. Alain Durmus, Szymon Majewski, and Bła˙zej Miasojedow. Analysis of Langevin Monte Carlo via convex optimization. JMLR, 2019. Abraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. SODA, 2005. Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society, 2011. Marcel Hirt, Michalis K. Titsias, and Petros Dellaportas. Entropy-based adaptive Hamiltonian Monte Carlo. NeurIPS, 2021. 10Published as a conference paper at ICLR 2023 Matthew D. Hoffman and Andrew Gelman. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. JMLR, 2014. Matthew D. Hoffman, Alexey Radul, and Pavel Sountsov. An adaptive-MCMC scheme for setting trajectory lengths in Hamiltonian Monte Carlo. AISTATS, 2021. Qijia Jiang. On the dissipation of ideal hamiltonian monte carlo sampler. arXiv:2209.07438, 2022. Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efﬁciently. ICML, 2017. Daphne Koller and Nir Friedman. Probabilistic graphical models: Principles and techniques. MIT Press, 2009. Ravin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo Martin. Arviz a uniﬁed library for exploratory analysis of bayesian models in python. The Journal of Open Source Software, 2019. Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Lower bounds on Metropolized sampling methods for well-conditioned distributions. NeurIPS, 2021a. Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Structured logconcave sampling with a restricted gaussian oracle. COLT, 2021b. Daniel Levy, Matthew D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. ICLR, 2018. Ruilin Li, Hongyuan Zha, and Molei Tao. Sqrt(d) dimension dependence of Langevin Monte Carlo. ICLR, 2022. Yi-An Ma, Niladri S. Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L. Bartlett, and Michael I. Jordan. Is there an analog of Nesterov acceleration for MCMC? Bernoulli, 2021. Oren Mangoubi and Aaron Smith. Mixing of Hamiltonian Monte Carlo on strongly logconcave distributions 2: Numerical integrators. AISTATS, 2019. Oren Mangoubi and Aaron Smith. Mixing of Hamiltonian Monte Carlo on strongly logconcave distributions 1: Continuous dynamics. Annals of Applied Probability, 2021. Oren Mangoubi and Nisheeth K. Vishnoi. Dimensionally tight bounds for second-order Hamiltonian Monte Carlo. NeurIPS, 2018. Radford M. Neal. MCMC using Hamiltonian dynamics. arXiv:1206.1901, 2012. Yurii Nesterov. Introductory lectures on convex optimization: a basic course. Springer, 2013. Fabian Pedregosa. Acceleration without momentum, 2021. URL http://fa.bianp.net/ blog/2021/no-momentum/. B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa- tional Mathematics and Mathematical Physics, 1964. Lionel Riou-Durand and Jure V ogrinc. Metropolis Adjusted Langevin trajectories: a robust alternative to Hamiltonian Monte Carlo. arXiv:2202.13230, 2022. Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. ICML, 2008. Greg Ver Steeg and Aram Galstyan. Hamiltonian dynamics with non-newtonian momentum for rapid sampling. NeurIPS, 2021. Nilesh Tripuraneni, Mark Rowland, Zoubin Ghahramani, and Richard Turner. Magnetic Hamiltonian Monte Carlo. ICML, 2017. Santosh S. Vempala and Andre Wibisono. Rapid convergence of the Unadjusted Langevin Algorithm: Isoperimetry sufﬁces. NeurIPS, 2019. 11Published as a conference paper at ICLR 2023 Nisheeth K. Vishnoi. An introduction to Hamiltonian Monte Carlo method for sampling. arXiv:2108.12107, 2021. Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. Escaping saddle points faster with stochastic momentum. ICLR, 2020. Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. A modular analysis of provable acceleration via Polyak’s momentum: Training a wide ReLU network and a deep linear network. ICML, 2021. Jun-Kun Wang, Chi-Heng Lin, Andre Wibisono, and Bin Hu. Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out. ICML, 2022. Max Welling and Yee Whye Teh. Bayesian learning via Stochastic Gradient Langevin dynamics. ICML, 2011. Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the Bayes posterior in deep neural networks really. ICML, 2020. Difan Zou and Quanquan Gu. On the convergence of Hamiltonian Monte Carlo with stochastic gradients. ICML, 2021. A A CONNECTION BETWEEN OPTIMIZATION AND SAMPLING To provide an intuition of why the technique of Chebyshev polynomials can help accelerate HMC for the case of the strongly convex quadratic potentials, we would like to describe the work of gradient descent with the Chebyshev step sizes Agarwal et al. (2021) in more detail, because we are going to draw a connection between optimization and sampling to showcase the intuition. Agarwal et al. (2021) provably show that gradient descent with a scheme of step sizes based on the Chebyshev Polynomials has an accelerated rate for minimizing strongly convex quadratic functions compared to GD with a constant step size, and their experiments show some promising results for minimizing smooth strongly convex functions beyond quadratics via the proposed scheme of step sizes. More precisely, deﬁne f(w) = 1 2 w⊤Aw, where A ∈Rd×d is a positive deﬁnite matrix which has eigenvalues L:= λ1 ≥λ2 ≥···≥ λd =: m. Agarwal et al. (2021) consider applying gradient descent wk+1 = wk −ηk∇f(wk) to minimize f(·), where ηk is the step size of gradient descent at iteration k. Let wbe the unique global minimizer of f(·). It is easy to show that the dynamic of the distance evolves as wk+1 −w∗= (Id −ηkA)(Id −ηk−1A) ···(Id −η1A)(w1 −w∗). Hence, the size of the distance to w∗at iteration K+ 1 is bounded by ∥wK+1 −w∗∥≤ max j∈[d] | K∏ k=1 (1 −ηkλj)|∥w1 −w∗∥. This shows that the convergence rate of GD is governed by maxj∈[d] |∏K k=1(1 −ηkλj)|. By setting ηk as the inverse of the Chebyshev root r(K) k or any permuted root r(K) σ(k) (see (8) for the deﬁnition), the polynomial ∏K k=1(1 −ηkλ) is actually the K-degree scale-and-shifted polynomial, i.e., ∏K k=1(1 −ηkλ) = ∏K k=1 ( 1 − λ r(K) σ(k) ) = ¯Φk(λ) (see (7) for the deﬁnition). It is well-known in the literature of optimization and numerical linear algebra that the K-degree scale-and-shifted polynomial satisﬁes max λ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐≤2 ( 1 −2 √m√ L+ √m )K = O (( 1 −Θ (√m L ))K) , 12Published as a conference paper at ICLR 2023 which is restated in Lemma 3 and its proof is replicated in Appendix B of our paper for the reader’s convenience. Applying this result, one gets a simple proof of the accelerated linear rate of GD with the proposed scheme of step sizes for minimizing quadratic functions. A nice blog article by Pedregosa (2021) explains this in detail. Now we are ready to highlight its connection with HMC. In Lemma 1 of the paper, we restate a known result in HMC literature, where its proof is also replicated in Appendix B for the reader’s convenience. The lemma indicates that the convergence rate of HMC is governed by maxj∈[d] |∏K k=1 cos( √ 2λjη(K) k )|. By way of comparison to that of GD for minimizing quadratic functions, i.e., maxj∈[d] |∏K k=1(1 −ηkλj)|, it appears that they share some similarity, which made us wonder if we could bound the former by the latter. We show in Lemma 4 that cos(π 2 √x) ≤1 −x, which holds for all x≥0, and consequently, |PCos K (λ)|:= ⏐⏐⏐⏐⏐⏐ K∏ k=1 cos  π 2 √ λ r(K) σ(k)   ⏐⏐⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐⏐⏐ K∏ k=1  1 − λ r(K) σ(k)   ⏐⏐⏐⏐⏐⏐ = ⏐⏐¯ΦK(λ) ⏐⏐, The key lemma above implies that if we set the integration time as η(K) k = π 2 1√ 2r(K) σ(k) , then we get acceleration of HMC. B P ROOFS OF LEMMAS IN SECTION 2 We restate the lemmas for the reader’s convenience. Lemma 1. (Vishnoi, 2021) Let x0,y0 ∈ Rd. Consider the following coupling: (xt,vt) = HMCt(x0,ξ) and (yt,ut) = HMC t(y0,ξ) for some ξ ∈ Rd. Then for all t ≥ 0 and for all j ∈[d], it holds that xt[j] −yt[j] = cos (√ 2λjt ) ×(x0[j] −y0[j]). Proof. Given (xt,vt) := HMC t(x0,ξ) and (yt,ut) := HMC t(y0,ξ), we have dvt dt −dut dt = −∇f(xt) + ∇f(yt) = 2Λ( yt −xt). Therefore, we have d2(xt[j]−yt[j]) dt2 = −2λj(xt[j] −yt[j]), for all j ∈[d]. Because of the initial condition dx0[j] dt = dy0[j] dt = ξ[j], the differential equation implies that xt[j] −yt[j] = cos (√ 2λjt ) ×(x0[j] −y0[j]). It is noted that the result also follows directly from the explicit solution (4). Lemma 2.(Vishnoi, 2021) Let π∝exp(−f) = N(0,Λ−1) be the target distribution, where f(x) is deﬁned on (3). Let ρK be the distribution of xK generated by Algorithm 1 at the ﬁnal iteration K. Then for any ρ0 and any K ≥1, we have W2(ρK,π) ≤maxj∈[d] ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐W2(ρ0,π). Proof. Starting from x0 ∼ρ0, draw an initial point y0 ∼π such that (x0,y0) has the optimal W2-coupling between ρ0 and π. Consider the following coupling at each iteration k: (xk,vk) = HMCη(K) k (xk−1,ξk) and (yk,uk) = HMC η(K) k (yk−1,ξk) where ξk ∼N (0,I) is an independent Gaussian. We collect {xk}K k=1 and {yk}K k=1 from Algorithm 1. We know each yk ∼π, since πis a 13Published as a conference paper at ICLR 2023 stationary distribution of the HMC Markov chain. Then by Lemma 1 we have W2 2 (ρK,π) ≤E[∥xK −yK∥2] = E[∑ j∈[d](xK[j] −yK[j])2] = E[∑ j∈[d] ( ΠK k=1cos (√ 2λjη(K) k ) ×(x0[j] −y0[j]) )2 ] ≤ ( maxj∈[d] ( ΠK k=1cos (√ 2λjη(K) k ))2) E[∑ j∈[d](x0[j] −y0[j])2] = ( maxj∈[d] ( ΠK k=1cos (√ 2λjη(K) k ))2) W2 2 (ρ0,π), (16) Taking the square root on both sides leads to the result. Lemma 3. (e.g., Section 2.3 in d’Aspremont et al. (2021)) For any positive integerK, we have maxλ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐≤2 ( 1 −2 √m√ L+√m )K = O (( 1 −Θ (√m L ))K) . (17) Proof. Observe that the numerator of ¯ΦK(λ) = ΦK(h(λ)) ΦK(h(0)) satisﬁes |ΦK(h(λ))| ≤1, since h(λ) ∈[−1,1] for λ ∈[m,L] and that the Chebyshev polynomial satisﬁes |ΦK(·)|≤ 1 when its argument is in [−1,1] by the deﬁnition. It remains to bound the denominator, which is ΦK(h(0)) = cosh ( Karccosh ( L+m L−m )) . Since arccosh ( L+m L−m ) = log ( L+m L−m + √( L+m L−m )2 −1 ) = log(θ), where θ:= √ L+√m√ L−√m, we have ΦK(h(0)) = cosh ( Karccosh ( L+m L−m )) = exp(Klog(θ))+exp(−Klog(θ)) 2 = θK+θ−K 2 ≥θK 2 . Combing the above inequalities, we obtain the desired result: max λ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐= max λ∈[m,L] ⏐⏐⏐⏐ ΦK(h(λ)) ΦK(h(0)) ⏐⏐⏐⏐≤ 2 θK = 2 ( 1 −2 √m√ L+ √m )K = O (( 1 −Θ (√m L ))K) . C P ROOF OF LEMMA 4 Lemma 4. Denote |PCos K (λ)|:= ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λ r(K) σ(k) )⏐⏐⏐⏐⏐. Suppose λ∈[m,L]. Then, we have for any positive integer K, |PCos K (λ)|≤ ⏐⏐¯ΦK(λ) ⏐⏐. (18) Proof. We use the fact that the K-degree scaled-and-shifted Chebyshev Polynomial can be written as, ¯ΦK(λ) = ΠK k=1 ( 1 − λ r(K) σ(k) ) , (19) 14Published as a conference paper at ICLR 2023 for any permutation σ(·), since {r(K) σ(k)}are its roots and ¯ΦK(0) = 1. So inequality (18) is equivalent to ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λ r(K) σ(k) )⏐⏐⏐⏐⏐≤ ⏐⏐⏐⏐ΠK k=1 ( 1 − λ r(K) σ(k) )⏐⏐⏐⏐. (20) To show (20), let us analyze the mapping ψ(x) := cos(π 2 √x) 1−x for x≥0, x̸= 1, with ψ(1) = π 4 by continuity, and show that maxx:x≥0 |ψ(x)|≤ 1, as (20) would be immediate. We have ψ′(x) = − π 4√x 1 1−x sin(π 2 √x) + cos(π 2 √x) 1 (1−x)2 .Hence, ψ′(x) = 0 when tan(π 2 √x) = 4√x π(1−x) . (21) Denote an extreme point of ψ(x) as ˆx, which satisﬁes (21). Then, using (21), we have |ψ(ˆx)|=⏐⏐⏐⏐ cos(π 2 √ ˆx) 1−ˆx ⏐⏐⏐⏐= ⏐⏐⏐⏐ π√ 16ˆx+π2(1−ˆx)2 ⏐⏐⏐⏐, where we used cos(π 2 √ ˆx) = π(1−ˆx)√ 16ˆx+π2(1−ˆx)2 or −π(1−ˆx)√ 16ˆx+π2(1−ˆx)2 . The denominator √ 16ˆx+ π2(1 −ˆx)2 has the smallest value at ˆx = 0 , which means that the largest value of |ψ(x)|happens at x= 0, which is 1. The proof is now completed. D A COMPARISON OF THE TOTAL INTEGRATION TIME (JIANG , 2022) Since the Chebyshev integration time are set to be some large values at some steps of HMC, it is natural to ask if the number of steps to get an ϵ2-Wasserstein distance is a fair metric. In this section, we consider the total integration time ∑K k=1 η(K) k to get an ϵdistance as another metric for the convergence. It is noted that the comparison between HMC with our integration time and HMC with the best constant integration time has been conducted by Jiang (2022), and our previous version did not have such a comparison. Below, we reproduce the comparision of Jiang (2022). Recall the number of iterations to get an ϵ 2-Wasserstein distance to the target distribution is K = O (√κlog (1 ϵ )) of HMC with the Chebyshev integration time (Theorem 1 in the paper). The average of the integration time is 1 K K∑ k=1 η(K) k = 1 K K∑ k=1 π 2 √ 2 1√ r(K) σ(k) = 1 K K∑ k=1 π 2 √ 2 1√ r(K) k , where we recall that a permutation σ(·) does not affect the average. Then, if Kis even, we can rewrite the averaged integration time as 1 K K∑ k=1 η(K) k = 1 K π 2 √ 2 K/2∑ k=1   1√ r(K) k + 1√ r(K) K+1−k  . Otherwise, Kis odd, and we can rewrite the averaged integration time as 1 K K∑ k=1 η(K) k = 1 K π 2 √ 2   1√ r(K) (K+1)/2 + (K−1)/2∑ k=1   1√ r(K) k + 1√ r(K) K+1−k    . We will show 1√ r(K) k + 1√ r(K) K+1−k ≤ 1√ r(K) ⌊K/2⌋ + 1√ r(K) K−⌊K/2⌋+1 , for any k= {1,2,..., ⌊K 2 ⌋}soon. Given this, we can further upper-bound the averaged integration time as 1 K K∑ k=1 η(K) k ≤ π 4 √ 2   1√ r(K) ⌊K/2⌋ + 1√ r(K) K−⌊K/2⌋+1  , 15Published as a conference paper at ICLR 2023 when Kis even; when Kis odd, we can upper-bound the averaged integration time as 1 K K∑ k=1 η(K) k ≤ 1 K π 2 √ 2   1√ r(K) (K+1)/2 + K−1 2   1√ r(K) ⌊K/2⌋ + 1√ r(K) K−⌊K/2⌋+1    . Using the deﬁnition of the Chebyshev root, we have r(K) ⌊K/2⌋= L+ m 2 −L−m 2 cos (( ⌊K 2 ⌋− 1 2 ) π K ) ≈L+ m 2 , where the approximation is because (⌊K 2 ⌋−1 2 )π K ≈π 2 when Kis large, and hencecos ( (⌊K 2 ⌋−1 2 )π K ) ≈ 0. Similarly, we can approximate r(K) K−⌊K/2⌋+1 = L+ m 2 −L−m 2 cos (( K−⌊K/2⌋+ 1 −1 2 ) π K ) ≈L+ m 2 as (K−⌊K/2⌋+1−1 2 )π K ≈π 2 when Kis large. Also, we can approximate r(K) (K+1)/2 ≈L+m 2 when Kis odd and large for the same reason. Combining the above, the total integration time of HMC with the Chebyshev scheme can be approxi- mated as number of iterations ×average integration time = √κlog (1 ϵ ) × 1 K K∑ k=1 η(K) k ≈√κlog (1 ϵ ) ×π 2 1√ L+ m. When κ:= L m is large, the total integration time becomes √κlog (1 ϵ ) ×π 2 1√ L+ m = Θ ( 1√mlog (1 ϵ )) . (22) Now let us switch to analyzing HMC with the best constant integration time η= Θ ( 1√ L ) (see e.g., (5), Vishnoi (2021)), which has the non-accelerated rate. Speciﬁcally, it needs K = O ( κlog (1 ϵ )) iterations to converge to the target distribution. Hence, the total integration time of HMC with the best constant integration time is number of iterations×average integration time = κlog (1 ϵ ) ×Θ ( 1√ L ) = Θ (√ L m log (1 ϵ )) . (23) By way of comparison ((22) vs. (23)), we see that the total integration time of HMC with the proposed scheme of Chebyshev integration time reduces by a factor √κ, compared with HMC with the best constant integration time. The remaining thing to show is the inequality 1√ r(K) k + 1√ r(K) K+1−k ≤ 1√ r(K) ⌊K/2⌋ + 1√ r(K) K+1−⌊K/2⌋ , (24) for any k= {1,2,..., ⌊K 2 ⌋}. 16Published as a conference paper at ICLR 2023 We have 1√ r(K) k + 1√ r(K) K+1−k = √ 2 ×   1√ L+ m−(L−m)cos ( (k−1 2 )π K )+ 1√ L+ m−(L−m)cos ( (K−k+ 1 2 )π K )   = √ 2 ×   1√ L+ m−(L−m)cos ( (k−1 2 )π K )+ 1√ L+ m+ (L−m)cos ( (k−1 2 )π K )   . (25) Now let us deﬁne H(k) :=   1√L+m−(L−m)cos ( (k−1 2 )π K ) + 1√L+m+(L−m)cos ( (k−1 2 )π K )  and treat kas a continuous variable. The derivative of H(k) is H′(k) = π 2K(L−m)sin (( k−1 2 ) π K ) ×   1 ( L+ m−(L−m)cos ( (k−1 2 )π K ))3/2 − 1 ( L+ m+ (L−m)cos ( (k−1 2 )π K ))3/2   >0. (26) That is, H′(k) is an increasing function of kwhen 1 ≤k≤⌊K 2 ⌋, which implies that the inequality (24). Now we have completed the analysis. 17Published as a conference paper at ICLR 2023 E E XPERIMENTS E.1 B AYESIAN LOGISTIC REGRESSION Table 4: Bayesian logistic regression HEART dataset ( ˆm= 2.59, ˆL= 92.43) Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 1693.71 ±63.53 520 .43 ±62.24 18 .54 ±2.88 5 .69 ±1.12 1 .00 ±0.00 0.001 Const. 312.18 ±12.65 80 .97 ±15.97 6 .57 ±0.42 1 .69 ±0.28 1 .00 ±0.00 0.005 Cheby. 1664.87 ±43.72 481 .76 ±49.00 82 .90 ±16.51 24 .08 ±5.72 0 .99 ±0.00 0.005 Const. 329.48 ±13.15 75 .78 ±17.30 31 .87 ±2.73 7 .40 ±2.06 0 .99 ±0.00 0.01 Cheby. 1648.25 ±47.50 508 .69 ±49.81 157 .09 ±26.70 48 .45 ±9.64 0 .99 ±0.00 0.01 Const. 307.52 ±8.77 82 .85 ±13.88 53 .89 ±6.37 14 .62 ±3.28 0 .99 ±0.00 0.05 Cheby. 1424.21 ±54.03 439 .88 ±56.25 458 .56 ±51.33 140 .51 ±16.58 0 .98 ±0.00 0.05 Const. 242.44 ±14.61 56 .42 ±17.68 103 .36 ±12.64 23 .90 ±7.40 0 .98 ±0.00 BREAST CANCER dataset ( ˆm= 1.81, ˆL= 69.28) Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 1037.98 ±34.46 575 .72 ±41.14 9 .40 ±0.31 5 .21 ±0.31 1 .00 ±0.00 0.001 Const. 174.73 ±13.91 78 .24 ±23.28 2 .59 ±0.29 2 .59 ±0.29 1 .00 ±0.00 0.005 Cheby. 1010.49 ±24.15 571 .03 ±36.64 43 .09 ±1.14 24 .35 ±1.70 0 .99 ±0.00 0.005 Const. 173.17 ±11.40 79 .76 ±13.49 11 .88 ±1.39 11 .88 ±1.39 0 .99 ±0.00 0.01 Cheby. 1038.10 ±31.48 565 .54 ±50.51 82 .82 ±3.51 45 .14 ±4.44 0 .99 ±0.00 0.01 Const. 162.64 ±9.43 58 .79 ±16.02 18 .92 ±2.59 18 .92 ±2.59 0 .99 ±0.00 0.05 Cheby. 886.24 ±38.92 499 .54 ±43.99 240 .08 ±12.55 135 .28 ±12.04 0 .98 ±0.00 0.05 Const. 99.48 ±10.10 44 .70 ±13.23 33 .25 ±6.50 33 .25 ±6.50 0 .98 ±0.00 DIABETES dataset ( ˆm= 4.96, ˆL= 270.20) Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 726.08 ±33.92 424 .59 ±58.77 11 .64 ±0.85 6 .83 ±1.16 0 .99 ±0.00 0.001 Const. 100.50 ±9.32 41 .84 ±19.33 3 .6 ±0.31 1 .50 ±0.68 0 .99 ±0.00 0.005 Cheby. 731.46 ±33.04 395 .82 ±47.98 54 .92 ±5.26 29 .61 ±3.75 0 .99 ±0.00 0.005 Const. 100.16 ±11.83 44 .62 ±20.81 14 .71 ±2.52 6 .67 ±3.37 0 .99 ±0.00 0.01 Cheby. 687.74 ±29.31 399 .44 ±45.01 93 .10 ±6.78 53 .90 ±5.38 0 .98 ±0.00 0.01 Const. 83.04 ±9.36 36 .39 ±12.43 20 .87 ±3.31 9 .09 ±3.25 0 .98 ±0.00 0.05 Cheby. 546.80 ±37.40 330 .09 ±34.31 206 .07 ±17.76 125 .07 ±18.87 0 .96 ±0.00 0.05 Const. 57.11 ±9.52 23 .44 ±9.57 27 .23 ±5.18 11 .02 ±4.34 0 .96 ±0.00 E.2 S AMPLING FROM A hard DISTRIBUTION Table 5: Sampling from a distribution π(x) ∝exp(−fh(x)) whose potential fh(·) is deﬁned on (15). Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob sampling from π(x) ∝exp(−f0.001(x)) 0.001 Cheby. 6222.21 ±88.90 453 .03 ±30.35 114 .74 ±7.59 8 .36 ±0.83 1 .00 ±0.00 0.001 Const. 2098.18 ±46.56 63 .53 ±15.00 82 .31 ±5.29 2 .50 ±0.63 1 .00 ±0.00 sampling from π(x) ∝exp(−f0.005(x)) 0.005 Cheby. 6271.43 ±117.71 429 .42 ±34.52 545 .76 ±26.10 37 .28 ±2.29 0 .99 ±0.00 0.005 Const. 2125.36 ±21.87 67 .42 ±16.51 361 .14 ±5.65 11 .44 ±2.76 0 .99 ±0.00 sampling from π(x) ∝exp(−f0.01(x)) 0.01 Cheby. 6523.21 ±95.65 459 .48 ±38.83 1070 .77 ±68.78 75 .61 ±9.79 0 .99 ±0.00 0.01 Const. 2125.04 ±31.83 69 .66 ±20.75 528 .35 ±80.17 17 .19 ±6.34 0 .99 ±0.00 sampling from π(x) ∝exp(−f0.05(x)) 0.05 Cheby. 6457.21 ±110.05 375 .97 ±30.64 3319 .51 ±134.92 193 .06 ±14.49 0 .97 ±0.00 0.05 Const. 2796.41 ±56.89 62 .33 ±13.26 1893 .99 ±57.23 42 .22 ±9.05 0 .97 ±0.00 18
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Acceleration via fractal learning rate schedules",
        "A conceptual introduction to Hamiltonian Monte Carlo",
        "Mixing time guarantees for unadjusted Hamiltonian Monte Carlo",
        "Randomized Hamiltonian Monte Carlo",
        "Evaluating the implicit midpoint integrator for Riemannian manifold Hamiltonian Monte Carlo",
        "Stochastic gradient Hamiltonian Monte Carlo",
        "Fast mixing of Metropolized Hamiltonian Monte Carlo: Beneﬁts of multi-step gradients",
        "Optimal convergence rate of Hamiltonian Monte Carlo for strongly logconcave distributions",
        "Exponential ergodicity of mirror-langevin diffusions",
        "Theoretical guarantees for approximate sampling from a smooth and log-concave density",
        "On sampling from a log-concave density using kinetic Langevin diffusions",
        "Acceleration methods",
        "Hybrid monte carlo",
        "Randomized smoothing for stochastic optimization",
        "On the convergence of Hamiltonian Monte Carlo",
        "Analysis of Langevin Monte Carlo via convex optimization",
        "Riemann manifold Langevin and Hamiltonian Monte Carlo methods",
        "Entropy-based adaptive Hamiltonian Monte Carlo",
        "The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo",
        "An adaptive-MCMC scheme for setting trajectory lengths in Hamiltonian Monte Carlo",
        "On the dissipation of ideal hamiltonian monte carlo sampler",
        "Optimal convergence rate of Hamiltonian Monte Carlo for strongly log-concave distributions",
        "Dimensionally tight bounds for second-order Hamiltonian Monte Carlo",
        "MCMC using Hamiltonian dynamics",
        "Introductory lectures on convex optimization: a basic course",
        "Acceleration without momentum",
        "Some methods of speeding up the convergence of iteration methods",
        "Metropolis Adjusted Langevin trajectories: a robust alternative to Hamiltonian Monte Carlo",
        "Bayesian learning via Stochastic Gradient Langevin dynamics",
        "Hamiltonian dynamics with non-newtonian momentum for rapid sampling",
        "Magnetic Hamiltonian Monte Carlo",
        "An introduction to Hamiltonian Monte Carlo method for sampling",
        "Escaping saddle points faster with stochastic momentum",
        "Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out",
        "How good is the Bayes posterior in deep neural networks really",
        "On the convergence of Hamiltonian Monte Carlo with stochastic gradients",
        "Lower bounds on Metropolized sampling methods for well-conditioned distributions",
        "Structured logconcave sampling with a restricted gaussian oracle",
        "Generalizing Hamiltonian Monte Carlo with neural networks",
        "Sqrt(d) dimension dependence of Langevin Monte Carlo",
        "Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out",
        "A modular analysis of provable acceleration via Polyak’s momentum: Training a wide ReLU network and a deep linear network",
        "How to escape saddle points efﬁciently",
        "A modular analysis of provable acceleration via Polyak’s momentum: Training a wide ReLU network and a deep linear network",
        "Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out",
        "Acceleration via fractal learning rate schedules",
        "How to escape saddle points efﬁciently",
        "Introductory lectures on convex optimization: a basic course"
    ]
}
