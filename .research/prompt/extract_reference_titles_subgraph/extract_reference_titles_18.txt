
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Karthik A. Sankararaman* 1 2 Soham De* 3 Zheng Xu 2 W. Ronny Huang2 Tom Goldstein2 Abstract This paper studies how neural network architec- ture affects the speed of training. We introduce a simple concept called gradient confusion to help formally analyze this. When gradient confusion is high, stochastic gradients produced by different data samples may be negatively correlated, slow- ing down convergence. But when gradient confu- sion is low, data samples interact harmoniously, and training proceeds quickly. Through theoreti- cal and experimental results, we demonstrate how the neural network architecture affects gradient confusion, and thus the efﬁciency of training. Our results show that, for popular initialization tech- niques, increasing the width of neural networks leads to lower gradient confusion, and thus faster model training. On the other hand, increasing the depth of neural networks has the opposite effect. Our results indicate that alternate initial- ization techniques or networks using both batch normalization and skip connections help reduce the training burden of very deep networks. 1. Introduction Stochastic gradient descent (SGD) (Robbins & Monro, 1951) and its variants with momentum have become the standard optimization routine for neural networks due to their fast convergence and good generalization properties (Wilson et al., 2017; Sutskever et al., 2013; Smith et al., 2020). Yet the convergence behavior of SGD on neural networks still eludes full theoretical understanding. Fur- thermore, it is not well understood how design choices on neural network architecture affect training performance. In this paper, we make progress on these open questions. *Equal contribution 1Facebook. 2University of Maryland, Col- lege Park. 3DeepMind, London.. Correspondence to: Karthik A. Sankararaman <karthikabinavs@gmail.com>, Soham De <so- hamde@google.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Classical stochastic optimization theory predicts that the learning rate of SGD needs to decrease over time for con- vergence to be guaranteed to the minimizer of a convex function (Shamir & Zhang, 2013; Bertsekas, 2011). For strongly convex functions for example, such results show that a decreasing learning rate schedule of O(1/k) is re- quired to guarantee convergence to within ϵ-accuracy of the minimizer in O(1/ϵ) iterations, where kdenotes the itera- tion number. Such decay schemes, however, typically lead to poor performance on standard neural network problems. Neural networks operate in a regime where the number of parameters is much larger than the number of training data. In this “over-parameterized” regime, SGD seems to converge quickly with constant learning rates. Most neu- ral network practitioners use a constant learning rate for the majority of training (with exponential decay only to- wards the end of training) without seeing the method stall (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016; Zagoruyko & Komodakis, 2016). With constant learning rates, theoretical guarantees show that SGD con- verges quickly to a neighborhood of the minimizer, but then reaches a noise ﬂoor beyond which it stops converging; this noise ﬂoor depends on the learning rate and the variance of the gradients (Moulines & Bach, 2011; Needell et al., 2014). Recent results show that convergence without a noise ﬂoor is possible without decaying the learning rate, provided the model is strongly convex and overﬁtting occurs (Schmidt & Roux, 2013; Ma et al., 2017; Vaswani et al., 2018). While these results do give important insights, they do not fully explain the dynamics of SGD on neural networks, and how they relate to over-parameterization. Furthermore, training performance is strongly inﬂuenced by network ar- chitecture. It is common knowledge among practitioners that, under standard Gaussian initialization techniques (Glo- rot & Bengio, 2010; He et al., 2015), deeper networks train slower (Bengio et al., 1994; Saxe et al., 2013). This has led to several innovations over the years to get deeper nets to train more easily, such as careful initialization strategies (Xiao et al., 2018), residual connections (He et al., 2016), and normalization schemes like batch normalization (Ioffe & Szegedy, 2015). Furthermore, there is evidence to indi- cate that wider networks are faster to train (Zagoruyko & Komodakis, 2016; Nguyen & Hein, 2017), and recent the- arXiv:1904.06963v5  [cs.LG]  6 Jul 2020The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent oretical results suggest that the dynamics of SGD simplify considerably for very wide networks (Jacot et al., 2018; Lee et al., 2019). In this paper, we make progress on the- oretically understanding these empirical observations and unifying existing theoretical results. To this end, we identify and analyze a condition that enables us to establish direct relationships between layer width, network depth, problem dimensionality, initialization schemes, and trainability and SGD dynamics for over-parameterized networks. Our contributions. Typical neural networks are over- parameterized (i.e., the number of parameters exceed the number of training points). In this paper, we ask how this over-parameterization, and more speciﬁcally the network ar- chitecture, affects the trainability of neural networks and the dynamics of SGD. Through extensive theoretical and exper- imental studies, we show how layer width, network depth, initialization schemes, and other architecture choices affect the dynamics. The following are our main contributions.1 • We identify a condition, termed gradient confusion, that impacts the convergence properties of SGD on over-parameterized models. We prove that high gradi- ent confusion may lead to slower convergence, while convergence is accelerated (and could be faster than predicted by existing theory) if confusion is low, in- dicating a regime where constant learning rates work well in practice (sections 2 and 3). We use the gradi- ent confusion condition to study the effect of various architecture choices on trainability and convergence. • We study the effect of neural network architecture on gradient confusion at standard Gaussian initialization schemes (section 4), and prove (a) gradient confusion increases as the network depth increases, and (b) wider networks have lower gradient confusion. These indi- cate that deeper networks are more difﬁcult to train and wider networks can improve trainability of networks. Directly analyzing the gradient confusion bound en- ables us to derive results on the effect of depth and width, without requiring restrictive assumptions like large layer widths (Du et al., 2018; Allen-Zhu et al., 2018). Our results hold for a large class of neural networks with different non-linear activations and loss- functions. In section 5, we present a more general result on the effect of depth on the trainability of net- works without assuming the network is at initialization. • We prove that for linear neural networks, gradient con- fusion is independent of depth when using orthogonal initialization schemes (section 6) (Saxe et al., 2013; Schoenholz et al., 2016). This indicates a way forward in developing techniques for training deeper models. 1To keep the main text of the paper concise, all proofs and sev- eral additional experimental results are delegated to the appendix. • We test our theoretical predictions using extensive experiments on wide residual networks (WRNs) (Zagoruyko & Komodakis, 2016), convolutional net- works (CNNs) and multi-layer perceptrons (MLPs) for image classiﬁcation tasks on CIFAR-10, CIFAR-100 and MNIST (section 7 and appendix A). We ﬁnd that our theoretical results consistently hold across all our experiments. We further show that the combination of batch normalization and skip connections in residual networks help lower gradient confusion, thus indicat- ing why SGD can efﬁciently train deep neural networks that employ such techniques. 2. Gradient confusion Notations. We denote vectors in bold lower-case and ma- trices in bold upper-case. We use (W)i,j to indicate the (i,j) cell in matrix W and (W)i for the ith row of matrix W. ∥W∥denotes the operator norm of W. [N] denotes {1,2,...,N }and [N]0 denotes {0,1,...,N }. Preliminaries. Given N training points (speciﬁed by the corresponding loss functions {fi}i∈[N]), we use SGD to solve empirical risk minimization problems of the form, minw∈Rd F(w) := minw∈Rd 1 N ∑N i=1 fi(w), (1) using the following iterative update rule for T rounds: wk+1 = wk −α∇˜fk(wk). (2) Here α is the learning rate and ˜fk is a function chosen uniformly at random from {fi}i∈[N] at iteration k ∈[T]. w⋆ = arg minw F(w) denotes the optimal solution. Gradient confusion. SGD works by iteratively selecting a random function ˜fk, and modifying the parameters to move in the direction of the negative gradient of˜fk. It may happen that the selected gradient ∇˜fk is negatively correlated with the gradient of another term ∇fj.When the gradients of dif- ferent mini-batches are negatively correlated, the objective terms disagree on which direction the parameters should move, and we say that there is gradient confusion.2 Deﬁnition 2.1. A set of objective functions {fi}i∈[N] has gradient confusion bound η≥0 if the pair-wise inner prod- ucts between gradients satisfy, for a ﬁxed w ∈Rd, ⟨∇fi(w),∇fj(w)⟩≥− η, ∀i̸= j ∈[N]. (3) Observations in simpliﬁed settings. SGD converges fast when gradient confusion is low along its path. To see why, 2Gradient confusion is related to both gradient variance and gradient diversity (Yin et al., 2017), but with important differences, which we discuss in section 9. We also discuss alternate deﬁnitions of the gradient confusion condition in section 8.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Figure 1.Linear regression on an over-parameterized (d = 120) and under-parameterized (d= 80) model with N = 100samples generated randomly from a Gaussian, trained using SGD with mini- batch size 1. Plots are averaged over 3 independent runs. Gradient cosine similarities were calculated over all pairs of gradients. consider the case of training a logistic regression model on a dataset with orthogonal vectors. We have fi(w) = L(yix⊤ i w),where L: R →R is the logistic loss,{xi}i∈[N] is a set of orthogonal training vectors, and yi ∈{−1,1} is the label for xi. We then have ∇fi(w) = ζixi,where ζi = yiL′(yi ·x⊤ i w).Note that the gradient confusion is 0 since ⟨∇fi(w),∇fj(w)⟩= ζiζj⟨xi,xj⟩= 0, ∀i,j ∈[N] and i ̸= j. Thus, an update in the gradient direction fi has no effect on the loss value of fj for i ̸= j. In this case, SGD decouples into (deterministic) gradient descent on each objective term separately, and we can expect to see the fast convergence rates attained by gradient descent. Can we expect a problem to have low gradient confusion in practice? From the logistic regression problem, we have: |⟨∇fi(w),∇fj(w)⟩|= |⟨xi,xj⟩|·|ζiζj|.This inner prod- uct is expected to be small for allw; the logistic loss satisﬁes |ζiζj|< 1, and for ﬁxed N the quantity maxij|⟨xj,xi⟩| is O(1/ √ d) whenever {xi}are randomly sampled from a sphere (see lemma B.1 for the formal statement).3 Thus, we would expect a random linear model to have nearly orthog- onal gradients, when the number of parameters is "large" and the number of training data is "small", i.e., when the model is over-parameterized. This is further evidenced by a toy example in ﬁgure 1, where we show a slightly over- parameterized linear regression model can have much faster convergence rates, as well as lower gradient confusion. One can prove a similar result for problems that have random and low-rank Hessians, which suggests that one might expect gradient to be small near the minimizer for many standard neural nets (see appendix C for more discussion). The above arguments are a bit simplistic, considering toy scenarios and ignoring issues like the effect of network structure. In the following sections, we rigorously analyze the effect of gradient confusion on the speed of convergence on non-convex problems, and the effect of width and depth of the neural network architecture on the gradient confusion. 3Generally, this is true whenever xi = 1√ dyi,where yi is an isotropic random vector (Vershynin, 2018). 3. SGD is fast when gradient confusion is low Several prior papers have analyzed the convergence rates of constant learning rate SGD (Nedi ´c & Bertsekas, 2001; Moulines & Bach, 2011; Needell et al., 2014). These re- sults show that for strongly convex and Lipschitz smooth functions, SGD with a constant learning rate αconverges linearly to a neighborhood of the minimizer. The noise ﬂoor it converges to depends on the learning rate αand the vari- ance of the gradients at the minimizer, i.e., Ei∥∇fi(w⋆)∥2. To guarantee convergence to ϵ-accuracy in such a setting, the learning rate needs to be small, i.e., α = O(ϵ), and the method requires T = O(1/ϵ) iterations. Some more recent results show convergence of constant learning rate SGD without a noise ﬂoor and without small step sizes for models that can completely ﬁt the data (Schmidt & Roux, 2013; Ma et al., 2017; Vaswani et al., 2018). Gradient confusion is related to these results. Cauchy- Schwarz inequality implies that if Ei∥∇fi(w⋆)∥2 = O(ϵ), then Ei,j|⟨∇fi(w⋆),∇fj(w⋆)⟩|= O(ϵ), ∀i,j. Thus the gradient confusion at the minimizer is small when the vari- ance of the gradients at the minimizer is small. Further note that when the variance of the gradients at the mini- mizer is O(ϵ), a direct application of the results in Moulines & Bach (2011) and Needell et al. (2014) shows that con- stant learning rate SGD has fast convergence to ϵ-accuracy in T = O(log(1/ϵ)) iterations, without the learning rate needing to be small. Generally however, bounded gradient confusion does not provide a bound on the variance of the gradients (see section 9). Thus, it is instructive to derive con- vergence bounds of SGD explicitly in terms of the gradient confusion to properly understand its effect. We ﬁrst consider functions satisfying the Polyak- Lojasiewicz (PL) inequality (Lojasiewicz, 1965), a condi- tion related to, but weaker than, strong convexity, and used in recent work (Karimi et al., 2016; De et al., 2017). We provide bounds on the rate of convergence in terms of the optimality gap. We start with two standard assumptions. (A1) {fi}i∈[N] are Lipschitz smooth: fi(w′) ≤fi(w)+ ∇fi(w)⊤(w′−w)+ L 2 ∥w′−w∥2. (A2) {fi}i∈[N] satisfy the PL inequality: 1 2 ∥∇fi(w)∥2 ≥µ(fi(w) −f⋆ i ),f ⋆ i = minw fi(w). We now state a convergence result of constant learning rate SGD in terms of the gradient confusion. Theorem 3.1. If the objective function satisﬁes (A1) and (A2), and has gradient confusion η, SGD converges linearly to a neighborhood of the minima of problem (1) as: E[F(wT) −F⋆] ≤ρT(F(w0) −F⋆) + αη 1−ρ, where α< 2 NL, ρ= 1−2µ N ( α−NLα2 2 ) , F⋆ = minw F(w) and w0 is the initialized weights.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent This result shows that SGD converges linearly to a neigh- borhood of a minimizer, and the size of this neighborhood depends on the level of gradient confusion. When the gra- dient confusion is small, i.e., η= O(ϵ), SGD has fast con- vergence to O(ϵ)-accuracy in T = O(log(1/ϵ)) iterations, without requiring the learning rate to be vanishingly small. We now extend this to general smooth functions. Theorem 3.2. If the objective satisﬁes (A1) and has gradi- ent confusion η, then SGD converges to a neighborhood of a stationary point of problem (1) as: mink=1,...,T E∥∇F(wk)∥2 ≤ρ(F(w1)−F⋆) T + ρη, for α< 2 NL, ρ= 2N 2−NLα, and F⋆ = minw F(w). Thus, as long as η = O(1/T), SGD has fast O(1/T) con- vergence on smooth non-convex functions. Theorems 3.1 and 3.2 predict an initial phase of optimization with fast con- vergence to the neighborhood of a minimizer or a stationary point. This behavior is often observed when optimizing neural nets (Darken & Moody, 1992; Sutskever et al., 2013), where a constant learning rate reaches a high level of ac- curacy on the model. As we show in subsequent sections, this is expected since for neural networks typically used, the gradient confusion is expected to be low. See section 9 for more discussion on the above results and how they relate to previous work. We stress that our goal is not to study convergence rates per se, nor is it to prove state-of-the-art rate bounds for this class of problems. Rather, we show the direct effect that the gradient confusion bound has on the convergence rate and the noise ﬂoor for constant learning rate SGD. As we show in the following sections, this new perspective in terms of the gradient confusion helps us more directly understand how neural network architecture design affects SGD dynamics and why. 4. Effect of neural network architecture at Gaussian initializations To draw a connection between neural network architecture and training performance, we analyze gradient confusion for generic (i.e., random) model problems using methods from high-dimensional probability. In this section, we analyze the effect of neural network architecture at the beginning of training, when using standard Gaussian initialization tech- niques. Analyzing these models at initialization is important to understand which architectures are more easily trainable than others. Our results cover a wide range of scenarios compared to prior work, require minimal additional assump- tions, and hold for a large family of neural networks with different non-linear activation functions and loss-functions. In particular, our results hold for fully connected networks (and can be extended to convolutional networks) with the square-loss and logistic-loss functions, and commonly used non-linear activations such as sigmoid, tanh and ReLU. We consider both the case where the input data is arbitrary but bounded (theorem 4.1, part 1), as well as where the input data is randomly drawn from the surface of a unit sphere (theorem 4.1, part 2). Setting. We consider training dataD= {(xi,C(xi))}i∈[N], with labeling function C: Rd →[−1,1]. For some of our results, we consider that the data points {xi}are drawn uniformly at random from the surface of a d-dimensional unit sphere. The labeling function satisﬁes |C(x)|≤ 1 and ∥∇xC(x)∥2 ≤1 for ∥x∥≤ 1.Note that this automatically holds for every model considered in this paper where the labeling function is realizable (i.e., where the model can express the labeling function using its parameters). More generally, this assumes a Lipschitz condition on the labels (i.e., the labels don’t change too quickly with the inputs). We consider two loss-functions: square-loss for regres- sion and logistic loss for classiﬁcation. The square-loss function is deﬁned as fi(w) = 1 2 (C(xi) −gw(xi))2 and the logistic function is deﬁned as fi(w) = log(1 + exp(−C(xi)gw(xi))). Here, gw : Rd →R denotes the parameterized function we ﬁt to the training data and fi(w) denotes the loss-function of hypothesis gw on data point xi. Let W0 ∈Rℓ1×d and {Wp}p∈[β] where Wp ∈Rℓp×ℓp−1 are weight matrices. Let W denote the tuple (Wp)p∈[β]0 . Deﬁne ℓ := maxp∈[β] ℓp to be the width and β to be the depth of the network. Then, the model gW is deﬁned as gW(x) :=σ(Wβσ(Wβ−1 ...σ (W1σ(W0x)) ... )), where σdenotes the non-linear activation function applied point-wise to its arguments. We assume that the activation is given by a function σ(x) with the following properties. • (P1) Boundedness: |σ(x)|≤ 1 for x∈[−1,1]. • (P2) Bounded differentials: Let σ′(x) and σ′′(x) de- note the ﬁrst and second sub-differentials respectively. Then, |σ′(x)|≤ 1 and |σ′′(x)|≤ 1 for all x∈[−1,1]. When ∥x∥≤ 1, activation functions such as sigmoid, tanh, softmax and ReLU satisfy these requirements. Furthermore, in this section, we consider the following Gaussian weight initialization strategy. Strategy 4.1. W0 ∈Rℓ×d has independent N(0,1 d) en- tries. For every p∈[β], the weights Wp ∈Rℓp×ℓp−1 have independent N ( 0, 1 κℓp−1 ) entries for some constant κ> 0. This initialization strategy with different settings of κare used almost universally for neural networks (Glorot & Ben- gio, 2010; LeCun et al., 2012; He et al., 2015). For instance,The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent typically κ= 1 2 when ReLU activations are used, andκ= 1 when tanh activations are used. Main result. The following theorem shows how the width ℓ:= maxp∈[β] ℓp and the depth βaffect the gradient confu- sion condition at standard initializations. We show that as width increases (for ﬁxed depth) or depth decreases (for ﬁxed width) the probability that the gradient confusion bound (equation 3) holds increases. Thus, as the depth in- creases (with ﬁxed width), training a model becomes harder, while as the width increases (with ﬁxed depth), training a model becomes easier. Furthermore, note that this result also implies that training very deep linear neural networks (with identity activation functions) with standard Gaussian initializations is hard. Throughout the paper, we deﬁne the parameter ζ0 := 2√β. See the appendix (Lemma D.1) for a more careful deﬁnition of this quantity. Theorem 4.1. Let W0,W1,..., Wβ be weight matrices chosen according to strategy 4.1. There exists ﬁxed con- stants c1,c2 >0 such that we have the following. 1. Consider a ﬁxed but arbitrary dataset x1,x2,..., xN with ∥xi∥ ≤1 for every i ∈[N]. For η > 4, the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −cℓ2β(η−4)2 64ζ4 0 (β+2)4 ) . 2. If the dataset {xi}i∈[N] is such that each xi is an i.i.d. sample from the surface of d-dimensional unit sphere, then for every η >0 the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −c2(ℓd+ℓ2β)η2 16ζ4 0 (β+2)4 ) . Theorem 4.1 shows that under popular Gaussian initializa- tions used, training becomes harder as networks get deeper. The result however also shows a way forward: layer width improves the trainability of deep networks. Other related work supports this showing that when the layers are in- ﬁnitely wide, the learning dynamics of gradient descent simpliﬁes considerably (Jacot et al., 2018; Lee et al., 2019). Hanin & Rolnick (2018) also suggest that the width should increase linearly with depth in a neural network to help dynamics at the beginning of training. In section 7 and appendix A, we show substantial empirical evidence that, given a sufﬁciently deep network, increasing the layer width often helps in lowering gradient confusion and speeding up convergence for a range of models. 5. A more general result on the effect of depth While our results in section 4 hold at standard initialization schemes, in this section we derive a more general version of the result. In particular, we assume the setting where the data is drawn uniformly at random from a unit sphere and the weights lie in a ball around a local minimizer. Our results hold for both fully connected networks and convolutional networks with the square-loss and logistic-loss functions, and commonly-used non-linear activations such as sigmoid, tanh, softmax and ReLU. We consider the same setup as in the previous section, and assume additionally that the data points {xi}are drawn uniformly from the surface of a d-dimensional unit sphere. Additionally, instead of studying the network at initializa- tion, we make the following assumption on the weights. Assumption 1 (Small Weights). We assume that the oper- ator norm of the weight matrices {Wi}i∈[β]0 are bounded above by 1, i.e., for every i∈[β]0 we have ∥Wi∥≤ 1. The operator norm of the weight matrices ∥W∥being close to 1 is important for the trainability of neural networks, as it ensures that the input signal is passed through the net- work without exploding or shrinking across layers (Glorot & Bengio, 2010). Proving non-vacuous bounds in case of such blow-ups in magnitude of the signal or the gradient is not possible in general, and thus, we consider this restricted class of weights. Most standard neural networks are trained using weight decay regularizers of the form ∑ i∥Wi∥2 F. This biases the weights to be small when training neural networks in practice. See appendix F for further discussion on the small weights assumption. We now present a more general version of theorem 4.1. Theorem 5.1. Let W0,W1,..., Wβ satisfy assumption 1. For some ﬁxed constantc> 0, the gradient confusion bound (equation 3) holds with probability at least 1 −N2 exp ( −cdη2 16ζ4 0 (β+2)4 ) . Theorem 5.1 shows that (for ﬁxed dimension dand number of samples N) when the depth β decreases, the probabil- ity that the gradient confusion bound in equation 3 holds increases, and vice versa. Thus, our results indicate that in the general case when the weights are small, increasing the network depth will typically lead to slower model training. Note that on assuming ∥W∥≤ 1 for each weight matrix W, the dependence of gradient confusion on the width goes away. To see why this, consider an example where each weight matrix in the neural network has exactly one non-zero element, which is set to 1. The operator norm of each such weight matrix is 1, but the forward or backward propagated signals would not depend on the width. Note that the convergence rate results of SGD in section 3 assume that the gradient confusion bound holds at every point along the path of SGD. On the other hand, theoremThe Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 5.1 shows concentration bounds for the gradient confusion at a ﬁxed weight W. Thus, to make the above result more relevant for the convergence of SGD on neural networks, we now make the concentration bound in theorem 5.1 uniform over all weights inside a ball Br of radius r. Corollary 5.1. Select a point W = (W0,W1,..., Wβ), satisfying assumption 1. Consider a ball Br centered at W of radius r > 0. If the data {xi}i∈[N] are sampled uniformly from a unit sphere, then the gradient confusion bound in equation 3 holds uniformly at all points W′∈Br with probability at least 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 ) , if r≤η/4ζ2 0 , 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 + 8dζ2 0 r η ) , otherwise. Corollary 5.1 shows that the probability that the gradient confusion bound holds decreases with increasing depth, for all weights in a ball around the minimizer.4 This explains why, in the general case, training very deep models might always be hard. This raises the question why most deep neural networks used in practice are so efﬁciently trained using SGD. While careful Gaussian initialization strategies prevent vanishing or exploding gradients, these strategies still suffer from high gradient confusion for very deep net- works unless the width is also increased with the depth, as we show in section 4. Practitioners over the years, however, have achieved state-of-the-art results by making networks deeper, without necessarily making networks wider. Thus, in section 7, we empirically study how popular techniques used in these models like skip connections and batch nor- malization affect gradient confusion. We ﬁnd that these techniques drastically lower gradient confusion, making deep networks signiﬁcantly easier to train. Furthermore, in the next section, we show how deep linear nets are train- able when used with orthogonal initialization techniques, indicating a way forward for training deeper models. 6. Gradient confusion is independent of depth for orthogonal initializations In this section, we show that for deep linear neural networks, gradient confusion is independent of depth when the weight matrices are initialized as orthogonal matrices.5 Consider the following linear neural network: gW(x) :=γWβ ·Wβ−1 ·... ·W1 ·x, (4) where the rescaling parameter γ = 1√2β, and assume we use the squared loss function. Then we have the following. 4The above results automatically hold for convolutional net- works, since a convolution operation on x can be represented as a matrix multiplication Ux for an appropriate Toeplitz matrix U. 5An orthogonal matrix A satisﬁes AT ·A = A ·AT = I. Theorem 6.1. Let {Wi}i∈[β] be arbitrary orthogonal ma- trices that satisfy assumption 1. Let the dataset {xi}i∈[N] be such that each xi is an i.i.d. sample from the surface of d-dimensional unit sphere. Consider the linear neural network in equation 4 that minimizes the empirical square loss function. For some ﬁxed constant c> 0, the gradient confusion bound (equation 3) holds with probability at least 1 −N2 exp ( −cdη2) . From Theorem 6.1, we see that the probability does not depend on the depth βor maximum width ℓ. Thus, trainabil- ity does not get worse with depth when using orthogonal initializations. This result matches previous theoretical and empirical results showing the efﬁciency of orthogonal ini- tialization techniques for training very deep linear or tanh networks (Saxe et al., 2013; Schoenholz et al., 2016; Xiao et al., 2018). However, orthogonal initializations are not compatable with non-linear activation functions like sig- moids or ReLUs, which limit their use in practice. Nonethe- less, this result suggests a promising direction in developing techniques for training deeper models. 7. Experimental results To test our theoretical results and to probe why standard neural networks are efﬁciently trained with SGD, we now present experimental results showing the effect of the neu- ral network architecture on the convergence of SGD and gradient confusion. It is worth noting that theorems 3.1 and 3.2 indicate that we would expect the effect of gradient confusion to be most prominent closer to the end of training. We performed experiments on wide residual networks (WRNs) (Zagoruyko & Komodakis, 2016), convolutional networks (CNNs) and multi-layer perceptrons (MLPs) for image classiﬁcation tasks on CIFAR-10, CIFAR-100 and MNIST. We present results for CNNs on CIFAR-10 in this section, and present all other results in appendix A. We use CNN-β-ℓto denote WRNs that have no skip connections or batch normalization, with a depth β and width factor ℓ.6 We turned off dropout and weight decay for all our experiments. We used SGD as the optimizer without any momentum. Following Zagoruyko & Komodakis (2016), we ran all experiments for 200 epochs with minibatches of size 128, and reduced the initial learning rate by a factor of 10 at epochs 80 and 160. We used the MSRA initializer (He et al., 2015) for the weights as is standard for this model, and used the same preprocessing steps for the CIFAR-10 im- ages as described in Zagoruyko & Komodakis (2016). We ran each experiment 5 times, and we show the standard de- viation across runs in our plots. We tuned the optimal initial 6The width factor denotes the number of ﬁlters relative to the original ResNet model (Zagoruyko & Komodakis, 2016).The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 0 25 50 75 100 125 150 175 200 epochs 0.0 0.5 1.0 1.5 2.0training loss depth 16 depth 22 depth 28 depth 34 depth 40 (a) 20 25 30 35 40 layer width −0.9 −0.8 −0.7 −0.6 −0.5 −0.4 −0.3 −0.2 −0.1min grad cosine similarity  (b) −0.4 −0.2 0.0 0.2 0.4 pairwise gradient cosine similarity 0 1 2 3 4 5 6 7density depth 16 depth 22 depth 28 depth 34 depth 40 (c) Figure 2.The effect of network depth with CNN-β-2 on CIFAR-10 for depths β = 16, 22, 28, 34 and 40. Plots show the (a) convergence curves for SGD, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training (over all independent runs). learning rate for each model over a logarithmically-spaced grid and selected the run that achieved the lowest training loss value. To measure gradient confusion, at the end of every training epoch, we sampled 100 pairs of mini-batches each of size 128 (the same size as the training batch). We calculated gradients on each mini-batch, and then computed pairwise cosine similarities. See appendix A.2 for more details on the experimental setup and architectures used. Effect of depth. To test our theoretical results, we consider CNNs with a ﬁxed width factor of 2 and varying network depth. From ﬁgure 2, we see that our theoretical results are backed by the experiments: increasing depth slows down convergence, and increases gradient confusion. We also notice that with increasing depth, the density of pairwise gradient cosine similarities concentrates less sharply around 0, which makes the network harder to train. Effect of width. We now consider CNNs with a ﬁxed depth of 16 and varying width factors. From ﬁgure 3, we see that increasing width results in faster convergence and lower gradient confusion. We further see that gradient co- sine similarities concentrate around 0 with growing width, indicating that SGD decouples across the training samples with growing width. Note that the smallest network consid- ered (CNN-16-2) is still over-parameterized and achieves a high level of performance (see appendix A.3). Effect of batch normalization and skip connections. Al- most all state-of-the-art neural networks currently contain both skip connections and normalization layers. To help un- derstand why such neural networks are so efﬁciently trained using SGD with constant learning rates, we test the effect of adding skip connections and batch normalization to CNNs of ﬁxed width and varying depth. Figure 4 shows that adding skip connections or batch normalization individually help in training deeper models, but these models still suffer from worsening results and increasing gradient confusion as the network gets deeper. When these techniques are used to- gether, the model has relatively low gradient confusion even for very deep networks, signiﬁcantly improving trainability of deep models. Note that our observations are consistent with prior work (De & Smith, 2020; Yang et al., 2019). 8. Alternate deﬁnitions of gradient confusion Note that the gradient confusion bound ηin equation 3 is deﬁned for the worst-case gradient inner product. However, all the results in this paper can be trivially extended to using a bound on the average gradient inner product of the form: ∑ N i,j=1⟨∇fi(w),∇fj(w)⟩/N2 ≥−η. In this case, all theoretical results would remain the same up to constants. We can also deﬁne a normalized variant of the gradient confusion condition: ⟨∇fi(w),∇fj(w)⟩/(∥∇fi(w)∥∥∇fj(w)∥) ≥−η. This condition inherently makes an additional assumption that the norm of the stochastic gradients, ∥∇fi(w)∥, is bounded, and thus the gradient variance is also bounded (see discussion in section 9). Thus, while all our theoretical results would qualitatively remain the same under this con- dition, we can prove tighter versions of our current results. Finally, note that gradient confusion condition in equation 3 is applicable even when the stochastic gradients are av- eraged over minibatches of size B. The variance of the gradient inner product scales down as 1/B2 in this case, and thus ηis expected to decrease as Bgrows. 9. Related work The gradient confusion bound and our theoretical results have interesting connections to prior work. In this section, we brieﬂy discuss some of these connections.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 0 25 50 75 100 125 150 175 200 epochs 10−5 10−4 10−3 10−2 10−1 100 training loss width 2 width 4 width 6 (a) 2 3 4 5 6 layer width −0.18 −0.16 −0.14 −0.12 −0.10 −0.08min grad cosine similarity  (b) −0.20 −0.15 −0.10 −0.05 0.00 0.05 0.10 0.15 0.20 pairwise gradient cosine similarity 0 2 4 6 8 10 12density width 2 width 3 width 4 width 5 width 6 (c) Figure 3.The effect of width with CNN-16-ℓon CIFAR-10 for width factorsℓ= 2, 3, 4, 5 and 6. Plots show the (a) convergence curves of SGD (for cleaner ﬁgures, we plot results for width factors 2, 4 and 6 here), (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training (over all independent runs). 20 30 40 50 60 70 80 90 100 network depth 10−4 10−3 10−2 10−1 100 final training loss no BN, no skip with BN, no skip no BN, with skip with BN & skip (a) 20 30 40 50 60 70 80 90 100 network depth −0.7 −0.6 −0.5 −0.4 −0.3 −0.2 −0.1 0.0 0.1 min grad cosine similarity no BN, no skip with BN, no skip no BN, with skip with BN & skip (b) 20 30 40 50 60 70 80 90 100 network depth 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9final test set accuracy no BN, no skip with BN, no skip no BN, with skip with BN & skip (c) Figure 4.The effect of adding skip connections and batch normalization to CNN-β-2 on CIFAR-10 for depths β = 16, 22, 28, 34, 40, 52, 76 and 100. Plots show the (a) optimal training losses, (b) minimum pairwise gradient cosine similarities, and the (c) test set accuracies at the end of training. Connections to the gradient variance : If we assume bounded gradient variance Ei∥∇fi(w) −∇F(w)∥2 ≤σ2, we can bound the gradient confusion parameter ηin terms of other quantities. For example, suppose the true gradient ∇F(w) = ∇f1(w)/2 +∇f2(w)/2. Then we can write: |⟨∇f1(w),∇f2(w)⟩| ≤σ2 + ∥∇F(w)∥2. However, in general one cannot bound the gradient variance in terms of the gradient confusion parameter. As a counter-example, consider a problem with the following distribution on the gradients: 1 1−p samples with gradient 1 ϵ and 1 p samples with gradient ϵ, where p = ϵ →0. In this case, the gradients are positive, so gradient confusion η= 0. The mean of the gradients is given by1+ ϵ(1−ϵ), which remains bounded as ϵ→0. On the other hand, the variance (and thus the squared norm of the stochastic gradients) is unbounded (O(1/ϵ) as ϵ→0). A consequence of this is that in theorems 3.1 and 3.2, the "noise term" (i.e., the second term in the RHS of the convergence bounds) does not depend on the learning rate in the general case. If gradients have unbounded variance, lowering the learning rate does not reduce the variance of the SGD updates, and thus does not reduce the noise term. Connections to gradient diversity: Gradient diversity (Yin et al., 2017) also measures the degree to which in- dividual gradients at different data samples are different from each other. However, the gradient diversity measure gets larger as the individual gradients become orthogonal to each other, and further increases as the gradients start pointing in opposite directions. On the other hand, gradient confusion between two individual gradients is zero unless the inner product between them is negative. As we show in this paper, this has important implications when we study the convergence of SGD in the over-parameterized setting: increased width makes gradients more orthogonal to each other improving trainability, while increased depth result in gradients pointing in opposite directions making networks harder to train. Thus, we view our papers to be complemen- tary, providing insights about different issues (large batch distributed training vs. small batch convergence). Related work on the impact of network architecture: Balduzzi et al. (2017) studied neural networks with ReLU activations at Gaussian initializations, and showed that gra- dients become increasingly negatively correlated with depth.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Hanin (2018) showed that the variance of gradients in fully connected networks with ReLU activations is exponential in the sum of the reciprocals of the hidden layer widths at Gaussian initializations. In a follow-up work, Hanin & Rol- nick (2018) showed that this sum of the reciprocals of the hidden layer widths determines the variance of the sizes of the activations at each layer. When this sum of reciprocals is too large, early training dynamics are very slow, suggesting the difﬁculties of starting training on deeper networks, as well as the beneﬁts of increased width. Other work on SGD convergence: There has recently been a lot of interest in analyzing conditions under which SGD converges to global minimizers of over-parameterized linear and non-linear neural networks. Arora et al. (2018) shows SGD converges linearly to global minimizers for linear neural networks under certain conditions. Du et al. (2018); Allen-Zhu et al. (2018); Zou et al. (2018); Brutzkus et al. (2017) also show convergence to global minimizers of SGD for non-linear networks. This paper complements these recent results by studying how low gradient confusion contributes to SGD’s success on over-parameterized neural networks used in practice. 10. Discussion In this paper, we study how neural network architecture af- fects the trainability of networks and the dynamics of SGD. To rigorously analyze this, we introduce a concept called gra- dient confusion, and show that when gradient confusion is low, SGD has fast convergence. We show at standard Gaus- sian initializations, increasing layer width leads to lower gradient confusion, making the model easier to train. In con- trast, increasing depth results in higher gradient confusion, making models harder to train. These results indicate that increasing the layer width with the network depth is impor- tant to maintain trainability of the neural network. This is supported by other recent work that suggest that the width should increase linearly with depth in a Gaussian-initialized neural network to help dynamics early in training (Hanin, 2018; Hanin & Rolnick, 2018). Many previous results have shown how deeper models are more efﬁcient at modeling higher complexity function classes than wider models, and thus depth is essential for the success of neural networks (Eldan & Shamir, 2016; Tel- garsky, 2016). Indeed, practitioners over the years have achieved state-of-the-art results on various tasks by mak- ing networks deeper, without necessarily making networks wider. We thus study techniques that enable us to train deep models without requiring us to increase the width with depth. Most state-of-the-art neural networks currently contain both skip connections and normalization layers. We thus, empir- ically study the effect of introducing batch normalization and skip connections to a neural network. We show that the combination of batch normalization and skip connections lower gradient confusion and help train very deep models, explaining why many neural networks used in practice are so efﬁciently trained. Furthermore, we show how orthogo- nal initialization techniques provide a promising direction for improving the trainability of very deep networks. Our results provide a number of important insights that can be used for neural network model design. We demonstrate that the gradient confusion condition could be useful as a measure of trainability of networks, and thus could po- tentially be used to develop algorithms for more efﬁcient training. Additionally, the correlation between gradient confusion and the test set accuracies shown in appendix A suggest that an interesting topic for future work would be to investigate the connection between gradient confusion and generalization (Fort et al., 2019). Our results also suggest the importance of further work on orthogonal initialization schemes for neural networks with non-linear activations that make training very deep models possible. Acknowledgements The authors thank Brendan Oâ ˘A ´ZDonoghue, Aleksandar Botev, James Martens, Sudha Rao, and Samuel L. Smith for helpful discussions and for reviewing earlier versions of this manuscript. This paper was supported by the ONR MURI program, AFOSR MURI Program, and the National Science Foundation DMS directorate. References Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. arXiv preprint arXiv:1811.03962, 2018. Arora, S., Cohen, N., Golowich, N., and Hu, W. A conver- gence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018. Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D., and McWilliams, B. The shattered gradients problem: If resnets are the answer, then what is the question? arXiv preprint arXiv:1702.08591, 2017. Bengio, Y ., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is difﬁcult. IEEE transactions on neural networks, 5(2):157–166, 1994. Bertsekas, D. P. Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. Optimization for Machine Learning, 2010(1-38):3, 2011. Boucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence . Oxford university press, 2013.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Brutzkus, A., Globerson, A., Malach, E., and Shalev- Shwartz, S. Sgd learns over-parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017. Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y ., Bal- dassi, C., Borgs, C., Chayes, J., Sagun, L., and Zecchina, R. Entropy-sgd: Biasing gradient descent into wide val- leys. arXiv preprint arXiv:1611.01838, 2016. Cooper, Y . The loss landscape of overparameterized neural networks. arXiv preprint arXiv:1804.10200, 2018. Darken, C. and Moody, J. Towards faster stochastic gradient search. In Advances in neural information processing systems, pp. 1009–1016, 1992. De, S. and Smith, S. L. Batch normalization biases residual blocks towards the identity function in deep networks. arXiv preprint arXiv:2002.10444, 2020. De, S., Yadav, A., Jacobs, D., and Goldstein, T. Automated inference with adaptive batches. In Artiﬁcial Intelligence and Statistics, pp. 1504–1513, 2017. Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018. Dziugaite, G. K. and Roy, D. M. Computing nonvacuous generalization bounds for deep (stochastic) neural net- works with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. Eldan, R. and Shamir, O. The power of depth for feedfor- ward neural networks. In Conference on Learning Theory, pp. 907–940, 2016. Fort, S., Nowak, P. K., and Narayanan, S. Stiffness: A new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019. Ghorbani, B., Krishnan, S., and Xiao, Y . An investiga- tion into neural net optimization via hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019. Glorot, X. and Bengio, Y . Understanding the difﬁculty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010. Hanin, B. Which neural net architectures give rise to ex- ploding and vanishing gradients? In Advances in Neural Information Processing Systems, pp. 582–591, 2018. Hanin, B. and Rolnick, D. How to start training: The effect of initialization and architecture. In Advances in Neural Information Processing Systems, pp. 571–581, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE inter- national conference on computer vision, pp. 1026–1034, 2015. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8580–8589, 2018. Karimi, H., Nutini, J., and Schmidt, M. Linear conver- gence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition. InJoint European Con- ference on Machine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems , pp. 1097–1105, 2012. LeCun, Y . A., Bottou, L., Orr, G. B., and Müller, K.-R. Efﬁcient backprop. In Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012. Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y ., Sohl- Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. arXiv preprint arXiv:1902.06720, 2019. Lojasiewicz, S. Ensembles semi-analytiques. Lectures Notes IHES (Bures-sur-Yvette), 1965. Ma, S., Bassily, R., and Belkin, M. The power of in- terpolation: Understanding the effectiveness of sgd in modern over-parametrized learning. arXiv preprint arXiv:1712.06559, 2017. Martens, J. Second-order optimization for neural networks. University of Toronto (Canada), 2016. Milman, V . D. and Schechtman, G. Asymptotic Theory of Finite Dimensional Normed Spaces. Springer-Verlag, Berlin, Heidelberg, 1986. ISBN 0-387-16769-2. Moulines, E. and Bach, F. R. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pp. 451–459, 2011.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Nagarajan, V . and Kolter, J. Z. Generalization in deep networks: The role of distance from initialization. arXiv preprint arXiv:1901.01672, 2019. Nedi´c, A. and Bertsekas, D. Convergence rate of incre- mental subgradient algorithms. In Stochastic optimiza- tion: algorithms and applications, pp. 223–264. Springer, 2001. Needell, D., Ward, R., and Srebro, N. Stochastic gradient de- scent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems, pp. 1017–1025, 2014. Nesterov, Y .Lectures on convex optimization, volume 137. Springer, 2018. Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y ., and Srebro, N. Towards understanding the role of over- parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018. Nguyen, Q. and Hein, M. The loss surface of deep and wide neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2603– 2612. JMLR. org, 2017. Oymak, S. and Soltanolkotabi, M. Overparameterized non- linear learning: Gradient descent takes the shortest path? arXiv preprint arXiv:1812.10004, 2018. Robbins, H. and Monro, S. A stochastic approximation method. The annals of mathematical statistics, pp. 400– 407, 1951. Sagun, L., Evci, U., Guney, V . U., Dauphin, Y ., and Bottou, L. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Schmidt, M. and Roux, N. L. Fast convergence of stochastic gradient descent under a strong growth condition. arXiv preprint arXiv:1308.6370, 2013. Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl- Dickstein, J. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016. Sedghi, H., Gupta, V ., and Long, P. M. The singular values of convolutional layers. arXiv preprint arXiv:1805.10408, 2018. Shamir, O. and Zhang, T. Stochastic gradient descent for non-smooth optimization: Convergence results and opti- mal averaging schemes. In International Conference on Machine Learning, pp. 71–79, 2013. Simonyan, K. and Zisserman, A. Very deep convolu- tional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Smith, S. L., Elsen, E., and De, S. On the generalization beneﬁt of noise in stochastic gradient descent. arXiv preprint arXiv:2006.15081, 2020. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learn- ing. In International conference on machine learning, pp. 1139–1147, 2013. Tao, T. Topics in random matrix theory, volume 132. Amer- ican Mathematical Soc., 2012. Telgarsky, M. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016. Vaswani, S., Bach, F., and Schmidt, M. Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron. arXiv preprint arXiv:1810.07288, 2018. Vershynin, R. High-dimensional probability: An introduc- tion with applications in data science, volume 47. Cam- bridge University Press, 2018. Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4151–4161, 2017. Wu, L., Zhu, Z., et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Xiao, L., Bahri, Y ., Sohl-Dickstein, J., Schoenholz, S. S., and Pennington, J. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla convolu- tional neural networks. arXiv preprint arXiv:1806.05393, 2018. Yang, G., Pennington, J., Rao, V ., Sohl-Dickstein, J., and Schoenholz, S. S. A mean ﬁeld theory of batch normal- ization. arXiv preprint arXiv:1902.08129, 2019. Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ram- chandran, K., and Bartlett, P. Gradient diversity: a key ingredient for scalable distributed learning.arXiv preprint arXiv:1706.05699, 2017. Zagoruyko, S. and Komodakis, N. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Appendix We ﬁrst brieﬂy outline the different sections in the appendix. • In appendix A, we provide details of our experimental setup, and provide additional empirical results on fully connected networks, convolutional networks and residual networks with the MNIST, CIFAR-10 and CIFAR-100 datasets. • In appendix B, we state and prove a lemma on the near orthogonality of random vectors, which we refer to in the main text. This result is often attributed to Milman & Schechtman (1986). • In appendix C, we provide some intuition on why many standard over-parameterized neural networks with low-rank Hessians might have low gradient confusion for a large set of weights near the minimizer. • In appendix D, we provide the proofs of the theorems presented in the main section. In appendix D.1, we provide proofs of theorems 3.1 and 3.2. In appendix D.2, we provide the proof of lemma D.1, which we refer to in the main text. In appendix D.3, we provide proofs of theorem 5.1 and corollary 5.1. In appendix D.4, we provide the proof of theorem 4.1. In appendix D.5, we provide the proof of theorem 6.1. • In appendix E, we brieﬂy describe a few lemmas that we require in our analysis. • In appendix F, we discuss the small weights assumption (assumption 1), which is required for theorem 5.1, corollary 5.1 and theorem 6.1 in the main text. A. Additional experimental results In this section, we present more details about our experimental setup, as well as, additional experimental results on a range of models (MLPs, CNNs and Wide ResNets) and a range of datasets (MNIST, CIFAR-10, CIFAR-100). A.1. MLPs on MNIST To further test the main claims in the paper, we performed additional experiments on an image classiﬁcation problem on the MNIST dataset using fully connected neural networks. We iterated over neural networks of varying depth and width, and considered both the identity activation function (i.e., linear neural networks) and the tanh activation function. We also considered two different weight initializations that are popularly used and appropriate for these activation functions: • The Glorot normal initializer (Glorot & Bengio, 2010) with weights initialized by sampling from the distribution N ( 0,2/(fan-in + fan-out) ) , where fan-in denotes the number of input units in the weight matrix, and fan-out denotes the number of output units in the weight matrix. • The LeCun normal initializer (LeCun et al., 2012) with weights initialized by sampling from the distribution N ( 0,1/fan-in ) . We considered the simpliﬁed case where all hidden layers have the same width ℓ. Thus, the ﬁrst weight matrix W0 ∈Rℓ×d, where d= 784for the 28 ×28-sized images of MNIST; all intermediate weight matrices {Wp}p∈[β−1] ∈Rℓ×ℓ; and the ﬁnal layer Wβ ∈R10×ℓ for the 10 image classes in MNIST. We added biases to each layer, which we initialized to 0. We used softmax cross entropy as the loss function. We use MLP-β-ℓto denote this fully connected network of depth βand width ℓ. We used the standard train-valid-test splits of 40000-10000-10000 for MNIST. This relatively simple model gave us the ability to iterate over a large number of combinations of network architectures of varying width and depth, and different activation functions and weight initializations. Linear neural networks are an efﬁcient way to directly understand the effect of changing depth and width without increasing model complexity over linear regression. Thus, we considered both linear and non-linear neural networks in our experiments. We used SGD with constant learning rates for training with a mini-batch size of 128 and trained each model for 40000 iterations (more than 100 epochs). The constant learning rate αwas tuned over a logarithmically-spaced grid: α∈{100,10−1,10−2,10−3,10−4,10−5,10−6}.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent We ran each experiment 10 times (making sure at least 8 of them ran till completion), and picked the learning rate that achieved the lowest training loss value on average at the end of training. Our grid search was such that the optimal learning rate never occurred at one of the extreme values tested. To measure gradient confusion at the end training, we sampled 1000 pairs of mini-batches each of size 128 (the same size as the training batch size). We calculated gradients on each of these pairs of mini-batches, and then calculated the cosine similarity between them. To measure the worse-case gradient confusion, we computed the lowest gradient cosine similarity among all pairs. We explored the effect of changing depth and changing width on the different activation functions and weight initializations. We plot the ﬁnal training loss achieved for each model and the minimum gradient cosine similarities calculated over the 1000 pairs of gradients at the end of training. For each point, we plot both the mean and the standard deviation over the 10 independent runs. The effect of depth. We ﬁrst present results showing the effect of network depth. We considered a ﬁxed width of ℓ= 100, and varied the depth of the neural network, on the log scale, as: β ∈{3,10,30,100,300,1000}. Figure 5 shows results on neural networks with identity and tanh activation functions for the two weight initializations considered (Glorot normal and LeCun normal). Similar to the experimental results in section 7, and matching our theoretical results in sections 4 and 5, we notice the consistent trend of gradient confusion increasing with increasing depth. This makes the networks harder to train with increasing depth, and this is evidenced by an increase in the ﬁnal training loss value. By depth β = 1000, the increased gradient confusion effectively makes the network untrainable when using tanh non-linearities. The effect of width. We explored the effect of width by varying the width of the neural network while keeping the depth ﬁxed at β = 300. We chose a very deep model, which is essentially untrainable for small widths (with standard initialization techniques) and helps better illustrate the effects of increasing width. We varied the width of the network, again on the log scale, as: ℓ∈{10,30,100,300,1000}. Crucially, note that the smallest network considered here, MLP-300-10, still has more than 50000 parameters (i.e., more than the number of training samples), and the network with width ℓ= 30has almost three times the number of parameters as the high-performing MLP-3-100 network considered in the previous section. Figure 6 show results on linear neural networks and neural networks with tanh activations for both the Glorot normal and LeCun normal initializations. As in the experimental results of section 7, we see the consistent trend of gradient confusion decreasing with increasing width. Thus, wider networks become easier to train and improve the ﬁnal training loss value. We further see that when the width is too small (ℓ= 30), the gradient confusion becomes drastically high and the network becomes completely untrainable. A.2. Additional experimental details for CNNs and WRNs In this section, we review the details of our setup for the image classiﬁcation experiments on CNNs and WRNs on the CIFAR-10 and CIFAR-100 datasets. WIDE RESIDUAL NETWORKS The Wide ResNet (WRN) architecture (Zagoruyko & Komodakis, 2016) for CIFAR datasets is a stack of three groups of residual blocks. There is a downsampling layer between two blocks, and the number of channels (width of a convolutional layer) is doubled after downsampling. In the three groups, the width of convolutional layers is {16ℓ,32ℓ,64ℓ}, respectively. Each group contains βr residual blocks, and each residual block contains two 3 ×3 convolutional layers equipped with ReLU activation, batch normalization and dropout. There is a 3 ×3 convolutional layer with 16 channels before the three groups of residual blocks. And there is a global average pooling, a fully-connected layer and a softmax layer after the three groups. The depth of WRN is β = 6βr + 4. For our experiments, we turned off dropout. Unless otherwise speciﬁed, we also turned off batch normalization. We added biases to the convolutional layers when not using batch normalization to maintain model expressivity. We used the MSRA initializer (He et al., 2015) for the weights as is standard for this model, and used the same preprocessing steps for the CIFAR images as described in Zagoruyko & Komodakis (2016). This preprocessing step involves normalizing the images and doing data augmentation (Zagoruyko & Komodakis, 2016). We denote this network as WRN-β-ℓ, where βrepresents the depth and ℓrepresents the width factor of the network.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 101 102 103 depth 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6final training loss value (a) Linear NN, Glorot init 101 102 103 depth 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 min grad cosine similarity  (b) Linear NN, Glorot init 101 102 103 depth 0.0 0.5 1.0 1.5final training loss value  (c) Linear NN, LeCun init 101 102 103 depth 1.0 0.8 0.6 0.4 0.2 min grad cosine similarity  (d) Linear NN, LeCun init 101 102 103 depth 0.0 0.5 1.0 1.5 2.0 2.5final training loss value (e) Tanh NN, Glorot init 101 102 103 depth 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 min grad cosine similarity  (f) Tanh NN, Glorot init 101 102 103 depth 0.0 0.5 1.0 1.5 2.0 2.5final training loss value  (g) Tanh NN, LeCun init 101 102 103 depth 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 min grad cosine similarity  (h) Tanh NN, LeCun init Figure 5.Effect of varying depth on MLP-β-100. 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value (a) Linear NN, Glorot init 101 102 103 width 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 min grad cosine similarity  (b) Linear NN, Glorot init 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value  (c) Linear NN, LeCun init 101 102 103 width 1.2 1.0 0.8 0.6 0.4 0.2 0.0 min grad cosine similarity  (d) Linear NN, LeCun init 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value (e) Tanh NN, Glorot init 101 102 103 width 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 min grad cosine similarity  (f) Tanh NN, Glorot init 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value  (g) Tanh NN, LeCun init 101 102 103 width 1.2 1.0 0.8 0.6 0.4 min grad cosine similarity  (h) Tanh NN, LeCun init Figure 6.Effect of varying width on MLP-300-ℓ. To study the effect of depth, we considered WRNs with width factor ℓ= 2and depth varying as: β ∈{16,22,28,34,40,52,76,100}. For cleaner ﬁgures, we sometimes plot a subset of these results: β ∈{16,28,40,52,76,100}.To study the effect of width, we considered WRNs with depth β = 16and width factor varying as: ℓ∈{2,3,4,5,6}.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent CONVOLUTIONAL NEURAL NETWORKS The WRN architecture contains skip connections that, as we show, help in training deep networks. To consider VGG-like convolutional networks, we consider a family of networks where we remove the skip connections from WRNs. Following the WRN convention, we denote these networks as CNN-β-ℓ, where βdenotes the depth and ℓdenotes the width factor. To study the effect of depth, we considered CNNs with width factor ℓ= 2and depth varying as: β ∈{16,22,28,34,40}. To study the effect of width, we considered CNNs with depth β = 16and width factor varying as: ℓ∈{2,3,4,5,6}. HYPERPARAMETER TUNING AND OTHER DETAILS We used SGD as the optimizer without any momentum. Following Zagoruyko & Komodakis (2016), we ran all experiments for 200 epochs with minibatches of size 128, and reduced the initial learning rate by a factor of 10 at epochs 80 and 160. We turned off weight decay for all our experiments. We ran each individual experiment 5 times. We ignored any runs that were unable to decrease the loss from its initial value. We also made sure at least 4 out of the 5 independent runs ran till completion. When the learning rate is close to the threshold at which training is still possible, some runs may converge, while others may fail to converge. Thus, these checks ensure that we pick a learning rate that converges reliably in most cases on each problem. We show the standard deviation across runs in our plots. We tuned the optimal initial learning rate for each model over a logarithmically-spaced grid: α∈{101,3 ×100,100,3 ×10−1,10−1,3 ×10−2,10−2,3 ×10−3,10−3,3 ×10−4,10−4,3 ×10−5}, and selected the run that achieved the lowest ﬁnal training loss value (averaged over the independent runs). Our grid search was such that the optimal learning rate never occurred at one of the extreme values tested. We used the standard train-valid-test splits of 40000-10000-10000 for CIFAR-10 and CIFAR-100. To measure gradient confusion, at the end of every training epoch, we sampled 100 pairs of mini-batches each of size 128 (the same size as the training batch size). We calculated gradients on each mini-batch, and then computed pairwise cosine similarities. To measure the worse-case gradient confusion, we computed the lowest gradient cosine similarity among all pairs. We also show the kernel density estimation of the pairwise gradient cosine similarities of the 100 minibatches sampled at the end of training (after 200 epochs), to see the concentration of the distribution. To do this, we combine together the 100 samples for each independent run and then perform kernel density estimation with a gaussian kernel on this data. A.3. Additional plots for CIFAR-10 on CNNs In section 7, we showed results for image classiﬁcation using CNNs on CIFAR-10. In this section, we show some additional plots for this experiment. Figure 7 shows the effect of changing the depth, while ﬁgure 8 shows the effect of changing the width factor of the CNN. We see that the ﬁnal training loss and test set accuracy values show the same trends as in section 7: deeper networks are harder to train, while wider networks are easier to train. As mentioned previously, theorems 3.1 and 3.2 indicate that we would expect the effect of gradient confusion to be more prominent near the end of training. From the plots we see that deeper networks have higher gradient confusion close to minimum, while wider networks have lower gradient confusion close to the minimum. A.4. CIFAR-100 on CNNs We now consider image classiﬁcations tasks with CNNs on the CIFAR-100 dataset. Figure 9 shows the effect of varying depth, while ﬁgure 10 shows the effect of varying width. We notice the same trends as in our results with CNNs on CIFAR-10. Interestingly, from the width results in ﬁgure 10, we see that while there is no perceptible change to the minimum pairwise gradient cosine similarity, the distribution still sharply concentrates around 0 with increasing width. Thus more gradients become orthogonal to each other with increasing width, implying that SGD on very wide networks becomes closer to decoupling over the data samples.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 15 20 25 30 35 40 network depth 0.0 0.5 1.0 1.5 2.0 2.5final training loss (a) 15 20 25 30 35 40 network depth 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9final test set accuracy  (b) 0 50 100 150 200 epochs 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 min grad cosine similarity depth 16 depth 22 depth 28 depth 34 depth 40 (c) Figure 7.The effect of network depth with CNN-β-2 on CIFAR-10. The plots show the (a) ﬁnal training loss values at the end of training, (b) ﬁnal test set accuracy values at the end of training, and (c) the minimum of pairwise gradient cosine similarities during training. 2 3 4 5 6 layer width 10-4 10-3 10-2 final training loss (a) 2 3 4 5 6 layer width 0.80 0.82 0.84 0.86 0.88 0.90 0.92final test set accuracy  (b) 0 50 100 150 200 epochs 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20 min grad cosine similarity width 2 width 4 width 6 (c) Figure 8.The effect of width with CNN-16-ℓon CIFAR-10. The plots show the (a) ﬁnal training loss values at the end of training, (b) ﬁnal test set accuracy values at the end of training, and the (c) minimum of pairwise gradient cosine similarities during training. A.5. Image classiﬁcation with WRNs on CIFAR-10 and CIFAR-100 We now show results for image classiﬁcation problems using wide residual networks (WRNs) on CIFAR-10 and CIFAR- 100. The WRNs we consider do not have any batch normalization. Later we show results on the effect of adding batch normalization to these networks. Figures 11 and 12 show results on the effect of depth using WRNs on CIFAR-10 and CIFAR-100 respectively. We again see the consistent trend of deeper networks having higher gradient confusion, making them harder to train. We further see that increasing depth results in the pairwise gradient cosine similarities concentrating less around 0. Figures 13 and 14 show results on the effect of width using WRNs on CIFAR-10 and CIFAR-100 respectively. We see that increasing width typically lowers gradient confusion and helps the network achieve lower loss values. The pairwise gradient cosine similarities also typically concentrate around 0 with higher width. We also notice from these ﬁgures that in some cases, increasing width might lead to diminishing returns, i.e., the beneﬁts of increased width diminish after a certain point, as one would expect. A.6. Effect of batch normalization and skip connections In section 7 we showed results on the effect of adding batch normalization and skip connections to CNNs and WRNs on an image classiﬁcation task on CIFAR-10. In this section, we present similar results for image classiﬁcation on CIFAR-100. Similar to section 7, ﬁgure 15 shows that adding skip connections or batch normalization individually help in training deeper models, but these models still suffer from worsening results and increasing gradient confusion as the network gets deeper. Both these techniques together keep the gradient confusion relatively low even for very deep networks, signiﬁcantly improving trainability of deep models.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 15 20 25 30 35 network depth 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5final training loss (a) 15 20 25 30 35 network depth 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 min grad cosine similarity  (b) 0.4  0.2  0.0 0.2 0.4 pairwise gradient cosine similarity 0 2 4 6 8 10 12density depth 16 depth 22 depth 28 depth 34 (c) Figure 9.The effect of network depth with CNN-β-2 on CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. 2 3 4 5 6 layer width 10-4 10-3 10-2 10-1 final training loss (a) 2 3 4 5 6 layer width 0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02 min grad cosine similarity  (b) 0.10  0.05  0.00 0.05 0.10 pairwise gradient cosine similarity 0 5 10 15 20 25density width 2 width 3 width 4 width 5 width 6 (c) Figure 10.The effect of width with CNN-16- ℓon CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. B. Near orthogonality of random vectors For completeness, we state and prove below a lemma on the near orthogonality of random vectors. This result is often attributed to Milman & Schechtman (1986). Lemma B.1 (Near orthogonality of random vectors) . For vectors {xi}i∈[N] drawn uniformly from a unit sphere in d dimensions, and ν >0, Pr [ ∃i,j |x⊤ i xj|>ν ] ≤N2√ π 8 exp ( −d−1 2 ν2) . Proof. Given a ﬁxed vector x,a uniform random vector y satisﬁes |x⊤y|≥ νonly if y lies in one of two spherical caps: one centered at x and the other at −x,and both with angular radius cos−1(ν) ≤π 2 −ν.A simple result often attributed to Milman & Schechtman (1986) bounds the probability of lying in either of these caps as Pr[|x⊤y|≥ ν] ≤ √ π 2 exp ( −d−1 2 ν2 ) . (5) Because of rotational symmetry, the bound (5) holds if both x and y are chosen uniformly at random. We next apply a union bound to control the probability that |x⊤ i xj|≥ νfor some pair (i,j).There are fewer than N2/2 such pairs, and so the probability of this condition is Pr[|x⊤ i xj|≥ ν,for some i,j] ≤N2 2 √ π 2 exp ( −d−1 2 ν2 ) .The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 30 40 50 60 70 80 90 100 network depth 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 final training loss (a) 30 40 50 60 70 80 90 100 network depth 0.35 0.30 0.25 0.20 0.15 0.10 0.05 min grad cosine similarity  (b) 0.4  0.2  0.0 0.2 0.4 pairwise gradient cosine similarity 0 2 4 6 8 10 12 14 16density depth 28 depth 40 depth 52 depth 76 depth 100 (c) Figure 11.The effect of depth with WRN-β-2 (no batch normalization) on CIFAR-10. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. 30 40 50 60 70 80 90 100 network depth 10-4 10-3 10-2 10-1 100 101 final training loss (a) 30 40 50 60 70 80 90 100 network depth 0.22 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 0.04 min grad cosine similarity  (b) 0.20  0.15  0.10  0.05  0.00 0.05 0.10 0.15 0.20 pairwise gradient cosine similarity 0 5 10 15 20 25density depth 28 depth 40 depth 52 depth 76 depth 100 (c) Figure 12.The effect of depth with WRN-β-2 (no batch normalization) on CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. C. Low-rank Hessians lead to low gradient confusion In this section, we show that low-rank random Hessians result in low gradient confusion. For clarity in presentation, suppose each fi has a minimizer at the origin (the same argument can be easily extended to the more general case). Suppose also that there is a Lipschitz constant for the Hessian of each function fi that satisﬁes ∥Hi(w) −Hi(w′)∥≤ LH∥w −w′∥(note that this is a standard optimization assumption (Nesterov, 2018), with evidence that it is applicable for neural networks (Martens, 2016)). Then ∇fi(w) = Hiw + e, where e is an error term bounded as: ∥e∥≤ 1 2 LH∥w∥2,and we use the shorthand Hi to denote Hi(0).Then we have: |⟨∇fi(w),∇fj(w)⟩|= |⟨Hiw,Hjw⟩|+ ⟨e,Hiw + Hjw⟩+ ∥e∥2 ≤∥w∥2∥Hi∥∥Hj∥+ ∥e∥∥w∥(∥Hi∥+ ∥Hj∥) +∥e∥2 ≤∥w∥2∥Hi∥∥Hj∥+ 1 2LH∥w∥3(∥Hi∥+ ∥Hj∥) +1 4L2 H∥w∥4. If the Hessians are sufﬁciently random and low-rank (e.g., of the form Hi = aia⊤ i where ai ∈RN×r are randomly sampled from a unit sphere), then one would expect the terms in this expression to be small for all w within a neighborhood of the minimizer. There is evidence that the Hessian at the minimizer is very low rank for many standard over-parameterized neural network models (Sagun et al., 2017; Cooper, 2018; Chaudhari et al., 2016; Wu et al., 2017; Ghorbani et al., 2019). While a bit non-rigorous, the above result nonetheless suggests that for many standard neural network models, the gradient confusion might be small for a large class of weights near the minimizer.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 2 3 4 5 6 layer width 10-4 10-3 10-2 final training loss (a) 2 3 4 5 6 layer width 0.24 0.22 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 min grad cosine similarity  (b) 0.20  0.15  0.10  0.05  0.00 0.05 0.10 0.15 0.20 pairwise gradient cosine similarity 0 2 4 6 8 10 12 14 16 18density width 2 width 3 width 4 width 5 width 6 (c) Figure 13.The effect of width with WRN-16-ℓ(no batch normalization) on CIFAR-10. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. 2 3 4 5 6 layer width 10-4 10-3 10-2 10-1 final training loss (a) 2 3 4 5 6 layer width 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00 min grad cosine similarity  (b) 0.08  0.06  0.04  0.02  0.00 0.02 0.04 0.06 0.08 pairwise gradient cosine similarity 0 5 10 15 20 25 30 35density width 2 width 3 width 4 width 5 width 6 (c) Figure 14.The effect of width with WRN-16-ℓ(no batch normalization) on CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. D. Missing proofs D.1. Proofs of theorems 3.1 and 3.2 This section presents proofs for the convergence theorems of SGD presented in section 3, under the assumption of low gradient confusion. For clarity of presentation, we re-state each theorem before its proof. Theorem 3.1. If the objective function satisﬁes (A1) and (A2), and has gradient confusion η, SGD converges linearly to a neighborhood of the minima of problem (1) as: E[F(wT) −F⋆] ≤ρT(F(w0) −F⋆) + αη 1−ρ, where α< 2 NL, ρ= 1−2µ N ( α−NLα2 2 ) , F⋆ = minw F(w) and w0 is the initialized weights. Proof. Let ˜i∈[N] denote the index of the realized function ˜fk in the uniform sampling from {fi}i∈[N] at step k. From assumption (A1), we have F(wk+1) ≤F(wk) +⟨∇F(wk), wk+1 −wk⟩+ L 2 ∥wk+1 −wk∥2 = F(wk) −α⟨∇F(wk), ∇˜fk(wk)⟩+ Lα2 2 ∥∇˜fk(wk)∥2 = F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 −α N ∑ ∀i:i̸=˜i ⟨∇fi(wk), ∇˜fk(wk)⟩The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 20 30 40 50 60 70 80 90 100 network depth 10-5 10-4 10-3 10-2 10-1 100 101 final training loss no BN, no skip with BN, no skip no BN, with skip with BN & skip (a) 20 30 40 50 60 70 80 90 100 network depth 0.6 0.5 0.4 0.3 0.2 0.1 0.0 min grad cosine similarity no BN, no skip with BN, no skip no BN, with skip with BN & skip (b) 20 30 40 50 60 70 80 90 100 network depth 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8final test set accuracy no BN, no skip with BN, no skip no BN, with skip with BN & skip (c) Figure 15.The effect of adding skip connections and batch normalization to CNN-β-2 on CIFAR-100. Plots show the (a) training loss, (b) minimum pairwise gradient cosine similarities, and the (c) test accuracies at the end of training. ≤F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 + α(N −1)η N , ≤F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 + αη, where the second-last inequality follows from deﬁnition 2.1. Let the learning rate α< 2/NL. Then, using assumption (A2) and subtracting by F⋆ = minw F(w) on both sides, we get F(wk+1) −F⋆ ≤F(wk) −F⋆ −2µ (α N −Lα2 2 ) ( ˜fk(wk) −˜f⋆ k) +αη, where ˜f⋆ k = minw ˜fk(w). It is easy to see that by deﬁnition we have, Ei[f⋆ i ] ≤F⋆. Moreover, from assumption that α< 2 NL, it implies that ( α N −Lα2 2 ) >0. Therefore, taking expectation on both sides we get, E[F(wk+1) −F⋆] ≤ ( 1 −2µα N + µLα2 ) E[F(wk) −F⋆] +αη. Writing ρ= 1−2µα N + µLα2, and unrolling the iterations, we get E[F(wk+1) −F⋆] ≤ρk+1(F(w0) −F⋆) + k∑ i=0 ρiαη ≤ρk+1(F(w0) −F⋆) + ∞∑ i=0 ρiαη = ρk+1(F(w0) −F⋆) + αη 1 −ρ. Theorem 3.2. If the objective satisﬁes (A1) and has gradient confusion η, then SGD converges to a neighborhood of a stationary point of problem (1) as: mink=1,...,T E∥∇F(wk)∥2 ≤ρ(F(w1)−F⋆) T + ρη, for α< 2 NL, ρ= 2N 2−NLα, and F⋆ = minw F(w). Proof. From theorem 3.1, we have: F(wk+1) ≤F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 + αη. (6)The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Now we know that: E∥∇˜fk(wk)∥2 = E∥∇˜fk(wk) −∇F(wk)∥2 + E∥∇F(wk)∥2 ≥E∥∇F(wk)∥2. Thus, taking expectation and assuming the step size α< 2/(NL), we can rewrite equation 6 as: E∥∇F(wk)∥2 ≤ 2N 2α−NLα2 E[F(wk) −F(wk+1)] + 2Nη 2 −NLα. Taking an average overT iterations, and using F⋆ = minw F(w), we get: min k=1,...,T E∥∇F(wk)∥2 ≤ 1 T ⊤∑ k=1 E∥∇F(wk)∥2 ≤ 2N 2α−NLα2 F(w1) −F⋆ T + 2Nη 2 −NLα. D.2. Proof of lemma D.1 Lemma D.1. Consider the set of loss-functions {fi(W)}i∈[N] where all fi are either the square-loss function or the logistic-loss function. Recall that fi(W) :=f(W,xi). Consider a feed-forward neural network as deﬁned in equation 4 whose weights W satisfy assumption 1. Consider the gradient ∇Wfi(W) of each function fi. From deﬁnition we have that ∇Wfi(W) =ζxi(W)∇WgW(xi), where we deﬁne ζxi(W) =∂fi(W)/∂gW. Then we have the following properties. 1. When ∥x∥≤ 1 for every p∈[β] we have ∥∇WpgW(xi)∥≤ 1. 2. There exists 0 <ζ0 ≤2√β, such that |ζxi(W)|≤ 2 , ∥∇xiζxi(W)∥2 ≤ζ0 , ∥∇Wζxi(W)∥2 ≤ζ0. Proof. The ﬁrst property is a direct consequence of assumption 1 and property (P2) of the activation function. Let W denote the tuple (Wp)p∈[β]0 . Consider |ζxi(W)|= |∂fi(W)/∂gW|. In the case of square-loss function this evaluates to |gW(x) −C(x)|≤ 2. In case of logistic regression, this evaluates to | −1 1+exp(C(xi)gW(xi)) |≤ 1. Now we consider ∥∇xiζxi(W)∥. Consider the squared loss function. We then have the following. ∥∇xiζxi(W)∥= ∥∇xif′(W)∥ = ∥∇xigW(xi) −C(xi)∥ ≤∥∇xigW(xi)∥+ 1. Likewise, consider the logistic-loss function. We then have the following. ∥∇xiζxi(W)∥≤  C(xi)2 (1 + exp(C(xi)gW(xi)))2 exp(C(xi)gW(xi)) ∥∇xigW(xi)∥ ≤∥∇xigW(xi)∥. Thus, it sufﬁces to bound ∥∇xigW(xi)∥. Using assumption 1 and the properties (P1), (P2) of σ, this can be upper-bounded by 1. Consider ∇Wpζxi(W) for some layer index p ∈[β]0. We will show that ∥∇Wpζxi(W)∥2 ≤2. Then it immediately follows that ∥∇Wζxi(W)∥2 ≤2√β. In the case of a squared loss function. We have the following. ∥∇Wpζxi(W)∥= ∥∇Wpf′(W)∥ = ∥∇WpgW(xi) −C(xi)∥ ≤∥∇WpgW(xi)∥+ 1. Likewise, consider the logistic-loss function. We then have the following. ∥∇Wpζxi(W)∥≤  C(xi)2 (1 + exp(C(xi)gW(xi)))2 exp(C(xi)gW(xi)) ∥∇WpgW(xi)∥ ≤∥∇WpgW(xi)∥. Since ∥∇WpgW(xi)∥≤ 1, we have that ∥∇Wpζxi(W)∥≤ 2 in both the cases. Thus, ζ0 = 2√β.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent D.3. Proofs of theorem 5.1 and corollary 5.1 In this section, we will present the proofs of theorem 5.1 and corollary 5.1. Theorem 5.1. Let W0,W1,..., Wβ satisfy assumption 1. For some ﬁxed constant c> 0, the gradient confusion bound (equation 3) holds with probability at least 1 −N2 exp ( −cdη2 16ζ4 0 (β+2)4 ) . Proof. We show two key properties, namely bounded gradient and non negative expectation. We will then use both these properties to complete the proof. Bounded gradient. For every i∈[n] deﬁne ζxi(W) :=f′(W). For every p∈[β] deﬁne Hp as follows. Hp(x) :=σ(Wp ·σ(Wp−1 ·σ(... ·σ(W0 ·x) ... ). Fix an i∈[N]. Then we have the following recurrence gβ(xi) :=σ′(Hβ(xi)) gp(xi) := (W⊤ p+1 ·gp+1(xi)) ·Diag(σ′(Hp(xi))) ∀p∈{0,1,...,β −1}. Then the gradients can be written in terms of the above quantities as follows. ∇Wpfi(W) =gp(xi) ·Hp−1(xi)⊤ ∀p∈[β]0. We can write, the gradient confusion denote by hW(xi,xj), as follows. ζxi(W)ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  . (7) We will now bound ∥∇(xi,xj)hW(xi,xj)∥2. Consider ∇xihW(xi,xj). This can be written as follows. (∇xiζxi(W))ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  + ζxi(W)ζxj(W) ∑ p∈[β]0 [ ∇xi ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) )]⊤ . (8) Observe that each of the entries in the diagonal matrix Diag(σ′(Hp(xi))) is at most 1. Thus, we have that ∥Diag(σ′(Hp(xi)))∥≤ 1. We have the following relationship. ∥gβ(xi)∥≤ 1 ∥gp(xi)∥≤∥ W⊤ p+1∥∥gp+1(xi))∥∥Diag(σ′(Hp(xi)))∥≤ 1 ∀p∈{0,1,...,β −1}. Moreover we have, ∥Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]∥≤∥ Hp−1(xi)∥∥gp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)⊤∥≤ 1. Consider ∥∇xi ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) ) ∥for every p∈[β]0. This can be upper-bounded by, ∥∇xiHp−1(xi)∥∥gp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥+ ∥Hp−1(xi)∥∥∇xigp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Note that ∇xiHp−1(xi) =g1(xi) ·Diag(σ′(W0 ·xi)) ·W⊤ 0 ·gp(xi)⊤. Thus, ∥∇xiHp−1(xi)∥≤ 1. We will now show that ∥∇xigp(xi)∥≤ β−p+ 1. We prove this inductively. Consider the base case when p= β. ∥∇xigβ(xi)∥= ∥∇xiσ′(Hβ(xi))∥≤ 1 =β−β+ 1. Now, the inductive step. ∥∇xigp(xi)∥≤∥∇ xigp+1(xi)∥+ ∥∇xi Diag(σ′(Hp(xi)))∥≤ β−p≤β−p+ 1. Thus, using equation 8 and the above arguments, we obtain, ∥∇xihW(xi,xj)∥2 ≤ζ2 0 (β+ 1) +ζ2 0 (β+ 1)(β+ 2)≤ 2ζ2 0 (β+ 2)2 and thus, ∥∇(xi,xj)hW(xi,xj)∥2 ≤4ζ2 0 (β+ 2)2. Non-negative expectation. Exi,xj[h(xi,xj)] =Exi,xj[⟨∇fi(W),∇fj(W)⟩] = ⟨Exi[∇fi(W)],Exj[∇fj(W)]⟩ = ∥Exi[∇fi(W)]∥2 ≥0. (9) We have used the fact that∇fi(W) and ∇fj(W) are identically distributed and independent. Concentration of Measure. We combine the two properties as follows. From Non-negative Expectation property and equation 26, we have that Pr[hW(xi,xj) ≤−η] ≤Pr[hW(xi,xj) ≤E(xi,xj)[hW(xi,xj)] −η] ≤exp ( −cdη2 16ζ4 0 (β+ 2)4 ) . (10) To obtain the probability that some value of hw(∇wfi,∇wfj) lies below −η,we use a union bound. There are N(N − 1)/2 <N 2/2 possible pairs of data points to consider, and so this probability is bounded above by N2 exp ( −cdη2 16ζ4 0 (β+2)4 ) . D.3.1. P ROOF OF COROLLARY 5.1 Before we prove corollary 5.1 we ﬁrst prove the following helper lemma. Lemma D.2. Suppose maxW ∥∇Wfi(W)∥≤ M,and both ∇Wfi(w) and ∇Wfj(W) are Lipschitz in W with constant L. Then hW(xi,xj) is Lipschitz in W with constant 2LM. Proof. We view W as ﬂattened vector. We now prove the above result for these two vectors. For two vectorsw,w′, |hw(xi,xj) −hw′(xi,xj)| = |⟨∇wfi(w),∇wfj(w)⟩−⟨∇w′fi(w′),∇w′fj(w′)⟩| = |⟨∇wfi(w) −∇w′fi(w′) +∇w′fi(w′),∇wfj(w)⟩ −⟨∇w′fi(w′),∇w′fj(w′) −∇wfj(w) +∇wfj(w)⟩| = |⟨∇wfi(w) −∇w′fi(w′),∇wfj(w)⟩−⟨∇w′fi(w′),∇w′fj(w′) −∇wfj(w)⟩| ≤|⟨∇wfi(w) −∇w′fi(w′),∇wfj(w)⟩|+ |⟨∇w′fi(w′),∇w′fj(w′) −∇wfj(w)⟩| ≤∥∇wfi(w) −∇w′fi(w′)∥∥∇wfj(w)∥+ ∥∇w′fi(w′)∥∥∇w′fj(w′) −∇wfj(w)∥ ≤L∥w −w′∥∥∇wfj(w)∥+ ∥∇w′fi(w′)∥L∥w′−w∥ ≤2LM∥w −w′∥. Here the ﬁrst inequality uses the triangle inequality, the second inequality uses the Cauchy-Schwartz inequality, and the third and fourth inequalities use the assumptions that ∇wfi(w) and ∇wfj(w) are Lipschitz in w and have bounded norm. We are now ready to prove the corollary, which we restate here. The proof uses a standard "epsilon-net" argument; we identify a ﬁne net of points within the ball Br.If the gradient confusion is small at every point in this discrete set, and the gradient confusion varies slowly enough with W,when we can guarantee small gradient confusion at every point in Br.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Corollary 5.1. Select a point W = (W0,W1,..., Wβ), satisfying assumption 1. Consider a ball Br centered at W of radius r> 0. If the data {xi}i∈[N] are sampled uniformly from a unit sphere, then the gradient confusion bound in equation 3 holds uniformly at all points W′∈Br with probability at least 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 ) , if r≤η/4ζ2 0 , 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 + 8dζ2 0 r η ) , otherwise. Proof. Deﬁne the function h+(W) = maxijhW(xi,xj).Our goal is to ﬁnd conditions under which h+(W) >−ηfor all W in a large set. To derive such conditions, we will need a Lipschitz constant for h+(W),which is no larger than the maximal Lipschitz constant of hW(xi,xj) for all i,j. We have that ∥∇Wfi∥= ∥ζxi(W)xi∥≤ ζ0.Now we need to get a W-Lipschitz constants for ∇xifi = ζxi(W)xi.By lemma D.1, we have ∥∇W(ζxi(W)xi)∥= ∥(∇Wζxi(W))xi∥≤ ζ0. Using lemma D.2, we see that 2ζ2 0 is a Lipschitz constant for hW(xi,xj),and thus also h+(W). Now, consider a minimizer W of the objective, and a ball Br around this point of radius r. Deﬁne the constant ϵ= η 4ζ2 0 ,and create an ϵ-net of points Nϵ = {Wi}inside the ball. This net is sufﬁciently dense that any W′∈Br is at most ϵunits away from some Wi ∈Nϵ.Furthermore, because h+(W) is Lipschitz in W,|h+(W′) −h+(Wi)|≤ 2ζ2 0 ϵ= η/2. We now know the following: if we can guarantee that h+(Wi) ≥−η/2, for all Wi ∈Nϵ, (11) then we also know that h+(W′) ≥−ηfor all W′∈Br. For this reason, we prove the result by bounding the probability that (11) holds. It is known that Nϵ can be constructed so that |Nϵ|≤ (2r/ϵ+ 1)d = (8ζ2 0 r/η+ 1)d (see Vershynin (2018), corollary 4.1.13). Theorem 5.1 provides a bound on the probability that each individual point in the net satisﬁes condition (11). Using a union bound, we see that all points in the net satisfy this condition with probability at least 1 −N2 (8ζ2 0 r η + 1 )d exp ( −cd(η/2)2 16ζ4 0 ) (12) = 1−N2 exp(dlog(8ζ2 0 r/η+ 1)) exp ( −cdη2 64ζ4 0 ) (13) ≥1 −N2 exp(8dζ2 0 r/η) exp ( −cdη2 64ζ4 0 ) (14) = 1−N2 exp ( −cdη2 64ζ4 0 + 8dζ2 0 r η ) . (15) Finally, note that, if r<ϵ, then we can form a net with |Nϵ|= 1. In this case, the probability of satisfying (11) is at least 1 −N2 exp ( −cd(η/2)2 64ζ4 0 ) . D.4. Proof of theorem 4.1 Theorem 4.1. Let W0,W1,..., Wβ be weight matrices chosen according to strategy 4.1. There exists ﬁxed constants c1,c2 >0 such that we have the following. 1. Consider a ﬁxed but arbitrary dataset x1,x2,..., xN with ∥xi∥≤ 1 for every i ∈[N]. For η >4, the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −cℓ2β(η−4)2 64ζ4 0 (β+2)4 ) . 2. If the dataset {xi}i∈[N] is such that each xi is an i.i.d. sample from the surface of d-dimensional unit sphere, then for every η >0 the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −c2(ℓd+ℓ2β)η2 16ζ4 0 (β+2)4 ) .The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Both parts in theorem 4.1 depend on the following argument. From theorem 2.3.8 and Proposition 2.3.10 in Tao (2012) with appropriate scaling7, we have for every p= 1,...,β we have that the matrix norm ∥Wp∥≤ 1 with probability at least 1 −βexp ( −c1κ2ℓ2) and ∥W0∥≤ 1 with probability at least 1 −exp ( −c1κ2d2) when the weight matrices are initialized according to strategy 4.1. Thus, conditioning on this event it implies that these matrices satisfy assumption 1. The proof strategy is similar to that of theorem 5.1. We will ﬁrst show that the gradient of the function h(.,.) as deﬁned in equation (7) with respect to the weights is bounded. Note that in part (1) the random variable is the set of weight matrices {Wp}p∈[β]. Thus, the dimension used to invoke theorem E.1 is at most ℓ2β. In part (2) along with the weights, the data x ∈Rd is also random. Thus, the dimension used to invoke theorem E.1 is at most ℓd+ ℓ2β. Combining this with theorem E.1, the bound on the gradient of h(.,.) and taking a union bound, we get the respective parts of the theorem. Thus, all it remains to prove is the bound on the gradient of the function h(.,.) as deﬁned in equation (7) with respect to the weights conditioning on the event that ∥Wp∥≤ 1 for every p∈{0,1,...,β }. We obtain the following analogue of equation (8). (∇Wζxi(W))ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  + (∇Wζxj(W))ζxi(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  + ζxi(W)ζxj(W) ∑ p∈[β]0 [ ∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) )]⊤ . (16) As in the case of the proof for theorem 5.1, we will upper-bound the ℓ2-norm of the above expression. In particular, we show the following. (∇Wζxi(W))ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]    2 ≤2ζ2 0 (β+ 2)2. (17) (∇Wζxj(W))ζxi(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]    2 ≤2ζ2 0 (β+ 2)2. (18) ζxi(W)ζxj(W) ∑ p∈[β]0 [ ∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) )]⊤ 2 ≤4ζ2 0 (β+ 2)2. (19) Equations (17) and 18 follow from the the fact that ∥(∇Wζxi(W))∥2 ≤ ζ0 and the arguments in the proof for theorem 5.1. We will now show the proof sketch for equation (19). For every p ∈ [β]0, consider ∥∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) ) ∥. Using the symmetry between xi and xj, the expression can be upper- bounded by, 2∥∇WHp−1(xi)∥∥gp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥+ 2∥Hp−1(xi)∥∥∇Wgp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥. As before we can use an inductive argument to ﬁnd the upper-bound and thus, we obtain the following which implies equation (19). ∥∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) ) ∥≤ 4(β+ 2)2. Next, we show that the expected value can be lower-bounded by−4 as in the case of theorem 4.1 above. Combining these two gives us the desired result. Consider EW[hW(xi,xj)]. We compute this expectation iteratively as follows. EW[hW(xi,xj)] = EW0 [EW1 [... EWβ[hW(xi,xj)] 7In particular, each entry has to be scaled by 1 ℓ for matrices {Wp}p∈[β] and 1 d for the matrix W0.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent ≥−4EW0  EW1  ... EWβ   ∑ p∈[β]0 Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤)      . The inequality combines equation 7 with Lemma D.1. We now prove the following inequality. EW0  EW1  ... EWβ   ∑ p∈[β]0 Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤)      ≤1. (20) Consider the inner-most expectation. Note that the only random variable is Wβ. Moreover, the term inside the trace is scalar. Note that the activation function σsatisﬁes |σ′(x)|≤ 1. Using the linearity of expectation, the LHS in equation (20) can be upper-bounded by the following. EW0 [ EW1 [ ... EWβ−1 [ Tr(Hβ−1(xi) ·Hβ−1(xi)⊤) ]]] (21) + EW0 [ EW1 [ ... EWβ [ ∑ p∈[β]0\{β} Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤) ]]] . (22) The ﬁrst sum in the above expression can be upper-bounded by 1, since |σ(x)|≤ 1. We will now show that the second sum is 0. Consider the inner-most expectation. The weights Wβ appears only in the expression gp(xi)⊤·gp(xj). Moreover, note that every entry in Wβ is an i.i.d. normal random variable with mean 0. Thus, the second summand simpliﬁes to, EW0 [ EW1 [ ... EWβ−1 [ ∑ p∈[β]0\{β,β−1} Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤) ]]] . Applying the above argument repeatedly we obtain that the second summand (equation (22)) is 0. Thus, we obtain the inequality in equation (20) which implies that EW[hW(xi,xj)] ≥−4. D.5. Proof of Theorem 6.1 In this section, we prove Theorem 6.1. The proof follows similar to those in previous sub-sections; we prove a bound on the gradient of the gradient inner-product and show that the expectation is non-negative. Combining these two with an argument similar to equation 10 we get the theorem. Note that the dataset is obtained by considering i.i.d. samples from a d-dimensional unit sphere. Thus, the lower-bound on the expectation (i.e., non-negative expectation of the gradient inner-product) follows from equation 9. Thus, it remains to prove an upper-bound on the norm of the gradient of the gradient inner-product term. Throughout this proof, we will use g(x) as a short-hand to denote gW(x). Consider the gradient ∇Wg(x). The the ith component of this can be written as follows. [∇Wg(x)]i = γ2ζx(W) ( WT β ·... WT i+1 ·xT ·WT 1 ·... WT i−1 ) . (23) Now consider, the gradient inner-product hW(xi,xj). We want to upper-bound the quantity ∥∇(xi,xj)hW(xi,xj)∥. From symmetry, this can be upper-bounded by 2∥∇xihW(xi,xj)∥. Consider the kth coordinate of ∇xihW(xi,xj). Using equation 23, the assumption that {Wi}i∈[β] are orthogonal matrices and taking the gradient, this can be written as, [∇xihW(xi,xj)]k = γ2ζxi(W)xj + α2 ( WT β ·... WT i+1 ·xT ·WT 1 ·... WT i−1 ) (∇xiζxi(W)) . (24) Combining assumption 1 with equation 24 we have that ∥∇xihW(xi,xj)∥is at most 2γ2β∥xj∥≤ 2γ2β. For the deﬁnition of the scaling factor γ = 1√2β, we have that 2γ2β = 1. Thus, ∥∇(xi,xj)hW(xi,xj)∥≤ 2. E. Technical lemmas We will brieﬂy describe some technical lemmas we require in our analysis. The following Chernoff-style concentration bound is proved in Chapter 5 of Vershynin (2018).The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Lemma E.1 (Concentration of Lipshitz function over a sphere). Let x ∈Rd be sampled uniformly from the surface of a d-dimensional sphere. Consider a Lipshitz function ℓ : Rd →R which is differentiable everywhere. Let ∥∇ℓ∥2 denote supx∈Rd ∥∇ℓ(x)∥2. Then for any t≥0 and some ﬁxed constant c≥0, we have the following. Pr [⏐⏐⏐ℓ(x) −E[ℓ(x)] ⏐⏐⏐≥t ] ≤2 exp ( −cdt2 ρ2 ) , (25) where ρ≥∥∇ℓ∥2. We will rely on the following generalization of lemma E.1. We would like to point out that the underlying metric is the Euclidean metric and thus we use the ∥.∥2-norm. Corollary E.1. Let x,y ∈Rd be two mutually independent vectors sampled uniformly from the surface of a d-dimensional sphere. Consider a Lipshitz function ℓ : Rd ×Rd → R which is differentiable everywhere. Let ∥∇ℓ∥2 denote sup(x,y)∈Rd×Rd ∥∇ℓ(x,y)∥2. Then for any t≥0 and some ﬁxed constant c≥0, we have the following. Pr [⏐⏐⏐ℓ(x,y) −E[ℓ(x,y)] ⏐⏐⏐≥t ] ≤2 exp ( −cdt2 ρ2 ) , (26) where ρ≥∥∇ℓ∥2. Proof. This corollary can be derived from lemma E.1 as follows. Note that for every ﬁxed ˜y ∈Rd, equation 25 holds. Additionally, we have that the vectors x and y are mutually independent. Hence we can write the LHS of equation 26 as the following. ∫ (˜y)1=∞ (˜y)1=−∞ ... ∫ (˜y)d=∞ (˜y)d=−∞ Pr [⏐⏐⏐ℓ(x,y) −E[ℓ(x,y)] ⏐⏐⏐≥t ⏐⏐⏐⏐⏐y = ˜y ⏐⏐⏐⏐⏐ ] φ(˜y)d(˜y)1 ...d (˜y)d. Here φ(˜y) refers to the pdf of the distribution of y. From independence, the inner term in the integral evaluates to Pr [⏐⏐⏐ℓ(x,˜y) −E[ℓ(x,˜y)] ⏐⏐⏐≥t ] . We know this is less than or equal to 2 exp ( −cdt2 ∥∇ℓ∥2 2 ) . Therefore, the integral can be upper bounded by the following. ∫ (˜y)1=∞ (˜y)1=−∞ ... ∫ (˜y)d=∞ (˜y)d=−∞ 2 exp ( − cdt2 ∥∇ℓ∥2 2 ) φ(˜y)d(˜y)1 ...d (˜y)d. Since φ(˜y) is a valid pdf, we get the required equation 26. Additionally, we will use the following facts about a normalized Gaussian random variable. Lemma E.2. For a normalized Gaussian x (i.e., an x sampled uniformly from the surface of a unit d-dimensional sphere) the following statements are true. 1. ∀p∈[d] we have that E[(x)p] = 0. 2. ∀p∈[d] we have that E[(x)2 p] = 1/d. Proof. Part (1) can be proved by observing that the normalized Gaussian random variable is spherically symmetric about the origin. In other words, for every p∈[d] the vectors (x1,x2,...,x p,...,x d) and (x1,x2,..., −xp,...,x d) are identically distributed. Hence E[xp] =E[−xp] which implies that E[xp] = 0. Part (2) can be proved by observing that for any p,p′∈[d], xp and xp′ are identically distributed. Fix any p∈[d]. We have that ∑ p′∈[d] E[x2 p′] =d×E[x2 p]. Note that we have ∑ p′∈[d] E[x2 p′] = ∫ (x)1=∞ (x)1=−∞ ... ∫ (x)d=∞ (x)d=−∞ ∑ p′∈[d] x2 p′ ∑ p′′∈[d] x2 p′′ φ(x)d(x)1 ...d (x)d = 1. Therefore E[x2 p] = 1/d.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent We use the following well-known Gaussian concentration inequality in our proofs ( e.g., Chapter 5 in Boucheron et al. (2013)). Lemma E.3 (Gaussian Concentration). Let x = (x1,x2,...,x d) be i.i.d. N(0,ν2) random variables. Consider a Lipshitz function ℓ: Rd →R which is differentiable everywhere. Let ∥∇ℓ∥2 denote supx∈Rd ∥∇ℓ(x)∥2. Then for any t≥0, we have the following. Pr [⏐⏐⏐ℓ(x) −E[ℓ(x)] ⏐⏐⏐≥t ] ≤2 exp ( − t2 2ν2ρ2 ) , (27) where ρ≥∥∇ℓ∥2. F. Additional discussion of the small weights assumption (assumption 1) Without the small-weights assumption, the signal propagated forward or the gradients ∇Wfi could potentially blow up in magnitude, making the network untrainable. Proving non-vacuous bounds in case of such blow-ups in magnitude of the signal or the gradient is not possible in general, and thus, we assume this restricted class of weights. Note that the small-weights assumption is not just a theoretical concern, but also usually holds in practice. Neural networks are often trained with weight decay regularizers of the form ∑ i∥Wi∥2 F, which keep the weights small during optimization. The operator norm of convolutional layers have also recently been used as an effective regularizer for image classiﬁcation tasks by Sedghi et al. (2018). In the proof of theorem 4.1 we showed that assumption 1 holds with high probability at standard Gaussian initializations used in practice. While, in general, there is no reason to believe that such a small-weights assumption would continue to hold during optimization without explicit regularizers like weight decay, some recent work has shown evidence that the weights do not move too far away during training from the random initialization point for overparameterized neural networks (Neyshabur et al., 2018; Dziugaite & Roy, 2017; Nagarajan & Kolter, 2019; Zou et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018; Oymak & Soltanolkotabi, 2018). It is worth noting though that all these results have been shown under some restrictive assumptions, such as the width requiring to be much larger than generally used by practitioners.
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "A convergence theory for deep learning via over-parameterization",
        "A convergence analysis of gradient descent for deep linear neural networks",
        "The shattered gradients problem: If resnets are the answer, then what is the question?",
        "Learning long-term dependencies with gradient descent is difﬁcult",
        "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey",
        "Concentration inequalities: A nonasymptotic theory of independence",
        "Sgd learns over-parameterized networks that provably generalize on linearly separable data",
        "Entropy-sgd: Biasing gradient descent into wide val- leys",
        "The loss landscape of overparameterized neural networks",
        "Towards faster stochastic gradient search",
        "Batch normalization biases residual blocks towards the identity function in deep networks",
        "Automated inference with adaptive batches",
        "Gradient descent provably optimizes over-parameterized neural networks",
        "Computing nonvacuous generalization bounds for deep (stochastic) neural net- works with many more parameters than training data",
        "The power of depth for feedfor- ward neural networks",
        "Stiffness: A new perspective on generalization in neural networks",
        "An investiga- tion into neural net optimization via hessian eigenvalue density",
        "Understanding the difﬁculty of training deep feedforward neural networks",
        "Deep residual learn- ing for image recognition",
        "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
        "Neural tangent kernel: Convergence and generalization in neural networks",
        "Linear conver- gence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition",
        "Imagenet classiﬁcation with deep convolutional neural networks",
        "Efﬁcient backprop",
        "Wide neural networks of any depth evolve as linear models under gradient descent",
        "Linear convergence of gradient descent for deep linear neural networks",
        "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm",
        "Lectures on convex optimization, volume 137",
        "Towards understanding the role of over- parameterization in generalization of neural networks",
        "The loss surface of deep and wide neural networks",
        "Overparameterized non-linear learning: Gradient descent takes the shortest path?",
        "A stochastic approximation method",
        "Empirical analysis of the hessian of over-parametrized neural networks",
        "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "Fast convergence of stochastic gradient descent under a strong growth condition",
        "Deep information propagation",
        "Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla convolu- tional neural networks",
        "The singular values of convolutional layers",
        "Stochastic gradient descent for non-smooth optimization: Convergence results and opti- mal averaging schemes",
        "Very deep convolu- tional networks for large-scale image recognition",
        "On the generalization beneﬁt of noise in stochastic gradient descent",
        "On the importance of initialization and momentum in deep learn- ing",
        "Topics in random matrix theory, volume 132",
        "Beneﬁts of depth in neural networks",
        "High-dimensional probability: An introduc- tion with applications in data science, volume 47",
        "The marginal value of adaptive gradient methods in machine learning",
        "Towards understanding generalization of deep learning: Perspective of loss landscapes",
        "A mean ﬁeld theory of batch normal- ization",
        "Gradient diversity: a key ingredient for scalable distributed learning",
        "Wide residual networks"
    ]
}
