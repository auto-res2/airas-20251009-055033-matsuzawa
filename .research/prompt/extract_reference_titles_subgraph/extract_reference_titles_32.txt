
Input:
You are an expert in academic paper analysis. 
Your task is to extract reference paper titles from the full text of research papers.

Instructions:
- Analyze the provided full text of research papers
- Extract all reference paper titles mentioned in the text
- Focus on titles that appear in reference sections, citations, or are explicitly mentioned as related work
- Return only the exact titles as they appear in the text
- Exclude general topics or field names that are not specific paper titles
- If no clear reference titles are found, return an empty list

Full Text:
---------------------------------
Tensor Normal Training for Deep Learning Models Yi Ren, Donald Goldfarb Department of Industrial Engineering and Operations Research Columbia University New York, NY 10027 {yr2322, goldfarb}@columbia.edu Abstract Despite the predominant use of ﬁrst-order methods for training deep learning mod- els, second-order methods, and in particular, natural gradient methods, remain of interest because of their potential for accelerating training through the use of cur- vature information. Several methods with non-diagonal preconditioning matrices, including KFAC [34], Shampoo [18], and K-BFGS [15], have been proposed and shown to be effective. Based on the so-called tensor normal (TN) distribution [31], we propose and analyze a brand new approximate natural gradient method, Tensor Normal Training (TNT), which like Shampoo, only requires knowledge of the shape of the training parameters. By approximating the probabilistically based Fisher matrix, as opposed to the empirical Fisher matrix, our method uses the block-wise covariance of the sampling based gradient as the pre-conditioning matrix. Moreover, the assumption that the sampling-based (tensor) gradient follows a TN distribution, ensures that its covariance has a Kronecker separable structure, which leads to a tractable approximation to the Fisher matrix. Consequently, TNT’s memory requirements and per-iteration computational costs are only slightly higher than those for ﬁrst-order methods. In our experiments, TNT exhibited superior optimization performance to state-of-the-art ﬁrst-order methods, and compara- ble optimization performance to the state-of-the-art second-order methods KFAC and Shampoo. Moreover, TNT demonstrated its ability to generalize as well as ﬁrst-order methods, while using fewer epochs. 1 Introduction First-order methods are currently by far the most popular and successful optimization methods for training deep learning models. Stochastic gradient descent (SGD) [39] uses the (stochastic) gradient direction to guide its update at every iteration. Adaptive learning rate methods, including AdaGrad [11], RMSprop [20], and Adam [23], scale each element of the gradient direction (possibly modiﬁed to incorporate momentum) by the square root of the second moment of each element of the gradient. These ﬁrst-order methods use little curvature information to "pre-condition" the gradient direction; SGD uses an identity pre-conditioning matrix, whereas the others use a diagonal matrix. On the other hand, second-order methods attempt to greatly accelerate the optimization process by exploring the rich curvature information of the problem. Traditional second-order methods such as Newton’s method, BFGS [6, 13, 14, 41], and limited-memory BFGS (L-BFGS) [ 28], without modiﬁcation, are not practical in a deep learning setting, because these methods require enormous amounts of memory and computational effort per iteration due to the huge number of parameters such models have. Some second-order methods have been proposed to deal with the non-convexity and stochasiticity of objective functions arising in machine learning (see e.g. [ 36, 7, 16, 44]), but directly using these methods to train deep learning models still requires large amounts of memory and computing resources. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.02925v3  [cs.LG]  21 Dec 2021Recently, there has been considerable advancement in the development of second-order methods that are suitable for deep learning models with huge numbers of parameters. These methods usually ap- proach pre-conditioning of the gradient in a modular way, resulting in block-diagonal pre-conditioning matrices, where each block corresponds to a layer or a set of trainable parameters in the model. Inspired by the idea of the natural gradient (NG) method [1], [34] proposed KFAC, an NG method that uses a Kronecker-factored approximation to the Fisher matrix as its pre-conditioning matrix that can be applied to multilayer perceptrons, and which has subsequently been extended to other architectures, such as convolutional neural networks [17] and recurrent neural networks [35]. Kronecker-factored preconditioners [15, 38] based on the structure of the Hessian and quasi-Newton methods have also been developed. Despite the great success of these efﬁcient and effective second-order methods, developing such methods requires careful examination of the structure of the preconditioning matrix to design appropriate approximations for each type of layer in a model. Another well-recognized second-order method, Shampoo [18, 3], extends the adaptive learning rate method AdaGrad, so that the gradient is pre-conditioned along every dimension of the underlying tensor of parameters in the model, essentially replacing the diagonal pre-conditioning matrix of the adaptive learning rate methods by a block diagonal Kronecker-factored matrix which can be viewed as an approximation to a fractional power of the empirical Fisher (EF) matrix. However, while estimating the Fisher matrix, in a deep learning setting, by the EF matrix saves some computational effort, it usually does not capture as much valuable curvature information as the Fisher matrix [26]. Variants of the normal distribution, i.e. the matrix-normal distribution [ 9] and the tensor-normal distribution [31], have been proposed to estimate the covariance of matrix and higher-order tensor observations, respectively. By imposing a Kronecker structure on the covariance matrix, the resulting covariance estimate requires a vastly reduced amount of memory, while still capturing the interactions between the various dimensions of the respective matrix or tensor. Iterative MLE methods for estimating the parameters of matrix-normal and tensor-normal distributions have been examined in e.g. [12, 31], and various ways to identify the unique representation of the distribution parameters have been proposed in [43, 10]. However, to the best of our knowledge, this advanced statistical methodology has not been used to develop optimization methods for deep learning. In this paper, we describe a ﬁrst attempt to do this and demonstrate its great potential. Our Contributions. In this paper, we propose a brand new approximate natural gradient (NG) method, Tensor-Normal Training (TNT), that makes use of the tensor normal distribution to approxi- mate the Fisher matrix. Signiﬁcantly, the TNT method can be applied to any model whose training parameters are a collection of tensors without knowing the exact structure of the model. To achieve this, we ﬁrst propose a new way, that is suitable for optimization, to identify the covariance parameters of tensor normal (TN) distributions, in which the average eigenvalues of the covariance matrices corresponding to each of the tensor dimensions are required to be the same (see Section 3). By using the Kronecker product structure of the TN covariance, TNT only introduces mild memory and per-iteration computational overhead compared with ﬁrst-order methods. Also, TNT’s memory usage is the same as Shampoo’s and no greater than KFAC’s, while its per-iteration computational needs are no greater than Shampoo’s and KFAC’s (see Section 5). The effectiveness of TNT is demonstrated on deep learning models. Speciﬁcally, on standard autoencoder problems, when optimization performance is compared, TNT converges faster than the benchmark ﬁrst-order methods and roughly the same rate as the benchmark second-order methods. Moreover, on standard CNN models, when generalization is concerned, TNT is able to achieve roughly the same level of validation accuracy as the ﬁrst-order methods, but using far fewer epochs (see Section 6). We also prove that, if the statistics used in TNT can be estimated ideally, it converges to a stationary point under mild assumptions (see Section 4). 2 Preliminaries Supervised Learning. Throughout this paper, we consider the classic supervised learning setting where we learn the parameters θof a model, by minimizing Lpθq“ 1 N řN i“1 lpyi,fθpxiqq, where tpxi,yiquN i“1 denotes a given dataset (xi being the input to the model and yi being the target), fθpxiq denotes the output of the model when xi is provided as the input, and l denotes a loss function 2(e.g. least-squares loss for regression and cross entropy loss for classiﬁcation) that measures the discrepancy between the model output fθpxiqand the target yi. Natural Gradient Method and the Fisher Matrix. In a ﬁrst-order method, say SGD, the updating direction is always derived from an estimate to the gradient direction ∇θLpθq. In a natural gradient (NG) method [1], however, the Fisher matrix is used as a pre-conditioning matrix that is applied to the gradient direction. As shown in [34], the Fisher matrix is deﬁned as F “Ex„Qx,y„pp¨|x,θq ” ∇θlog ppy|x,θqp∇θlog ppy|x,θqqJ ı , (1) where Qx is the data distribution of xand pp¨|x,θqis the density function of the conditional distribu- tion deﬁned by the model with a given input x. In many cases, such as when pis associated with a Gaussian distribution and the loss function l measures least-squares loss, or when pis associated with a multinomial distribution and lis cross- entropy loss, log pis equivalent to l(see e.g. [33, 34]). Hence, if Dθdenotes the gradient of lw.r.t. θ for a given xand y, we have that F “Ex„Qx,y„prDθDθJs. Consequently, one can sample xfrom Qx and perform a forward pass of the model, then sample yfrom pp¨|x,θq, and perform a backward pass to compute Dθ, and then use DθDθJto estimate F. We call Dθa sampling-based gradient, as opposed to the empirical gradient ∇θlpyi,fθpxiqqwhere pxi,yiqis one instance from the dataset. It is worth noting that the ﬁrst moment of Dθis zero. To see this, note that, with given x, Ey„pr∇θlog ppy|x,θqs“ ż ∇θlog ppy|x,θqppy|x,θqdy“ ż ∇θppy|x,θqdy “∇θ ˆż ppy|x,θqdy ˙ “∇θ1 “0. Hence, Ex„Qx,y„prDθs“ Ex„Qx tEy„pr∇θlog ppy|x,θqs| xu“ 0. Thus, the Fisher matrix F can be viewed as the covariance matrix of Dθ. Note that the empirical Fisher CANNOT be viewed as the covariance of the empirical gradient, because the ﬁrst moment of the latter is, in general, NOT zero. Tensor-Normal Distribution. The development of our new method makes use the so-called tensor- normal distribution [31, 10]: Deﬁnition 1. An arbitrary tensor GPRd1ˆ¨¨¨ˆdk is said to follow a tensor normal (TN) distribution with mean parameter M PRd1ˆ¨¨¨ˆdk and covariance parameters U1 PRd1ˆd1 , ..., Uk PRdkˆdk if and only if vecpGq„ NormalpvecpMq,U1 b¨¨¨b Ukq. In the above deﬁnition, the vec operation refers to the vectorization of a tensor, whose formal deﬁnition can be found in Sec A in the Appendix. Note that matrix-normal distribution can be viewed as a special case of TN distribution, where k “2. Compared with a regular normal distribution, whose covariance matrix has śk i“1 d2 i elements, the covariance of a k-way tensor-normal distribution is stored in ksmaller matrices with a total number of elements equal to řk i“1 d2 i. To estimate the covariance submatrices U1,...,U k, the following property (e.g., see [10]) is used: ErGpiqs“ Ui ¨ ź j‰i trpUjq, (2) where Gpiq :“matipGqmatipGqJ PRdiˆdi denotes the contraction of Gwith itself along all but the ith dimension and mati refers to matricization of a tensor (see Section A for the formal deﬁnitions). By (2), we can sample Gto obtain estimates of the Gpiq’s, and hence, estimates of the Ui’s. The complexity of computing Gpiqis di śk j“1 dj, which is also far less than the complexity of computing vecpGqvecpGqJneeded to estimate the covariance of a regular normal distribution. 3 Tensor-Normal Training In this section, we propose Tensor-Normal Training (TNT), a brand new variant of the natural gradient (NG) method that makes use of the tensor-normal distribution. 33.1 Block Diagonal Approximation In this paper, we consider the case where the parameters of the model θconsists of multiple tensor variables W1,...,W L, i.e. θ “ pvecpW1qJ,..., vecpWLqJqJ. This setting is applicable to most common models in deep learning such as multi-layer perceptrons, convolutional neural networks, recurrent neural networks, etc. In these models, the trainable parameter Wl (l “1,...,L ) come from the weights or biases of a layer, whether it be a feed-forward, convolutional, recurrent, or batch normalization layer, etc. Note that the index lof Wl refers to the index of a tensor variable, as opposed to a layer. To obtain a practical NG method, we assume, as in KFAC and Shampoo, that the pre-conditioning Fisher matrix is block diagonal. To be more speciﬁc, we assume that each block corresponds to the covariance of a tensor variable in the model. Hence, the approximate Fisher matrix is: F «diagL l“1 ␣ Ex„Qx,y„p “ vecpDWlqpvecpDWlqqJ‰( “diagL l“1 tVarpvecpDWlqqu. The remaining question is how should one approximate VarpvecpDWlqqfor l“1,...,L . 3.2 Computing the Approximate Natural Gradient Direction by TNT We consider a tensor variableW PRd1ˆ¨¨¨ˆdk in the model and assume that G:“DW PRd1ˆ¨¨¨ˆdk follows a TN distribution with zero mean and covariance parameters U1,...,U k where Ui PRdiˆdi. Thus, the Fisher matrix corresponding to W is FW “Ex„Qx,y„prVarpvecpGqqs“ U1 b¨¨¨b Uk. Loosely speaking, the idea of relating the Fisher matrix to the covariance matrix of some normal distribution has some connections to Bayesian learning methods and interpretations of NG methods (see e.g., [22]). Let ∇WL PRd1ˆ¨¨¨ˆdk denote the gradient of L w.r.t. W. The approximate NG updating direction for W is computed as F´1 W vecp∇WLq“p U´1 1 b¨¨¨b U´1 k qvecp∇WLq“ vec ` ∇WL ˆ1 U´1 1 ˆ2 ¨¨¨ˆ k U´1 k ˘ , (3) where ˆi (i“1,...,k ) denotes a mode-iproduct (see Section A in the Appendix). Note that the last equality of (3) makes use of the following proposition, which also appears in [18] (see Sec A in the Appendix for a proof): Proposition 1. Let GPRd1ˆ¨¨¨ˆdk and Ui PRdiˆdi for i“1,...,k . Then, we have ` bk i“1Ui ˘ vecpGq“ vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq. (4) To summarize, the generic Tensor-Normal Training algorithm is: Algorithm 1 Generic Tensor-Normal Training (TNT) Require: Given batch size m, and learning rate α 1: for t“1,2,... do 2: Sample mini-batch Mt of size m 3: Perform a forward-backward pass over Mt to compute the mini-batch gradient 4: Perform another backward pass over Mt with ysampled from the predictive distribution to compute Gl “DWl (l“1,...,L ) averaged across Mt 5: for l“1,...L do 6: Estimate ErGpiq l s(i“1,...,k l) from Gl 7: Determine Uplq 1 ,...,U plq kl from ErGp1q l s,..., ErGpklq l s 8: Compute the inverses of Uplq 1 ,...,U plq kl 9: Compute the updating direction pl by (3) 10: Wl “Wl ´α¨pl. 11: end for 12: end for 3.3 Identifying the Covariance Parameters of the Tensor Normal Distribution By (2), Ui can be inferred from ErGpiqsup to a constant multiplier. However, different sets of multipliers can generate the same F, i.e. the same distribution. This is less of a problem if one 4only cares about F. However, we need F´1 to compute the approximate natural gradient. That is, we ﬁrst must choose a representation of F “cp˜U1 b¨¨¨b ˜Ukq(see below), and then compute F´1 “c´1pp˜U1 `ϵIq´1 b¨¨¨bp ˜Uk `ϵIq´1qwith a proper choice of ϵ ą0, where ϵI plays a damping role in the preconditioning matrix. In this case, different representations of F will lead to different F´1. The statistics community has proposed various representations for ˜Ui’s. For example, [43] imposed that c “ 1 and the ﬁrst element of ˜Ui to be one for i “ 1,...,k ´1, whereas [ 10] imposed that trp˜Uiq“ 1 for i“1,...,k . Although these representations have nice statistical properties, they are not ideal from the perspective of inverting the covariance for use in a NG method in optimization. We now describe one way to determine ˜U1,..., ˜Uk, and cfrom ErGp1qs,..., ErGpkqs. In particular, we ﬁrst set c“1, so that F´1 has a constant upper bound ϵ´kI. We then require that trp˜Uiq di is constant w.r.t i. In other words, the average of the eigenvalues of each of the ˜Ui’s is the same. This helps the ˜Ui’s have similar overall "magnitude" so that a suitableϵcan be found that works for all dimensions. Moreover, this shares some similarity with how KFAC splits the overall damping term between KFAC matrices, although KFAC adjusts the damping values, whereas TNT adjusts the matrices. A bit of algebra gives the formula ˜Ui “ ErGpiqs ck´1 0 ś j‰idj , (5) where c0 “ ´ trpErGpiqsqś jdj ¯1{k . 3.4 Comparison with Shampoo and KFAC Shampoo, proposed in [18], and later modiﬁed and extended in [3], is closely related to TNT. Both methods use a block-diagonal Kronecker-factored preconditioner based on second-order statistics of the gradient and are able to handle all sorts of tensors, and hence, can be applied to all sorts of deep neural network models, easily and seamlessly. The major differences between them are: (i) The TN distribution cannot be directly applied to EF, which is used in Shampoo, because the empirical gradient does not have a zero mean; hence its covariance and second moment are different. It is also believed that EF does not capture as much valuable curvature information as Fisher [26]. (ii) Using the statistics ErGpiqs’s, TNT approximates the Fisher matrix as the covariance of the block-wise sampling-based gradients assuming that they are TN distributed. On the other hand, Shampoo computes 1{2k-th power of the statistics of each direction of the tensor-structured empirical gradient and forms a preconditioning matrix from the Kronecker product of them. It is unclear to us how to interpret statistically such a matrix other than by its connection to EF. We further note that Shampoo was developed as a Kronecker-factored approximation to the full-matrix version of AdaGrad [11], whereas TNT was developed as a NG method using a TN-distributed approximation to the Fisher matrix. (iii) TNT computes the updating direction using the inverse (i.e. power of ´1) of the Kronecker factors of the approximate Fisher matrix, whereas Shampoo uses the ´1{2k-th power1 of the Kronecker factors of the EF matrix. Another method related to TNT is KFAC [34, 17], which, like TNT, uses Fisher as its preconditioning matrix. Their major differences are: (i) KFAC develops its approximation based on the structure of the gradient and Fisher matrix for each type of layer. Admittedly, this could lead to better approximations. But it is relatively hard to implement (e.g. one need to store some intermediate variables to construct the KFAC matrices). Also, if new types of layers with different structures are considered, one needs to develop suitable Kronecker factorizations, i.e., KFAC matrices. On the contrary, TNT, like Shampoo, is a model- agnostic method, in the sense that, TNT can be directly applied as long as the shape of the tensor variables are speciﬁed. 1In [3], for autoencoder problems involving tensors of order 2, the power was set to ´α 2 , where αPr0,1s was treated as a hyper-parameter which required tuning, and was set to α“1 after tuning. 50 500 1000 iteration 0.4 0.5 0.6 0.7 0.8 0.9 1.0cosine similarity l = 1 0 500 1000 iteration 0.4 0.5 0.6 0.7 0.8 0.9 l = 2 0 500 1000 iteration 0.5 0.6 0.7 0.8 0.9 l = 3 0 500 1000 iteration 0.5 0.6 0.7 0.8 0.9 l = 4 TNT KFAC Shampoo Adam SGD-m Figure 1: Cosine similarity between the directions produced by the methods shown in the legend and that of a block Fisher method. The algorithms were run on a 16 ˆ16 down-scaled MNIST [27] dataset and a small feed-forward NN with layer widths 256-20-20-20-20-20-10 described in [34]. As in [34], we only show the middle four layers. (ii) Each block of TNT corresponds to a tensor variable whose shape needs to be speciﬁed, whereas each block of KFAC corresponds to all variables in a layer. For example, for a linear or convo- lutional layer, the KFAC block would correspond to the Fisher of both its weights and bias (and their correlation), whereas TNT would produce two blocks corresponding to the weights and bias, respectively. In order to gain more insight into how well TNT approximates the Fisher matrix compared with other methods, we computed the cosine similarity between the direction produced by each method and that by a block Fisher method, where each block corresponded to one layer’s full Fisher matrix in the model (see Figure 1). For all methods shown in Figure 1, we always followed the trajectory produced by the block Fisher method. In our implementation of the block Fisher method, both the gradient and the block-Fisher matrices were estimated with a moving-average scheme, with the decay factors being 0.9. In all of the other methods compared to the block Fisher method, moving averages were also used, with the decay factors being 0.9, as described in Section D in the Appendix, to compute the relevant gradients and approximate block-Fisher matrices used by them, based on values computed at points generated by the block-Fisher method. As shown in Figure 1, the cosine similarity for TNT is always around 0.7 to 0.8, which is similar to (and sometimes higher) than the structure-aware method KFAC, and always better than Shampoo. To provide more information, we also include SGD with momentum and Adam, whose similarity to the block Fisher direction is usually lower that of the second-order methods. 4 Convergence In this section, we present results on the convergence of an idealized version of TNT that uses the actual covariance of Dθ, rather than a statistical estimate of it (see Algorithm 2 in the Appendix). In particular, our results show that Algorithm 2, with constant batch size and decreasing step size, converges to a stationary point under some mild assumptions. For simplicity, we assume that the model only contains one tensor variableW. However, our results can be easily extended to the case of multiple tensor variables. To start with, our proofs, which are delayed to Section B in the Appendix, require the following assumptions: Assumption 1. L : Rn ÑR is continuously differentiable. Lpθqis lower bounded by a real number Llow for any θPRn. ∇L is globally Lipschitz continuous with Lipschitz constant L; namely for any θ,θ1 PRn, }∇Lpθq´ ∇Lpθ1q}ď L}θ´θ1}. Assumption 2. For any iteration t, we have aq Eξt r∇lpθt,ξtqs“ ∇Lpθtq bq Eξt ” }∇lpθt,ξtq´ ∇Lpθtq}2 ı ďσ2 where σ ą 0 is the noise level of the gradient estimation, and ξt,t “ 1,2,..., are independent samples, and for a given tthe random variable ξt is independent of tθjut j“1 Assumption 3. Let G :“ Dθ. For any θ P Rn, the norm of the Fisher matrix F “ Ex„Qx,y„prvecpGqvecpGqJsis bounded above. Since F represents the curvature of the KL divergence of the model’s predictive distribution, As- sumption 3 controls the change of predictive distribution when the model’s parameters change; hence, 6Table 1: Memory and per-iteration time complexity beyond that required by SGD Name Memory Time (per-iteration) TNT Opřk i“1 d2 iq Opp1 T1 m`řk i“1 diqśk i“1 di ` 1 T2 řk i“1 d3 iq Shampoo Opřk i“1 d2 iq Oppřk i“1 diqśk i“1 di `p 1 T2 řk i“1 d3 i- if using SVDqq Adam-like Opśk i“1 diq Opśk i“1 diq Newton-like Opśk i“1 d2 iq - depends on speciﬁc algorithm it is a mild assumption for reasonable deep learning models. Essentially, we would like to prove that, if the Fisher matrix is upper bounded, our approximated Fisher (by TNT) is also upper bounded. We now present two lemmas and our main theorem; see Section B in the Appendix for proofs. Lemma 1. }Ex„Qx,y„prGpiqs}ď ´ 1 di śk i1“1 di1 ¯ }Ex„Qx,y„prvecpGqvecpGqJs}, @i“1,...,k. Lemma 2. Suppose Assumption 3 holds. Let FTNT :“p ˜U1 `ϵIqb¨¨¨bp ˜Uk `ϵIqwhere ˜Ui’s are deﬁned in (5). Then, the norm of FTNT is bounded both above and below. Theorem 1. Suppose that Assumptions 1, 2, and 3 hold for tθtugenerated by Algorithm 2 with batch size mt “mfor all t. If we choose αt “ κ L¯κ2 t´β, with β Pp0.5,1q, then 1 N Nÿ t“1 Etξju8 j“1 ” }∇Lpθtq}2 ı ď 2L ` ML ´Llow˘ ¯κ2 κ2 Nβ´1 ` σ2 p1 ´βqm ` N´β ´N´1˘ , where N denotes the iteration number and the constant ML ą0 depends only on L. Moreover, for a given δ P p0,1q,to guarantee that 1 N řN t“1 Etξju8 j“1 ” }∇Lpθtq}2 ı ă δ, N needs to be at most O ´ δ´ 1 1´β ¯ . 5 Implementation Details of TNT and Comparison on Complexity Implementation Details of TNT. In practice, we compute G “DW averaged over a minibatch of data at every iteration, and use the value of G piq to update a moving average estimate yGpiq of ErGpiqs. The extra work for these computations (as well as for updating the inverses of ˜Ui) compared with a stochastic gradient descent method is amortized by only performing them every T1 (and T2) iterations, which is also the approach used in KFAC and Shampoo, and does not seems to degrade the overall performance of the TNT algorithm. Moreover, we compute ErGpiqsusing the whole dataset at the initialization point as a warm start, which is also done in our implementations of Shampoo and KFAC. See Algorithm 3 in the Appendix for the detailed implementation of TNT. A Comparison on Memory and Per-iteration Time Complexity.To compare the memory require- ments and per-iteration time complexities of different methods, we consider the case where we optimize one tensor variable of size d1 ˆ¨¨¨ˆ dk using minibatches of size mat every iteration. A plain SGD method requires Opśk i“1 diqto store the model parameters and the gradient, whereas its per-iteration time complexity is Opmśk i“1 diq. Table 1 lists the memory requirements and per-iteration time complexities in excess of that required by SGD for different methods. Compared with a classic Newton-like method (e.g. BFGS), TNT (as well as Shampoo) reduces the memory requirement from Opśk i“1 d2 iqto Opřk i“1 d2 iq, which is comparable to that of Adam-like adaptive gradient methods. In fact, if the di’s are all equal to dand 3 ďk ăăd, the Kronecker- factored TNT pre-conditioning matrix requires kd2 storage, which is less than that required by the diagonal pre-conditioners used by Adam-like methods. On the other hand, in terms of per-iteration time complexity, TNT (as well as Shampoo) only introduces a mild overhead for estimating the statistics ErGpiqs’s, inverting the pre-conditioning matrices, and computing the updating direction. Also, the ﬁrst two of these operations can be amortized by only performing them every T1 and T2 iterations. Lastly, the extra work of Op1 T1 mśk i“1 diqrequired by TNT relative to Shampoo is due to the extra backward pass needed to estimate the true Fisher, as opposed to the EF. 7Moreover, although TNT and Shampoo both incur 1 T2 řk i“1 d3 i amortized time to invert the pre- conditioning matrices, the SVD operation in Shampoo can take much more time than the matrix inverse operation in TNT, especially when the matrix size is large2. The per-iteration computational complexity of KFAC is more complicated because it depends on the type of the layer/variable. For linear layers, TNT and KFAC both uses two matrices, whose sizes are the number of input nodes and output nodes, respectively. For convolutional layers, TNT uses three matrices, whose sizes are the size of ﬁlter, number of input channels, and number of output channels, whereas KFAC uses two matrices whose sizes are the size of ﬁlter times number of input channels, and number of output channels. As a result, the ﬁrst KFAC matrix requires much more memory. In general, the per-iteration complexity of KFAC is no less than that of TNT. 6 Experiments In this section, we compare TNT with some state-of-the-art second-order (KFAC, Shampoo) and ﬁrst-order (SGD with momentum, Adam) methods (see Section D.1 in the Appendix on how these methods were implemented). The Hessian-based K-BFGS method [15, 38] is another state-of-the-art Kronecker-factored second-order method for training deep learning models. Since our focus is on optimizers that use Fisher or empirical Fisher as the preconditioning matrix, we did not include K-BFGS in our comparison. Our experiments were run on a machine with one V100 GPU and eight Xeon Gold 6248 CPUs using PyTorch [37]. Each algorithm was run using the best hyper-parameters, determined by an appropriate grid search (speciﬁed below), and 5 different random seeds. In Figures 2 and 3 the performance of each algorithm is plotted: the solid curves give results obtained by averaging the 5 different runs, and the shaded area depicts the ˘standard deviation range for these runs. Our code is available at https://github.com/renyiryry/tnt_neurips_2021. 6.1 Optimization: Autoencoder Problems 0 100 200 300 400 500 epoch 102 train loss 0 100 200 300 400 500 process time (second) 102 train loss TNT KFAC Shampoo Adam SGD-m a) MNIST autoencoder 0 250 500 750 1000 1250 epoch 101 102 train loss 0 500 1000 1500 2000 process time (second) 101 102 train loss TNT KFAC Shampoo Adam SGD-m b) FACES autoencoder Figure 2: Optimization performance of TNT, KFAC, Shampoo, Adam, and SGD-m on two autoen- coder problems We ﬁrst compared the optimization performance of each algorithm on two autoencoder problems [21] with datasets MNIST [27] and FACES3, which were also used in [32, 34, 5, 15] as benchmarks to compare different algorithms. For each algorithm, we conducted a grid search on the learning rate and damping value based on the criteria of minimal training loss. We set the Fisher matrix update frequency T1 “1 and inverse update frequency T2 “20 for all of the second-order methods. Details of our experiment settings are listed in Section D.2 in the Appendix. From Figure 2, it is clear that TNT outperformed SGD with momentum and Adam, both in terms of per-epoch progress and process time. Moreover, TNT performed (at least) as well as KFAC and Shampoo, with a particularly strong performance on the FACES dataset. We repeated these experiments using a grid search on more hyper-parameters, and obtained results (see Figure 6 in Sec D.5) that further support our observations based on Figure 2. 2In [ 3] it is shown that replacing the SVD operation by a coupled Schur-Newton method saves time for matrices of size greater than 1000 ˆ1000. In our experiments, we used the coupled Newton method implementation of Shampoo. 3https://cs.nyu.edu/~roweis/data.html 86.2 Generalization: Convolutional Neural Networks 0 50 100 150 200 epoch 10 2 10 1 100 train loss 0 2000 4000 process time (second) 10 2 10 1 100 train loss 0 50 100 150 200 epoch 10 1 100 val error 0 2000 4000 process time (second) 10 1 100 val error TNT KFAC Shampoo Adam SGD-m a) CIFAR-10, ResNet-32 0 50 100 150 200 epoch 10 2 10 1 100 train loss 0 5000 10000 process time (second) 10 2 10 1 100 train loss 0 50 100 150 200 epoch 100 3 × 10 1 4 × 10 1 6 × 10 1 val error 0 5000 10000 process time (second) 100 3 × 10 1 4 × 10 1 6 × 10 1 val error TNT KFAC Shampoo Adam SGD-m b) CIFAR-100, VGG16 Figure 3: Generalization ability of TNT, KFAC, Shampoo, Adam, and SGD-m on two CNN models. Upper row depicts the training loss whereas lower row depicts the validation classiﬁcation error. We then compared the generalization ability of each algorithm on two CNN models, namely, ResNet32 [19] (with CIFAR10 dataset [24]) and VGG16 [42] (with CIFAR100 dataset [24]). The ﬁrst-order methods were run for 200 epochs during which the learning rate was decayed by a factor of 0.1 every 60 epochs, whereas the second-order methods were run for 100 epochs during which the learning rate was decayed by a factor of 0.1 every 40 epochs; (these settings are the same as in [45]). Moreover, as indicated in [29, 45], weight decay, different from the L2 regularization added to the loss function, helps improve generalization across different optimizers. Thus, for each algorithm, we conducted a grid search on the initial learning rate and the weight decay factor based on the criteria of maximal validation classiﬁcation accuracy. The damping parameter was set to 1e-8 for Adam (following common practice), and 0.03 for KFAC4. For TNT and Shampoo, we set ϵ“0.01. We set T1 “10 and T2 “ 100 for the three second-order methods (same as in [ 45]). Details of our experiment settings and a further discussion of the choice of damping hyper-parameters can be found in Section D.3 in the Appendix. The results in Figure 3 indicate that, with a proper learning rate and weight decay factor, second-order methods and Adam exhibit roughly the same generalization performance as SGD with momentum, which corroborate the results in [29, 45]. In particular, TNT has a similar (and sometimes better) generalization performance than the other methods. For example, comparing TNT with SGD-m, TNT (SGD-m) achieves 93.08% (93.06%) validation accuracy with ResNet32 on CIFAR10 and 73.33% (73.43%) validation accuracy with VGG16 on CIFAR-100, after 100 (200) epochs (see Table 3 in the Appendix for the accuracy achieved by the other algorithms). Moreover, in terms of process time, TNT is roughly twice (equally) as fast as SGD with momentum on ResNet32/CIFAR10 in Figure 3a (on VGG16 on CIFAR-100 in Figure 3b). This illustrates the fact that TNT usually requires only moderately more computational effort per-iteration but fewer iterations to converge than ﬁrst-order methods. Also, as shown on the VGG16 model, KFAC seems to be much slower than TNT and Shampoo on larger models. This is because the most recent version of KFAC, which we implemented, uses SVD (i.e., eigenvalue decomposition) to compute inverse matrices (see Section D.1.2 in the Appendix for a discussion of this). In contrast, TNT does not need to use SVD, and the most recent version of Shampoo replaces SVD with a coupled Newton method in [3]. We also compared TNT with a variant of it that uses the empirical rather than the true Fisher as the preconditioning matrix. The results of this comparison, which are presented in Section D.4 in the Appendix, suggest that it is preferable to use Fisher rather than empirical Fisher as pre-conditioning matrices in TNT. 4The value of 0.03 is suggested in https://github.com/alecwangcq/KFAC-Pytorch, a github repo by the authors of [45]. 97 Conclusion and Further Discussions In this paper, we proposed a new second-order method, and in particular, an approximate natural gradient method TNT, for training deep learning models. By approximating the Fisher matrix using the structure imposed by the tensor normal distribution, TNT only requires mild memory and computational overhead compared with ﬁrst-order methods. Our experiments on various deep learning models and datasets, demonstrate that TNT provides comparable and sometimes better results than the state-of-the-art (SOTA) methods, both from the optimization and generalization perspectives. Due to space and computational resource constraints, we did not run experiments on even larger models such as ImageNet and advanced models for NLP tasks. However, the results in this paper already show very strong evidence of the potential of the TNT method. We also did not explore extending our method to a distributed setting, which has been shown to be a promising direction for second-order methods such as KFAC and Shampoo [4, 3]. Since TNT already performs very well on a single machine, we expect that it will continue to do so in a distributed setting. These issues will be addressed in future research. We did not compare TNT with the SOTA Kronecker-based quasi-Newton methods [15, 38], since they are not as closely related to TNT as are Shampoo and KFAC. Their performance relative to TNT can be inferred from the comparisons here combined with those reported in [15, 38, 3]. As a ﬁnal note 5, the preconditioning matrices of TNT (as well as those of Shampoo) are derived from the speciﬁc shape of the (tensor) parameters of the particular deep learning model that is being trained. One can, of course, reshape these parameters, e.g., by ﬂattening the tensors into vectors, which gives rise to very different preconditioning matrices. The method proposed in this paper can be applied to any deep learning or machine learning model. If the model and/or data has a ﬂawed design or contains bias, this could potentially have negative societal impacts. However, this possibility is beyond the scope of the work presented in this paper. 5We thank the program chair for pointing this out. 10Acknowledgments and Disclosure of Funding We would like to thank the anonymous reviewers for their very helpful comments and suggestions. The research efforts of D. Goldfarb and Y . Ren on this paper were supported in part by NSF Grant IIS-1838061. We acknowledge computing resources from Columbia University’s Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893- 01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. References [1] S.-I. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning for multilayer perceptrons. Neural computation, 12(6):1399–1409, 2000. [2] E. Amid, R. Anil, and M. K. Warmuth. Locoprop: Enhancing backprop via local loss optimiza- tion. arXiv preprint arXiv:2106.06199, 2021. [3] R. Anil, V . Gupta, T. Koren, K. Regan, and Y . Singer. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2021. [4] J. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using Kronecker- factored approximations. 2016. [5] A. Botev, H. Ritter, and D. Barber. Practical Gauss-Newton optimisation for deep learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 557–565. JMLR. org, 2017. [6] C. G. Broyden. The convergence of a class of double-rank minimization algorithms 1. general considerations. IMA Journal of Applied Mathematics, 6(1):76–90, 1970. [7] R. H. Byrd, S. L. Hansen, J. Nocedal, and Y . Singer. A stochastic quasi-Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016. [8] D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl. On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446, 2019. [9] A. P. Dawid. Some matrix-variate distribution theory: notational considerations and a Bayesian application. Biometrika, 68(1):265–274, 1981. [10] B. S. Dees and D. P. Mandic. A statistically identiﬁable model for tensor-valued Gaussian random variables. arXiv preprint arXiv:1911.02915, 2019. [11] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011. [12] P. Dutilleul. The mle algorithm for the matrix normal distribution. Journal of statistical computation and simulation, 64(2):105–123, 1999. [13] R. Fletcher. A new approach to variable metric algorithms. The computer journal , 13(3): 317–322, 1970. [14] D. Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26, 1970. [15] D. Goldfarb, Y . Ren, and A. Bahamou. Practical quasi-Newton methods for training deep neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, ed- itors, Advances in Neural Information Processing Systems , volume 33, pages 2386–2396. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf. [16] R. Gower, D. Goldfarb, and P. Richtárik. Stochastic block BFGS: Squeezing more curvature out of data. In International Conference on Machine Learning, pages 1869–1878, 2016. [17] R. Grosse and J. Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573–582, 2016. [18] V . Gupta, T. Koren, and Y . Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pages 1842–1850. PMLR, 2018. 11[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016. [20] G. Hinton, N. Srivastava, and K. Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8), 2012. [21] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504–507, 2006. [22] M. E. Khan and H. Rue. The Bayesian learning rule. arXiv preprint arXiv:2107.04562, 2021. [23] D. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2014. [24] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. [26] F. Kunstner, P. Hennig, and L. Balles. Limitations of the empirical Fisher approximation for natural gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/46a558d97954d0692411c861cf78ef79-Paper.pdf. [27] Y . LeCun, C. Cortes, and C. Burges. MNIST handwritten digit database.ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [28] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528, 1989. [29] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Con- ference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. [30] H. Lu, K. N. Plataniotis, and A. Venetsanopoulos.Multilinear subspace learning: dimensionality reduction of multidimensional data. CRC press, 2013. [31] A. M. Manceur and P. Dutilleul. Maximum likelihood estimation for the tensor normal dis- tribution: Algorithm, minimum sample size, and empirical bias and dispersion. Journal of Computational and Applied Mathematics, 239:37–49, 2013. [32] J. Martens. Deep learning via Hessian-free optimization. In ICML, volume 27, pages 735–742, 2010. [33] J. Martens. New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014. [34] J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408–2417, 2015. [35] J. Martens, J. Ba, and M. Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=HyMTkQZAb. [36] A. Mokhtari and A. Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE Transactions on Signal Processing, 62(23):6089–6104, 2014. [37] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URLhttp://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf. [38] Y . Ren and D. Goldfarb. Kronecker-factored quasi-Newton methods for convolutional neural networks. arXiv preprint arXiv:2102.06737, 2021. [39] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951. 12[40] R. M. Schmidt, F. Schneider, and P. Hennig. Descending through a crowded valley- benchmarking deep learning optimizers. In International Conference on Machine Learning, pages 9367–9376. PMLR, 2021. [41] D. F. Shanno. Conditioning of quasi-Newton methods for function minimization. Mathematics of computation, 24(111):647–656, 1970. [42] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [43] M. Singull, M. R. Ahmad, and D. von Rosen. More on the Kronecker structured covariance matrix. Communications in Statistics-Theory and Methods, 41(13-14):2512–2523, 2012. [44] X. Wang, S. Ma, D. Goldfarb, and W. Liu. Stochastic quasi-Newton methods for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017. [45] G. Zhang, C. Wang, B. Xu, and R. Grosse. Three mechanisms of weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=B1lz-3Rct7. 13A Some Tensor Deﬁnitions and Properties We present in this section fairly standard notation and deﬁnitions regarding tensors, e.g., see [18] and Chapter 3 of [30], that we use throughout the paper. Let APRd1ˆ¨¨¨ˆdk denote a tensor of order k. • slices of Aalong its i-th dimension: for i “1,...,k and j “1,...,d i, the j-th slice of A along its i-th dimension, Ai j denotes the d1 ˆ¨¨¨ˆ di´1 ˆdi`1 ˆ¨¨¨ˆ dk tensor of order k´1, composed from all of the entries of Awhose ith index is j. • vectorization of A: denoted as vecpAq, is deﬁned recursively as vecpAq“ ¨ ˚˝ vecpA1 1q ... vecpA1 d1 q ˛ ‹‚, where for the base case, in which Ais one-dimensional tensor (i.e., a vector), vecpAq“ A. Note that when Ais a matrix, this corresponds to the row-major vectorization of A. • matricization of A: denoted as matipAq, for i“1,...,k , is deﬁned as matipAq“ ¨ ˚˝ vecpAi 1qJ ... vecpAi diqJ ˛ ‹‚. Note that vecpAq“ vecpmat1pAqq. • contraction of Awith itself along all but the ith dimension: denoted as Apiq, is deﬁned as matipAqmatipAqJ. • mode-iproduct of Aby a matrix U PRd1 iˆdi: the operation is denoted as ˆi. Let B “ AˆiU PRd1ˆ¨¨¨ˆdi´1ˆd1 iˆdi`1ˆ¨¨¨ˆdk denote the resulting tensor. Bj1,...,ji´1,j1 i,ji`1,...,jk “ř ji Aj1,...,jkUj1 i,ji. Note that in the matrix case (k“2), Aˆ1 U “UA, Aˆ2 U “AUJ. Lemma 3. Let X PRmˆn, APRmˆm, B PRnˆn. Then, we have pAbBJqvecpXq“ vecpAXBq. Note that the above lemma is slightly different from the most common version of it, which uses a column-major vectorization of the matrix X. Proposition 1. Let GPRd1ˆ¨¨¨ˆdk and Ui PRdiˆdi for i“1,...,k . Then, we have ` bk i“1Ui ˘ vecpGq“ vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq. (6) Proof of Proposition 1: Proof. Our proof, which is largely inspired by the one in [18], is by induction on k. When k“1, it is easy to see that (6) holds by the deﬁnition of the mode-iproduct. When k“2, (6) follows from Lemma 3. Now assume that (6) holds for 1,2,...,k ´1. For k, we let H “bk i“2Ui By the induction hypothesis, mat1pGqHJ “ ` Hmat1pGqJ˘J “ ` H `vecpG1 1q ¨¨¨ vecpG1 d1 q˘˘J (7) “ `HvecpG1 1q ¨¨¨ HvecpG1 d1 q˘J (8) “ `vecpG1 1 ˆ1 U2 ¨¨¨ˆ k´1 Ukq ¨¨¨ vecpG1 d1 ˆ1 U2 ¨¨¨ˆ k´1 Ukq˘J (9) “ ¨ ˚˝ vecpG1 1 ˆ1 U2 ¨¨¨ˆ k´1 UkqJ ... vecpG1 d1 ˆ1 U2 ¨¨¨ˆ k´1 UkqJ ˛ ‹‚“mat1pGˆ2 U2 ¨¨¨ˆ k Ukq (10) 14By Lemma 3 and (10), ` bk i“1Ui ˘ vecpGq“p U1 bHqvecpmat1pGqq“ vecpU1mat1pGqHJq “vecpU1mat1pGˆ2 U2 ¨¨¨ˆ k Ukqq “vecpmat1pGˆ2 U2 ¨¨¨ˆ k Uk ˆ1 U1qq “vecpGˆ2 U2 ¨¨¨ˆ k Uk ˆ1 U1q “vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq, where the third from last equality comes from the fact that BmatipAq“ matipAˆi Bq, and the last equality comes from the fact that mode-iproducts are commutative. B Proofs of Lemmas and Theorem 1 Algorithm 2 Idealized Version of TNT Require: Given θ1 PRn, batch sizes tmtutě1, step sizes tαtutě1, and damping value ϵą0 1: for t“1,2,... do 2: Sample mini-batch of size mt: Mt “tξt,i,i “1,...,m tu 3: Calculate y∇Lt “ 1 mt ř ξt,iPMt ∇lpθt,ξt,iq 4: Compute ˜Ui (i “1,...,k ) by formula (5), using the true values of Ex„Qx,y„prGpiqs(i “ 1,...,k ) at the current parameter θt. 5: Compute pt “vec ´ y∇Lt ˆ1 p˜U1 `ϵIq´1 ˆ2 ¨¨¨ˆ k p˜Uk `ϵIq´1 ¯ 6: Calculate θt`1 “θt ´αtpt 7: end for Algorithm 2 describes an idealized version of TNT, whose convergence is veriﬁed by the proofs of Lemmas 1 and 2, and Theorem 1 below. Lemma 1. }Ex„Qx,y„prGpiqs}ď ´ 1 di śk i1“1 di1 ¯ }Ex„Qx,y„prvecpGqvecpGqJs}, @i“1,...,k. Proof of Lemma 1: Proof. Let X P Rmˆn be a random matrix, and xi P Rm denote its ith column ( i “ 1,...,n ). Because vecpXqis a vector containing all the elements of all the xi’s,xixJ i is a square submatrix of vecpXqvecpXqJ. Hence, }ErxixJ i s}ď} ErvecpXqvecpXqJs}, and we have that }ErXXJs}“} Er nÿ i“1 xixJ i s}“} nÿ i“1 ErxixJ i s}ď nÿ i“1 }ErxixJ i s}ď n}ErvecpXqvecpXqJs}. Letting X “matipGqP Rdiˆpd1¨¨¨di´1di`1¨¨¨dkq, it then follows that }Ex„Qx,y„prGpiqs}ďp d1 ¨¨¨ di´1di`1 ¨¨¨ dkq}ErvecpmatipGqqvecpmatipGqqJs} “p 1 di kź i1“1 d1 iq}ErvecpGqvecpGqJs}. Lemma 2. Suppose Assumption 3 holds. Let FTNT :“p ˜U1 `ϵIqb¨¨¨bp ˜Uk `ϵIq, where the ˜Ui’s are deﬁned in (5). Then, the norm of FTNT is bounded both above and below. Proof of Lemma 2: 15Proof. It is clear that ||FTNT||“ śk i“1 ||˜Ui `ϵI||ě ϵk. On the other hand, for i“1,...,k , if we denote the eigenvalues of ErGpiqsby λ1 ď¨¨¨ď λdi, we have from (5) that }˜Ui}“ }ErGpiqs} ´ trpErGpiqsqś jdj ¯pk´1q{kś j‰idj “ λdi ´λ1`¨¨¨`λdiś jdj ¯pk´1q{kś j‰idj ď λdi ´ λdiś jdj ¯pk´1q{kś j‰idj “ diλ1{k di pś jdjq1{k “ di}ErGpiqs}1{k pś jdjq1{k . Thus, since ||FTNT||“ śk i“1 ||˜Ui `ϵI||“ śk i“1p||˜Ui||` ϵq, by the above and Lemma 1, ||FTNT||ď kź i“1 ˜ di}ErGpiqs}1{k pś jdjq1{k `ϵ ¸ ď kź i“1 ´ d1´1{k i ||ErvecpGqvecpGqJs||1{k `ϵ ¯ . Then, by Assumption 3, we have that ||FTNT||is bounded above. Proof of Theorem 1: Proof. The proof of Theorem 1 follows from Theorem 2.8 in [44]. Clearly, Algorithm 2 falls under the scope of the stochastic quasi-Newton (SQN) method in [44]. In particular, by Proposition 1, the pre-conditioning matrix H “F´1 TNT. Moreover, to apply Theorem 2.8 in [44], we need to show that AS.1 - AS.4 in [44] hold. First, AS.1 and AS.2 in [44] are the same as Assumption 1 and Assumption 2, respectively in Section 4 in our paper. Second, by Lemma 2, since ||FTNT||is both upper and lower bounded, so is ||F´1 TNT||. Hence, AS.3 in [44] is ensured. Finally, Algorithm 2 itself ensures AS.4 in [44] holds. Hence, by Theorem 2.8 of [44], the result is guaranteed. C Pseudo-code for TNT In Algorithm 3, we present a detailed pseudo-code for our actual implementation of TNT. The highlighted parts, i.e., Lines 7, 15 and 16, indicate where TNT differs signiﬁcantly from Shampoo. D Details of the Experiments In our implementations of the algorithms that we compared to TNT, we included in all of the techniques like weight decay and momentum, so that our numerical experiments would provide a FAIR comparison. Consequently, we did not include some special techniques that have been incorporated in some of the algorithms as described in previously published papers, since to keep the comparisons fair, we would have had to incorporate such techniques in all of the algorithms (see Section D.1.1 for more details). D.1 Competing Algorithms In SGD with momentum, we updated the momentum of the gradientm“µ¨m`gat every iteration, where gdenotes the minibatch gradient at current iteration. The gradient momentum is also used in the second-order methods, in our implementations. For Adam, we follow exactly the algorithm in [23] with β1 “0.9 and β2 “0.999. In particular, we follow the approach in [23] in estimating the momentum of gradient by m“β1 ¨m`p1 ´β1q¨ g. The role of β1 and β2 is similar to that of µand βin Algorithm 3 and Algorithm 4, as we will describe below. In the experiments on CNNs, we use weight decay (same as in Algorithms 3 and 4) on SGD and Adam, similar to SGDW and AdamW in [29] (for further details, see Section D.3). 16Algorithm 3 Tensor-Normal Training Require: Given batch size m, learning rate tαtutě1, weight decay factor γ, damping value ϵ, statistics update frequency T1, inverse update frequency T2 1: µ“0.9, β “0.9 2: Initialize yGpiq l “ ErGpiq l s(l “ 1,..,k , i “ 1,...,k l) by iterating through the whole dataset, {∇WlL “0 (l“1,...,L ) 3: for t“1,2,... do 4: Sample mini-batch Mt of size m 5: Perform a forward-backward pass over Mt to compute the mini-batch gradient ∇L 6: if t”0 pmod T1qthen 7: Perform another backward pass over Mt with ysampled from the predictive distribution to compute Gl “DWl averaged across Mt (l“1,...,L ) 8: end if 9: for l“1,...L do 10: {∇WlL “µ{∇WlL `∇WlL 11: if t”0 pmod T1qthen 12: Update yGpiq l “βyGpiq l `p1 ´βqGl piq for i“1,...,k l 13: end if 14: if t”0 pmod T2qthen 15: Determine ˜Uplq 1 ,..., ˜Uplq kl from yGp1q l ,..., zGpklq l by (5) 16: Recompute p˜Uplq 1 `ϵIq´1,..., p˜Uplq kl `ϵIq´1 17: end if 18: pl “ {∇WlL ˆ1 p˜Uplq 1 `ϵIq´1 ˆ2 ¨¨¨ˆ k p˜Uplq k `ϵIq´1 19: pl “pl `γWl 20: Wl “Wl ´αt ¨pl. 21: end for 22: end for 17D.1.1 Shampoo Algorithm 4 Shampoo Require: Given batch size m, learning rate tαtutě1, weight decay factor γ, damping value ϵ, statistics update frequency T1, inverse update frequency T2 1: µ“0.9, β “0.9 2: Initialize yGpiq l “ ErGpiq l s(l “ 1,..,k , i “ 1,...,k l) by iterating through the whole dataset, {∇WlL “0 (l“1,...,L ) 3: for t“1,2,... do 4: Sample mini-batch Mt of size m 5: Perform a forward-backward pass over the current mini-batch Mt to compute the minibatch gradient ∇L 6: for l“1,...L do 7: {∇WlL “µ{∇WlL `∇WlL 8: if t”0 pmod T1qthen 9: Update yGpiq l “βyGpiq l `p1 ´βqGl piq for i“1,...,k l where Gl “∇WlL 10: end if 11: if t”0 pmod T2qthen 12: Recompute ˆyGp1q l `ϵI ˙´1{2kl ,..., ˆzGpklq l `ϵI ˙´1{2kl with the coupled Newton method 13: end if 14: pl “ {∇WlL ˆ1 ˆyGp1q l `ϵI ˙´1{2kl ˆ2 ¨¨¨ˆ k ˆzGpklq l `ϵI ˙´1{2kl 15: pl “pl `γWl 16: Wl “Wl ´αt ¨pl 17: end for 18: end for In Algorithm 4, we present our implementation of Shampoo, which mostly follows the description of it given in [18]. Several major improvements are also included, following the suggestions in [3], including: 1. In Line 9 of Algorithm 4, a moving average is used to update the estimates yGpiq l , as is done in our implementations of TNT and KFAC. This approach is also used in Adam, whereas summing the Gpiq l ’s over all iterations, as in [18], is analogous to what is done in AdaGrad, upon which Shampoo is based. 2. In Line 12 of Algorithm 4, we use a coupled Newton method to compute inverse roots of the matrices (as proposed in [3]), rather than using SVD. The coupled Newton approach has been shown to be much faster than SVD, and also preserves relatively good accuracy in terms of computing inverse roots. The coupled Newton method performs reasonably well (without tuning) using a max iteration number of 100 and an error tolerance of 1e-6. Some other modiﬁcations proposed in [3] are not included in our implementation of Shampoo, mainly because these modiﬁcations can also be applied to TNT, and including them only in Shampoo would introduce other confounding factors. (i) We did not explore multiplying the damping term in the pre-conditioner by the maximum eigenvalue λmax of the contraction matrix. Moreover, this modiﬁcation is somewhat problematic, since, if the model contains any variables that always have a zero gradient (e.g. the bias in a convolutional layer that is followed by a BN layer), the optimizer would become unstable because the pre-conditioner of the zero-gradient variables would be the zero matrix, (note that in this case λmax “0). (ii) We did not explore the diagonal variant of Shampoo, as we mainly focused on the comparison between different pre-conditioning matrices, and TNT can also be extended to a diagonal 18version; similarly, we did not explore the variant proposed in [3] that divides large tensors into small blocks. D.1.2 KFAC In this subsection, we brieﬂy describe our implementation of KFAC. The preconditioning matrices that we used for linear layers and convolutional layers are precisely as those described in [34] and [17], respectively. For the parameters in the BN layers, we used the gradient direction, exactly as in https://github.com/alecwangcq/KFAC-Pytorch. As in our implementations of TNT and Shampoo, and as suggested in [17], we did a warm start to estimate the pre-conditioning KFAC matrices in an initialization step that iterated through the whole data set, and adopted a moving average scheme to update them with β “0.9 afterwards. In inverting the KFAC matrices and computing the updating direction, • for the autoencoder experiments, we inverted the damped KFAC matrices and used them to compute the updating direction, where the damping factors for both Aand Gwere set to be? λ, where λis the overall damping value;6 • for the CNN experiments, we followed the SVD (i.e. eigenvalue decomposition) implemen- tation suggested in https://github.com/alecwangcq/KFAC-Pytorch, which, as we veriﬁed, performs better than splitting the damping value and inverting the damped KFAC matrices (as suggested in [34, 17]). Further, we implemented weight decay exactly as in TNT (Algorithm 3) and Shampoo (Algorithm 4). D.2 Experiment Settings for the Autoencoder Problems Table 2: Hyper-parameters (learning rate, damping) used to produce Figure 2 Name MNIST FACES TNT (1e-4, 0.1) (1e-6, 0.003) KFAC (0.003, 0.3) (0.1, 10) Shampoo (3e-4, 3e-4) (3e-4, 3e-4) Adam (1e-4, 1e-4) (1e-4, 1e-4) SGD-m (0.003, -) (0.001, -) MNIST has 60,000 training data, whereas FACES7 has 103,500 training data. For all algorithms, we used a batch size of 1,000 at every iteration. The autoencoder model used for MNIST has layer widths 784-1000-500-250-30-250-500-1000-784 with ReLU activation functions, except for the middle layer which uses a linear function and the last layer which uses a sigmoid function. The autoencoder model used for FACES has layer widths 625-2000-1000-500-30-500-1000-2000-625 with ReLU activation functions, except for the middle and last layers which use linear functions. We used binary entropy loss for MNIST and squared error loss for FACES. The above settings largely mimic the settings in [32, 34, 5, 15]. Since we primarily focused on optimization rather than generalization in these tasks, we did not includeL2 regularization or weight decay. In order to obtain Figure 2, we ﬁrst conducted a grid search on the learning rate (lr) and damping value based on the criteria of minimizing the training loss. The ranges of the grid searches used for the algorithms in our tests were: • SGD-m: – lr: 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 6Note that there are more sophisticated ways of splitting the damping value, such as one that makes use of the norms of the undamped matrices, to enforce that the two matrices have the same norm. See [34] and [17] for more on this. 7Downloadable at www.cs.toronto.edu/~jmartens/newfaces_rot_single.mat. 19– damping: not applicable • Adam: – lr: 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01 – damping (i.e. the ϵhyperparameter of Adam): 1e-8, 1e-4, 1e-2 • Shampoo: – lr: 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003 – damping (i.e. ϵin Algorithm 4): 1e-4, 3e-4, 0.001, 0.003, 0.01 • TNT: – lr: 1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001 – damping (i.e. ϵin Algorithm 3): 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 • KFAC: – lr: 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 – damping: 0.01, 0.03, 0.1, 0.3, 1, 3, 10 The best hyper-parameter values determined by our grid searches are listed in Table 2. D.3 Experiment Settings for the CNN Problems Table 3: Hyper-parameters ( initial learning rate, weight decay factor) used to produce Figure 3 and the average validation accuracy across 5 runs with different random seeds shown in Figure 3 Name CIFAR-10 + ResNet32 CIFAR-100 + VGG16 TNT (1e-4, 10) Ñ93.08% (3e-5, 10) Ñ73.33% KFAC (0.01, 0.1) Ñ92.85% (3e-4, 0.1) Ñ74.33% Shampoo (0.01, 0.1) Ñ92.63% (0.003, 0.1) Ñ72.82% Adam (0.003, 0.1) Ñ92.92% (3e-5, 10) Ñ72.27% SGD-m (0.03, 0.01) Ñ93.06% (0.03, 0.01) Ñ73.44% Both CIFAR-10 and CIFAR-100 have 50,000 training data and 10,000 testing data (used as the validation set in our experiments). For all algorithms, we used a batch size of 128 at every iteration. In training, we applied data augmentation as described in [25], including random horizontal ﬂip and random crop. The ResNet32 model refers to the one in Table 6 of [19], whereas the VGG16 model refers to model D of [42], with the modiﬁcation that batch normalization layers were added after all of the convolutional layers in the model. It is worth noting that, in TNT and Shampoo, for the weight tensor in the convolutional layers, instead of viewing it as a 4-way tensor, we view it as a 3-way tensor, where the size of its 3 ways (dimensions) corresponds to the size of the ﬁlter, the number of input channel, and the number of the output channel, respectively. As a result, the preconditioning matrices of TNT and Shampoo will come from the Kronecker product of three matrices, rather than four matrices. Weight decay, which is related to, but not the same asL2 regularization added to the loss function, has been shown to help improve generalization performance across different optimizers [29, 45]. In our experiments, we adopted weight decay for all algorithms. The use of weight decay for TNT and Shampoo is described in Algorithm 3 and Algorithm 4, respectively, and is similarly applied to KFAC. Also note that weight decay is equivalent to L2 regularization for pure SGD (without momentum). However, the equivalence does not hold for SGD with momentum. For the sake of a fair comparison, we also applied weight decay for SGD-m. For TNT and Shampoo, we set ϵ “0.01. We also tried values around 0.01 and the results were not sensitive to the value of ϵ; hence, ϵcan be set to 0.01 as a default value. For KFAC, we set the overall damping value to be 0.03, as suggested in the implementation in https://github.com/ alecwangcq/KFAC-Pytorch. We also tried values around 0.03 for KFAC and conﬁrmed that 0.03 is a good default value. 20In order to obtain Figure 3, we ﬁrst conducted a grid search on the initial learning rate (lr) and weight decay (wd) factor based on the criteria of maximizing the classiﬁcation accuracy on the validation set. The range of the grid searches for the algorithms in our tests were: • SGD-m: – lr: 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 – wd: 0.001, 0.01, 0.1, 1 • Adam: – lr: 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 – wd: 1e-4, 0.001, 0.01, 0.1, 1, 10, 100 • Shampoo: – lr: 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1 – wd: 0.01, 0.1, 1, 10 • TNT: – lr: 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001 – wd: 1, 10, 100 • KFAC: – lr: 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 – wd: 0.01, 0.1, 1 The best hyper-parameter values, and the validation classiﬁcation accuracy obtained using them, are listed in Table 3. D.4 A Comparison between TNT and TNT-EF 0 100 200 300 epoch 102 training loss 0 200 400 process time (second) 102 training loss TNT TNT-EF a) MNIST autoencoder 0 200 400 epoch 101 102 training loss 0 1000 2000 process time (second) 101 102 training loss TNT TNT-EF b) FACES autoencoder Figure 4: Optimization performance comparison of the TNT and TNT-EF algorithms on two autoencoder problems. 210 50 100 epoch 10 1 100 training loss 0 1000 2000 process time (second) 10 1 100 training loss 0 50 100 epoch 10 1 100 testing error 0 1000 2000 process time (second) 10 1 100 testing error TNT TNT-EF a) CIFAR-10, ResNet-32 0 50 100 epoch 10 2 10 1 100 training loss 0 2000 4000 process time (second) 10 2 10 1 100 training loss 0 50 100 epoch 100 3 × 10 1 4 × 10 1 6 × 10 1 testing error 0 2000 4000 process time (second) 100 3 × 10 1 4 × 10 1 6 × 10 1 testing error TNT TNT-EF b) CIFAR-100, VGG16 Figure 5: Generalization ability comparison of the TNT and TNT-EF algorithms on two CNN models. The upper row depicts the training loss, whereas the lower row depicts the validation classiﬁcation error. Table 4: Hyper-parameters (learning rate, damping) used to produce Figure 4 Name MNIST FACES TNT-EF (3e-6, 0.01) (3e-6, 0.01) Table 5: Hyper-parameters ( initial learning rate, weight decay factor) used to produce Figure 5 Name CIFAR-10 + ResNet32 CIFAR-100 + VGG16 TNT-EF (1e-4, 10) Ñ93.62% (3e-6, 100) Ñ72.85% In this subsection, we compare our proposed TNT algorithm against a variant of it, TNT-EF, which uses an empirical Fisher (EF) preconditioning matrix in place of the true Fisher matrix. In other words, TNT-EF does everything speciﬁed in Algorithm 3, except that it does not perform the extra backward pass in Line 7 of Algorithm 3. When updating the matricesyGpiq l , TNT-EF uses the empirical minibatch gradient, rather than the sampling-based minibatch gradient, i.e. the one coming from the extra backward pass. We conducted a hyper-parameter grid search for TNT-EF, following the same procedure as the one that was used for TNT, whose performance was plotted in Figures 2 and 3. The best values for the TNT-EF hyper-parameters that we obtained are listed in Tables 4 and 5. We then plotted in Figures 4 and 5, the performance of TNT-EF, along with that of TNT, using for it the hyper-parameters given in Tables 2 and 3. As shown in Figures 4 and 5, TNT performed at least as well as TNT-EF, on the MNIST and CIFAR-10 problems, and performed somewhat better on the FACES and CIFAR-100 problems, which conﬁrms the widely held opinion that the Fisher matrix usually carries more valuable curvature information than the empirical Fisher metrix. 22D.5 More on Hyper-parameter Tuning 0 200 400 600 epoch 102 training loss 0 200 400 process time (second) 102 training loss TNT KFAC Shampoo Adam SGD-m a) MNIST autoencoder 0 500 1000 epoch 101 102 training loss 0 1000 2000 process time (second) 101 102 training loss TNT KFAC Shampoo Adam SGD-m b) FACES autoencoder Figure 6: Optimization performance of TNT, KFAC, Shampoo, Adam, and SGD-m on two autoen- coder problems, with more extensive tuning Table 6: Hyper-parameter values used to produce Figure 6 Problem Algorithm (learning rate, damping, µ, β) MNIST TNT (1e-4, 0.1, 0.9, 0.9) MNIST KFAC (3e-5, 0.01, 0.999, 0.999) MNIST Shampoo (1e-4, 3e-4, 0.99, 0.99) MNIST Adam (1e-4, 1e-4, 0.99, 0.99) MNIST SGD-m (0.001, -, 0.99, -) FACES TNT (1e-6, 0.003, 0.9, 0.9) FACES KFAC (0.01, 3, 0.99, 0.99) FACES Shampoo (1e-4, 3e-4, 0.99, 0.999) FACES Adam (1e-4, 1e-4, 0.9, 0.9) FACES SGD-m (0.001, -, 0.9, -) In this subsection, we expand on the experiments whose results are plotted in Figure 2, by in- corporating the tuning of more hyper-parameters. To be more speciﬁc, we tuned the following hyper-parameters jointly: 1. SGD-m: learning rate and µ; 2. all other algorithms 8: learning rate, damping, µ, and β. The searching range for learning rate and damping is the same as in Sec D.2, whereas the searching range for µand βwere set to be t0.9,0.99,0.999u. The obtained values for the hyper-parameters are listed in Table 6. Figure 6 depicts the performance of different algorithms with hyper-parameters obtained from the aforementioned more extensive tuning process. Comparing the performance of different algorithms in Figure 6, we can see that the observations we made from Figure 2 still hold to a large extent. Moreover, with extensive tuning, second-order methods seem to perform similarly with each other, and are usually better than well-tuned ﬁrst order methods on these problems. As a ﬁnal point, we would like to mention that one could also replace the constant learning rate for all of the algorithms tested with a "warm-up, then decay" schedule, which has been shown to result in good performance on these problems in [3]. Also, one could perform a more extensive tuning for the CNN problems. In particular, one could tune the initial learning rate, weight decay factor, damping, µ, and βjointly for the CNN problems. See more in [8, 40] for the importance and suggestions on hyper-parameter tuning. Moreover, see [2] for other relevant numerical results, in particular for KFAC and Shampoo. In [2], KFAC is shown to work extremely well with a higher frequency of inversion, another direction for experiments that could be explored. 8For Adam, µand βrefer to β1 and β2, respectively. 23Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 7. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 7. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 4. (b) Did you include complete proofs of all theoretical results? [Yes] See Section B in the Appendix. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] The code and instructions are included in the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 6, and Section D in the Appendix. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Section 6. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 6. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 6. (b) Did you mention the license of the assets? [Yes] The data and models used in the paper have been properly cited. Licenses can be found in the corresponding citations, if they exist. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 24
---------------------------------

Please extract all reference paper titles and return them as a list of strings.
Output:
{
    "reference_titles": [
        "Adaptive method of realizing natural gradient learning for multilayer perceptrons",
        "Locoprop: Enhancing backprop via local loss optimiza- tion",
        "Scalable second order optimization for deep learning",
        "Distributed second-order optimization using Kronecker- factored approximations",
        "Practical Gauss-Newton optimisation for deep learning",
        "The convergence of a class of double-rank minimization algorithms 1. general considerations",
        "A stochastic quasi-Newton method for large-scale optimization",
        "On empirical comparisons of optimizers for deep learning",
        "Some matrix-variate distribution theory: notational considerations and a Bayesian application",
        "A statistically identiﬁable model for tensor-valued Gaussian random variables",
        "Adaptive subgradient methods for online learning and stochastic optimization",
        "The mle algorithm for the matrix normal distribution",
        "A new approach to variable metric algorithms",
        "A family of variable-metric methods derived by variational means",
        "Practical quasi-Newton methods for training deep neural networks",
        "Stochastic block BFGS: Squeezing more curvature out of data",
        "A Kronecker-factored approximate Fisher matrix for convolution layers",
        "Shampoo: Preconditioned stochastic tensor optimization",
        "Deep residual learning for image recognition",
        "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent",
        "Reducing the dimensionality of data with neural networks",
        "The Bayesian learning rule",
        "Adam: A method for stochastic optimization",
        "Imagenet classiﬁcation with deep convolutional neural networks",
        "Limitations of the empirical Fisher approximation for natural gradient descent",
        "MNIST handwritten digit database",
        "On the limited memory BFGS method for large scale optimization",
        "Decoupled weight decay regularization",
        "Multilinear subspace learning: dimensionality reduction of multidimensional data",
        "Maximum likelihood estimation for the tensor normal dis- tribution: Algorithm, minimum sample size, and empirical bias and dispersion",
        "Deep learning via Hessian-free optimization",
        "New insights and perspectives on the natural gradient method",
        "Optimizing neural networks with Kronecker-factored approximate curvature",
        "Kronecker-factored curvature approximations for recurrent neural networks",
        "Res: Regularized stochastic BFGS algorithm",
        "Pytorch: An imperative style, high-performance deep learning library",
        "Kronecker-factored quasi-Newton methods for convolutional neural networks",
        "A stochastic approximation method",
        "Descending through a crowded valley- benchmarking deep learning optimizers",
        "Conditioning of quasi-Newton methods for function minimization",
        "Very deep convolutional networks for large-scale image recognition",
        "More on the Kronecker structured covariance matrix",
        "Stochastic quasi-Newton methods for nonconvex stochastic optimization",
        "Three mechanisms of weight decay regularization"
    ]
}
