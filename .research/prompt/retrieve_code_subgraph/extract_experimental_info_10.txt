
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
ATTA formally defines dynamic model optimization for streaming test data by actively selecting informative instances for labeling. The theoretical foundation uses learning bounds based on H∆H-distance and VC-dimension to guarantee mitigation of distribution shifts (Theorem 1 and 2). Catastrophic forgetting is addressed through selective entropy minimization, where low-entropy samples (pseudo-labeled by the frozen pre-trained source model) are included alongside actively labeled high-entropy samples to maintain source domain performance (Corollary 3 and 4). The SimATTA algorithm partitions incoming unlabeled test samples into high- and low-entropy sets. Low-entropy samples are pseudo-labeled and stored. High-entropy samples are selected using an incremental clustering technique, which employs weighted K-means to store representative 'anchors' of seen distributions, adapting to new distributions while managing a budget. The model is then fine-tuned on both actively labeled high-entropy samples and pseudo-labeled low-entropy samples, balancing their influence according to theoretical insights.

# Repository Content
File Path: ATTA/__init__.py
Content:
from .utils import config_summoner, args_parser
from ATTA.utils.register import register
from . import data, networks
File Path: ATTA/data/__init__.py
Content:
r"""
This data module includes 11 ATTA datasets and a dataloader for an organized data loading process.
"""
from .loaders import *
from .domainbed_datasets import *

File Path: ATTA/data/domainbed_datasets/__init__.py
Content:
import glob
from os.path import dirname, basename, isfile, join

modules = glob.glob(join(dirname(__file__), "*.py"))
__all__ = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]

from . import *
File Path: ATTA/data/domainbed_datasets/db_datasets.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import os

import torch
import torchvision.datasets.folder
from PIL import Image, ImageFile
from torch.utils.data import TensorDataset, Subset
from torchvision import transforms
from torchvision.datasets import MNIST, ImageFolder
from torchvision.transforms.functional import rotate
from wilds.datasets.camelyon17_dataset import Camelyon17Dataset
from wilds.datasets.fmow_dataset import FMoWDataset

from ATTA import register

ImageFile.LOAD_TRUNCATED_IMAGES = True

DATASETS = [
    # Debug
    "Debug28",
    "Debug224",
    # Small images
    "ColoredMNIST",
    "RotatedMNIST",
    # Big images
    "VLCS",
    "PACS",
    "OfficeHome",
    "TerraIncognita",
    "DomainNet",
    "SVIRO",
    # WILDS datasets
    "WILDSCamelyon",
    "WILDSFMoW",
    # ImageNet-C
    'ImageNetC'
]


def get_dataset_class(dataset_name):
    """Return the dataset class with the given name."""
    if dataset_name not in globals():
        raise NotImplementedError("Dataset not found: {}".format(dataset_name))
    return globals()[dataset_name]


def num_environments(dataset_name):
    return len(get_dataset_class(dataset_name).ENVIRONMENTS)


class MultipleDomainDataset:
    N_STEPS = 5001  # Default, subclasses may override
    CHECKPOINT_FREQ = 100  # Default, subclasses may override
    N_WORKERS = 8  # Default, subclasses may override
    ENVIRONMENTS = None  # Subclasses should override
    INPUT_SHAPE = None  # Subclasses should override

    def __getitem__(self, index):
        return self.datasets[index]

    def __len__(self):
        return len(self.datasets)


class Debug(MultipleDomainDataset):
    def __init__(self, root, test_envs, hparams):
        super().__init__()
        self.input_shape = self.INPUT_SHAPE
        self.num_classes = 2
        self.datasets = []
        for _ in [0, 1, 2]:
            self.datasets.append(
                TensorDataset(
                    torch.randn(16, *self.INPUT_SHAPE),
                    torch.randint(0, self.num_classes, (16,))
                )
            )


@register.dataset_register
class Debug28(Debug):
    INPUT_SHAPE = (3, 28, 28)
    ENVIRONMENTS = ['0', '1', '2']


@register.dataset_register
class Debug224(Debug):
    INPUT_SHAPE = (3, 224, 224)
    ENVIRONMENTS = ['0', '1', '2']


class MultipleEnvironmentMNIST(MultipleDomainDataset):
    def __init__(self, root, environments, dataset_transform, input_shape,
                 num_classes):
        super().__init__()
        if root is None:
            raise ValueError('Data directory not specified!')

        original_dataset_tr = MNIST(root, train=True, download=True)
        original_dataset_te = MNIST(root, train=False, download=True)

        original_images = torch.cat((original_dataset_tr.data,
                                     original_dataset_te.data))

        original_labels = torch.cat((original_dataset_tr.targets,
                                     original_dataset_te.targets))

        shuffle = torch.randperm(len(original_images))

        original_images = original_images[shuffle]
        original_labels = original_labels[shuffle]

        self.datasets = []

        for i in range(len(environments)):
            images = original_images[i::len(environments)]
            labels = original_labels[i::len(environments)]
            self.datasets.append(dataset_transform(images, labels, environments[i]))

        self.input_shape = input_shape
        self.num_classes = num_classes


@register.dataset_register
class ColoredMNIST(MultipleEnvironmentMNIST):
    ENVIRONMENTS = ['+90%', '+80%', '-90%']
    metric = 'Accuracy'
    task = 'Binary classification'

    def __init__(self, root, test_envs, hparams):
        super(ColoredMNIST, self).__init__(root, [0.1, 0.2, 0.9],
                                           self.color_dataset, (2, 28, 28,), 2)

        self.input_shape = (2, 28, 28,)
        self.num_classes = 2

    def color_dataset(self, images, labels, environment):
        # # Subsample 2x for computational convenience
        # images = images.reshape((-1, 28, 28))[:, ::2, ::2]
        # Assign a binary label based on the digit
        labels = (labels < 5).float()
        # Flip label with probability 0.25
        labels = self.torch_xor_(labels,
                                 self.torch_bernoulli_(0.25, len(labels)))

        # Assign a color based on the label; flip the color with probability e
        colors = self.torch_xor_(labels,
                                 self.torch_bernoulli_(environment,
                                                       len(labels)))
        images = torch.stack([images, images], dim=1)
        # Apply the color to the image by zeroing out the other color channel
        images[torch.tensor(range(len(images))), (
                                                         1 - colors).long(), :, :] *= 0

        x = images.float().div_(255.0)
        # y = labels.view(-1).long()
        y = labels.view(-1, 1).float()

        return TensorDataset(x, y)

    def torch_bernoulli_(self, p, size):
        return (torch.rand(size) < p).float()

    def torch_xor_(self, a, b):
        return (a - b).abs()


@register.dataset_register
class RotatedMNIST(MultipleEnvironmentMNIST):
    ENVIRONMENTS = ['0', '15', '30', '45', '60', '75']
    metric = 'Accuracy'
    task = 'Binary classification'

    def __init__(self, root, test_envs, hparams):
        super(RotatedMNIST, self).__init__(root, [0, 15, 30, 45, 60, 75],
                                           self.rotate_dataset, (1, 28, 28,), 10)

    def rotate_dataset(self, images, labels, angle):
        rotation = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Lambda(lambda x: rotate(x, angle, fill=(0,),
                                               interpolation=torchvision.transforms.InterpolationMode.BILINEAR)),
            transforms.ToTensor()])

        x = torch.zeros(len(images), 1, 28, 28)
        for i in range(len(images)):
            x[i] = rotation(images[i])

        # y = labels.view(-1)
        y = labels.view(-1, 1).float()

        return TensorDataset(x, y)


class MultipleEnvironmentImageFolder(MultipleDomainDataset):
    def __init__(self, root, test_envs, augment, hparams, environment_names=None):
        super().__init__()
        environments = [f.name for f in os.scandir(root) if f.is_dir()]
        environments = sorted(environments) if environment_names is None else environment_names
        print("Found environments: {}".format(environments))

        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        augment_transform = transforms.Compose([
            # transforms.Resize((224,224)),
            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),
            transforms.RandomGrayscale(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        self.datasets = []
        for i, environment in enumerate(environments):

            if augment and (i not in test_envs):
                env_transform = augment_transform
            else:
                env_transform = transform

            if 'ImageNet' in hparams.dataset.name or 'CIFAR' in hparams.dataset.name:
                path = os.path.join(root, environment, '1')
            else:
                path = os.path.join(root, environment)
            env_dataset = ImageFolder(path,
                                      transform=env_transform)

            self.datasets.append(env_dataset)

        self.input_shape = (3, 224, 224,)
        self.num_classes = len(self.datasets[-1].classes)


@register.dataset_register
class VLCS(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ["C", "L", "S", "V"]
    metric = 'Accuracy'
    task = 'Multi-label classification'

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "VLCS/")
        if not os.path.exists(self.dir):
            from .download import download_vlcs
            download_vlcs(root)
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams)



@register.dataset_register
class PACS(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ["A", "C", "P", "S"]
    metric = 'Accuracy'
    task = 'Multi-label classification'

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "PACS/")
        if not os.path.exists(self.dir):
            from .download import download_pacs
            download_pacs(root)
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams)


@register.dataset_register
class DomainNet(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 1000
    ENVIRONMENTS = ["clip", "info", "paint", "quick", "real", "sketch"]

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "domain_net/")
        if not os.path.exists(self.dir):
            from .download import download_domain_net
            download_domain_net(root)
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams)


@register.dataset_register
class OfficeHome(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ["A", "C", "P", "R"]
    metric = 'Accuracy'
    task = 'Multi-label classification'

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "office_home/")
        if not os.path.exists(self.dir):
            from .download import download_office_home
            download_office_home(root)
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams)


@register.dataset_register
class ImageNetC(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur',
                      'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',
                      'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
    metric = 'Accuracy'
    task = 'Multi-label classification'

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "..", '..', '..', "tta_datasets", 'ImageNet-C')
        if not os.path.exists(self.dir):
            raise ValueError("ImageNet-C dataset not found!")
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams, environment_names=self.ENVIRONMENTS)

@register.dataset_register
class TinyImageNetC(MultipleDomainDataset):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur',
                      'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',
                      'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
    metric = 'Accuracy'
    task = 'Multi-label classification'

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, 'Tiny-ImageNet-C')
        if not os.path.exists(self.dir):
            raise ValueError("ImageNet-C dataset not found!")
        # super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams, environment_names=self.ENVIRONMENTS)
        super().__init__()
        environments = [f.name for f in os.scandir(self.dir) if f.is_dir()]
        environments = sorted(environments) if self.ENVIRONMENTS is None else self.ENVIRONMENTS
        print("Found environments: {}".format(environments))

        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        augment_transform = transforms.Compose([
            # transforms.Resize((224,224)),
            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),
            transforms.RandomGrayscale(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        self.datasets = []
        for i, environment in enumerate(environments):

            if hparams.dataset['data_augmentation'] and (i not in test_envs):
                env_transform = augment_transform
            else:
                env_transform = transform

            if 'ImageNet' in hparams.dataset.name or 'CIFAR' in hparams.dataset.name:
                path = os.path.join(self.dir, environment, '5')
            else:
                path = os.path.join(self.dir, environment)
            env_dataset = ImageFolder(path,
                                      transform=env_transform)

            self.datasets.append(env_dataset)

        self.input_shape = (3, 224, 224,)
        self.num_classes = len(self.datasets[-1].classes)

@register.dataset_register
class CIFAR100C(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur',
                      'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',
                      'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
    metric = 'Accuracy'
    task = 'Multi-label classification'

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "..", '..', '..', "tta_datasets", 'CIFAR-100-C')
        if not os.path.exists(self.dir):
            raise ValueError("ImageNet-C dataset not found!")
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams, environment_names=self.ENVIRONMENTS)


@register.dataset_register
class TerraIncognita(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ["L100", "L38", "L43", "L46"]

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "terra_incognita/")
        super().__init__(self.dir, test_envs, hparams.dataset['data_augmentation'], hparams)


@register.dataset_register
class SVIRO(MultipleEnvironmentImageFolder):
    CHECKPOINT_FREQ = 300
    ENVIRONMENTS = ["aclass", "escape", "hilux", "i3", "lexus", "tesla", "tiguan", "tucson", "x5", "zoe"]

    def __init__(self, root, test_envs, hparams):
        self.dir = os.path.join(root, "sviro/")
        super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)


class WILDSEnvironment:
    def __init__(
            self,
            wilds_dataset,
            metadata_name,
            metadata_value,
            transform=None):
        self.name = metadata_name + "_" + str(metadata_value)

        metadata_index = wilds_dataset.metadata_fields.index(metadata_name)
        metadata_array = wilds_dataset.metadata_array
        subset_indices = torch.where(
            metadata_array[:, metadata_index] == metadata_value)[0]

        self.dataset = wilds_dataset
        self.indices = subset_indices
        self.transform = transform

    def __getitem__(self, i):
        x = self.dataset.get_input(self.indices[i])
        if type(x).__name__ != "Image":
            x = Image.fromarray(x)

        y = self.dataset.y_array[self.indices[i]]
        if self.transform is not None:
            x = self.transform(x)
        return x, y

    def __len__(self):
        return len(self.indices)


class WILDSDataset(MultipleDomainDataset):
    INPUT_SHAPE = (3, 224, 224)

    def __init__(self, dataset, metadata_name, test_envs, augment, hparams):
        super().__init__()

        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        augment_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),
            transforms.RandomGrayscale(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

        self.datasets = []

        for i, metadata_value in enumerate(
                self.metadata_values(dataset, metadata_name)):
            if augment and (i not in test_envs):
                env_transform = augment_transform
            else:
                env_transform = transform

            env_dataset = WILDSEnvironment(
                dataset, metadata_name, metadata_value, env_transform)

            self.datasets.append(env_dataset)

        self.input_shape = (3, 224, 224,)
        self.num_classes = dataset.n_classes

    def metadata_values(self, wilds_dataset, metadata_name):
        metadata_index = wilds_dataset.metadata_fields.index(metadata_name)
        metadata_vals = wilds_dataset.metadata_array[:, metadata_index]
        return sorted(list(set(metadata_vals.view(-1).tolist())))


@register.dataset_register
class WILDSCamelyon(WILDSDataset):
    ENVIRONMENTS = ["hospital_0", "hospital_1", "hospital_2", "hospital_3",
                    "hospital_4"]

    def __init__(self, root, test_envs, hparams):
        dataset = Camelyon17Dataset(root_dir=root)
        super().__init__(
            dataset, "hospital", test_envs, hparams['data_augmentation'], hparams)


@register.dataset_register
class WILDSFMoW(WILDSDataset):
    ENVIRONMENTS = ["region_0", "region_1", "region_2", "region_3",
                    "region_4", "region_5"]

    def __init__(self, root, test_envs, hparams):
        dataset = FMoWDataset(root_dir=root)
        super().__init__(
            dataset, "region", test_envs, hparams['data_augmentation'], hparams)

File Path: ATTA/data/domainbed_datasets/download.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

from torchvision.datasets import MNIST
import xml.etree.ElementTree as ET
from zipfile import ZipFile
import argparse
import tarfile
import shutil
import gdown
import uuid
import json
import os

from wilds.datasets.camelyon17_dataset import Camelyon17Dataset
from wilds.datasets.fmow_dataset import FMoWDataset


# utils #######################################################################

def stage_path(data_dir, name):
    full_path = os.path.join(data_dir, name)

    if not os.path.exists(full_path):
        os.makedirs(full_path)

    return full_path


def download_and_extract(url, dst, remove=True):
    gdown.download(url, dst, quiet=False)

    if dst.endswith(".tar.gz"):
        tar = tarfile.open(dst, "r:gz")
        tar.extractall(os.path.dirname(dst))
        tar.close()

    if dst.endswith(".tar"):
        tar = tarfile.open(dst, "r:")
        tar.extractall(os.path.dirname(dst))
        tar.close()

    if dst.endswith(".zip"):
        zf = ZipFile(dst, "r")
        zf.extractall(os.path.dirname(dst))
        zf.close()

    if remove:
        os.remove(dst)


# VLCS ########################################################################

# Slower, but builds dataset from the original sources
#
# def download_vlcs(data_dir):
#     full_path = stage_path(data_dir, "VLCS")
#
#     tmp_path = os.path.join(full_path, "tmp/")
#     if not os.path.exists(tmp_path):
#         os.makedirs(tmp_path)
#
#     with open("domainbed/misc/vlcs_files.txt", "r") as f:
#         lines = f.readlines()
#         files = [line.strip().split() for line in lines]
#
#     download_and_extract("http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar",
#                          os.path.join(tmp_path, "voc2007_trainval.tar"))
#
#     download_and_extract("https://drive.google.com/uc?id=1I8ydxaAQunz9R_qFFdBFtw6rFTUW9goz",
#                          os.path.join(tmp_path, "caltech101.tar.gz"))
#
#     download_and_extract("http://groups.csail.mit.edu/vision/Hcontext/data/sun09_hcontext.tar",
#                          os.path.join(tmp_path, "sun09_hcontext.tar"))
#
#     tar = tarfile.open(os.path.join(tmp_path, "sun09.tar"), "r:")
#     tar.extractall(tmp_path)
#     tar.close()
#
#     for src, dst in files:
#         class_folder = os.path.join(data_dir, dst)
#
#         if not os.path.exists(class_folder):
#             os.makedirs(class_folder)
#
#         dst = os.path.join(class_folder, uuid.uuid4().hex + ".jpg")
#
#         if "labelme" in src:
#             # download labelme from the web
#             gdown.download(src, dst, quiet=False)
#         else:
#             src = os.path.join(tmp_path, src)
#             shutil.copyfile(src, dst)
#
#     shutil.rmtree(tmp_path)


def download_vlcs(data_dir):
    # Original URL: http://www.eecs.qmul.ac.uk/~dl307/project_iccv2017
    full_path = stage_path(data_dir, "VLCS")

    download_and_extract("https://drive.google.com/uc?id=1skwblH1_okBwxWxmRsp9_qi15hyPpxg8",
                         os.path.join(data_dir, "VLCS.tar.gz"))


# MNIST #######################################################################

def download_mnist(data_dir):
    # Original URL: http://yann.lecun.com/exdb/mnist/
    full_path = stage_path(data_dir, "MNIST")
    MNIST(full_path, download=True)


# PACS ########################################################################

def download_pacs(data_dir):
    # Original URL: http://www.eecs.qmul.ac.uk/~dl307/project_iccv2017
    full_path = stage_path(data_dir, "PACS")

    download_and_extract("https://drive.google.com/uc?id=1JFr8f805nMUelQWWmfnJR3y4_SYoN5Pd",
                         os.path.join(data_dir, "PACS.zip"))

    os.rename(os.path.join(data_dir, "kfold"),
              full_path)


# Office-Home #################################################################

def download_office_home(data_dir):
    # Original URL: http://hemanthdv.org/OfficeHome-Dataset/
    full_path = stage_path(data_dir, "office_home")

    download_and_extract("https://drive.google.com/uc?id=1uY0pj7oFsjMxRwaD3Sxy0jgel0fsYXLC",
                         os.path.join(data_dir, "office_home.zip"))

    os.rename(os.path.join(data_dir, "OfficeHomeDataset_10072016"),
              full_path)


# DomainNET ###################################################################

def download_domain_net(data_dir):
    # Original URL: http://ai.bu.edu/M3SDA/
    full_path = stage_path(data_dir, "domain_net")

    urls = [
        "http://csr.bu.edu/ftp/visda/2019/multi-source/groundtruth/clipart.zip",
        "http://csr.bu.edu/ftp/visda/2019/multi-source/infograph.zip",
        "http://csr.bu.edu/ftp/visda/2019/multi-source/groundtruth/painting.zip",
        "http://csr.bu.edu/ftp/visda/2019/multi-source/quickdraw.zip",
        "http://csr.bu.edu/ftp/visda/2019/multi-source/real.zip",
        "http://csr.bu.edu/ftp/visda/2019/multi-source/sketch.zip"
    ]

    for url in urls:
        download_and_extract(url, os.path.join(full_path, url.split("/")[-1]))

    with open("domainbed/misc/domain_net_duplicates.txt", "r") as f:
        for line in f.readlines():
            try:
                os.remove(os.path.join(full_path, line.strip()))
            except OSError:
                pass


# TerraIncognita ##############################################################

def download_terra_incognita(data_dir):
    # Original URL: https://beerys.github.io/CaltechCameraTraps/
    # New URL: http://lila.science/datasets/caltech-camera-traps

    full_path = stage_path(data_dir, "terra_incognita")

    download_and_extract(
        "https://lilablobssc.blob.core.windows.net/caltechcameratraps/eccv_18_all_images_sm.tar.gz",
        os.path.join(full_path, "terra_incognita_images.tar.gz"))

    download_and_extract(
        "https://lilablobssc.blob.core.windows.net/caltechcameratraps/labels/caltech_camera_traps.json.zip",
        os.path.join(full_path, "caltech_camera_traps.json.zip"))

    include_locations = ["38", "46", "100", "43"]

    include_categories = [
        "bird", "bobcat", "cat", "coyote", "dog", "empty", "opossum", "rabbit",
        "raccoon", "squirrel"
    ]

    images_folder = os.path.join(full_path, "eccv_18_all_images_sm/")
    annotations_file = os.path.join(full_path, "caltech_images_20210113.json")
    destination_folder = full_path

    stats = {}

    if not os.path.exists(destination_folder):
        os.mkdir(destination_folder)

    with open(annotations_file, "r") as f:
        data = json.load(f)

    category_dict = {}
    for item in data['categories']:
        category_dict[item['id']] = item['name']

    for image in data['images']:
        image_location = image['location']

        if image_location not in include_locations:
            continue

        loc_folder = os.path.join(destination_folder,
                                  'location_' + str(image_location) + '/')

        if not os.path.exists(loc_folder):
            os.mkdir(loc_folder)

        image_id = image['id']
        image_fname = image['file_name']

        for annotation in data['annotations']:
            if annotation['image_id'] == image_id:
                if image_location not in stats:
                    stats[image_location] = {}

                category = category_dict[annotation['category_id']]

                if category not in include_categories:
                    continue

                if category not in stats[image_location]:
                    stats[image_location][category] = 0
                else:
                    stats[image_location][category] += 1

                loc_cat_folder = os.path.join(loc_folder, category + '/')

                if not os.path.exists(loc_cat_folder):
                    os.mkdir(loc_cat_folder)

                dst_path = os.path.join(loc_cat_folder, image_fname)
                src_path = os.path.join(images_folder, image_fname)

                shutil.copyfile(src_path, dst_path)

    shutil.rmtree(images_folder)
    os.remove(annotations_file)


# SVIRO #################################################################

def download_sviro(data_dir):
    # Original URL: https://sviro.kl.dfki.de
    full_path = stage_path(data_dir, "sviro")

    download_and_extract("https://sviro.kl.dfki.de/?wpdmdl=1731",
                         os.path.join(data_dir, "sviro_grayscale_rectangle_classification.zip"))

    os.rename(os.path.join(data_dir, "SVIRO_DOMAINBED"),
              full_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Download datasets')
    parser.add_argument('--data_dir', type=str, required=True)
    args = parser.parse_args()

    download_mnist(args.data_dir)
    download_pacs(args.data_dir)
    download_office_home(args.data_dir)
    download_domain_net(args.data_dir)
    download_vlcs(args.data_dir)
    download_terra_incognita(args.data_dir)
    download_sviro(args.data_dir)
    Camelyon17Dataset(root_dir=args.data_dir, download=True)
    FMoWDataset(root_dir=args.data_dir, download=True)

File Path: ATTA/data/loaders/__init__.py
Content:
r"""
This module includes ATTA dataloaders.

- Paired Balanced dataloader: PairDataLoader
"""

import glob
from os.path import dirname, basename, isfile, join

modules = glob.glob(join(dirname(__file__), "*.py"))
__all__ = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]

from . import *
File Path: ATTA/data/loaders/fast_data_loader.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import torch
from torch.utils.data import DataLoader


class _InfiniteSampler(torch.utils.data.Sampler):
    """Wraps another Sampler to yield an infinite stream."""
    def __init__(self, sampler):
        self.sampler = sampler

    def __iter__(self):
        while True:
            for batch in self.sampler:
                yield batch

class InfiniteDataLoader:
    def __init__(self, dataset, weights, batch_size, num_workers, sequential=False, subset=None):
        super().__init__()
        self.dataset = dataset
        if weights is not None:
            sampler = torch.utils.data.WeightedRandomSampler(weights,
                replacement=True,
                num_samples=batch_size)
        elif sequential:
            if subset is None:
                sampler = torch.utils.data.SequentialSampler(dataset)
            else:
                sampler = ActualSequentialSampler(subset)
        elif subset is not None:
            sampler = torch.utils.data.SubsetRandomSampler(subset)
        else:
            sampler = torch.utils.data.RandomSampler(dataset,
                replacement=True)
        self.sampler = sampler

        if weights == None:
            weights = torch.ones(len(dataset))

        batch_sampler = torch.utils.data.BatchSampler(
            sampler,
            batch_size=batch_size,
            drop_last=False)

        self._infinite_iterator = iter(torch.utils.data.DataLoader(
            dataset,
            num_workers=num_workers,
            batch_sampler=_InfiniteSampler(batch_sampler),
            pin_memory=False,
            persistent_workers=True if num_workers > 0 else False
        ))

    def __iter__(self):
        while True:
            yield next(self._infinite_iterator)

    def __len__(self):
        raise ValueError

class FastDataLoader:
    """DataLoader wrapper with slightly improved speed by not respawning worker
    processes at every epoch."""
    def __init__(self, dataset, weights, batch_size, num_workers, sequential=False, subset=None):
        super().__init__()
        self.dataset = dataset
        if weights is not None:
            sampler = torch.utils.data.WeightedRandomSampler(weights,
                replacement=False,
                num_samples=batch_size)
        elif sequential:
            if subset is None:
                sampler = torch.utils.data.SequentialSampler(dataset)
            else:
                sampler = ActualSequentialSampler(subset)
        elif subset is not None:
            sampler = torch.utils.data.SubsetRandomSampler(subset)
        else:
            sampler = torch.utils.data.RandomSampler(dataset,
                replacement=False)
        self.sampler = sampler

        batch_sampler = torch.utils.data.BatchSampler(
            sampler,
            batch_size=batch_size,
            drop_last=False
        )

        self._infinite_iterator = iter(torch.utils.data.DataLoader(
            dataset,
            num_workers=num_workers,
            batch_sampler=_InfiniteSampler(batch_sampler),
            pin_memory=False,
            persistent_workers=True if num_workers > 0 else False
        ))

        self._length = len(batch_sampler)

    def __iter__(self):
        for _ in range(len(self)):
            yield next(self._infinite_iterator)

    def __len__(self):
        return self._length


class ActualSequentialSampler(torch.utils.data.Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """

    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(self.data_source)

    def __len__(self):
        return len(self.data_source)
File Path: ATTA/data/loaders/misc.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

"""
Things that don't belong anywhere else
"""

import hashlib
import sys
from collections import OrderedDict
from numbers import Number
import operator

import numpy as np
import torch
from collections import Counter
from itertools import cycle


def distance(h1, h2):
    ''' distance of two networks (h1, h2 are classifiers)'''
    dist = 0.
    for param in h1.state_dict():
        h1_param, h2_param = h1.state_dict()[param], h2.state_dict()[param]
        dist += torch.norm(h1_param - h2_param) ** 2  # use Frobenius norms for matrices
    return torch.sqrt(dist)

def proj(delta, adv_h, h):
    ''' return proj_{B(h, \delta)}(adv_h), Euclidean projection to Euclidean ball'''
    ''' adv_h and h are two classifiers'''
    dist = distance(adv_h, h)
    if dist <= delta:
        return adv_h
    else:
        ratio = delta / dist
        for param_h, param_adv_h in zip(h.parameters(), adv_h.parameters()):
            param_adv_h.data = param_h + ratio * (param_adv_h - param_h)
        # print("distance: ", distance(adv_h, h))
        return adv_h

def l2_between_dicts(dict_1, dict_2):
    assert len(dict_1) == len(dict_2)
    dict_1_values = [dict_1[key] for key in sorted(dict_1.keys())]
    dict_2_values = [dict_2[key] for key in sorted(dict_1.keys())]
    return (
        torch.cat(tuple([t.view(-1) for t in dict_1_values])) -
        torch.cat(tuple([t.view(-1) for t in dict_2_values]))
    ).pow(2).mean()

class MovingAverage:

    def __init__(self, ema, oneminusema_correction=True):
        self.ema = ema
        self.ema_data = {}
        self._updates = 0
        self._oneminusema_correction = oneminusema_correction

    def update(self, dict_data):
        ema_dict_data = {}
        for name, data in dict_data.items():
            data = data.view(1, -1)
            if self._updates == 0:
                previous_data = torch.zeros_like(data)
            else:
                previous_data = self.ema_data[name]

            ema_data = self.ema * previous_data + (1 - self.ema) * data
            if self._oneminusema_correction:
                # correction by 1/(1 - self.ema)
                # so that the gradients amplitude backpropagated in data is independent of self.ema
                ema_dict_data[name] = ema_data / (1 - self.ema)
            else:
                ema_dict_data[name] = ema_data
            self.ema_data[name] = ema_data.clone().detach()

        self._updates += 1
        return ema_dict_data



def make_weights_for_balanced_classes(dataset):
    counts = Counter()
    classes = []
    for _, y in dataset:
        y = int(y)
        counts[y] += 1
        classes.append(y)

    n_classes = len(counts)

    weight_per_class = {}
    for y in counts:
        weight_per_class[y] = 1 / (counts[y] * n_classes)

    weights = torch.zeros(len(dataset))
    for i, y in enumerate(classes):
        weights[i] = weight_per_class[int(y)]

    return weights

def pdb():
    sys.stdout = sys.__stdout__
    import pdb
    print("Launching PDB, enter 'n' to step to parent function.")
    pdb.set_trace()

def seed_hash(*args):
    """
    Derive an integer hash from all args, for use as a random seed.
    """
    args_str = str(args)
    return int(hashlib.md5(args_str.encode("utf-8")).hexdigest(), 16) % (2**31)

def print_separator():
    print("="*80)

def print_row(row, colwidth=10, latex=False):
    if latex:
        sep = " & "
        end_ = "\\\\"
    else:
        sep = "  "
        end_ = ""

    def format_val(x):
        if np.issubdtype(type(x), np.floating):
            x = "{:.10f}".format(x)
        return str(x).ljust(colwidth)[:colwidth]
    print(sep.join([format_val(x) for x in row]), end_)

class _SplitDataset(torch.utils.data.Dataset):
    """Used by split_dataset"""
    def __init__(self, underlying_dataset, keys):
        super(_SplitDataset, self).__init__()
        self.underlying_dataset = underlying_dataset
        self.keys = keys
    def __getitem__(self, key):
        return self.underlying_dataset[self.keys[key]]
    def __len__(self):
        return len(self.keys)

def split_dataset(dataset, n, seed=0):
    """
    Return a pair of datasets corresponding to a random split of the given
    dataset, with n datapoints in the first dataset and the rest in the last,
    using the given random seed
    """
    assert(n <= len(dataset))
    keys = list(range(len(dataset)))
    np.random.RandomState(seed).shuffle(keys)
    keys_1 = keys[:n]
    keys_2 = keys[n:]
    return _SplitDataset(dataset, keys_1), _SplitDataset(dataset, keys_2)

def random_pairs_of_minibatches(minibatches):
    perm = torch.randperm(len(minibatches)).tolist()
    pairs = []

    for i in range(len(minibatches)):
        j = i + 1 if i < (len(minibatches) - 1) else 0

        xi, yi = minibatches[perm[i]][0], minibatches[perm[i]][1]
        xj, yj = minibatches[perm[j]][0], minibatches[perm[j]][1]

        min_n = min(len(xi), len(xj))

        pairs.append(((xi[:min_n], yi[:min_n]), (xj[:min_n], yj[:min_n])))

    return pairs

def split_meta_train_test(minibatches, num_meta_test=1):
    n_domains = len(minibatches)
    perm = torch.randperm(n_domains).tolist()
    pairs = []
    meta_train = perm[:(n_domains-num_meta_test)]
    meta_test = perm[-num_meta_test:]

    for i,j in zip(meta_train, cycle(meta_test)):
         xi, yi = minibatches[i][0], minibatches[i][1]
         xj, yj = minibatches[j][0], minibatches[j][1]

         min_n = min(len(xi), len(xj))
         pairs.append(((xi[:min_n], yi[:min_n]), (xj[:min_n], yj[:min_n])))

    return pairs

def accuracy(network, loader, weights, device):
    correct = 0
    total = 0
    weights_offset = 0

    network.eval()
    with torch.no_grad():
        for i, (x, y) in enumerate(loader):
            # print(i)
            x = x.to(device)
            y = y.to(device)
            p = network.predict(x)
            if weights is None:
                batch_weights = torch.ones(len(x))
            else:
                batch_weights = weights[weights_offset : weights_offset + len(x)]
                weights_offset += len(x)
            batch_weights = batch_weights.to(device)
            if p.size(1) == 1:
                correct += (p.gt(0).eq(y).float() * batch_weights.view(-1, 1)).sum().item()
            else:
                correct += (p.argmax(1).eq(y).float() * batch_weights).sum().item()
            total += batch_weights.sum().item()
    network.train()

    return correct / total

class Tee:
    def __init__(self, fname, mode="a"):
        self.stdout = sys.stdout
        self.file = open(fname, mode)

    def write(self, message):
        self.stdout.write(message)
        self.file.write(message)
        self.flush()

    def flush(self):
        self.stdout.flush()
        self.file.flush()

class ParamDict(OrderedDict):
    """Code adapted from https://github.com/Alok/rl_implementations/tree/master/reptile.
    A dictionary where the values are Tensors, meant to represent weights of
    a model. This subclass lets you perform arithmetic on weights directly."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, *kwargs)

    def _prototype(self, other, op):
        if isinstance(other, Number):
            return ParamDict({k: op(v, other) for k, v in self.items()})
        elif isinstance(other, dict):
            return ParamDict({k: op(self[k], other[k]) for k in self})
        else:
            raise NotImplementedError

    def __add__(self, other):
        return self._prototype(other, operator.add)

    def __rmul__(self, other):
        return self._prototype(other, operator.mul)

    __mul__ = __rmul__

    def __neg__(self):
        return ParamDict({k: -v for k, v in self.items()})

    def __rsub__(self, other):
        # a- b := a + (-b)
        return self.__add__(other.__neg__())

    __sub__ = __rsub__

    def __truediv__(self, other):
        return self._prototype(other, operator.truediv)

File Path: ATTA/definitions.py
Content:
r"""
Only for project usage. This module includes a preset project root and a storage root.
"""
import os

# root outside the ATTA
STORAGE_DIR = os.path.join(os.path.abspath(os.path.dirname(os.path.dirname(__file__))), 'storage')  #: :str - Storage root=<Project root>/storage/.
ROOT_DIR = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))  #: str - Project root or Repo root.
OOM_CODE = 88

File Path: ATTA/kernel/__init__.py
Content:
r"""
This module contains project kernel configuring, training, testing, and evaluation pipelines.
"""
from .algorithms import *
File Path: ATTA/kernel/alg_main.py
Content:
r"""Kernel pipeline: main pipeline, initialization, task loading, etc.
"""
import time
from typing import Tuple, Union

import torch.nn
from torch.utils.data import DataLoader

from ATTA import config_summoner
# from ATTA.utils.config_reader import config_summoner
from ATTA.utils.load_manager import load_atta_algorithm
from ATTA.utils.args import args_parser
from ATTA.utils.config_reader import Conf
from ATTA.utils.initial import reset_random_seed
from ATTA.utils.logger import load_logger
from ATTA.definitions import OOM_CODE
import multiprocessing as mp

def main():
    #
    args = args_parser()
    config = config_summoner(args)
    if config.mp_spawn:
        # torch.set_num_threads(5)
        mp.set_start_method('spawn') # ImageFolder and Subprocess may cause deadlock with multiprocessing fork
    load_logger(config)

    alg = load_atta_algorithm(config)
    tik = time.time()
    alg()
    print(f"Time cost: {time.time() - tik}s")

if __name__ == '__main__':
    main()

File Path: ATTA/kernel/algorithms/Base.py
Content:
import os

import numpy as np
import torch
from torch import nn
from torch.utils.data import ConcatDataset
# import models for resnet18

from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader
from ATTA.utils.config_reader import Conf
from ATTA.utils.initial import reset_random_seed
from ATTA.utils.register import register


# import torch.multiprocessing
# torch.multiprocessing.set_sharing_strategy('file_system')

@register.alg_register
class AlgBase:
    def __init__(self, config: Conf):
        super(AlgBase, self).__init__()

        if not os.path.exists(config.ckpt_dir):
            os.makedirs(config.ckpt_dir)

        reset_random_seed(config)
        self.dataset = register.datasets[config.dataset.name](config.dataset.dataset_root, config.dataset.test_envs,
                                                              config)
        config.dataset.dataset_type = 'image'
        config.dataset.input_shape = self.dataset.input_shape
        config.dataset.num_classes = 1 if self.dataset.num_classes == 2 else self.dataset.num_classes
        config.model.model_level = 'image'
        config.metric.set_score_func(self.dataset.metric)
        config.metric.set_loss_func(self.dataset.task)

        self.config = config

        self.inf_loader = [InfiniteDataLoader(env, weights=None, batch_size=self.config.train.train_bs,
                                              num_workers=self.config.num_workers) for env in self.dataset]
        reset_random_seed(config)
        self.train_split = [np.random.choice(len(env), size=int(len(env) * 0.8), replace=False) for env in self.dataset]
        print(self.train_split)
        self.val_split = [np.setdiff1d(np.arange(len(env)), self.train_split[i]) for i, env in enumerate(self.dataset)]
        self.train_loader = [InfiniteDataLoader(env, weights=None, batch_size=self.config.train.train_bs,
                                                num_workers=self.config.num_workers, subset=self.train_split[i]) for
                             i, env in enumerate(self.dataset)]
        self.val_loader = [FastDataLoader(env, weights=None, batch_size=self.config.train.train_bs,
                                          num_workers=self.config.num_workers,
                                          subset=self.val_split[i], sequential=True) for i, env in
                           enumerate(self.dataset)]
        reset_random_seed(config)
        fast_random = [np.random.permutation(len(env)) for env in self.dataset]
        self.fast_loader = [FastDataLoader(env, weights=None,
                                           batch_size=self.config.atta.batch_size,
                                           num_workers=self.config.num_workers,
                                           subset=fast_random[i], sequential=True) for i, env in
                            enumerate(self.dataset)]

        reset_random_seed(config)
        self.target_dataset = ConcatDataset(
            [env for i, env in enumerate(self.dataset) if i in config.dataset.test_envs[1:]])
        len_target = len(self.target_dataset)
        target_choices = np.random.permutation(len_target)
        len_split = len_target // 4
        self.target_splits = [target_choices[i * len_split: (i + 1) * len_split] for i in range(4)]
        self.target_splits[-1] = target_choices[3 * len_split:]
        self.target_loader = [FastDataLoader(self.target_dataset, weights=None,
                                             batch_size=self.config.atta.batch_size,
                                             num_workers=self.config.num_workers, subset=self.target_splits[i],
                                             sequential=True) for i in range(4)]

        self.encoder = register.models[config.model.name](config).to(self.config.device)

        #     self.fc = self.encoder.fc
        #     self.model = nn.Sequential(self.encoder, self.fc).to(self.config.device)
        # else:
        self.fc = nn.Linear(self.encoder.n_outputs, config.dataset.num_classes).to(self.config.device)
        self.model = nn.Sequential(self.encoder, self.fc).to(self.config.device)

        if 'ImageNet' in config.dataset.name or 'CIFAR' in config.dataset.name:
            self.train_on_env(self.config.dataset.test_envs[0], train_only_fc=True, train_or_load='load')
        else:
            self.train_on_env(self.config.dataset.test_envs[0], train_only_fc=False, train_or_load='load')

    def __call__(self, *args, **kwargs):
        for env_id in self.config.dataset.test_envs:
            self.test_on_env(env_id)

    @torch.no_grad()
    def test_on_env(self, env_id):
        # self.encoder.eval()
        # self.fc.eval()
        self.model.eval()
        test_loss = 0
        test_acc = 0
        for data, target in self.fast_loader[env_id]:
            data, target = data.to(self.config.device), target.to(self.config.device)
            output = self.fc(self.encoder(data))
            test_loss += self.config.metric.loss_func(output, target, reduction='sum').item()
            test_acc += self.config.metric.score_func(target, output) * len(data)
        test_loss /= len(self.fast_loader[env_id].dataset)
        test_acc /= len(self.fast_loader[env_id].dataset)
        print(f'#I#Env {env_id} Test set: Average loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')
        return test_loss, test_acc

    @torch.no_grad()
    def val_on_env(self, env_id):
        self.model.eval()
        val_loss = 0
        val_acc = 0
        for data, target in self.val_loader[env_id]:
            data, target = data.to(self.config.device), target.to(self.config.device)
            output = self.fc(self.encoder(data))
            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()
            val_acc += self.config.metric.score_func(target, output) * len(data)
        val_loss /= len(self.val_split[env_id])
        val_acc /= len(self.val_split[env_id])
        return val_loss, val_acc

    @torch.enable_grad()
    def train_on_env(self, env_id, train_only_fc=True, train_or_load='train'):
        if train_or_load == 'train' or not os.path.exists(self.config.ckpt_dir + f'/encoder_{env_id}.pth'):
            # if train_only_fc:
            #     self.encoder.train()
            #     self.fc.train()
            #     optimizer = torch.optim.Adam(self.fc.parameters(), lr=self.config.train.lr)
            #     inf_loader = self.extract_pretrained_feat(self.fast_loader[env_id], self.config.train.train_bs)
            #     for batch_idx, (data, target) in enumerate(inf_loader):
            #         optimizer.zero_grad()
            #         data, target = data.to(self.config.device), target.to(self.config.device)
            #         output = self.fc(data)
            #         loss = self.config.metric.loss_func(output, target)
            #         acc = self.config.metric.score_func(target, output)
            #         loss.backward()
            #         optimizer.step()
            #         if batch_idx % self.config.train.log_interval == 0:
            #             print(f'Iteration: {batch_idx} Loss: {loss.item():.4f} Acc: {acc:.4f}')
            #         if batch_idx > self.config.train.max_iters:
            #             break
            # else:
            # best_val_loss = float('inf')
            best_val_acc = 0
            if train_only_fc:
                self.model.eval()
                self.fc.train()
                optimizer = torch.optim.Adam(self.fc.parameters(), lr=self.config.train.lr)
            else:
                self.model.train()
                optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.train.lr)
            for batch_idx, (data, target) in enumerate(self.train_loader[env_id]):
                optimizer.zero_grad()
                data, target = data.to(self.config.device), target.to(self.config.device)
                output = self.fc(self.encoder(data))
                loss = self.config.metric.loss_func(output, target)
                acc = self.config.metric.score_func(target, output)
                loss.backward()
                optimizer.step()
                if batch_idx % self.config.train.log_interval == 0:
                    print(f'Iteration: {batch_idx} Loss: {loss.item():.4f} Acc: {acc:.4f}')
                    val_loss, val_acc = self.val_on_env(env_id)
                    if val_acc > best_val_acc:
                        print(f'New best val acc: {val_acc:.4f}')
                        best_val_acc = val_acc
                        torch.save(self.encoder.state_dict(), self.config.ckpt_dir + f'/encoder_{env_id}.pth')
                        torch.save(self.fc.state_dict(), self.config.ckpt_dir + f'/fc_{env_id}.pth')
                    self.model.train()
                if batch_idx > self.config.train.max_iters:
                    break
        else:
            self.encoder.load_state_dict(
                torch.load(self.config.ckpt_dir + f'/encoder_{env_id}.pth', map_location=self.config.device), strict=False)
            self.fc.load_state_dict(
                torch.load(self.config.ckpt_dir + f'/fc_{env_id}.pth', map_location=self.config.device))

    # @torch.no_grad()
    # def extract_pretrained_feat(self, loader, batch_size):
    #     print('Extracting features from the loader')
    #     feats, targets = [], []
    #     for data, target in loader:
    #         data, target = data.to(self.config.device), target.to(self.config.device)
    #         feat = self.encoder(data)
    #         feats.append(feat.cpu())
    #         targets.append(target.cpu())
    #     feats = torch.cat(feats, dim=0)
    #     targets = torch.cat(targets, dim=0)
    #     feat_dataset = TensorDataset(feats, targets)
    #     weights = misc.make_weights_for_balanced_classes(
    #         feat_dataset) if self.config.dataset.class_balanced else None
    #     inf_loader = InfiniteDataLoader(dataset=feat_dataset, weights=weights,
    #                                     batch_size=batch_size, num_workers=self.config.num_workers)
    #     return inf_loader

File Path: ATTA/kernel/algorithms/CLUE.py
Content:
import numpy as np
import torch
from joblib import parallel_backend
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from torch.utils.data import ConcatDataset
# import models for resnet18

from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader
from ATTA.utils.config_reader import Conf
from ATTA.utils.register import register
from .Base import AlgBase


@register.alg_register
class CLUE(AlgBase):
    def __init__(self, config: Conf):
        super(CLUE, self).__init__(config)
        self.budgets = self.config.atta.budgets
        self.anchors = []
        self.buffer = []

    def __call__(self, *args, **kwargs):
        self.adapt()
        for env_id in self.config.dataset.test_envs:
            self.test_on_env(env_id)

    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if x.shape[1] == 1:
            x = torch.cat([x, -x], dim=1)
        return -(x.softmax(1) * x.log_softmax(1)).sum(1)

    @torch.no_grad()
    def adapt(self):
        idxs_lb = np.zeros(len(self.target_dataset), dtype=bool)
        for round in range(10):
            data_loader = torch.utils.data.DataLoader(self.target_dataset, sampler=ActualSequentialSampler(np.arange(len(self.target_dataset))),
                                                      num_workers=self.config.num_workers,
                                                      batch_size=self.config.train.train_bs, drop_last=False)
            self.model.eval()
            buffer = []
            for data, target in data_loader:
                data, target = data.to(self.config.device), target.to(self.config.device)
                feats = self.encoder(data)
                output = self.fc(feats)
                entropy = self.softmax_entropy(output)
                buffer.append((data, target, feats.cpu().numpy(), entropy.cpu().numpy()))
            data, target, feats, entropy = zip(*buffer)
            feats = np.concatenate(feats)[~idxs_lb]
            entropy = np.concatenate(entropy)[~idxs_lb]
            if round == 9:
                n_clusters = self.budgets - idxs_lb.sum()
            else:
                n_clusters = self.budgets // 10
            with parallel_backend('threading', n_jobs=8):
                kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats, sample_weight=entropy)
                closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats)
            idxs_lb_id = idxs_lb.nonzero()[0]
            # print(f'closest: {closest}')
            for idx in closest:
                true_idx = idx
                while true_idx != (idxs_lb_id <= true_idx).sum() + idx:
                    true_idx += 1
                # assert not idxs_lb[true_idx], f'true_idx: {true_idx}, (idxs_lb_id < true_idx).sum(): {(idxs_lb_id < true_idx).sum()}, idx: {idx}'
                idxs_lb[true_idx] = True
            # print(f'idxs_lb: {idxs_lb.nonzero()[0]}')

            anchor_loader = InfiniteDataLoader(self.target_dataset, weights=None,
                                               batch_size=self.config.train.train_bs,
                                               num_workers=self.config.num_workers, subset=np.nonzero(idxs_lb)[0])
            self.model.train()
            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)
            print('Cluster train')
            with torch.enable_grad():
                for i, (data, target) in enumerate(anchor_loader):
                    data, target = data.to(self.config.device), target.to(self.config.device)
                    optimizer.zero_grad()
                    output = self.fc(self.encoder(data))
                    loss = self.config.metric.loss_func(output, target)
                    loss.backward()
                    optimizer.step()
                    if i > self.config.atta.SimATTA.steps:
                        break


class ActualSequentialSampler(torch.utils.data.Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """

    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(self.data_source)

    def __len__(self):
        return len(self.data_source)

File Path: ATTA/kernel/algorithms/CoTTA.py
Content:
from typing import Dict
from typing import Union

import math
import numpy as np
import torch
import torch.nn as nn
import torchvision.models
from torchvision import transforms
from torch.utils.data import DataLoader

from ATTA.utils.config_reader import Conf
from ATTA.utils.register import register
from copy import deepcopy
from torch import nn
import torch
import torch.nn.functional as F
from torch.distributions import Normal, kl_divergence
# import models for resnet18
from torchvision.models import resnet18
import itertools
import os
import ATTA.data.loaders.misc as misc
from ATTA import register
from ATTA.utils.config_reader import Conf
from ATTA.utils.config_reader import Conf
from ATTA.utils.initial import reset_random_seed
from ATTA.utils.initial import reset_random_seed
from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader
from torch.utils.data import TensorDataset, Subset
import torch
import torchvision.transforms.functional as F
from torchvision.transforms import ColorJitter, Compose, Lambda
from numpy import random
import PIL
from .Base import AlgBase
import pandas as pd

@register.alg_register
class CoTTA(AlgBase):
    def __init__(self, config: Conf):
        super(CoTTA, self).__init__(config)
        print('#D#Config model')
        self.configure_model()
        params, param_names = self.collect_params()
        # print(f'#I#{param_names}')
        self.optimizer = torch.optim.SGD(params, lr=self.config.atta.SAR.lr, momentum=0.9)
        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \
            self.copy_model_and_optimizer()
        self.transform = get_tta_transforms()


    def __call__(self, *args, **kwargs):
        # super(CoTTA, self).__call__()

        self.continue_result_df = pd.DataFrame(
            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)
        self.random_result_df = pd.DataFrame(
            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)

        for adapt_id in self.config.dataset.test_envs[1:]:
            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]

        self.__init__(self.config)
        for target_split_id in range(4):
            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader,
                                                                                           target_split_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]

        print(self.continue_result_df.round(4).to_markdown(), '\n')
        print(self.random_result_df.round(4).to_markdown())

    @staticmethod
    def softmax_entropy(x, x_ema):  # -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        return -0.5 * (x_ema.softmax(1) * x.log_softmax(1)).sum(1) - 0.5 * (x.softmax(1) * x_ema.log_softmax(1)).sum(1)

    # Active contrastive learning
    @torch.enable_grad()
    def adapt_on_env(self, loader, env_id):
        steps = self.config.atta.SAR.steps
        self.configure_model()
        acc = 0
        for data, targets in loader[env_id]:
            targets = targets.to(self.config.device)
            gt_mask = torch.rand(targets.shape[0], device=targets.device) < self.config.atta.al_rate
            for _ in range(steps):
                data = data.to(self.config.device)
                outputs = self.forward_and_adapt(data, targets, gt_mask)
            acc += self.config.metric.score_func(targets, outputs) * len(data)
        acc /= len(loader[env_id].sampler)
        print(f'Env {env_id} real-time Acc.: {acc:.4f}')
        return acc

    def copy_model_and_optimizer(self):
        """Copy the model and optimizer states for resetting after adaptation."""
        model_state = deepcopy(self.model.state_dict())
        model_anchor = deepcopy(self.model)
        optimizer_state = deepcopy(self.optimizer.state_dict())
        ema_model = deepcopy(self.model)
        for param in ema_model.parameters():
            param.detach_()
        return model_state, optimizer_state, ema_model, model_anchor

    def load_model_and_optimizer(self):
        """Restore the model and optimizer states from copies."""
        self.model.load_state_dict(self.model_state, strict=True)
        self.optimizer.load_state_dict(self.optimizer_state)

    def reset(self):
        if self.model_state is None or self.optimizer_state is None:
            raise Exception("cannot reset without saved model/optimizer state")
        self.load_model_and_optimizer()
        # use this line if you want to reset the teacher model as well. Maybe you also
        # want to del self.model_ema first to save gpu memory.
        self.model_state, self.optimizer_state, self.model_ema, self.model_anchor = \
            self.copy_model_and_optimizer()

    def configure_model(self):
        """Configure model for use with tent."""
        # train mode, because tent optimizes the model to minimize entropy
        self.model.train()
        # disable grad, to (re-)enable only what tent updates
        self.model.requires_grad_(False)
        # configure norm for tent updates: enable grad + force batch statisics
        for m in self.model.modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                m.requires_grad_(True)
                # force use of batch stats in train and eval modes
                m.track_running_stats = False
                m.running_mean = None
                m.running_var = None
            else:
                m.requires_grad_(True)

    def collect_params(self):
        """Collect all trainable parameters.
        Walk the model's modules and collect all parameters.
        Return the parameters and their names.
        Note: other choices of parameterization are possible!
        """
        params = []
        names = []
        for nm, m in self.model.named_modules():
            if True:  # isinstance(m, nn.BatchNorm2d): collect all
                for np, p in m.named_parameters():
                    if np in ['weight', 'bias'] and p.requires_grad:
                        params.append(p)
                        names.append(f"{nm}.{np}")
                        print(nm, np)
        return params, names


    @torch.enable_grad()  # ensure grads in possible no grad context for testing
    def forward_and_adapt(self, data, targets, gt_mask):
        outputs = self.model(data)
        self.model_ema.train()
        # Teacher Prediction
        anchor_prob = torch.nn.functional.softmax(self.model_anchor(data), dim=1).max(1)[0]
        standard_ema = self.model_ema(data)
        # Augmentation-averaged Prediction
        N = 32
        outputs_emas = []
        to_aug = anchor_prob.mean(0) < 0.1
        if to_aug:
            for i in range(N):
                outputs_  = self.model_ema(self.transform(data)).detach()
                outputs_emas.append(outputs_)
        # Threshold choice discussed in supplementary
        if to_aug:
            outputs_ema = torch.stack(outputs_emas).mean(0)
        else:
            outputs_ema = standard_ema
        # Augmentation-averaged Prediction
        # Student update
        loss = (self.softmax_entropy(outputs, outputs_ema.detach())).mean(0)

        # --- AL learning ---
        if self.config.atta.al_rate is not None and gt_mask.sum() > 0:
            loss += self.config.metric.loss_func(outputs[gt_mask], targets[gt_mask])


        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        # Teacher update
        self.model_ema = update_ema_variables(ema_model = self.model_ema, model = self.model, alpha_teacher=0.999)
        # Stochastic restore
        # print('Stochastic restore')
        if True:
            for nm, m  in self.model.named_modules():
                for npp, p in m.named_parameters():
                    if npp in ['weight', 'bias'] and p.requires_grad:
                        mask = (torch.rand(p.shape)<0.001).float().to(self.config.device)
                        with torch.no_grad():
                            p.data = self.model_state[f"{nm}.{npp}"] * mask + p * (1.-mask)
        return outputs_ema

def update_ema_variables(ema_model, model, alpha_teacher):#, iteration):
    for ema_param, param in zip(ema_model.parameters(), model.parameters()):
        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]
    return ema_model

def get_tta_transforms(gaussian_std: float=0.005, soft=False, clip_inputs=False):
    img_shape = (224, 224, 3)
    n_pixels = img_shape[0]

    clip_min, clip_max = 0.0, 1.0

    p_hflip = 0.5

    tta_transforms = transforms.Compose([
        Clip(0.0, 1.0),
        ColorJitterPro(
            brightness=[0.8, 1.2] if soft else [0.6, 1.4],
            contrast=[0.85, 1.15] if soft else [0.7, 1.3],
            saturation=[0.75, 1.25] if soft else [0.5, 1.5],
            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],
            gamma=[0.85, 1.15] if soft else [0.7, 1.3]
        ),
        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),
        transforms.RandomAffine(
            degrees=[-8, 8] if soft else [-15, 15],
            translate=(1/16, 1/16),
            scale=(0.95, 1.05) if soft else (0.9, 1.1),
            shear=None,
            resample=PIL.Image.BILINEAR,
            fillcolor=None
        ),
        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),
        transforms.CenterCrop(size=n_pixels),
        transforms.RandomHorizontalFlip(p=p_hflip),
        GaussianNoise(0, gaussian_std),
        Clip(clip_min, clip_max)
    ])
    return tta_transforms

class GaussianNoise(torch.nn.Module):
    def __init__(self, mean=0., std=1.):
        super().__init__()
        self.std = std
        self.mean = mean

    def forward(self, img):
        noise = torch.randn(img.size()) * self.std + self.mean
        noise = noise.to(img.device)
        return img + noise

    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)

class Clip(torch.nn.Module):
    def __init__(self, min_val=0., max_val=1.):
        super().__init__()
        self.min_val = min_val
        self.max_val = max_val

    def forward(self, img):
        return torch.clip(img, self.min_val, self.max_val)

    def __repr__(self):
        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)

class ColorJitterPro(ColorJitter):
    """Randomly change the brightness, contrast, saturation, and gamma correction of an image."""

    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):
        super().__init__(brightness, contrast, saturation, hue)
        self.gamma = self._check_input(gamma, 'gamma')

    @staticmethod
    @torch.jit.unused
    def get_params(brightness, contrast, saturation, hue, gamma):
        """Get a randomized transform to be applied on image.
        Arguments are same as that of __init__.
        Returns:
            Transform which randomly adjusts brightness, contrast and
            saturation in a random order.
        """
        transforms = []

        if brightness is not None:
            brightness_factor = random.uniform(brightness[0], brightness[1])
            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))

        if contrast is not None:
            contrast_factor = random.uniform(contrast[0], contrast[1])
            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))

        if saturation is not None:
            saturation_factor = random.uniform(saturation[0], saturation[1])
            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))

        if hue is not None:
            hue_factor = random.uniform(hue[0], hue[1])
            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))

        if gamma is not None:
            gamma_factor = random.uniform(gamma[0], gamma[1])
            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))

        random.shuffle(transforms)
        transform = Compose(transforms)

        return transform

    def forward(self, img):
        """
        Args:
            img (PIL Image or Tensor): Input image.
        Returns:
            PIL Image or Tensor: Color jittered image.
        """
        fn_idx = torch.randperm(5)
        for fn_id in fn_idx:
            if fn_id == 0 and self.brightness is not None:
                brightness = self.brightness
                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()
                img = F.adjust_brightness(img, brightness_factor)

            if fn_id == 1 and self.contrast is not None:
                contrast = self.contrast
                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()
                img = F.adjust_contrast(img, contrast_factor)

            if fn_id == 2 and self.saturation is not None:
                saturation = self.saturation
                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()
                img = F.adjust_saturation(img, saturation_factor)

            if fn_id == 3 and self.hue is not None:
                hue = self.hue
                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()
                img = F.adjust_hue(img, hue_factor)

            if fn_id == 4 and self.gamma is not None:
                gamma = self.gamma
                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()
                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma
                                            # after contrast
                img = F.adjust_gamma(img, gamma_factor)

        return img

    def __repr__(self):
        format_string = self.__class__.__name__ + '('
        format_string += 'brightness={0}'.format(self.brightness)
        format_string += ', contrast={0}'.format(self.contrast)
        format_string += ', saturation={0}'.format(self.saturation)
        format_string += ', hue={0})'.format(self.hue)
        format_string += ', gamma={0})'.format(self.gamma)
        return format_string
File Path: ATTA/kernel/algorithms/EATA.py
Content:
# import models for resnet18
import os
from copy import deepcopy

import math
import numpy as np
import torch
from torch import nn
from torch.utils.data import ConcatDataset
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset

import ATTA.data.loaders.misc as misc
from ATTA import register
from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader
from ATTA.utils.config_reader import Conf
from ATTA.utils.initial import reset_random_seed
import torch.nn.functional as F
from .Base import AlgBase
import pandas as pd


@register.alg_register
class EATA(AlgBase):

    def __init__(self, config: Conf):
        super(EATA, self).__init__(config)
        num_classes = 2 if config.dataset.num_classes == 1 else config.dataset.num_classes

        self.num_samples_update_1 = 0  # number of samples after First filtering, exclude unreliable samples
        self.num_samples_update_2 = 0  # number of samples after Second filtering, exclude both unreliable and redundant samples
        self.e_margin = 0.4 * math.log(num_classes)  # hyper-parameter E_0 (Eqn. 3)
        self.d_margin = self.config.atta.EATA.d_margin  # hyper-parameter \epsilon for consine simlarity thresholding (Eqn. 5)

        self.current_model_probs = None  # the moving average of probability vector (Eqn. 4)

        self.fishers = None  # fisher regularizer items for anti-forgetting, need to be calculated pre model adaptation (Eqn. 9)
        self.fisher_alpha = self.config.atta.EATA.fisher_alpha  # trade-off \beta for two losses (Eqn. 8)

        print('load fisher loader')
        fisher_loader = DataLoader(self.target_dataset, sampler=torch.utils.data.SubsetRandomSampler(
            np.random.choice(len(self.target_dataset), size=2000, replace=False)), batch_size=64,
                                   num_workers=self.config.num_workers)

        self.configure_model()
        params, param_names = self.collect_params()
        # fishers = None
        ewc_optimizer = torch.optim.SGD(params, 0.001)
        fishers = {}
        train_loss_fn = nn.CrossEntropyLoss().to(self.config.device)
        print('train fisher')
        for iter_, (images, targets) in enumerate(fisher_loader, start=1):
            print(iter_)
            images, targets = images.to(self.config.device), targets.to(self.config.device)
            outputs = self.model(images)
            _, targets = outputs.max(1)
            loss = train_loss_fn(outputs, targets)
            loss.backward()
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    if iter_ > 1:
                        fisher = param.grad.data.clone().detach() ** 2 + fishers[name][0]
                    else:
                        fisher = param.grad.data.clone().detach() ** 2
                    if iter_ == len(fisher_loader):
                        fisher = fisher / iter_
                    fishers.update({name: [fisher, param.data.clone().detach()]})
            ewc_optimizer.zero_grad()
        print("compute fisher matrices finished")
        del ewc_optimizer

        self.fishers = fishers

        self.optimizer = torch.optim.SGD(params, self.config.atta.EATA.lr, momentum=0.9)
        self.model_state, self.optimizer_state = \
            self.copy_model_and_optimizer()


    def __call__(self, *args, **kwargs):
        # super(EATA, self).__call__()

        self.continue_result_df = pd.DataFrame(
            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)
        self.random_result_df = pd.DataFrame(
            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)

        for adapt_id in self.config.dataset.test_envs[1:]:
            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]

        self.__init__(self.config)
        for target_split_id in range(4):
            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader,
                                                                                           target_split_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]

        print(self.continue_result_df.round(4).to_markdown(), '\n')
        print(self.random_result_df.round(4).to_markdown())

    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        temprature = 1
        x = x / temprature
        x = -(x.softmax(1) * x.log_softmax(1)).sum(1)
        return x

    # Active contrastive learning
    @torch.enable_grad()
    def adapt_on_env(self, loader, env_id):
        steps = self.config.atta.EATA.steps
        self.configure_model()
        acc = 0
        for data, targets in loader[env_id]:
            targets = targets.to(self.config.device)
            gt_mask = torch.rand(targets.shape[0], device=targets.device) < self.config.atta.al_rate
            for _ in range(steps):
                data = data.to(self.config.device)
                outputs, num_counts_2, num_counts_1, updated_probs = self.forward_and_adapt(data, targets, gt_mask)
                self.num_samples_update_2 += num_counts_2
                self.num_samples_update_1 += num_counts_1
                self.reset_model_probs(updated_probs)
            acc += self.config.metric.score_func(targets, outputs) * len(data)
        acc /= len(loader[env_id].sampler)
        print(f'Env {env_id} real-time Acc.: {acc:.4f}')
        return acc

    def reset_model_probs(self, probs):
        self.current_model_probs = probs

    def copy_model_and_optimizer(self):
        """Copy the model and optimizer states for resetting after adaptation."""
        model_state = deepcopy(self.model.state_dict())
        optimizer_state = deepcopy(self.optimizer.state_dict())
        return model_state, optimizer_state

    def load_model_and_optimizer(self):
        """Restore the model and optimizer states from copies."""
        self.model.load_state_dict(self.model_state, strict=True)
        self.optimizer.load_state_dict(self.optimizer_state)

    def reset(self):
        if self.model_state is None or self.optimizer_state is None:
            raise Exception("cannot reset without saved model/optimizer state")
        self.load_model_and_optimizer()
        self.ema = None

    def configure_model(self):
        """Configure model for use with tent."""
        # train mode, because tent optimizes the model to minimize entropy
        self.model.train()
        # disable grad, to (re-)enable only what tent updates
        self.model.requires_grad_(False)
        # configure norm for tent updates: enable grad + force batch statisics
        for m in self.model.modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                m.requires_grad_(True)
                # force use of batch stats in train and eval modes
                m.track_running_stats = False
                m.running_mean = None
                m.running_var = None
            if isinstance(m, (nn.GroupNorm, nn.LayerNorm)):
                m.requires_grad_(True)

    def collect_params(self):
        """Collect the affine scale + shift parameters from norm layers.
        Walk the model's modules and collect all normalization parameters.
        Return the parameters and their names.
        Note: other choices of parameterization are possible!
        """
        params = []
        names = []
        for nm, m in self.model.named_modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):
                for np, p in m.named_parameters():
                    if np in ['weight', 'bias']:  # weight is scale, bias is shift
                        params.append(p)
                        names.append(f"{nm}.{np}")

        return params, names

    @torch.enable_grad()  # ensure grads in possible no grad context for testing
    def forward_and_adapt(self, data, targets, gt_mask):
        """Forward and adapt model on batch of data.
        Measure entropy of the model prediction, take gradients, and update params.
        Return:
        1. model outputs;
        2. the number of reliable and non-redundant samples;
        3. the number of reliable samples;
        4. the moving average  probability vector over all previous samples
        """
        # forward
        outputs = self.model(data)
        # adapt
        entropys = self.softmax_entropy(outputs)
        # filter unreliable samples
        filter_ids_1 = torch.where(entropys < self.e_margin)
        ids1 = filter_ids_1
        ids2 = torch.where(ids1[0]>-0.1)
        entropys = entropys[filter_ids_1]
        # filter redundant samples
        if self.current_model_probs is not None:
            cosine_similarities = F.cosine_similarity(self.current_model_probs.unsqueeze(dim=0), outputs[filter_ids_1].softmax(1), dim=1)
            filter_ids_2 = torch.where(torch.abs(cosine_similarities) < self.d_margin)
            entropys = entropys[filter_ids_2]
            ids2 = filter_ids_2
            updated_probs = self.update_model_probs(outputs[filter_ids_1][filter_ids_2].softmax(1))
        else:
            updated_probs = self.update_model_probs(outputs[filter_ids_1].softmax(1))
        coeff = 1 / (torch.exp(entropys.clone().detach() - self.e_margin))
        # implementation version 1, compute loss, all samples backward (some unselected are masked)
        entropys = entropys.mul(coeff) # reweight entropy losses for diff. samples
        loss = entropys.mean(0)
        """
        # implementation version 2, compute loss, forward all batch, forward and backward selected samples again.
        # loss = 0
        # if x[ids1][ids2].size(0) != 0:
        #     loss = softmax_entropy(model(x[ids1][ids2])).mul(coeff).mean(0) # reweight entropy losses for diff. samples
        """
        if self.fishers is not None:
            ewc_loss = 0
            for name, param in self.model.named_parameters():
                if name in self.fishers:
                    ewc_loss += self.fisher_alpha * (self.fishers[name][0] * (param - self.fishers[name][1])**2).sum()
            loss += ewc_loss

        # --- AL learning ---
        if self.config.atta.al_rate is not None and gt_mask.sum() > 0:
            loss += self.config.metric.loss_func(outputs[gt_mask], targets[gt_mask])

        if data[ids1][ids2].size(0) != 0:
            loss.backward()
            self.optimizer.step()
        self.optimizer.zero_grad()
        return outputs, entropys.size(0), filter_ids_1[0].size(0), updated_probs

    def update_model_probs(self, new_probs):
        if self.current_model_probs is None:
            if new_probs.size(0) == 0:
                return None
            else:
                with torch.no_grad():
                    return new_probs.mean(0)
        else:
            if new_probs.size(0) == 0:
                with torch.no_grad():
                    return self.current_model_probs
            else:
                with torch.no_grad():
                    return 0.9 * self.current_model_probs + (1 - 0.9) * new_probs.mean(0)



class ActualSequentialSampler(torch.utils.data.Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """

    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(self.data_source)

    def __len__(self):
        return len(self.data_source)

File Path: ATTA/kernel/algorithms/Entropy.py
Content:
import numpy as np
import torch
from torch.utils.data import ConcatDataset
# import models for resnet18

from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader
from ATTA.utils.config_reader import Conf
from ATTA.utils.register import register
from .Base import AlgBase

@register.alg_register
class Entropy(AlgBase):
    def __init__(self, config: Conf):
        super(Entropy, self).__init__(config)
        self.budgets = self.config.atta.budgets
        self.anchors = []
        self.buffer = []

    def __call__(self, *args, **kwargs):
        self.adapt()
        for env_id in self.config.dataset.test_envs:
            self.test_on_env(env_id)

    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if x.shape[1] == 1:
            x = torch.cat([x, -x], dim=1)
        return -(x.softmax(1) * x.log_softmax(1)).sum(1)

    @torch.no_grad()
    def adapt(self):
        idxs_lb = np.zeros(len(self.target_dataset), dtype=bool)
        for round in range(10):
            data_loader = torch.utils.data.DataLoader(self.target_dataset, sampler=ActualSequentialSampler(
                np.arange(len(self.target_dataset))),
                                                      num_workers=self.config.num_workers,
                                                      batch_size=self.config.train.train_bs, drop_last=False)
            self.model.eval()
            buffer = []
            for data, target in data_loader:
                data, target = data.to(self.config.device), target.to(self.config.device)
                feats = self.encoder(data)
                output = self.fc(feats)
                entropy = self.softmax_entropy(output)
                buffer.append((data, target, feats.cpu().numpy(), entropy.cpu().numpy()))
            data, target, feats, entropy = zip(*buffer)
            feats = np.concatenate(feats)[~idxs_lb]
            entropy = np.concatenate(entropy)[~idxs_lb]
            if round == 9:
                n_clusters = self.budgets - idxs_lb.sum()
            else:
                n_clusters = self.budgets // 10
            closest = entropy.argsort()[-n_clusters:]
            idxs_lb_id = idxs_lb.nonzero()[0]
            for idx in closest:
                true_idx = idx
                while true_idx != (idxs_lb_id <= true_idx).sum() + idx:
                    true_idx += 1
                assert not idxs_lb[true_idx]
                idxs_lb[true_idx] = True
            # print(f'closest: {closest}\nidxs_lb: {idxs_lb.nonzero()[0]}')

            anchor_loader = InfiniteDataLoader(self.target_dataset, weights=None,
                                               batch_size=self.config.train.train_bs,
                                               num_workers=self.config.num_workers, subset=np.nonzero(idxs_lb)[0])
            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)
            self.model.train()
            with torch.enable_grad():
                for i, (data, target) in enumerate(anchor_loader):
                    data, target = data.to(self.config.device), target.to(self.config.device)
                    optimizer.zero_grad()
                    output = self.fc(self.encoder(data))
                    loss = self.config.metric.loss_func(output, target)
                    loss.backward()
                    optimizer.step()
                    if i > self.config.atta.SimATTA.steps:
                        break


class ActualSequentialSampler(torch.utils.data.Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """

    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(self.data_source)

    def __len__(self):
        return len(self.data_source)

File Path: ATTA/kernel/algorithms/Kmeans.py
Content:
import numpy as np
import torch
from joblib import parallel_backend
from sklearn.cluster import KMeans
# from ATTA.utils.fast_pytorch_kmeans import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from torch.utils.data import ConcatDataset
# import models for resnet18

from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader
from ATTA.utils.config_reader import Conf
from ATTA.utils.register import register
from .Base import AlgBase

@register.alg_register
class Kmeans(AlgBase):
    def __init__(self, config: Conf):
        super(Kmeans, self).__init__(config)
        self.budgets = self.config.atta.budgets
        self.anchors = []
        self.buffer = []


    def __call__(self, *args, **kwargs):
        self.adapt()
        for env_id in self.config.dataset.test_envs:
            self.test_on_env(env_id)

    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if x.shape[1] == 1:
            x = torch.cat([x, -x], dim=1)
        return -(x.softmax(1) * x.log_softmax(1)).sum(1)

    @torch.no_grad()
    def adapt(self):
        idxs_lb = np.zeros(len(self.target_dataset), dtype=bool)
        for round in range(10):
            data_loader = torch.utils.data.DataLoader(self.target_dataset, sampler=ActualSequentialSampler(np.arange(len(self.target_dataset))),
                                                      num_workers=self.config.num_workers,
                                                      batch_size=self.config.train.train_bs, drop_last=False)
            self.model.eval()
            buffer = []
            for data, target in data_loader:
                data, target = data.to(self.config.device), target.to(self.config.device)
                feats = self.encoder(data)
                output = self.fc(feats)
                entropy = self.softmax_entropy(output)
                buffer.append((data, target, feats.cpu().numpy(), entropy.cpu().numpy()))
            data, target, feats, entropy = zip(*buffer)
            feats = np.concatenate(feats)[~idxs_lb]
            # entropy = np.concatenate(entropy)[~idxs_lb]
            if round == 9:
                n_clusters = self.budgets - idxs_lb.sum()
            else:
                n_clusters = self.budgets // 10
            with parallel_backend('threading', n_jobs=8):
                kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats)
                closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats)
            idxs_lb_id = idxs_lb.nonzero()[0]
            # print(f'closest: {closest}')
            for idx in closest:
                true_idx = idx
                while true_idx != (idxs_lb_id <= true_idx).sum() + idx:
                    true_idx += 1
                # assert not idxs_lb[true_idx], f'true_idx: {true_idx}, (idxs_lb_id < true_idx).sum(): {(idxs_lb_id < true_idx).sum()}, idx: {idx}'
                idxs_lb[true_idx] = True
            # print(f'idxs_lb: {idxs_lb.nonzero()[0]}')

            anchor_loader = InfiniteDataLoader(self.target_dataset, weights=None,
                                               batch_size=self.config.train.train_bs,
                                               num_workers=self.config.num_workers, subset=np.nonzero(idxs_lb)[0])
            self.model.train()
            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)
            print('Cluster train')
            with torch.enable_grad():
                for i, (data, target) in enumerate(anchor_loader):
                    data, target = data.to(self.config.device), target.to(self.config.device)
                    optimizer.zero_grad()
                    output = self.fc(self.encoder(data))
                    loss = self.config.metric.loss_func(output, target)
                    loss.backward()
                    optimizer.step()
                    if i > self.config.atta.SimATTA.steps:
                        break


class ActualSequentialSampler(torch.utils.data.Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """

    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(self.data_source)

    def __len__(self):
        return len(self.data_source)

File Path: ATTA/kernel/algorithms/Random.py
Content:
import numpy as np
import torch
from torch.utils.data import ConcatDataset
# import models for resnet18

from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader
from ATTA.utils.config_reader import Conf
from ATTA.utils.register import register
from .Base import AlgBase

@register.alg_register
class Random(AlgBase):
    def __init__(self, config: Conf):
        super(Random, self).__init__(config)
        self.budgets = self.config.atta.budgets
        self.anchors = []
        self.buffer = []


    def __call__(self, *args, **kwargs):
        self.adapt()
        for env_id in self.config.dataset.test_envs:
            self.test_on_env(env_id)

    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if x.shape[1] == 1:
            x = torch.cat([x, -x], dim=1)
        return -(x.softmax(1) * x.log_softmax(1)).sum(1)

    @torch.no_grad()
    def adapt(self):
        idxs_lb = np.zeros(len(self.target_dataset), dtype=bool)
        for round in range(10):
            if round == 9:
                n_clusters = self.budgets - idxs_lb.sum()
            else:
                n_clusters = self.budgets // 10
            closest = np.random.choice(np.nonzero(~idxs_lb)[0], n_clusters, replace=False)
            idxs_lb_id = idxs_lb.nonzero()[0]
            idxs_lb[closest] = True
            # print(f'closest: {closest}\nidxs_lb: {idxs_lb.nonzero()[0]}')

            anchor_loader = InfiniteDataLoader(self.target_dataset, weights=None,
                                               batch_size=self.config.train.train_bs,
                                               num_workers=self.config.num_workers, subset=np.nonzero(idxs_lb)[0])
            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)
            self.model.train()
            print('Cluster train')
            with torch.enable_grad():
                for i, (data, target) in enumerate(anchor_loader):
                    data, target = data.to(self.config.device), target.to(self.config.device)
                    optimizer.zero_grad()
                    output = self.fc(self.encoder(data))
                    loss = self.config.metric.loss_func(output, target)
                    loss.backward()
                    optimizer.step()
                    if i > self.config.atta.SimATTA.steps:
                        break


class ActualSequentialSampler(torch.utils.data.Sampler):
    r"""Samples elements sequentially, always in the same order.
    Arguments:
        data_source (Dataset): dataset to sample from
    """

    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(self.data_source)

    def __len__(self):
        return len(self.data_source)

File Path: ATTA/kernel/algorithms/SAR.py
Content:
from typing import Dict
from typing import Union

import math
import numpy as np
import torch
import torch.nn as nn
import torchvision.models
from torchvision import transforms
from torch.utils.data import DataLoader

from ATTA.utils.config_reader import Conf
from ATTA.utils.register import register
from copy import deepcopy
from torch import nn
import torch
import torch.nn.functional as F
from torch.distributions import Normal, kl_divergence
# import models for resnet18
from torchvision.models import resnet18
import itertools
import os
import ATTA.data.loaders.misc as misc
from ATTA import register
from ATTA.utils.config_reader import Conf
from ATTA.utils.config_reader import Conf
from ATTA.utils.initial import reset_random_seed
from ATTA.utils.initial import reset_random_seed
from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader
from torch.utils.data import TensorDataset, Subset
from .Base import AlgBase
import pandas as pd


@register.alg_register
class SAR(AlgBase):
    def __init__(self, config: Conf):
        super(SAR, self).__init__(config)
        num_classes = 2 if config.dataset.num_classes == 1 else config.dataset.num_classes
        self.margin_e0, self.reset_constant_em, self.ema = 0.4 * math.log(
            num_classes), config.atta.SAR.reset_constant_em, None

        print('#D#Config model')
        self.configure_model()
        params, param_names = self.collect_params()
        # print(f'#I#{param_names}')
        self.optimizer = SAM(params, torch.optim.SGD, lr=self.config.atta.SAR.lr, momentum=0.9)
        self.model_state, self.optimizer_state = \
            self.copy_model_and_optimizer()

    def __call__(self, *args, **kwargs):
        # super(SAR, self).__call__(*args, **kwargs)

        self.continue_result_df = pd.DataFrame(
            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)
        self.random_result_df = pd.DataFrame(
            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)

        for adapt_id in self.config.dataset.test_envs[1:]:
            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]

        self.__init__(self.config)
        for target_split_id in range(4):
            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader,
                                                                                           target_split_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]

        print(self.continue_result_df.round(4).to_markdown(), '\n')
        print(self.random_result_df.round(4).to_markdown())

    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if x.shape[1] == 1:
            x = torch.cat([x, -x], dim=1)
        return -(x.softmax(1) * x.log_softmax(1)).sum(1)

    # Active contrastive learning
    @torch.enable_grad()
    def adapt_on_env(self, loader, env_id):
        steps = self.config.atta.SAR.steps
        self.configure_model()
        acc = 0
        for data, targets in loader[env_id]:
            targets = targets.to(self.config.device)
            gt_mask = torch.rand(targets.shape[0], device=targets.device) < self.config.atta.al_rate
            for _ in range(steps):
                data = data.to(self.config.device)
                outputs, reset_flag = self.forward_and_adapt(data, targets, gt_mask)
                if reset_flag:
                    self.reset()
            acc += self.config.metric.score_func(targets, outputs) * len(data)
        acc /= len(loader[env_id].sampler)
        print(f'Env {env_id} real-time Acc.: {acc:.4f}')
        return acc

    def copy_model_and_optimizer(self):
        """Copy the model and optimizer states for resetting after adaptation."""
        model_state = deepcopy(self.model.state_dict())
        optimizer_state = deepcopy(self.optimizer.state_dict())
        return model_state, optimizer_state

    def load_model_and_optimizer(self):
        """Restore the model and optimizer states from copies."""
        self.model.load_state_dict(self.model_state, strict=True)
        self.optimizer.load_state_dict(self.optimizer_state)

    def reset(self):
        if self.model_state is None or self.optimizer_state is None:
            raise Exception("cannot reset without saved model/optimizer state")
        self.load_model_and_optimizer()
        self.ema = None

    def configure_model(self):
        """Configure model for use with tent."""
        # train mode, because tent optimizes the model to minimize entropy
        self.model.train()
        # disable grad, to (re-)enable only what tent updates
        self.model.requires_grad_(False)
        # configure norm for tent updates: enable grad + force batch statisics
        for m in self.model.modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                m.requires_grad_(True)
                # force use of batch stats in train and eval modes
                m.track_running_stats = False
                m.running_mean = None
                m.running_var = None
            if isinstance(m, (nn.GroupNorm, nn.LayerNorm)):
                m.requires_grad_(True)

    def collect_params(self):
        """Collect the affine scale + shift parameters from norm layers.
        Walk the model's modules and collect all normalization parameters.
        Return the parameters and their names.
        Note: other choices of parameterization are possible!
        """
        params = []
        names = []
        for nm, m in self.model.named_modules():
            if 'convs.2.nn' in nm or 'norms.2' in nm:
                continue
            # skip top layers for adaptation: layer4 for ResNets and blocks9-11 for Vit-Base
            if 'layer4' in nm:
                continue
            if 'blocks.9' in nm:
                continue
            if 'blocks.10' in nm:
                continue
            if 'blocks.11' in nm:
                continue
            if 'norm.' in nm:
                continue
            if nm in ['norm']:
                continue

            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):
                for np, p in m.named_parameters():
                    if np in ['weight', 'bias']:  # weight is scale, bias is shift
                        params.append(p)
                        names.append(f"{nm}.{np}")

        return params, names

    def update_ema(self, new_data):
        if self.ema is None:
            return new_data
        else:
            with torch.no_grad():
                return 0.9 * self.ema + (1 - 0.9) * new_data

    @torch.enable_grad()  # ensure grads in possible no grad context for testing
    def forward_and_adapt(self, data, targets, gt_mask):
        """Forward and adapt model input data.
        Measure entropy of the model prediction, take gradients, and update params.
        """
        self.optimizer.zero_grad()
        # forward
        outputs = self.model(data)
        # adapt
        # filtering reliable samples/gradients for further adaptation; first time forward
        entropys = self.softmax_entropy(outputs)
        filter_ids_1 = torch.where(entropys < self.margin_e0)
        entropys = entropys[filter_ids_1]
        loss = entropys.mean(0)

        # --- AL learning ---
        if self.config.atta.al_rate is not None and gt_mask.sum() > 0:
            loss += self.config.metric.loss_func(outputs[gt_mask], targets[gt_mask])


        loss.backward()

        self.optimizer.first_step(
            zero_grad=True)  # compute \hat{\epsilon(\Theta)} for first order approximation, Eqn. (4)
        entropys2 = self.softmax_entropy(self.model(data))
        entropys2 = entropys2[filter_ids_1]  # second time forward
        loss_second_value = entropys2.clone().detach().mean(0)
        filter_ids_2 = torch.where(
            entropys2 < self.margin_e0)  # here filtering reliable samples again, since model weights have been changed to \Theta+\hat{\epsilon(\Theta)}
        loss_second = entropys2[filter_ids_2].mean(0)
        if not np.isnan(loss_second.item()):
            self.ema = self.update_ema(loss_second.item())  # record moving average loss values for model recovery

        # second time backward, update model weights using gradients at \Theta+\hat{\epsilon(\Theta)}
        loss_second.backward()
        self.optimizer.second_step(zero_grad=True)

        # perform model recovery
        reset_flag = False
        if self.ema is not None:
            if self.ema < self.reset_constant_em:
                print(f"ema < {self.reset_constant_em}, now reset the model")
                reset_flag = True

        return outputs, reset_flag


class SAM(torch.optim.Optimizer):
    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):
        assert rho >= 0.0, f"Invalid rho, should be non-negative: {rho}"

        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)
        super(SAM, self).__init__(params, defaults)

        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)
        self.param_groups = self.base_optimizer.param_groups
        self.defaults.update(self.base_optimizer.defaults)

    @torch.no_grad()
    def first_step(self, zero_grad=False):
        grad_norm = self._grad_norm()
        for group in self.param_groups:
            scale = group["rho"] / (grad_norm + 1e-12)

            for p in group["params"]:
                if p.grad is None: continue
                self.state[p]["old_p"] = p.data.clone()
                e_w = (torch.pow(p, 2) if group["adaptive"] else 1.0) * p.grad * scale.to(p)
                p.add_(e_w)  # climb to the local maximum "w + e(w)"

        if zero_grad: self.zero_grad()

    @torch.no_grad()
    def second_step(self, zero_grad=False):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None: continue
                p.data = self.state[p]["old_p"]  # get back to "w" from "w + e(w)"

        self.base_optimizer.step()  # do the actual "sharpness-aware" update

        if zero_grad: self.zero_grad()

    @torch.no_grad()
    def step(self, closure=None):
        assert closure is not None, "Sharpness Aware Minimization requires closure, but it was not provided"
        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass

        self.first_step(zero_grad=True)
        closure()
        self.second_step()

    def _grad_norm(self):
        shared_device = self.param_groups[0]["params"][
            0].device  # put everything on the same device, in case of model parallelism
        norm = torch.norm(
            torch.stack([
                ((torch.abs(p) if group["adaptive"] else 1.0) * p.grad).norm(p=2).to(shared_device)
                for group in self.param_groups for p in group["params"]
                if p.grad is not None
            ]),
            p=2
        )
        return norm

    def load_state_dict(self, state_dict):
        super().load_state_dict(state_dict)
        self.base_optimizer.param_groups = self.param_groups

File Path: ATTA/kernel/algorithms/SimATTA.py
Content:
import copy
import pathlib
import time
from typing import Union

import numpy as np
# from sklearnex import patch_sklearn, config_context
# patch_sklearn()

# from sklearn.cluster import KMeans
# from ATTA.utils.fast_pytorch_kmeans import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from typing import Literal

from torch import nn
import torch
# import models for resnet18
from munch import Munch
from ATTA import register
from ATTA.utils.config_reader import Conf
from ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader
from torch.utils.data import TensorDataset
from tqdm import tqdm
from .Base import AlgBase
import pandas as pd
from ATTA.definitions import STORAGE_DIR



@register.alg_register
class SimATTA(AlgBase):
    def __init__(self, config: Conf):
        super(SimATTA, self).__init__(config)

        self.teacher = copy.deepcopy(self.model.to('cpu'))

        self.model.to(config.device)
        self.teacher.to(config.device)
        self.update_teacher(0)  # copy student to teacher

        self.budgets = 0
        self.anchors = None
        self.source_anchors = None
        self.buffer = []
        self.n_clusters = 10
        self.nc_increase = self.config.atta.SimATTA.nc_increase
        self.source_n_clusters = 100

        self.cold_start = self.config.atta.SimATTA.cold_start

        self.consistency_weight = 0
        self.alpha_teacher = 0
        self.accumulate_weight = True
        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'
        self.aggressive = True
        self.beta = self.config.atta.SimATTA.beta
        self.alpha = 0.2

        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False
        self.LE = True if self.config.atta.SimATTA.LE else False
        self.vis_round = 0


    def __call__(self, *args, **kwargs):
        # super(SimATTA, self).__call__()
        self.continue_result_df = pd.DataFrame(
            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)
        self.random_result_df = pd.DataFrame(
            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)

        self.enable_bn(self.model)
        if 'ImageNet' not in self.config.dataset.name:
            for env_id in self.config.dataset.test_envs:
                acc = self.test_on_env(env_id)[1]
                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc
                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc

        for adapt_id in self.config.dataset.test_envs[1:]:
            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)
            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets
            print(self.budgets)
            if 'ImageNet' not in self.config.dataset.name:
                for env_id in self.config.dataset.test_envs:
                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]

        self.__init__(self.config)
        for target_split_id in range(4):
            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)
            self.random_result_df.loc['Budgets', target_split_id] = self.budgets
            print(self.budgets)
            if 'ImageNet' not in self.config.dataset.name:
                for env_id in self.config.dataset.test_envs:
                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]

        print(f'#IM#\n{self.continue_result_df.round(4).to_markdown()}\n'
              f'{self.random_result_df.round(4).to_markdown()}')
        # print(self.random_result_df.round(4).to_markdown(), '\n')
        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')
        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')


    @torch.no_grad()
    def val_anchor(self, loader):
        self.model.eval()
        val_loss = 0
        val_acc = 0
        for data, target in loader:
            data, target = data.to(self.config.device), target.to(self.config.device)
            output = self.fc(self.encoder(data))
            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()
            val_acc += self.config.metric.score_func(target, output) * len(data)
        val_loss /= len(loader.sampler)
        val_acc /= len(loader.sampler)
        return val_loss, val_acc

    def update_teacher(self, alpha_teacher):  # , iteration):
        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):
            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]
        if not self.config.model.freeze_bn:
            for tm, m in zip(self.teacher.modules(), self.model.modules()):
                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean
                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var

    @torch.enable_grad()
    def cluster_train(self, target_anchors, source_anchors):
        self.model.train()

        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,
                                           batch_size=self.config.train.train_bs,
                                           num_workers=self.config.num_workers)
        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,
                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)
        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())
        if source_anchors.num_elem() < self.cold_start:
            alpha = min(0.2, alpha)

        ST_loader = iter(zip(source_loader, target_loader))
        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,
                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)
        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)
        # print('Cluster train')
        delay_break = False
        loss_window = []
        tol = 0
        lowest_loss = float('inf')
        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):
            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)
            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)
            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)
            # self.update_teacher(self.alpha_teacher)
            if len(loss_window) < self.config.atta.SimATTA.stop_tol:
                loss_window.append(L_T.item())
            else:
                mean_loss = np.mean(loss_window)
                tol += 1
                if mean_loss < lowest_loss:
                    lowest_loss = mean_loss
                    tol = 0
                if tol > 5:
                    break
                loss_window = []
            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:
                if i > self.config.atta.SimATTA.steps:
                    break


    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):
        # print('one step train')
        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)
        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)
        loss = (1 - alpha) * L_S + alpha * L_T
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        return L_T

    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if y is None:
            if x.shape[1] == 1:
                x = torch.cat([x, -x], dim=1)
            return -(x.softmax(1) * x.log_softmax(1)).sum(1)
        else:
            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)

    def update_anchors(self, anchors, data, target, feats, weight):
        if anchors is None:
            anchors = Munch()
            anchors.data = data
            anchors.target = target
            anchors.feats = feats
            anchors.weight = weight
            anchors.num_elem = lambda: len(anchors.data)
        else:
            anchors.data = torch.cat([anchors.data, data])
            anchors.target = torch.cat([anchors.target, target])
            anchors.feats = torch.cat([anchors.feats, feats])
            anchors.weight = torch.cat([anchors.weight, weight])
        return anchors

    def update_anchors_feats(self, anchors):
        # sequential_data = torch.arange(200)[:, None]
        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,
                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)

        anchors.feats = None
        self.model.eval()
        for data in anchors_loader:
            # print(data)
            data = data[0].to(self.config.device)
            if anchors.feats is None:
                anchors.feats = self.model[0](data).cpu().detach()
            else:
                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])

        return anchors

    @torch.no_grad()
    def adapt_on_env(self, loader, env_id):
        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)
        acc = 0
        for data, target in tqdm(loader[env_id]):
            data, target = data.to(self.config.device), target.to(self.config.device)
            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)
            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]
            if self.LE:
                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,
                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)
            else:
                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)
            if not self.target_cluster:
                self.n_clusters = 0
            self.source_n_clusters = 100

            self.budgets += len(closest)
            self.n_clusters += self.nc_increase
            self.source_n_clusters += 1

            print(self.anchors.num_elem(), self.source_anchors.num_elem())
            if self.source_anchors.num_elem() > 0:
                self.cluster_train(self.anchors, self.source_anchors)
            else:
                self.cluster_train(self.anchors, self.anchors)
            self.anchors = self.update_anchors_feats(self.anchors)
        acc /= len(loader[env_id].sampler)
        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')
        return acc

    @torch.no_grad()
    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):
        model.eval()
        feats = model[0](data)
        outputs = model[1](feats)
        pseudo_label = outputs.argmax(1).cpu().detach()
        data = data.cpu().detach()
        feats = feats.cpu().detach()
        target = target.cpu().detach()
        entropy = self.softmax_entropy(outputs).cpu()
        if not incremental_cluster:
            entropy = entropy.numpy()
            if ent_beta == 0:
                closest = np.argsort(entropy)[: n_clusters]
                closest = closest[entropy[closest] < ent_bound]
            elif ent_beta == 1:
                closest = np.argsort(entropy)[- n_clusters:]
                closest = closest[entropy[closest] >= ent_bound]
            else:
                raise NotImplementedError
            weights = torch.zeros(len(closest), dtype=torch.float)
        else:
            if ent_beta == 0:
                sample_choice = entropy < ent_bound
            elif ent_beta == 1:
                sample_choice = entropy >= ent_bound
            else:
                raise NotImplementedError

            data = data[sample_choice]
            target = target[sample_choice]
            feats = feats[sample_choice]
            pseudo_label = pseudo_label[sample_choice]

            if anchors:
                feats4cluster = torch.cat([anchors.feats, feats])
                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])
            else:
                feats4cluster = feats
                sample_weight = torch.ones(len(feats), dtype=torch.float)

            if self.config.atta.gpu_clustering:
                from ATTA.utils.fast_pytorch_kmeans import KMeans
                from joblib import parallel_backend
                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(
                    feats4cluster.to(self.config.device),
                    sample_weight=sample_weight.to(self.config.device))
                with parallel_backend('threading', n_jobs=8):
                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)
                kmeans_labels = kmeans.labels_
            # elif self.config.atta.gpu_clustering == 'jax':
            #     from ott.tools.k_means import k_means as KMeans
            #     import jax
            #     import jax.numpy as jnp
            #     tik = time.time()
            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)
            #     mit = time.time()
            #     print(f'#IN#Kmeans time: {mit - tik}')
            #     @jax.jit
            #     def jax_pairwise_distances_argmin(c, feats):
            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())
            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))
            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)
            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))
            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')
            #     kmeans_labels = np.array(kmeans.assignment)
            else:
                from joblib import parallel_backend
                from sklearn.cluster import KMeans
                with parallel_backend('threading', n_jobs=8):
                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,
                                                                                                  sample_weight=sample_weight)
                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)
                kmeans_labels = kmeans.labels_



            if anchors:
                num_anchors = anchors.num_elem()
                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)

                if self.accumulate_weight:
                    # previous anchor weight accumulation
                    # Average the weight of the previous anchor if sharing the same cluster
                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)
                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)
                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \
                    num_prev_anchors_per_cluster[1]

                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)
                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)
                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]
                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0
                    # accumulate the weight of the previous anchor
                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \
                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()

                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,
                                                                                                    prev_anchor_cluster.unique().long(),
                                                                                                    True)
                new_cluster_mask = ~ anchored_cluster_mask

                closest = raw_closest[new_cluster_mask] - num_anchors
                if (closest < 0).sum() != 0:
                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.
                    print('new_cluster_mask: ', new_cluster_mask)
                    new_cluster_mask = torch.where(new_cluster_mask)[0]
                    print('new_cluster_mask: ', new_cluster_mask)
                    print(closest)
                    print(closest >= 0)
                    new_cluster_mask = new_cluster_mask[closest >= 0]
                    closest = closest[closest >= 0]


                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]
            else:
                num_anchors = 0
                closest = raw_closest
                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]

        if use_pseudo_label:
            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)
        else:
            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)

        return outputs, closest, anchors

    def enable_bn(self, model):
        if not self.config.model.freeze_bn:
            for m in model.modules():
                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                    m.momentum = 0.1



File Path: ATTA/kernel/algorithms/__init__.py
Content:
import glob
from os.path import dirname, basename, isfile, join

modules = glob.glob(join(dirname(__file__), "*.py"))
__all__ = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]

from . import *
File Path: ATTA/kernel/algorithms/tent.py
Content:
from torch import nn
import torch
# import models for resnet18
from ATTA import register
from ATTA.utils.config_reader import Conf
from .Base import AlgBase
import pandas as pd

@register.alg_register
class Tent(AlgBase):
    def __init__(self, config: Conf):
        super(Tent, self).__init__(config)
        print('#D#Config model')
        self.configure_model()
        params, param_names = self.collect_params()
        print(f'#I#{param_names}')
        self.optimizer = torch.optim.SGD(params, lr=self.config.atta.Tent.lr, momentum=0.9)

    def __call__(self, *args, **kwargs):
        # super(Tent, self).__call__(*args, **kwargs)

        self.continue_result_df = pd.DataFrame(
            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)
        self.random_result_df = pd.DataFrame(
            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],
            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)

        for adapt_id in self.config.dataset.test_envs[1:]:
            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]

        self.__init__(self.config)
        for target_split_id in range(4):
            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)
            # for env_id in self.config.dataset.test_envs:
            #     self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]

        print(self.continue_result_df.round(4).to_markdown(), '\n')
        print(self.random_result_df.round(4).to_markdown())




    def softmax_entropy(self, x: torch.Tensor) -> torch.Tensor:
        """Entropy of softmax distribution from logits."""
        if x.shape[1] == 1:
            x = torch.cat([x, -x], dim=1)
        return -(x.softmax(1) * x.log_softmax(1)).sum(1)

    # Active contrastive learning
    @torch.enable_grad()
    def adapt_on_env(self, loader, env_id):
        steps = self.config.atta.Tent.steps
        self.configure_model()
        acc = 0
        for data, targets in loader[env_id]:
            targets = targets.to(self.config.device)
            if self.config.atta.al_rate is not None:
                gt_mask = torch.rand(targets.shape[0], device=targets.device) < self.config.atta.al_rate
            for _ in range(steps):
                data = data.to(self.config.device)
                outputs = self.model(data)
                loss = self.softmax_entropy(outputs).mean(0)

                # --- AL learning ---
                if self.config.atta.al_rate is not None and gt_mask.sum() > 0:
                    loss += self.config.metric.loss_func(outputs[gt_mask], targets[gt_mask])

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                # for env_id in self.config.dataset.test_envs:
                #     self.test_on_env(env_id)
            acc += self.config.metric.score_func(targets, outputs) * len(data)
        acc /= len(loader[env_id].sampler)
        print(f'Env {env_id} real-time Acc.: {acc:.4f}')
        return acc

    def configure_model(self):
        """Configure model for use with tent."""
        # train mode, because tent optimizes the model to minimize entropy
        self.model.train()
        # disable grad, to (re-)enable only what tent updates
        self.model.requires_grad_(False)
        # configure norm for tent updates: enable grad + force batch statisics
        for m in self.model.modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):
                m.requires_grad_(True)
                # force use of batch stats in train and eval modes
                m.track_running_stats = False
                m.running_mean = None
                m.running_var = None
            if isinstance(m, (nn.GroupNorm, nn.LayerNorm)):
                m.requires_grad_(True)

    def collect_params(self):
        """Collect the affine scale + shift parameters from batch norms.
        Walk the model's modules and collect all batch normalization parameters.
        Return the parameters and their names.
        Note: other choices of parameterization are possible!
        """
        params = []
        names = []
        for nm, m in self.model.named_modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm)):
                for np, p in m.named_parameters():
                    if np in ['weight', 'bias']:  # weight is scale, bias is shift
                        params.append(p)
                        names.append(f"{nm}.{np}")
        return params, names

File Path: ATTA/kernel/launch.py
Content:
import itertools
import os
import os.path
import sys
from pathlib import Path

from ATTA.definitions import ROOT_DIR
from ATTA.utils.load_manager import load_launcher
from ATTA.utils.args import AutoArgs
from ATTA.utils.config_reader import load_config


def launch():
    conda_interpreter = sys.executable
    conda_goodtg = os.path.join(sys.exec_prefix, 'bin', 'attatg')
    auto_args = AutoArgs().parse_args(known_only=True)
    auto_args.config_root = get_config_root(auto_args)

    jobs_group = make_list_cmds(auto_args, conda_goodtg)
    launcher = load_launcher(auto_args.launcher)
    launcher(jobs_group, auto_args)


def get_config_root(auto_args):
    if auto_args.config_root:
        if os.path.isabs(auto_args.config_root):
            config_root = Path(auto_args.config_root)
        else:
            config_root = Path(ROOT_DIR, 'configs', auto_args.config_root)
    else:
        config_root = Path(ROOT_DIR, 'configs', 'TTA_configs')
    return config_root


def make_list_cmds(auto_args, conda_goodtg):
    args_group = [
        f'{conda_goodtg} --task train --config_path {auto_args.config_root / dataset / "SimATTA.yaml"} --atta.SimATTA.cold_start 100 ' \
        f'--atta.SimATTA.el {el} --atta.SimATTA.nc_increase {k} --atta.gpu_clustering --exp_round 1 --atta.SimATTA.LE {le} ' \
        f'--atta.SimATTA.target_cluster {ic} --log_file SimATTA_{dataset}_LE{le}_IC{ic}_k{k}_el{el} --num_workers 4 '
        for dataset, el in [('VLCS', 1e-3)] # ('PACS', 1e-4),
        for k in [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3]
        for le in [0, 1]
        for ic in [0, 1] if not (ic == 0 and k % 1 != 0)]

    return args_group


if __name__ == '__main__':
    launch()

File Path: ATTA/kernel/launchers/__init__.py
Content:
r"""
This module includes Benchmarking launchers.
"""

import glob
from os.path import dirname, basename, isfile, join

modules = glob.glob(join(dirname(__file__), "*.py"))
__all__ = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]

from . import *
File Path: ATTA/kernel/launchers/ada_launcher.py
Content:
import subprocess
import os
import shlex
import subprocess
import time

import psutil
import pynvml
from tqdm import trange

from ATTA import register
from ATTA.definitions import OOM_CODE
from .basic_launcher import Launcher
from threading import Thread


@register.launcher_register
class AdaLauncher(Launcher):
    def __init__(self):
        super(AdaLauncher, self).__init__()
        self.initial_aggressive = 80
        self.cpu_use_limit = 90
        self.ram_use_limit = 80
        self.cpu_max_wait = 120
        self.gpu_use_limit = 90
        self.num_process_limit = 1

        self.summary_string = ''
        self.allow_devices = []

    def __call__(self, jobs_group, auto_args):
        jobs_group = super(AdaLauncher, self).__call__(jobs_group, auto_args)
        pynvml.nvmlInit()
        print("Driver Version:", pynvml.nvmlSystemGetDriverVersion())
        device_count = pynvml.nvmlDeviceGetCount()
        print("Number of devices:", device_count)
        handles = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in range(device_count)]
        handles = list(enumerate(handles))

        self.allow_devices = auto_args.allow_devices
        _thread = Thread(target=self.change_device, daemon=True).start()

        aggressive = {cmd_args: self.initial_aggressive for cmd_args in jobs_group}
        total_num_args = jobs_group.__len__()

        process_pool = {}
        jobs_status = {'done': [], 'failed': []}
        while jobs_group:
            # process_pool: in progress process (this program thinks)
            # jobs_group: processes that didn't finish successfully
            #   including in progress Process and failure processes
            #   when it is empty: all processes finished

            # --- Process emit ---
            summary_leave = False
            for check_count, cmd_args in enumerate(jobs_group):
                self.summary_string = f'Waiting: {len(jobs_group) - len(process_pool.keys())} - In progress: {len(process_pool.keys())} ' \
                                 f'- Finished: {len(jobs_status["done"])} - Failed: {len(jobs_status["failed"])}'

                if cmd_args in process_pool.keys():
                    continue

                # --- check cpu usage ---
                wait_count = self.wait_cpu()
                if wait_count >= self.cpu_max_wait:
                    print(f'\r{self.summary_string}| Wait too long, check process.', end='')
                    break

                print(f'\r{self.summary_string}| CPU/RAM available.', end='')

                for cur_i, (device_idx, device_handle) in enumerate(handles):
                    meminfo, usageinfo = self.get_gpu_info(device_handle, device_idx)
                    # fork children

                    if device_idx in self.allow_devices \
                            and meminfo < aggressive[cmd_args] \
                            and usageinfo < self.gpu_use_limit \
                            and len(process_pool.keys()) < self.num_process_limit:
                        print(f'\n\033[1;34mEmit\033[0m process on device:{device_idx}:\n{cmd_args}')

                        process = subprocess.Popen(shlex.split(cmd_args) + ['--gpu_idx', f'{device_idx}'],
                                                   close_fds=True,
                                                   stdout=open(os.devnull, 'w'),
                                                   stderr=open(os.devnull, 'w'),
                                                   cwd=os.getcwd(),
                                                   env=os.environ,
                                                   start_new_session=False)
                        process_pool[cmd_args] = process

                        for _ in trange(10, desc=f'{self.summary_string}| Interval...', leave=False):
                            time.sleep(1)
                        break
                handles.append(handles.pop(cur_i))
                if (check_count + 1) % 50 == 0 or len(process_pool.keys()) >= self.num_process_limit:
                    break

            # --- Process check ---
            ceased_processes = []
            ceased_exist = False
            for cmd_args, process in process_pool.items():

                return_code = process.poll()
                if return_code is not None:
                    if not ceased_exist:
                        print('')
                        ceased_exist = True

                    # process ceased
                    ceased_processes.append(cmd_args)

                    if return_code == 0:
                        print(f'\033[1;32mFinished\033[0m:{cmd_args}')
                        jobs_group.remove(cmd_args)
                        jobs_status['done'].append(cmd_args)
                    elif return_code == OOM_CODE:
                        if aggressive[cmd_args] > 15:
                            aggressive[cmd_args] -= 10
                            print(
                                f'\033[1;33mAbort\033[0m process:{cmd_args} due to CUDA out of memory. [decrease aggressive: {aggressive[cmd_args]}]')
                        else:
                            print(
                                f'\033[1;31mAbort\033[0m process:{cmd_args} due to CUDA memory not enough. Return code: {return_code}')
                            jobs_group.remove(cmd_args)
                            jobs_status['failed'].append(cmd_args)
                    else:
                        print(
                            f'\033[1;31mAbort\033[0m process:{cmd_args} due to other issues. Return code: {return_code}')
                        jobs_group.remove(cmd_args)
                        jobs_status['failed'].append(cmd_args)

            for ceased_process in ceased_processes:
                process_pool.pop(ceased_process)

            # --- Temporary summary ---
            self.summary_string = f'Waiting: {len(jobs_group) - len(process_pool.keys())} - In progress: {len(process_pool.keys())} ' \
                             f'- Finished: {len(jobs_status["done"])} - Failed: {len(jobs_status["failed"])}'
            for _ in trange(20, desc=f'{self.summary_string}| Waiting for emit...', leave=summary_leave):
                time.sleep(1)

    def wait_cpu(self):
        cpu_percent = psutil.cpu_percent()
        ram_percent = psutil.virtual_memory().percent
        wait_count = 0
        available_count = 0
        while wait_count < self.cpu_max_wait:
            if cpu_percent < self.cpu_use_limit and ram_percent < self.ram_use_limit:
                available_count += 1
            else:
                available_count = 0
            if available_count >= 2:
                break
            print(f'\r{self.summary_string}| Waiting for cpu/ram: {cpu_percent}/{ram_percent}', end='')
            time.sleep(1)
            cpu_percent = psutil.cpu_percent()
            ram_percent = psutil.virtual_memory().percent
            wait_count += 1
        return wait_count

    def get_gpu_info(self, device_handle, device_idx):
        meminfo = pynvml.nvmlDeviceGetMemoryInfo(device_handle)
        meminfo = meminfo.used / meminfo.total * 100
        usageinfo = []
        for _ in range(5):
            usageinfo.append(pynvml.nvmlDeviceGetUtilizationRates(device_handle).gpu)
            time.sleep(0.1)
        usageinfo = max(usageinfo)
        print(f'\r{self.summary_string}| Try device {device_idx} usage/mem: {usageinfo}/{meminfo:.2f}', end='')
        return meminfo, usageinfo

    def change_device(self):
        while True:
            command = input()
            op = ''
            if command.startswith(':+'):
                device_no = int(command.strip(':+'))
                op = 'add'
            elif command.startswith(':-'):
                device_no = int(command.strip(':-'))
                op = 'remove'
            else:
                print(f'Invalid command {command}')
                continue
            if isinstance(device_no, int) and 0 <= device_no <= 9:
                assert op != ''
                if op == 'add':
                    if device_no not in self.allow_devices:
                        self.allow_devices.append(device_no)
                    else:
                        print(f'Device {device_no} is already in the queue.')
                elif op == 'remove':
                    if device_no in self.allow_devices:
                        self.allow_devices.remove(device_no)
                    else:
                        print(f'Device {device_no} is not in the queue.')
                self.allow_devices.sort()
                print(f'Allowed device: {self.allow_devices}.')
            else:
                print(f'Invalid device number: {device_no}')


File Path: ATTA/kernel/launchers/basic_launcher.py
Content:
import os
import shlex
from multiprocessing import Pool

from ATTA import config_summoner
from ATTA import register
from ATTA.utils.args import args_parser


@register.launcher_register
class Launcher:
    def __init__(self):
        super(Launcher, self).__init__()

    def __call__(self, jobs_group, auto_args):
        ready_jobs_group = []
        with Pool(20) as pool:
            read_results = pool.map(self.log_reader, jobs_group)
        for cmd_args, last_line in read_results:
            if last_line.startswith('INFO: ChartInfo'):
                print(cmd_args, '\033[1;32m[DONE]\033[0m')
            else:
                print(cmd_args, '\033[1;33m[READY]\033[0m')
                ready_jobs_group.append(cmd_args)

        ans = input(f'Launch unfinished {len(ready_jobs_group)} jobs or all {len(jobs_group)} jobs? [u/a]')
        while ans != 'u' and ans != 'a':
            ans = input(f'Invalid input: {ans}. Please answer u or a.')
        if ans == 'u':
            jobs_group = ready_jobs_group
        elif ans == 'a':
            pass
        else:
            raise ValueError(f'Unexpected value {ans}.')

        ans = input(f'Sure to launch {len(jobs_group)} jobs? [y/n]')
        while ans != 'y' and ans != 'n':
            ans = input(f'Invalid input: {ans}. Please answer y or n.')
        if ans == 'y':
            return jobs_group
        elif ans == 'n':
            print(f'See you later. :)')
            exit(0)
        else:
            raise ValueError(f'Unexpected value {ans}.')

    def log_reader(self, cmd_args):
        args = args_parser(shlex.split(cmd_args)[1:])
        config = config_summoner(args)
        last_line = self.harvest(config.log_path)
        return cmd_args, last_line

    def harvest(self, log_path):
        try:
            with open(log_path, 'rb') as f:
                try:  # catch OSError in case of a one line file
                    f.seek(-2, os.SEEK_END)
                    while f.read(1) != b'\n':
                        f.seek(-2, os.SEEK_CUR)
                except OSError:
                    f.seek(0)
                last_line = f.readline().decode()
                return last_line
        except FileNotFoundError:
            return 'FileNotFoundError'

File Path: ATTA/kernel/launchers/harvest_launcher.py
Content:
import copy
import os
import shlex
import shutil
from pathlib import Path
import distutils.dir_util

import numpy as np
from ruamel.yaml import YAML
from tqdm import tqdm

from ATTA import config_summoner
from ATTA import register
from ATTA.definitions import ROOT_DIR
from ATTA.utils.args import AutoArgs
from ATTA.utils.args import args_parser
from ATTA.utils.config_reader import load_config, args2config, merge_dicts
from .basic_launcher import Launcher


@register.launcher_register
class HarvestLauncher(Launcher):
    def __init__(self):
        super(HarvestLauncher, self).__init__()
        self.watch = True
        self.pick_reference = [-1]

    def __call__(self, jobs_group, auto_args: AutoArgs):
        result_dict = self.harvest_all_fruits(jobs_group)
        best_fruits = self.picky_farmer(result_dict)

        if auto_args.sweep_root:
            self.process_final_root(auto_args)
            self.update_best_config(auto_args, best_fruits)

    def update_best_config(self, auto_args, best_fruits):
        for ddsa_key in best_fruits.keys():
            final_path = (auto_args.final_root / '/'.join(ddsa_key.split(' '))).with_suffix('.yaml')
            top_config, _, _ = load_config(final_path, skip_include=True)
            whole_config, _, _ = load_config(final_path)
            args_list = shlex.split(best_fruits[ddsa_key][0])
            args = args_parser(['--config_path', str(final_path)] + args_list)
            args2config(whole_config, args)
            args_keys = [item[2:] for item in args_list if item.startswith('--')]
            # print(args_keys)
            modified_config = self.filter_config(whole_config, args_keys)
            # print(top_config)
            # print(modified_config)
            final_top_config, _ = merge_dicts(top_config, modified_config)
            # print(final_top_config)
            yaml = YAML()
            yaml.indent(offset=2)
            # yaml.dump(final_top_config, sys.stdout)
            yaml.dump(final_top_config, final_path)

    def process_final_root(self, auto_args):
        if auto_args.final_root is None:
            auto_args.final_root = auto_args.config_root
        else:
            if os.path.isabs(auto_args.final_root):
                auto_args.final_root = Path(auto_args.final_root)
            else:
                auto_args.final_root = Path(ROOT_DIR, 'configs', auto_args.final_root)
        if auto_args.final_root.exists():
            ans = input(f'Overwrite {auto_args.final_root} by {auto_args.config_root}? [y/n]')
            while ans != 'y' and ans != 'n':
                ans = input(f'Invalid input: {ans}. Please answer y or n.')
            if ans == 'y':
                distutils.dir_util.copy_tree(str(auto_args.config_root), str(auto_args.final_root))
            elif ans == 'n':
                pass
            else:
                raise ValueError(f'Unexpected value {ans}.')
        else:
            shutil.copytree(auto_args.config_root, auto_args.final_root)

    def picky_farmer(self, result_dict):
        best_fruits = dict()
        sorted_fruits = dict()
        for ddsa_key in result_dict.keys():
            for key, value in result_dict[ddsa_key].items():
                result_dict[ddsa_key][key] = np.stack([np.mean(value, axis=1), np.std(value, axis=1)], axis=1)
            # lambda x: x[1][?, 0]  - ? denotes the result used to choose the best setting.
            if self.watch:
                sorted_fruits[ddsa_key] = sorted(list(result_dict[ddsa_key].items()), key=lambda x: sum(x[1][i, 0] for i in self.pick_reference), reverse=True)
            else:
                best_fruits[ddsa_key] = max(list(result_dict[ddsa_key].items()), key=lambda x: sum(x[1][i, 0] for i in self.pick_reference))
            # best_fruits[ddsa_key] = sorted_fruits[ddsa_key][0]
        if self.watch:
            print(sorted_fruits)
            exit(0)
        print(best_fruits)
        return best_fruits

    def filter_config(self, config: dict, target_keys):
        new_config = copy.deepcopy(config)
        for key in config.keys():
            if type(config[key]) is dict:
                new_config[key] = self.filter_config(config[key], target_keys)
                if not new_config[key]:
                    new_config.pop(key)
            else:
                if key not in target_keys:
                    new_config.pop(key)
        return new_config

    def harvest_all_fruits(self, jobs_group):
        all_finished = True
        result_dict = dict()
        for cmd_args in tqdm(jobs_group, desc='Harvesting ^_^'):
            args = args_parser(shlex.split(cmd_args)[1:])
            config = config_summoner(args)
            last_line = self.harvest(config.log_path)

            if not last_line.startswith('INFO: ChartInfo'):
                print(cmd_args, 'Unfinished')
                all_finished = False
                continue
            result = last_line.split(' ')[2:]
            num_result = len(result)
            key_args = shlex.split(cmd_args)[1:]
            round_index = key_args.index('--exp_round')
            key_args = key_args[:round_index] + key_args[round_index + 2:]

            config_path_index = key_args.index('--config_path')
            key_args.pop(config_path_index)  # Remove --config_path
            config_path = Path(key_args.pop(config_path_index))  # Remove and save its value
            config_path_parents = config_path.parents
            dataset, domain, shift, algorithm = config_path_parents[2].stem, config_path_parents[1].stem, \
                                                config_path_parents[0].stem, config_path.stem
            ddsa_key = ' '.join([dataset, domain, shift, algorithm])
            if ddsa_key not in result_dict.keys():
                result_dict[ddsa_key] = dict()

            key_str = ' '.join(key_args)
            if key_str not in result_dict[ddsa_key].keys():
                result_dict[ddsa_key][key_str] = [[] for _ in range(num_result)]
            # print(f'{ddsa_key}_{key_str}: {result}')
            result_dict[ddsa_key][key_str] = [r + [eval(result[i])] for i, r in
                                              enumerate(result_dict[ddsa_key][key_str])]
        # if not all_finished:
        #     print('Please launch unfinished jobs using other launchers before harvesting.')
        #     exit(1)
        return result_dict


File Path: ATTA/kernel/launchers/multi_launcher.py
Content:
import shlex
import subprocess
import time

from ATTA import register
from .basic_launcher import Launcher


@register.launcher_register
class MultiLauncher(Launcher):
    def __init__(self):
        super(MultiLauncher, self).__init__()

    def __call__(self, jobs_group, auto_args):
        jobs_group = super(MultiLauncher, self).__call__(jobs_group, auto_args)
        procs_by_gpu = [None] * len(auto_args.allow_devices)

        while len(jobs_group) > 0:
            for idx, gpu_idx in enumerate(auto_args.allow_devices):
                proc = procs_by_gpu[idx]
                if (proc is None) or (proc.poll() is not None):
                    # No process or a process has just finished
                    cmd_args = jobs_group.pop(0)
                    new_proc = subprocess.Popen(shlex.split(cmd_args) + ['--gpu_idx', f'{gpu_idx}'],
                                                close_fds=True,
                                                stdout=open('debug_out.log', 'a'), stderr=open('debug_error.log', 'a'),
                                                start_new_session=False)
                    procs_by_gpu[idx] = new_proc
                    break
            time.sleep(1)
            print(f'\rWaiting jobs: {len(jobs_group)}', end='')

        for p in procs_by_gpu:
            if p is not None:
                p.wait()

File Path: ATTA/kernel/launchers/sensitive_launcher.py
Content:
import copy
import os
import shlex
import shutil
from pathlib import Path
import distutils.dir_util

import numpy as np
from ruamel.yaml import YAML
from tqdm import tqdm

from ATTA import config_summoner
from ATTA import register
from ATTA.definitions import ROOT_DIR, STORAGE_DIR
from ATTA.utils.args import AutoArgs
from ATTA.utils.args import args_parser
from ATTA.utils.config_reader import load_config, args2config, merge_dicts
from .basic_launcher import Launcher
import matplotlib.pyplot as plt


@register.launcher_register
class SensitiveLauncher(Launcher):
    def __init__(self):
        super(SensitiveLauncher, self).__init__()
        self.watch = True
        self.pick_reference = [-1, -2]
        self.hparam = 'E'
        plt.rcParams['font.family'] = 'DeJavu Serif'
        plt.rcParams['font.serif'] = ['Times New Roman']
        plt.rcParams['font.size'] = 12

    def __call__(self, jobs_group, auto_args: AutoArgs):
        result_dict = self.harvest_all_fruits(jobs_group)
        best_fruits = self.picky_farmer(result_dict)

        if auto_args.sweep_root:
            self.process_final_root(auto_args)
            self.update_best_config(auto_args, best_fruits)

    def update_best_config(self, auto_args, best_fruits):
        for ddsa_key in best_fruits.keys():
            final_path = (auto_args.final_root / '/'.join(ddsa_key.split(' '))).with_suffix('.yaml')
            top_config, _, _ = load_config(final_path, skip_include=True)
            whole_config, _, _ = load_config(final_path)
            args_list = shlex.split(best_fruits[ddsa_key][0])
            args = args_parser(['--config_path', str(final_path)] + args_list)
            args2config(whole_config, args)
            args_keys = [item[2:] for item in args_list if item.startswith('--')]
            # print(args_keys)
            modified_config = self.filter_config(whole_config, args_keys)
            # print(top_config)
            # print(modified_config)
            final_top_config, _ = merge_dicts(top_config, modified_config)
            # print(final_top_config)
            yaml = YAML()
            yaml.indent(offset=2)
            # yaml.dump(final_top_config, sys.stdout)
            yaml.dump(final_top_config, final_path)

    def process_final_root(self, auto_args):
        if auto_args.final_root is None:
            auto_args.final_root = auto_args.config_root
        else:
            if os.path.isabs(auto_args.final_root):
                auto_args.final_root = Path(auto_args.final_root)
            else:
                auto_args.final_root = Path(ROOT_DIR, 'configs', auto_args.final_root)
        if auto_args.final_root.exists():
            ans = input(f'Overwrite {auto_args.final_root} by {auto_args.config_root}? [y/n]')
            while ans != 'y' and ans != 'n':
                ans = input(f'Invalid input: {ans}. Please answer y or n.')
            if ans == 'y':
                distutils.dir_util.copy_tree(str(auto_args.config_root), str(auto_args.final_root))
            elif ans == 'n':
                pass
            else:
                raise ValueError(f'Unexpected value {ans}.')
        else:
            shutil.copytree(auto_args.config_root, auto_args.final_root)

    def picky_farmer(self, result_dict):
        best_fruits = dict()
        sorted_fruits = dict()
        for ddsa_key in result_dict.keys():
            for key, value in result_dict[ddsa_key].items():
                result_dict[ddsa_key][key] = np.stack([np.mean(value, axis=1), np.std(value, axis=1)], axis=1)[-3]
            # lambda x: x[1][?, 0]  - ? denotes the result used to choose the best setting.
            if self.watch:
                sorted_fruits[ddsa_key] = sorted(list(result_dict[ddsa_key].items()), key=lambda x: x[0])
            else:
                best_fruits[ddsa_key] = max(list(result_dict[ddsa_key].items()), key=lambda x: sum(x[1][i, 0] for i in self.pick_reference))
            # best_fruits[ddsa_key] = sorted_fruits[ddsa_key][0]
        if self.watch:
            print(sorted_fruits)
            for ddsa_key in result_dict.keys():
                dataset_key = ddsa_key.split(' ')[0]
                ERM_results = {'GOODMotif': 60.93, 'GOODTwitter': 57.04, 'GOODHIV': 70.37}
                x = []
                y1 = []
                y2 = []
                for item in sorted_fruits[ddsa_key]:
                    x.append(item[0])
                    y1.append(item[1][0] - item[1][1])
                    y2.append(item[1][0] + item[1][1])
                y1 = np.array(y1) * 100
                y2 = np.array(y2) * 100
                standard_ticks = np.arange(len(x))
                fig, ax = plt.subplots(dpi=300)
                ax.fill_between(standard_ticks, y1, y2, alpha=.5, linewidth=0)
                ax.axhline(y=ERM_results[dataset_key], color='r', linestyle='--', label='ERM')
                ax.plot(standard_ticks, (y1 + y2) / 2, '.-', linewidth=2, label='LECI')
                ax.set(xticks=standard_ticks, xticklabels=x)
                ax.set_ylabel('Test metric', fontsize=22)
                ax.set_xlabel(f'$\lambda_\mathtt{{{self.hparam}}}$', fontsize=22)
                ax.legend(loc=1, prop={'size': 16})
                plt.subplots_adjust(left=0.15, bottom=0.15, right=0.95, top=0.95, wspace=0, hspace=0)
                # plt.show()
                save_path = os.path.join(STORAGE_DIR, 'figures', 'sensitive_study', f'{dataset_key}')
                if not os.path.exists(save_path):
                    os.makedirs(save_path)
                fig.savefig(os.path.join(save_path, f'{self.hparam}.png'))
            exit(0)
        print(best_fruits)
        return best_fruits

    def filter_config(self, config: dict, target_keys):
        new_config = copy.deepcopy(config)
        for key in config.keys():
            if type(config[key]) is dict:
                new_config[key] = self.filter_config(config[key], target_keys)
                if not new_config[key]:
                    new_config.pop(key)
            else:
                if key not in target_keys:
                    new_config.pop(key)
        return new_config

    def harvest_all_fruits(self, jobs_group):
        all_finished = True
        result_dict = dict()
        for cmd_args in tqdm(jobs_group, desc='Harvesting ^_^'):
            args = args_parser(shlex.split(cmd_args)[1:])
            config = config_summoner(args)
            last_line = self.harvest(config.log_path)

            if not last_line.startswith('INFO: ChartInfo'):
                print(cmd_args, 'Unfinished')
                all_finished = False
                continue
            result = last_line.split(' ')[2:]
            num_result = len(result)
            key_args = shlex.split(cmd_args)[1:]
            round_index = key_args.index('--exp_round')
            key_args = key_args[:round_index] + key_args[round_index + 2:]

            config_path_index = key_args.index('--config_path')
            key_args.pop(config_path_index)  # Remove --config_path
            config_path = Path(key_args.pop(config_path_index))  # Remove and save its value
            config_path_parents = config_path.parents
            dataset, domain, shift, algorithm = config_path_parents[2].stem, config_path_parents[1].stem, \
                                                config_path_parents[0].stem, config_path.stem
            ddsa_key = ' '.join([dataset, domain, shift, algorithm])
            if ddsa_key not in result_dict.keys():
                result_dict[ddsa_key] = dict()

            L_E = 1 if self.hparam == 'L' else 3
            key_str = float(key_args[key_args.index('--extra_param') + L_E])  # +1: LA +3: EA
            if key_str not in result_dict[ddsa_key].keys():
                result_dict[ddsa_key][key_str] = [[] for _ in range(num_result)]
            # print(f'{ddsa_key}_{key_str}: {result}')
            result_dict[ddsa_key][key_str] = [r + [eval(result[i])] for i, r in
                                              enumerate(result_dict[ddsa_key][key_str])]
        # if not all_finished:
        #     print('Please launch unfinished jobs using other launchers before harvesting.')
        #     exit(1)
        return result_dict


File Path: ATTA/kernel/launchers/single_launcher.py
Content:
import shlex
import subprocess

from tqdm import tqdm

from ATTA import register
from .basic_launcher import Launcher


@register.launcher_register
class SingleLauncher(Launcher):
    def __init__(self):
        super(SingleLauncher, self).__init__()

    def __call__(self, jobs_group, auto_args):
        jobs_group = super(SingleLauncher, self).__call__(jobs_group, auto_args)
        for cmd_args in tqdm(jobs_group):
            subprocess.run(shlex.split(cmd_args) + ['--gpu_idx', f'{auto_args.allow_devices[0]}'], close_fds=True,
                           stdout=open('debug_out.log', 'a'), stderr=open('debug_error.log', 'a'),
                           start_new_session=False)

File Path: ATTA/networks/__init__.py
Content:
r"""
This module includes GNNs used in our leaderboard. It includes: GINs, GINvirtualnodes, and GCNs, in which GCNs are only
for node classifications.
"""

from .domainbed_networks import *

File Path: ATTA/networks/domainbed_networks/__init__.py
Content:
import glob
from os.path import dirname, basename, isfile, join

modules = glob.glob(join(dirname(__file__), "*.py"))
__all__ = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]

from . import *
File Path: ATTA/networks/domainbed_networks/networks.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models

import ATTA.networks.domainbed_networks.wide_resnet as wide_resnet
import copy
from ATTA.utils.config_reader import Conf
from ATTA import register


def remove_batch_norm_from_resnet(model):
    fuse = torch.nn.utils.fusion.fuse_conv_bn_eval
    model.eval()

    model.conv1 = fuse(model.conv1, model.bn1)
    model.bn1 = Identity()

    for name, module in model.named_modules():
        if name.startswith("layer") and len(name) == 6:
            for b, bottleneck in enumerate(module):
                for name2, module2 in bottleneck.named_modules():
                    if name2.startswith("conv"):
                        bn_name = "bn" + name2[-1]
                        setattr(bottleneck, name2,
                                fuse(module2, getattr(bottleneck, bn_name)))
                        setattr(bottleneck, bn_name, Identity())
                if isinstance(bottleneck.downsample, torch.nn.Sequential):
                    bottleneck.downsample[0] = fuse(bottleneck.downsample[0],
                                                    bottleneck.downsample[1])
                    bottleneck.downsample[1] = Identity()
    model.train()
    return model


class Identity(nn.Module):
    """An identity layer"""
    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x


class MLP(nn.Module):
    """Just  an MLP"""
    def __init__(self, n_inputs, n_outputs, hparams):
        super(MLP, self).__init__()
        self.input = nn.Linear(n_inputs, hparams['mlp_width'])
        self.dropout = nn.Dropout(hparams['mlp_dropout'])
        self.hiddens = nn.ModuleList([
            nn.Linear(hparams['mlp_width'], hparams['mlp_width'])
            for _ in range(hparams['mlp_depth']-2)])
        self.output = nn.Linear(hparams['mlp_width'], n_outputs)
        self.n_outputs = n_outputs

    def forward(self, x):
        x = self.input(x)
        x = self.dropout(x)
        x = F.relu(x)
        for hidden in self.hiddens:
            x = hidden(x)
            x = self.dropout(x)
            x = F.relu(x)
        x = self.output(x)
        return x

@register.model_register
class ResNet(torch.nn.Module):
    """ResNet with the softmax chopped off and the batchnorm frozen"""
    def __init__(self, hparams):
        super(ResNet, self).__init__()
        input_shape = hparams.dataset.input_shape
        if hparams.model['resnet18']:
            self.network = torchvision.models.resnet18(pretrained=True)
            self.n_outputs = 512
        else:
            self.network = torchvision.models.resnet50(pretrained=True)
            self.n_outputs = 2048

        # self.network = remove_batch_norm_from_resnet(self.network)

        # adapt number of channels
        nc = input_shape[0]
        if nc != 3:
            tmp = self.network.conv1.weight.data.clone()

            self.network.conv1 = nn.Conv2d(
                nc, 64, kernel_size=(7, 7),
                stride=(2, 2), padding=(3, 3), bias=False)

            for i in range(nc):
                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]

        # save memory
        # self.fc = copy.deepcopy(self.network.fc)
        del self.network.fc
        self.network.fc = Identity()

        self.fr_bn = hparams.model['freeze_bn']
        self.freeze_bn()
        self.hparams = hparams
        self.dropout = nn.Dropout(hparams.model['dropout_rate'])

    def forward(self, x):
        """Encode x into a feature vector of size n_outputs."""
        return self.dropout(self.network(x))

    def train(self, mode=True):
        """
        Override the default train() to freeze the BN parameters
        """
        super().train(mode)
        self.freeze_bn()

    def freeze_bn(self):
        if self.fr_bn:
            for m in self.network.modules():
                if isinstance(m, nn.BatchNorm2d):
                    m.eval()


@register.model_register
class MNIST_CNN(nn.Module):
    """
    Hand-tuned architecture for MNIST.
    Weirdness I've noticed so far with this architecture:
    - adding a linear layer after the mean-pool in features hurts
        RotatedMNIST-100 generalization severely.
    """
    n_outputs = 128

    def __init__(self, hparams):
        super(MNIST_CNN, self).__init__()
        input_shape = hparams.dataset.input_shape
        self.conv1 = nn.Conv2d(input_shape[0], 64, 3, 1, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(128, 128, 3, 1, padding=1)
        self.conv4 = nn.Conv2d(128, 128, 3, 1, padding=1)

        self.bn0 = nn.GroupNorm(8, 64)
        self.bn1 = nn.GroupNorm(8, 128)
        self.bn2 = nn.GroupNorm(8, 128)
        self.bn3 = nn.GroupNorm(8, 128)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.bn0(x)

        x = self.conv2(x)
        x = F.relu(x)
        x = self.bn1(x)

        x = self.conv3(x)
        x = F.relu(x)
        x = self.bn2(x)

        x = self.conv4(x)
        x = F.relu(x)
        x = self.bn3(x)

        x = self.avgpool(x)
        x = x.view(len(x), -1)
        return x


@register.model_register
class MNIST_CNN_VAE(nn.Module):
    """
    Hand-tuned architecture for MNIST.
    Weirdness I've noticed so far with this architecture:
    - adding a linear layer after the mean-pool in features hurts
        RotatedMNIST-100 generalization severely.
    """
    n_outputs = 128

    def __init__(self, hparams):
        super(MNIST_CNN_VAE, self).__init__()
        input_shape = hparams.dataset.input_shape
        self.conv1 = nn.Conv2d(input_shape[0], 64, 3, 1, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(128, 128, 3, 1, padding=1)
        self.conv4 = nn.Conv2d(128, 128, 3, 1, padding=1)

        self.bn0 = nn.GroupNorm(8, 64)
        self.bn1 = nn.GroupNorm(8, 128)
        self.bn2 = nn.GroupNorm(8, 128)
        self.bn3 = nn.GroupNorm(8, 128)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        # x = self.bn0(x)

        x = self.conv2(x)
        x = F.relu(x)
        # x = self.bn1(x)

        x = self.conv3(x)
        x = F.relu(x)
        # x = self.bn2(x)

        x = self.conv4(x)
        x = F.relu(x)
        # x = self.bn3(x)

        x = self.avgpool(x)
        x = x.view(len(x), -1)
        return x


class ContextNet(nn.Module):
    def __init__(self, input_shape):
        super(ContextNet, self).__init__()

        # Keep same dimensions
        padding = (5 - 1) // 2
        self.context_net = nn.Sequential(
            nn.Conv2d(input_shape[0], 64, 5, padding=padding),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, 5, padding=padding),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 1, 5, padding=padding),
        )

    def forward(self, x):
        return self.context_net(x)


def Featurizer(input_shape, hparams):
    """Auto-select an appropriate featurizer for the given input shape."""
    if len(input_shape) == 1:
        return MLP(input_shape[0], hparams["mlp_width"], hparams)
    elif input_shape[1:3] == (28, 28):
        return MNIST_CNN(input_shape)
    elif input_shape[1:3] == (32, 32):
        return wide_resnet.Wide_ResNet(input_shape, 16, 2, 0.)
    elif input_shape[1:3] == (224, 224):
        return ResNet(hparams)
    else:
        raise NotImplementedError


def Classifier(in_features, out_features, is_nonlinear=False):
    if is_nonlinear:
        return torch.nn.Sequential(
            torch.nn.Linear(in_features, in_features // 2),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features // 2, in_features // 4),
            torch.nn.ReLU(),
            torch.nn.Linear(in_features // 4, out_features))
    else:
        return torch.nn.Linear(in_features, out_features)


# A naive class ImageNN that combines the featurizer and classifier. It takes only config as input.

@register.model_register
class ImageNN(nn.Module):
    def __init__(self, config: Conf):
        super(ImageNN, self).__init__()
        input_shape = config.dataset.input_shape
        num_classes = config.dataset.num_classes
        self.featurizer = Featurizer(input_shape, config)
        self.classifier = Classifier(
            self.featurizer.n_outputs,
            num_classes,
            config.model.nonlinear_classifier)

    def forward(self, x):
        return self.classifier(self.featurizer(x))



class WholeFish(nn.Module):
    def __init__(self, input_shape, num_classes, hparams, weights=None):
        super(WholeFish, self).__init__()
        featurizer = Featurizer(input_shape, hparams)
        classifier = Classifier(
            featurizer.n_outputs,
            num_classes,
            hparams['nonlinear_classifier'])
        self.net = nn.Sequential(
            featurizer, classifier
        )
        if weights is not None:
            self.load_state_dict(copy.deepcopy(weights))

    def reset_weights(self, weights):
        self.load_state_dict(copy.deepcopy(weights))

    def forward(self, x):
        return self.net(x)

File Path: ATTA/networks/domainbed_networks/resnet_decoder.py
Content:
from functools import partial
from typing import Any, Callable, List, Optional, Type, Union

import torch
import torch.nn as nn
from torch import Tensor

'''
Modified from https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py
'''


def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1,
            output_padding: int = 0) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.ConvTranspose2d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=dilation,
        output_padding=output_padding,
        groups=groups,
        bias=False,
        dilation=dilation,
    )


def conv1x1(in_planes: int, out_planes: int, stride: int = 1, output_padding: int = 0) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False,
                              output_padding=output_padding)


class Bottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion: int = 4

    def __init__(
            self,
            inplanes: int,
            planes: int,
            stride: int = 1,
            output_padding: int = 0,
            upsample: Optional[nn.Module] = None,
            groups: int = 1,
            base_width: int = 64,
            dilation: int = 1,
            norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.0)) * groups
        # Both self.conv2 and self.upsample layers upsample the input when stride != 1
        self.conv3 = conv1x1(planes * self.expansion, width)
        self.bn3 = norm_layer(planes)

        self.conv2 = conv3x3(width, width, stride, groups, dilation, output_padding)
        self.bn2 = norm_layer(width)

        self.conv1 = conv1x1(width, inplanes)
        self.bn1 = norm_layer(inplanes)

        self.relu = nn.ReLU(inplace=True)
        self.upsample = upsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv3(x)
        out = self.bn3(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv1(out)
        out = self.bn1(out)
        if self.upsample is not None:
            identity = self.upsample(x)
        out += identity
        out = self.relu(out)

        return out


class ResNetDecoder(nn.Module):
    def __init__(
            self,
            block,
            layers: List[int],
            num_classes: int = 1000,
            zero_init_residual: bool = False,
            groups: int = 1,
            width_per_group: int = 64,
            norm_layer: Optional[Callable[..., nn.Module]] = None,
            # indices = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 2048
        self.dilation = 1
        self.groups = groups
        self.base_width = width_per_group
        self.de_conv1 = nn.ConvTranspose2d(64, 3, kernel_size=7, stride=2, padding=3, bias=False)
        self.unpool = nn.MaxUnpool2d(kernel_size=3, stride=2, padding=1)
        self.bn1 = norm_layer(3)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.unsample = nn.Upsample(size=7, mode='nearest')

        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer1 = self._make_layer(block, 64, layers[0], output_padding=0, last_block_dim=64)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck) and m.bn3.weight is not None:
                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
                # elif isinstance(m, BasicBlock) and m.bn2.weight is not None:
                #     nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]

    def _make_layer(
            self,
            block,
            planes: int,
            blocks: int,
            stride: int = 1,
            output_padding: int = 1,
            last_block_dim: int = 0,
    ) -> nn.Sequential:
        norm_layer = self._norm_layer
        upsample = None
        previous_dilation = self.dilation

        layers = []
        self.inplanes = planes * block.expansion
        if last_block_dim == 0:
            last_block_dim = self.inplanes // 2
        if stride != 1 or self.inplanes != planes * block.expansion or output_padding == 0:
            upsample = nn.Sequential(
                conv1x1(planes * block.expansion, last_block_dim, stride, output_padding),
                # norm_layer(planes * block.expansion),
                norm_layer(last_block_dim),
            )
        last_block = block(
            last_block_dim, planes, stride, output_padding, upsample, self.groups, self.base_width, previous_dilation,
            norm_layer
        )

        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=norm_layer,
                )
            )
        layers.append(last_block)
        return nn.Sequential(*layers)

    def _forward_impl(self, x: Tensor, indices) -> Tensor:
        # See note [TorchScript super()]
        x = self.unsample(x)
        x = self.layer4(x)
        x = self.layer3(x)
        x = self.layer2(x)
        x = self.layer1(x)

        # print(x.shape)
        x = self.unpool(x, indices)
        x = self.de_conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        return x

    def _forward_cnns_only(self, x: Tensor) -> Tensor:
        # See note [TorchScript super()]
        x = self.unsample(x)
        x = self.layer4(x)
        x = self.layer3(x)
        x = self.layer2(x)
        x = self.layer1(x)
        return x

    def forward(self, x: Tensor, indices=None) -> Tensor:
        if indices is None:
            return self._forward_cnns_only(x)
        return self._forward_impl(x, indices)
File Path: ATTA/networks/domainbed_networks/wide_resnet.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

"""
From https://github.com/meliketoy/wide-resnet.pytorch
"""

import sys

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
from torch.autograd import Variable


def conv3x3(in_planes, out_planes, stride=1):
    return nn.Conv2d(
        in_planes,
        out_planes,
        kernel_size=3,
        stride=stride,
        padding=1,
        bias=True)


def conv_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.xavier_uniform_(m.weight, gain=np.sqrt(2))
        init.constant_(m.bias, 0)
    elif classname.find('BatchNorm') != -1:
        init.constant_(m.weight, 1)
        init.constant_(m.bias, 0)


class wide_basic(nn.Module):
    def __init__(self, in_planes, planes, dropout_rate, stride=1):
        super(wide_basic, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, padding=1, bias=True)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(
            planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes, planes, kernel_size=1, stride=stride,
                    bias=True), )

    def forward(self, x):
        out = self.dropout(self.conv1(F.relu(self.bn1(x))))
        out = self.conv2(F.relu(self.bn2(out)))
        out += self.shortcut(x)

        return out


class Wide_ResNet(nn.Module):
    """Wide Resnet with the softmax layer chopped off"""
    def __init__(self, input_shape, depth, widen_factor, dropout_rate):
        super(Wide_ResNet, self).__init__()
        self.in_planes = 16

        assert ((depth - 4) % 6 == 0), 'Wide-resnet depth should be 6n+4'
        n = (depth - 4) / 6
        k = widen_factor

        # print('| Wide-Resnet %dx%d' % (depth, k))
        nStages = [16, 16 * k, 32 * k, 64 * k]

        self.conv1 = conv3x3(input_shape[0], nStages[0])
        self.layer1 = self._wide_layer(
            wide_basic, nStages[1], n, dropout_rate, stride=1)
        self.layer2 = self._wide_layer(
            wide_basic, nStages[2], n, dropout_rate, stride=2)
        self.layer3 = self._wide_layer(
            wide_basic, nStages[3], n, dropout_rate, stride=2)
        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)

        self.n_outputs = nStages[3]

    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):
        strides = [stride] + [1] * (int(num_blocks) - 1)
        layers = []

        for stride in strides:
            layers.append(block(self.in_planes, planes, dropout_rate, stride))
            self.in_planes = planes

        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.relu(self.bn1(out))
        out = F.avg_pool2d(out, 8)
        return out[:, :, 0, 0]

File Path: ATTA/utils/__init__.py
Content:
r"""
This module contains various utilities, such as a component register, an argument parser, a configuration reader, and
a metric object.
"""
from .config_reader import config_summoner
from .args import args_parser
from .logger import load_logger

File Path: ATTA/utils/args.py
Content:
r"""An important module that is used to define all arguments for both argument container and configuration container.
"""
import pathlib
import sys
from typing import List

from tap import Tap
from typing_extensions import Literal

from ATTA.definitions import ROOT_DIR


class TreeTap(Tap):

    def __init__(self, argv=None, tree_parser=None):
        super(TreeTap, self).__init__()
        self.skipped_args = []
        if tree_parser:
            self.argv = []
            skip_arg = False
            for arg in argv:
                if arg.startswith('--'):
                    skip_arg = True
                    if arg.startswith(f'--{tree_parser}.'):
                        self.argv.append(f'--{".".join(arg.split(".")[1:])}')
                        skip_arg = False
                elif not skip_arg:
                    self.argv.append(arg)
                if skip_arg:
                    self.skipped_args.append(arg)
        else:
            self.argv = sys.argv[1:] if argv is None else argv

    def parse_args(self):
        return super(TreeTap, self).parse_args(self.argv, known_only=True)

    def process_args(self) -> None:
        super(TreeTap, self).process_args()
        for action in self._actions:
            if isinstance(action.type, type) and issubclass(action.type, Tap):
                setattr(self, action.dest, action.type(self.extra_args, tree_parser=action.dest).parse_args())

                # Remove parsed arguments
                self.extra_args = getattr(self, action.dest).skipped_args
        if self.extra_args:
            extra_keys = [arg[2:] for arg in self.extra_args if arg.startswith('--')]
            raise ValueError(f"Unexpected arguments [{', '.join(extra_keys)}] in {self.__class__.__name__}")

class TrainArgs(TreeTap):
    r"""
    Correspond to ``train`` configs in config files.
    """
    tr_ctn: bool = None  #: Flag for training continue.
    ctn_epoch: int = None  #: Start epoch for continue training.
    max_epoch: int = None  #: Max epochs for training stop.
    save_gap: int = None  #: Hard checkpoint saving gap.
    pre_train: int = None  #: Pre-train epoch before picking checkpoints.
    log_interval: int = None  #: Logging interval.
    max_iters: int = None  #: Max iterations for training stop.

    train_bs: int = None  #: Batch size for training.
    val_bs: int = None  #: Batch size for validation.
    test_bs: int = None  #: Batch size for test.
    num_steps: int = None  #: Number of steps in each epoch for node classifications.

    lr: float = None  #: Learning rate.
    epoch: int = None  #: Current training epoch. This value should not be set manually.
    stage_stones: List[int] = None  #: The epoch for starting the next training stage.
    mile_stones: List[int] = None  #: Milestones for a scheduler to decrease learning rate: 0.1
    weight_decay: float = None  #: Weight decay.
    gamma: float = None  #: Gamma for a scheduler to decrease learning rate: 0.1

    alpha = None  #: A parameter for DANN.


class DatasetArgs(TreeTap):
    r"""
    Correspond to ``dataset`` configs in config files.
    """
    name: str = None  #: Name of the chosen dataset.
    dataloader_name: str = None#: Name of the chosen dataloader. The default is BaseDataLoader.
    shift_type: Literal['no_shift', 'covariate', 'concept'] = None  #: The shift type of the chosen dataset.
    domain: str = None  #: Domain selection.
    generate: bool = None  #: The flag for generating ATTA datasets from scratch instead of downloading
    dataset_root: str = None  #: Dataset storage root. Default STORAGE_ROOT/datasets
    dataset_type: str = None  #: Dataset type: molecule, real-world, synthetic, etc. For special usages.
    class_balanced: bool = None  #: Whether to use class balanced sampler.
    data_augmentation: bool = None  #: Whether to use data augmentation.


    dim_node: int = None  #: Dimension of node
    dim_edge: int = None  #: Dimension of edge
    num_classes: int = None  #: Number of labels for multi-label classifications.
    num_envs: int = None  #: Number of environments in training set.
    num_domains: int = None  #: Number of domains in training set.
    feat_dims: List[int] = None  #: Number of integer values for each x feature.
    edge_feat_dims: List[int] = None  #: Number of integer values for each edge feature.
    test_envs: List[int] = None  #: Test environments.


class ModelArgs(TreeTap):
    r"""
    Correspond to ``model`` configs in config files.
    """
    name: str = None  #: Name of the chosen GNN.
    model_layer: int = None  #: Number of the GNN layer.
    model_level: Literal['node', 'link', 'graph', 'image'] = 'graph'  #: What is the model use for? Node, link, or graph predictions.
    nonlinear_classifier: bool = None  #: Whether to use a nonlinear classifier.
    resnet18: bool = None  #: Whether to use a ResNet18 backbone.

    dim_hidden: int = None  #: Node hidden feature's dimension.
    dim_ffn: int = None  #: Final linear layer dimension.
    global_pool: str = None  #: Readout pooling layer type. Currently allowed: 'max', 'mean'.
    dropout_rate: float = None  #: Dropout rate.
    freeze_bn: bool = None  #: Whether to freeze batch normalization layers.


class OODArgs(TreeTap):
    r"""
    Correspond to ``ood`` configs in config files.
    """
    alg: str = None  #: Name of the chosen OOD algorithm.
    ood_param: float = None  #: OOD algorithms' hyperparameter(s). Currently, most of algorithms use it as a float value.
    extra_param: List = None  #: OOD algorithms' extra hyperparameter(s).

    def process_args(self) -> None:
        self.extra_param = [eval(param) for param in self.extra_param] if self.extra_param is not None else None

class SARArgs(TreeTap):
    steps: int = None
    reset_constant_em: float = None
    lr: float = None

class TentArgs(TreeTap):
    steps: int = None
    lr: float = None

class EATAArgs(TreeTap):
    steps: int = None
    lr: float = None
    d_margin: float = None
    fisher_alpha: float = None

class TICPArgs(TreeTap):
    num_drops: int = None #: Number of pseudo environments.
    classifier_epochs: int = None #: Number of epochs for training the classifiers
    classifier_lr: float = None #: Learning rate for training the classifiers
    num_gaussians: int = None #: Number of Gaussian distributions for the mixture model
    alpha: float = None #: Alpha parameter for the mixture model
    beta: float = None #: Beta parameter for the mixture model
    test_interval: int = None #: Interval for testing the classifiers
    train_linear_classifier: bool = None #: Whether to train a linear classifier
    steps: int = None #: Number of steps for the TICP algorithm
    temperature: float = None #: Temperature for the TICP algorithm
    train_or_load: str = None #: Whether to train or load the classifiers/vaes

class TTAArgs(TreeTap):
    name: str = None #: Name of the TTA algorithm
    episodic: bool = None
    SAR: SARArgs = None
    Tent: TentArgs = None
    TICP: TICPArgs = None

class TALArgs(TreeTap):
    steps: int = None
    lr: float = None
    e0: float = None


class SimATTAArgs(TreeTap):
    steps: int = None #: Number of steps for the ATTA algorithm
    lr: float = None  #: Learning rate for the ATTA algorithm
    eh: float = None #: Initial entropy high threshold
    el: float = None #: Initial entropy low threshold
    cold_start: int = None #: Number of steps for the cold start phase
    beta: float = None #: Beta parameter for the ATTA algorithm
    nc_increase: float = None #: Number of clusters to increase for each time step
    stop_tol: int = None #: Stopping tolerance for the ATTA algorithm
    target_cluster: int = None #: Whether to use target clustering for the ATTA algorithm
    LE: int = None #: Whether to use low-entropy samples for the ATTA algorithm

class ATTAArgs(TreeTap):
    name: str = None #: Name of the ATTA algorithm
    budgets: int = None #: Number of samples to be labeled
    episodic: bool = None
    batch_size: int = None #: Batch size for the ATTA algorithms
    al_rate: float = None #: Whether to use active learning for the ATTA algorithm
    gpu_clustering: bool = None #: GPU flag for the torch clustering implementation.
    SimATTA: SimATTAArgs = None
    Tent: TentArgs = None
    TAL: TALArgs = None
    SAR: SARArgs = None
    EATA: EATAArgs = None


class AutoArgs(Tap):
    config_root: str = None  #: The root of input configuration files.
    sweep_root: str = None  #: The root of hyperparameter searching configurations.
    final_root: str = None  #: The root of output final configuration files.
    launcher: str = None  #: The launcher name.


    allow_datasets: List[str] = None  #: Allow datasets in list to run.
    allow_domains: List[str] = None  #: Allow domains in list to run.
    allow_shifts: List[str] = None  #: Allow shifts.
    allow_algs: List[str] = None  #: Allowed OOD algorithms.
    allow_devices: List[int] = None  #: Devices allowed to run.
    allow_rounds: List[int] = None # The numbers of experiment round.


class CommonArgs(TreeTap):
    r"""
    Correspond to general configs in config files.
    """
    config_path: pathlib.Path  #: (Required) The path for the config file.

    task: Literal['train', 'test', 'adapt'] = None  #: Running mode. Allowed: 'train' and 'test'.
    random_seed: int = None  #: Fixed random seed for reproducibility.
    exp_round: int = None  #: Current experiment round.
    pytest: bool = None
    pipeline: str = None  #: Training/test controller.

    ckpt_root: str = None  #: Checkpoint root for saving checkpoint files, where inner structure is automatically generated
    ckpt_dir: str = None  #: The direct directory for saving ckpt files
    test_ckpt: str = None  #: Path of the model general test or out-of-domain test checkpoint
    id_test_ckpt: str = None  #: Path of the model in-domain checkpoint
    save_tag: str = None  #: Special save tag for distinguishing special training checkpoints.
    other_saved = None  #: Other info that need to be stored in a checkpoint.
    clean_save: bool = None  #: Only save necessary checkpoints.
    full_clean: bool = None

    gpu_idx: int = None  #: GPU index.
    device = None  #: Automatically generated by choosing gpu_idx.
    num_workers: int = None  #: Number of workers used by data loaders.

    log_file: str = None  #: Log file name.
    log_path: str = None  #: Log file path.

    tensorboard_logdir: str = None  #: Tensorboard logging place.

    mp_spawn: bool = None  #: Whether to use multiprocessing spawn method.

    # For code auto-complete
    train: TrainArgs = None  #: For code auto-complete
    model: ModelArgs = None  #: For code auto-complete
    dataset: DatasetArgs = None  #: For code auto-complete
    ood: OODArgs = None  #: For code auto-complete
    tta: TTAArgs = None
    atta: ATTAArgs = None

    def __init__(self, argv):
        super(CommonArgs, self).__init__(argv)

        from ATTA.utils.metric import Metric
        self.metric: Metric = None

    def process_args(self) -> None:
        super().process_args()
        if not self.config_path.is_absolute():
            self.config_path = pathlib.Path(ROOT_DIR) / 'configs' / self.config_path


def args_parser(argv: list=None):
    r"""
    Arguments parser.

    Args:
        argv: Input arguments. *e.g.*, ['--config_path', config_path,
            '--ckpt_root', os.path.join(STORAGE_DIR, 'reproduce_ckpts'),
            '--exp_round', '1']

    Returns:
        General arguments

    """
    common_args = CommonArgs(argv=argv).parse_args()
    return common_args

File Path: ATTA/utils/config_reader.py
Content:
r"""A project configuration module that reads config argument from a file; set automatic generated arguments; and
overwrite configuration arguments by command arguments.
"""

import copy
import sys
import typing
import warnings
from os.path import join as opj
from pathlib import Path

import torch
from munch import munchify
from ruamel.yaml import YAML

from ATTA.definitions import STORAGE_DIR
from ATTA.utils.args import CommonArgs, TreeTap
from ATTA.utils.metric import Metric


# This two import cannot be removed
from typing import Union
from munch import Munch

Conf = typing.TypeVar('Conf', bound='Union[CommonArgs, Munch]')


def merge_dicts(dict1: dict, dict2: dict):
    """Recursively merge two dictionaries.
    Values in dict2 override values in dict1. If dict1 and dict2 contain a dictionary as a
    value, this will call itself recursively to merge these dictionaries.
    This does not modify the input dictionaries (creates an internal copy).
    Additionally returns a list of detected duplicates.
    Adapted from https://github.com/TUM-DAML/seml/blob/master/seml/utils.py
    Parameters
    ----------
    dict1: dict
        First dict.
    dict2: dict
        Second dict. Values in dict2 will override values from dict1 in case they share the same key.
    Returns
    -------
    return_dict: dict
        Merged dictionaries.
    """
    if not isinstance(dict1, dict):
        raise ValueError(f"Expecting dict1 to be dict, found {type(dict1)}.")
    if not isinstance(dict2, dict):
        raise ValueError(f"Expecting dict2 to be dict, found {type(dict2)}.")

    return_dict = copy.deepcopy(dict1)
    duplicates = []

    for k, v in dict2.items():
        if k not in dict1:
            return_dict[k] = v
        else:
            if isinstance(v, dict) and isinstance(dict1[k], dict):
                return_dict[k], duplicates_k = merge_dicts(dict1[k], dict2[k])
                duplicates += [f"{k}.{dup}" for dup in duplicates_k]
            else:
                return_dict[k] = dict2[k]
                duplicates.append(k)

    return return_dict, duplicates


def load_config(path: str, previous_includes: list = [], skip_include=False) -> dict:
    r"""Config loader.
    Loading configs from a config file.

    Args:
        path (str): The path to your yaml configuration file.
        previous_includes (list): Included configurations. It is for the :obj:`include` configs used for recursion.
            Please leave it blank when call this function outside.

    Returns:
        config (dict): config dictionary loaded from the given yaml file.
    """
    path = Path(path)
    if path in previous_includes:
        raise ValueError(
            f"Cyclic config include detected. {path} included in sequence {previous_includes}."
        )
    previous_includes = previous_includes + [path]

    yaml = YAML(typ='safe')
    direct_config = yaml.load(open(path, "r"))
    if skip_include:
        return direct_config, None, None
    # direct_config = yaml.safe_load(open(path, "r"))

    # Load config from included files.
    if "includes" in direct_config:
        includes = direct_config.pop("includes")
    else:
        includes = []
    if not isinstance(includes, list):
        raise AttributeError(
            "Includes must be a list, '{}' provided".format(type(includes))
        )

    config = {}
    duplicates_warning = []
    duplicates_error = []

    for include in includes:
        include = path.parent / include
        include_config, inc_dup_warning, inc_dup_error = load_config(
            include, previous_includes
        )
        duplicates_warning += inc_dup_warning
        duplicates_error += inc_dup_error

        # Duplicates between includes causes an error
        config, merge_dup_error = merge_dicts(config, include_config)
        duplicates_error += merge_dup_error

    # Duplicates between included and main file causes warnings
    config, merge_dup_warning = merge_dicts(config, direct_config)
    duplicates_warning += merge_dup_warning

    return config, duplicates_warning, duplicates_error


# def search_tap_args(args: CommonArgs, query: str):
#     r"""
#     Search a key in command line arguments.
#
#     Args:
#         args (CommonArgs): Command line arguments.
#         query (str): The query for the target argument.
#
#     Returns:
#         A found or not flag and the target value if found.
#     """
#     found = False
#     value = None
#     for key in args.class_variables.keys():
#         if query == key:
#             found = True
#             value = getattr(args, key)
#         # elif issubclass(type(getattr(args, key)), Tap):
#         #     found, value = search_tap_args(getattr(args, key), query)
#         if found:
#             break
#     return found, value

def custom_show_warning(message, category, filename, lineno, file=None, line=None):
    formatted_warning = f"{filename}:{lineno}: {category.__name__}: {message}"
    print(formatted_warning, file=sys.stderr)

warnings.showwarning = custom_show_warning

def args2config(config: Conf, args: CommonArgs, stem=''):
    r"""
    Overwrite config by assigned arguments.
    If an argument is not :obj:`None`, this argument has the highest priority; thus, it will overwrite the corresponding
    config.

    Args:
        config (Conf): Loaded configs.
        args (CommonArgs): Command line arguments.

    Returns:
        Overwritten configs.
    """
    for key in args._get_annotations().keys():
        args_value = getattr(args, key)
        if args_value is None:
            if key not in config.keys():
                # warnings.warn(f'Missing argument "{stem + key}" in the config file.', stacklevel=0)
                config[key] = None
            continue
        # if key not in config.keys():
        #     warnings.warn(f'Missing argument "{stem + key}" in the config file.', stacklevel=0)
        if isinstance(type(args_value), type) and isinstance(args_value, TreeTap):
            if key not in config.keys():
                config[key] = dict()
            args2config(config[key], args_value, stem + f'{key}.')
        else:
            config[key] = args_value
    for key in config.keys():
        if not hasattr(args, key):
            warnings.warn(f'Missing argument "{stem + key}" in CLI parser classes, which is shown in the config file.', stacklevel=0)
            continue
        # if type(config[key]) is dict:
        #     args2config(config[key], getattr(args, key), stem + f'{key}.')
        # else:
        #     if value := getattr(args, key):
        #         config[key] = value


def process_configs(config: Conf):
    r"""
    Process loaded configs.
    This process includes setting storage places for datasets, tensorboard logs, logs, and checkpoints. In addition,
    we also set random seed for each experiment round, checkpoint saving gap, and gpu device. Finally, we connect the
    config with two components :class:`ATTA.utils.metric.Metric` and :class:`ATTA.utils.train.TrainHelper` for easy and
    unified accesses.

    Args:
        config (Conf): Loaded configs.

    Returns:
        Configs after setting.
    """
    # --- Dataset setting ---
    if config.dataset.dataset_root is None:
        config.dataset.dataset_root = opj(STORAGE_DIR, 'datasets')

    # --- Round setting ---
    if config.exp_round:
        config.random_seed = config.exp_round * 97 + 13

    # --- Directory name definitions ---
    # If config.dataset has attribute domain, it means that the dataset is a GOOD dataset
    if config.dataset.domain:
        dataset_dirname = config.dataset.name + '_' + config.dataset.domain
    else:
        dataset_dirname = opj(config.dataset.name, str(config.dataset.test_envs))
    if config.dataset.shift_type:
        dataset_dirname += '_' + config.dataset.shift_type
    model_dirname = f'{config.model.name}{18 if config.model.resnet18 else 50}_{config.model.model_layer}l_{config.model.global_pool}pool_{config.model.dropout_rate}dp_{config.model.freeze_bn}fzbn'
    train_dirname = f'{config.train.lr}lr_{config.train.weight_decay}wd'
    ood_dirname = config.ood.alg
    if config.ood.ood_param is not None and config.ood.ood_param >= 0:
        ood_dirname += f'_{config.ood.ood_param}'
    else:
        ood_dirname += '_no_param'
    if config.ood.extra_param is not None:
        for i, param in enumerate(config.ood.extra_param):
            ood_dirname += f'_{param}'

    # --- Log setting ---
    log_dir_root = opj(STORAGE_DIR, 'log', 'round' + str(config.exp_round))
    log_dirs = opj(log_dir_root, dataset_dirname, model_dirname, train_dirname, ood_dirname)
    if config.save_tag:
        log_dirs = opj(log_dirs, config.save_tag)
    config.log_path = opj(log_dirs, config.log_file + '.log')

    # --- tensorboard directory setting ---
    config.tensorboard_logdir = opj(STORAGE_DIR, 'tensorboard', 'round' + str(config.exp_round), dataset_dirname, model_dirname, train_dirname, ood_dirname)
    if config.save_tag:
        config.tensorboard_logdir = opj(config.tensorboard_logdir, config.save_tag)

    # --- Checkpoint setting ---
    if config.ckpt_root is None:
        config.ckpt_root = opj(STORAGE_DIR, 'checkpoints')
    if config.ckpt_dir is None:
        config.ckpt_dir = opj(config.ckpt_root, 'round' + str(config.exp_round))
        config.ckpt_dir = opj(config.ckpt_dir, dataset_dirname, model_dirname, train_dirname, ood_dirname)
        if config.save_tag:
            config.ckpt_dir = opj(config.ckpt_dir, config.save_tag)
    config.test_ckpt = opj(config.ckpt_dir, f'best.ckpt')
    config.id_test_ckpt = opj(config.ckpt_dir, f'id_best.ckpt')

    # --- Other settings ---
    if config.train.max_epoch > 1000:
        config.train.save_gap = config.train.max_epoch // 100
    config.device = torch.device(f'cuda:{config.gpu_idx}' if torch.cuda.is_available() else 'cpu')
    config.train.stage_stones.append(100000)

    # --- Attach train_helper and metric modules ---
    config.metric = Metric()


def config_summoner(args: CommonArgs) -> Conf:
    r"""
    A config loading and postprocessing function.

    Args:
        args (CommonArgs): Command line arguments.

    Returns:
        Processed configs.
    """
    config, duplicate_warnings, duplicate_errors = load_config(args.config_path)
    args2config(config, args)
    config = munchify(config)
    process_configs(config)
    return config

File Path: ATTA/utils/data.py
Content:
r"""
Some data process utils including construction of molecule PyG graph from smile (for compatibility).
"""

import torch
from torch_geometric.data import Batch, Data
from torch_geometric.utils.num_nodes import maybe_num_nodes


# A pytorch compatible ImageData class that contains two attributes: x and y. It supports .to() and .cuda() methods.
class ImageData:
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def to(self, device):
        self.x = self.x.to(device)
        self.y = self.y.to(device)
        return self

    def cuda(self):
        self.x = self.x.cuda()
        self.y = self.y.cuda()
        return self


def batch_input(G: Data, batch_size: int, num_nodes: int = None, node_attrs: list =['color']):
    r"""
    Repeat a graph ``batch_size`` times and pack into a Batch.

    Args:
        G (Data): The given graph G.
        batch_size (int): Batch size.
        num_nodes (int): The number of node of the graph. If :obj:`None`, it will use maybe_numb_nodes.
        node_attrs (list): The preserved node attributes.

    Returns:
        Repeated graph batch.
    """
    x = G.x
    edge_index = G.edge_index
    device = edge_index.device
    num_edges = edge_index.shape[1]
    num_nodes = maybe_num_nodes(edge_index, num_nodes=num_nodes)
    batch_x = x.repeat(batch_size, 1)
    batch_batch = torch.arange(batch_size, device=device).unsqueeze(1).repeat(1, num_nodes).view(-1)
    batch_edge_batch = torch.arange(batch_size, device=device).unsqueeze(1).repeat(1, num_edges).view(-1)
    batch_edge_index = edge_index.repeat(1, batch_size) + batch_edge_batch * num_nodes
    batch = Batch(x=batch_x, edge_index=batch_edge_index, batch=batch_batch)
    if node_attrs:
        for node_attr in node_attrs:
            if hasattr(G, node_attr) and getattr(G, node_attr) is not None:
                batch.__setattr__(node_attr, getattr(G, node_attr).repeat(batch_size, 1))
    return batch


x_map = {
    'atomic_num':
        list(range(0, 119)),
    'chirality': [
        'CHI_UNSPECIFIED',
        'CHI_TETRAHEDRAL_CW',
        'CHI_TETRAHEDRAL_CCW',
        'CHI_OTHER',
    ],
    'degree':
        list(range(0, 11)),
    'formal_charge':
        list(range(-5, 7)),
    'num_hs':
        list(range(0, 9)),
    'num_radical_electrons':
        list(range(0, 5)),
    'hybridization': [
        'UNSPECIFIED',
        'S',
        'SP',
        'SP2',
        'SP3',
        'SP3D',
        'SP3D2',
        'OTHER',
    ],
    'is_aromatic': [False, True],
    'is_in_ring': [False, True],
}

e_map = {
    'bond_type': [
        'misc',
        'SINGLE',
        'DOUBLE',
        'TRIPLE',
        'AROMATIC',
    ],
    'stereo': [
        'STEREONONE',
        'STEREOZ',
        'STEREOE',
        'STEREOCIS',
        'STEREOTRANS',
        'STEREOANY',
    ],
    'is_conjugated': [False, True],
}


def from_smiles(smiles: str, with_hydrogen: bool = False,
                kekulize: bool = False):
    r"""Converts a SMILES string to a `torch_geometric.data.data.Data`
    instance.

    Args:
        smiles (string, optional): The SMILES string.
        with_hydrogen (bool, optional): If set to :obj:`True`, will store
            hydrogens in the molecule graph. (default: :obj:`False`)
        kekulize (bool, optional): If set to :obj:`True`, converts aromatic
            bonds to single/double bonds. (default: :obj:`False`)
    """
    from rdkit import Chem, RDLogger

    from torch_geometric.data import Data

    RDLogger.DisableLog('rdApp.*')

    mol = Chem.MolFromSmiles(smiles)

    if mol is None:
        mol = Chem.MolFromSmiles('')
    if with_hydrogen:
        mol = Chem.AddHs(mol)
    if kekulize:
        mol = Chem.Kekulize(mol)

    xs = []
    for atom in mol.GetAtoms():
        x = []
        x.append(x_map['atomic_num'].index(atom.GetAtomicNum()))
        x.append(x_map['chirality'].index(str(atom.GetChiralTag())))
        x.append(x_map['degree'].index(atom.GetTotalDegree()))
        x.append(x_map['formal_charge'].index(atom.GetFormalCharge()))
        x.append(x_map['num_hs'].index(atom.GetTotalNumHs()))
        x.append(x_map['num_radical_electrons'].index(
            atom.GetNumRadicalElectrons()))
        x.append(x_map['hybridization'].index(str(atom.GetHybridization())))
        x.append(x_map['is_aromatic'].index(atom.GetIsAromatic()))
        x.append(x_map['is_in_ring'].index(atom.IsInRing()))
        xs.append(x)

    x = torch.tensor(xs, dtype=torch.long).view(-1, 9)

    edge_indices, edge_attrs = [], []
    for bond in mol.GetBonds():
        i = bond.GetBeginAtomIdx()
        j = bond.GetEndAtomIdx()

        e = []
        e.append(e_map['bond_type'].index(str(bond.GetBondType())))
        e.append(e_map['stereo'].index(str(bond.GetStereo())))
        e.append(e_map['is_conjugated'].index(bond.GetIsConjugated()))

        edge_indices += [[i, j], [j, i]]
        edge_attrs += [e, e]

    edge_index = torch.tensor(edge_indices)
    edge_index = edge_index.t().to(torch.long).view(2, -1)
    edge_attr = torch.tensor(edge_attrs, dtype=torch.long).view(-1, 3)

    if edge_index.numel() > 0:  # Sort indices.
        perm = (edge_index[0] * x.size(0) + edge_index[1]).argsort()
        edge_index, edge_attr = edge_index[:, perm], edge_attr[perm]

    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles), mol

File Path: ATTA/utils/evaluation.py
Content:
r"""
Evaluation: model evaluation functions.
"""

import numpy as np
import torch

from typing import List, Tuple
from ATTA.utils.config_reader import Conf
from typing import Union


def eval_data_preprocess(y: torch.Tensor,
                         raw_pred: torch.Tensor,
                         mask: torch.Tensor,
                         config: Conf
                         ) -> Tuple[Union[np.ndarray, List], Union[np.ndarray, List]]:
    r"""
    Preprocess data for evaluations by converting data into np.ndarray or List[np.ndarray] (Multi-task) format.
    When the task of the dataset is not multi-task, data is converted into np.ndarray.
    When it is multi-task, data is converted into List[np.ndarray] in which each np.ndarray in the list represents
    one task. For example, ATTA-PCBA is a 128-task binary classification dataset. Therefore, the output list will
    contain 128 elements.

    Args:
        y (torch.Tensor): Ground truth values.
        raw_pred (torch.Tensor): Raw prediction values without softmax or sigmoid.
        mask (torch.Tensor): Ground truth NAN mask for removing empty label.
        config (Conf): The required config is
            ``config.metric.dataset_task``

    Returns:
        Processed prediction values and ground truth values.

    """
    if config.metric.dataset_task == 'Binary classification':
        pred_prob = raw_pred.sigmoid()
        if y.shape[1] > 1:
            # multi-task
            preds = []
            targets = []
            for i in range(y.shape[1]):
                # pred and target per task
                preds.append(pred_prob[:, i][mask[:, i]].detach().cpu().numpy())
                targets.append(y[:, i][mask[:, i]].detach().cpu().numpy())
            return preds, targets
        pred = pred_prob[mask].reshape(-1).detach().cpu().numpy()
    elif config.metric.dataset_task == 'Multi-label classification':
        pred_prob = raw_pred.softmax(dim=1)
        pred = pred_prob[mask].detach().cpu().numpy()
    elif 'Regression' in config.metric.dataset_task:
        pred = raw_pred[mask].reshape(-1).detach().cpu().numpy()
    else:
        raise ValueError('Dataset task value error.')

    target = y[mask].reshape(-1).detach().cpu().numpy()

    return pred, target


def eval_score(pred_all: Union[List[np.ndarray], List[List[np.ndarray]]],
               target_all: Union[List[np.ndarray], List[List[np.ndarray]]],
               config: Conf
               ) -> Union[np.ndarray, float, float]:
    r"""
    Calculate metric scores given preprocessed prediction values and ground truth values.

    Args:
        pred_all (Union[List[np.ndarray], List[List[np.ndarray]]]): Prediction value list. It is a list of output pred
            of :func:`eval_data_preprocess`.
        target_all (Union[List[np.ndarray], List[List[np.ndarray]]]): Ground truth value list. It is a list of output
            target of :func:`eval_data_preprocess`.
        config (Conf): The required config is ``config.metric.score_func`` that is a function for
            score calculation (*e.g.*, :func:`ATTA.utils.metric.Metric.acc`).

    Returns:
        A float score value.
    """
    np.seterr(invalid='ignore')

    assert type(pred_all) is list, 'Wrong prediction input.'
    if type(pred_all[0]) is list:
        # multi-task
        all_task_preds = []
        all_task_targets = []
        for task_i in range(len(pred_all[0])):
            preds = []
            targets = []
            for pred, target in zip(pred_all, target_all):
                preds.append(pred[task_i])
                targets.append(target[task_i])
            all_task_preds.append(np.concatenate(preds))
            all_task_targets.append(np.concatenate(targets))

        scores = []
        for i in range(len(all_task_preds)):
            if all_task_targets[i].shape[0] > 0:
                scores.append(np.nanmean(config.metric.score_func(all_task_targets[i], all_task_preds[i])))
        score = np.nanmean(scores)
    else:
        pred_all = np.concatenate(pred_all)
        target_all = np.concatenate(target_all)
        score = np.nanmean(config.metric.score_func(target_all, pred_all))
    return score



File Path: ATTA/utils/fast_pytorch_kmeans/__init__.py
Content:
'''
copy and modify from: https://github.com/DeMoriarty/fast_pytorch_kmeans
'''

from .kmeans import KMeans
from .multi_kmeans import MultiKMeans
File Path: ATTA/utils/fast_pytorch_kmeans/init_methods.py
Content:
import torch


def _kpp(data: torch.Tensor, k: int, sample_size: int = -1):
    """ Picks k points in the data based on the kmeans++ method.

    Parameters
    ----------
    data : torch.Tensor
        Expect a rank 1 or 2 array. Rank 1 is assumed to describe 1-D
        data, rank 2 multidimensional data, in which case one
        row is one observation.
    k : int
        Number of samples to generate.
    sample_size : int
        sample data to avoid memory overflow during calculation

    Returns
    -------
    init : ndarray
        A 'k' by 'N' containing the initial centroids.

    References
    ----------
    .. [1] D. Arthur and S. Vassilvitskii, "k-means++: the advantages of
       careful seeding", Proceedings of the Eighteenth Annual ACM-SIAM Symposium
       on Discrete Algorithms, 2007.
    .. [2] scipy/cluster/vq.py: _kpp
    """
    if sample_size is not None and sample_size > 0:
        data = data[torch.randint(0, int(data.shape[0]),
                                  [min(100000, data.shape[0])], device=data.device)]
    dims = data.shape[1] if len(data.shape) > 1 else 1
    init = torch.zeros((k, dims)).to(data.device)

    r = torch.distributions.uniform.Uniform(0, 1)
    for i in range(k):
        if i == 0:
            init[i, :] = data[torch.randint(data.shape[0], [1])]

        else:
            D2 = torch.cdist(init[:i, :][None, :], data[None, :], p=2)[0].amin(dim=0)
            probs = D2 / torch.sum(D2)
            cumprobs = torch.cumsum(probs, dim=0)
            init[i, :] = data[torch.searchsorted(
                cumprobs, r.sample([1]).to(data.device))]
    return init


def _krandinit(data: torch.Tensor, k: int, sample_size: int = -1):
    """Returns k samples of a random variable whose parameters depend on data.

    More precisely, it returns k observations sampled from a Gaussian random
    variable whose mean and covariances are the ones estimated from the data.

    Parameters
    ----------
    data : torch.Tensor
        Expect a rank 1 or 2 array. Rank 1 is assumed to describe 1-D
        data, rank 2 multidimensional data, in which case one
        row is one observation.
    k : int
        Number of samples to generate.
    sample_size : int
        sample data to avoid memory overflow during calculation

    Returns
    -------
    x : ndarray
        A 'k' by 'N' containing the initial centroids

    References
    ----------
    .. [1] scipy/cluster/vq.py: _krandinit
    """
    mu = data.mean(axis=0)
    if sample_size > 0:
        data = data[torch.randint(0, int(data.shape[0]),
                                  [min(100000, data.shape[0])], device=data.device)]
    if data.ndim == 1:
        cov = torch.cov(data)
        x = torch.randn(k, device=data.device)
        x *= np.sqrt(cov)
    elif data.shape[1] > data.shape[0]:
        # initialize when the covariance matrix is rank deficient
        _, s, vh = data.svd(data - mu, full_matrices=False)
        x = torch.randn(k, s.shape[0])
        sVh = s[:, None] * vh / torch.sqrt(data.shape[0] - 1)
        x = x.dot(sVh)
    else:
        cov = torch.atleast_2d(torch.cov(data.T))

        # k rows, d cols (one row = one obs)
        # Generate k sample of a random variable ~ Gaussian(mu, cov)
        x = torch.randn(k, mu.shape[0], device=data.device)
        x = torch.matmul(x, torch.linalg.cholesky(cov).T)
    x += mu
    return x


def _kpoints(data, k, sample_size=-1):
    """Pick k points at random in data (one row = one observation).

    Parameters
    ----------
    data : ndarray
        Expect a rank 1 or 2 array. Rank 1 are assumed to describe one
        dimensional data, rank 2 multidimensional data, in which case one
        row is one observation.
    k : int
        Number of samples to generate.
    sample_size : int (not used)
        sample data to avoid memory overflow during calculation

    Returns
    -------
    x : ndarray
        A 'k' by 'N' containing the initial centroids

    """
    return data[torch.randint(0, data.shape[0], size=[k], device=data.device)]


init_methods = {
    "gaussian": _krandinit,
    "kmeans++": _kpp,
    "random": _kpoints,
}
File Path: ATTA/utils/fast_pytorch_kmeans/kmeans.py
Content:
import warnings

import math
import torch
from time import time
import numpy as np
import pynvml
from .init_methods import init_methods


class KMeans:
    '''
    Kmeans clustering algorithm implemented with PyTorch

    Parameters:
      n_clusters: int,
        Number of clusters

      max_iter: int, default: 100
        Maximum number of iterations

      tol: float, default: 0.0001
        Tolerance

      verbose: int, default: 0
        Verbosity

      mode: {'euclidean', 'cosine'}, default: 'euclidean'
        Type of distance measure

      init_method: {'random', 'point', '++'}
        Type of initialization

      minibatch: {None, int}, default: None
        Batch size of MinibatchKmeans algorithm
        if None perform full KMeans algorithm

    Attributes:
      centroids: torch.Tensor, shape: [n_clusters, n_features]
        cluster centroids
    '''

    def __init__(self, n_clusters, max_iter=300, tol=0.0001, verbose=0, mode="euclidean", init_method="kmeans++",
                 minibatch=None, n_init=None, algorithm=None, device=None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.verbose = verbose
        self.mode = mode
        self.init_method = init_method
        self.minibatch = minibatch
        self._loop = False
        self._show = False

        self.n_init = n_init

        if algorithm is not None:
            warnings.warn("The parameter algorithm is not valid in this implementation of KMeans. Default: 'lloyd'")

        try:
            import pynvml
            self._pynvml_exist = True
        except ModuleNotFoundError:
            self._pynvml_exist = False

        self.device = device
        self.cluster_centers_ = None
        self.labels_ = None

    @staticmethod
    # @torch.compile
    def cos_sim(a, b):
        """
          Compute cosine similarity of 2 sets of vectors

          Parameters:
          a: torch.Tensor, shape: [m, n_features]

          b: torch.Tensor, shape: [n, n_features]
        """
        a_norm = a.norm(dim=-1, keepdim=True)
        b_norm = b.norm(dim=-1, keepdim=True)
        a = a / (a_norm + 1e-8)
        b = b / (b_norm + 1e-8)
        return a @ b.transpose(-2, -1)

    @staticmethod
    # @torch.compile
    def euc_sim(a, b):
        """
          Compute euclidean similarity of 2 sets of vectors

          Parameters:
          a: torch.Tensor, shape: [m, n_features]

          b: torch.Tensor, shape: [n, n_features]
        """
        return 2 * a @ b.transpose(-2, -1) - (a ** 2).sum(dim=1)[..., :, None] - (b ** 2).sum(dim=1)[..., None, :]

    def remaining_memory(self):
        """
          Get remaining memory in gpu
        """
        with torch.cuda.device(self.device):
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
        if self._pynvml_exist:
            pynvml.nvmlInit()
            gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(self.device.index)
            info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)
            remaining = info.free
        else:
            remaining = torch.cuda.memory_allocated()
        return remaining

    # @torch.compile
    def max_sim(self, a, b):
        """
          Compute maximum similarity (or minimum distance) of each vector
          in a with all of the vectors in b

          Parameters:
          a: torch.Tensor, shape: [m, n_features]

          b: torch.Tensor, shape: [n, n_features]
        """
        batch_size = a.shape[0]
        if self.mode == 'cosine':
            sim_func = self.cos_sim
        elif self.mode == 'euclidean':
            sim_func = self.euc_sim

        if self.device == 'cpu':
            sim = sim_func(a, b)
            max_sim_v, max_sim_i = sim.max(dim=-1)
            return max_sim_v, max_sim_i
        else:
            if a.dtype == torch.double:
                expected = a.shape[0] * a.shape[1] * b.shape[0] * 8
            if a.dtype == torch.float:
                expected = a.shape[0] * a.shape[1] * b.shape[0] * 4
            elif a.dtype == torch.half:
                expected = a.shape[0] * a.shape[1] * b.shape[0] * 2
            ratio = math.ceil(expected / self.remaining_memory())
            subbatch_size = math.ceil(batch_size / ratio)
            msv, msi = [], []
            for i in range(ratio):
                if i * subbatch_size >= batch_size:
                    continue
                sub_x = a[i * subbatch_size: (i + 1) * subbatch_size]
                sub_sim = sim_func(sub_x, b)
                sub_max_sim_v, sub_max_sim_i = sub_sim.max(dim=-1)
                del sub_sim
                msv.append(sub_max_sim_v)
                msi.append(sub_max_sim_i)
            if ratio == 1:
                max_sim_v, max_sim_i = msv[0], msi[0]
            else:
                max_sim_v = torch.cat(msv, dim=0)
                max_sim_i = torch.cat(msi, dim=0)
            return max_sim_v, max_sim_i

    # @torch.compile
    def fit_predict(self, X, sample_weight=None, centroids=None):
        """
          Combination of fit() and predict() methods.
          This is faster than calling fit() and predict() seperately.

          Parameters:
          X: torch.Tensor, shape: [n_samples, n_features]

          centroids: {torch.Tensor, None}, default: None
            if given, centroids will be initialized with given tensor
            if None, centroids will be randomly chosen from X

          Return:
          labels: torch.Tensor, shape: [n_samples]
        """
        assert isinstance(X, torch.Tensor), "input must be torch.Tensor"
        assert X.dtype in [torch.half, torch.float, torch.double], "input must be floating point"
        assert X.ndim == 2, "input must be a 2d tensor with shape: [n_samples, n_features] "

        batch_size, emb_dim = X.shape
        X = X.to(self.device)
        if sample_weight is None:
            sample_weight = torch.ones(batch_size, device=self.device, dtype=X.dtype)
        else:
            sample_weight = sample_weight.to(self.device)
        start_time = time()
        cluster_centers_ = centroids
        num_points_in_clusters = torch.ones(self.n_clusters, device=self.device, dtype=X.dtype)
        closest = None
        closest, cluster_centers_, i, sample_weight, sim_score = self.fit_loop(X, batch_size, closest, cluster_centers_,
                                                                               num_points_in_clusters, sample_weight)

        if self.verbose >= 1:
            print(
                f'used {i + 1} iterations ({round(time() - start_time, 4)}s) to cluster {batch_size} items into {self.n_clusters} clusters')

        inertia = (sim_score * sample_weight).sum().neg()
        return cluster_centers_.detach(), closest.detach(), inertia.detach()

    @torch.compile
    def fit_loop(self, X, batch_size, closest, cluster_centers_, num_points_in_clusters, sample_weight):
        for i in range(self.max_iter):
            iter_time = time()
            if self.minibatch is not None:
                minibatch_idx = np.random.choice(batch_size, size=[self.minibatch], replace=False)
                x = X[minibatch_idx]
                sample_weight = sample_weight[minibatch_idx]
            else:
                x = X

            sim_score, closest = self.max_sim(a=x, b=cluster_centers_)
            matched_clusters, counts = closest.unique(return_counts=True)
            unmatched_clusters = torch.where(
                torch.ones(len(cluster_centers_), dtype=torch.bool, device=self.device).index_fill_(0,
                                                                                                    matched_clusters.long(),
                                                                                                    False) == True)[0]
            # reallocate unmatched clusters according to the machanism described
            # in https://github.com/scikit-learn/scikit-learn/blob/4af30870b0a09bf0a04d704bea4c5d861eae7c83/sklearn/cluster/_k_means_lloyd.pyx#L156
            while unmatched_clusters.shape[0] > 0:
                worst_x = x[sim_score.argmin(dim=0)]
                cluster_centers_[unmatched_clusters[0]] = worst_x
                sim_score, closest = self.max_sim(a=x, b=cluster_centers_)
                matched_clusters, counts = closest.unique(return_counts=True)
                unmatched_clusters = torch.where(
                    torch.ones(len(cluster_centers_), dtype=torch.bool, device=self.device).index_fill_(0,
                                                                                                        matched_clusters.long(),
                                                                                                        False) == True)[
                    0]

            c_grad = torch.zeros_like(cluster_centers_)
            expanded_closest = closest[None].expand(self.n_clusters, -1)
            mask = (expanded_closest == torch.arange(self.n_clusters, device=self.device)[:, None]).to(
                X.dtype)  # [n_clusters, minibatch] one-hot sample masks for each cluster
            mask = mask * sample_weight[None, :]
            c_grad = mask @ x / mask.sum(-1)[..., :, None]
            c_grad[c_grad != c_grad] = 0  # remove NaNs

            error = (c_grad - cluster_centers_).pow(2).sum()
            if self.minibatch is not None:
                lr = 1 / num_points_in_clusters[:, None] * 0.9 + 0.1
                # lr = 1/num_points_in_clusters[:,None]**0.1
            else:
                lr = 1
            num_points_in_clusters[matched_clusters] += counts
            cluster_centers_ = cluster_centers_ * (1 - lr) + c_grad * lr
            if self.verbose >= 2:
                print('iter:', i, 'error:', error.item(), 'time spent:', round(time() - iter_time, 4))
            if error <= self.tol:
                break
        return closest, cluster_centers_, i, sample_weight, sim_score

    def predict(self, X):
        """
          Predict the closest cluster each sample in X belongs to

          Parameters:
          X: torch.Tensor, shape: [n_samples, n_features]

          Return:
          labels: torch.Tensor, shape: [n_samples]
        """
        assert isinstance(X, torch.Tensor), "input must be torch.Tensor"
        assert X.dtype in [torch.half, torch.float, torch.double], "input must be floating point"
        assert X.ndim == 2, "input must be a 2d tensor with shape: [n_samples, n_features] "

        return self.max_sim(a=X, b=self.cluster_centers_)[1]

    def fit(self, X, sample_weight=None, centroids=None):
        """
          Perform kmeans clustering

          Parameters:
          X: torch.Tensor, shape: [n_samples, n_features]
        """
        assert isinstance(X, torch.Tensor), "input must be torch.Tensor"
        assert X.dtype in [torch.half, torch.float, torch.double], "input must be floating point"
        assert X.ndim == 2, "input must be a 2d tensor with shape: [n_samples, n_features] "

        self.cluster_centers_, self.labels_, self.inertia_ = [], [], []

        if centroids is None:
            cluster_centers_ = [init_methods[self.init_method](X, self.n_clusters, self.minibatch) for _ in range(self.n_init)]
        else:
            cluster_centers_ = centroids
        cluster_centers_ = torch.stack(cluster_centers_)

        # self.cluster_centers_, self.labels, self.inertia = torch.compile(torch.vmap(self.fit_predict, in_dims=(None, None, 0)))(X, sample_weight, cluster_centers_)

        for i in range(self.n_init):
            cluster_centers, labels, inertia = self.fit_predict(X, sample_weight, cluster_centers_[i])
            self.cluster_centers_.append(cluster_centers)
            self.labels_.append(labels)
            self.inertia_.append(inertia)
        best_cluster_idx = torch.argmin(torch.stack(self.inertia_))
        self.cluster_centers_, self.labels_, self.inertia_ = self.cluster_centers_[best_cluster_idx].cpu().numpy(), self.labels_[best_cluster_idx].cpu().numpy(), self.inertia_[best_cluster_idx].cpu().numpy()
        return self
File Path: ATTA/utils/fast_pytorch_kmeans/multi_kmeans.py
Content:
import math
import torch
from time import time
import numpy as np
from .init_methods import init_methods


class MultiKMeans:
    '''
    Kmeans clustering algorithm implemented with PyTorch
    Parameters:
      n_clusters: int,
        Number of clusters
      max_iter: int, default: 100
        Maximum number of iterations
      tol: float, default: 0.0001
        Tolerance

      verbose: int, default: 0
        Verbosity
      mode: {'euclidean', 'cosine'}, default: 'euclidean'
        Type of distance measure
      init_method: {'gaussian', 'random', 'k-means++'}
        Type of initialization
      minibatch: {None, int}, default: None
        Batch size of MinibatchKmeans algorithm
        if None perform full KMeans algorithm

    Attributes:
      centroids: torch.Tensor, shape: [n_clusters, n_features]
        cluster centroids
    '''

    def __init__(self, n_clusters, n_kmeans, max_iter=100, tol=0.0001, verbose=0, mode="euclidean",
                 init_method='kmeans++', minibatch=None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.verbose = verbose
        self.mode = mode
        self.init_method = init_method
        self.minibatch = minibatch
        self._loop = False
        self._show = False

        try:
            import PYNVML
            self._pynvml_exist = True
        except ModuleNotFoundError:
            self._pynvml_exist = False

        self.centroids = None

    @staticmethod
    def cos_sim(a, b):
        """
          Compute cosine similarity of 2 sets of vectors
          Parameters:
          a: torch.Tensor, shape: [m, n_features]
          b: torch.Tensor, shape: [n, n_features]
        """
        a_norm = a.norm(dim=-1, keepdim=True)
        b_norm = b.norm(dim=-1, keepdim=True)
        a = a / (a_norm + 1e-8)
        b = b / (b_norm + 1e-8)
        return a @ b.transpose(-2, -1)

    @staticmethod
    def euc_sim(a, b):
        """
          Compute euclidean similarity of 2 sets of vectors
          Parameters:
          a: torch.Tensor, shape: [m, n_features]
          b: torch.Tensor, shape: [n, n_features]
        """
        return 2 * a @ b.transpose(-2, -1) - (a ** 2).sum(dim=-1)[..., :, None] - (b ** 2).sum(dim=-1)[..., None, :]

    def remaining_memory(self):
        """
          Get remaining memory in gpu
        """
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
        if self._pynvml_exist:
            pynvml.nvmlInit()
            gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)
            remaining = info.free
        else:
            remaining = torch.cuda.memory_allocated()
        return remaining

    def max_sim(self, a, b):
        """
          Compute maximum similarity (or minimum distance) of each vector
          in a with all of the vectors in b
          Parameters:
          a: torch.Tensor, shape: [m, n_features]
          b: torch.Tensor, shape: [n, n_features]
        """
        device = a.device.type
        n_samples = a.shape[-2]
        if self.mode == 'cosine':
            sim_func = self.cos_sim
        elif self.mode == 'euclidean':
            sim_func = self.euc_sim

        sim = sim_func(a, b)
        max_sim_v, max_sim_i = sim.max(dim=-1)
        return max_sim_v, max_sim_i

    def fit_predict(self, X, centroids=None):
        """
          Combination of fit() and predict() methods.
          This is faster than calling fit() and predict() seperately.
          Parameters:
          X: torch.Tensor, shape: [n_samples, n_features]
          centroids: {torch.Tensor, None}, default: None
            if given, centroids will be initialized with given tensor
            if None, centroids will be randomly chosen from X
          Return:
          labels: torch.Tensor, shape: [n_samples]
        """
        assert isinstance(X, torch.Tensor), "input must be torch.Tensor"
        assert X.dtype in [torch.half, torch.float, torch.double], "input must be floating point"
        assert X.ndim == 3, "input must be a 3d tensor with shape: [n_kmeans, n_samples, n_features]"

        n_kmeans, n_samples, n_features = X.shape
        self.n_kmeans = n_kmeans

        device = X.device.type
        start_time = time()
        if self.centroids is None:
            self.centroids = torch.stack(
                [init_methods[self.init_method](X[n], self.n_clusters, self.minibatch) for n in range(X.shape[0])],
                dim=0)

        if centroids is not None:
            self.centroids = centroids
        num_points_in_clusters = torch.ones(self.n_kmeans, self.n_clusters, device=device, dtype=X.dtype)
        closest = None
        for i in range(self.max_iter):
            iter_time = time()
            if self.minibatch is not None:
                x = X[:, np.random.choice(n_samples, size=[self.minibatch], replace=False)]
            else:
                x = X
            closest = self.max_sim(a=x, b=self.centroids)[1]
            uniques = [closest[i].unique(return_counts=True) for i in range(self.n_kmeans)]
            c_grad = torch.zeros_like(self.centroids)

            expanded_closest = closest[:, None].expand(-1, self.n_clusters, -1)
            mask = (expanded_closest == torch.arange(self.n_clusters, device=device)[None, :, None]).to(X.dtype)
            c_grad = mask @ x / mask.sum(-1, keepdim=True)
            c_grad[c_grad != c_grad] = 0  # remove NaNs

            error = (c_grad - self.centroids).pow(2).sum()
            if self.minibatch is not None:
                lr = 1 / num_points_in_clusters[:, :, None] * 0.9 + 0.1
            else:
                lr = 1
            for j in range(self.n_kmeans):
                num_points_in_clusters[j, uniques[j][0]] += uniques[j][1]
            self.centroids = self.centroids * (1 - lr) + c_grad * lr
            if self.verbose >= 2:
                print('iter:', i, 'error:', error.item(), 'time spent:', round(time() - iter_time, 4))
            if error <= self.tol * self.n_kmeans:
                break

        if self.verbose >= 1:
            print(
                f'used {i + 1} iterations ({round(time() - start_time, 4)}s) to cluster {self.n_kmeans}x{n_samples} items into {self.n_clusters} clusters')
        return closest

    def predict(self, X):
        """
          Predict the closest cluster each sample in X belongs to
          Parameters:
          X: torch.Tensor, shape: [n_kmeans, n_samples, n_features]
          Return:
          labels: torch.Tensor, shape: [n_kmeans, n_samples]
        """
        assert isinstance(X, torch.Tensor), "input must be torch.Tensor"
        assert X.dtype in [torch.half, torch.float, torch.double], "input must be floating point"
        assert X.ndim == 3, "input must be a 3d tensor with shape: [n_kmeans, n_samples, n_features]"

        return self.max_sim(a=X, b=self.centroids)[1]

    def fit(self, X, centroids=None):
        """
          Perform kmeans clustering
          Parameters:
          X: torch.Tensor, shape: [n_kmeans, n_samples, n_features]
        """
        assert isinstance(X, torch.Tensor), "input must be torch.Tensor"
        assert X.dtype in [torch.half, torch.float, torch.double], "input must be floating point"
        assert X.ndim == 3, "input must be a 3d tensor with shape: [n_kmeans, n_samples, n_features]"

        self.fit_predict(X, centroids)
File Path: ATTA/utils/initial.py
Content:
r"""Initial process for fixing all possible random seed.
"""

import random

import numpy as np
import torch

from ATTA.utils.config_reader import Conf


def reset_random_seed(config: Conf):
    r"""
    Initial process for fixing all possible random seed.

    Args:
       config (Conf): munchified dictionary of args (:obj:`config.random_seed`)


    """
    # Fix Random seed
    random.seed(config.random_seed)
    np.random.seed(config.random_seed)
    torch.manual_seed(config.random_seed)
    torch.cuda.manual_seed(config.random_seed)
    torch.cuda.manual_seed_all(config.random_seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Default state is a training state
    torch.enable_grad()

File Path: ATTA/utils/load_manager.py
Content:
from typing import Union, Dict

import torch.nn
from torch.utils.data import DataLoader

from ATTA import register
from ATTA.kernel.launchers.basic_launcher import Launcher
from ATTA.utils.config_reader import Conf
from ATTA.utils.initial import reset_random_seed


def load_launcher(name: str) -> Launcher:
    r"""
    A launcher loader.
    Args:
        name (str): Name of the chosen launcher

    Returns:
        A instantiated launcher.

    """
    try:
        launcher = register.launchers[name]()
    except KeyError as e:
        print(f'#E#Launcher {name} dose not exist.')
        raise e
    return launcher

def load_atta_algorithm(config: Conf):
    r"""
    A pipeline loader.
    Args:
        name (str): Name of the chosen pipeline
        config (Conf): Please refer to specific GNNs for required configs and formats.

    Returns:
        A instantiated pipeline.

    """
    try:
        reset_random_seed(config)
        pipeline = register.algs[config.atta.name](config)
    except KeyError as e:
        print(f'#E#TTA algorithm {config.atta.name} dose not exist.')
        raise e
    return pipeline

File Path: ATTA/utils/logger.py
Content:
r"""A logger related utils file: tqdm style, logger loader.
"""
import os
from datetime import datetime

from cilog import create_logger
from torch.utils.tensorboard import SummaryWriter

pbar_setting = {'colour': '#a48fff', 'bar_format': '{l_bar}{bar:20}{r_bar}',
                'dynamic_ncols': True, 'ascii': '░▒█'}

from ATTA.utils.config_reader import Conf


def load_logger(config: Conf, sub_print=True):
    r"""
    Logger loader

    Args:
        config (Conf): munchified dictionary of args (:obj:`config.log_path`, :obj:`config.tensorboard_logdir`, :obj:`config.log_file`)
        sub_print (bool): Whether the logger substitutes general print function. If Ture, logger.info will be equal to
            print(f'#IN#Message'), where #IN# represents info. Similarly, other level of log can be used by adding prefixes
            (Not capital sensitive): Debug: #d#, #De#, #Debug#, etc. Info: #I#, #In#, #inf#, #INFO#, etc. Important: #IM#,
            #important#, etc. Warning: #W#, #war#, etc. Error: #E#, #err#, etc. Critical: #C#, #Cri#, #critical#, etc. If
            there is no prefix, the general print function will be used.

    Returns:
        [cilog Logger, tensorboard summary writer]

    """
    if sub_print:
        print("This logger will substitute general print function")
    logger = create_logger(name='GNN_log',
                           file=config.log_path,
                           enable_mail=False,
                           sub_print=sub_print)

    current_time = datetime.now().strftime('%b%d_%H-%M-%S')
    writer = SummaryWriter(
        log_dir=os.path.join(config.tensorboard_logdir, f'{config.log_file}_{current_time}'))
    return logger, writer

File Path: ATTA/utils/metric.py
Content:
r"""A metric function module that is consist of a Metric class which incorporate many score and loss functions.
"""

from math import sqrt

import torch
from sklearn.metrics import roc_auc_score as sk_roc_auc, mean_squared_error, \
    accuracy_score, average_precision_score, mean_absolute_error, f1_score, matthews_corrcoef
from torch.nn.functional import cross_entropy, l1_loss, binary_cross_entropy_with_logits





class Metric(object):
    r"""
    Metric function module that is consist of a Metric class which incorporate many score and loss functions
    """


    def __init__(self):
        self.task2loss = {
            'Binary classification': binary_cross_entropy_with_logits,
            'Multi-label classification': self.cross_entropy_with_logit,
            'Regression': l1_loss
        }
        self.score_name2score = {
            'RMSE': self.rmse,
            'MAE': mean_absolute_error,
            'Average Precision': self.ap,
            'F1': self.f1,
            'ROC-AUC': self.roc_auc_score,
            'Accuracy': self.acc,
            'MCC': self.mcc
        }
        self.loss_func = self.cross_entropy_with_logit
        self.score_func = self.roc_auc_score
        self.dataset_task = ''
        self.score_name = ''

        self.lower_better = -1

        self.best_stat = {'score': None, 'loss': float('inf')}
        self.id_best_stat = {'score': None, 'loss': float('inf')}

    def set_loss_func(self, task_name):
        r"""
        Set the loss function

        Args:
            task_name (str): name of task

        Returns:
            None

        """
        self.dataset_task = task_name
        self.loss_func = self.task2loss.get(task_name)
        assert self.loss_func is not None

    def set_score_func(self, metric_name):
        r"""
        Set the metric function

        Args:
            metric_name: name of metric

        Returns:
            None

        """
        self.score_func = self.score_name2score.get(metric_name)
        assert self.score_func is not None
        self.score_name = metric_name.upper()
        if self.score_name in ['RMSE', 'MAE']:
            self.lower_better = 1
        else:
            self.lower_better = -1

    def mcc(self, y_true, y_pred):
        r"""
        Calculate Matthews correlation coefficient score

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            Matthews correlation coefficient score

        """

        true = torch.tensor(y_true)
        pred_label = torch.tensor(y_pred)
        pred_label = pred_label.round() if self.dataset_task == "Binary classification" else torch.argmax(pred_label,
                                                                                                          dim=1)
        return matthews_corrcoef(true, pred_label)

    def f1(self, y_true, y_pred):
        r"""
        Calculate F1 score

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            F1 score

        """
        true = torch.tensor(y_true)
        pred_label = torch.tensor(y_pred)
        pred_label = pred_label.round() if self.dataset_task == "Binary classification" else torch.argmax(pred_label,
                                                                                                            dim=1)
        return f1_score(true, pred_label, average='micro')

    def ap(self, y_true, y_pred):
        r"""
        Calculate AP score

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            AP score

        """
        return average_precision_score(torch.tensor(y_true).long(), torch.tensor(y_pred))

    def roc_auc_score(self, y_true, y_pred):
        r"""
        Calculate roc_auc score

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            roc_auc score

        """
        if y_true.max() == 1 and y_pred.ndim > 1:
            y_pred = y_pred[:, 1]
        return sk_roc_auc(torch.tensor(y_true).long(), torch.tensor(y_pred), multi_class='ovo')

    def reg_absolute_error(self, y_true, y_pred):
        r"""
        Calculate absolute regression error

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            absolute regression error

        """
        return mean_absolute_error(torch.tensor(y_true), torch.tensor(y_pred))

    def acc(self, y_true, y_pred):
        r"""
        Calculate accuracy score

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            accuracy score

        """
        true = torch.tensor(y_true) if not isinstance(y_true, torch.Tensor) else y_true.clone().detach()
        pred_label = torch.tensor(y_pred) if not isinstance(y_pred, torch.Tensor) else y_pred.clone().detach()
        pred_label = pred_label.round() if self.dataset_task == "Binary classification" else torch.argmax(pred_label,
                                                                                                            dim=1)
        return accuracy_score(true.cpu(), pred_label.cpu())

    def rmse(self, y_true, y_pred):
        r"""
        Calculate RMSE

        Args:
            y_true (torch.tensor): input labels
            y_pred (torch.tensor): label predictions

        Returns (float):
            RMSE

        """
        return sqrt(mean_squared_error(y_true, y_pred))

    def cross_entropy_with_logit(self, y_pred: torch.Tensor, y_true: torch.Tensor, **kwargs):
        r"""
        Calculate cross entropy loss

        Args:
            y_pred (torch.tensor): label predictions
            y_true (torch.tensor): input labels
            **kwargs: key word arguments for the use of :func:`~torch.nn.functional.cross_entropy`

        Returns:
            cross entropy loss

        """
        return cross_entropy(y_pred, y_true, **kwargs)


File Path: ATTA/utils/register.py
Content:
r"""A kernel module that contains a global register for unified model, dataset, and OOD algorithms access.
"""


class Register(object):
    r"""
    Global register for unified model, dataset, and OOD algorithms access.
    """

    def __init__(self):
        self.launchers = dict()
        self.models = dict()
        self.datasets = dict()
        self.algs = dict()

    def launcher_register(self, launcher_class):
        r"""
        Register for pipeline access.

        Args:
            launcher_class (class): pipeline class

        Returns (class):
            pipeline class

        """
        self.launchers[launcher_class.__name__] = launcher_class
        return launcher_class

    def model_register(self, model_class):
        r"""
        Register for model access.

        Args:
            model_class (class): model class

        Returns (class):
            model class

        """
        self.models[model_class.__name__] = model_class
        return model_class

    def dataset_register(self, dataset_class):
        r"""
        Register for dataset access.

        Args:
            dataset_class (class): dataset class

        Returns (class):
            dataset class

        """
        self.datasets[dataset_class.__name__] = dataset_class
        return dataset_class

    def alg_register(self, alg_class):
        r"""
        Register for OOD algorithms access.

        Args:
            alg_class (class): OOD algorithms class

        Returns (class):
            OOD algorithms class

        """
        self.algs[alg_class.__name__] = alg_class
        return alg_class


register = Register()  #: The ATTA register object used for accessing models, datasets and OOD algorithms.



File Path: ATTA/utils/table_extractor.py
Content:

File Path: ATTA/utils/train.py
Content:
r"""Training utils.
"""

import torch
from torch import Tensor
from torch.nn.functional import gumbel_softmax
from torch_geometric.data import Batch

from ATTA.utils.config_reader import Conf


def nan2zero_get_mask(data, task, config: Conf):
    r"""
    Training data filter masks to process NAN.

    Args:
        data (Batch): input data
        task (str): mask function type
        config (Conf): munchified dictionary of args (:obj:`config.model.model_level`)

    Returns (Tensor):
        [mask (Tensor) - NAN masks for data formats, targets (Tensor) - input labels]

    """
    if config.model.model_level == 'node':
        if 'train' in task:
            mask = data.train_mask
        elif task == 'id_val':
            mask = data.get('id_val_mask')
        elif task == 'id_test':
            mask = data.get('id_test_mask')
        elif task == 'val':
            mask = data.val_mask
        elif task == 'test':
            mask = data.test_mask
        else:
            raise ValueError(f'Task should be train/id_val/id_test/val/test, but got {task}.')
    else:
        mask = ~torch.isnan(data.y)
    if mask is None:
        return None, None
    targets = torch.clone(data.y).detach()
    targets[~mask] = 0

    return mask, targets


def gumbel_sigmoid(logits: Tensor, tau: float = 1) -> Tensor:
    r"""
    Gumbel sigmoid trick.

    Implemented by using gumbel_softmax from PyTorch.
    Args:
        logits (Tensor): The logits input before sigmoid function.
        tau (float): The temperature of gumbel-sigmoid. Default 1.

    Returns: Gumbel softly sampled mask.

    """
    return gumbel_softmax(torch.stack([logits, torch.zeros_like(logits)], dim=0), dim=0, tau=tau)[0]


def at_stage(i, config):
    r"""
    Test if the current training stage at stage i.

    Args:
        i: Stage that is possibly 1, 2, 3, ...
        config: config object.

    Returns: At stage i.

    """
    if i - 1 < 0:
        raise ValueError(f"Stage i must be equal or larger than 0, but got {i}.")
    if i > len(config.train.stage_stones):
        raise ValueError(f"Stage i should be smaller than the largest stage {len(config.train.stage_stones)},"
                         f"but got {i}.")
    if i - 2 < 0:
        return config.train.epoch <= config.train.stage_stones[i - 1]
    else:
        return config.train.stage_stones[i - 2] < config.train.epoch <= config.train.stage_stones[i - 1]

File Path: setup.py
Content:
import setuptools

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

install_requires = [
    # 'cilog>=1.2.3',
]

setuptools.setup(
    name="atta",
    version="0.0.2",
    author="",
    author_email="",
    description=".",
    long_description=long_description,
    long_description_content_type="text/markdown",
    license='GPLv3',
    url=".",
    project_urls={
        "Bug Tracker": ".",
    },
    classifiers=[
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.11",
        "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
        "Operating System :: OS Independent",
    ],
    package_dir={"ATTA": "ATTA"},
    install_requires=install_requires,
    entry_points = {
        'console_scripts': [
            # 'gttatg = ATTA.kernel.main:gttatg',
            'attatg = ATTA.kernel.alg_main:main',
            'attatl = ATTA.kernel.launch:launch'
        ]
    },
    python_requires=">=3.8",
)
Output:
{
    "experimental_code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1\n",
    "experimental_info": "The SimATTA algorithm addresses dynamic model optimization for streaming test data. It employs a teacher-student framework where a `teacher` model, a deep copy of the initially trained source model, is used for pseudo-labeling low-entropy samples. The `model` (student) is fine-tuned. The core process involves partitioning incoming unlabeled test samples into high- and low-entropy sets using `softmax_entropy`.\n\n**Low-entropy samples (Pseudo-labeling):**\n- Identified by `ent_beta=0` and `entropy < ent_bound` (where `ent_bound` is `self.config.atta.SimATTA.el`).\n- Pseudo-labels are generated using the `teacher` model's predictions (`outputs.argmax(1)`).\n- These samples are stored as `source_anchors` using `update_anchors`.\n- `self.LE` (controlled by `atta.SimATTA.LE`) determines whether to use low-entropy samples. When `LE` is 0, no source anchors are collected.\n\n**High-entropy samples (Active Labeling & Incremental Clustering):**\n- Identified by `ent_beta=1` and `entropy >= ent_bound` (where `ent_bound` is `self.config.atta.SimATTA.eh`).\n- An incremental clustering technique (`target_cluster` flag) employs weighted K-means (`KMeans` from `ATTA.utils.fast_pytorch_kmeans` or `sklearn.cluster.KMeans` with `sample_weight`).\n- The clustering identifies representative 'anchors' of seen distributions, managing a budget.\n- `self.n_clusters` (initial 10) for target clusters increases by `self.nc_increase` (controlled by `atta.SimATTA.nc_increase`, tested with values like 0.25, 0.5, ..., 3).\n- Selected high-entropy samples are stored as `anchors` (`target_anchors`) using `update_anchors`.\n\n**Model Fine-tuning:**\n- The model is fine-tuned (`cluster_train`) on both actively labeled high-entropy samples (`target_anchors`) and pseudo-labeled low-entropy samples (`source_anchors`).\n- Influence is balanced using `alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`.\n- During a `cold_start` phase (`atta.SimATTA.cold_start`, set to 100), `alpha` is capped at `min(0.2, alpha)`.\n- The optimizer used is `torch.optim.SGD` with learning rate `atta.SimATTA.lr` and momentum 0.9.\n- Training steps per batch are controlled by `atta.SimATTA.steps`.\n- Training stops if a `loss_window` (size `atta.SimATTA.stop_tol`) shows no improvement for 5 consecutive checks.\n\n**Budget Management:**\n- `self.budgets` accumulates the number of actively labeled samples (length of `closest` in `sample_select`).\n\n**Experimental Settings from `ATTA/kernel/launch.py`:**\n- **Datasets:** Primarily `VLCS` (with `el=1e-3`) and potentially `PACS` (with `el=1e-4`).\n- **Hyperparameters:**\n    - `atta.SimATTA.cold_start`: 100\n    - `atta.SimATTA.el` (low-entropy bound): 1e-3 (for VLCS), 1e-4 (for PACS)\n    - `atta.SimATTA.nc_increase` (cluster increase rate, denoted as `k`): Tested values include 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3.\n    - `atta.SimATTA.LE` (low-entropy sample usage, denoted as `le`): Tested values 0 (False) and 1 (True).\n    - `atta.SimATTA.target_cluster` (incremental clustering, denoted as `ic`): Tested values 0 (False) and 1 (True).\n    - `atta.gpu_clustering`: True (GPU-accelerated clustering).\n- **General Settings:**\n    - `exp_round`: 1\n    - `num_workers`: 4\n- **Adaptation Loop:** The adaptation process iterates through `self.config.dataset.test_envs[1:]` (target domains)."
}
