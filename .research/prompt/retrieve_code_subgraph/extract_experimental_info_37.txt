
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The core methodology revolves around optimization-based meta-learning, building upon the Model-Agnostic Meta-Learning (MAML) and Online-aware Meta-Learning (OML) objectives. The base algorithm, C-MAML, optimizes an online OML objective (Eq. 5) to minimize cumulative risk across tasks. This objective's k-step MAML update is shown to be equivalent to an asymmetric CL objective (Eq. 6) that aligns the current task's gradients with the average gradient of previous tasks, offering a speedup over MER. La-MAML extends C-MAML by incorporating learnable, per-parameter LRs into the inner updates (Algorithm 1). These LRs are updated asynchronously in the meta-update, with their gradients (gMAML(αj)) reflecting the alignment between old and new tasks (Eq. 8). The LRs are clipped to positive values to prevent ascending gradients and mitigate catastrophic forgetting. This asynchronous update mechanism allows the model to conservatively modulate the pace and direction of learning, thereby accelerating progress on new tasks while facilitating knowledge transfer from old ones. The approach integrates concepts from replay-based and prior-based methods by using a replay buffer and data-driven LR modulation.

# Repository Content
File Path: dataloaders/cifar_info.py
Content:
from __future__ import print_function
from PIL import Image
import os
import os.path
import numpy as np
import sys

if sys.version_info[0] == 2:
    import cPickle as pickle
else:
    import pickle

from torchvision.datasets.vision import VisionDataset
from torchvision.datasets.utils import check_integrity, download_and_extract_archive
import ipdb


# defining the mapping from parent classes to fine-grained classes in cifar
# in case one needs to split tasks by parent class
super_class_to_class = {
    'aquatic_mammals'   :  ['beaver', 'dolphin', 'otter', 'seal', 'whale'],
    'fish'  :  ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],
    'flowers'  :   ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],
    'food_containers'  :   ['bottle', 'bowl', 'can', 'cup', 'plate'],
    'fruit_and_vegetables'  :  ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],
    'household_electrical_devices' :   ['clock', 'keyboard', 'lamp', 'telephone', 'television'],
    'household_furniture'  :   ['bed', 'chair', 'couch', 'table', 'wardrobe'],
    'insects'  :   ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],
    'large_carnivores'  :  ['bear', 'leopard', 'lion', 'tiger', 'wolf'],
    'large_man-made_outdoor_things' :  ['bridge', 'castle', 'house', 'road', 'skyscraper'],
    'large_natural_outdoor_scenes'  :  ['cloud', 'forest', 'mountain', 'plain', 'sea'],
    'large_omnivores_and_herbivores' : ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],
    'medium_mammals'  :  ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],
    'non-insect_invertebrates'  :  ['crab', 'lobster', 'snail', 'spider', 'worm'],
    'people'  : ['baby', 'boy', 'girl', 'man', 'woman'],
    'reptiles'  :  ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],
    'small_mammals'  : ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],
    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],
    'vehicles_1' :  ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],
    'vehicles_2'  : ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']
}

class CIFAR10(VisionDataset):
    """`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.

    Args:
        root (string): Root directory of dataset where directory
            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.
        train (bool, optional): If True, creates dataset from training set, otherwise
            creates from test set.
        transform (callable, optional): A function/transform that takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
        download (bool, optional): If true, downloads the dataset from the internet and
            puts it in root directory. If dataset is already downloaded, it is not
            downloaded again.

    """
    base_folder = 'cifar-10-batches-py'
    url = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
    filename = "cifar-10-python.tar.gz"
    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'
    train_list = [
        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],
        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],
        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],
        ['data_batch_4', '634d18415352ddfa80567beed471001a'],
        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],
    ]

    test_list = [
        ['test_batch', '40351d587109b95175f43aff81a1287e'],
    ]
    meta = {
        'filename': 'batches.meta',
        'key': 'label_names',
        'md5': '5ff9c542aee3614f3951f8cda6e48888',
    }

    def __init__(self, root, train=True, transform=None, target_transform=None,
                 download=False):

        super(CIFAR10, self).__init__(root, transform=transform,
                                      target_transform=target_transform)

        self.train = train  # training set or test set

        if download:
            self.download()

        if not self._check_integrity():
            raise RuntimeError('Dataset not found or corrupted.' +
                               ' You can use download=True to download it')

        if self.train:
            downloaded_list = self.train_list
        else:
            downloaded_list = self.test_list

        self.high_level_supervise = True
        self.data = []
        self.targets = []
        self.super_targets = []

        # now load the picked numpy arrays
        for file_name, checksum in downloaded_list:
            file_path = os.path.join(self.root, self.base_folder, file_name)
            with open(file_path, 'rb') as f:
                if sys.version_info[0] == 2:
                    entry = pickle.load(f)
                else:
                    entry = pickle.load(f, encoding='latin1')
                self.data.append(entry['data'])

                if 'labels' in entry:
                    self.targets.extend(entry['labels'])
                else:
                    self.targets.extend(entry['fine_labels'])             

                self.super_targets.extend(entry['coarse_labels'])           


        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)
        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC
    

        self._load_meta()

    def _load_meta(self):
        path = os.path.join(self.root, self.base_folder, self.meta['filename'])

        if not check_integrity(path, self.meta['md5']):
            raise RuntimeError('Dataset metadata file not found or corrupted.' +
                               ' You can use download=True to download it')
        with open(path, 'rb') as infile:
            if sys.version_info[0] == 2:
                data = pickle.load(infile)
            else:
                data = pickle.load(infile, encoding='latin1')

            self.classes = data[self.meta['key']]
            self.super_classes = data[self.meta['coarse_key']]

        self.get_class_ids()

    def get_class_ids(self):
        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}
        self.super_class_to_idx = {_class: i for i, _class in enumerate(self.super_classes)}

        high_ids = []
        low_ids = []
        low_idxs = np.arange(len(self.classes))
        for key in super_class_to_class:
            for classes in super_class_to_class[key]:
                high_ids.append(self.super_class_to_idx[key])
                low_ids.append(self.class_to_idx[classes])

        high_ids_np = np.array(high_ids) 
        low_ids_np = np.array(low_ids)
        self.low_high_map = np.stack([low_ids_np, high_ids_np], axis = 1)

    def __getitem__(self, index):
        """
        Args:
            index (int): Index

        Returns:
            tuple: (image, target) where target is index of the target class.
        """

        img, target, super_target = self.data[index], self.targets[index], self.super_targets[index]

        if(self.high_level_supervise):
            target = super_target


        # doing this so that it is consistent with all other datasets
        # to return a PIL Image
        img = Image.fromarray(img)

        if self.transform is not None:
            img = self.transform(img)

        if self.target_transform is not None:
            target = self.target_transform(target)

        return img, target, super_target


    def __len__(self):
        return len(self.data)

    def _check_integrity(self):
        root = self.root
        for fentry in (self.train_list + self.test_list):
            filename, md5 = fentry[0], fentry[1]
            fpath = os.path.join(root, self.base_folder, filename)
            if not check_integrity(fpath, md5):
                return False
        return True

    def download(self):
        if self._check_integrity():
            print('Files already downloaded and verified')
            return
        download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)

    def extra_repr(self):
        return "Split: {}".format("Train" if self.train is True else "Test")



class CIFAR100(CIFAR10):
    """`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.

    This is a subclass of the `CIFAR10` Dataset.
    """
    base_folder = 'cifar-100-python'
    url = "https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz"
    filename = "cifar-100-python.tar.gz"
    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'
    train_list = [
        ['train', '16019d7e3df5f24257cddd939b257f8d'],
    ]

    test_list = [
        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],
    ]
    meta = {
        'filename': 'meta',
        'key': 'fine_label_names',
        'coarse_key': 'coarse_label_names',        
        'md5': '7973b15100ade9c7d40fb424638fde48',
    }


File Path: dataloaders/class_incremental_loader.py
Content:
import random

import numpy as np
import torch
from PIL import Image
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import datasets, transforms

from dataloaders.idataset import _get_datasets, DummyDataset

import random
import ipdb

# --------
# Datasets CIFAR and TINYIMAGENET
# --------

class IncrementalLoader:

    def __init__(
        self,
        opt,
        shuffle=True,
        seed=1,
    ):
        self._opt = opt
        dataset_name=opt.dataset
        validation_split=opt.validation
        self.increment=opt.increment

        datasets = _get_datasets(dataset_name)
        self._setup_data(
            datasets,
            class_order_type=opt.class_order,
            seed=seed,
            increment=self.increment,
            validation_split=validation_split
        )
        self.validation_split = validation_split
        self.train_transforms = datasets[0].train_transforms
        self.common_transforms = datasets[0].common_transforms
        self.top_transforms = datasets[0].top_transforms

        self._current_task = 0

        self._batch_size = opt.batch_size
        self._test_batch_size = opt.test_batch_size        
        self._workers = opt.workers
        self._shuffle = shuffle

        self._setup_test_tasks(validation_split)

    @property
    def n_tasks(self):
        return len(self.increments)

    def new_task(self, memory=None):
        if self._current_task >= len(self.increments):
            raise Exception("No more tasks.")

        min_class = sum(self.increments[:self._current_task])
        max_class = sum(self.increments[:self._current_task + 1])
        x_train, y_train = self._select(
            self.data_train, self.targets_train, low_range=min_class, high_range=max_class
        )
        x_val, y_val = self._select(
            self.data_val, self.targets_val, low_range=min_class, high_range=max_class
        )
        x_test, y_test = self._select(self.data_test, self.targets_test, high_range=max_class)

        if memory is not None:
            data_memory, targets_memory = memory
            print("Set memory of size: {}.".format(data_memory.shape[0]))
            x_train = np.concatenate((x_train, data_memory))
            y_train = np.concatenate((y_train, targets_memory))

        train_loader = self._get_loader(x_train, y_train, mode="train")
        val_loader = self._get_loader(x_val, y_val, mode="train") if len(x_val) > 0 else None
        test_loader = self._get_loader(x_test, y_test, mode="test")

        task_info = {
            "min_class": min_class,
            "max_class": max_class,
            "increment": self.increments[self._current_task],
            "task": self._current_task,
            "max_task": len(self.increments),
            "n_train_data": x_train.shape[0],
            "n_test_data": x_test.shape[0]
        }

        self._current_task += 1

        return task_info, train_loader, val_loader, test_loader

    def _setup_test_tasks(self, validation_split):
        self.test_tasks = []
        self.val_tasks = []
        for i in range(len(self.increments)):
            min_class = sum(self.increments[:i])
            max_class = sum(self.increments[:i + 1])

            x_test, y_test = self._select(self.data_test, self.targets_test, low_range=min_class, high_range=max_class)
            self.test_tasks.append(self._get_loader(x_test, y_test, mode="test"))

            if validation_split > 0.0:
                x_val, y_val = self._select(self.data_val, self.targets_val, low_range=min_class, high_range=max_class)
                self.val_tasks.append(self._get_loader(x_val, y_val, mode="test"))

    def get_tasks(self, dataset_type='test'):
        if dataset_type == 'val':
            if self.validation_split > 0.0:
                return self.val_tasks
            else:
                return self.test_tasks
        elif dataset_type == 'test':
            return self.test_tasks
        else:
            raise NotImplementedError("Unknown mode {}.".format(dataset_type))

    def get_dataset_info(self):
        if(self._opt.dataset == 'tinyimagenet'):
            n_inputs = 3*64*64        
        else:
            n_inputs = self.data_train.shape[3]*self.data_train.shape[1]*self.data_train.shape[2]
        n_outputs = self._opt.increment * len(self.increments)
        n_task = len(self.increments)
        return n_inputs, n_outputs, n_task
  
    def _select(self, x, y, low_range=0, high_range=0):
        idxes = np.where(np.logical_and(y >= low_range, y < high_range))[0]
        return x[idxes], y[idxes]

    def _get_loader(self, x, y, shuffle=True, mode="train"):
        if mode == "train":
            pretrsf = transforms.Compose([*self.top_transforms])
            trsf = transforms.Compose([*self.train_transforms, *self.common_transforms])
            batch_size = self._batch_size
        elif mode == "test":
            pretrsf = transforms.Compose([*self.top_transforms])
            trsf = transforms.Compose(self.common_transforms)
            batch_size = self._test_batch_size
        elif mode == "flip":
            trsf = transforms.Compose(
                [transforms.RandomHorizontalFlip(p=1.), *self.common_transforms]
            )
            batch_size = self._test_batch_size
        else:
            raise NotImplementedError("Unknown mode {}.".format(mode))

        return DataLoader(
            DummyDataset(x, y, trsf, pretrsf, self._opt.dataset=='tinyimagenet'),
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=self._workers
        )


    def _setup_data(self, datasets, class_order_type=False, seed=1, increment=10, validation_split=0.):
        # FIXME: handles online loading of images
        self.data_train, self.targets_train = [], []
        self.data_test, self.targets_test = [], []
        self.data_val, self.targets_val = [], []
        self.increments = []
        self.class_order = []

        current_class_idx = 0  # When using multiple datasets
        for dataset in datasets:

            if(self._opt.dataset == 'tinyimagenet'):
                root_path = self._opt.data_path
                train_dataset = dataset.base_dataset(root_path + 'train/')
                test_dataset = dataset.base_dataset(root_path + 'val/')

                train_dataset.data = train_dataset.samples
                test_dataset.data = test_dataset.samples

                x_train, y_train = train_dataset.data, np.array(train_dataset.targets)
                x_val, y_val, x_train, y_train = self._list_split_per_class(
                    x_train, y_train, validation_split
                )
                x_test, y_test = test_dataset.data, np.array(test_dataset.targets)

                order = [i for i in range(len(np.unique(y_train)))]
                if class_order_type == 'random':
                    random.seed(seed)  # Ensure that following order is determined by seed:
                    random.shuffle(order)
                    print("Class order:", order)
                elif class_order_type == 'old' and dataset.class_order is not None:
                    order = dataset.class_order
                else:
                    print("Classes are presented in a chronological order")

            else:
                root_path =  self._opt.data_path
                train_dataset = dataset.base_dataset(root_path, train=True, download=True)
                test_dataset = dataset.base_dataset(root_path, train=False, download=True)

                x_train, y_train = train_dataset.data, np.array(train_dataset.targets)
                x_val, y_val, x_train, y_train = self._split_per_class(
                    x_train, y_train, validation_split
                )
                x_test, y_test = test_dataset.data, np.array(test_dataset.targets)

                order = [i for i in range(len(np.unique(y_train)))]
                if class_order_type == 'random':
                    random.seed(seed)  # Ensure that following order is determined by seed:
                    random.shuffle(order)
                    print("Class order:", order)
                elif class_order_type == 'old' and dataset.class_order is not None:
                    order = dataset.class_order
                elif class_order_type == 'super' and dataset.class_order_super is not None:
                    order = dataset.class_order_super
                else:
                    print("Classes are presented in a chronological order")

            self.class_order.append(order)

            y_train = self._map_new_class_index(y_train, order)
            y_val = self._map_new_class_index(y_val, order)
            y_test = self._map_new_class_index(y_test, order)

            y_train += current_class_idx
            y_val += current_class_idx
            y_test += current_class_idx

            current_class_idx += len(order)
            if len(datasets) > 1:
                self.increments.append(len(order))
            else:
                self.increments = [increment for _ in range(len(order) // increment)]

            self.data_train.append(x_train)
            self.targets_train.append(y_train)
            self.data_val.append(x_val)
            self.targets_val.append(y_val)
            self.data_test.append(x_test)
            self.targets_test.append(y_test)

            self.data_train = np.concatenate(self.data_train)
            self.targets_train = np.concatenate(self.targets_train)
            self.data_val = np.concatenate(self.data_val)
            self.targets_val = np.concatenate(self.targets_val)
            self.data_test = np.concatenate(self.data_test)
            self.targets_test = np.concatenate(self.targets_test)

    @staticmethod
    def _map_new_class_index(y, order):
        """Transforms targets for new class order."""

        return np.array(list(map(lambda x: order.index(x), y)))

    @staticmethod
    def _split_per_class(x, y, validation_split=0.):
        """Splits train data for a subset of validation data.
        Split is done so that each class has a much data.
        """
        shuffled_indexes = np.random.permutation(x.shape[0])
        x = x[shuffled_indexes]
        y = y[shuffled_indexes]

        x_val, y_val = [], []
        x_train, y_train = [], []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            nb_val_elts = int(class_indexes.shape[0] * validation_split)

            val_indexes = class_indexes[:nb_val_elts]
            train_indexes = class_indexes[nb_val_elts:]

            x_val.append(x[val_indexes])
            y_val.append(y[val_indexes])
            x_train.append(x[train_indexes])
            y_train.append(y[train_indexes])

        x_val, y_val = np.concatenate(x_val), np.concatenate(y_val)
        x_train, y_train = np.concatenate(x_train), np.concatenate(y_train)

        return x_val, y_val, x_train, y_train

    @staticmethod
    def _list_split_per_class(x, y, validation_split=0.):
        """Splits train data for a subset of validation data.
        Split is done so that each class has a much data.
        """
        c = list(zip(x, y))
        random.shuffle(c)
        x, y = zip(*c)

        x_val, y_val = [], []
        x_train, y_train = [], []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            nb_val_elts = int(class_indexes.shape[0] * validation_split)

            val_indexes = class_indexes[:nb_val_elts]
            train_indexes = class_indexes[nb_val_elts:]

            x_val_i = [x[i] for i in val_indexes]
            y_val_i = [y[i] for i in val_indexes]

            x_train_i = [x[i] for i in train_indexes]
            y_train_i = [y[i] for i in train_indexes]

            x_val.append(x_val_i)
            y_val.append(y_val_i)

            x_train.append(x_train_i)
            y_train.append(y_train_i)

        x_val, y_val = np.concatenate(x_val), np.concatenate(y_val)
        x_train, y_train = np.concatenate(x_train), np.concatenate(y_train)

        return x_val, y_val, x_train, y_train

    def get_idx_data(self, idx, batch_size, mode="test", data_source="train"):
        """Returns a custom loader with specific idxs only.
        :param idx: A list of data indexes that we want.
        :param mode: Various mode for the transformations applied on it.
        :param data_source: Whether to fetch from the train, val, or test set.
        :return: The raw data and a loader.
        """
        if data_source == "train":
            x, y = self.data_train, self.targets_train
        elif data_source == "val":
            x, y = self.data_val, self.targets_val
        elif data_source == "test":
            x, y = self.data_test, self.targets_test
        else:
            raise ValueError("Unknown data source <{}>.".format(data_source))
        y, sorted_idx = y.sort()

        sampler = torch.utils.data.sampler.SubsetRandomSampler(idx)
        trsf = transforms.Compose(self.common_transforms)

        loader = DataLoader(
            DummyDataset(x[sorted_idx], y, trsf),
            sampler=sampler,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self._workers)

    def get_custom_loader(self, class_indexes, mode="test", data_source="train"):
        """Returns a custom loader.
        :param class_indexes: A list of class indexes that we want.
        :param mode: Various mode for the transformations applied on it.
        :param data_source: Whether to fetch from the train, val, or test set.
        :return: The raw data and a loader.
        """
        if not isinstance(class_indexes, list):  # TODO: deprecated, should always give a list
            class_indexes = [class_indexes]

        if data_source == "train":
            x, y = self.data_train, self.targets_train
        elif data_source == "val":
            x, y = self.data_val, self.targets_val
        elif data_source == "test":
            x, y = self.data_test, self.targets_test
        else:
            raise ValueError("Unknown data source <{}>.".format(data_source))

        data, targets = [], []
        for class_index in class_indexes:
            class_data, class_targets = self._select(
                x, y, low_range=class_index, high_range=class_index + 1
            )
            data.append(class_data)
            targets.append(class_targets)

        data = np.concatenate(data)
        targets = np.concatenate(targets)

        return data, self._get_loader(data, targets, shuffle=False, mode=mode)
File Path: dataloaders/idataset.py
Content:

import numpy as np
from PIL import Image
import torch
from torchvision import datasets, transforms
import os
from dataloaders import cifar_info

class DummyDataset(torch.utils.data.Dataset):

    def __init__(self, x, y, trsf, pretrsf = None, imgnet_like = False, super_y = None):
        self.x, self.y = x, y
        self.super_y = super_y

        # transforms to be applied before and after conversion to imgarray
        self.trsf = trsf
        self.pretrsf = pretrsf

        # if not from imgnet, needs to be converted to imgarray first
        self.imgnet_like = imgnet_like

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, idx):
        x, y = self.x[idx], self.y[idx]
        if self.super_y is not None:
            super_y = self.super_y[idx]

        if(self.pretrsf is not None):
            x = self.pretrsf(x)    
        
        if(not self.imgnet_like):
            x = Image.fromarray(x)
        x = self.trsf(x)

        if self.super_y is not None:
            return x, y, super_y
        else:
            return x, y

class DummyArrayDataset(torch.utils.data.Dataset):

    def __init__(self, x, y):
        self.x, self.y = x, y

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, idx):
        x, y = self.x[idx], self.y[idx]

        return x, y

def _get_datasets(dataset_names):
    return [_get_dataset(dataset_name) for dataset_name in dataset_names.split("-")]


def _get_dataset(dataset_name):
    dataset_name = dataset_name.lower().strip()

    if dataset_name == "cifar10":
        return iCIFAR10
    elif dataset_name == "cifar100":
        return iCIFAR100
    elif dataset_name == "tinyimagenet":
        return iImgnet
    else:
        raise NotImplementedError("Unknown dataset {}.".format(dataset_name))

class DataHandler:
    base_dataset = None
    train_transforms = []
    common_transforms = [transforms.ToTensor()]
    class_order = None
    

class iImgnet(DataHandler):

    base_dataset = datasets.ImageFolder

    top_transforms = [
        lambda x: Image.open(x[0]).convert('RGB'),
    ]

    train_transforms = [
        transforms.RandomCrop(64, padding=4),           
        transforms.RandomHorizontalFlip() #,
        #transforms.ColorJitter(brightness=63 / 255)
    ]

    common_transforms = [
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))    
    ]

    class_order = [                                                                     
        i for i in range(200)
    ]

class iCIFAR10(DataHandler):
    base_dataset = datasets.cifar.CIFAR10
    base_dataset_hierarchy = cifar_info.CIFAR10

    top_transforms = [
    ]


    train_transforms = [
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=63 / 255)
    ]
    common_transforms = [
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ]

class iCIFAR100(iCIFAR10):
    base_dataset = datasets.cifar.CIFAR100
    base_dataset_hierarchy = cifar_info.CIFAR100

    common_transforms = [
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
    ]

    # update: class order can now be chosen randomly since it just depends on seed
    class_order = [
        87, 0, 52, 58, 44, 91, 68, 97, 51, 15, 94, 92, 10, 72, 49, 78, 61, 14, 8, 86, 84, 96, 18,
        24, 32, 45, 88, 11, 4, 67, 69, 66, 77, 47, 79, 93, 29, 50, 57, 83, 17, 81, 41, 12, 37, 59,
        25, 20, 80, 73, 1, 28, 6, 46, 62, 82, 53, 9, 31, 75, 38, 63, 33, 74, 27, 22, 36, 3, 16, 21,
        60, 19, 70, 90, 89, 43, 5, 42, 65, 76, 40, 30, 23, 85, 2, 95, 56, 48, 71, 64, 98, 13, 99, 7,
        34, 55, 54, 26, 35, 39
    ]   ## some random class order

    class_order_super = [4, 95, 55, 30, 72, 73, 1, 67, 32, 91, 62, 92, 70, 54, 82, 10, 61, 28, 9, 16, 53,
        83, 51, 0, 57, 87, 86, 40, 39, 22, 25, 5, 94, 84, 20, 18, 6, 7, 14, 24, 88, 97,
        3, 43, 42, 17, 37, 12, 68, 76, 71, 60, 33, 23, 49, 38, 21, 15, 31, 19, 75, 66, 34,
        63, 64, 45, 99, 26, 77, 79, 46, 98, 11, 2, 35, 93, 78, 44, 29, 27, 80, 65, 74, 50,
        36, 52, 96, 56, 47, 59, 90, 58, 48, 13, 8, 69, 81, 41, 89, 85
    ]   ## parent-wise split

File Path: dataloaders/multi_task_loader.py
Content:
import random

import numpy as np
import torch
from PIL import Image
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import datasets, transforms

from dataloaders.idataset import _get_datasets, DummyDataset
from dataloaders.task_sampler import MultiTaskSampler

import random
import ipdb


class IncrementalLoader:

    def __init__(
        self,
        opt,
        shuffle=True,
        seed=1,
    ):
        self._opt = opt
        dataset_name=opt.dataset
        validation_split=opt.validation
        self.increment=opt.increment

        datasets = _get_datasets(dataset_name)
        self._setup_data(
            datasets,
            class_order_type=opt.class_order,
            seed=seed,
            increment=self.increment,
            validation_split=validation_split
        )

        self.validation_split = validation_split
        self.train_transforms = datasets[0].train_transforms
        self.common_transforms = datasets[0].common_transforms
        self.top_transforms = datasets[0].top_transforms

        self._current_task = 0

        self._batch_size = opt.batch_size
        self._test_batch_size = opt.test_batch_size        
        self._workers = opt.workers
        self._shuffle = shuffle

        self._setup_test_tasks(validation_split)

    @property
    def n_tasks(self):
        return len(self.increments)
    
    def new_task(self):
        min_class = 0
        max_class = max(self.targets_train) + 1
        x_train, y_train, super_y_train = self._select(
            self.data_train, self.targets_train, self.super_targets_train, low_range=min_class, high_range=max_class
        )
        x_val, y_val, super_y_val = self._select(
            self.data_val, self.targets_val, self.super_targets_val, low_range=min_class, high_range=max_class
        )
        x_test, y_test, super_y_test = self._select(self.data_test, self.targets_test, self.super_targets_test, high_range=max_class)

        train_loader = self._get_loader(x_train, y_train, super_y_train, mode="train")
        val_loader = self._get_loader(x_val, y_val, super_y_val, mode="train") if len(x_val) > 0 else None
        test_loader = self._get_loader(x_test, y_test, super_y_test, mode="test")

        task_info = {
            "min_class": min_class,
            "max_class": max_class,
            "increment": self.increments[self._current_task],
            "task": self._current_task,
            "max_task": len(self.increments),
            "n_train_data": x_train.shape[0],
            "n_test_data": x_test.shape[0]
        }

        self._current_task += 1

        return task_info, train_loader, val_loader, test_loader

    def _setup_test_tasks(self, validation_split):
        self.test_tasks = []
        self.val_tasks = []
        for i in range(len(self.increments)):
            min_class = i
            max_class = i+1
            x_test, y_test, super_y_test = self._select_super(self.data_test, self.targets_test, self.super_targets_test, low_range=min_class, high_range=max_class)
            self.test_tasks.append(self._get_loader(x_test, y_test, super_y_test, mode="test"))

            if validation_split > 0.0:
                x_val, y_val, super_y_val = self._select_super(self.data_val, self.targets_val, self.super_targets_val, low_range=min_class, high_range=max_class)
                self.val_tasks.append(self._get_loader(x_val, y_val, super_y_val, mode="test"))


    def get_tasks(self, dataset_type='test'):
        if dataset_type == 'val':
            if self.validation_split > 0.0:
                return self.val_tasks
            else:
                return self.test_tasks
        elif dataset_type == 'test':
            return self.test_tasks
        else:
            raise NotImplementedError("Unknown mode {}.".format(dataset_type))

    def get_dataset_info(self):
        if(self._opt.dataset == 'tinyimagenet'):
            n_inputs = 3*64*64        
        else:
            n_inputs = self.data_train.shape[3]*self.data_train.shape[1]*self.data_train.shape[2]
        n_outputs = self._opt.increment * len(self.increments)
        n_task = len(self.increments)
        return n_inputs, n_outputs, n_task

    def _select(self, x, y, super_y, low_range=0, high_range=0):
        idxes = np.where(np.logical_and(y >= low_range, y < high_range))[0]
        return x[idxes], y[idxes], super_y[idxes]

    def _select_super(self, x, y, super_y, low_range=0, high_range=0):
        idxes = np.where(np.logical_and(super_y >= low_range, super_y < high_range))[0]
        return x[idxes], y[idxes], super_y[idxes]

    def _get_loader(self, x, y, super_y, shuffle=True, mode="train"):
        if mode == "train":
            pretrsf = transforms.Compose([*self.top_transforms])
            trsf = transforms.Compose([*self.train_transforms, *self.common_transforms])
            batch_size = self._batch_size

            sampler = self._get_sampler(super_y, mode)

            return DataLoader(
                DummyDataset(x, y, trsf, pretrsf, self._opt.dataset=='tinyimagenet', super_y),
                batch_sampler=sampler,
                shuffle=False,
                num_workers=self._workers
            )
        elif mode == "test" or mode == "flip":
            if mode == "test":
                pretrsf = transforms.Compose([*self.top_transforms])
                trsf = transforms.Compose(self.common_transforms)
                batch_size = self._test_batch_size
            elif mode == "flip":
                trsf = transforms.Compose(
                    [transforms.RandomHorizontalFlip(p=1.), *self.common_transforms]
                )
                batch_size = self._test_batch_size

            return DataLoader(
                DummyDataset(x, y, trsf, pretrsf, self._opt.dataset=='tinyimagenet', super_y),
                batch_size=batch_size,
                shuffle=shuffle,
                num_workers=self._workers
            )
        else:
            raise NotImplementedError("Unknown mode {}.".format(mode))


    def _get_sampler(self, labels, mode):
        assert self._batch_size%self._opt.classes_per_it == 0, \
            "Batch size should be a multiple of number of desired classes in a iter"
        if 'train' in mode:
            classes_per_it = self._opt.classes_per_it
            num_samples = int(self._batch_size/self._opt.classes_per_it)
        elif 'val' in mode:
            classes_per_it = self._opt.classes_per_it
            num_samples = int(self._batch_size/self._opt.classes_per_it)
        else:
            raise NotImplementedError("Unknown mode {}.".format(mode))

        return MultiTaskSampler(labels=labels,
                                classes_per_it=classes_per_it,
                                num_samples=num_samples,
                                iterations=self._opt.iterations)

    def _setup_data(self, datasets, class_order_type=False, seed=1, increment=10, validation_split=0.):
        # FIXME: handles online loading of images
        self.data_train, self.targets_train, self.super_targets_train = [], [], []
        self.data_test, self.targets_test, self.super_targets_test = [], [], []
        self.data_val, self.targets_val, self.super_targets_val = [], [], []
        self.increments = []
        self.class_order = []

        current_class_idx = 0  # When using multiple datasets
        for dataset in datasets:

            if(self._opt.dataset == 'tinyimagenet'):
                root_path = self._opt.data_path
                train_dataset = dataset.base_dataset(root_path + 'train/')
                test_dataset = dataset.base_dataset(root_path + 'val/')

                train_dataset.data = train_dataset.samples
                test_dataset.data = test_dataset.samples

                x_train, y_train = train_dataset.data, np.array(train_dataset.targets)
                x_val, y_val, x_train, y_train = self._list_split_per_class(
                    x_train, y_train
                )
                x_test, y_test = test_dataset.data, np.array(test_dataset.targets)

                order = [i for i in range(len(np.unique(y_train)))]
                if class_order_type == 'random':
                    random.seed(seed)  # Ensure that following order is determined by seed:
                    random.shuffle(order)
                    print("Class order:", order)
                elif class_order_type == 'old' and dataset.class_order is not None:
                    order = dataset.class_order
                else:
                    print("Classes are presented in a chronological order")

                self.class_order.append(order)

                y_train = self._map_new_class_index(y_train, order)
                y_val = self._map_new_class_index(y_val, order)
                y_test = self._map_new_class_index(y_test, order)

                super_y_train = self._make_super_classes(y_train, self.increment)
                super_y_test = self._make_super_classes(y_test, self.increment)
                super_y_val = self._make_super_classes(y_val, self.increment)

                y_train += current_class_idx
                y_val += current_class_idx
                y_test += current_class_idx

                current_class_idx += len(order)
                if len(datasets) > 1:
                    self.increments.append(len(order))
                else:
                    self.increments = [increment for _ in range(len(order) // increment)]
            elif ((self._opt.dataset == 'cifar100') and (self._opt.model=="iid2")):
                root_path = self._opt.data_path
                train_dataset = dataset.base_dataset(root_path, train=True, download=True)
                test_dataset = dataset.base_dataset(root_path, train=False, download=True)

                x_train, y_train = train_dataset.data, np.array(train_dataset.targets)
                x_val, y_val, x_train, y_train = self._list_split_per_class(
                    x_train, y_train, validation_split
                )
                x_test, y_test = test_dataset.data, np.array(test_dataset.targets)

                order = [i for i in range(len(np.unique(y_train)))]
                if class_order_type == 'random':
                    random.seed(seed)  # Ensure that following order is determined by seed:
                    random.shuffle(order)
                    print("Class order:", order)
                elif class_order_type == 'old' and dataset.class_order is not None:
                    order = dataset.class_order
                elif class_order_type == 'super' and dataset.class_order_super is not None:
                    order = dataset.class_order_super
                else:
                    print("Classes are presented in a chronological order")

                self.class_order.append(order)

                y_train = self._map_new_class_index(y_train, order)
                y_val = self._map_new_class_index(y_val, order)
                y_test = self._map_new_class_index(y_test, order)

                super_y_train = self._make_super_classes(y_train, self.increment)
                super_y_test = self._make_super_classes(y_test, self.increment)
                super_y_val = self._make_super_classes(y_val, self.increment)

                y_train += current_class_idx
                y_val += current_class_idx
                y_test += current_class_idx

                current_class_idx += len(order)
                if len(datasets) > 1:
                    self.increments.append(len(order))
                else:
                    self.increments = [increment for _ in range(len(order) // increment)]
        
            else:
                root_path = self._opt.data_path
                train_dataset = dataset.base_dataset_hierarchy(root_path, train=True, download=True)
                test_dataset = dataset.base_dataset_hierarchy(root_path, train=False, download=True)

                x_train, y_train, super_y_train = train_dataset.data, np.array(train_dataset.targets), np.array(train_dataset.super_targets)
                x_val, y_val, super_y_val, x_train, y_train, super_y_train = self._split_per_class(
                    x_train, y_train, super_y_train, validation_split
                )
                x_test, y_test, super_y_test = test_dataset.data, np.array(test_dataset.targets), np.array(test_dataset.super_targets)
                
                idxs = np.argsort(super_y_test)
                x_test = x_test[idxs]
                y_test = y_test[idxs]
                super_y_test = super_y_test[idxs]

                idxs = np.argsort(super_y_train)
                x_train = x_train[idxs]
                y_train = y_train[idxs]
                super_y_train = super_y_train[idxs]

                idxs = np.argsort(super_y_val)
                x_val = x_val[idxs]
                y_val = y_val[idxs]
                super_y_val = super_y_val[idxs]

                idxs = np.unique(y_test, return_index=True)[1]
                unique_y_order = [y_test[id] for id in sorted(idxs)]
                unique_supery_order = [super_y_test[id] for id in sorted(idxs)]
                print(unique_supery_order)
                print(unique_y_order)

                y_train = self._map_new_class_index(y_train, unique_y_order)
                y_val = self._map_new_class_index(y_val, unique_y_order)
                y_test = self._map_new_class_index(y_test, unique_y_order)

                
                y_train += current_class_idx
                y_val += current_class_idx
                y_test += current_class_idx

                # current_class_idx += len(order)
                # if len(datasets) > 1:
                #     raise(Exception("current_class_idx doesnt work for more than one dataset right now, correct it"))
                #     self.increments.append(len(order))
                # else:
                self.increments = [increment for _ in range(20)]

            self.data_train.append(x_train)
            self.targets_train.append(y_train)
            self.super_targets_train.append(super_y_train)
            self.data_val.append(x_val)
            self.targets_val.append(y_val)
            self.super_targets_val.append(super_y_val)
            self.data_test.append(x_test)
            self.targets_test.append(y_test)
            self.super_targets_test.append(super_y_test)

            # print(self.increments)

            self.data_train = np.concatenate(self.data_train)
            self.targets_train = np.concatenate(self.targets_train)
            self.super_targets_train = np.concatenate(self.super_targets_train)
            self.data_val = np.concatenate(self.data_val)
            self.targets_val = np.concatenate(self.targets_val)
            self.super_targets_val = np.concatenate(self.super_targets_val)
            self.data_test = np.concatenate(self.data_test)
            self.targets_test = np.concatenate(self.targets_test)
            self.super_targets_test = np.concatenate(self.super_targets_test)

    def _make_super_classes(self, y, increment):
        unique_y = np.unique(y)
        super_y = [int(i/increment) for i in range(len(unique_y))]
        super_order = [super_y[y[i]] for i in range(len(y))]
        return super_order
    
    @staticmethod
    def _map_new_class_index(y, order):
        """Transforms targets for new class order."""
        return np.array(list(map(lambda x: order.index(x), y)))


    @staticmethod
    def _split_per_class(x, y, super_y, validation_split=0.):
        """Splits train data for a subset of validation data.
        Split is done so that each class has same amount of data.
        """
        shuffled_indexes = np.random.permutation(x.shape[0])
        # idxs = np.argsort(y)
        x = x[shuffled_indexes]
        y = y[shuffled_indexes]
        super_y = super_y[shuffled_indexes]

        x_val, y_val, super_y_val = [], [], []
        x_train, y_train, super_y_train = [], [], []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            nb_val_elts = int(class_indexes.shape[0] * validation_split)

            val_indexes = class_indexes[:nb_val_elts]
            train_indexes = class_indexes[nb_val_elts:]

            x_val.append(x[val_indexes])
            y_val.append(y[val_indexes])
            super_y_val.append(super_y[val_indexes])

            x_train.append(x[train_indexes])
            y_train.append(y[train_indexes])
            super_y_train.append(super_y[train_indexes])


        x_val, y_val, super_y_val = np.concatenate(x_val), np.concatenate(y_val), np.concatenate(super_y_val) 
        x_train, y_train, super_y_train = np.concatenate(x_train), np.concatenate(y_train), np.concatenate(super_y_train) 

        return x_val, y_val, super_y_val, x_train, y_train, super_y_train

    @staticmethod
    def _list_split_per_class(x, y, validation_split=0.):
        """Splits train data for a subset of validation data.
        Split is done so that each class has a much data.
        """
        c = list(zip(x, y))
        random.shuffle(c)
        x, y = zip(*c)

        x_val, y_val = [], []
        x_train, y_train = [], []

        for class_id in np.unique(y):
            class_indexes = np.where(y == class_id)[0]
            nb_val_elts = int(class_indexes.shape[0] * validation_split)

            val_indexes = class_indexes[:nb_val_elts]
            train_indexes = class_indexes[nb_val_elts:]

            x_val_i = [x[i] for i in val_indexes]
            y_val_i = [y[i] for i in val_indexes]
            
            x_train_i = [x[i] for i in train_indexes]
            y_train_i = [y[i] for i in train_indexes]
            
            x_val.append(x_val_i)
            y_val.append(y_val_i)
            
            x_train.append(x_train_i)
            y_train.append(y_train_i)
            
        x_val, y_val = np.concatenate(x_val), np.concatenate(y_val)
        x_train, y_train = np.concatenate(x_train), np.concatenate(y_train)

        return x_val, y_val, x_train, y_train

###### Unused functions
    def get_idx_data(self, idx, batch_size, mode="test", data_source="train"):
        """Returns a custom loader with specific idxs only.
        :param idx: A list of data indexes that we want.
        :param mode: Various mode for the transformations applied on it.
        :param data_source: Whether to fetch from the train, val, or test set.
        :return: The raw data and a loader.
        """
        if data_source == "train":
            x, y, super_y = self.data_train, self.targets_train, self.super_targets_train
        elif data_source == "val":
            x, y, super_y = self.data_val, self.targets_val, self.super_targets_val
        elif data_source == "test":
            x, y, super_y = self.data_test, self.targets_test, self.super_targets_test
        else:
            raise ValueError("Unknown data source <{}>.".format(data_source))
        y, sorted_idx = y.sort()

        sampler = torch.utils.data.sampler.SubsetRandomSampler(idx)
        trsf = transforms.Compose(self.common_transforms)

        loader = DataLoader(
            DummyDataset(x[sorted_idx], y, trsf, super_y=super_y[sorted_idx]),
            sampler=sampler,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self._workers)

    def get_custom_loader(self, class_indexes, mode="test", data_source="train"):
        """Returns a custom loader.
        :param class_indexes: A list of class indexes that we want.
        :param mode: Various mode for the transformations applied on it.
        :param data_source: Whether to fetch from the train, val, or test set.
        :return: The raw data and a loader.
        """
        if not isinstance(class_indexes, list):  # TODO: deprecated, should always give a list
            class_indexes = [class_indexes]

        if data_source == "train":
            x, y, super_y = self.data_train, self.targets_train, self.super_targets_train
        elif data_source == "val":
            x, y, super_y = self.data_val, self.targets_val, self.super_targets_val
        elif data_source == "test":
            x, y, super_y = self.data_test, self.targets_test, self.super_targets_test
        else:
            raise ValueError("Unknown data source <{}>.".format(data_source))

        data, targets, super_targets = [], [], []
        for class_index in class_indexes:
            class_data, class_targets, super_class_targets  = self._select(
                x, y, super_y, low_range=class_index, high_range=class_index + 1
            )
            data.append(class_data)
            targets.append(class_targets)
            super_targets.append(super_class_targets)            

        data = np.concatenate(data)
        targets = np.concatenate(targets)
        super_targets = np.concatenate(super_targets)

        return data, self._get_loader(data, targets, super_targets, shuffle=False, mode=mode)
    

File Path: dataloaders/task_incremental_loader.py
Content:
import numpy as np
import torch
from PIL import Image
from torch.utils.data import DataLoader
from torchvision import datasets

from dataloaders.idataset import DummyArrayDataset
import os


class IncrementalLoader:

    def __init__(
        self,
        opt,
        shuffle=True,
        seed=1,
    ):
        self._opt = opt
        validation_split=opt.validation
        increment=opt.increment

        self._setup_data(
            class_order_type=opt.class_order,
            seed=seed,
            increment=increment,
            validation_split=validation_split
        )

        self._current_task = 0

        self._batch_size = opt.batch_size
        self._test_batch_size = opt.test_batch_size        
        self._workers = opt.workers
        self._shuffle = shuffle

        self._setup_test_tasks()

    @property
    def n_tasks(self):
        return len(self.test_dataset)
    
    def new_task(self):
        if self._current_task >= len(self.test_dataset):
            raise Exception("No more tasks.")

        p = self.sample_permutations[self._current_task]
        x_train, y_train = self.train_dataset[self._current_task][1][p], self.train_dataset[self._current_task][2][p]
        x_test, y_test = self.test_dataset[self._current_task][1], self.test_dataset[self._current_task][2]

        train_loader = self._get_loader(x_train, y_train, mode="train")
        test_loader = self._get_loader(x_test, y_test, mode="test")

        task_info = {
            "min_class": 0,
            "max_class": self.n_outputs,
            "increment": -1,
            "task": self._current_task,
            "max_task": len(self.test_dataset),
            "n_train_data": len(x_train),
            "n_test_data": len(x_test)
        }

        self._current_task += 1

        return task_info, train_loader, None, test_loader

    def _setup_test_tasks(self):
        self.test_tasks = []
        for i in range(len(self.test_dataset)):
            self.test_tasks.append(self._get_loader(self.test_dataset[i][1], self.test_dataset[i][2], mode="test"))

    def get_tasks(self, dataset_type='test'):
        if dataset_type == 'test':
            return self.test_dataset
        elif dataset_type == 'val':
            return self.test_dataset
        else:
            raise NotImplementedError("Unknown mode {}.".format(dataset_type))

    def get_dataset_info(self):
        n_inputs = self.train_dataset[0][1].size(1)
        n_outputs = 0
        for i in range(len(self.train_dataset)):
            n_outputs = max(n_outputs, self.train_dataset[i][2].max())
            n_outputs = max(n_outputs, self.test_dataset[i][2].max())
        self.n_outputs = n_outputs
        return n_inputs, n_outputs.item()+1, self.n_tasks


    def _get_loader(self, x, y, shuffle=True, mode="train"):
        if mode == "train":
            batch_size = self._batch_size
        elif mode == "test":
            batch_size = self._test_batch_size
        else:
            raise NotImplementedError("Unknown mode {}.".format(mode))

        return DataLoader(
            DummyArrayDataset(x, y),
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=self._workers
        )


    def _setup_data(self, class_order_type=False, seed=1, increment=10, validation_split=0.):
        # FIXME: handles online loading of images
        torch.manual_seed(seed)

        self.train_dataset, self.test_dataset = torch.load(os.path.join(self._opt.data_path, self._opt.dataset + ".pt"))

        self.sample_permutations = []

        # for every task, accumulate a shuffled set of samples_per_task
        for t in range(len(self.train_dataset)):
            N = self.train_dataset[t][1].size(0)
            if self._opt.samples_per_task <= 0:
                n = N
            else:
                n = min(self._opt.samples_per_task, N)


            p = torch.randperm(N)[0:n]
            self.sample_permutations.append(p)

File Path: dataloaders/task_sampler.py
Content:
# coding=utf-8
import numpy as np
import torch
import warnings
import ipdb

class MultiTaskSampler(object):
    '''
    MultiTaskSampler: yield a batch of indexes at each iteration.
    Indexes are calculated by keeping in account 'classes_per_it' and 'num_samples',
    In fact at every iteration the batch indexes will refer to  'num_support' + 'num_query' samples
    for 'classes_per_it' random classes.

    __len__ returns the number of episodes per epoch (same as 'self.iterations').
    '''

    def __init__(self, labels, classes_per_it, num_samples, iterations):
        '''
        Initialize the MultiTaskSampler object
        Args:
        - labels: an iterable containing all the labels for the current dataset
        samples indexes will be infered from this iterable.
        - classes_per_it: number of random classes for each iteration
        - num_samples: number of samples for each iteration for each class (support + query)
        - iterations: number of iterations (episodes) per epoch
        '''
        super(MultiTaskSampler, self).__init__()
        self.labels = labels
        self.classes_per_it = classes_per_it
        self.sample_per_class = num_samples
        self.iterations = iterations

        self.classes, self.counts = np.unique(self.labels, return_counts=True)

        if self.classes_per_it > len(self.classes):
            warnings.warn('Number of classes per iteration is higher than the number of unique labels')
            self.classes_per_it = len(self.classes)

        self.classes = torch.LongTensor(self.classes)
        # create a matrix, indexes, of dim: classes X max(elements per class)
        # fill it with nans
        # for every class c, fill the relative row with the indices samples belonging to c
        # in numel_per_class we store the number of samples for each class/row
        self.idxs = range(len(self.labels))
        self.indexes = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan
        self.indexes = torch.Tensor(self.indexes)
        self.numel_per_class = torch.zeros_like(self.classes)
        for idx, label in enumerate(self.labels):
            label_idx = np.argwhere(self.classes == label).item()                         
            self.indexes[label_idx, np.where(np.isnan(self.indexes[label_idx]))[0][0]] = idx    
            self.numel_per_class[label_idx] += 1

    def __iter__(self):
        '''
        yield a batch of indexes
        '''

        spc = self.sample_per_class
        cpi = self.classes_per_it

        for it in range(self.iterations):
            batch_size = spc * cpi
            batch = torch.LongTensor(batch_size)

            
            c_idxs = torch.randperm(len(self.classes))[:cpi]

            for i, c in enumerate(self.classes[c_idxs]):
                s = slice(i * spc, (i + 1) * spc)
                # FIXME when torch.argwhere will exist
                label_idx = torch.arange(len(self.classes)).long()[self.classes == c].item()
                sample_idxs = torch.randperm(self.numel_per_class[label_idx])[:spc]
                batch[s] = self.indexes[label_idx][sample_idxs]

            batch = batch[torch.randperm(len(batch))]

            yield batch

    def __len__(self):
        '''
        returns the number of iterations (episodes) per epoch
        '''
        return self.iterations

File Path: download.py
Content:
########################################################################
#
# Functions for downloading and extracting data-files from the internet.
#
# Implemented in Python 3.5
#
########################################################################
#
# This file is part of the TensorFlow Tutorials available at:
#
# https://github.com/Hvass-Labs/TensorFlow-Tutorials
#
# Published under the MIT License. See the file LICENSE for details.
#
# Copyright 2016 by Magnus Erik Hvass Pedersen
#
########################################################################

import sys
import os
import urllib.request
import tarfile
import zipfile

########################################################################


def _print_download_progress(count, block_size, total_size):
    """
    Function used for printing the download progress.
    Used as a call-back function in maybe_download_and_extract().
    """

    # Percentage completion.
    pct_complete = float(count * block_size) / total_size

    # Limit it because rounding errors may cause it to exceed 100%.
    pct_complete = min(1.0, pct_complete)

    # Status-message. Note the \r which means the line should overwrite itself.
    msg = "\r- Download progress: {0:.1%}".format(pct_complete)

    # Print it.
    sys.stdout.write(msg)
    sys.stdout.flush()


########################################################################

def download(base_url, filename, download_dir):
    """
    Download the given file if it does not already exist in the download_dir.

    :param base_url: The internet URL without the filename.
    :param filename: The filename that will be added to the base_url.
    :param download_dir: Local directory for storing the file.
    :return: Nothing.
    """

    # Path for local file.
    save_path = os.path.join(download_dir, filename)

    # Check if the file already exists, otherwise we need to download it now.
    if not os.path.exists(save_path):
        # Check if the download directory exists, otherwise create it.
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)

        print("Downloading", filename, "...")

        # Download the file from the internet.
        url = base_url + filename
        file_path, _ = urllib.request.urlretrieve(url=url,
                                                  filename=save_path,
                                                  reporthook=_print_download_progress)

        print(" Done!")


def maybe_download_and_extract(url, download_dir):
    """
    Download and extract the data if it doesn't already exist.
    Assumes the url is a tar-ball file.

    :param url:
        Internet URL for the tar-file to download.
        Example: "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"

    :param download_dir:
        Directory where the downloaded file is saved.
        Example: "data/CIFAR-10/"

    :return:
        Nothing.
    """

    # Filename for saving the file downloaded from the internet.
    # Use the filename from the URL and add it to the download_dir.
    filename = url.split('/')[-1]
    file_path = os.path.join(download_dir, filename)

    # Check if the file already exists.
    # If it exists then we assume it has also been extracted,
    # otherwise we need to download and extract it now.
    if not os.path.exists(file_path):
        # Check if the download directory exists, otherwise create it.
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)

        # Download the file from the internet.
        file_path, _ = urllib.request.urlretrieve(url=url,
                                                  filename=file_path,
                                                  reporthook=_print_download_progress)

        print()
        print("Download finished. Extracting files.")

        if file_path.endswith(".zip"):
            # Unpack the zip-file.
            zipfile.ZipFile(file=file_path, mode="r").extractall(download_dir)
        elif file_path.endswith((".tar.gz", ".tgz")):
            # Unpack the tar-ball.
            tarfile.open(name=file_path, mode="r:gz").extractall(download_dir)

        print("Done.")
    else:
        print("Data has apparently already been downloaded and unpacked.")


########################################################################

File Path: get_data.py
Content:
# Copyright 2019-present, IBM Research
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import os
import download
import argparse

def get_mnist_data(url, data_dir):
    print("Downloading {} into {}".format(url, data_dir))
    download.maybe_download_and_extract(url, data_dir)

def get_datasets():
    parser = argparse.ArgumentParser()
    parser.add_argument("dataset", help="Either the name of the dataset (rotations, permutations, manypermutations), or `all` to download all datasets")
    args = parser.parse_args()

    # Change dir to the location of this file (repo's root)
    get_data_path = os.path.realpath(__file__)
    os.chdir(os.path.dirname(get_data_path))
    data_dir = os.path.join(os.getcwd(), 'data')

    # get files
    mnist_rotations = "https://nlp.stanford.edu/data/mer/mnist_rotations.tar.gz"
    mnist_permutations = "https://nlp.stanford.edu/data/mer/mnist_permutations.tar.gz"
    mnist_many = "https://nlp.stanford.edu/data/mer/mnist_manypermutations.tar.gz"

    all = {"rotations": mnist_rotations, "permutations": mnist_permutations, "manypermutations": mnist_many}

    if args.dataset == "all":
        for dataset in all.values():
            get_mnist_data(dataset, data_dir)
    else:
        get_mnist_data(all[args.dataset], data_dir)

if __name__ == "__main__":
    get_datasets()

File Path: main.py
Content:
import importlib
import datetime
import argparse
import time
import os
import ipdb
from tqdm import tqdm

import torch
from torch.autograd import Variable

import parser as file_parser
from metrics.metrics import confusion_matrix
from utils import misc_utils
from main_multi_task import life_experience_iid, eval_iid_tasks

def eval_class_tasks(model, tasks, args):

    model.eval()
    result = []
    for t, task_loader in enumerate(tasks):
        rt = 0

        for (i, (x, y)) in enumerate(task_loader):
            if args.cuda:
                x = x.cuda()
            _, p = torch.max(model(x, t).data.cpu(), 1, keepdim=False)
            rt += (p == y).float().sum()

        result.append(rt / len(task_loader.dataset))
    return result

def eval_tasks(model, tasks, args):

    model.eval()
    result = []
    for i, task in enumerate(tasks):
        t = i
        x = task[1]
        y = task[2]
        rt = 0
        
        eval_bs = x.size(0)

        for b_from in range(0, x.size(0), eval_bs):
            b_to = min(b_from + eval_bs, x.size(0) - 1)
            if b_from == b_to:
                xb = x[b_from].view(1, -1)
                yb = torch.LongTensor([y[b_to]]).view(1, -1)
            else:
                xb = x[b_from:b_to]
                yb = y[b_from:b_to]
            if args.cuda:
                xb = xb.cuda()
            _, pb = torch.max(model(xb, t).data.cpu(), 1, keepdim=False)
            rt += (pb == yb).float().sum()

        result.append(rt / x.size(0))

    return result

def life_experience(model, inc_loader, args):
    result_val_a = []
    result_test_a = []

    result_val_t = []
    result_test_t = []

    time_start = time.time()
    test_tasks = inc_loader.get_tasks("test")
    val_tasks = inc_loader.get_tasks("val")
    
    evaluator = eval_tasks
    if args.loader == "class_incremental_loader":
        evaluator = eval_class_tasks

    for task_i in range(inc_loader.n_tasks):
        task_info, train_loader, _, _ = inc_loader.new_task()
        for ep in range(args.n_epochs):

            model.real_epoch = ep

            prog_bar = tqdm(train_loader)
            for (i, (x, y)) in enumerate(prog_bar):

                if((i % args.log_every) == 0):
                    result_val_a.append(evaluator(model, val_tasks, args))
                    result_val_t.append(task_info["task"])

                v_x = x
                v_y = y
                if args.arch == 'linear':
                    v_x = x.view(x.size(0), -1)
                if args.cuda:
                    v_x = v_x.cuda()
                    v_y = v_y.cuda()

                model.train()

                loss = model.observe(Variable(v_x), Variable(v_y), task_info["task"])

                prog_bar.set_description(
                    "Task: {} | Epoch: {}/{} | Iter: {} | Loss: {} | Acc: Total: {} Current Task: {} ".format(
                        task_info["task"], ep+1, args.n_epochs, i%(1000*args.n_epochs), round(loss, 3),
                        round(sum(result_val_a[-1]).item()/len(result_val_a[-1]), 5), round(result_val_a[-1][task_info["task"]].item(), 5)
                    )
                )

        result_val_a.append(evaluator(model, val_tasks, args))
        result_val_t.append(task_info["task"])

        if args.calc_test_accuracy:
            result_test_a.append(evaluator(model, test_tasks, args))
            result_test_t.append(task_info["task"])


    print("####Final Validation Accuracy####")
    print("Final Results:- \n Total Accuracy: {} \n Individual Accuracy: {}".format(sum(result_val_a[-1])/len(result_val_a[-1]), result_val_a[-1]))

    if args.calc_test_accuracy:
        print("####Final Test Accuracy####")
        print("Final Results:- \n Total Accuracy: {} \n Individual Accuracy: {}".format(sum(result_test_a[-1])/len(result_test_a[-1]), result_test_a[-1]))


    time_end = time.time()
    time_spent = time_end - time_start
    return torch.Tensor(result_val_t), torch.Tensor(result_val_a), torch.Tensor(result_test_t), torch.Tensor(result_test_a), time_spent

def save_results(args, result_val_t, result_val_a, result_test_t, result_test_a, model, spent_time):
    fname = os.path.join(args.log_dir, 'results')

    # save confusion matrix and print one line of stats
    val_stats = confusion_matrix(result_val_t, result_val_a, args.log_dir, 'results.txt')
    
    one_liner = str(vars(args)) + ' # val: '
    one_liner += ' '.join(["%.3f" % stat for stat in val_stats])

    test_stats = 0
    if args.calc_test_accuracy:
        test_stats = confusion_matrix(result_test_t, result_test_a, args.log_dir, 'results.txt')
        one_liner += ' # test: ' +  ' '.join(["%.3f" % stat for stat in test_stats])

    print(fname + ': ' + one_liner + ' # ' + str(spent_time))

    # save all results in binary file
    torch.save((result_val_t, result_val_a, model.state_dict(),
                val_stats, one_liner, args), fname + '.pt')
    return val_stats, test_stats

def main():
    parser = file_parser.get_parser()

    args = parser.parse_args()

    # initialize seeds
    misc_utils.init_seed(args.seed)

    # set up loader
    # 2 options: class_incremental and task_incremental
    # experiments in the paper only use task_incremental
    Loader = importlib.import_module('dataloaders.' + args.loader)
    loader = Loader.IncrementalLoader(args, seed=args.seed)
    n_inputs, n_outputs, n_tasks = loader.get_dataset_info()

    # setup logging
    timestamp = misc_utils.get_date_time()
    args.log_dir, args.tf_dir = misc_utils.log_dir(args, timestamp)

    # load model
    Model = importlib.import_module('model.' + args.model)
    model = Model.Net(n_inputs, n_outputs, n_tasks, args)
    if args.cuda:
        try:
            model.net.cuda()            
        except:
            pass 

    # run model on loader
    if args.model == "iid2":
        # oracle baseline with all task data shown at same time
        result_val_t, result_val_a, result_test_t, result_test_a, spent_time = life_experience_iid(
            model, loader, args)
    else:
        # for all the CL baselines
        result_val_t, result_val_a, result_test_t, result_test_a, spent_time = life_experience(
            model, loader, args)

        # save results in files or print on terminal
        save_results(args, result_val_t, result_val_a, result_test_t, result_test_a, model, spent_time)


if __name__ == "__main__":
    main()

File Path: main_multi_task.py
Content:
import time
import os
from tqdm import tqdm

import torch
from torch.autograd import Variable

def eval_iid_tasks(model, tasks, args):

    model.eval()
    result = []
    for t, task_loader in enumerate(tasks):
        rt = 0

        for (i, (x, y, super_y)) in enumerate(task_loader):
            if args.cuda:
                x = x.cuda()
            _, p = torch.max(model(x, super_y).data.cpu(), 1, keepdim=False)
            rt += (p == y).float().sum()

        result.append(rt / len(task_loader.dataset))
    return result

def life_experience_iid(model, inc_loader, args):
    result_val_a = []
    result_test_a = []

    result_val_t = []
    result_test_t = []

    time_start = time.time()
    test_tasks = inc_loader.get_tasks("test")
    val_tasks = inc_loader.get_tasks("val")
    task_info, train_loader, _, _ = inc_loader.new_task()
    evaluator = eval_iid_tasks

    for ep in range(args.n_epochs):

        model.real_epoch = ep

        prog_bar = tqdm(train_loader)
        for (i, (x, y, super_y)) in enumerate(prog_bar):
            if((i % args.log_every) == 0):
                result_val_a.append(evaluator(model, val_tasks, args))
                result_val_t.append(task_info["task"])

            v_x = x
            v_y = y
            if args.arch == 'linear':
                v_x = x.view(x.size(0), -1)
            super_v_y = super_y

            if args.cuda:
                v_x = v_x.cuda()
                v_y = v_y.cuda()
                super_v_y = super_v_y.cuda()

            model.train()

            loss = model.observe(Variable(v_x), Variable(v_y), Variable(super_v_y))

            prog_bar.set_description(
                "Epoch: {}/{} | Iter: {} | Loss: {} | Acc: Total: {}".format(
                    ep+1, args.n_epochs, i%(1000*args.n_epochs), round(loss, 3),
                    round(sum(result_val_a[-1]).item()/len(result_val_a[-1]), 5)
                )
            )

    result_val_a.append(evaluator(model, val_tasks, args))
    result_val_t.append(task_info["task"])

    if args.calc_test_accuracy:
        result_test_a.append(evaluator(model, test_tasks, args))
        result_test_t.append(task_info["task"])


    print("####Final Validation Accuracy####")
    print("Final Results:- \n Total Accuracy: {} \n Individual Accuracy: {}".format(sum(result_val_a[-1])/len(result_val_a[-1]), result_val_a[-1]))

    if args.calc_test_accuracy:
        print("####Final Test Accuracy####")
        print("Final Results:- \n Total Accuracy: {} \n Individual Accuracy: {}".format(sum(result_test_a[-1])/len(result_test_a[-1]), result_test_a[-1]))


    time_end = time.time()
    time_spent = time_end - time_start
    return torch.Tensor(result_val_t), torch.Tensor(result_val_a), torch.Tensor(result_test_t), torch.Tensor(result_test_a), time_spent


File Path: metrics/metrics.py
Content:
### We directly copied the metrics.py model file from the GEM project https://github.com/facebookresearch/GradientEpisodicMemory

# Copyright 2019-present, IBM Research
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from __future__ import print_function

import ipdb
import os
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import numpy as np

import torch


def task_changes(result_t):
    n_tasks = int(result_t.max() + 1)
    changes = []
    current = result_t[0]
    for i, t in enumerate(result_t):
        if t != current:
            changes.append(i)
            current = t

    return n_tasks, changes


def confusion_matrix(result_t, result_a, log_dir, fname=None):
    nt, changes = task_changes(result_t)
    fname = os.path.join(log_dir, fname)

    baseline = result_a[0]
    changes = torch.LongTensor(changes + [result_a.size(0)]) - 1
    result = result_a[(torch.LongTensor(changes))]

    # acc[t] equals result[t,t]
    acc = result.diag()
    fin = result[nt - 1]
    # bwt[t] equals result[T,t] - acc[t]
    bwt = result[nt - 1] - acc

    # fwt[t] equals result[t-1,t] - baseline[t]
    fwt = torch.zeros(nt)
    for t in range(1, nt):
        fwt[t] = result[t - 1, t] - baseline[t]

    if fname is not None:
        f = open(fname, 'w')

        print(' '.join(['%.4f' % r for r in baseline]), file=f)
        print('|', file=f)
        for row in range(result.size(0)):
            print(' '.join(['%.4f' % r for r in result[row]]), file=f)
        print('', file=f)
        print('Diagonal Accuracy: %.4f' % acc.mean(), file=f)
        print('Final Accuracy: %.4f' % fin.mean(), file=f)
        print('Backward: %.4f' % bwt.mean(), file=f)
        print('Forward:  %.4f' % fwt.mean(), file=f)
        f.close()

    colors = cm.nipy_spectral(np.linspace(0, 1, len(result)))
    figure = plt.figure(figsize=(8, 8))
    ax = plt.gca()
    data = np.array(result_a)
    for i in range(len(data[0])):
        plt.plot(range(data.shape[0]), data[:,i], label=str(i), color=colors[i], linewidth=2)
        
    plt.savefig(log_dir + '/' + 'task_wise_accuracy.png')

    stats = []
    stats.append(acc.mean())
    stats.append(fin.mean())
    stats.append(bwt.mean())
    stats.append(fwt.mean())

    return stats

File Path: model/agem.py
Content:
### This is a pytorch implementation of AGEM based on https://github.com/facebookresearch/agem. 

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable

import ipdb
import model.meta.learner as Learner
import model.meta.modelfactory as mf
import numpy as np
import random

# Auxiliary functions useful for AGEM's inner optimization.

def compute_offsets(task, nc_per_task, is_cifar):
    """
        Compute offsets for cifar to determine which
        outputs to select for a given task.
    """
    if is_cifar:
        offset1 = task * nc_per_task
        offset2 = (task + 1) * nc_per_task
    else:
        offset1 = 0
        offset2 = nc_per_task
    return offset1, offset2


def store_grad(pp, grads, grad_dims, tid):
    """
        This stores parameter gradients of past tasks.
        pp: parameters
        grads: gradients
        grad_dims: list with number of parameters per layers
        tid: task id
    """
    # store the gradients
    grads[:, tid].fill_(0.0)
    cnt = 0
    for param in pp():
        if param.grad is not None:
            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])
            en = sum(grad_dims[:cnt + 1])
            grads[beg: en, tid].copy_(param.grad.data.view(-1))
        cnt += 1


def overwrite_grad(pp, newgrad, grad_dims):
    """
        This is used to overwrite the gradients with a new gradient
        vector, whenever violations occur.
        pp: parameters
        newgrad: corrected gradient
        grad_dims: list storing number of parameters at each layer
    """
    cnt = 0
    for param in pp():
        if param.grad is not None:
            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])
            en = sum(grad_dims[:cnt + 1])
            this_grad = newgrad[beg: en].contiguous().view(
                param.grad.data.size())
            param.grad.data.copy_(this_grad)
        cnt += 1



def projectgrad(gradient, memories, margin=0.5, eps = 1e-3, oiter = 0):
    """
        Solves the GEM dual QP described in the paper given a proposed
        gradient "gradient", and a memory of task gradients "memories".
        Overwrites "gradient" with the final projected update.
        input:  gradient, p-vector
        input:  memories, (t * p)-vector
        output: x, p-vector
    """

    similarity = torch.nn.functional.cosine_similarity(gradient.t(), memories.t().mean(dim=0).unsqueeze(0))

    memories_np = memories.cpu().t().double().numpy()
    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()

    # merge memories
    t = memories_np.shape[0]

    memories_np2 = memories_np.mean(axis=0).reshape(1, memories_np.shape[1])

    ref_mag = np.dot(memories_np2, memories_np2.transpose())
    dotp = np.dot(gradient_np.reshape(1, -1), memories_np2.transpose())

    if(oiter%100==0):
        print('similarity : ', similarity.item())
        print('dotp:', dotp)

    if(dotp[0,0]<0):
        proj = gradient_np.reshape(1, -1) - ((dotp/ ref_mag) * memories_np2)
        gradient.copy_(torch.Tensor(proj).view(-1, 1))


class Net(nn.Module):
    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,
                 args):
        super(Net, self).__init__()
        self.args = args

        nl, nh = args.n_layers, args.n_hiddens
        self.margin = args.memory_strength
        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))

        nl, nh = args.n_layers, args.n_hiddens
        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)
        self.net = Learner.Learner(config, args)

        self.ce = nn.CrossEntropyLoss()
        self.bce = torch.nn.CrossEntropyLoss()
        self.n_outputs = n_outputs
        self.glances = args.glances

        self.opt = optim.SGD(self.parameters(), args.lr)

        self.n_memories = args.n_memories
        self.gpu = args.cuda

        self.age = 0
        self.M = []
        self.memories = args.memories
        self.grad_align = []
        self.grad_task_align = {}
        self.current_task = 0

        # allocate episodic memory
        self.memory_data = torch.FloatTensor(
            n_tasks, self.n_memories, n_inputs)
        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)
        if args.cuda:
            self.memory_data = self.memory_data.cuda()
            self.memory_labs = self.memory_labs.cuda()

        # allocate temporary synaptic memory
        self.grad_dims = []
        for param in self.parameters():
            self.grad_dims.append(param.data.numel())
        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)
        if args.cuda:
            self.grads = self.grads.cuda()

        # allocate counters
        self.observed_tasks = []
        self.mem_cnt = 0
        if self.is_cifar:
            self.nc_per_task = int(n_outputs / n_tasks)
        else:
            self.nc_per_task = n_outputs
        
        if args.cuda:
            self.cuda()

        self.iter = 0

    def forward(self, x, t):
        if self.args.dataset == 'tinyimagenet':
            x = x.view(-1, 3, 64, 64)
        elif self.args.dataset == 'cifar100':
            x = x.view(-1, 3, 32, 32)
        output = self.net.forward(x)
        if self.is_cifar:
            # make sure we predict classes within the current task
            offset1 = int(t * self.nc_per_task)
            offset2 = int((t + 1) * self.nc_per_task)
            if offset1 > 0:
                output[:, :offset1].data.fill_(-10e10)
            if offset2 < self.n_outputs:
                output[:, offset2:self.n_outputs].data.fill_(-10e10)
        return output

    def observe(self, x, y, t):

        self.iter +=1
        x = x.view(x.size(0), -1)
        # update memory
        if t != self.current_task:
            self.observed_tasks.append(t)
            self.current_task = t
            self.grad_align.append([])

        for pass_itr in range(self.glances):

            if(pass_itr==0):
                # Update ring buffer storing examples from current task
                bsz = y.data.size(0)
                endcnt = min(self.mem_cnt + bsz, self.n_memories)
                effbsz = endcnt - self.mem_cnt
                self.memory_data[t, self.mem_cnt: endcnt].copy_(
                    x.data[: effbsz])
                if bsz == 1:
                    self.memory_labs[t, self.mem_cnt] = y.data[0]
                else:
                    self.memory_labs[t, self.mem_cnt: endcnt].copy_(
                        y.data[: effbsz])
                self.mem_cnt += effbsz
                if self.mem_cnt == self.n_memories:
                    self.mem_cnt = 0

            # compute gradient on previous tasks
            if len(self.observed_tasks) > 1:
                for tt in range(len(self.observed_tasks) - 1):
                    self.zero_grad()
                    # fwd/bwd on the examples in the memory
                    past_task = self.observed_tasks[tt]

                    offset1, offset2 = compute_offsets(past_task, self.nc_per_task,
                                                       self.is_cifar)
                    ptloss = self.ce(
                        self.forward(
                            Variable(self.memory_data[past_task]),
                            past_task)[:, offset1: offset2],
                        Variable(self.memory_labs[past_task] - offset1))
                    ptloss.backward()
                    torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

                    store_grad(self.parameters, self.grads, self.grad_dims,
                               past_task)

            # now compute the grad on the current minibatch
            self.zero_grad()

            offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar)
            loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

            # check if gradient violates constraints                                                           
            if len(self.observed_tasks) > 1:
                # copy gradient
                store_grad(self.parameters, self.grads, self.grad_dims, t)
                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \
                    else torch.LongTensor(self.observed_tasks[:-1])

                projectgrad(self.grads[:, t].unsqueeze(1),                                           
                              self.grads.index_select(1, indx), self.margin, oiter = self.iter)
                # copy gradients back
                overwrite_grad(self.parameters, self.grads[:, t],
                               self.grad_dims)

            self.opt.step()
        
        xi = x.data.cpu().numpy()
        yi = y.data.cpu().numpy()
        for i in range(0,x.size()[0]):
            self.age += 1
            # Reservoir sampling memory update:
            if len(self.M) < self.memories:
                self.M.append([xi[i],yi[i],t])

            else:
                p = random.randint(0,self.age)
                if p < self.memories:
                    self.M[p] = [xi[i],yi[i],t]

        return loss.item()

File Path: model/eralg4.py
Content:
# An implementation of Experience Replay (ER) with reservoir sampling and without using tasks from Algorithm 4 of https://openreview.net/pdf?id=B1gTShAct7

# Copyright 2019-present, IBM Research
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable

import numpy as np

import random
from torch.nn.modules.loss import CrossEntropyLoss
from random import shuffle
import sys
import warnings
import math

import model.meta.modelfactory as mf
import model.meta.learner as Learner
warnings.filterwarnings("ignore")

class Net(nn.Module):
    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,
                 args):
        super(Net, self).__init__()

        self.args = args
        nl, nh = args.n_layers, args.n_hiddens

        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)
        self.net = Learner.Learner(config, args)

        self.opt_wt = optim.SGD(self.parameters(), lr=args.lr)

        if self.args.learn_lr:
            self.net.define_task_lr_params(alpha_init = args.alpha_init)
            self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr)          

        self.loss = CrossEntropyLoss()
        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))
        self.glances = args.glances

        self.current_task = 0
        self.memories = args.memories
        self.batchSize = int(args.replay_batch_size)

        # allocate buffer
        self.M = []
        self.age = 0
        
        # handle gpus if specified
        self.cuda = args.cuda
        if self.cuda:
            self.net = self.net.cuda()

        self.n_outputs = n_outputs
        if self.is_cifar:
            self.nc_per_task = int(n_outputs / n_tasks)
        else:
            self.nc_per_task = n_outputs


    def compute_offsets(self, task):
        offset1 = task * self.nc_per_task
        offset2 = (task + 1) * self.nc_per_task
        return int(offset1), int(offset2)
            
    def take_multitask_loss(self, bt, logits, y):
        loss = 0.0
        for i, ti in enumerate(bt):
            offset1, offset2 = self.compute_offsets(ti)
            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0)-offset1)
        return loss/len(bt)

    def forward(self, x, t):
        output = self.net.forward(x)
        if self.is_cifar:
            # make sure we predict classes within the current task
            offset1, offset2 = self.compute_offsets(t)
            if offset1 > 0:
                output[:, :offset1].data.fill_(-10e10)
            if offset2 < self.n_outputs:
                output[:, offset2:self.n_outputs].data.fill_(-10e10)
        return output

    def getBatch(self, x, y, t):
        if(x is not None):
            mxi = np.array(x)
            myi = np.array(y)
            mti = np.ones(x.shape[0], dtype=int)*t            
        else:
            mxi = np.empty( shape=(0, 0) )
            myi = np.empty( shape=(0, 0) )
            mti = np.empty( shape=(0, 0) )

        bxs = []
        bys = []
        bts = []

        if len(self.M) > 0:
            order = [i for i in range(0,len(self.M))]
            osize = min(self.batchSize,len(self.M))
            for j in range(0,osize):
                shuffle(order)
                k = order[j]
                x,y,t = self.M[k]
                xi = np.array(x)
                yi = np.array(y)
                ti = np.array(t)
                
                bxs.append(xi)
                bys.append(yi)
                bts.append(ti)

        for i in range(len(myi)):
            bxs.append(mxi[i])
            bys.append(myi[i])
            bts.append(mti[i])

        bxs = Variable(torch.from_numpy(np.array(bxs))).float()
        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)
        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)
        
        # handle gpus if specified
        if self.cuda:
            bxs = bxs.cuda()
            bys = bys.cuda()
            bts = bts.cuda()
 
        return bxs,bys,bts


    def observe(self, x, y, t):
        ### step through elements of x

        xi = x.data.cpu().numpy()
        yi = y.data.cpu().numpy()

        if t != self.current_task:
           self.current_task = t

        if self.args.learn_lr:
            loss = self.la_ER(x, y, t)
        else:
            loss = self.ER(xi, yi, t)

        for i in range(0, x.size()[0]):
            self.age += 1
            # Reservoir sampling memory update:
            if len(self.M) < self.memories:
                self.M.append([xi[i], yi[i], t])

            else:
                p = random.randint(0,self.age)
                if p < self.memories:
                    self.M[p] = [xi[i], yi[i], t]

        return loss.item()

    def ER(self, x, y, t):
        for pass_itr in range(self.glances):

            self.net.zero_grad()
            
            # Draw batch from buffer:
            bx,by,bt = self.getBatch(x,y,t)

            bx = bx.squeeze()
            prediction = self.net.forward(bx)
            loss = self.take_multitask_loss(bt, prediction, by)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

            self.opt_wt.step()
        
        return loss

    def inner_update(self, x, fast_weights, y, t):
        """
        Update the fast weights using the current samples and return the updated fast
        """

        if self.is_cifar:
            offset1, offset2 = self.compute_offsets(t)            
            logits = self.net.forward(x, fast_weights)[:, :offset2]
            loss = self.loss(logits[:, offset1:offset2], y-offset1)
        else:
            logits = self.net.forward(x, fast_weights)
            loss = self.loss(logits, y)   

        if fast_weights is None:
            fast_weights = self.net.parameters()

        graph_required = self.args.second_order
        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))

        for i in range(len(grads)):
            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)

        fast_weights = list(
            map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))
        return fast_weights, loss.item()


    def la_ER(self, x, y, t):
        """
        this ablation tests whether it suffices to just do the learning rate modulation
        guided by gradient alignment + clipping (that La-MAML does implciitly through autodiff)
        and use it with ER (therefore no meta-learning for the weights)

        """
        for pass_itr in range(self.glances):
            
            perm = torch.randperm(x.size(0))
            x = x[perm]
            y = y[perm]

            batch_sz = x.shape[0]
            n_batches = self.args.cifar_batches
            rough_sz = math.ceil(batch_sz/n_batches)
            fast_weights = None
            meta_losses = [0 for _ in range(n_batches)] 

            bx, by, bt = self.getBatch(x.cpu().numpy(), y.cpu().numpy(), t)
            bx = bx.squeeze()
            
            for i in range(n_batches):

                batch_x = x[i*rough_sz : (i+1)*rough_sz]
                batch_y = y[i*rough_sz : (i+1)*rough_sz]

                # assuming labels for inner update are from the same 
                fast_weights, inner_loss = self.inner_update(batch_x, fast_weights, batch_y, t)

                prediction = self.net.forward(bx, fast_weights)
                meta_loss = self.take_multitask_loss(bt, prediction, by)
                meta_losses[i] += meta_loss

            # update alphas
            self.net.zero_grad()
            self.opt_lr.zero_grad()

            meta_loss = meta_losses[-1] #sum(meta_losses)/len(meta_losses)
            meta_loss.backward()

            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)
            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)
            
            # update the LRs (guided by meta-loss, but not the weights)
            self.opt_lr.step()

            # update weights
            self.net.zero_grad()

            # compute ER loss for network weights
            prediction = self.net.forward(bx)
            loss = self.take_multitask_loss(bt, prediction, by)

            loss.backward()

            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

            # update weights with grad from simple ER loss 
            # and LRs obtained from meta-loss guided by old and new tasks
            for i,p in enumerate(self.net.parameters()):                                 
                p.data = p.data - (p.grad * nn.functional.relu(self.net.alpha_lr[i]))       
            self.net.zero_grad()
            self.net.alpha_lr.zero_grad()

        return loss
File Path: model/gem.py
Content:
### This is a copy of GEM from https://github.com/facebookresearch/GradientEpisodicMemory. 
### In order to ensure complete reproducability, we do not change the file and treat it as a baseline.

# Copyright 2019-present, IBM Research
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable

import model.meta.learner as Learner
import model.meta.modelfactory as mf
import numpy as np
import quadprog

# Auxiliary functions useful for GEM's inner optimization.

def compute_offsets(task, nc_per_task, is_cifar):
    """
        Compute offsets for cifar to determine which
        outputs to select for a given task.
    """
    if is_cifar:
        offset1 = task * nc_per_task
        offset2 = (task + 1) * nc_per_task
    else:
        offset1 = 0
        offset2 = nc_per_task
    return offset1, offset2


def store_grad(pp, grads, grad_dims, tid):
    """
        This stores parameter gradients of past tasks.
        pp: parameters
        grads: gradients
        grad_dims: list with number of parameters per layers
        tid: task id
    """
    # store the gradients
    grads[:, tid].fill_(0.0)
    cnt = 0
    for param in pp():
        if param.grad is not None:
            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])
            en = sum(grad_dims[:cnt + 1])
            grads[beg: en, tid].copy_(param.grad.data.view(-1))
        cnt += 1


def overwrite_grad(pp, newgrad, grad_dims):
    """
        This is used to overwrite the gradients with a new gradient
        vector, whenever violations occur.
        pp: parameters
        newgrad: corrected gradient
        grad_dims: list storing number of parameters at each layer
    """
    cnt = 0
    for param in pp():
        if param.grad is not None:
            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])
            en = sum(grad_dims[:cnt + 1])
            this_grad = newgrad[beg: en].contiguous().view(
                param.grad.data.size())
            param.grad.data.copy_(this_grad)
        cnt += 1



def project2cone2(gradient, memories, margin=0.5, eps = 1e-3):
    """
        Solves the GEM dual QP described in the paper given a proposed
        gradient "gradient", and a memory of task gradients "memories".
        Overwrites "gradient" with the final projected update.
        input:  gradient, p-vector
        input:  memories, (t * p)-vector
        output: x, p-vector
    """
    memories_np = memories.cpu().t().double().numpy()
    gradient_np = gradient.cpu().contiguous().view(-1).double().numpy()
    t = memories_np.shape[0]
    P = np.dot(memories_np, memories_np.transpose())
    P = 0.5 * (P + P.transpose())  + np.eye(t) * eps
    q = np.dot(memories_np, gradient_np) * -1
    G = np.eye(t)
    h = np.zeros(t) + margin
    v = quadprog.solve_qp(P, q, G, h)[0]
    x = np.dot(v, memories_np) + gradient_np
    gradient.copy_(torch.Tensor(x).view(-1, 1))


class Net(nn.Module):
    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,
                 args):
        super(Net, self).__init__()
        self.args = args
        self.margin = args.memory_strength
        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))

        nl, nh = args.n_layers, args.n_hiddens
        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)
        self.net = Learner.Learner(config, args=args)

        self.netforward = self.net.forward

        self.ce = nn.CrossEntropyLoss()
        self.n_outputs = n_outputs
        self.glances = args.glances

        self.opt = optim.SGD(self.parameters(), args.lr)

        self.n_memories = args.n_memories
        self.gpu = args.cuda

        # allocate episodic memory
        self.memory_data = torch.FloatTensor(
            n_tasks, self.n_memories, n_inputs)
        self.memory_labs = torch.LongTensor(n_tasks, self.n_memories)
        if args.cuda:
            self.memory_data = self.memory_data.cuda()
            self.memory_labs = self.memory_labs.cuda()

        # allocate temporary synaptic memory
        self.grad_dims = []
        for param in self.parameters():
            self.grad_dims.append(param.data.numel())
        self.grads = torch.Tensor(sum(self.grad_dims), n_tasks)
        if args.cuda:
            self.grads = self.grads.cuda()

        # allocate counters
        self.observed_tasks = []
        self.old_task = -1
        self.mem_cnt = 0
        if self.is_cifar:
            self.nc_per_task = int(n_outputs / n_tasks)
        else:
            self.nc_per_task = n_outputs
        
        if args.cuda:
            self.cuda()

    def forward(self, x, t):
        if self.args.dataset == 'tinyimagenet':
            x = x.view(-1, 3, 64, 64)
        elif self.args.dataset == 'cifar100':
            x = x.view(-1, 3, 32, 32)
        output = self.netforward(x)
        if self.is_cifar:
            # make sure we predict classes within the current task
            offset1 = int(t * self.nc_per_task)
            offset2 = int((t + 1) * self.nc_per_task)
            if offset1 > 0:
                output[:, :offset1].data.fill_(-10e10)
            if offset2 < self.n_outputs:
                output[:, offset2:self.n_outputs].data.fill_(-10e10)
        return output

    def observe(self, x, y, t):
        """
        Function equivalent to a single fwd+bkwd loop on one batch
        of x,y,t: inputs, labels, task IDs

        for each batch of (x,y,t) input to this function
        the upadte is done 'glnaces' number of times
        since in the single-pass setting, this batch is seen only once
        and so enough updates need to be made on each data point 
        """
        x = x.view(x.size(0), -1)
        # update memory
        if t != self.old_task:
            self.observed_tasks.append(t)
            self.old_task = t
             
        # in the single-pass setting, take multiple glances over every batch   
        for pass_itr in range(self.glances):

            # only make changes like pushing to buffer once per batch and not for every glance
            if(pass_itr==0):
                # Update ring buffer storing examples from current task
                bsz = y.data.size(0)
                endcnt = min(self.mem_cnt + bsz, self.n_memories)
                effbsz = endcnt - self.mem_cnt
                self.memory_data[t, self.mem_cnt: endcnt].copy_(
                    x.data[: effbsz])
                if bsz == 1:
                    self.memory_labs[t, self.mem_cnt] = y.data[0]
                else:
                    self.memory_labs[t, self.mem_cnt: endcnt].copy_(
                        y.data[: effbsz])
                self.mem_cnt += effbsz
                if self.mem_cnt == self.n_memories:
                    self.mem_cnt = 0

            # compute gradient on previous tasks
            if len(self.observed_tasks) > 1:
                for tt in range(len(self.observed_tasks) - 1):
                    self.zero_grad()
                    # fwd/bwd on the examples in the memory
                    past_task = self.observed_tasks[tt]

                    offset1, offset2 = compute_offsets(past_task, self.nc_per_task,
                                                       self.is_cifar)
                    ptloss = self.ce(
                        self.forward(
                            Variable(self.memory_data[past_task]),
                            past_task)[:, offset1: offset2],
                        Variable(self.memory_labs[past_task] - offset1))
                    ptloss.backward()
                    torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

                    store_grad(self.parameters, self.grads, self.grad_dims,
                               past_task)

            # now compute the grad on the current minibatch
            self.zero_grad()

            offset1, offset2 = compute_offsets(t, self.nc_per_task, self.is_cifar)
            loss = self.ce(self.forward(x, t)[:, offset1: offset2], y - offset1)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

            # check if gradient violates constraints
            if len(self.observed_tasks) > 1:
                # copy gradient
                store_grad(self.parameters, self.grads, self.grad_dims, t)
                indx = torch.cuda.LongTensor(self.observed_tasks[:-1]) if self.gpu \
                    else torch.LongTensor(self.observed_tasks[:-1])
                dotp = torch.mm(self.grads[:, t].unsqueeze(0),
                                self.grads.index_select(1, indx))
                if (dotp < 0).sum() != 0:
                    project2cone2(self.grads[:, t].unsqueeze(1),
                                  self.grads.index_select(1, indx), self.margin)
                    # copy gradients back
                    overwrite_grad(self.parameters, self.grads[:, t],
                                   self.grad_dims)

            self.opt.step()
        return loss.item()

File Path: model/icarl.py
Content:
# Copyright 2017-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch

import numpy as np
import random

import model.meta.learner as Learner
import model.meta.modelfactory as mf
import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("once")

class Net(torch.nn.Module):
    # Re-implementation of
    # S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert.
    # iCaRL: Incremental classifier and representation learning.
    # CVPR, 2017.
    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,
                 args):
        super(Net, self).__init__()
        self.args = args
        self.nt = n_tasks
        self.reg = args.memory_strength
        self.n_memories = args.n_memories
        self.num_exemplars = 0
        self.n_feat = n_outputs
        self.n_classes = n_outputs
        self.samples_per_task = args.samples_per_task * (1.0 - args.validation)
        if self.samples_per_task <= 0:
            error('set explicitly args.samples_per_task')
        self.examples_seen = 0

        self.glances = args.glances
        # setup network

        nl, nh = args.n_layers, args.n_hiddens
        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)
        self.net = Learner.Learner(config, args)

        # setup optimizer
        self.opt = torch.optim.SGD(self.parameters(), lr=args.lr)

        # setup losses
        self.bce = torch.nn.CrossEntropyLoss()
        self.kl = torch.nn.KLDivLoss()  # for distillation
        self.lsm = torch.nn.LogSoftmax(dim=1)
        self.sm = torch.nn.Softmax(dim=1)

        # memory
        self.memx = None  # stores raw inputs, PxD
        self.memy = None
        self.mem_class_x = {}  # stores exemplars class by class
        self.mem_class_y = {}

        self.gpu = args.cuda
        self.nc_per_task = int(n_outputs / n_tasks)
        self.n_outputs = n_outputs

    def netforward(self, x):
        if self.args.dataset == 'tinyimagenet':
            x = x.view(-1, 3, 64, 64)
        elif self.args.dataset == 'cifar100':
            x = x.view(-1, 3, 32, 32)

        return self.net.forward(x)

    def compute_offsets(self, task):
        offset1 = task * self.nc_per_task
        offset2 = (task + 1) * self.nc_per_task
        return int(offset1), int(offset2)

    def forward(self, x, t):
        # nearest neighbor
        nd = self.n_feat
        ns = x.size(0)
        if t * self.nc_per_task not in self.mem_class_x.keys():
            # no exemplar in memory yet, output uniform distr. over classes in
            # task t above, we check presence of first class for this task, we
            # should check them all
            out = torch.Tensor(ns, self.n_classes).fill_(-10e10)
            out[:, int(t * self.nc_per_task): int((t + 1) * self.nc_per_task)].fill_(
                1.0 / self.nc_per_task)
            if self.gpu:
                out = out.cuda()
            return out
        means = torch.ones(self.nc_per_task, nd) * float('inf')
        if self.gpu:
            means = means.cuda()
        offset1, offset2 = self.compute_offsets(t)
        for cc in range(offset1, offset2):
            means[cc -
                  offset1] =self.netforward(self.mem_class_x[cc]).data.mean(0)
        classpred = torch.LongTensor(ns)
        preds = self.netforward(x).data.clone()
        for ss in range(ns):
            dist = (means - preds[ss].expand(self.nc_per_task, nd)).norm(2, 1)
            _, ii = dist.min(0)
            ii = ii.squeeze()
            classpred[ss] = ii.item() + offset1

        out = torch.zeros(ns, self.n_classes)
        if self.gpu:
            out = out.cuda()
        for ss in range(ns):
            out[ss, classpred[ss]] = 1
        return out  # return 1-of-C code, ns x nc

    def forward_training(self, x, t):
        output = self.netforward(x)
        # make sure we predict classes within the current task
        offset1, offset2 = self.compute_offsets(t)

        # zero out all the logits outside the task's range
        # since the output vector from the model is of dimension (num_tasks * num_classes_per_task)
        if offset1 > 0:
            output[:, :offset1].data.fill_(-10e10)
        if offset2 < self.n_outputs:
            output[:, offset2:self.n_outputs].data.fill_(-10e10)
        return output

    def observe(self, x, y, t):

        x = x.view(x.size(0), -1)
        self.net.train()

        for pass_itr in range(self.glances):

            # only make changes like pushing to buffer once per batch and not for every glance
            if(pass_itr==0):
                self.examples_seen += x.size(0)

                if self.examples_seen < self.samples_per_task:
                    if self.memx is None:
                        self.memx = x.data.clone()
                        self.memy = y.data.clone()
                    else:
                        self.memx = torch.cat((self.memx, x.data.clone()))
                        self.memy = torch.cat((self.memy, y.data.clone()))

            self.net.zero_grad()
            offset1, offset2 = self.compute_offsets(t)
            loss = self.bce((self.netforward(x)[:, offset1: offset2]),
                            y - offset1)

            if self.num_exemplars > 0:
                # distillation
                for tt in range(t):
                    # first generate a minibatch with one example per class from
                    # previous tasks
                    inp_dist = torch.zeros(self.nc_per_task, x.size(1))
                    target_dist = torch.zeros(self.nc_per_task, self.n_feat)
                    offset1, offset2 = self.compute_offsets(tt)
                    if self.gpu:
                        inp_dist = inp_dist.cuda()
                        target_dist = target_dist.cuda()
                    for cc in range(self.nc_per_task):
                        indx = random.randint(0, len(self.mem_class_x[cc + offset1]) - 1)
                        inp_dist[cc] = self.mem_class_x[cc + offset1][indx].clone()
                        target_dist[cc] = self.mem_class_y[cc +
                                                           offset1][indx].clone()
                    # Add distillation loss
                    loss += self.reg * self.kl(
                        self.lsm(self.netforward(inp_dist)
                                 [:, offset1: offset2]),
                        self.sm(target_dist[:, offset1: offset2])) * self.nc_per_task
            # bprop and update
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)

            self.opt.step()

        # check whether this is the last minibatch of the current task
        # We assume only 1 epoch!
        if self.examples_seen == self.args.n_epochs * self.samples_per_task:
            self.examples_seen = 0
            # get labels from previous task; we assume labels are consecutive
            if self.gpu:
                all_labs = torch.LongTensor(np.unique(self.memy.cpu().numpy()))
            else:
                all_labs = torch.LongTensor(np.unique(self.memy.numpy()))
            num_classes = all_labs.size(0)
            assert(num_classes == self.nc_per_task)
            # Reduce exemplar set by updating value of num. exemplars per class
            self.num_exemplars = int(self.n_memories /
                                     (num_classes + len(self.mem_class_x.keys())))
            offset1, offset2 = self.compute_offsets(t)
            for ll in range(num_classes):
                lab = all_labs[ll].cuda()
                indxs = (self.memy == lab).nonzero().squeeze()
                cdata = self.memx.index_select(0, indxs)
                # Construct exemplar set for last task
                mean_feature = self.netforward(cdata)[
                    :, offset1: offset2].data.clone().mean(0)
                nd = self.nc_per_task
                exemplars = torch.zeros(self.num_exemplars, x.size(1))
                if self.gpu:
                    exemplars = exemplars.cuda()
                ntr = cdata.size(0)
                # used to keep track of which examples we have already used
                taken = torch.zeros(ntr)
                model_output = self.netforward(cdata)[
                    :, offset1: offset2].data.clone()
                for ee in range(self.num_exemplars):
                    prev = torch.zeros(1, nd)
                    if self.gpu:
                        prev = prev.cuda()
                    if ee > 0:
                        prev = self.netforward(exemplars[:ee])[
                            :, offset1: offset2].data.clone().sum(0)
                    cost = (mean_feature.expand(ntr, nd) - (model_output
                                                            + prev.expand(ntr, nd)) / (ee + 1)).norm(2, 1).squeeze()
                    _, indx = cost.sort(0)
                    winner = 0
                    while winner < indx.size(0) and taken[indx[winner]] == 1:
                        winner += 1
                    if winner < indx.size(0):
                        taken[indx[winner]] = 1
                        exemplars[ee] = cdata[indx[winner]].clone()
                    else:
                        exemplars = exemplars[:indx.size(0), :].clone()
                        self.num_exemplars = indx.size(0)
                        break
                # update memory with exemplars
                self.mem_class_x[lab.item()] = exemplars.clone()

            # recompute outputs for distillation purposes
            for cc in self.mem_class_x.keys():
                self.mem_class_x[cc] = self.mem_class_x[cc][:self.num_exemplars]
                self.mem_class_y[cc] = self.netforward(
                    self.mem_class_x[cc]).data.clone()
            self.memx = None
            self.memy = None
            print(len(self.mem_class_x[0]))

        return loss.item()
File Path: model/iid2.py
Content:
import torch

import numpy as np
import random

import model.meta.learner as Learner
import model.meta.modelfactory as mf
import ipdb
import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("once")

"""
Multi task
    big batch size, set increment 100 so that it is treated as 1 task with all classes in the dataset
    inference time for acc eval, use offsets
"""

class Net(torch.nn.Module):
    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,
                 args):
        super(Net, self).__init__()
        self.args = args
        self.nt = n_tasks

        self.n_feat = n_outputs
        self.n_classes = n_outputs

        arch = args.arch
        nl, nh = args.n_layers, args.n_hiddens
        config = mf.ModelFactory.get_model(model_type = arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)
        self.net = Learner.Learner(config, args)

        # setup optimizer
        self.opt = torch.optim.SGD(self.parameters(), lr=args.lr)

        # setup losses
        self.loss = torch.nn.CrossEntropyLoss()

        self.gpu = args.cuda
        self.nc_per_task = int(n_outputs / n_tasks)
        self.n_outputs = n_outputs

    def compute_offsets(self, task):
        offset1 = task * self.nc_per_task
        offset2 = (task + 1) * self.nc_per_task
        return int(offset1), int(offset2)

    def take_multitask_loss(self, bt, logits, y):
        loss = 0.0
        for i, ti in enumerate(bt):
            offset1, offset2 = self.compute_offsets(ti)
            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0)-offset1)
        return loss/len(bt)

    def forward(self, x, t):                                  
                                                
        output = self.net.forward(x)

        # make sure we predict classes within the current task
        if torch.unique(t).shape[0] == 1:
            offset1, offset2 = self.compute_offsets(t[0].item())
            if offset1 > 0:
                output[:, :offset1].data.fill_(-10e10)
            if offset2 < self.n_outputs:
                output[:, int(offset2):self.n_outputs].data.fill_(-10e10)
        else:
            for i in range(len(t)):
                offset1, offset2 = self.compute_offsets(t[i])
                if offset1 > 0:
                    output[i, :offset1].data.fill_(-10e10)
                if offset2 < self.n_outputs:
                    output[i, int(offset2):self.n_outputs].data.fill_(-10e10)
        return output

    def observe(self, x, y, t):
        self.net.train()

        self.net.zero_grad()
        logits = self.net.forward(x)
        loss = self.take_multitask_loss(t, logits, y) 
        loss.backward()
        self.opt.step()

        return loss.item()
File Path: model/lamaml.py
Content:
import random
import numpy as np
import ipdb
import math

import torch
import torch.nn as nn
from model.lamaml_base import *


class Net(BaseNet):

    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,           
                 args):
        super(Net, self).__init__(n_inputs,
                                 n_outputs,
                                 n_tasks,           
                                 args)

        self.nc_per_task = n_outputs

    def forward(self, x, t):
        output = self.net.forward(x)
        return output

    def meta_loss(self, x, fast_weights, y, t):
        """
        differentiate the loss through the network updates wrt alpha
        """
        logits = self.net.forward(x, fast_weights)
        loss_q = self.loss(logits.squeeze(1), y)
        return loss_q, logits

    def inner_update(self, x, fast_weights, y, t):
        """
        Update the fast weights using the current samples and return the updated fast
        """
        logits = self.net.forward(x, fast_weights)
        loss = self.loss(logits, y)   

        if fast_weights is None:
            fast_weights = self.net.parameters() 

        # NOTE if we want higher order grads to be allowed, change create_graph=False to True
        graph_required = self.args.second_order
        grads = torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required)

        for i in range(len(grads)):
            torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)

        fast_weights = list(
                map(lambda p: p[1][0] - p[0] * nn.functional.relu(p[1][1]), zip(grads, zip(fast_weights, self.net.alpha_lr))))
        return fast_weights

    def observe(self, x, y, t):
        self.net.train() 

        for pass_itr in range(self.glances):
            self.pass_itr = pass_itr
            
            perm = torch.randperm(x.size(0))
            x = x[perm]
            y = y[perm]
            
            self.epoch += 1
            self.zero_grads()

            if t != self.current_task:
                self.M = self.M_new
                self.current_task = t

            batch_sz = x.shape[0]
            meta_losses = [0 for _ in range(batch_sz)] 

            bx, by, bt = self.getBatch(x.cpu().numpy(), y.cpu().numpy(), t)
            fast_weights = None

            for i in range(0, batch_sz):
                batch_x = x[i].unsqueeze(0)
                batch_y = y[i].unsqueeze(0)

                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)
                if(self.real_epoch == 0):
                    self.push_to_mem(batch_x, batch_y, torch.tensor(t))

                meta_loss, logits = self.meta_loss(bx, fast_weights, by, t) 
                meta_losses[i] += meta_loss
    
            # Taking the meta gradient step (will update the learning rates)
            self.zero_grads()

            meta_loss = sum(meta_losses)/len(meta_losses)

            meta_loss.backward()

            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)
            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)

            if self.args.learn_lr:
                self.opt_lr.step()

            if(self.args.sync_update):
                self.opt_wt.step()
            else:  
                for i,p in enumerate(self.net.parameters()):
                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])
         
            self.net.zero_grad()
            self.net.alpha_lr.zero_grad()

        return meta_loss.item()
File Path: model/lamaml_base.py
Content:
import random
from random import shuffle
import numpy as np
import ipdb
import math
import torch
from torch.autograd import Variable
import torch.nn as nn
import model.meta.learner as Learner
import model.meta.modelfactory as mf
from scipy.stats import pearsonr
import datetime

class BaseNet(torch.nn.Module):

    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,           
                 args):
        super(BaseNet, self).__init__()

        self.args = args
        nl, nh = args.n_layers, args.n_hiddens

        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)

        self.net = Learner.Learner(config, args)

        # define the lr params
        self.net.define_task_lr_params(alpha_init = args.alpha_init)

        self.opt_wt = torch.optim.SGD(list(self.net.parameters()), lr=args.opt_wt)     
        self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr) 

        self.epoch = 0
        # allocate buffer
        self.M = []        
        self.M_new = []
        self.age = 0

        # setup losses
        self.loss = torch.nn.CrossEntropyLoss()
        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))
        self.glances = args.glances
        self.pass_itr = 0
        self.real_epoch = 0

        self.current_task = 0
        self.memories = args.memories
        self.batchSize = int(args.replay_batch_size)

        self.cuda = args.cuda
        if self.cuda:
            self.net = self.net.cuda()

        self.n_outputs = n_outputs

    def push_to_mem(self, batch_x, batch_y, t):
        """
        Reservoir sampling to push subsampled stream
        of data points to replay/memory buffer
        """

        if(self.real_epoch > 0 or self.pass_itr>0):
            return
        batch_x = batch_x.cpu()
        batch_y = batch_y.cpu()              
        t = t.cpu()

        for i in range(batch_x.shape[0]):
            self.age += 1
            if len(self.M_new) < self.memories:
                self.M_new.append([batch_x[i], batch_y[i], t])
            else:
                p = random.randint(0,self.age)  
                if p < self.memories:
                    self.M_new[p] = [batch_x[i], batch_y[i], t]


    def getBatch(self, x, y, t, batch_size=None):
        """
        Given the new data points, create a batch of old + new data, 
        where old data is sampled from the memory buffer
        """

        if(x is not None):
            mxi = np.array(x)
            myi = np.array(y)
            mti = np.ones(x.shape[0], dtype=int)*t        
        else:
            mxi = np.empty( shape=(0, 0) )
            myi = np.empty( shape=(0, 0) )
            mti = np.empty( shape=(0, 0) )    

        bxs = []
        bys = []
        bts = []

        if self.args.use_old_task_memory and t>0:
            MEM = self.M
        else:
            MEM = self.M_new

        batch_size = self.batchSize if batch_size is None else batch_size

        if len(MEM) > 0:
            order = [i for i in range(0,len(MEM))]
            osize = min(batch_size,len(MEM))
            for j in range(0,osize):
                shuffle(order)
                k = order[j]
                x,y,t = MEM[k]

                xi = np.array(x)
                yi = np.array(y)
                ti = np.array(t)
                bxs.append(xi)
                bys.append(yi)
                bts.append(ti)

        for j in range(len(myi)):
            bxs.append(mxi[j])
            bys.append(myi[j])
            bts.append(mti[j])

        bxs = Variable(torch.from_numpy(np.array(bxs))).float() 
        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)
        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)
        
        # handle gpus if specified
        if self.cuda:
            bxs = bxs.cuda()
            bys = bys.cuda()
            bts = bts.cuda()

        return bxs,bys,bts

    def compute_offsets(self, task):
        # mapping from classes [1-100] to their idx within a task
        offset1 = task * self.nc_per_task
        offset2 = (task + 1) * self.nc_per_task
        return int(offset1), int(offset2)

    def zero_grads(self):
        if self.args.learn_lr:
            self.opt_lr.zero_grad()
        self.opt_wt.zero_grad()
        self.net.zero_grad()
        self.net.alpha_lr.zero_grad()
File Path: model/lamaml_cifar.py
Content:
import random
import numpy as np
import ipdb
import math
import torch
import torch.nn as nn
from model.lamaml_base import *

class Net(BaseNet):

    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,           
                 args):
        super(Net, self).__init__(n_inputs,
                                 n_outputs,
                                 n_tasks,           
                                 args)
        self.nc_per_task = n_outputs / n_tasks

    def take_loss(self, t, logits, y):
        # compute loss on data from a single task
        offset1, offset2 = self.compute_offsets(t)
        loss = self.loss(logits[:, offset1:offset2], y-offset1)

        return loss

    def take_multitask_loss(self, bt, t, logits, y):
        # compute loss on data from a multiple tasks
        # separate from take_loss() since the output positions for each task's
        # logit vector are different and we nly want to compute loss on the relevant positions
        # since this is a task incremental setting

        loss = 0.0

        for i, ti in enumerate(bt):
            offset1, offset2 = self.compute_offsets(ti)
            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0)-offset1)
        return loss/len(bt)


    def forward(self, x, t):
        output = self.net.forward(x)
        # make sure we predict classes within the current task
        offset1, offset2 = self.compute_offsets(t)
        if offset1 > 0:
            output[:, :offset1].data.fill_(-10e10)
        if offset2 < self.n_outputs:
            output[:, int(offset2):self.n_outputs].data.fill_(-10e10)
        return output

    def meta_loss(self, x, fast_weights, y, bt, t):
        """
        differentiate the loss through the network updates wrt alpha
        """

        offset1, offset2 = self.compute_offsets(t)

        logits = self.net.forward(x, fast_weights)[:, :offset2]
        loss_q = self.take_multitask_loss(bt, t, logits, y)

        return loss_q, logits

    def inner_update(self, x, fast_weights, y, t):
        """
        Update the fast weights using the current samples and return the updated fast
        """

        offset1, offset2 = self.compute_offsets(t)            

        logits = self.net.forward(x, fast_weights)[:, :offset2]
        loss = self.take_loss(t, logits, y)

        if fast_weights is None:
            fast_weights = self.net.parameters()

        # NOTE if we want higher order grads to be allowed, change create_graph=False to True
        graph_required = self.args.second_order
        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))

        for i in range(len(grads)):
            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)

        fast_weights = list(
            map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))

        return fast_weights


    def observe(self, x, y, t):
        self.net.train() 
        for pass_itr in range(self.glances):
            self.pass_itr = pass_itr
            perm = torch.randperm(x.size(0))
            x = x[perm]
            y = y[perm]

            self.epoch += 1
            self.zero_grads()

            if t != self.current_task:
                self.M = self.M_new.copy()
                self.current_task = t

            batch_sz = x.shape[0]
            n_batches = self.args.cifar_batches
            rough_sz = math.ceil(batch_sz/n_batches)
            fast_weights = None
            meta_losses = [0 for _ in range(n_batches)]

            # get a batch by augmented incming data with old task data, used for 
            # computing meta-loss
            bx, by, bt = self.getBatch(x.cpu().numpy(), y.cpu().numpy(), t)             

            for i in range(n_batches):

                batch_x = x[i*rough_sz : (i+1)*rough_sz]
                batch_y = y[i*rough_sz : (i+1)*rough_sz]

                # assuming labels for inner update are from the same 
                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)   
                # only sample and push to replay buffer once for each task's stream
                # instead of pushing every epoch     
                if(self.real_epoch == 0):
                    self.push_to_mem(batch_x, batch_y, torch.tensor(t))
                meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) 
                
                meta_losses[i] += meta_loss

            # Taking the meta gradient step (will update the learning rates)
            self.zero_grads()

            meta_loss = sum(meta_losses)/len(meta_losses)            
            meta_loss.backward()

            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)
            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)
            if self.args.learn_lr:
                self.opt_lr.step()

            # if sync-update is being carried out (as in sync-maml) then update the weights using the optimiser
            # otherwise update the weights with sgd using updated LRs as step sizes
            if(self.args.sync_update):
                self.opt_wt.step()
            else:            
                for i,p in enumerate(self.net.parameters()):          
                    # using relu on updated LRs to avoid negative values           
                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])            
            self.net.zero_grad()
            self.net.alpha_lr.zero_grad()

        return meta_loss.item()


File Path: model/meralg1.py
Content:
# An implementation of MER Algorithm 1 from https://openreview.net/pdf?id=B1gTShAct7

# Copyright 2019-present, IBM Research
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable

import numpy as np

import random
from torch.nn.modules.loss import CrossEntropyLoss
from random import shuffle
import sys
import ipdb
from copy import deepcopy
import warnings
import model.meta.learner as Learner
import model.meta.modelfactory as mf
warnings.filterwarnings("ignore")

class Net(nn.Module):
    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,
                 args):
        super(Net, self).__init__()
        self.args = args
        nl, nh = args.n_layers, args.n_hiddens

        self.is_cifar = (args.dataset == 'cifar100' or args.dataset == 'tinyimagenet')
        config = mf.ModelFactory.get_model(args.arch, sizes=[n_inputs] + [nh] * nl + [n_outputs], dataset=args.dataset, args=args)
        self.net = Learner.Learner(config, args=args)

        self.netforward = self.net.forward

        self.bce = torch.nn.CrossEntropyLoss()

        self.n_outputs = n_outputs
        if self.is_cifar:
            self.nc_per_task = n_outputs / n_tasks
        else:
            self.nc_per_task = n_outputs

        self.opt = optim.SGD(self.parameters(), args.lr)
        self.batchSize = int(args.replay_batch_size)

        self.memories = args.memories
        self.steps = int(args.batches_per_example)
        self.beta = args.beta
        self.gamma = args.gamma

        # allocate buffer
        self.M = []
        self.age = 0

        # handle gpus if specified
        self.cuda = args.cuda
        if self.cuda:
            self.net = self.net.cuda()


    def forward(self, x, t):
        output = self.netforward(x)
        if self.is_cifar:
            offset1, offset2 = self.compute_offsets(t)
            if offset1 > 0:
                output[:, :offset1].data.fill_(-10e10)
            if offset2 < self.n_outputs:
                output[:, int(offset2):self.n_outputs].data.fill_(-10e10)
        return output
    
    def compute_offsets(self, task):
        if self.is_cifar:
            offset1 = task * self.nc_per_task
            offset2 = (task + 1) * self.nc_per_task
        else:
            offset1 = 0
            offset2 = self.n_outputs
        return int(offset1), int(offset2)

    def getBatch(self,x,y,t):
        if(x is not None):
            xi = Variable(torch.from_numpy(np.array(x))).float().unsqueeze(0) #.view(1,-1)
            yi = Variable(torch.from_numpy(np.array(y))).long()
            ti = Variable(torch.from_numpy(np.array(t))).long()

            if self.cuda:
                xi = xi.cuda()
                yi = yi.cuda()
                ti = ti.cuda()

            bxs = [xi]
            bys = [yi]
            bts = [ti]

        else:
            bxs = []
            bys = []
            bts = []

        if len(self.M) > 0:
            order = [i for i in range(0,len(self.M))]
            osize = min(self.batchSize,len(self.M))
            for j in range(0,osize):
                shuffle(order)
                k = order[j]
                x,y,t = self.M[k]
                xi = Variable(torch.from_numpy(np.array(x))).float().unsqueeze(0) #.view(1,-1)
                yi = Variable(torch.from_numpy(np.array(y))).long()
                ti = Variable(torch.from_numpy(np.array(t))).long()

                # handle gpus if specified
                if self.cuda:
                    xi = xi.cuda()
                    yi = yi.cuda()
                    ti = ti.cuda()

                bxs.append(xi)
                bys.append(yi)
                bts.append(ti)

        return bxs,bys,bts
               

    def observe(self, x, y, t):

        # step through elements of x
        for i in range(0,x.size()[0]):

            self.age += 1
            xi = x[i].data.cpu().numpy()
            yi = y[i].data.cpu().numpy()
            self.net.zero_grad()

            before = deepcopy(self.net.state_dict())
            for step in range(0,self.steps):
                weights_before = deepcopy(self.net.state_dict())
                ##Check for nan
                if weights_before != weights_before:
                    ipdb.set_trace()
                # Draw batch from buffer:
                bxs, bys, bts = self.getBatch(xi,yi,t)          
                loss = 0.0
                total_loss = 0.0
                for idx in range(len(bxs)):
                    
                    self.net.zero_grad()
                    bx = bxs[idx] 
                    by = bys[idx] 
                    bt = bts[idx]

                    if self.is_cifar:
                        offset1, offset2 = self.compute_offsets(bt)
                        prediction = (self.netforward(bx)[:, offset1:offset2])
                        loss = self.bce(prediction,
                                        by.unsqueeze(0)-offset1)
                    else:
                        prediction = self.forward(bx,0)
                        loss = self.bce(prediction, by.unsqueeze(0))
                    if torch.isnan(loss):
                        ipdb.set_trace()

                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)
                    self.opt.step()
                    total_loss += loss.item()
                weights_after = self.net.state_dict()
                if weights_after != weights_after:
                    ipdb.set_trace()

                # Within batch Reptile meta-update:
                self.net.load_state_dict({name : weights_before[name] + ((weights_after[name] - weights_before[name]) * self.beta) for name in weights_before})

            after = self.net.state_dict()

            # Across batch Reptile meta-update:
            self.net.load_state_dict({name : before[name] + ((after[name] - before[name]) * self.gamma) for name in before})

            # Reservoir sampling memory update:
            if len(self.M) < self.memories:
                self.M.append([xi,yi,t])

            else:
                p = random.randint(0,self.age)
                if p < self.memories:
                    self.M[p] = [xi,yi,t]

        return total_loss/self.steps


File Path: model/meta-bgd.py
Content:
import random
from random import shuffle
import numpy as np
import ipdb
import math

import torch
from torch.autograd import Variable
import torch.nn as nn
import model.meta.learner as Learner
import model.meta.modelfactory as mf

from model.optimizers_lib import optimizers_lib
from ast import literal_eval

"""
This baseline/ablation is constructed by merging C-MAML and BGD
By assigning a variance parameter to each NN parameter in the model
and using BGD's bayesian update to update these means (the NN parameters) and variances
(the learning rates in BGD are derived from the variances)

The 'n' bayesian samples in this case are the 'n' cumulative meta-losses sampled when 
C-MAML is run with 'n' different initial theta vectors as the NN means sampled from the 
(means, variances) stored for the model parameters.
The weight update is then carried out using the BGD formula that implicitly 
uses the variances to derive the learning rates for the parameters
"""

class Net(torch.nn.Module):

    def __init__(self,
                 n_inputs,
                 n_outputs,
                 n_tasks,           
                 args):
        super(Net, self).__init__()
        self.args = args
        nl, nh = args.n_layers, args.n_hiddens

        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],
                                                dataset = args.dataset, args=args)

        self.net = Learner.Learner(config, args)

        # define the lr params
        self.net.define_task_lr_params(alpha_init = args.alpha_init)

        self.cuda = args.cuda
        if self.cuda:
            self.net = self.net.cuda()

        # optimizer model
        optimizer_model = optimizers_lib.__dict__[args.bgd_optimizer]
        # params used to instantiate the BGD optimiser
        optimizer_params = dict({ #"logger": logger,
                                 "mean_eta": args.mean_eta,
                                 "std_init": args.std_init,
                                 "mc_iters": args.train_mc_iters}, **literal_eval(" ".join(args.optimizer_params)))
        self.optimizer = optimizer_model(self.net, **optimizer_params)

        self.epoch = 0
        # allocate buffer
        self.M = []        
        self.M_new = []
        self.age = 0

        # setup losses
        self.loss = torch.nn.CrossEntropyLoss()
        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))
        self.glances = args.glances
        self.pass_itr = 0
        self.real_epoch = 0

        # setup memories
        self.current_task = 0

        self.memories = args.memories
        self.batchSize = int(args.replay_batch_size)

        if self.is_cifar:
            self.nc_per_task = n_outputs / n_tasks
        else:
            self.nc_per_task = n_outputs
        self.n_outputs = n_outputs

        self.obseve_itr = 0

    def take_multitask_loss(self, bt, t, logits, y):
        loss = 0.0

        for i, ti in enumerate(bt):
            offset1, offset2 = self.compute_offsets(ti)
            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0)-offset1)
        return loss/len(bt)


    def forward(self, x, t, fast_weights=None):
        self.optimizer.randomize_weights(force_std=0)  
        output = self.net.forward(x, vars=fast_weights)
        if self.is_cifar:
            # make sure we predict classes within the current task
            offset1, offset2 = self.compute_offsets(t)
            if offset1 > 0:
                output[:, :offset1].data.fill_(-10e10)
            if offset2 < self.n_outputs:
                output[:, int(offset2):self.n_outputs].data.fill_(-10e10)
        return output

    def meta_loss(self, x, fast_weights, y, bt, t):
        """
        differentiate the loss through the network updates wrt alpha
        """

        if self.is_cifar:
            offset1, offset2 = self.compute_offsets(t)
            logits = self.net.forward(x, fast_weights)[:, :offset2]                   

            loss_q = self.take_multitask_loss(bt, t, logits, y)
        else:
            logits = self.net.forward(x, fast_weights)
            # Cross Entropy Loss over data
            loss_q = self.loss(logits, y)
        return loss_q, logits

    def compute_offsets(self, task):
        if self.is_cifar:
            offset1 = task * self.nc_per_task
            offset2 = (task + 1) * self.nc_per_task
        else:
            offset1 = 0
            offset2 = self.n_outputs
        return int(offset1), int(offset2)

    def push_to_mem(self, batch_x, batch_y, t):
        """
        Reservoir sampling memory update
        """

        if(self.real_epoch > 0 or self.pass_itr>0):
            return
        batch_x = batch_x.cpu()
        batch_y = batch_y.cpu()              
        t = t.cpu()

        for i in range(batch_x.shape[0]):
            self.age += 1
            if len(self.M_new) < self.memories:
                self.M_new.append([batch_x[i], batch_y[i], t])
            else:
                p = random.randint(0,self.age)  
                if p < self.memories:
                    self.M_new[p] = [batch_x[i], batch_y[i], t]


    def getBatch(self, x, y, t):
        """
        Given the new data points, create a batch of old + new data, 
        where old data is part of the memory buffer
        """

        if(x is not None):
            mxi = np.array(x)
            myi = np.array(y)
            mti = np.ones(x.shape[0], dtype=int)*t            
        else:
            mxi = np.empty( shape=(0, 0) )
            myi = np.empty( shape=(0, 0) )
            mti = np.empty( shape=(0, 0) )

        bxs = []
        bys = []
        bts = []

        if self.args.use_old_task_memory: # and t>0:
            MEM = self.M
        else:
            MEM = self.M_new
        
        if len(MEM) > 0:
            order = [i for i in range(0,len(MEM))]
            osize = min(self.batchSize,len(MEM))
            for j in range(0,osize):
                shuffle(order)
                k = order[j]
                x,y,t = MEM[k]

                xi = np.array(x)
                yi = np.array(y)
                ti = np.array(t)
                bxs.append(xi)
                bys.append(yi)
                bts.append(ti)

        for j in range(len(myi)):
            bxs.append(mxi[j])
            bys.append(myi[j])
            bts.append(mti[j])

        bxs = Variable(torch.from_numpy(np.array(bxs))).float()
        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)
        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)
        
        # handle gpus if specified
        if self.cuda:
            bxs = bxs.cuda()
            bys = bys.cuda()
            bts = bts.cuda()

        return bxs,bys,bts

    def take_loss(self, t, logits, y):
        offset1, offset2 = self.compute_offsets(t)
        loss = self.loss(logits[:, offset1:offset2], y-offset1)

        return loss

    def inner_update(self, x, fast_weights, y, t):            
        """
        Update the fast weights using the current samples and return the updated fast
        """
        if self.is_cifar:
            offset1, offset2 = self.compute_offsets(t)
            logits = self.net.forward(x, fast_weights)[:, :offset2]

            loss = self.take_loss(t, logits, y)
            # loss = self.loss(logits, y)
        else:
            logits = self.net.forward(x, fast_weights)
            loss = self.loss(logits, y)   

        if fast_weights is None:
            fast_weights = self.net.parameters()      

        # NOTE if we want higher order grads to be allowed, change create_graph=False to True
        graph_required = True
        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))
        
        for i in range(len(grads)):
            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)            

        # get fast weights vector by taking SGD step on grads
        fast_weights = list(
            map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))
        return fast_weights


    def observe(self, x, y, t):
        self.net.train()             
        self.obseve_itr += 1
                                                
        num_of_mc_iters = self.optimizer.get_mc_iters()

        for glance_itr in range(self.glances):

            mc_meta_losses = [0 for _ in range(num_of_mc_iters)]

            # running C-MAML num_of_mc_iters times to get montecarlo samples of meta-loss
            for pass_itr in range(num_of_mc_iters):
                self.optimizer.randomize_weights()                                                          

                self.pass_itr = pass_itr
                self.epoch += 1
                self.net.zero_grad()                      

                perm = torch.randperm(x.size(0))
                x = x[perm]
                y = y[perm]


                if pass_itr==0 and glance_itr ==0 and t != self.current_task:
                    self.M = self.M_new
                    self.current_task = t

                batch_sz = x.shape[0]

                n_batches = self.args.cifar_batches
                rough_sz = math.ceil(batch_sz/n_batches)

                # the samples of new task to iterate over in inner update trajectory
                iterate_till = 1 #batch_sz 
                meta_losses = [0 for _ in range(n_batches)]  
                accuracy_meta_set = [0 for _ in range(n_batches)] 

                # put some asserts to make sure replay batch size can accomodate old and new samples
                bx, by = None, None
                bx, by, bt = self.getBatch(x.cpu().numpy(), y.cpu().numpy(), t)             
                    
                fast_weights = None
                # inner loop/fast updates where learn on 1-2 samples in each inner step
                for i in range(n_batches):

                    batch_x = x[i*rough_sz : (i+1)*rough_sz]
                    batch_y = y[i*rough_sz : (i+1)*rough_sz]
                    fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)

                    if(pass_itr==0 and glance_itr==0):
                        self.push_to_mem(batch_x, batch_y, torch.tensor(t))

                    # the meta loss is computed at each inner step
                    # as this is shown to work better in Reptile [] 
                    meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) 
                    meta_losses[i] += meta_loss

                self.optimizer.zero_grad()                                                                   
                meta_loss = sum(meta_losses)/len(meta_losses)
                if torch.isnan(meta_loss):
                    ipdb.set_trace()
                meta_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)
                mc_meta_losses[pass_itr] = meta_loss
                self.optimizer.aggregate_grads(batch_size=batch_sz)           
             
            print_std = False                        
            if(self.obseve_itr%220==0):
                print_std = True                                                         
            self.optimizer.step(print_std = print_std)

        meta_loss_return = sum(mc_meta_losses)/len(mc_meta_losses)

        return meta_loss_return.item()


File Path: model/meta/learner.py
Content:
import math
import os
import sys
import traceback
import numpy as np
import ipdb

import torch
from torch import nn
from torch.nn import functional as F

class Learner(nn.Module):

    def __init__(self, config, args = None):
        """

        :param config: network config file, type:list of (string, list)
        :param imgc: 1 or 3
        :param imgsz:  28 or 84
        """
        super(Learner, self).__init__()

        self.config = config
        self.tf_counter = 0
        self.args = args

        # this dict contains all tensors needed to be optimized
        self.vars = nn.ParameterList()
        # running_mean and running_var
        self.vars_bn = nn.ParameterList()

        self.names = []

        for i, (name, param, extra_name) in enumerate(self.config):
            if name is 'conv2d':
                # [ch_out, ch_in, kernelsz, kernelsz]                
                if(self.args.xav_init):
                    w = nn.Parameter(torch.ones(*param[:4]))
                    b = nn.Parameter(torch.zeros(param[0]))
                    torch.nn.init.xavier_normal_(w.data)
                    b.data.normal_(0, math.sqrt(2)/math.sqrt(1+9*b.data.shape[0]))
                    self.vars.append(w)
                    self.vars.append(b)
                else:
                    w = nn.Parameter(torch.ones(*param[:4]))
                    # gain=1 according to cbfin's implementation
                    torch.nn.init.kaiming_normal_(w)
                    self.vars.append(w)
                    # [ch_out]
                    self.vars.append(nn.Parameter(torch.zeros(param[0])))

            elif name is 'convt2d':
                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]
                w = nn.Parameter(torch.ones(*param[:4]))
                # gain=1 according to cbfin's implementation
                torch.nn.init.kaiming_normal_(w)
                self.vars.append(w)
                # [ch_in, ch_out]
                self.vars.append(nn.Parameter(torch.zeros(param[1])))

            elif name is 'linear':
                # layer += 1
                if(self.args.xav_init):
                    w = nn.Parameter(torch.ones(*param))
                    # b = nn.Parameter(torch.zeros(param[0]))
                    torch.nn.init.xavier_normal_(w.data)
                    # b.data.normal_(0, math.sqrt(2)/math.sqrt(1+9*b.data.shape[0]))
                    self.vars.append(w)
                    # self.vars.append(b)
                else:     
                    # [ch_out, ch_in]
                    w = nn.Parameter(torch.ones(*param))
                    # gain=1 according to cbfinn's implementation
                    torch.nn.init.kaiming_normal_(w)
                    self.vars.append(w)
                # [ch_out]
                self.vars.append(nn.Parameter(torch.zeros(param[0])))

            elif name is 'cat':
                pass
            elif name is 'cat_start':
                pass
            elif name is "rep":
                pass
            elif name in ["residual3", "residual5", "in"]:
                pass
            elif name is 'bn':
                # [ch_out]
                w = nn.Parameter(torch.ones(param[0]))
                self.vars.append(w)
                # [ch_out]
                self.vars.append(nn.Parameter(torch.zeros(param[0])))

                # must set requires_grad=False
                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)
                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)
                self.vars_bn.extend([running_mean, running_var])


            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',
                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:
                continue
            else:
                raise NotImplementedError

    def extra_repr(self):

        info = ''

        for name, param, extra_name in self.config:
            if name is 'conv2d':
                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)' \
                      % (param[1], param[0], param[2], param[3], param[4], param[5],)
                info += tmp + '\n'

            elif name is 'convt2d':
                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)' \
                      % (param[0], param[1], param[2], param[3], param[4], param[5],)
                info += tmp + '\n'

            elif name is 'linear':
                tmp = 'linear:(in:%d, out:%d)' % (param[1], param[0])
                info += tmp + '\n'

            elif name is 'leakyrelu':
                tmp = 'leakyrelu:(slope:%f)' % (param[0])
                info += tmp + '\n'

            elif name is 'cat':
                tmp = 'cat'
                info += tmp + "\n"
            elif name is 'cat_start':
                tmp = 'cat_start'
                info += tmp + "\n"

            elif name is 'rep':
                tmp = 'rep'
                info += tmp + "\n"


            elif name is 'avg_pool2d':
                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)' % (param[0], param[1], param[2])
                info += tmp + '\n'
            elif name is 'max_pool2d':
                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)' % (param[0], param[1], param[2])
                info += tmp + '\n'
            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:
                tmp = name + ':' + str(tuple(param))
                info += tmp + '\n'
            else:
                raise NotImplementedError

        return info

    def forward(self, x, vars=None, bn_training=False, feature=False):
        """
        This function can be called by finetunning, however, in finetunning, we dont wish to update
        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.
        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False
        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.
        :param x: [b, 1, 28, 28]
        :param vars:
        :param bn_training: set False to not update
        :return: x, loss, likelihood, kld
        """

        cat_var = False
        cat_list = []

        if vars is None:
            vars = self.vars

        idx = 0
        bn_idx = 0

        try:

            for (name, param, extra_name) in self.config:
                # assert(name == "conv2d")
                if name == 'conv2d':
                    w, b = vars[idx], vars[idx + 1]
                    x = F.conv2d(x, w, b, stride=param[4], padding=param[5])
                    idx += 2

                    # print(name, param, '\tout:', x.shape)
                elif name == 'convt2d':
                    w, b = vars[idx], vars[idx + 1]
                    x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])
                    idx += 2


                elif name == 'linear':

                    # ipdb.set_trace()
                    if extra_name == 'cosine':
                        w = F.normalize(vars[idx])
                        x = F.normalize(x)
                        x = F.linear(x, w)
                        idx += 1
                    else:
                        w, b = vars[idx], vars[idx + 1]
                        x = F.linear(x, w, b)
                        idx += 2

                    if cat_var:
                        cat_list.append(x)

                elif name == 'rep':
                    # print('rep')
                    # print(x.shape)
                    if feature:
                        return x

                elif name == "cat_start":
                    cat_var = True
                    cat_list = []

                elif name == "cat":
                    cat_var = False
                    x = torch.cat(cat_list, dim=1)

                elif name == 'bn':
                    w, b = vars[idx], vars[idx + 1]
                    running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx + 1]
                    x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)
                    idx += 2
                    bn_idx += 2
                elif name == 'flatten':
                    # print('flatten')
                    # print(x.shape)

                    x = x.view(x.size(0), -1)

                elif name == 'reshape':
                    # [b, 8] => [b, 2, 2, 2]
                    x = x.view(x.size(0), *param)
                elif name == 'relu':
                    x = F.relu(x, inplace=param[0])
                elif name == 'leakyrelu':
                    x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])
                elif name == 'tanh':
                    x = F.tanh(x)
                elif name == 'sigmoid':
                    x = torch.sigmoid(x)
                elif name == 'upsample':
                    x = F.upsample_nearest(x, scale_factor=param[0])
                elif name == 'max_pool2d':
                    x = F.max_pool2d(x, param[0], param[1], param[2])
                elif name == 'avg_pool2d':
                    x = F.avg_pool2d(x, param[0], param[1], param[2])

                else:
                    print(name)
                    raise NotImplementedError

        except:
            traceback.print_exc(file=sys.stdout)
            ipdb.set_trace()

        # make sure variable is used properly
        assert idx == len(vars)
        assert bn_idx == len(self.vars_bn)

        return x


    def zero_grad(self, vars=None):
        """

        :param vars:
        :return:
        """
        with torch.no_grad():
            if vars is None:
                for p in self.vars:
                    if p.grad is not None:
                        p.grad.zero_()
            else:
                for p in vars:
                    if p.grad is not None:
                        p.grad.zero_()

    def define_task_lr_params(self, alpha_init=1e-3): 
        # Setup learning parameters
        self.alpha_lr = nn.ParameterList([])

        self.lr_name = []
        for n, p in self.named_parameters():
            self.lr_name.append(n)

        for p in self.parameters():
            self.alpha_lr.append(nn.Parameter(alpha_init * torch.ones(p.shape, requires_grad=True)))                                           

    def parameters(self):
        """
        override this function since initial parameters will return with a generator.
        :return:
        """
        return self.vars



File Path: model/meta/modelfactory.py
Content:
import ipdb

class ModelFactory():
    def __init__(self):
        pass

    @staticmethod
    def get_model(model_type, sizes, dataset='mnist', args=None):

        net_list = []
        if "mnist" in dataset:
            if model_type=="linear":
                for i in range(0, len(sizes) - 1):
                    net_list.append(('linear', [sizes[i+1], sizes[i]], ''))
                    if i < (len(sizes) - 2):
                        net_list.append(('relu', [True], ''))
                    if i == (len(sizes) - 2):
                        net_list.append(('rep', [], ''))
                return net_list

        elif dataset == "tinyimagenet":

            if model_type == 'pc_cnn':
                channels = 160
                return [
                    ('conv2d', [channels, 3, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),

                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),

                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),

                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),

                    ('flatten', [], ''),
                    ('rep', [], ''),

                    ('linear', [640, 16 * channels], ''),
                    ('relu', [True], ''),

                    ('linear', [640, 640], ''),
                    ('relu', [True], ''),
                    ('linear', [sizes[-1], 640], '')
                ]

        elif dataset == "cifar100":


            if model_type == 'pc_cnn':
                channels = 160
                return [
                    ('conv2d', [channels, 3, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),
                    
                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),

                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),
                    ('relu', [True], ''),

                    ('flatten', [], ''),
                    ('rep', [], ''),

                    ('linear', [320, 16 * channels], ''),
                    ('relu', [True], ''),

                    ('linear', [320, 320], ''),
                    ('relu', [True], ''),
                    ('linear', [sizes[-1], 320], '')
                ]

        else:
            print("Unsupported model; either implement the model in model/ModelFactory or choose a different model")
            assert (False)



 
File Path: model/optimizers_lib/__init__.py
Content:
from .optimizers_lib import *
File Path: model/optimizers_lib/bgd_optimizer.py
Content:
import torch
from torch.optim.optimizer import Optimizer

class BGD(Optimizer):
    """Implements BGD.
    A simple usage of BGD would be:
    for samples, labels in batches:
        for mc_iter in range(mc_iters):
            optimizer.randomize_weights()
            output = model.forward(samples)
            loss = cirterion(output, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.aggregate_grads()
        optimizer.step()
    """
    def __init__(self, params, std_init, mean_eta=1, mc_iters=10):
        """
        Initialization of BGD optimizer
        group["mean_param"] is the learned mean.
        group["std_param"] is the learned STD.
        :param params: List of model parameters
        :param std_init: Initialization value for STD parameter
        :param mean_eta: Eta value
        :param mc_iters: Number of Monte Carlo iteration. Used for correctness check.
                         Use None to disable the check.
        """
        super(BGD, self).__init__(params, defaults={})
        assert mc_iters is None or (type(mc_iters) == int and mc_iters > 0), "mc_iters should be positive int or None."
        self.std_init = std_init
        self.mean_eta = mean_eta
        self.mc_iters = mc_iters
        # Initialize mu (mean_param) and sigma (std_param)
        for group in self.param_groups:
            assert len(group["params"]) == 1, "BGD optimizer does not support multiple params in a group"
            # group['params'][0] is the weights
            assert isinstance(group["params"][0], torch.Tensor), "BGD expect param to be a tensor"
            # We use the initialization of weights to initialize the mean.
            group["mean_param"] = group["params"][0].data.clone()
            group["std_param"] = torch.zeros_like(group["params"][0].data).add_(self.std_init)
        self._init_accumulators()

    def get_mc_iters(self):
        return self.mc_iters

    def _init_accumulators(self):
        self.mc_iters_taken = 0
        for group in self.param_groups:
            group["eps"] = None
            group["grad_mul_eps_sum"] = torch.zeros_like(group["params"][0].data).cuda()
            group["grad_sum"] = torch.zeros_like(group["params"][0].data).cuda()

    def randomize_weights(self, force_std=-1):
        """
        Randomize the weights according to N(mean, std).
        :param force_std: If force_std>=0 then force_std is used for STD instead of the learned STD.
        :return: None
        """
        for group in self.param_groups:
            mean = group["mean_param"]
            std = group["std_param"]
            if force_std >= 0:
                std = std.mul(0).add(force_std)
            group["eps"] = torch.normal(torch.zeros_like(mean), 1).cuda()
            # Reparameterization trick (here we set the weights to their randomized value):
            group["params"][0].data.copy_(mean.add(std.mul(group["eps"])))

    def aggregate_grads(self, batch_size):
        """
        Aggregates a single Monte Carlo iteration gradients. Used in step() for the expectations calculations.
        optimizer.zero_grad() should be used before calling .backward() once again.
        :param batch_size: BGD is using non-normalized gradients, but PyTorch gives normalized gradients.
                            Therefore, we multiply the gradients by the batch size.
        :return: None
        """
        self.mc_iters_taken += 1
        groups_cnt = 0
        for group in self.param_groups:
            if group["params"][0].grad is None:
                continue
            assert group["eps"] is not None, "Must randomize weights before using aggregate_grads"
            groups_cnt += 1
            grad = group["params"][0].grad.data.mul(batch_size)
            group["grad_sum"].add_(grad)
            group["grad_mul_eps_sum"].add_(grad.mul(group["eps"]))
            group["eps"] = None
        assert groups_cnt > 0, "Called aggregate_grads, but all gradients were None. Make sure you called .backward()"

    def step(self, closure=None, print_std = False):
        """
        Updates the learned mean and STD.
        :return:
        """
        # Makes sure that self.mc_iters had been taken.
        assert self.mc_iters is None or self.mc_iters == self.mc_iters_taken, "MC iters is set to " \
                                                                              + str(self.mc_iters) \
                                                                              + ", but took " + \
                                                                              str(self.mc_iters_taken) + " MC iters"

        for group in self.param_groups:
            mean = group["mean_param"]
            std = group["std_param"]
            
            # Divide gradients by MC iters to get expectation
            e_grad = group["grad_sum"].div(self.mc_iters_taken)
            e_grad_eps = group["grad_mul_eps_sum"].div(self.mc_iters_taken)
            # Update mean and STD params
            mean.add_(-std.pow(2).mul(e_grad).mul(self.mean_eta))
            sqrt_term = torch.sqrt(e_grad_eps.mul(std).div(2).pow(2).add(1)).mul(std)
            std.copy_(sqrt_term.add(-e_grad_eps.mul(std.pow(2)).div(2)))
        self.randomize_weights(force_std=0)
        self._init_accumulators()
File Path: model/optimizers_lib/optimizers_lib.py
Content:
import torch.optim as optim
from .bgd_optimizer import BGD


def bgd(model, **kwargs):
    # logger = kwargs.get("logger", None)
    # assert(logger is not None)
    bgd_params = {
        "mean_eta": kwargs.get("mean_eta", 1),
        "std_init": kwargs.get("std_init", 0.02),
        "mc_iters": kwargs.get("mc_iters", 10)
    }
    # logger.info("BGD params: " + str(bgd_params))
    all_params = [{'params': params} for l, (name, params) in enumerate(model.named_parameters())]
    return BGD(all_params, **bgd_params)


def sgd(model, **kwargs):
    # logger = kwargs.get("logger", None)
    # assert(logger is not None)
    sgd_params = {
        "momentum": kwargs.get("momentum", 0.9),
        "lr": kwargs.get("lr", 0.1),
        "weight_decay": kwargs.get("weight_decay", 5e-4)
    }
    # logger.info("SGD params: " + str(sgd_params))
    all_params = [{'params': params, 'name': name, 'initial_lr': kwargs.get("lr", 0.1)} for l, (name, params) in enumerate(model.named_parameters())]
    return optim.SGD(all_params, **sgd_params)


def adam(model, **kwargs):
    # logger = kwargs.get("logger", None)
    # assert(logger is not None)
    adam_params = {
        "eps": kwargs.get("eps", 1e-08),
        "lr": kwargs.get("lr", 0.001),
        "betas": kwargs.get("betas", (0.9, 0.999)),
        "weight_decay": kwargs.get("weight_decay", 0)
    }
    # logger.info("ADAM params: " + str(adam_params))
    all_params = [{'params': params, 'name': name, 'initial_lr': kwargs.get("lr", 0.001)} for l, (name, params) in enumerate(model.named_parameters())]
    return optim.Adam(all_params, **adam_params)


def adagrad(model, **kwargs):
    # logger = kwargs.get("logger", None)
    # assert(logger is not None)
    adam_params = {
        "lr": kwargs.get("lr", 0.01),
        "weight_decay": kwargs.get("weight_decay", 0)
    }
    # logger.info("Adagrad params: " + str(adam_params))
    all_params = [{'params': params, 'name': name, 'initial_lr': kwargs.get("lr", 0.01)} for l, (name, params) in enumerate(model.named_parameters())]
    return optim.Adagrad(all_params, **adam_params)

File Path: parser.py
Content:
# coding=utf-8
import os
import argparse

def get_parser():
    parser = argparse.ArgumentParser(description='Continual learning')
    parser.add_argument('--expt_name', type=str, default='test_lamaml',
                    help='name of the experiment')
    
    # model details
    parser.add_argument('--model', type=str, default='single',
                        help='algo to train')
    parser.add_argument('--arch', type=str, default='linear', 
                        help='arch to use for training', choices = ['linear', 'pc_cnn'])
    parser.add_argument('--n_hiddens', type=int, default=100,
                        help='number of hidden neurons at each layer')
    parser.add_argument('--n_layers', type=int, default=2,
                        help='number of hidden layers')
    parser.add_argument('--xav_init', default=False , action='store_true',
                        help='Use xavier initialization')



    # optimizer parameters influencing all models
    parser.add_argument("--glances", default=1, type=int,
                        help="Number of times the model is allowed to train over a set of samples in the single pass setting") 
    parser.add_argument('--n_epochs', type=int, default=1,
                        help='Number of epochs per task')
    parser.add_argument('--batch_size', type=int, default=1,
                        help='the amount of items received by the algorithm at one time (set to 1 across all ' +
                        'experiments). Variable name is from GEM project.')
    parser.add_argument('--replay_batch_size', type=float, default=20,
                        help='The batch size for experience replay.')
    parser.add_argument('--memories', type=int, default=5120, 
                        help='number of total memories stored in a reservoir sampling based buffer')
    parser.add_argument('--lr', type=float, default=1e-3,
                        help='learning rate (For baselines)')

    
    # experiment parameters
    parser.add_argument('--cuda', default=False , action='store_true',
                        help='Use GPU')
    parser.add_argument('--seed', type=int, default=0,
                        help='random seed of model')
    parser.add_argument('--log_every', type=int, default=1000,
                        help='frequency of checking the validation accuracy, in minibatches')
    parser.add_argument('--log_dir', type=str, default='logs/',
                        help='the directory where the logs will be saved')
    parser.add_argument('--tf_dir', type=str, default='',
                        help='(not set by user)')
    parser.add_argument('--calc_test_accuracy', default=False , action='store_true',
                        help='Calculate test accuracy along with val accuracy')

    # data parameters
    parser.add_argument('--data_path', default='data/',
                        help='path where data is located')
    parser.add_argument('--loader', type=str, default='task_incremental_loader',
                        help='data loader to use')
    parser.add_argument('--samples_per_task', type=int, default=-1,
                        help='training samples per task (all if negative)')
    parser.add_argument('--shuffle_tasks', default=False, action='store_true',
                        help='present tasks in order')
    parser.add_argument('--classes_per_it', type=int, default=4,
                        help='number of classes in every batch')
    parser.add_argument('--iterations', type=int, default=5000,
                        help='number of classes in every batch')
    parser.add_argument("--dataset", default="mnist_rotations", type=str,
                    help="Dataset to train and test on.")
    parser.add_argument("--workers", default=3, type=int,
                        help="Number of workers preprocessing the data.")
    parser.add_argument("--validation", default=0., type=float,
                        help="Validation split (0. <= x <= 1.).")
    parser.add_argument("-order", "--class_order", default="old", type=str,
                        help="define classes order of increment ",
                        choices = ["random", "chrono", "old", "super"])
    parser.add_argument("-inc", "--increment", default=5, type=int,
                        help="number of classes to increment by in class incremental loader")
    parser.add_argument('--test_batch_size', type=int, default=100000 ,
                        help='batch size to use during testing.')


    # La-MAML parameters
    parser.add_argument('--opt_lr', type=float, default=1e-1,
                        help='learning rate for LRs')
    parser.add_argument('--opt_wt', type=float, default=1e-1,
                        help='learning rate for weights')
    parser.add_argument('--alpha_init', type=float, default=1e-3,
                        help='initialization for the LRs')
    parser.add_argument('--learn_lr', default=False, action='store_true',
                        help='model should update the LRs during learning')
    parser.add_argument('--sync_update', default=False , action='store_true',
                        help='the LRs and weights should be updated synchronously')

    parser.add_argument('--grad_clip_norm', type=float, default=2.0,
                        help='Clip the gradients by this value')
    parser.add_argument("--cifar_batches", default=3, type=int,
                        help="Number of batches in inner trajectory") 
    parser.add_argument('--use_old_task_memory', default=False, action='store_true', 
                        help='Use only old task samples for replay buffer data')    
    parser.add_argument('--second_order', default=False , action='store_true',
                        help='use second order MAML updates')


   # memory parameters for GEM | AGEM | ICARL 
    parser.add_argument('--n_memories', type=int, default=0,
                        help='number of memories per task')
    parser.add_argument('--memory_strength', default=0, type=float,
                        help='memory strength (meaning depends on memory)')
    parser.add_argument('--steps_per_sample', default=1, type=int,
                        help='training steps per batch')


    # parameters specific to MER 
    parser.add_argument('--gamma', type=float, default=1.0,
                        help='gamma learning rate parameter')
    parser.add_argument('--beta', type=float, default=1.0,
                        help='beta learning rate parameter')
    parser.add_argument('--s', type=float, default=1,
                        help='current example learning rate multiplier (s)')
    parser.add_argument('--batches_per_example', type=float, default=1,
                        help='the number of batch per incoming example')


    # parameters specific to Meta-BGD
    parser.add_argument('--bgd_optimizer', type=str, default="bgd", choices=["adam", "adagrad", "bgd", "sgd"],
                    help='Optimizer.')
    parser.add_argument('--optimizer_params', default="{}", type=str, nargs='*',
                        help='Optimizer parameters')

    parser.add_argument('--train_mc_iters', default=5, type=int,
                        help='Number of MonteCarlo samples during training(default 10)')
    parser.add_argument('--std_init', default=5e-2, type=float,
                        help='STD init value (default 5e-2)')
    parser.add_argument('--mean_eta', default=1, type=float,
                        help='Eta for mean step (default 1)')
    parser.add_argument('--fisher_gamma', default=0.95, type=float,
                        help='')

    return parser
File Path: utils/misc_utils.py
Content:
import datetime
import glob
import json
import os
import random
import ipdb
import numpy as np
import torch
from tqdm import tqdm



def to_onehot(targets, n_classes):
    onehot = torch.zeros(targets.shape[0], n_classes).to(targets.device)
    onehot.scatter_(dim=1, index=targets.long().view(-1, 1), value=1.)
    return onehot

def _check_loss(loss):
    return not bool(torch.isnan(loss).item()) and bool((loss >= 0.).item())

def compute_accuracy(ypred, ytrue, task_size=10):
    all_acc = {}

    all_acc["total"] = round((ypred == ytrue).sum() / len(ytrue), 3)

    for class_id in range(0, np.max(ytrue), task_size):
        idxes = np.where(
                np.logical_and(ytrue >= class_id, ytrue < class_id + task_size)
        )[0]

        label = "{}-{}".format(
                str(class_id).rjust(2, "0"),
                str(class_id + task_size - 1).rjust(2, "0")
        )
        all_acc[label] = round((ypred[idxes] == ytrue[idxes]).sum() / len(idxes), 3)

    return all_acc


def get_date():
    return datetime.datetime.now().strftime("%Y%m%d")


def get_date_time():
    return datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')[:-2]


def log_dir(opt, timestamp=None):
    if timestamp is None:
        timestamp = get_date_time()

    rand_num = str(random.randint(1,1001))
    logdir = opt.log_dir + '/%s/%s-%s/%s' % (opt.model, opt.expt_name, timestamp, opt.seed)
    tfdir = opt.log_dir +  '/%s/%s-%s/%s/%s' % (opt.model, opt.expt_name, timestamp, opt.seed, "tfdir")

    mkdir(logdir)
    mkdir(tfdir)
    
    with open(logdir + '/training_parameters.json', 'w') as f:
        json.dump(vars(opt), f, indent=4)
    
    return logdir, tfdir


def save_list_to_file(path, thelist):
    with open(path, 'w') as f:
        for item in thelist:
            f.write("%s\n" % item)


def find_latest_checkpoint(folder_path):
    print('searching for checkpoint in : '+folder_path)
    files = sorted(glob.iglob(folder_path+'/*.pth'), key=os.path.getmtime, reverse=True)
    print('latest checkpoint is:')
    print(files[0])
    return files[0]


def init_seed(seed):
    '''
    Disable cudnn to maximize reproducibility
    '''
    print("Set seed", seed)
    random.seed(seed)
    torch.cuda.cudnn_enabled = False
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

    torch.backends.cudnn.enabled = False


def find_latest_checkpoint_name(folder_path):
    print('searching for checkpoint in : '+folder_path)
    files = glob.glob(folder_path+'/*.pth')
    min_num = 0
    filename = ''
    for i, filei in enumerate(files):
        ckpt_name = os.path.splitext(filei) 
        ckpt_num = int(ckpt_name.split('_')[-1])
        if(ckpt_num>min_num):
            min_num = ckpt_num
            filename = filei
    print('latest checkpoint is:')
    print(filename)
    return filename


def mkdir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def mkdirs(paths):
    if isinstance(paths, list) and not isinstance(paths, str):
        for path in paths:
            mkdir(path)
    else:
        mkdir(paths)

def to_numpy(input):
    if isinstance(input, torch.Tensor):
        return input.cpu().numpy()
    elif isinstance(input, np.ndarray):
        return input
    else:
        raise TypeError('Unknown type of input, expected torch.Tensor or '\
            'np.ndarray, but got {}'.format(type(input)))


def log_sum_exp(input, dim=None, keepdim=False):
    """Numerically stable LogSumExp.

    Args:
        input (Tensor)
        dim (int): Dimension along with the sum is performed
        keepdim (bool): Whether to retain the last dimension on summing

    Returns:
        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).
    """
    # For a 1-D array x (any array along a single dimension),
    # log sum exp(x) = s + log sum exp(x - s)
    # with s = max(x) being a common choice.
    if dim is None:
        input = input.view(-1)
        dim = 0
    max_val = input.max(dim=dim, keepdim=True)[0]
    output = max_val + (input - max_val).exp().sum(dim=dim, keepdim=True).log()
    if not keepdim:
        output = output.squeeze(dim)
    return output
File Path: val_data_format.py
Content:
import io
import glob
import os
from shutil import move
from os.path import join
from os import listdir, rmdir

target_folder = './tiny-imagenet-200/val/'

val_dict = {}
with open('./tiny-imagenet-200/val/val_annotations.txt', 'r') as f:
    for line in f.readlines():
        split_line = line.split('\t')
        val_dict[split_line[0]] = split_line[1]
        
paths = glob.glob('./tiny-imagenet-200/val/images/*')
for path in paths:
    file = path.split('/')[-1]
    folder = val_dict[file]
    if not os.path.exists(target_folder + str(folder)):
        os.mkdir(target_folder + str(folder))
        os.mkdir(target_folder + str(folder) + '/images')
        
        
for path in paths:
    file = path.split('/')[-1]
    folder = val_dict[file]
    dest = target_folder + str(folder) + '/images/' + str(file)
    move(path, dest)
    
rmdir('./tiny-imagenet-200/val/images')

Output:
{
    "experimental_code": "# From model/meta/learner.py\nclass Learner(nn.Module):\n    def define_task_lr_params(self, alpha_init=1e-3): \n        self.alpha_lr = nn.ParameterList([])\n        for p in self.parameters():\n            self.alpha_lr.append(nn.Parameter(alpha_init * torch.ones(p.shape, requires_grad=True)))\n\n# From model/lamaml_base.py\nclass BaseNet(torch.nn.Module):\n    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n        super(BaseNet, self).__init__()\n        self.args = args\n        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [args.n_hiddens] * args.n_layers + [n_outputs],\n                                                dataset = args.dataset, args=args)\n        self.net = Learner.Learner(config, args)\n        self.net.define_task_lr_params(alpha_init = args.alpha_init)\n        self.opt_wt = torch.optim.SGD(list(self.net.parameters()), lr=args.opt_wt)     \n        self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr) \n        self.loss = torch.nn.CrossEntropyLoss()\n        self.memories = args.memories\n        self.M = [] # Main replay buffer\n        self.M_new = [] # Temporary buffer for current task\n        self.age = 0 # Counter for reservoir sampling\n        self.current_task = 0\n        # ... (other initializations)\n\n    def push_to_mem(self, batch_x, batch_y, t):\n        if self.real_epoch == 0 and self.pass_itr == 0: # Only push once per task's initial stream\n            batch_x, batch_y, t = batch_x.cpu(), batch_y.cpu(), t.cpu()\n            for i in range(batch_x.shape[0]):\n                self.age += 1\n                if len(self.M_new) < self.memories:\n                    self.M_new.append([batch_x[i], batch_y[i], t])\n                else:\n                    p = random.randint(0, self.age)  \n                    if p < self.memories: self.M_new[p] = [batch_x[i], batch_y[i], t]\n\n    def getBatch(self, x_current, y_current, t_current, batch_size=None):\n        if x_current is not None:\n            mxi = x_current.cpu().numpy()\n            myi = y_current.cpu().numpy()\n            mti = np.full(x_current.shape[0], t_current, dtype=int)\n        else:\n            # Placeholder for correct dimension handling if x_current can be None and needed for empty array shape\n            mxi = np.empty(shape=(0,))\n            myi = np.empty(shape=(0,))\n            mti = np.empty(shape=(0,))\n\n        bxs, bys, bts = [], [], []\n\n        MEM = self.M if self.args.use_old_task_memory and t_current > 0 else self.M_new\n        batch_size = self.batchSize if batch_size is None else self.batchSize\n\n        if len(MEM) > 0:\n            order = list(range(len(MEM)))\n            random.shuffle(order)\n            osize = min(batch_size, len(MEM))\n            for j in range(osize):\n                k = order[j]\n                x_mem, y_mem, t_mem = MEM[k]\n                bxs.append(x_mem.cpu().numpy() if isinstance(x_mem, torch.Tensor) else x_mem)\n                bys.append(y_mem.cpu().numpy() if isinstance(y_mem, torch.Tensor) else y_mem)\n                bts.append(t_mem.cpu().numpy() if isinstance(t_mem, torch.Tensor) else t_mem)\n\n        for j in range(len(myi)):\n            bxs.append(mxi[j])\n            bys.append(myi[j])\n            bts.append(mti[j])\n\n        bxs = Variable(torch.from_numpy(np.array(bxs))).float() \n        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)\n        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)\n        \n        if self.cuda:\n            bxs = bxs.cuda()\n            bys = bys.cuda()\n            bts = bts.cuda()\n        return bxs, bys, bts\n\n    def zero_grads(self):\n        if self.args.learn_lr: self.opt_lr.zero_grad()\n        self.opt_wt.zero_grad()\n        self.net.zero_grad()\n        self.net.alpha_lr.zero_grad()\n\n# From model/lamaml_cifar.py (adapts BaseNet for class-incremental tasks)\nclass Net(BaseNet):\n    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n        super(Net, self).__init__(n_inputs, n_outputs, n_tasks, args)\n        self.nc_per_task = n_outputs / n_tasks # Classes per task for offset calculation\n\n    def compute_offsets(self, task):\n        offset1 = task * self.nc_per_task\n        offset2 = (task + 1) * self.nc_per_task\n        return int(offset1), int(offset2)\n\n    def take_loss(self, t, logits, y):\n        offset1, offset2 = self.compute_offsets(t)\n        loss = self.loss(logits[:, offset1:offset2], y - offset1)\n        return loss\n\n    def take_multitask_loss(self, bt, t_current, logits, y):\n        loss = 0.0\n        for i, ti in enumerate(bt):\n            offset1, offset2 = self.compute_offsets(ti)\n            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0) - offset1)\n        return loss / len(bt)\n\n    def inner_update(self, x, fast_weights, y, t):\n        offset1, offset2 = self.compute_offsets(t)            \n        logits = self.net.forward(x, fast_weights)[:, :offset2] # Forward pass with current/fast weights\n        loss = self.take_loss(t, logits, y) # Calculate inner loss\n\n        if fast_weights is None: fast_weights = self.net.parameters()\n\n        graph_required = self.args.second_order\n        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))\n        \n        for i in range(len(grads)):\n            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)\n        \n        fast_weights = list(map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))\n        return fast_weights\n\n    def meta_loss(self, x, fast_weights, y, bt, t_current):\n        offset1, offset2 = self.compute_offsets(t_current)\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss_q = self.take_multitask_loss(bt, t_current, logits, y)\n        return loss_q, logits\n\n    def observe(self, x, y, t):\n        self.net.train() \n        for pass_itr in range(self.glances): # 'glances' for single-pass setting\n            self.pass_itr = pass_itr\n            perm = torch.randperm(x.size(0)); x = x[perm]; y = y[perm]\n            \n            self.epoch += 1; self.zero_grads()\n            if t != self.current_task: # If new task, update main memory from new task's samples\n                self.M = self.M_new.copy()\n                self.current_task = t\n\n            batch_sz = x.shape[0]; n_batches = self.args.cifar_batches # n_batches for inner trajectory steps\n            rough_sz = math.ceil(batch_sz/n_batches); fast_weights = None; meta_losses = [0 for _ in range(n_batches)]\n\n            bx, by, bt = self.getBatch(x, y, t)             \n\n            # Inner loop trajectory\n            for i in range(n_batches):\n                batch_x = x[i*rough_sz : (i+1)*rough_sz]; batch_y = y[i*rough_sz : (i+1)*rough_sz]\n                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)   \n                \n                if self.real_epoch == 0: self.push_to_mem(batch_x, batch_y, torch.tensor(t))\n                \n                meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) \n                meta_losses[i] += meta_loss\n\n            self.zero_grads() # Zero all gradients before meta-update\n            meta_loss = sum(meta_losses)/len(meta_losses); meta_loss.backward() # Backpropagate meta-loss\n            \n            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)\n            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)\n            \n            if self.args.learn_lr: self.opt_lr.step()\n\n            if self.args.sync_update:\n                self.opt_wt.step() # Synchronous update using opt_wt\n            else:            \n                # Asynchronous update: apply gradients using clipped (ReLU) learnable LRs as step sizes\n                for i,p in enumerate(self.net.parameters()):          \n                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])            \n            self.net.zero_grad(); self.net.alpha_lr.zero_grad() # Clear gradients\n        return meta_loss.item()",
    "experimental_info": "The La-MAML method is evaluated on diverse datasets including 'mnist_rotations', 'cifar100', and 'tinyimagenet'. Data loading is handled by 'task_incremental_loader' or 'class_incremental_loader'. For image datasets, specific transformations are applied, such as `RandomCrop`, `RandomHorizontalFlip`, `ColorJitter` (for CIFAR-100), `ToTensor`, and `Normalize` (with dataset-specific mean/std values).\n\nKey experimental settings are configured via command-line arguments:\n- **Model Architecture**: Defined by `--arch` (e.g., 'linear', 'pc_cnn'), `--n_hiddens` (default 100), and `--n_layers` (default 2). Xavier initialization (`--xav_init`) can be enabled.\n- **Optimization Parameters**:\n    - `glances`: Number of training iterations over a batch in the single-pass setting (default 1).\n    - `n_epochs`: Training epochs per task (default 1).\n    - `batch_size`: Size of the incoming data batch (default 1).\n    - `replay_batch_size`: Size of the batch sampled from the experience replay buffer (default 20).\n    - `memories`: Maximum capacity of the reservoir sampling-based replay buffer (default 5120 samples).\n    - `opt_lr`: Learning rate for optimizing the per-parameter learnable learning rates (default 1e-1).\n    - `opt_wt`: Learning rate for updating the network's weights, applicable when `--sync_update` is active (default 1e-1).\n    - `alpha_init`: Initial value for the learnable per-parameter learning rates (default 1e-3).\n    - `learn_lr`: A boolean flag (default False) to enable the optimization of per-parameter learning rates. It is essential for La-MAML.\n    - `sync_update`: A boolean flag (default False) to choose between synchronous (optimizer updates both LRs and weights) or asynchronous (weights are updated manually using ReLU-clipped learnable LRs) update mechanisms.\n    - `grad_clip_norm`: The maximum L2 norm for gradient clipping (default 2.0).\n    - `second_order`: A boolean flag (default False) to enable second-order MAML updates.\n    - `cifar_batches`: The number of mini-batches within the inner update trajectory for CIFAR-like datasets (default 3).\n    - `use_old_task_memory`: A boolean flag (default False) to determine if the replay buffer should store samples from all previously encountered tasks.\n- **Data Handling**:\n    - `data_path`: Specifies the base directory where datasets are stored.\n    - `samples_per_task`: The number of training samples considered per task (default -1, implying all available samples).\n    - `class_order`: Strategy for presenting classes within tasks ('random', 'chrono', 'old', 'super').\n    - `increment`: Number of classes added in each incremental task step for class-incremental learning (default 5).\n    - `validation`: Proportion of the training data allocated for validation (default 0.0).\n- **Evaluation**:\n    - `log_every`: Frequency (in minibatches) for logging and evaluating validation accuracy (default 1000).\n    - `calc_test_accuracy`: A boolean flag (default False) to enable calculation of test accuracy alongside validation accuracy.\n    - `test_batch_size`: Batch size used during model evaluation (default 100000)."
}
