
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
TENT optimizes the model during testing by minimizing the Shannon entropy of its predictions on target data. This is an unsupervised objective. The adaptation is achieved by modulating features within the model's normalization layers. Specifically, the method estimates normalization statistics (mean µ and standard deviation σ) batch-by-batch from the test data and optimizes channel-wise affine transformation parameters (scale γ and shift β) using the gradient of the prediction entropy. Other model parameters remain fixed. The algorithm involves an initialization step where affine parameters are collected and source normalization statistics are discarded, an iterative step where statistics are estimated during the forward pass and affine parameters are updated during the backward pass (one gradient per point for efficiency), and can operate in online or offline adaptation modes. The method requires the base model to be trained for a supervised task, probabilistic, and differentiable.

# Repository Content
File Path: cifar10c.py
Content:
import logging

import torch
import torch.optim as optim

from robustbench.data import load_cifar10c
from robustbench.model_zoo.enums import ThreatModel
from robustbench.utils import load_model
from robustbench.utils import clean_accuracy as accuracy

import tent
import norm

from conf import cfg, load_cfg_fom_args


logger = logging.getLogger(__name__)


def evaluate(description):
    load_cfg_fom_args(description)
    # configure model
    base_model = load_model(cfg.MODEL.ARCH, cfg.CKPT_DIR,
                       cfg.CORRUPTION.DATASET, ThreatModel.corruptions).cuda()
    if cfg.MODEL.ADAPTATION == "source":
        logger.info("test-time adaptation: NONE")
        model = setup_source(base_model)
    if cfg.MODEL.ADAPTATION == "norm":
        logger.info("test-time adaptation: NORM")
        model = setup_norm(base_model)
    if cfg.MODEL.ADAPTATION == "tent":
        logger.info("test-time adaptation: TENT")
        model = setup_tent(base_model)
    # evaluate on each severity and type of corruption in turn
    for severity in cfg.CORRUPTION.SEVERITY:
        for corruption_type in cfg.CORRUPTION.TYPE:
            # reset adaptation for each combination of corruption x severity
            # note: for evaluation protocol, but not necessarily needed
            try:
                model.reset()
                logger.info("resetting model")
            except:
                logger.warning("not resetting model")
            x_test, y_test = load_cifar10c(cfg.CORRUPTION.NUM_EX,
                                           severity, cfg.DATA_DIR, False,
                                           [corruption_type])
            x_test, y_test = x_test.cuda(), y_test.cuda()
            acc = accuracy(model, x_test, y_test, cfg.TEST.BATCH_SIZE)
            err = 1. - acc
            logger.info(f"error % [{corruption_type}{severity}]: {err:.2%}")


def setup_source(model):
    """Set up the baseline source model without adaptation."""
    model.eval()
    logger.info(f"model for evaluation: %s", model)
    return model


def setup_norm(model):
    """Set up test-time normalization adaptation.

    Adapt by normalizing features with test batch statistics.
    The statistics are measured independently for each batch;
    no running average or other cross-batch estimation is used.
    """
    norm_model = norm.Norm(model)
    logger.info(f"model for adaptation: %s", model)
    stats, stat_names = norm.collect_stats(model)
    logger.info(f"stats for adaptation: %s", stat_names)
    return norm_model


def setup_tent(model):
    """Set up tent adaptation.

    Configure the model for training + feature modulation by batch statistics,
    collect the parameters for feature modulation by gradient optimization,
    set up the optimizer, and then tent the model.
    """
    model = tent.configure_model(model)
    params, param_names = tent.collect_params(model)
    optimizer = setup_optimizer(params)
    tent_model = tent.Tent(model, optimizer,
                           steps=cfg.OPTIM.STEPS,
                           episodic=cfg.MODEL.EPISODIC)
    logger.info(f"model for adaptation: %s", model)
    logger.info(f"params for adaptation: %s", param_names)
    logger.info(f"optimizer for adaptation: %s", optimizer)
    return tent_model


def setup_optimizer(params):
    """Set up optimizer for tent adaptation.

    Tent needs an optimizer for test-time entropy minimization.
    In principle, tent could make use of any gradient optimizer.
    In practice, we advise choosing Adam or SGD+momentum.
    For optimization settings, we advise to use the settings from the end of
    trainig, if known, or start with a low learning rate (like 0.001) if not.

    For best results, try tuning the learning rate and batch size.
    """
    if cfg.OPTIM.METHOD == 'Adam':
        return optim.Adam(params,
                    lr=cfg.OPTIM.LR,
                    betas=(cfg.OPTIM.BETA, 0.999),
                    weight_decay=cfg.OPTIM.WD)
    elif cfg.OPTIM.METHOD == 'SGD':
        return optim.SGD(params,
                   lr=cfg.OPTIM.LR,
                   momentum=cfg.OPTIM.MOMENTUM,
                   dampening=cfg.OPTIM.DAMPENING,
                   weight_decay=cfg.OPTIM.WD,
                   nesterov=cfg.OPTIM.NESTEROV)
    else:
        raise NotImplementedError


if __name__ == '__main__':
    evaluate('"CIFAR-10-C evaluation.')

File Path: conf.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

"""Configuration file (powered by YACS)."""

import argparse
import os
import sys
import logging
import random
import torch
import numpy as np
from datetime import datetime
from iopath.common.file_io import g_pathmgr
from yacs.config import CfgNode as CfgNode


# Global config object (example usage: from core.config import cfg)
_C = CfgNode()
cfg = _C


# ----------------------------- Model options ------------------------------- #
_C.MODEL = CfgNode()

# Check https://github.com/RobustBench/robustbench for available models
_C.MODEL.ARCH = 'Standard'

# Choice of (source, norm, tent)
# - source: baseline without adaptation
# - norm: test-time normalization
# - tent: test-time entropy minimization (ours)
_C.MODEL.ADAPTATION = 'source'

# By default tent is online, with updates persisting across batches.
# To make adaptation episodic, and reset the model for each batch, choose True.
_C.MODEL.EPISODIC = False

# ----------------------------- Corruption options -------------------------- #
_C.CORRUPTION = CfgNode()

# Dataset for evaluation
_C.CORRUPTION.DATASET = 'cifar10'

# Check https://github.com/hendrycks/robustness for corruption details
_C.CORRUPTION.TYPE = ['gaussian_noise', 'shot_noise', 'impulse_noise',
                      'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur',
                      'snow', 'frost', 'fog', 'brightness', 'contrast',
                      'elastic_transform', 'pixelate', 'jpeg_compression']
_C.CORRUPTION.SEVERITY = [5, 4, 3, 2, 1]

# Number of examples to evaluate (10000 for all samples in CIFAR-10)
_C.CORRUPTION.NUM_EX = 10000

# ------------------------------- Batch norm options ------------------------ #
_C.BN = CfgNode()

# BN epsilon
_C.BN.EPS = 1e-5

# BN momentum (BN momentum in PyTorch = 1 - BN momentum in Caffe2)
_C.BN.MOM = 0.1

# ------------------------------- Optimizer options ------------------------- #
_C.OPTIM = CfgNode()

# Number of updates per batch
_C.OPTIM.STEPS = 1

# Learning rate
_C.OPTIM.LR = 1e-3

# Choices: Adam, SGD
_C.OPTIM.METHOD = 'Adam'

# Beta
_C.OPTIM.BETA = 0.9

# Momentum
_C.OPTIM.MOMENTUM = 0.9

# Momentum dampening
_C.OPTIM.DAMPENING = 0.0

# Nesterov momentum
_C.OPTIM.NESTEROV = True

# L2 regularization
_C.OPTIM.WD = 0.0

# ------------------------------- Testing options --------------------------- #
_C.TEST = CfgNode()

# Batch size for evaluation (and updates for norm + tent)
_C.TEST.BATCH_SIZE = 128

# --------------------------------- CUDNN options --------------------------- #
_C.CUDNN = CfgNode()

# Benchmark to select fastest CUDNN algorithms (best for fixed input sizes)
_C.CUDNN.BENCHMARK = True

# ---------------------------------- Misc options --------------------------- #

# Optional description of a config
_C.DESC = ""

# Note that non-determinism is still present due to non-deterministic GPU ops
_C.RNG_SEED = 1

# Output directory
_C.SAVE_DIR = "./output"

# Data directory
_C.DATA_DIR = "./data"

# Weight directory
_C.CKPT_DIR = "./ckpt"

# Log destination (in SAVE_DIR)
_C.LOG_DEST = "log.txt"

# Log datetime
_C.LOG_TIME = ''

# # Config destination (in SAVE_DIR)
# _C.CFG_DEST = "cfg.yaml"

# --------------------------------- Default config -------------------------- #
_CFG_DEFAULT = _C.clone()
_CFG_DEFAULT.freeze()


def assert_and_infer_cfg():
    """Checks config values invariants."""
    err_str = "Unknown adaptation method."
    assert _C.MODEL.ADAPTATION in ["source", "norm", "tent"]
    err_str = "Log destination '{}' not supported"
    assert _C.LOG_DEST in ["stdout", "file"], err_str.format(_C.LOG_DEST)


def merge_from_file(cfg_file):
    with g_pathmgr.open(cfg_file, "r") as f:
        cfg = _C.load_cfg(f)
    _C.merge_from_other_cfg(cfg)


def dump_cfg():
    """Dumps the config to the output directory."""
    cfg_file = os.path.join(_C.SAVE_DIR, _C.CFG_DEST)
    with g_pathmgr.open(cfg_file, "w") as f:
        _C.dump(stream=f)


def load_cfg(out_dir, cfg_dest="config.yaml"):
    """Loads config from specified output directory."""
    cfg_file = os.path.join(out_dir, cfg_dest)
    merge_from_file(cfg_file)


def reset_cfg():
    """Reset config to initial state."""
    cfg.merge_from_other_cfg(_CFG_DEFAULT)


def load_cfg_fom_args(description="Config options."):
    """Load config from command line args and set any specified options."""
    current_time = datetime.now().strftime("%y%m%d_%H%M%S")
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument("--cfg", dest="cfg_file", type=str, required=True,
                        help="Config file location")
    parser.add_argument("opts", default=None, nargs=argparse.REMAINDER,
                        help="See conf.py for all options")
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)
    args = parser.parse_args()

    merge_from_file(args.cfg_file)
    cfg.merge_from_list(args.opts)

    log_dest = os.path.basename(args.cfg_file)
    log_dest = log_dest.replace('.yaml', '_{}.txt'.format(current_time))

    g_pathmgr.mkdirs(cfg.SAVE_DIR)
    cfg.LOG_TIME, cfg.LOG_DEST = current_time, log_dest
    cfg.freeze()

    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] [%(filename)s: %(lineno)4d]: %(message)s",
        datefmt="%y/%m/%d %H:%M:%S",
        handlers=[
            logging.FileHandler(os.path.join(cfg.SAVE_DIR, cfg.LOG_DEST)),
            logging.StreamHandler()
        ])

    np.random.seed(cfg.RNG_SEED)
    torch.manual_seed(cfg.RNG_SEED)
    random.seed(cfg.RNG_SEED)
    torch.backends.cudnn.benchmark = cfg.CUDNN.BENCHMARK

    logger = logging.getLogger(__name__)
    version = [torch.__version__, torch.version.cuda,
               torch.backends.cudnn.version()]
    logger.info(
        "PyTorch Version: torch={}, cuda={}, cudnn={}".format(*version))
    logger.info(cfg)

File Path: norm.py
Content:
from copy import deepcopy

import torch
import torch.nn as nn


class Norm(nn.Module):
    """Norm adapts a model by estimating feature statistics during testing.

    Once equipped with Norm, the model normalizes its features during testing
    with batch-wise statistics, just like batch norm does during training.
    """

    def __init__(self, model, eps=1e-5, momentum=0.1,
                 reset_stats=False, no_stats=False):
        super().__init__()
        self.model = model
        self.model = configure_model(model, eps, momentum, reset_stats,
                                     no_stats)
        self.model_state = deepcopy(self.model.state_dict())

    def forward(self, x):
        return self.model(x)

    def reset(self):
        self.model.load_state_dict(self.model_state, strict=True)


def collect_stats(model):
    """Collect the normalization stats from batch norms.

    Walk the model's modules and collect all batch normalization stats.
    Return the stats and their names.
    """
    stats = []
    names = []
    for nm, m in model.named_modules():
        if isinstance(m, nn.BatchNorm2d):
            state = m.state_dict()
            if m.affine:
                del state['weight'], state['bias']
            for ns, s in state.items():
                stats.append(s)
                names.append(f"{nm}.{ns}")
    return stats, names


def configure_model(model, eps, momentum, reset_stats, no_stats):
    """Configure model for adaptation by test-time normalization."""
    for m in model.modules():
        if isinstance(m, nn.BatchNorm2d):
            # use batch-wise statistics in forward
            m.train()
            # configure epsilon for stability, and momentum for updates
            m.eps = eps
            m.momentum = momentum
            if reset_stats:
                # reset state to estimate test stats without train stats
                m.reset_running_stats()
            if no_stats:
                # disable state entirely and use only batch stats
                m.track_running_stats = False
                m.running_mean = None
                m.running_var = None
    return model

File Path: tent.py
Content:
from copy import deepcopy

import torch
import torch.nn as nn
import torch.jit


class Tent(nn.Module):
    """Tent adapts a model by entropy minimization during testing.

    Once tented, a model adapts itself by updating on every forward.
    """
    def __init__(self, model, optimizer, steps=1, episodic=False):
        super().__init__()
        self.model = model
        self.optimizer = optimizer
        self.steps = steps
        assert steps > 0, "tent requires >= 1 step(s) to forward and update"
        self.episodic = episodic

        # note: if the model is never reset, like for continual adaptation,
        # then skipping the state copy would save memory
        self.model_state, self.optimizer_state = \
            copy_model_and_optimizer(self.model, self.optimizer)

    def forward(self, x):
        if self.episodic:
            self.reset()

        for _ in range(self.steps):
            outputs = forward_and_adapt(x, self.model, self.optimizer)

        return outputs

    def reset(self):
        if self.model_state is None or self.optimizer_state is None:
            raise Exception("cannot reset without saved model/optimizer state")
        load_model_and_optimizer(self.model, self.optimizer,
                                 self.model_state, self.optimizer_state)


@torch.jit.script
def softmax_entropy(x: torch.Tensor) -> torch.Tensor:
    """Entropy of softmax distribution from logits."""
    return -(x.softmax(1) * x.log_softmax(1)).sum(1)


@torch.enable_grad()  # ensure grads in possible no grad context for testing
def forward_and_adapt(x, model, optimizer):
    """Forward and adapt model on batch of data.

    Measure entropy of the model prediction, take gradients, and update params.
    """
    # forward
    outputs = model(x)
    # adapt
    loss = softmax_entropy(outputs).mean(0)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    return outputs


def collect_params(model):
    """Collect the affine scale + shift parameters from batch norms.

    Walk the model's modules and collect all batch normalization parameters.
    Return the parameters and their names.

    Note: other choices of parameterization are possible!
    """
    params = []
    names = []
    for nm, m in model.named_modules():
        if isinstance(m, nn.BatchNorm2d):
            for np, p in m.named_parameters():
                if np in ['weight', 'bias']:  # weight is scale, bias is shift
                    params.append(p)
                    names.append(f"{nm}.{np}")
    return params, names


def copy_model_and_optimizer(model, optimizer):
    """Copy the model and optimizer states for resetting after adaptation."""
    model_state = deepcopy(model.state_dict())
    optimizer_state = deepcopy(optimizer.state_dict())
    return model_state, optimizer_state


def load_model_and_optimizer(model, optimizer, model_state, optimizer_state):
    """Restore the model and optimizer states from copies."""
    model.load_state_dict(model_state, strict=True)
    optimizer.load_state_dict(optimizer_state)


def configure_model(model):
    """Configure model for use with tent."""
    # train mode, because tent optimizes the model to minimize entropy
    model.train()
    # disable grad, to (re-)enable only what tent updates
    model.requires_grad_(False)
    # configure norm for tent updates: enable grad + force batch statisics
    for m in model.modules():
        if isinstance(m, nn.BatchNorm2d):
            m.requires_grad_(True)
            # force use of batch stats in train and eval modes
            m.track_running_stats = False
            m.running_mean = None
            m.running_var = None
    return model


def check_model(model):
    """Check model for compatability with tent."""
    is_training = model.training
    assert is_training, "tent needs train mode: call model.train()"
    param_grads = [p.requires_grad for p in model.parameters()]
    has_any_params = any(param_grads)
    has_all_params = all(param_grads)
    assert has_any_params, "tent needs params to update: " \
                           "check which require grad"
    assert not has_all_params, "tent should not update all params: " \
                               "check which require grad"
    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])
    assert has_bn, "tent needs normalization for its optimization"

Output:
{
    "experimental_code": "File Path: tent.py\\nContent:\\nfrom copy import deepcopy\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.jit\\n\\n\\nclass Tent(nn.Module):\\n    \"\"\"Tent adapts a model by entropy minimization during testing.\\n\\n    Once tented, a model adapts itself by updating on every forward.\\n    \"\"\"\\n    def __init__(self, model, optimizer, steps=1, episodic=False):\\n        super().__init__()\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.steps = steps\\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\\n        self.episodic = episodic\\n\\n        # note: if the model is never reset, like for continual adaptation,\\n        # then skipping the state copy would save memory\\n        self.model_state, self.optimizer_state = \\\\n            copy_model_and_optimizer(self.model, self.optimizer)\\n\\n    def forward(self, x):\\n        if self.episodic:\\n            self.reset()\\n\\n        for _ in range(self.steps):\\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\\n\\n        return outputs\\n\\n    def reset(self):\\n        if self.model_state is None or self.optimizer_state is None:\\n            raise Exception(\"cannot reset without saved model/optimizer state\")\\n        load_model_and_optimizer(self.model, self.optimizer,\\n                                 self.model_state, self.optimizer_state)\\n\\n\\n@torch.jit.script\\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\\n\\n\\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\\ndef forward_and_adapt(x, model, optimizer):\\n    \"\"\"Forward and adapt model on batch of data.\\n\\n    Measure entropy of the model prediction, take gradients, and update params.\\n    \"\"\"\\n    # forward\\n    outputs = model(x)\\n    # adapt\\n    loss = softmax_entropy(outputs).mean(0)\\n    loss.backward()\\n    optimizer.step()\\n    optimizer.zero_grad()\\n    return outputs\\n\\n\\ndef collect_params(model):\\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\\n\\n    Walk the model's modules and collect all batch normalization parameters.\\n    Return the parameters and their names.\\n\\n    Note: other choices of parameterization are possible!\\n    \"\"\"\\n    params = []\\n    names = []\\n    for nm, m in model.named_modules():\\n        if isinstance(m, nn.BatchNorm2d):\\n            for np, p in m.named_parameters():\\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\\n                    params.append(p)\\n                    names.append(f\\\"{nm}.{np}\\\")\\n    return params, names\\n\\n\\ndef copy_model_and_optimizer(model, optimizer):\\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\\n    model_state = deepcopy(model.state_dict())\\n    optimizer_state = deepcopy(optimizer.state_dict())\\n    return model_state, optimizer_state\\n\\n\\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\\n    model.load_state_dict(model_state, strict=True)\\n    optimizer.load_state_dict(optimizer_state)\\n\\n\\ndef configure_model(model):\\n    \"\"\"Configure model for use with tent.\"\"\"\\n    # train mode, because tent optimizes the model to minimize entropy\\n    model.train()\\n    # disable grad, to (re-)enable only what tent updates\\n    model.requires_grad_(False)\\n    # configure norm for tent updates: enable grad + force batch statisics\\n    for m in model.modules():\\n        if isinstance(m, nn.BatchNorm2d):\\n            m.requires_grad_(True)\\n            # force use of batch stats in train and eval modes\\n            m.track_running_stats = False\\n            m.running_mean = None\\n            m.running_var = None\\n    return model\\n\\n\\ndef check_model(model):\\n    \"\"\"Check model for compatability with tent.\"\"\"\\n    is_training = model.training\\n    assert is_training, \"tent needs train mode: call model.train()\"\\n    param_grads = [p.requires_grad for p in model.parameters()]\\n    has_any_params = any(param_grads)\\n    has_all_params = all(param_grads)\\n    assert has_any_params, \"tent needs params to update: \" \\\\n                           \"check which require grad\"\\n    assert not has_all_params, \"tent should not update all params: \" \\\\n                               \"check which require grad\"\\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\\n    assert has_bn, \"tent needs normalization for its optimization\"\\n\\nFile Path: cifar10c.py\\nContent:\\nimport logging\\nimport torch.optim as optim\\nimport tent\\n\\nfrom conf import cfg, load_cfg_fom_args\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef setup_tent(model):\\n    \"\"\"Set up tent adaptation.\\n\\n    Configure the model for training + feature modulation by batch statistics,\\n    collect the parameters for feature modulation by gradient optimization,\\n    set up the optimizer, and then tent the model.\\n    \"\"\"\\n    model = tent.configure_model(model)\\n    params, param_names = tent.collect_params(model)\\n    optimizer = setup_optimizer(params)\\n    tent_model = tent.Tent(model, optimizer,\\n                           steps=cfg.OPTIM.STEPS,\\n                           episodic=cfg.MODEL.EPISODIC)\\n    logger.info(f\\\"model for adaptation: %s\\\", model)\\n    logger.info(f\\\"params for adaptation: %s\\\", param_names)\\n    logger.info(f\\\"optimizer for adaptation: %s\\\", optimizer)\\n    return tent_model\\n\\n\\ndef setup_optimizer(params):\\n    \"\"\"Set up optimizer for tent adaptation.\\n\\n    Tent needs an optimizer for test-time entropy minimization.\\n    In principle, tent could make use of any gradient optimizer.\\n    In practice, we advise choosing Adam or SGD+momentum.\\n    For optimization settings, we advise to use the settings from the end of\\n    trainig, if known, or start with a low learning rate (like 0.001) if not.\\n\\n    For best results, try tuning the learning rate and batch size.\\n    \"\"\"\\n    if cfg.OPTIM.METHOD == 'Adam':\\n        return optim.Adam(params,\\n                    lr=cfg.OPTIM.LR,\\n                    betas=(cfg.OPTIM.BETA, 0.999),\\n                    weight_decay=cfg.OPTIM.WD)\\n    elif cfg.OPTIM.METHOD == 'SGD':\\n        return optim.SGD(params,\\n                   lr=cfg.OPTIM.LR,\\n                   momentum=cfg.OPTIM.MOMENTUM,\\n                   dampening=cfg.OPTIM.DAMPENING,\\n                   weight_decay=cfg.OPTIM.WD,\\n                   nesterov=cfg.OPTIM.NESTEROV)\\n    else:\\n        raise NotImplementedError",
    "experimental_info": "Model Adaptation:\\n- Method: TENT (Test-time ENtropy Minimization)\\n- Base Model Architecture: 'Standard'\\n- Episodic Adaptation: False (updates persist across batches)\\n\\nOptimization Settings:\\n- Steps per batch (updates): 1\\n- Learning Rate (LR): 1e-3\\n- Optimizer Method: Adam (default)\\n  - Beta for Adam: 0.9\\n  - Weight Decay (L2 regularization): 0.0\\n- Alternative Optimizer: SGD\\n  - Momentum: 0.9\\n  - Dampening: 0.0\\n  - Nesterov: True\\n  - Weight Decay (L2 regularization): 0.0\\n\\nData and Evaluation Settings:\\n- Dataset: CIFAR-10-C\\n- Corruption Types: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']\\n- Corruption Severities: [5, 4, 3, 2, 1]\\n- Number of Examples for Evaluation: 10000 (all samples in CIFAR-10)\\n- Batch Size for Evaluation and Updates: 128\\n\\nMiscellaneous:\\n- Random Seed: 1\\n- CUDNN Benchmark: True"
}
