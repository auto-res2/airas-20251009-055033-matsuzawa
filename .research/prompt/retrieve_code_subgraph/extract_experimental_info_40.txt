
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
TNT approximates the natural gradient by using a block-diagonal Fisher matrix, with each block corresponding to the covariance of a tensor variable. It assumes that the sampling-based gradient for each tensor variable (Wl) follows a zero-mean Tensor-Normal (TN) distribution, resulting in a Kronecker-factored Fisher matrix (FW = U1 iguplus ... iguplus Uk). The method identifies unique covariance parameters (	ilda{U}i) by enforcing that the average eigenvalues of the covariance matrices for each tensor dimension are equal. The update direction is calculated by applying the inverse of the Kronecker-factored Fisher matrix to the gradient. For practical implementation, moving average estimates of relevant statistics are used, and inverse computations are amortized over multiple iterations.

# Repository Content
File Path: utils_git/jupyter_functions.py
Content:
from utils_git.utils import *
from utils_git.utils_plot import *

def train_model(home_path = '/home/jupyter/',
                dataset_name = 'CIFAR-10',
                algorithm = 'TNT',
                lr = 1e-4,
                damping_value = 0.01,
                weight_decay = 0,
               ):

    args = {}
    
    
    
    args['list_lr'] = [lr]
    
    args['weight_decay'] = weight_decay
    

    
    if dataset_name == 'CIFAR-10':
        args['dataset'] = 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'
        args['initialization_pkg'] = 'kaiming_normal'
    elif dataset_name == 'CIFAR-100':
        args['dataset'] = 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization'
        args['initialization_pkg'] = 'normal'
    elif dataset_name == 'MNIST':
        args['dataset'] = 'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization'
        args['initialization_pkg'] = 'normal'
    elif dataset_name == 'FACES':
        args['dataset'] = 'FacesMartens-autoencoder-relu-no-regularization'
        args['initialization_pkg'] = 'normal'
    else:
        print('dataset_name')
        print(dataset_name)
        sys.exit()
    
    
    
    
#     print('need to change by if lr decay')



    
    if dataset_name in ['MNIST']:
        
        # args['if_max_epoch'] = 1 # 0 means max_time
        args['if_max_epoch'] = 0 # 0 means max_time
    
    
    
        args['max_epoch/time'] = 500
        
    elif dataset_name in ['FACES']:
        
        # args['if_max_epoch'] = 1 # 0 means max_time
        args['if_max_epoch'] = 0 # 0 means max_time
    
    
    
        args['max_epoch/time'] = 2000
        
    elif dataset_name in ['CIFAR-10', 'CIFAR-100']:
        
        
        
        if algorithm in ['SGD-m', 'Adam']:
            
            args['if_max_epoch'] = 1 # 0 means max_time
#             args['if_max_epoch'] = 0 # 0 means max_time
    
    
    
            args['max_epoch/time'] = 200
            
            args['num_epoch_to_decay'] = 60
            args['lr_decay_rate'] = 0.1
            
        elif algorithm in ['TNT', 'Shampoo', 'KFAC']:
            
            args['if_max_epoch'] = 1 # 0 means max_time
#             args['if_max_epoch'] = 0 # 0 means max_time
    
    
    
            args['max_epoch/time'] = 100
            
            args['num_epoch_to_decay'] = 40
            args['lr_decay_rate'] = 0.1
            
        else:
            print('algorithm')
            print(algorithm)
        
            sys.exit()
        
    else:
        print('dataset_name')
        print(dataset_name)
    
        sys.exit()
        
    args['momentum_gradient_rho'] = 0.9
    
    if algorithm == 'SGD-m':
        
        args['momentum_gradient_dampening'] = 0
        
        if dataset_name in ['MNIST', 'FACES']:
            args['algorithm'] = 'SGD-momentum'
        elif dataset_name in ['CIFAR-10', 'CIFAR-100']:
            args['algorithm'] = 'SGD-LRdecay-momentum'
        else:
            print('dataset_name')
            print(dataset_name)
            sys.exit()
        
            
        
        
    elif algorithm == 'Adam':
        
        args['RMSprop_epsilon'] = damping_value
        
        args['RMSprop_beta_2'] = 0.999
        
        args['momentum_gradient_dampening'] = 0.9 # i.e. beta_1
        
        if dataset_name in ['CIFAR-10', 'CIFAR-100']:
            
            args['algorithm'] = 'Adam-noWarmStart-momentum-grad-LRdecay'
            
        elif dataset_name in ['MNIST', 'FACES']:
            args['algorithm'] = 'Adam-noWarmStart-momentum-grad'
        else:
            print('dataset_name')
            print(dataset_name)
            sys.exit()
            
            
        
    elif algorithm in ['TNT', 'Shampoo']:
        
        if algorithm in ['Shampoo']:
            args['shampoo_if_coupled_newton'] = True
        elif algorithm in ['TNT']:
            args['shampoo_if_coupled_newton'] = False
        
        args['shampoo_epsilon'] = damping_value
        
        args['if_Hessian_action'] = False
        
        args['shampoo_decay'] = 0.9
        args['shampoo_weight'] = 0.1
        
        args['momentum_gradient_dampening'] = 0
        
        if dataset_name in ['CIFAR-10', 'CIFAR-100']:
            
            if algorithm == 'TNT':
                args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'
            elif algorithm == 'Shampoo':
                args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'
            
            args['shampoo_update_freq'] = 10
            args['shampoo_inverse_freq'] = 100
            
        elif dataset_name in ['MNIST', 'FACES']:
            
            if algorithm == 'TNT':
                args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
            elif algorithm == 'Shampoo':
                args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'
            
            args['shampoo_update_freq'] = 1
            args['shampoo_inverse_freq'] = 20
            
        else:
            print('dataset_name')
            print(dataset_name)
            sys.exit()
            
        
    elif algorithm == 'KFAC':
        
        
        args['kfac_if_update_BN'] = True
        args['kfac_if_BN_grad_direction'] = True
        
        args['kfac_rho'] = 0.9
        args['kfac_damping_lambda'] = damping_value
        
        args['momentum_gradient_dampening'] = 0
        
        
        
        if dataset_name in ['FACES', 'MNIST']:
            
            args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad'
            
            args['kfac_if_svd'] = False
            
            args['kfac_cov_update_freq'] = 1
            args['kfac_inverse_update_freq'] = 20
            
        elif dataset_name in ['CIFAR-100', 'CIFAR-10']:
            
            args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay'

            args['kfac_if_svd'] = False

            args['kfac_cov_update_freq'] = 10
            args['kfac_inverse_update_freq'] = 100
            
#             args['num_epoch_to_decay'] = 40
#             args['lr_decay_rate'] = 0.1
#             print('could change to only compare dataset')
            
        else:
            print('dataset_name')
            print(dataset_name)
        
            sys.exit()
        
            


            
    else:
        print('algorithm')
        print(algorithm)
        sys.exit()
        

   
    
    
    


    
    
    args['record_epoch'] = 1

    args['seed_number'] = 9999

    args['num_threads'] = 8

    # args['initialization_pkg'] = 'numpy'
    # args['initialization_pkg'] = 'default'
#     args['initialization_pkg'] = 'normal'
#     print('need to change pkg')
    
    
    
    args['home_path'] = home_path

    

    args['if_gpu'] = True
    # args['if_gpu'] = False
    
    

    args['if_test_mode'] = False
    # args['if_test_mode'] = True
    
    

    args['if_auto_tune_lr'] = False
#     args['if_auto_tune_lr'] = True

    args['if_grafting'] = False

    _ = tune_lr(args)





    return


def plot_results(
    home_path = '/home/jupyter/',
    dataset_name = 'CIFAR-10',
    algorithms_jupyter = [],
):
    
    
    
    args = {}
    
    args['home_path'] = home_path
    
    

    args['if_gpu'] = True
        
    if dataset_name == 'CIFAR-10':
        name_dataset = 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'
#         args['initialization_pkg'] = 'kaiming_normal'
    elif dataset_name == 'CIFAR-100':
        name_dataset = 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization'
#         args['initialization_pkg'] = 'normal'
    elif dataset_name == 'MNIST':
        name_dataset = 'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization'
#         args['initialization_pkg'] = 'normal'
    elif dataset_name == 'FACES':
        name_dataset = 'FacesMartens-autoencoder-relu-no-regularization'
#         args['initialization_pkg'] = 'normal'
    else:
        print('dataset_name')
        print(dataset_name)
        sys.exit()



    name_dataset_legend = name_dataset
    
    

    algorithms = []
    
    for algorithm_jupyter in algorithms_jupyter:
        
        
        
#         print('need to add other algorithm')
        
        if algorithm_jupyter['name'] == 'TNT':
            
            
            
            algorithm = {}
            algorithm['params'] = {}
            
            if dataset_name in ['CIFAR-10', 'CIFAR-100']:
                algorithm['name'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'
                algorithm['params']['weight_decay'] = algorithm_jupyter['weight_decay']
            elif dataset_name in ['MNIST', 'FACES']:
                algorithm['name'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
                algorithm['params']['shampoo_epsilon'] = algorithm_jupyter['damping_value']
            
            
            
            
            
            algorithm['legend'] = 'TNT'
            algorithms.append(copy.deepcopy(algorithm))
            
        elif algorithm_jupyter['name'] == 'KFAC':
            
            algorithm = {}
            algorithm['params'] = {}
            
            if dataset_name in ['CIFAR-10', 'CIFAR-100']:
                algorithm['name'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay'
                algorithm['params']['weight_decay'] = algorithm_jupyter['weight_decay']
            elif dataset_name in ['MNIST', 'FACES']:
#                 algorithm['name'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
#                 algorithm['params']['shampoo_epsilon'] = algorithm_jupyter['damping_value']
            
                algorithm['name'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad'
                algorithm['params']['kfac_damping_lambda'] = algorithm_jupyter['damping_value']
            
            algorithm['legend'] = 'KFAC'
            algorithms.append(copy.deepcopy(algorithm))
            
            
        elif algorithm_jupyter['name'] == 'Shampoo':
            
            algorithm = {}
            algorithm['params'] = {}
            
            if dataset_name in ['CIFAR-10', 'CIFAR-100']:
                algorithm['name'] = 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'
                algorithm['params']['weight_decay'] = algorithm_jupyter['weight_decay']
            elif dataset_name in ['MNIST', 'FACES']:
#                 algorithm['name'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
#                 algorithm['params']['shampoo_epsilon'] = algorithm_jupyter['damping_value']
            
                algorithm['name'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'
            
                algorithm['params']['shampoo_epsilon'] = algorithm_jupyter['damping_value']
                
            algorithm['legend'] = 'Shampoo'
            algorithms.append(copy.deepcopy(algorithm))
            
        elif algorithm_jupyter['name'] == 'Adam':
            
            
            algorithm = {}
            algorithm['params'] = {}
            
            if dataset_name in ['CIFAR-10', 'CIFAR-100']:
                algorithm['name'] = 'Adam-noWarmStart-momentum-grad-LRdecay'
                algorithm['params']['weight_decay'] = algorithm_jupyter['weight_decay']
            elif dataset_name in ['MNIST', 'FACES']:
#                 algorithm['name'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
#                 algorithm['params']['shampoo_epsilon'] = algorithm_jupyter['damping_value']
            
                algorithm['name'] = 'Adam-noWarmStart-momentum-grad'
            
                algorithm['params']['RMSprop_epsilon'] = algorithm_jupyter['damping_value']
                
            algorithm['legend'] = 'Adam'

            algorithms.append(copy.deepcopy(algorithm))
            
            
            
        elif algorithm_jupyter['name'] == 'SGD-m':
            
            algorithm = {}
            algorithm['params'] = {}
            
            if dataset_name in ['CIFAR-10', 'CIFAR-100']:
                algorithm['name'] = 'SGD-LRdecay-momentum'
                algorithm['params']['weight_decay'] = algorithm_jupyter['weight_decay']
            elif dataset_name in ['MNIST', 'FACES']:
                algorithm['name'] = 'SGD-momentum'
                
            
            algorithm['legend'] = 'SGD-m'
            algorithms.append(copy.deepcopy(algorithm))
            
        else:
            
            print('algorithm_jupyter')
            print(algorithm_jupyter)

            sys.exit()




    
    
    if dataset_name in ['CIFAR-10', 'CIFAR-100']:
        
        args['tuning_criterion'] = 'test_acc'
        
        args['list_y'] = ['training unregularized minibatch loss',
                  'testing error']
        
        args['if_max_epoch'] = 1
        
    elif dataset_name in ['MNIST', 'FACES']:
        
        # args['tuning_criterion'] = 'test_acc'
        # args['tuning_criterion'] = 'train_loss'
        # args['tuning_criterion'] = 'train_acc'
        # args['tuning_criterion'] = 'train_minibatch_acc'
        args['tuning_criterion'] = 'train_minibatch_loss'


#     args['list_y'] = ['training unregularized minibatch loss',
#                   'testing error']
        args['list_y'] = ['training unregularized minibatch loss']
    
        # args['if_max_epoch'] = 1
        args['if_max_epoch'] = 0
        
    else:
        print('dataset_name')
        print(dataset_name)
    
        sys.exit()









    args['list_x'] = ['epoch', 'cpu time']


    



    args['x_scale'] = 'linear'
    # args['x_scale'] = 'log'

    args['if_lr_in_legend'] = True

    args['if_show_legend'] = True

    args['if_test_mode'] = False

    

    args['color'] = None
    
    args['if_title'] = False


    get_plot(name_dataset, name_dataset_legend, algorithms, args)




    
    
    return 

File Path: utils_git/utils.py
Content:
import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import numpy as np
import scipy
import copy
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

import time
import pickle


import os
import math
import psutil
import itertools
import datetime
import shutil

import torchvision.transforms as transforms
import torchvision.datasets as datasets

import warnings
warnings.filterwarnings('error')
# https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility
warnings.filterwarnings("ignore", message="numpy.ufunc size changed")
warnings.filterwarnings("ignore", message="numpy.dtype size changed")

# warnings.filterwarnings("ignore", category=UserWarning)

# from utils_git.utils_plot import *
# from utils_git.utils_shampoo import *
# from utils_git.utils_kbfgs import *

from utils_git.utils_get_params import get_params



os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

list_lr_complete = [
    1e-10, 3e-10, 1e-9, 3e-9, 1e-8, 3e-8, 1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000
]

def tune_lr_and_wd(args):
    
    logs = []
    
    for i, wd in enumerate(args['list_wd']):
        args['weight_decay'] = wd
    
    
        if i == 0:
            args['list_lr'] = args['list_lr_for_first_wd']
        else:
            
#             print('tune_lr_output')
#             print(tune_lr_output)
            
            assert wd > args['list_wd'][i-1]
            
            # wd goes up -> lr goes down
            
            best_lr_prev = tune_lr_output['best_lr']
            
            args['list_lr'] = [
                list_lr_complete[list_lr_complete.index(best_lr_prev) - 1],
                best_lr_prev
            ]
        
        tune_lr_output = tune_lr(args)
        
        logs.append(tune_lr_output)
        
        

        data_ = None
        torch.cuda.empty_cache()


    print('\n')
    for wd, log_wd in zip(args['list_wd'], logs):
        
        print('wd, list_lr_tried, best_lr')
        print(wd, log_wd['list_lr_tried'], log_wd['best_lr'])

def coupled_newton(mat_g, p, ridge_epsilon, device, if_damped_already):
    # revised from 
    # https://github.com/google-research/google-research/blob/master/scalable_shampoo/jax/shampoo.py#L340
    
    # if_damped_already == True: mat_g is already damped, ridge_epsilon is dummy
    
    iter_count=100
    error_tolerance=1e-6
    
    def coupled_newton_while_loop(cond_fun, body_fun, init_val):
        val = init_val
        while cond_fun(val):
            val = body_fun(val)
        return val
    
    def coupled_newton_iter_condition(state):
        (i, unused_mat_m, unused_mat_h, unused_old_mat_h, error,
         run_step) = state
        
#         error_above_threshold = jnp.logical_and(
#             error > error_tolerance, run_step)
        error_above_threshold = ((error > error_tolerance) and run_step)
        
#         sys.exit()
        
#         return jnp.logical_and(i < iter_count, error_above_threshold)
        return (i < iter_count) and error_above_threshold
    
    def coupled_newton_iter_body(state):
        (i, mat_m, mat_h, unused_old_mat_h, error, unused_run_step) = state
        mat_m_i = (1 - alpha) * identity + alpha * mat_m
        
        # slight different from jax code
        if int(p) == 1:
            new_mat_m = torch.matmul(mat_m_i, mat_m)
        elif int(p) == 2:
            mat_pow_2 = torch.matmul(mat_m_i, mat_m_i)
            new_mat_m = torch.matmul(
                mat_pow_2, mat_m,
            )
        elif int(p) == 4:
            mat_pow_2 = torch.matmul(mat_m_i, mat_m_i)
            mat_pow_4 = torch.matmul(mat_pow_2, mat_pow_2)
            new_mat_m = torch.matmul(
                mat_pow_4, mat_m,
            )
        elif int(p) == 6:
            mat_pow_2 = torch.matmul(mat_m_i, mat_m_i)
            mat_pow_4 = torch.matmul(mat_pow_2, mat_pow_2)
            mat_pow_6 = torch.matmul(mat_pow_2, mat_pow_4)
            new_mat_m = torch.matmul(
                mat_pow_6, mat_m,
            )
        else:
            print('p')
            print(p)
            sys.exit()
        
#         new_mat_m = jnp.matmul(
#             mat_power(mat_m_i, p), mat_m, precision=_INVERSE_PTH_ROOT_PRECISION)
        
        
        
#         new_mat_h = jnp.matmul(
#             mat_h, mat_m_i, precision=_INVERSE_PTH_ROOT_PRECISION)
        new_mat_h = torch.matmul(mat_h, mat_m_i,)
        
        
#         new_error = jnp.max(jnp.abs(new_mat_m - identity))
        new_error = torch.max(torch.abs(new_mat_m - identity)).item()
    
#         print('i+1')
#         print(i+1)
        
        # sometimes error increases after an iteration before decreasing and
        # converging. 1.2 factor is used to bound the maximal allowed increase.
        return (i + 1, new_mat_m, new_mat_h, mat_h, new_error,
                new_error < error * 1.2)
    
    mat_g_size = mat_g.shape[0]
    
#     alpha = jnp.asarray(-1.0 / p, _INVERSE_PTH_ROOT_DATA_TYPE)
    alpha = -1.0 / p

    identity = torch.eye(mat_g_size, device=device)
        
    
    
#     _, max_ev, _ = power_iter(mat_g)
#     ridge_epsilon = ridge_epsilon * jnp.maximum(max_ev, 1e-16)
    
    assert mat_g_size > 1
    
    if if_damped_already:
        damped_mat_g = mat_g
    else:
        damped_mat_g = mat_g + ridge_epsilon * identity
    
#     z = (1 + p) / (2 * jnp.linalg.norm(damped_mat_g))
    z = (1 + p) / (2 * torch.linalg.norm(damped_mat_g))
    
    new_mat_m_0 = damped_mat_g * z
    
#     new_error = jnp.max(jnp.abs(new_mat_m_0 - identity))
    new_error = torch.max(torch.abs(new_mat_m_0 - identity)).item()
    
#     new_mat_h_0 = identity * jnp.power(z, 1.0 / p)
    new_mat_h_0 = identity * torch.pow(z, 1.0 / p)
    
    
    
    init_state = tuple(
        [0, new_mat_m_0, new_mat_h_0, new_mat_h_0, new_error, True]
    )
    
#     _, mat_m, mat_h, old_mat_h, error, convergence = lax.while_loop(
#         _iter_condition, _iter_body, init_state)
    _, mat_m, mat_h, old_mat_h, error, convergence = coupled_newton_while_loop(
        coupled_newton_iter_condition, coupled_newton_iter_body, init_state)
    
#     error = jnp.max(jnp.abs(mat_m - identity))
    error = torch.max(torch.abs(mat_m - identity)).item()
    
#     is_converged = jnp.asarray(convergence, old_mat_h.dtype)
    is_converged = convergence
    
    resultant_mat_h = is_converged * mat_h + (1 - is_converged) * old_mat_h
    
#     resultant_mat_h = jnp.asarray(resultant_mat_h, mat_g.dtype)
    
#     return resultant_mat_h, error
    return resultant_mat_h

def get_block_Fisher(h, a, l, params):
    
    device = params['device']
    
    size_minibatch = h[l].size(0)
                
    homo_h_l = torch.cat(
        (h[l].data, torch.ones(size_minibatch, 1, device=device)),
        dim=1
    )

    a_l_grad = size_minibatch * a[l].grad.data

    a_a_T = torch.einsum('ij,ik->ijk', homo_h_l, homo_h_l)

    g_g_T = torch.einsum('ij,ik->ijk', a_l_grad, a_l_grad)

#                 print('torch.einsum(eab,ecd->eacbd, a_a_T, g_g_T).view(a_a_T.size(1)*g_g_T.size(1),  a_a_T.size(2)*g_g_T.size(2)).size()')
#                 print(torch.einsum("eab,ecd->eacbd", a_a_T, g_g_T).view(a_a_T.size(1)*g_g_T.size(1),  a_a_T.size(2)*g_g_T.size(2)).size())

    G_j = torch.zeros(homo_h_l.size(1)*a_l_grad.size(1), homo_h_l.size(1)*a_l_grad.size(1), device=device)

    for dp in range(size_minibatch):
        G_j += torch.kron(a_a_T[dp], g_g_T[dp])

    G_j /= size_minibatch
    
    return G_j

def Fisher_BD_update(data_, params):
    
    i = params['i']
    device = params['device']
    
    model_grad = data_['model_grad_used_torch']
    
    homo_model_grad = get_homo_grad(model_grad, params)
    
    delta = []
    for l in range(params['numlayers']):
        
        # compute statistics
        
        if i == 0:
            pass
        else:
            F_l = get_block_Fisher(data_['h_N2'], data_['a_N2'], l, params)
            
#             print('F_l.size()')
#             print(F_l.size())
            
            data_['block_Fisher'][l] *= 0.9
            data_['block_Fisher'][l] += 0.1 * F_l
        
#             sys.exit()
        
        # compute direction
        
        homo_grad_l_vec = torch.reshape(homo_model_grad[l].t(), (-1, 1))
        
#         F_l_LM = data_['block_Fisher'][l] + 0.001 * torch.eye(data_['block_Fisher'][l].size(0), device=device)
        F_l_LM = data_['block_Fisher'][l] + params['Fisher_BD_damping'] * torch.eye(data_['block_Fisher'][l].size(0), device=device)
        
#         F_l_LM = torch.eye(data_['block_Fisher'][l].size(0), device=device)
#         print('need to change back')
        
        homo_delta_l, _ = torch.solve(homo_grad_l_vec, F_l_LM)
        
#         print('homo_delta_l.size()')
#         print(homo_delta_l.size())
        
        homo_delta_l = torch.reshape(homo_delta_l, homo_model_grad[l].t().size()).t()

        delta_l = from_homo_to_weight_and_bias(homo_delta_l, l, params)
    
        delta.append(delta_l)
        
    p = get_opposite(delta)
    
    data_['p_torch'] = p
        
    
#     sys.exit()
    
    return data_, params


def get_BFGS_formula_v2(H, s, y, g_k, if_test_mode):
    
#     print('should move to utils.py')
    
    s = s.data
    y = y.data

    # ger(a, b) = a b^T
    rho_inv = torch.dot(s, y)

    if rho_inv <= 0:
#     if rho_inv <= 10**(-3):
#         print('BFGS not updated (case 1).')
#         print('rho_inv')
#         print(rho_inv)
    
        return H, 1
#     elif rho_inv <= 10**(-4) * torch.dot(s, s) * np.sqrt(torch.dot(g_k, g_k).item()):
        
#         sys.exit()
        
#         return H, 2

    # sHs = torch.dot(s, torch.mv(H, s))
    # if rho_inv < 0.25 * sHs:
        # theta = (0.75 * sHs) / (sHs - rho_inv)



    rho = 1 / rho_inv

    # s = s / np.sqrt(rho_inv.item())
    # y = y / np.sqrt(rho_inv.item())
    # rho = 1

    Hy = torch.mv(H, y)
    

    
#     H_new = H.data +\
#     (rho**2 * torch.dot(y, Hy) + rho) * torch.ger(s, s) -\
#     (torch.ger(rho*s, Hy) + torch.ger(Hy, rho*s))
    
#     H_new = H.data +\
#     torch.ger((rho**2 * torch.dot(y, Hy) + rho)*s, s) -\
#     (torch.ger(rho*s, Hy) + torch.ger(Hy, rho*s))
    
    H_new = H.data +\
    torch.ger((rho * torch.dot(y, Hy) + 1)*s, rho*s) -\
    (torch.ger(rho*s, Hy) + torch.ger(Hy, rho*s))
    
#     if torch.max(torch.isinf(H_new)):
#         print('inf in H')
#         print('s')
#         print(s)
#         print('y')
#         print(y)
        
#         print('torch.norm(H_new)')
#         print(torch.norm(H_new))
        
#         print('torch.norm(H)')
#         print(torch.norm(H))
        
#         print('torch.norm(s)')
#         print(torch.norm(s))
        
#         print('torch.norm(y)')
#         print(torch.norm(y))
        
#         print('rho_inv')
#         print(rho_inv)
        
#         print('torch.norm(torch.ger((rho**2 * torch.dot(y, Hy) + rho)*s, s))')
#         print(
#             torch.norm(
#                 torch.ger(
#                     (rho**2 * torch.dot(y, Hy) + rho)*s, s
#                 )
#             )
#         )
        
#         print('torch.norm(torch.ger((rho * torch.dot(y, Hy) + 1)*s, rho*s))')
#         print(torch.norm(torch.ger((rho * torch.dot(y, Hy) + 1)*s, rho*s)))
        
#         print('torch.norm((rho**2 * torch.dot(y, Hy) + rho)*s)')
#         print(torch.norm((rho**2 * torch.dot(y, Hy) + rho)*s))
        
#         print('rho**2 * torch.dot(y, Hy) + rho')
#         print(rho**2 * torch.dot(y, Hy) + rho)
        
#         print('rho**2 * torch.dot(y, Hy)')
#         print(rho**2 * torch.dot(y, Hy))
        
#         print('rho**2')
#         print(rho**2)
        
#         print('torch.dot(y, Hy)')
#         print(torch.dot(y, Hy))
        
#         print('torch.norm(Hy)')
#         print(torch.norm(Hy))
        
#         print('torch.dot(rho*y, rho*Hy)')
#         print(torch.dot(rho*y, rho*Hy))
        
#         print('torch.norm(torch.ger(rho*s, Hy))')
#         print(torch.norm(torch.ger(rho*s, Hy)))
        
#         print('torch.norm(torch.ger(Hy, rho*s))')
#         print(torch.norm(torch.ger(Hy, rho*s)))
        
#         sys.exit()
    
    if if_test_mode:
        
        print('torch.norm(s)')
        print(torch.norm(s))
        
        print('torch.norm(y)')
        print(torch.norm(y))
        
        print('torch.norm((rho**2 * torch.dot(y, torch.mv(H, y)) + rho) * torch.ger(s, s))')
        print(torch.norm((rho**2 * torch.dot(y, torch.mv(H, y)) + rho) * torch.ger(s, s)))

        print('torch.norm(rho**2 * torch.dot(y, torch.mv(H, y)) * torch.ger(s, s))')
        print(torch.norm(rho**2 * torch.dot(y, torch.mv(H, y)) * torch.ger(s, s)))
        
        print('torch.norm(rho * torch.dot(y, torch.mv(H, y)))')
        print(torch.norm(rho * torch.dot(y, torch.mv(H, y))))
        
        print('torch.norm(rho * torch.ger(s, s))')
        print(torch.norm(rho * torch.ger(s, s)))

        print('rho')
        print(rho)
        
    

    
#     if torch.norm(H_new) > 2 * torch.norm(H):
#         return H, 3
#     if torch.max(torch.isinf(H_new)):
#         return H, 4
#     else:
    H = H_new

    

    return H, 0



def from_unregularized_grad_to_regularized_grad(model_grad_torch, data_, params):
    
    if params['if_regularized_grad']:
        # if you want unregularized grad, you should NOT
        # backward the regularized grad and then subtract the 
        # regularization term. Because the accurate value will be
        # overwhelmed by the noise (numerical error?). 

        # however, if you want regularized grad, you can backward 
        # the unregularized grad and then add the regularization,
        # which will gives you the same value as backward
        # the regularized grad

        if params['tau'] == 0:
            1
        else:
            
            model = data_['model']
            
#             print('params[tau]')
#             print(params['tau'])

            model_grad_torch = get_plus_torch(
        model_grad_torch,
        get_multiply_scalar_no_grad(params['tau'], model.layers_weight)
        )
    else:
        1
        
    return model_grad_torch
    

def add_res_block(layers_, in_channels_1, out_channels_1, stride1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias):
    
#     print('need to change name for no bias')
    
#     print('how about 1 * 1 conv?')
    
    layer_ = {}
    
    
    if if_conv_bias == False:
        
        assert if_downsample_only == True
        
#         print('shortcut_type')
#         print(shortcut_type)
        
        
        
        
        
        if shortcut_type == 'padding':
            
#             print('if_BNNoAffine')
#             print(if_BNNoAffine)
            
#             sys.exit()
            
            if stride1 > 1:
                if if_BNNoAffine:
                    layer_['name'] = 'ResBlock-BNNoAffine-PaddingShortcut-NoBias'
                else:
                    layer_['name'] = 'ResBlock-BN-PaddingShortcut-NoBias'
            else:
                if if_BNNoAffine:
                    layer_['name'] = 'ResBlock-BNNoAffine-identityShortcut-NoBias'
                else:
                    layer_['name'] = 'ResBlock-BN-identityShortcut-NoBias'
                
        elif shortcut_type == 'conv':
            
            assert if_BNNoAffine == False
            
            assert if_BN_shortcut == True

            if stride1 > 1:
                layer_['name'] = 'ResBlock-BN-BNshortcut-NoBias'
            else:
                layer_['name'] = 'ResBlock-BN-identityShortcut-NoBias'
        else:
            print('shortcut_type')
            print(shortcut_type)
            sys.exit()
        
            
        
    else:
        
        
    
        if if_downsample_only:

            assert if_BNNoAffine == False

            assert if_BN_shortcut == True

            if stride1 > 1:
                layer_['name'] = 'ResBlock-BN-BNshortcut'
            else:
                layer_['name'] = 'ResBlock-BN-identityShortcut'

        else:

            if if_BNNoAffine:

                assert if_BN_shortcut == False

                layer_['name'] = 'ResBlock-BNNoAffine'
            else:
                if if_BN_shortcut:
                    layer_['name'] = 'ResBlock-BN-BNshortcut'
                else:
                    layer_['name'] = 'ResBlock-BN'

    layer_['conv1'] = {}
    
#     layer_['conv1']['conv_in_channels'] = 16
    layer_['conv1']['conv_in_channels'] = in_channels_1
    
#     layer_['conv1']['conv_out_channels'] = 16
    layer_['conv1']['conv_out_channels'] = out_channels_1
    
    layer_['conv1']['conv_kernel_size'] = 3
    
#     layer_['conv1']['conv_stride'] = 1
    layer_['conv1']['conv_stride'] = stride1
    
    layer_['conv1']['conv_padding'] = 1
    
#     print('if_conv_bias')
#     print(if_conv_bias)
    
    
    
    layer_['conv1']['conv_bias'] = if_conv_bias

    
    
    if if_BNNoAffine:
        layer_['BNNoAffine1'] = {}
        layer_['BNNoAffine1']['num_features'] = out_channels_1
    else:
        layer_['BN1'] = {}
        layer_['BN1']['num_features'] = out_channels_1

    layer_['conv2'] = {}
    

    layer_['conv2']['conv_in_channels'] = out_channels_1
    
    layer_['conv2']['conv_out_channels'] = out_channels_1
    
    layer_['conv2']['conv_kernel_size'] = 3
    layer_['conv2']['conv_stride'] = 1
    layer_['conv2']['conv_padding'] = 1
    layer_['conv2']['conv_bias'] = if_conv_bias

    
    
    if if_BNNoAffine:
        layer_['BNNoAffine2'] = {}
        layer_['BNNoAffine2']['num_features'] = out_channels_1
    else:
        layer_['BN2'] = {}
        layer_['BN2']['num_features'] = out_channels_1
        
        
#     print('layer_[name]')
#     print(layer_['name'])
    
    
    
    if layer_['name'] in ['ResBlock-BN-identityShortcut',
                          'ResBlock-BN-identityShortcut-NoBias',
                          'ResBlock-BN-PaddingShortcut-NoBias']:
        1
    else:
        # 1*1 conv for shortcut
        layer_['conv3'] = {}

        layer_['conv3']['conv_in_channels'] = in_channels_1

        layer_['conv3']['conv_out_channels'] = out_channels_1

        layer_['conv3']['conv_kernel_size'] = 1

        layer_['conv3']['conv_stride'] = stride1

        layer_['conv3']['conv_padding'] = 0
        
        layer_['conv3']['conv_bias'] = if_conv_bias

        if if_BN_shortcut:
            assert if_BNNoAffine == False

            layer_['BN3'] = {}
            layer_['BN3']['num_features'] = out_channels_1



    layers_.append(layer_)
    
    return layers_

def add_conv_block(layers_, in_channels, out_channels, kernel_size, stride, padding, params):
    # conv + (possible) BN + activation
    
    layer_2 = {}
    
    if params['name_dataset'] in ['CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                                  'CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',
                                  'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                  'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                  'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',]:
        layer_2['name'] = 'conv-no-bias-no-activation'
    elif params['name_dataset'] in ['CIFAR-10-AllCNNC',
                                    'CIFAR-10-N1-128-AllCNNC',
                                    'CIFAR-10-N1-512-AllCNNC',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                    'CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization',
                                    'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization',
                                    'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                    'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                                    'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                                    'CIFAR-10-onTheFly-ResNet32-BN',
                                    'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                                    'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                    'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                                    'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                    'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                    'CIFAR-100-onTheFly-AllCNNC',
                                    'CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                                    'CIFAR-100-onTheFly-ResNet34-BN',
                                    'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                                    'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',]:
        layer_2['name'] = 'conv-no-activation'
    else:
        print('params[name_dataset]')
        print(params['name_dataset'])
        sys.exit()
        
    
    layer_2['conv_in_channels'] = in_channels
    layer_2['conv_out_channels'] = out_channels
    
    
    layer_2['conv_kernel_size'] = kernel_size
    
    layer_2['conv_stride'] = stride
    
    layer_2['conv_padding'] = padding
    
    
    layer_2['activation'] = None
    layers_.append(layer_2)
    
    if params['name_dataset'] in ['CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                  'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                  'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                  'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                                  'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization',
                                  'CIFAR-10-onTheFly-ResNet32-BN',
                                  'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                                  'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                                  'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                  'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                  'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                  'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                  'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                  'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                                  'CIFAR-100-onTheFly-ResNet34-BN',
                                  'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                                  'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                                  'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                                  'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',]:
        # references:
        # http://torch.ch/blog/2015/07/30/cifar.html
        # https://github.com/kuangliu/pytorch-cifar
        layer_2 = {}
        layer_2['name'] = 'BN'
        layer_2['num_features'] = out_channels
        layer_2['activation'] = None
        layers_.append(layer_2)
    elif params['name_dataset'] in ['CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                    'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                                    'CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',
                                    'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                    'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                    'CIFAR-100-onTheFly-ResNet34-BNNoAffine',]:
        layer_2 = {}
        layer_2['name'] = 'BNNoAffine'
        layer_2['num_features'] = out_channels
        layer_2['activation'] = None
        layers_.append(layer_2)
    elif params['name_dataset'] in ['CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-AllCNNC',
                                    'CIFAR-10-N1-128-AllCNNC',
                                    'CIFAR-10-N1-512-AllCNNC',
                                    'CIFAR-10-ConvPoolCNNC',
                                    'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization',
                                    'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                    'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                                    'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                    'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                    'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                    'CIFAR-100-onTheFly-AllCNNC']:
        1
    else:
        print('error: need to check for ' + params['name_dataset'])
        sys.exit()

    layer_2 = {}
    layer_2['name'] = 'relu'
    layers_.append(layer_2)
    
    return layers_

def get_next_lr(list_lr_tried, best_lr):
    
    if best_lr == min(list_lr_tried):
        
        print('list_lr_complete.index(best_lr)')
        print(list_lr_complete.index(best_lr))
        
        if list_lr_complete.index(best_lr) == 0:
            print('error: need to expand list_lr_complete')
            sys.exit()
        else:
            return list_lr_complete[list_lr_complete.index(best_lr) - 1]
    elif best_lr == max(list_lr_tried):
        
        if list_lr_complete.index(best_lr) == len(list_lr_complete) - 1:
            print('error: need to expand list_lr_complete')
            sys.exit()
        else:
            return list_lr_complete[list_lr_complete.index(best_lr) + 1]
        
    elif best_lr > min(list_lr_tried) and best_lr < max(list_lr_tried):
        return -1
    else:
        print('there is an error')
        sys.exit()
    
    return learning_rate

def add_some_if_record_to_args(args):
    
    if not 'if_record_sgd_norm' in args:
        args['if_record_sgd_norm'] = False
    if not 'if_record_sgn_norm' in args:
        args['if_record_sgn_norm'] = False
    if not 'if_record_p_norm' in args:
        args['if_record_p_norm'] = False
#     if not 'if_record_kron_bfgs_cosine' in args:
#         args['if_record_kron_bfgs_cosine'] = False
    if not 'if_record_kfac_p_norm' in args:
        args['if_record_kfac_p_norm'] = False
    if not 'if_record_kfac_p_cosine' in args:
        args['if_record_kfac_p_cosine'] = False
    if not 'if_record_res_grad_norm' in args:
        args['if_record_res_grad_norm'] = False
    if not 'if_record_res_grad_random_norm' in args:
        args['if_record_res_grad_random_norm'] = False
    if not 'if_record_res_grad_grad_norm' in args:
        args['if_record_res_grad_grad_norm'] = False
    if not 'if_record_res_grad_norm_per_iter' in args:
        args['if_record_res_grad_norm_per_iter'] = False
    
    return args

def add_matrix_name_to_args(args):
    
    if args['algorithm'] == 'SMW-GN':
        args['matrix_name'] = 'GN'
    elif args['algorithm'] in ['SMW-Fisher-signVAsqrt-p',
                               'SMW-Fisher-VA-p',
                               'SMW-Fisher-momentum-p-sign',
                               'SMW-Fisher-momentum-p',
                               'SMW-Fisher-sign',
                               'SMW-Fisher-different-minibatch',
                               'SMW-Fisher',
                               'SMW-Fisher-momentum',
                               'SMW-Fisher-batch-grad-momentum-exponential-decay',
                               'SMW-Fisher-batch-grad-momentum',
                               'SMW-Fisher-batch-grad',
                               'shampoo-no-sqrt-Fisher-momentum-grad',
                               'shampoo-no-sqrt-Fisher-momentum-grad-test',
                               'matrix-normal',
                               'matrix-normal-momentum-grad',
                               'matrix-normal-allVariables-momentum-grad',
                               'matrix-normal-allVariables-warmStart-momentum-grad',
                               'matrix-normal-allVariables-warmStart-MaxEigDamping-momentum-grad',
                               'matrix-normal-allVariables-warmStart-noPerDimDamping-momentum-grad',
                               'matrix-normal-LM-momentum-grad',
                               'matrix-normal-same-trace',
                               'matrix-normal-same-trace-momentum-grad',
                               'matrix-normal-same-trace-warmStart-momentum-grad',
                               'matrix-normal-same-trace-warmStart-noPerDimDamping-momentum-grad',
                               'matrix-normal-same-trace-allVariables-momentum-grad',
                               'matrix-normal-same-trace-allVariables-warmStart-momentum-grad',
                               'matrix-normal-same-trace-allVariables-warmStart-momentum-grad-LRdecay',
                               'matrix-normal-same-trace-allVariables-filterFlattening-warmStart-momentum-grad',
                               'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad',
                               'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad-LRdecay',
                               'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping-momentum-grad',
                               'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping-momentum-grad',
                               'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping-momentum-grad',
                               'matrix-normal-same-trace-LM-momentum-grad',
                               'kfac-TR',
                               'kfac-CG',
                               'kfac-momentum-grad-CG',
                               'kfac-momentum-grad-TR',
                               'kfac',
                               'kfac-momentum-grad',
                               'kfac-no-max',
                               'kfac-no-max-no-LM',
                               'kfac-no-max-no-LM-momentum-grad',
                               'kfac-warmStart-no-max-no-LM-momentum-grad',
                               'kfac-warmStart-lessInverse-no-max-no-LM-momentum-grad',
                               'kfac-warmStart-lessInverse-no-max-no-LM-momentum-grad-LRdecay',
                               'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM-momentum-grad',
                               'kfac-no-max-epsilon-A-G-no-LM-momentum-grad',
                               'kfac-no-max-momentum-grad',
                               'kfac-NoMaxNoSqrt-momentum-grad',
                               'kfac-NoMaxNoSqrt-no-LM-momentum-grad']:
        args['matrix_name'] = 'Fisher'
    elif args['algorithm'] in ['Fisher-BD',
                               'Fisher-BD-momentum-grad',
                               'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad',
                               'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay',
                               'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM-momentum-grad',
                               'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM-momentum-grad-LRdecay',
                               'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM-momentum-grad',
                               'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM-momentum-grad',
                               'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad',
                               'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-momentum-grad',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping-momentum-grad',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping-momentum-grad',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping-momentum-grad',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-momentum-grad',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad-LRdecay',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad-LRdecay',]:
        args['matrix_name'] = 'Fisher-correct'
    elif args['algorithm'] in ['RMSprop-individual-grad',
                               'RMSprop-individual-grad-no-sqrt',
                               'RMSprop-individual-grad-no-sqrt-LM',
                               'kfac-EF',
                               'ekfac-EF-VA',
                               'ekfac-EF',
                               'Kron-BFGS',
                               'Kron-BFGS-no-norm-gate-regularized-grad',
                               'Kron-BFGS-no-norm-gate-momentum-s-y-regularized-grad',
                               'Kron-BFGS-no-norm-gate-momentum-s-y-damping-regularized-grad',
                               'Kron-BFGS-no-norm-gate-momentum-s-y-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-momentum-s-y-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-momentum-s-y-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-damping-regularized-grad',
                               'Kron-BFGS-no-norm-gate-Shiqian-damping-regularized-grad',
                               'Kron-BFGS-no-norm-gate-Shiqian-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-Shiqian-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-Shiqian-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-PowellBDamping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-unregularized-grad-momentum-grad-LRdecay',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad-LRdecay',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep-regularized-grad-momentum-grad',
                               'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-doubleGrad-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping-regularized-grad-momentum-grad',
                               'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-identity-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-identity-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-damping-unregularized-grad-momentum-grad',
                               'Kron-BFGS-homo-no-norm-gate-damping-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-regularized-grad-momentum-grad',
                               'Kron-BFGS-no-norm-gate-unregularized-grad-momentum-grad',
                               'Kron-BFGS-momentum-grad',
                               'Kron-BFGS-regularized-grad',
                               'Kron-BFGS-homo-regularized-grad',
                               'Kron-BFGS-homo-regularized-grad-momentum-grad',
                               'Kron-BFGS-homo-unregularized-grad-momentum-grad',
                               'Kron-BFGS-unregularized-grad',
                               'Kron-BFGS-wrong-unregularized-grad',
                               'Kron-BFGS-unregularized-grad-momentum-grad',
                               'Kron-BFGS-wrong-unregularized-grad-momentum-grad',
                               'Kron-BFGS-regularized-grad-momentum-grad',
                               'Kron-BFGS-Hessian-action',
                               'Kron-BFGS-Hessian-action-unregularized-grad',
                               'Kron-BFGS-Hessian-action-momentum-grad',
                               'Kron-BFGS-Hessian-action-unregularized-grad-momentum-grad',
                               'Kron-BFGS-wrong-Hessian-action-unregularized-grad-momentum-grad',
                               'Kron-BFGS-LM',
                               'Kron-BFGS-LM-regularized-grad',
                               'Kron-BFGS-LM-sqrt-regularized-grad',
                               'Kron-BFGS-LM-sqrt-regularized-grad-momentum-grad',
                               'Kron-BFGS-LM-unregularized-grad',
                               'Kron-BFGS-LM-unregularized-grad-momentum-grad',
                               'Kron-BFGS-LM-regularized-grad-momentum-grad',
                               'Kron-BFGS-LM-momentum-grad',
                               'Kron-BFGS-1st-layer-only',
                               'Kron-BFGS-block',
                               'Kron-SGD',
                               'Kron-SGD-test']:
        args['matrix_name'] = 'EF'
    elif args['algorithm'] == 'RMSprop-individual-grad-no-sqrt-Fisher':
        args['matrix_name'] = 'Fisher'
    elif args['algorithm'] in ['SGD-yura-BD',
                               'SGD-yura-old',
                               'SGD-yura',
                               'SGD-yura-MA',
                               'SGD-VA',
                               'SGD-sign',
                               'SGD-signVA',
                               'SGD-signVAerf',
                               'SGD-signVAsqrt',
                               'SGD-momentum-yura',
                               'SGD-momentum',
                               'SGD-LRdecay-momentum',
                               'SGD',
                               'RMSprop',
                               'RMSprop-test',
                               'RMSprop-momentum-grad',
                               'RMSprop-warmStart-momentum-grad',
                               'RMSprop-momentum-grad-test',
                               'RMSprop-no-sqrt',
                               'shampoo',
                               'shampoo-momentum-grad',
                               'shampoo-allVariables-momentum-grad',
                               'shampoo-allVariables-warmStart-momentum-grad',
                               'shampoo-allVariables-warmStart-lessInverse-momentum-grad',
                               'shampoo-allVariables-filterFlattening-warmStart-momentum-grad',
                               'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay',
                               'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad',
                               'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad-LRdecay',
                               'shampoo-no-sqrt-momentum-grad',
                               'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart-momentum-grad',
                               'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay',
                               'Adam-momentum-grad',
                               'Adam-noWarmStart-momentum-grad',
                               'Adam-noWarmStart-momentum-grad-LRdecay',
                               'BFGS',
                               'BFGS-homo']:
        args['matrix_name'] = 'None'
    else:
        print('Error: undefined matrix name for ' + args['algorithm'])
        sys.exit()
    
    return args

def get_warm_start(data_, params):
    
    N1 = params['N1']
    
    assert N1 < params['num_train_data']
    # i.e. stochastic setting
    
    device = params['device']
    numlayers = params['numlayers']
    layers_params = params['layers_params']
    
    model = data_['model']
        
    i = 0 # position of training data
    j = 0 # position of mini-batch

    from utils_git.utils_kfac import get_g_g_T, get_g_g_T_BN
    
    from utils_git.utils_shampoo import shampoo_kron_matrices_warm_start_per_variable
    
    print('Begin warm start...')

    while i + N1 <= params['num_train_data']:



        X_mb, t_mb = data_['dataset'].train.next_batch(N1)

#         X_mb = torch.from_numpy(X_mb).to(device)
        
        if not params['if_dataset_onTheFly']:
            X_mb = torch.from_numpy(X_mb)
        X_mb = X_mb.to(device)



        z, a, h = model.forward(X_mb)
        
        if params['matrix_name'] in ['Fisher',
                                     'Fisher-correct']:
            params['N2_index'] = list(range(N1))
            t_mb_pred = sample_from_pred_dist(z, params)
            del params['N2_index']

            t_mb_used = t_mb_pred
        elif params['matrix_name'] == 'None':
            # None is actually EF
            
#             print('t_mb')
#             print(t_mb)
            
#             t_mb = torch.from_numpy(t_mb).to(device)
            
            if not params['if_dataset_onTheFly']:
                t_mb = torch.from_numpy(t_mb)
            t_mb = t_mb.to(device)
            
            t_mb_used = t_mb
        else:
            print('params[matrix_name]')
            print(params['matrix_name'])
            sys.exit()
        
        '''
        if params['algorithm'] == 'shampoo-allVariables-warmStart':
            t_mb = torch.from_numpy(t_mb).to(device)
            t_mb_used = t_mb
        elif params['algorithm'] in ['kfac-warmStart-no-max-no-LM',
                                     'kfac-warmStart-lessInverse-no-max-no-LM',
                                     'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                     'kfac-no-max-no-LM',
                                     'matrix-normal-allVariables-warmStart',
                                     'matrix-normal-allVariables-warmStart-noPerDimDamping']:
            params['N2_index'] = list(range(N1))
            t_mb_pred = sample_from_pred_dist(z, params)
            del params['N2_index']

            t_mb_used = t_mb_pred
        else:
            print('error: need to check for ' + params['algorithm'])
            sys.exit()
        '''
        
        

            

        loss = get_loss_from_z(model, z, t_mb_used, reduction='mean') # not regularized

        model.zero_grad()
        loss.backward()
        
        if params['if_model_grad_N2'] or\
        params['algorithm'] in ['shampoo-allVariables-warmStart',
                                'shampoo-allVariables-warmStart-lessInverse',
                                'shampoo-allVariables-filterFlattening-warmStart',
                                'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                                'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
            
#             assert params['tau'] == 0
            
            model_grad_N2 = get_model_grad(model, params)

        i += N1
        j += 1

        for l in range(numlayers):
            # bar_A_j = 1 / j * (A_1 + ... + A_j)
            # bar_A_j = (j-1) / j * bar_A_{j-1} + 1 / j * A_j

#                     homo_h_l =\
#                     torch.cat((h[l], torch.ones(N1, 1, device=device)), dim=1)
#                     A_j = 1/N1 * torch.mm(homo_h_l.t(), homo_h_l).data


#             print('params[Kron_BFGS_if_homo]')
#             print(params['Kron_BFGS_if_homo'])

            
            if params['algorithm'] in ['matrix-normal-allVariables-warmStart',
                                       'matrix-normal-allVariables-warmStart-MaxEigDamping',
                                       'matrix-normal-allVariables-warmStart-noPerDimDamping',
                                       'matrix-normal-same-trace-warmStart',
                                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                                       'matrix-normal-same-trace-allVariables-warmStart',
                                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                                       'shampoo-allVariables-warmStart',
                                       'shampoo-allVariables-warmStart-lessInverse',
                                       'shampoo-allVariables-filterFlattening-warmStart',
                                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse',]:
                
                for name_variable in data_['model'].layers_weight[l].keys():
                    shampoo_kron_matrices_warm_start_per_variable(j, model_grad_N2, l, name_variable, data_, params)
                    
                    if params['if_Hessian_action'] and not i + N1 <= params['num_train_data']:
                    
                        # to constrain epsilon
                        
                        assert params['algorithm'] == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse'
                        epsilon = params['shampoo_epsilon']
                
                        H = data_['shampoo_H']
                    
                        H_l_LM_minus_2k = []
                    
                        for ii in range(len(H[l][name_variable])):
                            
                            H_l_ii_LM = H[l][name_variable][ii] + epsilon * torch.eye(H[l][name_variable][ii].shape[0], device=device)
                        
                            H_l_LM_minus_2k.append(H_l_ii_LM.inverse())
                            
#                         sys.exit()
                        
                        data_['shampoo_H_LM_minus_2k'][l][name_variable] = H_l_LM_minus_2k
                
                
                    
            
                
            elif params['algorithm'] in ['kfac-no-max-no-LM',
                                         'kfac-warmStart-no-max-no-LM',
                                         'kfac-warmStart-lessInverse-no-max-no-LM',
                                         'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                         'kfac-correctFisher-warmStart-no-max-no-LM',
                                         'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                         'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                         'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',]:
                
                if layers_params[l]['name'] in ['conv',
                                                'conv-no-activation',
                                                'conv-no-bias-no-activation',
                                                'fully-connected']:
                    
                
                    A_j = get_A_A_T(h, l, data_, params)
                    data_['A'][l] *= (j-1)/j
                    data_['A'][l] += 1/j * A_j

                    G_j = get_g_g_T(a, l, params)
                    data_['G'][l] *= (j-1)/j
                    data_['G'][l] += 1/j * G_j
                    
                elif layers_params[l]['name'] == 'BN':
                    
                    if params['kfac_if_update_BN'] and not params['kfac_if_BN_grad_direction']:
                    
                        G_j = get_g_g_T_BN(model, l, N1)
                    
                        data_['G'][l] *= (j-1)/j
                        data_['G'][l] += 1/j * G_j
                    
                else:
                    print('error: need to check for ' + layers_params[l]['name'])
                    sys.exit()
                    
            elif params['algorithm'] in ['Fisher-BD',]:
                
                print('i')
                print(i)
                
                
        
                G_j = get_block_Fisher(h, a, l, params)
                
                if j == 1:
                    data_['block_Fisher'][l] = G_j
                else:
                    data_['block_Fisher'][l] *= (j-1)/j
                    data_['block_Fisher'][l] += 1/j * G_j
                
            else:
                print('error: need to check for ' + params['algorithm'])
                sys.exit()
                
                
def get_h_l_unfolded_noHomo_noFlatten(h, l, params):
    # for the use of stride, see _extract_patches in
    # https://github.com/gpauloski/kfac_pytorch/blob/master/kfac/utils.py
    
    layers_params = params['layers_params']
    
    assert layers_params[l]['name'] in ['conv', 'conv-no-activation']
    
    padding = layers_params[l]['conv_padding']
    kernel_size = layers_params[l]['conv_kernel_size']
    device = params['device']
    
    stride = layers_params[l]['conv_stride']
    
    assert 2 * padding + 1 == kernel_size
    
    # 2d-conv: a[l]: M * I * |T|, where |T| has two dimensions
        
    # (Take Fashion-MNIST as an example)
    # h[l]: 1000 * 1 * 28 * 28
    # 1000: size of minibatch
    # 1: conv_in_channels
    # 28 * 28: size of input
    # h_l_padded_unfolded: 1000 * 1 * 32 * 32
    # 32 * 32: size of padded input
    h_l_padded = F.pad(
        h[l].data, (padding, padding, padding, padding), "constant", 0
    )

    # h_l_padded_unfolded: 1000 * 1 * 28 * 32 * 5
    # 5: conv_kernel_size
    h_l_padded_unfolded = h_l_padded.unfold(2, kernel_size, stride)

    h_l_padded_unfolded = h_l_padded_unfolded.unfold(3, kernel_size, stride)
    # h_l_padded_unfolded: 1000 * 1 * 28 * 28 * 5 * 5
    
#     return h_l_padded_unfolded

    h_l_padded_unfolded = h_l_padded_unfolded.permute(0, 2, 3, 1, 4, 5)
    # h_l_padded_unfolded: 1000 * 28 * 28 * 1 * 5 * 5

#     h_l_padded_unfolded = h_l_padded_unfolded.flatten(start_dim=3)
    # h_l_padded_unfolded: 1000 * 28 * 28 * 25
    
    
    return h_l_padded_unfolded
                
                
def get_h_l_unfolded_noHomo(h, l, params):
    # for the use of stride, see _extract_patches in
    # https://github.com/gpauloski/kfac_pytorch/blob/master/kfac/utils.py
    
    layers_params = params['layers_params']
    
    assert layers_params[l]['name'] in ['conv', 'conv-no-activation', 'conv-no-bias-no-activation']
    
    padding = layers_params[l]['conv_padding']
    kernel_size = layers_params[l]['conv_kernel_size']
    device = params['device']
    
    stride = layers_params[l]['conv_stride']
    
    assert 2 * padding + 1 == kernel_size
    
    # 2d-conv: a[l]: M * I * |T|, where |T| has two dimensions
        
    # (Take Fashion-MNIST as an example)
    # h[l]: 1000 * 1 * 28 * 28
    # 1000: size of minibatch
    # 1: conv_in_channels
    # 28 * 28: size of input
    # h_l_padded_unfolded: 1000 * 1 * 32 * 32
    # 32 * 32: size of padded input
    h_l_padded = F.pad(
        h[l].data, (padding, padding, padding, padding), "constant", 0
    )

    # h_l_padded_unfolded: 1000 * 1 * 28 * 32 * 5
    # 5: conv_kernel_size
    h_l_padded_unfolded = h_l_padded.unfold(2, kernel_size, stride)

    h_l_padded_unfolded = h_l_padded_unfolded.unfold(3, kernel_size, stride)
    # h_l_padded_unfolded: 1000 * 1 * 28 * 28 * 5 * 5
    
#     return h_l_padded_unfolded

    h_l_padded_unfolded = h_l_padded_unfolded.permute(0, 2, 3, 1, 4, 5)
    # h_l_padded_unfolded: 1000 * 28 * 28 * 1 * 5 * 5

    h_l_padded_unfolded = h_l_padded_unfolded.flatten(start_dim=3)
    # h_l_padded_unfolded: 1000 * 28 * 28 * 25
    
    
    return h_l_padded_unfolded

                

def get_h_l_unfolded(h, l, data_, params):
    # for the use of stride, see _extract_patches in
    # https://github.com/gpauloski/kfac_pytorch/blob/master/kfac/utils.py
    
#     print('should be deprecated')
    
    layers_params = params['layers_params']
    
#     print('layers_params[l][name]')
#     print(layers_params[l]['name'])
    
    assert layers_params[l]['name'] in ['conv',
                                        'conv-no-activation',
                                        'conv-no-bias-no-activation']
    
    padding = layers_params[l]['conv_padding']
    kernel_size = layers_params[l]['conv_kernel_size']
    device = params['device']
    
    stride = layers_params[l]['conv_stride']
    
    assert 2 * padding + 1 == kernel_size
    
    # 2d-conv: a[l]: M * I * |T|, where |T| has two dimensions
        
    # (Take Fashion-MNIST as an example)
    # h[l]: 1000 * 1 * 28 * 28
    # 1000: size of minibatch
    # 1: conv_in_channels
    # 28 * 28: size of input
    # h_l_padded_unfolded: 1000 * 1 * 32 * 32
    # 32 * 32: size of padded input
    h_l_padded = F.pad(
        h[l].data, (padding, padding, padding, padding), "constant", 0
    )

    # h_l_padded_unfolded: 1000 * 1 * 28 * 32 * 5
    # 5: conv_kernel_size
#     h_l_padded_unfolded = h_l_padded.unfold(2, kernel_size, 1)
    h_l_padded_unfolded = h_l_padded.unfold(2, kernel_size, stride)

#     h_l_padded_unfolded = h_l_padded_unfolded.unfold(3, kernel_size, 1)
    h_l_padded_unfolded = h_l_padded_unfolded.unfold(3, kernel_size, stride)
    # h_l_padded_unfolded: 1000 * 1 * 28 * 28 * 5 * 5
    
#     print('need to change back the correct return')
    
#     return h_l_padded_unfolded
    
#     sys.exit()

    h_l_padded_unfolded = h_l_padded_unfolded.permute(0, 2, 3, 1, 4, 5)
    # h_l_padded_unfolded: 1000 * 28 * 28 * 1 * 5 * 5

    h_l_padded_unfolded = h_l_padded_unfolded.flatten(start_dim=3)
    # h_l_padded_unfolded: 1000 * 28 * 28 * 25


    if 'b' not in data_['model'].layers_weight[l].keys():
        
        pass

    elif params['Kron_BFGS_if_homo']:

        h_homo_ones = torch.ones(
        h_l_padded_unfolded.size(0), h_l_padded_unfolded.size(1), h_l_padded_unfolded.size(2), 1, device=device
    )

        h_l_padded_unfolded = torch.cat(
            (h_l_padded_unfolded, h_homo_ones), 
            dim=3
        )
        # h_l_padded_unfolded: 1000 * 28 * 28 * 26

    else:
        print('error: need to check')
        sys.exit()
    
    
    return h_l_padded_unfolded

def get_A_A_T_v_kfac_v2(v, h, l, params, data_):
    
    layers_params = params['layers_params']
    device = params['device']
    
    kernel_size = layers_params[l]['conv_kernel_size']
    padding = layers_params[l]['conv_padding']

    if layers_params[l]['name'] == '1d-conv':
        
        print('error: need to change so that it is averaged on minibatch')
        
        sys.exit()
        
        
        
        # 1d-conv: a[l]: M * I * |T|
        h_l_padded = F.pad(h[l].data, (padding, padding), "constant", 0)
        
        # M * J * |T| * |Delta|
        h_l_padded_unfolded = h_l_padded.unfold(2, kernel_size, 1)
        
        h_l_padded_unfolded = h_l_padded_unfolded.permute(0, 2, 1, 3)
        
        h_l_padded_unfolded = h_l_padded_unfolded.flatten(start_dim=2)
        
        
        print('following wrong for kfac?')
        sys.exit()
        
        if params['Kron_BFGS_if_homo']:
            
            h_homo_ones = torch.ones(
            h_l_padded_unfolded.size(0), h_l_padded_unfolded.size(1), 1, device=device
        )
            
            h_l_padded_unfolded = torch.cat(
                (h_l_padded_unfolded, h_homo_ones), 
                dim=2
            )
            
            
        
        test_2_A_j = torch.einsum('sti,stj->ij', h_l_padded_unfolded, h_l_padded_unfolded)
    elif layers_params[l]['name'] in ['conv',
                                      'conv-no-activation',
                                      'conv-no-bias-no-activation',]:
        
#         h_l_padded_unfolded = data_['h_N2_unfolded'][l]
    
#         size_minibatch = h_l_padded_unfolded.size(0)
        
        
        
#         h_l_padded_unfolded_viewed = h_l_padded_unfolded.view(-1, h_l_padded_unfolded.size(-1))    
        
    
#         Av = torch.mv(h_l_padded_unfolded_viewed.t(), torch.mv(h_l_padded_unfolded_viewed, v))



    
        h_l_padded_unfolded_noHomo = data_['h_N2_unfolded_noHomo'][l]
        
        size_minibatch = h_l_padded_unfolded_noHomo.size(0)
    
        h_l_padded_unfolded_noHomo_viewed = h_l_padded_unfolded_noHomo.view(-1, h_l_padded_unfolded_noHomo.size(-1))






#         h_l_padded_unfolded_noHomo_noFlatten = data_['h_N2_unfolded_noHomo_noFlatten'][l]

#         size_minibatch = h_l_padded_unfolded_noHomo_noFlatten.size(0)

        assert params['Kron_BFGS_if_homo']
    
        if layers_params[l]['name'] in ['conv',
                                        'conv-no-activation',]:
            
            Av = torch.mv(h_l_padded_unfolded_noHomo_viewed, v[:-1]) + v[-1].item()
            
            Av = torch.cat(
                (
                    torch.mv(h_l_padded_unfolded_noHomo_viewed.t(), Av),
                    torch.sum(Av).unsqueeze(dim=0)
                )
            )



        elif layers_params[l]['name'] in ['conv-no-bias-no-activation',]:
            Av = torch.mv(h_l_padded_unfolded_noHomo_viewed, v)
            Av = torch.mv(h_l_padded_unfolded_noHomo_viewed.t(), Av)
        else:
            print('error: need to check for ' + layers_params[l]['name'])
            sys.exit()
        
        Av = Av / size_minibatch
        
        
       
    else:
        print('error: not implemented for ' + layers_params[l]['name'])
        sys.exit()
    
    return Av


def get_A_A_T_kfac_v2(h, l, data_, params):
    
    layers_params = params['layers_params']
    device = params['device']
    
    kernel_size = layers_params[l]['conv_kernel_size']
    padding = layers_params[l]['conv_padding']


#     if layers_params[l]['name'] == '1d-conv':
#         size_test_2_A_j = kernel_size *\
# layers_params[l]['conv_in_channels']
#     elif layers_params[l]['name'] == 'conv':
        
#         size_test_2_A_j = kernel_size**2 *\
# layers_params[l]['conv_in_channels']


#     test_2_A_j = torch.zeros(
#         size_test_2_A_j, size_test_2_A_j, device=device
#     )

    if layers_params[l]['name'] == '1d-conv':
        
        print('need to make it averaged over minibatch')
        
        
        
        # 1d-conv: a[l]: M * I * |T|
        h_l_padded = F.pad(h[l].data, (padding, padding), "constant", 0)
        
        # M * J * |T| * |Delta|
        h_l_padded_unfolded = h_l_padded.unfold(2, kernel_size, 1)
        
        h_l_padded_unfolded = h_l_padded_unfolded.permute(0, 2, 1, 3)
        
        h_l_padded_unfolded = h_l_padded_unfolded.flatten(start_dim=2)
        
        
        print('following wrong for kfac?')
        sys.exit()
        
        if params['Kron_BFGS_if_homo']:
            
            h_homo_ones = torch.ones(
            h_l_padded_unfolded.size(0), h_l_padded_unfolded.size(1), 1, device=device
        )
            
            h_l_padded_unfolded = torch.cat(
                (h_l_padded_unfolded, h_homo_ones), 
                dim=2
            )
            
            
        
        test_2_A_j = torch.einsum('sti,stj->ij', h_l_padded_unfolded, h_l_padded_unfolded)
        
        
        
#         sys.exit()
        
#         h_homo_ones = torch.ones(h_l_padded.size(0), 1 ,device=device)
        
        

#         for t in range(a[l].size(2)):
            # a[l].size(2) = |T|
            
#             h_l_t = h_l_padded[:, :, t:t+kernel_size].data

            # in the flatten, delta changes the fastest
#             h_l_t_flatten = h_l_t.flatten(start_dim=1)
            
#             if params['Kron_BFGS_if_homo']:
#                 h_l_t_flatten = torch.cat((h_l_t_flatten, h_homo_ones), dim=1)

#             test_2_A_j += torch.mm(h_l_t_flatten.t(), h_l_t_flatten)
    elif layers_params[l]['name'] in ['conv',
                                      'conv-no-activation',
                                      'conv-no-bias-no-activation']:
        
        h_l_padded_unfolded = get_h_l_unfolded(h, l, data_, params)
    
        size_minibatch = h_l_padded_unfolded.size(0)
        
        
            

        
        test_2_A_j = torch.einsum('stli,stlj->ij', h_l_padded_unfolded, h_l_padded_unfolded)
        # test_2_A_j: 26 * 26
        
        test_2_A_j = test_2_A_j / size_minibatch
        
#         h_homo_ones = torch.ones(h_l_padded.size(0), 1 ,device=device)

#         for t1 in range(a[l].size(2)):
#             for t2 in range(a[l].size(3)):
                
#                 h_l_t =\
# h_l_padded[:, :, t1:t1+kernel_size, t2:t2+kernel_size].data

#                 h_l_t_flatten = h_l_t.flatten(start_dim=1)
                
#                 if params['Kron_BFGS_if_homo']:
#                     h_l_t_flatten = torch.cat((h_l_t_flatten, h_homo_ones), dim=1)
    
#                 test_2_A_j += torch.mm(h_l_t_flatten.t(), h_l_t_flatten)
       
    else:
        print('error: not implemented for ' + layers_params[l]['name'])
        sys.exit()
    
                
    return test_2_A_j

def get_A_A_T_v(v, h, l, params, data_):
    
    layers_params = params['layers_params']
    
    device = params['device']
    
    if layers_params[l]['name'] == 'fully-connected':
        
        
        
        size_minibatch = h[l].size(0)
    
        if params['algorithm'] in ['kfac-no-max-no-LM',
                                   'kfac-warmStart-no-max-no-LM',
                                   'kfac-warmStart-lessInverse-no-max-no-LM'] or\
        params['Kron_BFGS_if_homo']:
            
            homo_h_l = torch.cat(
                (h[l], torch.ones(size_minibatch, 1, device=device)),
                dim=1
            )
        elif algorithm in ['Kron-BFGS',
                           'Kron-BFGS-no-norm-gate',
                           'Kron-BFGS-no-norm-gate-momentum-s-y',
                           'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                           'Kron-BFGS-no-norm-gate-damping',
                           'Kron-BFGS-no-norm-gate-Shiqian-damping',
                           'Kron-BFGS-wrong',
                           'Kron-BFGS-Hessian-action',
                           'Kron-BFGS-wrong-Hessian-action',
                           'Kron-BFGS-LM',
                           'Kron-BFGS-LM-sqrt']:
            homo_h_l = h[l]
        else:
            print('error: not implemented')
            sys.exit()
            
#         print('homo_h_l.size()')
#         print(homo_h_l.size())
        
#         print('v.size()')
#         print(v.size())
            
#         sys.exit()

#         Av = (1/size_minibatch * torch.mm(homo_h_l.t(), homo_h_l).data) * v
#         Av = (1/size_minibatch * homo_h_l.t() * homo_h_l) * v
        Av = torch.mv(homo_h_l.t(), torch.mv(homo_h_l, v)) /size_minibatch

    elif layers_params[l]['name'] in ['1d-conv',
                                      'conv',
                                      'conv-no-activation',
                                      'conv-no-bias-no-activation',]:
        
        
        
        
        




        ########################################

        # a more space consuming but possibly faster way

        Av = get_A_A_T_v_kfac_v2(v, h, l, params, data_)
        
    else:
        print('error in get_A_A_T unknown: ' + layers_params[l]['name'])
        sys.exit()
    
    return Av


def get_A_A_T(h, l, data_, params):
    
    # return the AVERAGED A_A_T over a minibatch
    
    layers_params = params['layers_params']
    
    device = params['device']
    
    if layers_params[l]['name'] == 'fully-connected':
#         N1 = params['N1']
        
        size_minibatch = h[l].size(0)
    
#         if params['algorithm'] in ['kfac-no-max-no-LM',
#                                    'kfac-warmStart-no-max-no-LM',
#                                    'kfac-warmStart-lessInverse-no-max-no-LM'] or\
#         params['Kron_BFGS_if_homo']:
        if params['Kron_BFGS_if_homo']:
            
            homo_h_l = torch.cat(
                (h[l], torch.ones(size_minibatch, 1, device=device)),
                dim=1
            )
        elif algorithm in ['Kron-BFGS',
                           'Kron-BFGS-no-norm-gate',
                           'Kron-BFGS-no-norm-gate-momentum-s-y',
                           'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                           'Kron-BFGS-no-norm-gate-damping',
                           'Kron-BFGS-no-norm-gate-Shiqian-damping',
                           'Kron-BFGS-wrong',
                           'Kron-BFGS-Hessian-action',
                           'Kron-BFGS-wrong-Hessian-action',
                           'Kron-BFGS-LM',
                           'Kron-BFGS-LM-sqrt']:
            homo_h_l = h[l]
        else:
            print('error: not implemented')
            sys.exit()

        A_j = 1/size_minibatch * torch.mm(homo_h_l.t(), homo_h_l).data

    elif layers_params[l]['name'] in ['1d-conv',
                                      'conv',
                                      'conv-no-activation',
                                      'conv-no-bias-no-activation',]:
    
        



        ####################################

        # benchmark, not use for now

        # get_A_A_T_kfac_v2 is significantly faster than this

    #                         test_2_A_j = get_A_A_T_kfac(a, h, l, params)




        ########################################

        # a more space consuming but possibly faster way
        
#         from utils_git.utils import get_A_A_T_kfac_v2
#         print('need to move the function to this file')

        test_2_A_j = get_A_A_T_kfac_v2(h, l, data_, params)



        #########################################################
        # my original way (worked for in-channel = 1)
        # then improve (worked for 1d-conv)
        # then improve
        # then use F.conv 
        # then revise the part of diff
        # then re-do einsum
        # then re-do slicing



    #                         test_8_A_j = get_A_A_T_kron_bfgs_v5(h, l, params)



        #########################################################
        # my original way (worked for in-channel = 1)
        # then improve (worked for 1d-conv)
        # then improve
        # then use F.conv 
        # then revise the part of diff
        # then re-do einsum

        # clearly slowert than v5



    #                         test_7_A_j = get_A_A_T_kron_bfgs_v4(h, l, params)

        #########################################################
        # my original way (worked for in-channel = 1)
        # then improve (worked for 1d-conv)
        # then improve
        # then use F.conv 
        # then revise the part of diff

        # slower than v4



    #                         test_6_A_j = get_A_A_T_kron_bfgs_v3(h, l, params)

        #########################################################
        # my original way (worked for in-channel = 1)
        # then improve (worked for 1d-conv)
        # then improve
        # then use F.conv 

        # slower than v3 in 2d conv



    #                         test_5_A_j = get_A_A_T_kron_bfgs_v2(h, l, params)



        #########################################################
        # my original way (worked for in-channel = 1)
        # then improve (worked for 1d-conv)
        # then improve

        # this is always slower than get_A_A_T_kron_bfgs_v2

    #                         test_6_A_j = get_A_A_T_kron_bfgs(h, l, params)






        A_j = test_2_A_j
    else:
        print('error in get_A_A_T unknown: ' + layers_params[l]['name'])
        sys.exit()
    
    return A_j

def from_homo_to_weight_and_bias(homo_delta_l, l, params):
    
    layers_params_l = params['layers_params'][l]
    
    delta_l = {}
    if layers_params_l['name'] == 'fully-connected':
        delta_l['W'] = homo_delta_l[:, :-1]
        delta_l['b'] = homo_delta_l[:, -1]
    elif layers_params_l['name'] in ['conv',
                                     'conv-no-activation']:
        # take Fashion-MNIST as an example
        # model_grad_N1[l]['W']: 32 * 1 * 5 * 5
        # model_grad_N1[l]['b']: 32
        # 32: conv_out_channels
        # 1: conv_in_channels
        # 5 * 5: conv_kernel_size



        delta_l['b'] = homo_delta_l[:, -1]
        
#         delta_l['W'] = homo_delta_l[:, :-1].reshape(model_grad_N1[l]['W'].size())
        delta_l['W'] = homo_delta_l[:, :-1].reshape(
            (homo_delta_l.size(0), layers_params_l['conv_in_channels'], layers_params_l['conv_kernel_size'], layers_params_l['conv_kernel_size'])
        )
    
    elif layers_params_l['name'] in ['conv-no-bias-no-activation']:
        
        delta_l['W'] = homo_delta_l.reshape(
            (homo_delta_l.size(0), layers_params_l['conv_in_channels'], layers_params_l['conv_kernel_size'], layers_params_l['conv_kernel_size'])
        )

    elif layers_params_l['name'] in ['BN']:
        
        assert homo_delta_l.size(0) == 2 * layers_params_l['num_features']
        
        delta_l['W'] = homo_delta_l[:layers_params_l['num_features']]
        delta_l['b'] = homo_delta_l[layers_params_l['num_features']:]
        
#         sys.exit()

    else:
        print('Error: unsupported layer when store the data for ' + params['layers_params'][l]['name'])
        sys.exit()
        
    return delta_l

def get_best_params(args, if_plot):
    
    result_path = args['home_path'] + 'result/'
    
    if 'algorithm_dict' in args:
        algorithm_dict = args['algorithm_dict']
    else:
        algorithm_dict = {}
        algorithm_dict['name'] = args['algorithm']
        algorithm_dict['params'] = {}
    
    
    # plot lr vs test accuracy
    if 'list_lr' in args:
        list_lr_try = args['list_lr']
    else:
        '''
        list_lr_try = [\
            0.0005,
            0.001,
            0.005,
            0.01,
            0.05,
            0.1,
            0.2,
            0.4,
            0.5,
            1,
            2,
            4,
            10]
        list_lr_try = [\
            0.0003,
            0.001,
            0.003,
            0.01,
            0.03,
            0.1,
            0.3,
            1,
            3,
            10]
        # list_lr = [\
            # 0.005]
        # print('test lr')
        '''
        
        print('algorithm_dict[name]')
        print(algorithm_dict['name'])
        
        fake_params = {}
        fake_params['algorithm'] = algorithm_dict['name']
        fake_params['if_gpu'] = args['if_gpu']

        test_list_lr_try = os.listdir(result_path + args['dataset'] + '/' + get_name_algorithm(fake_params)[0] + '/')
        
#         print('test_list_lr_try')
#         print(test_list_lr_try)

        test_list_lr_try = [lr_ for lr_ in test_list_lr_try if lr_.startswith('alpha_')]

        test_list_lr_try = [lr_.replace('alpha_','') for lr_ in test_list_lr_try]

#         test_list_lr_try = [float(lr_) for lr_ in test_list_lr_try]

#         print('test_list_lr_try')
#         print(test_list_lr_try)
        
        list_lr_try = test_list_lr_try
        
#         print('list_lr_try')
#         print(list_lr_try)
        
#         print('sorted(list_lr_try, key=float)')
#         print(sorted(list_lr_try, key=float))
        
        list_lr_try = sorted(list_lr_try, key=float)
        
#         list_lr_try = sorted([float(lr) for lr in list_lr_try])
        
#         print('list_lr_try')
#         print(list_lr_try)
        
#         list_lr_try = [str(lr) for lr in list_lr_try]
        
        print('list_lr_try')
        print(list_lr_try)


    

    


    
    
    

    os.chdir(result_path)

    
    list_acc = []
    list_name_result_pkl = []
    list_lr = []
#     for lr, epsilon in itertools.product(list_lr_try, list_epsilon_try):
    for lr in list_lr_try:

        fake_params = {}
        fake_params['alpha'] = lr
        fake_params['N1'] = args['N1']
        fake_params['N2'] = args['N2']

        # fake_params['algorithm'] = args['algorithm']
        fake_params['algorithm'] = algorithm_dict['name']
        fake_params['if_gpu'] = args['if_gpu']

        name_algorithm_with_params = get_name_algorithm_with_params(fake_params)

        path_to_result = args['dataset'] + '/' + name_algorithm_with_params + '/'
        


        if os.path.isdir(path_to_result):
            onlyfiles = [f for f in os.listdir(
            path_to_result) if os.path.isfile(os.path.join(path_to_result, f))]
        else:
            continue
        
#         print('os.listdir(path_to_result)')
#         print(os.listdir(path_to_result))
            
#         print('onlyfiles')
#         print(onlyfiles)

        for f_ in onlyfiles:
            with open(path_to_result + f_, 'rb') as handle:
                
#                 print('path_to_result + f_')
#                 print(path_to_result + f_)
                
#                 print('handle')
#                 print(handle)
                
                record_result = pickle.load(handle)

#             print('params in algorithm_dict')
#             print('params' in algorithm_dict)

            if_candidate_result = True
            if 'params' in algorithm_dict:
                if 'params' in record_result:
                    for key in algorithm_dict['params']:
                        if key in record_result['params']:
                            if algorithm_dict['params'][key] != record_result['params'][key]:
                                if_candidate_result = False
                                break
                        else:
                            if_candidate_result = False
                            break
                else:
                    if algorithm_dict['params'] == {}:
                        # if ('params' in algorithm_dict) and ('params' not in record_result)
                        # and (algorithm_dict['params'] == {})
                        1
                    else:
                        if_candidate_result = False
            else:
                if 'params' in record_result:
                    if_candidate_result = False
                    


            if if_candidate_result == False:
                continue

#             print('record_result.keys()')
#             print(record_result.keys())
            
            if args['tuning_criterion'] in ['test_acc',
                                            'train_acc',
                                            'train_minibatch_acc']:
                
                if args['tuning_criterion'] == 'train_acc':
                    
                    if 'train_acces' in record_result:
                        record_acc = record_result['train_acces']
                    else:
                        print('error: train_acces not in record_result')
                        sys.exit()
                elif args['tuning_criterion'] == 'train_minibatch_acc':
                    
                    assert 'train_minibatch_acces' in record_result
                    
                    record_acc = record_result['train_minibatch_acces']
                    
                elif args['tuning_criterion'] == 'test_acc':
                    if 'test_acces' in record_result:
                        record_acc = record_result['test_acces']
                    else:
                        record_acc = record_result['acces']
                else:
                    print('error: need to check for ' + args['tuning_criterion'])
                    sys.exit()
                
                    

                if args['name_loss'] in ['logistic-regression',
                                         'logistic-regression-sum-loss',
                                         'linear-regression',
                                         'linear-regression-half-MSE']:
                    list_acc.append(np.min(record_acc))
                elif args['name_loss'] in ['multi-class classification',
                                           'binary classification']:
                    list_acc.append(np.max(record_acc))
                else:
                    print('Error: unknown name loss.')
                    sys.exit()
                    
                    
            elif args['tuning_criterion'] == 'train_loss':
                list_acc.append(np.min(record_result['train_losses']))
            elif args['tuning_criterion'] == 'train_minibatch_loss':
                list_acc.append(np.min(record_result['train_unregularized_minibatch_losses']))
            else:
                print('error: unknown tuning criterion for ' + args['tuning_criterion'])
                sys.exit()

            list_name_result_pkl.append(f_)
            list_lr.append(lr)

            
    if list_acc == []:
        return None, None, None
    





    # save the best lr result
    os.chdir(args['home_path'] + 'result/')


    list_acc = np.asarray(list_acc)
    list_name_result_pkl = np.asarray(list_name_result_pkl)
    
    if args['tuning_criterion'] in ['test_acc',
                                    'train_acc',
                                    'train_minibatch_acc']:

        if args['name_loss'] in ['logistic-regression',
                                 'logistic-regression-sum-loss',
                                 'linear-regression',
                                 'linear-regression-half-MSE']:
            # max_indices = np.unravel_index(np.argmin(list_acc, axis=None), list_acc.shape)
            max_indices = np.argmin(list_acc, axis=None)
        elif args['name_loss'] in ['multi-class classification',
                                   'binary classification']:
            # max_indices = np.unravel_index(np.argmax(list_acc, axis=None), list_acc.shape)
            max_indices = np.argmax(list_acc, axis=None)
        else:
            print('Error: unknown name loss when max indices.')
            sys.exit()
            
    elif args['tuning_criterion'] in ['train_loss',
                                      'train_minibatch_loss']:
        
        max_indices = np.argmin(list_acc, axis=None)
            
    else:
        print('error: unknown tuning criterion 2 for ' + args['tuning_criterion'])
        sys.exit()

    print('list_acc[max_indices]')
    print(list_acc[max_indices])

    

    best_lr = list_lr[max_indices]
    best_name_result_pkl = list_name_result_pkl[max_indices]


    # save best params
    fake_params = {}

    # fake_params['algorithm'] = args['algorithm']
    fake_params['algorithm'] = algorithm_dict['name']

    fake_params['if_gpu'] = args['if_gpu']

    name_algorithm, _ = get_name_algorithm(fake_params)
    

    
#     np.savez(
#         args['dataset'] + '/' + name_algorithm + '/' + 'best_params' + '.npz',
#             best_lr=best_lr, best_epsilon=best_epsilon
#     )
    np.savez(
        args['dataset'] + '/' + name_algorithm + '/' + 'best_params' + '.npz',
            best_lr=best_lr
    )

    
    # visualize how to find the best
    if len(list_lr) == len(list_acc) and if_plot:
        

        plt.plot(list_lr, list_acc)
        plt.xlabel('learning rate')
        plt.ylabel('test accuracy')
        plt.xscale('log')
        # plt.title(name_result)
        plt.title(args['dataset'] + '/' + name_algorithm)

        if not os.path.exists(args['home_path'] + 'logs/plot_tune_lr/'):
            os.makedirs(args['home_path'] + 'logs/plot_tune_lr/')
        plt.savefig(args['home_path'] + 'logs/plot_tune_lr/' + str(datetime.datetime.now()) + '.pdf')
        plt.show()
        
#     print('best_lr, best_name_result_pkl')
#     print(best_lr, best_name_result_pkl)


    return best_lr, None, best_name_result_pkl



def get_name_algorithm_with_params(params):
    name_algorithm, _ = get_name_algorithm(params)
    
    if isinstance(params['alpha'], str):
        name_algorithm_with_params = name_algorithm + '/' +\
    'alpha_' + params['alpha'] + '/' +\
    'N1_' + str(params['N1']) + '/' +\
    'N2_' + str(params['N2'])
    else:
        name_algorithm_with_params = name_algorithm + '/' +\
    'alpha_' + str(params['alpha']) + '/' +\
    'N1_' + str(params['N1']) + '/' +\
    'N2_' + str(params['N2'])
    
    return name_algorithm_with_params

def get_name_algorithm(params):
    no_algorithm = 'if_gpu_' + str(params['if_gpu'])
    name_algorithm = params['algorithm'] + '/' + no_algorithm
    return name_algorithm, no_algorithm

def get_model_grad(model, params):
    model_grad_torch = []
    for l in range(model.numlayers):
        # model_grad_l = {}
        model_grad_torch_l = {}
        for key in model.layers_weight[l]:
            
            
            
            model_grad_torch_l[key] = copy.deepcopy(model.layers_weight[l][key].grad)
            
#             print('model.layers_weight[l][key].size()')
#             print(model.layers_weight[l][key].size())
#             print('model.layers_weight[l][key]')
#             print(model.layers_weight[l][key])
#             print('model.layers_weight[l][key].grad.size()')
#             print(model.layers_weight[l][key].grad.size())
#             print('model_grad_torch_l[key].size()')
#             print(model_grad_torch_l[key].size())
            
        # model_grad.append(model_grad_l)
        model_grad_torch.append(model_grad_torch_l)
        
    return model_grad_torch


    

def get_statistics(X_train):
    # X_train: N * m

    print('\n')
    print('max value:')
    print(np.max(X_train))
    print('min value:')
    print(np.min(X_train))

    print('max of per feature mean:')
    print(np.max(np.mean(X_train, axis=0)))
    print('min of per feature mean:')
    print(np.min(np.mean(X_train, axis=0)))

    print('max of per feature std:')
    print(np.max(np.std(X_train, axis=0)))
    print('min of per feature std:')
    print(np.min(np.std(X_train, axis=0)))

    print('\n')
    
    
def sample_from_pred_dist(z, params):
    
    name_loss = params['name_loss']
    N2_index = params['N2_index']

    if name_loss == 'multi-class classification':
        from torch.utils.data import WeightedRandomSampler

#         pred_dist_N2 = F.softmax(a[-1][N2_index], dim=1)
        pred_dist_N2 = F.softmax(z[N2_index], dim=1)

        t_mb_pred_N2 = list(WeightedRandomSampler(pred_dist_N2, 1))
        
        
#         print('torch.tensor(t_mb_pred_N2).to(params[device])')
#         print(torch.tensor(t_mb_pred_N2).to(params['device']))
#         print('torch.tensor(t_mb_pred_N2).to(params[device]).size()')
#         print(torch.tensor(t_mb_pred_N2).to(params['device']).size())
#         print('torch.tensor(t_mb_pred_N2).to(params[device]).dtype')
#         print(torch.tensor(t_mb_pred_N2).to(params['device']).dtype)
        
        t_mb_pred_N2 = torch.tensor(t_mb_pred_N2)
        t_mb_pred_N2 = t_mb_pred_N2.squeeze(dim=1)
        # this will gives a int64 tensor
        
#         print('test_t_mb_pred_N2')
#         print(test_t_mb_pred_N2)
#         print('test_t_mb_pred_N2.size()')
#         print(test_t_mb_pred_N2.size())
#         print('test_t_mb_pred_N2.dtype')
#         print(test_t_mb_pred_N2.dtype)
        

        
#         t_mb_pred_N2 = np.asarray(t_mb_pred_N2)
#         t_mb_pred_N2 = np.squeeze(t_mb_pred_N2, axis=1)
#         t_mb_pred_N2 = torch.from_numpy(t_mb_pred_N2).to(params['device'])


    elif name_loss == 'binary classification':

        pred_dist_N2 = torch.sigmoid(a[-1][N2_index]).cpu().data.numpy()

        t_mb_pred_N2 = np.random.binomial(n=1, p=pred_dist_N2)

        t_mb_pred_N2 = np.squeeze(t_mb_pred_N2, axis=1)

        print('check if need long')
        sys.exit()

        t_mb_pred_N2 = torch.from_numpy(t_mb_pred_N2).long()



    elif name_loss in ['logistic-regression',
                       'logistic-regression-sum-loss']:
        # pred_dist_N2 = torch.sigmoid(a[-1][N2_index]).cpu().data.numpy()
#         pred_dist_N2 = torch.sigmoid(a[-1][N2_index]).data
        pred_dist_N2 = torch.sigmoid(z[N2_index]).data

        if not (torch.max(pred_dist_N2) <= 1):
            print('torch.max(pred_dist_N2)')
            print(torch.max(pred_dist_N2))
            print('a[-1][N2_index]')
            print(a[-1][N2_index])
            print('get_if_nan(model.layers_weight)')
            print(get_if_nan(model.layers_weight))
            for l in range(len(model.layers_weight)):
                for key in model.layers_weight[l]:
                    print('torch.max(model.layers_weight[l][key])')
                    print(torch.max(model.layers_weight[l][key]))
                    print('torch.min(model.layers_weight[l][key])')
                    print(torch.min(model.layers_weight[l][key]))
        if not (torch.min(pred_dist_N2) >= 0):
            print('torch.min(pred_dist_N2)')
            print(torch.min(pred_dist_N2))
            print('a[-1][N2_index]')
            print(a[-1][N2_index])
            print('get_if_nan(model.layers_weight)')
            print(get_if_nan(model.layers_weight))
            for l in range(len(model.layers_weight)):
                for key in model.layers_weight[l]:
                    print('torch.max(model.layers_weight[l][key])')
                    print(torch.max(model.layers_weight[l][key]))
                    print('torch.min(model.layers_weight[l][key])')
                    print(torch.min(model.layers_weight[l][key]))

        # t_mb_pred_N2 = np.random.binomial(n=1, p=pred_dist_N2)
        t_mb_pred_N2 = torch.distributions.Bernoulli(pred_dist_N2).sample()

#                 t_mb_pred_N2 = t_mb_pred_N2.long().to(params['device'])
        t_mb_pred_N2 = t_mb_pred_N2
    
    elif name_loss == 'linear-regression':
        # see register_normal_predictive_distribution in
        # https://github.com/tensorflow/kfac/blob/master/kfac/python/ops/layer_collection.py
        
        # oevrall loss = 2_norm / #minibacth / # feature
        # loss for one data = 2_norm / # feature
        # let coeff = 1, 1 / (2 * var) = 1 / #feature
        # => 2 * var = # feature
        # => var = #feature / 2
        
        # oevrall loss = 2_norm / #minibacth
        # loss for one data = 2_norm
        # let coeff = 1, 1 / (2 * var) = 1
        # => 2 * var = 1
        # => var = 1 / 2
        
        # #feature = z.size(1)
        

#         t_mb_pred_N2 = torch.distributions.Normal(loc=z[N2_index], scale=z.size(1)/2).sample()
        
        t_mb_pred_N2 = torch.distributions.Normal(loc=z[N2_index], scale=1/2).sample()
    
    elif name_loss == 'linear-regression-half-MSE':
        
        # oevrall loss = 2_norm / #minibacth / 2
        # loss for one data = 2_norm / 2
        # let coeff = 1, 1 / (2 * var) = 1 / 2
        # => 2 * var = 2
        # => var = 1
        
        t_mb_pred_N2 = torch.distributions.Normal(loc=z[N2_index], scale=1).sample()
        
#         sys.exit()

    else:
        print('Error: sampling not supported.')
        sys.exit()
        
    t_mb_pred_N2 = t_mb_pred_N2.to(params['device'])
        
    return t_mb_pred_N2


def get_second_order_caches(z, a, h, data_, params):
        
    matrix_name = params['matrix_name']
    model = data_['model']
        
    N1 = params['N1']
    N2 = params['N2']
    
    assert N1 == N2
    
    
    
    if matrix_name in ['Fisher',
                       'EF']:

#     N2_index = np.random.permutation(N1)[:N2]
        N2_index = np.random.permutation(N1)
    elif matrix_name == 'Fisher-correct':
        N2_index = list(range(N1))
    else:
        
        print('matrix_name')
        print(matrix_name)
        
        sys.exit()
    
        
    
    

    X_mb = data_['X_mb']

    
    
    assert params['if_different_minibatch'] == False

    if params['if_different_minibatch']:
        
        print('error: should not reach here')
        sys.exit()
        
        X_mb_N2, _ = data_['dataset'].train.next_batch(N2)
        X_mb_N2 = torch.from_numpy(X_mb_N2).to(params['device'])
        # if name_dataset == 'MNIST-autoencoder':
            # t_mb = X_mb
    else:
        
        if matrix_name in ['Fisher',
                           'EF']:
        
            X_mb_N2 = X_mb[N2_index]
        elif matrix_name == 'Fisher-correct':
            X_mb_N2 = X_mb
        else:
            print('matrix_name')
            print(matrix_name)
            sys.exit()
            
        
    params['N2_index'] = N2_index
    
    data_['X_mb_N1'] = X_mb
    data_['X_mb_N2'] = X_mb_N2

    










    if matrix_name == 'EF':
        if params['if_different_minibatch']:
            print('error: need to check for different minibatch when EF')
            sys.exit()
        else:



            t_mb = data_['t_mb']

            data_['t_mb_pred_N2'] = t_mb[N2_index]

#                 data_['a_grad_N2'] = [N2 * (a_l.grad)[N2_index] for a_l in a]
            data_['mean_a_grad_N2'] = [torch.mean(N2 * (a_l.grad)[N2_index], dim=0).data for a_l in a]
            # use in K-BFGS, for BFGS on G

#                 for h_l in h:
#                     print('h_l.size()')
#                     print(h_l.size())


            data_['h_N2'] = [h_l[N2_index].data if len(h_l) else [] for h_l in h]

#                 data_['a_N2'] = [a_l[N2_index].data for a_l in a]
            data_['mean_a_N2'] = [torch.mean(a_l[N2_index], dim=0).data for a_l in a]
    elif matrix_name == 'Fisher':
        
        if params['i'] % params['shampoo_update_freq'] != 0:
            return data_


        t_mb_pred_N2 = sample_from_pred_dist(z, params)


        data_['t_mb_pred_N2'] = t_mb_pred_N2
        
        z, a_N2, h_N2 = model.forward(X_mb_N2)

        

        reduction = 'mean'
        loss = get_loss_from_z(model, z, t_mb_pred_N2, reduction) # this is unregularized loss


        model.zero_grad()

        loss.backward()

        l = -1
        for a_l in a_N2:
            l += 1

            if torch.sum(a_l.grad != a_l.grad):
                print('nan in a_l.grad')
                print('l')
                print(l)
                print('torch.sum(a_N2[l].grad, dim=0)')
                print(torch.sum(a_N2[l].grad, dim=0))
                print('get_if_nan(model.layers_weight)')
                print(get_if_nan(model.layers_weight))
                print('torch.sum(model.layers_weight[l][W].grad != model.layers_weight[l][W].grad)')
                print(torch.sum(model.layers_weight[l]['W'].grad != model.layers_weight[l]['W'].grad))
                print('a_l.grad')
                print(a_l.grad)
                print('t_mb_pred_N2')
                print(t_mb_pred_N2)
                print('t_mb_pred_N2.size()')
                print(t_mb_pred_N2.size())
                print('torch.sum(t_mb_pred_N2 != t_mb_pred_N2)')
                print(torch.sum(t_mb_pred_N2 != t_mb_pred_N2))
                print('get_if_nan(model.layers_weight)')
                print(get_if_nan(model.layers_weight))
                print('torch.sum(z != z)')
                print(torch.sum(z != z))





        data_['a_grad_N2'] = [N2 * (a_l.grad) for a_l in a_N2]
        # not used for the algorithms that we currently care about

        data_['h_N2'] = h_N2

        data_['a_N2'] = a_N2



        if params['if_model_grad_N2']:
            # this is unregularized grad
            data_['model_grad_N2'] = get_model_grad(model, params)
            
    elif matrix_name == 'Fisher-correct':
        
#         assert params['algorithm'] in params['list_algorithm_shampoo']
        
        if params['algorithm'] in params['list_algorithm_shampoo'] and params['i'] % params['shampoo_update_freq'] != 0:
            return data_
        
        if params['algorithm'] in params['list_algorithm_kfac'] and params['i'] % params['kfac_cov_update_freq'] != 0:
            return data_

#         z, a_N2, h_N2 = model.forward(X_mb_N2)

        a_N2 = a
        h_N2 = h

        t_mb_pred_N2 = sample_from_pred_dist(z, params)


        data_['t_mb_pred_N2'] = t_mb_pred_N2
        
        

        

        reduction = 'mean'
        loss = get_loss_from_z(model, z, t_mb_pred_N2, reduction) # this is unregularized loss


        model.zero_grad()

        loss.backward()
#         loss.backward(retain_graph=True)
        
        if params['algorithm'] in ['Fisher-BD',
                                   'kfac-correctFisher-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',]:


#         data_['a_grad_N2'] = [N2 * (a_l.grad) for a_l in a_N2]
        # not used for the algorithms that we currently care about

            data_['h_N2'] = h_N2

            data_['a_N2'] = a_N2



        if params['if_model_grad_N2']:
            # this is unregularized grad
            data_['model_grad_N2'] = get_model_grad(model, params)

    elif matrix_name == 'GN':
        # print('error: need to check for different minibatch')
        # sys.exit()

        # test_time_wall_clock = time.time()

        data_['z_N2'] = a[-1][N2_index]

        m_L = data_['model'].layersizes[-1]
        params['m_L'] = m_L

        # X_mb_N2 = X_mb[N2_index]

        # numlayers = params['numlayers']
        # m_L = params['m_L']
        N2 = params['N2']

        list_a = [ [] for i in range(m_L) ]

        a_grad_momentum = []
        for l in range(model.numlayers):
            a_grad_momentum.append(
                torch.zeros(m_L, N2, data_['model'].layersizes[l+1], device=params['device']))

        for i in range(m_L):



            # z, list_a[i], h = model.forward(X_mb[N2_index])
            z, a, h = model.forward(X_mb[N2_index])

            # print('time for GN after forward:')
            # print(time.time() - test_time_wall_clock)

            fake_loss = torch.sum(z[:, i])

            test_time_zero = time.time()

            # model = get_model_grad_zerod(model)
            model.zero_grad()

            # print('time for GN zero:')
            # print(time.time() - test_time_zero)

            fake_loss.backward()

            # print('time for GN before second zero:')
            # print(time.time() - test_time_wall_clock)

            for l in range(model.numlayers):
                a_grad_momentum[l][i] = a[l].grad

        # print('time for GN after backward:')
        # print(time.time() - test_time_wall_clock)

        # a_grad_momentum = []
        # for l in range(model.numlayers):
            # a_grad_momentum.append(torch.stack([a[l].grad for a in list_a]))

        # for l in range(model.numlayers):
            # a_grad_momentum[l] = torch.stack([a[l].grad for a in list_a])

            # for i in range(m_L):
                # a_grad_momentum[l][i] = list_a[i][l].grad

        # test_start_time_h = time.process_time()

        h_momentum = [hi.data for hi in h]

        # test_time_1 = time.process_time() - test_start_time_h
        # print('test_time_1')
        # print(test_time_1)

        # print('a_grad_momentum[1].size()')
        # print(a_grad_momentum[1].size())

        for l in range(model.numlayers):
            a_grad_momentum[l] = a_grad_momentum[l].permute(1, 0, 2)

        # print('a_grad_momentum[1].size()')
        # print(a_grad_momentum[1].size())


        data_['a_grad'] = a_grad_momentum
        data_['h'] = h_momentum



        # print('h_momentum[1].size()')
        # print(h_momentum[1].size())

        #====

        # test_start_time_h = time.process_time()



        # test_time_2 = time.process_time() - test_start_time_h
        # print('test_time_2')
        # print(test_time_2)

        # print('test GN')
        # sys.exit()






        # data_['GN_cache'] = GN_cache

        # print('time for GN:')
        # print(time.time() - test_time_wall_clock)
        # print('\n')

    else:
        print('Error: unknown matrix name for ' + matrix_name)
        sys.exit()
            
    return data_

def get_Adam_direction(p, data_, params):
    beta_1 = params['Adam_beta_1']
    beta_2 = params['Adam_beta_2']
    epsilon = params['Adam_epsilon']

    i = params['i'] + 1

    # model_grad = data_['model_grad']
    

    data_['model_grad_Adam_momentum_1'] = get_plus(
    get_multiply_scalar(beta_1, data_['model_grad_Adam_momentum_1']),
    get_multiply_scalar(1 - beta_1, p))

    data_['model_grad_Adam_momentum_2'] = get_plus(
    get_multiply_scalar(beta_2, data_['model_grad_Adam_momentum_2']),
    get_multiply_scalar(1 - beta_2, 
                 get_square(p)))

    hat_m = get_multiply_scalar(1 / (1 - beta_1 ** i), data_['model_grad_Adam_momentum_1'])
    hat_v = get_multiply_scalar(1 / (1 - beta_2 ** i), data_['model_grad_Adam_momentum_2'])


    p_Adam = get_divide(
        hat_m,
        get_plus_scalar(epsilon,
                        get_sqrt(hat_v)))

    return p_Adam, data_

def get_name_loss(dataset):
    if dataset in ['MNIST',
                   'MNIST-no-regularization',
                   'MNIST-N1-1000',
                   'MNIST-one-layer',
                   'DownScaledMNIST-no-regularization',
                   'DownScaledMNIST-N1-1000-no-regularization',
                   'CIFAR',
                   'CIFAR-deep',
                   'CIFAR-10-vgg16',
                   'CIFAR-10-vgg11',
                   'CIFAR-10-NoAugmentation-vgg11',
                   'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                   'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                   'CIFAR-10-vgg16-GAP',
                   'CIFAR-10-AllCNNC',
                   'CIFAR-10-N1-128-AllCNNC',
                   'CIFAR-10-N1-512-AllCNNC',
                   'CIFAR-10-ConvPoolCNNC',
                   'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                   'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                   'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                   'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                   'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                   'CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization',
                   'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization',
                   'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                   'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                   'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                   'CIFAR-10-onTheFly-ResNet32-BN',
                   'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                   'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                   'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                   'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                   'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                   'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                   'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                   'CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',
                   'CIFAR-100',
                   'CIFAR-100-NoAugmentation',
                   'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                   'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                   'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                   'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                   'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                   'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                   'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                   'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                   'CIFAR-100-onTheFly-AllCNNC',
                   'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                   'CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                   'CIFAR-100-onTheFly-ResNet34-BN',
                   'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                   'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                   'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                   'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                   'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',
                   'Fashion-MNIST',
                   'Fashion-MNIST-N1-60',
                   'Fashion-MNIST-N1-60-no-regularization',
                   'Fashion-MNIST-N1-256-no-regularization',
                   'Fashion-MNIST-GAP-N1-60-no-regularization',
                   'STL-10-simple-CNN',
                   'Subsampled-ImageNet-simple-CNN',
                   'Subsampled-ImageNet-vgg16']:
        return 'multi-class classification'
        
    elif dataset == 'webspam':
        return 'binary classification'
    
    elif dataset in ['MNIST-autoencoder',
                     'MNIST-autoencoder-no-regularization',
                     'MNIST-autoencoder-N1-1000',
                     'MNIST-autoencoder-N1-1000-no-regularization',
                     'CURVES-autoencoder-no-regularization',
                     'CURVES-autoencoder',
                     'CURVES-autoencoder-Botev',
                     'CURVES-autoencoder-shallow',
                     'FACES-autoencoder',
                     'FACES-autoencoder-no-regularization']:
        return 'logistic-regression'
    
    elif dataset in ['MNIST-autoencoder-N1-1000-sum-loss',
                     'MNIST-autoencoder-N1-1000-sum-loss-no-regularization',
                     'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization',
                     'MNIST-autoencoder-relu-N1-1000-sum-loss',
                     'MNIST-autoencoder-relu-N1-100-sum-loss',
                     'MNIST-autoencoder-relu-N1-500-sum-loss',
                     'MNIST-autoencoder-relu-N1-1-sum-loss',
                     'MNIST-autoencoder-reluAll-N1-1-sum-loss',
                     'FACES-autoencoder-sum-loss-no-regularization',
                     'FACES-autoencoder-relu-sum-loss-no-regularization',
                     'FACES-autoencoder-relu-sum-loss',
                     'FACES-autoencoder-sum-loss',
                     'CURVES-autoencoder-sum-loss-no-regularization',
                     'CURVES-autoencoder-sum-loss',
                     'CURVES-autoencoder-relu-sum-loss-no-regularization',
                     'CURVES-autoencoder-relu-sum-loss',
                     'CURVES-autoencoder-relu-N1-100-sum-loss',
                     'CURVES-autoencoder-relu-N1-500-sum-loss',
                     'CURVES-autoencoder-Botev-sum-loss-no-regularization',]:
        
        return 'logistic-regression-sum-loss'
    
    elif dataset in ['sythetic-linear-regression',
                     'sythetic-linear-regression-N1-1']:
        return 'linear-regression'
    elif dataset in ['FacesMartens-autoencoder-relu',
                     'FacesMartens-autoencoder-relu-no-regularization',
                     'FacesMartens-autoencoder-relu-N1-500',
                     'FacesMartens-autoencoder-relu-N1-100']:
        return 'linear-regression-half-MSE'
    else:
        print('Error: Problem not specified.')
        sys.exit()
        
def from_dataset_to_N1_N2(args):
    
    if not 'tau' in args:
        args['tau'] = 10**(-5) # https://arxiv.org/pdf/1503.05671.pdf
    
#     if 'N1' in args or 'N2' in args:
#         print('error: N1, N2 not automated')
#         sys.exit()
#     else:

    if 1:
        if args['dataset'] == 'MNIST-N1-1000':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'Fashion-MNIST':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'Fashion-MNIST-N1-60':
            args['N1'] = 60
            args['N2'] = 60
        elif args['dataset'] == 'Fashion-MNIST-N1-60-no-regularization':
            args['N1'] = 60
            args['N2'] = 60
            args['tau'] = 0
        elif args['dataset'] == 'Fashion-MNIST-N1-256-no-regularization':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 0
        elif args['dataset'] == 'Fashion-MNIST-GAP-N1-60-no-regularization':
            args['N1'] = 60
            args['N2'] = 60
            args['tau'] = 0
        elif args['dataset'] == 'webspam':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'MNIST':
            args['N1'] = 60
            args['N2'] = 60
        elif args['dataset'] == 'MNIST-no-regularization':
            args['N1'] = 60
            args['N2'] = 60
            args['tau'] = 0
        elif args['dataset'] == 'DownScaledMNIST-no-regularization':
            args['N1'] = 60
            args['N2'] = 60
            args['tau'] = 0
        elif args['dataset'] == 'DownScaledMNIST-N1-1000-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'MNIST-autoencoder':
            args['N1'] = 60
            args['N2'] = 60
        elif args['dataset'] == 'MNIST-autoencoder-no-regularization':
            args['N1'] = 60
            args['N2'] = 60
            args['tau'] = 0
        elif args['dataset'] == 'MNIST-autoencoder-N1-1000':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'MNIST-autoencoder-N1-1000-sum-loss':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'MNIST-autoencoder-N1-1000-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'MNIST-autoencoder-N1-1000-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'MNIST-autoencoder-relu-N1-1000-sum-loss':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'MNIST-autoencoder-relu-N1-100-sum-loss':
            args['N1'] = 100
            args['N2'] = 100
            args['tau'] = 10**(-5)
        elif args['dataset'] == 'MNIST-autoencoder-relu-N1-500-sum-loss':
            args['N1'] = 500
            args['N2'] = 500
        elif args['dataset'] == 'MNIST-autoencoder-relu-N1-1-sum-loss':
            args['N1'] = 1
            args['N2'] = 1
        elif args['dataset'] == 'MNIST-autoencoder-reluAll-N1-1-sum-loss':
            args['N1'] = 1
            args['N2'] = 1
        elif args['dataset'] == 'CURVES-autoencoder':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'CURVES-autoencoder-Botev':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'CURVES-autoencoder-Botev-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'CURVES-autoencoder-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'CURVES-autoencoder-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'CURVES-autoencoder-relu-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'CURVES-autoencoder-relu-sum-loss':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'CURVES-autoencoder-relu-N1-500-sum-loss':
            args['N1'] = 500
            args['N2'] = 500
        elif args['dataset'] == 'CURVES-autoencoder-relu-N1-100-sum-loss':
            args['N1'] = 100
            args['N2'] = 100
            args['tau'] = 10**(-5)
        elif args['dataset'] == 'CURVES-autoencoder-sum-loss':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'FACES-autoencoder':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'FACES-autoencoder-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'FACES-autoencoder-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'FACES-autoencoder-relu-sum-loss-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'FACES-autoencoder-relu-sum-loss':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'FacesMartens-autoencoder-relu':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'FacesMartens-autoencoder-relu-no-regularization':
            args['N1'] = 1000
            args['N2'] = 1000
            args['tau'] = 0
        elif args['dataset'] == 'FacesMartens-autoencoder-relu-N1-500':
            args['N1'] = 500
            args['N2'] = 500
        elif args['dataset'] == 'FacesMartens-autoencoder-relu-N1-100':
            args['N1'] = 100
            args['N2'] = 100
            args['tau'] = 10**(-5)
        elif args['dataset'] == 'FACES-autoencoder-sum-loss':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'sythetic-linear-regression':
            args['N1'] = 900
            args['N2'] = 900
        elif args['dataset'] == 'sythetic-linear-regression-N1-1':
            args['N1'] = 1
            args['N2'] = 1
        elif args['dataset'] == 'MNIST-one-layer':
            args['N1'] = 60000
            args['N2'] = 60000
        elif args['dataset'] == 'UCI-HAR':
            args['N1'] = 32
            args['N2'] = 32
        elif args['dataset'] == 'CIFAR-100':
            
            print('error: data is augmented but not on the fly')
            sys.exit()
            
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'CIFAR-100-NoAugmentation':
            
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout':
            
            args['N1'] = 1000
            args['N2'] = 1000
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout':
            
            args['N1'] = 256
            args['N2'] = 256
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout':
            
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization':
            
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization':
            
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN':
            
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization':
            
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization':
            
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout':
            
            args['N1'] = 256
            args['N2'] = 256
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization':
            
            args['N1'] = 256
            args['N2'] = 256
            
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine':
            
            args['N1'] = 256
            args['N2'] = 256
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-AllCNNC':
            
            args['N1'] = 256
            args['N2'] = 256
            
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-deep':
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-vgg11':
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-NoAugmentation-vgg11':
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-vgg11-test':
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-vgg16':
            # https://arxiv.org/pdf/1910.05446.pdf
            
            print('adaptive avg pool is not neede for CIFAR10 + vgg16')
            sys.exit()
            
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
        elif args['dataset'] == 'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout':
            
            print('error: should use CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout')
            sys.exit()
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 512
            args['N2'] = 512
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization':
            
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0
            
        elif args['dataset'] == 'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
        elif args['dataset'] == 'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
        elif args['dataset'] == 'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
        elif args['dataset'] == 'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005 # https://arxiv.org/pdf/1910.05446.pdf
        elif args['dataset'] == 'CIFAR-10-vgg16-GAP':
            
            print('GAP is not needed for CIFAR10 + vgg16')
            sys.exit()
            
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-ResNet32-BNNoAffine':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-ResNet32-BN':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            
            args['tau'] = 0
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            
            # sec 4.2 of https://arxiv.org/pdf/1512.03385.pdf
            args['tau'] = 0.0001 # inherit from VGG
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-ResNet34-BNNoAffine':
            # https://zhenye-na.github.io/2018/10/07/pytorch-resnet-cifar100.html
            
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 1e-5
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN':
            # https://zhenye-na.github.io/2018/10/07/pytorch-resnet-cifar100.html
            
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 1e-5
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut':
            # https://zhenye-na.github.io/2018/10/07/pytorch-resnet-cifar100.html
            
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 1e-5
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly':
            # https://zhenye-na.github.io/2018/10/07/pytorch-resnet-cifar100.html
            
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 1e-5
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias':
            # https://zhenye-na.github.io/2018/10/07/pytorch-resnet-cifar100.html
            
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 1e-5
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias':
            # https://github.com/weiaicunzai/pytorch-cifar100/blob/master/train.py
            # https://github.com/bearpaw/pytorch-classification/blob/master/cifar.py
            
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 5e-4
            
        elif args['dataset'] == 'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias':
            # https://github.com/weiaicunzai/pytorch-cifar100/blob/master/train.py
            # https://github.com/bearpaw/pytorch-classification/blob/master/cifar.py
            
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 5e-4
            
        elif args['dataset'] == 'CIFAR-10-AllCNNC':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-N1-128-AllCNNC':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 128
            args['N2'] = 128
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-N1-512-AllCNNC':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 512
            args['N2'] = 512
            args['tau'] = 0.0005
        elif args['dataset'] == 'CIFAR-10-ConvPoolCNNC':
            # https://arxiv.org/pdf/1910.05446.pdf
            args['N1'] = 256
            args['N2'] = 256
            args['tau'] = 0.0005
        elif args['dataset'] == 'STL-10-simple-CNN':
            args['N1'] = 1000
            args['N2'] = 1000
        elif args['dataset'] == 'Subsampled-ImageNet-simple-CNN':
            args['N1'] = 100
            args['N2'] = 100
        elif args['dataset'] == 'Subsampled-ImageNet-vgg16':
            args['N1'] = 10
            args['N2'] = 10
        else:
            print('error: unknown dataset for ' + args['dataset'])
            sys.exit()
    return args

def tune_lr(args):
    
    assert 'max_epoch/time' in args
    assert 'record_epoch' in args
    assert 'if_test_mode' in args
    
    assert 'if_grafting' in args
    assert 'weight_decay' in args

    

    args['name_loss'] = get_name_loss(args['dataset'])
    args = from_dataset_to_N1_N2(args)
    args = add_matrix_name_to_args(args)
    args = add_some_if_record_to_args(args)
    
#     args['momentum_gradient_rho'] = 0.9
    args['lambda_'] = 1
    
    if args['if_auto_tune_lr']:
        
        assert len(args['list_lr']) == 2
        
        for learning_rate in args['list_lr']:
            args['alpha'] = learning_rate
            name_result, data_, params_saved = train(args)
            
            data_ = None
            torch.cuda.empty_cache()
            
            
            
        list_lr_tried = args['list_lr']
        
        while 1:
            fake_args = {}
            fake_args['home_path'] = args['home_path']
            fake_args['algorithm_dict'] = {}
            fake_args['algorithm_dict']['name'] = args['algorithm']
            fake_args['algorithm_dict']['params'] = params_saved
            fake_args['if_gpu'] = args['if_gpu']
            fake_args['dataset'] = args['dataset']
            fake_args['N1'] = args['N1']
            fake_args['N2'] = args['N2']
            fake_args['name_loss'] = args['name_loss']

            fake_args['tuning_criterion'] = args['tuning_criterion']
            fake_args['list_lr'] = list_lr_tried

            best_lr, _, best_name_result_pkl = get_best_params(fake_args, False)

            print('best_lr')
            print(best_lr)
            

            
            learning_rate = get_next_lr(list_lr_tried, best_lr)
            
            if learning_rate < 0:
                break
            else:
                args['alpha'] = learning_rate
                name_result, data_, params_saved = train(args)
                
                data_ = None
                torch.cuda.empty_cache()
                
                
                
                if learning_rate < min(list_lr_tried):
                    list_lr_tried = [learning_rate] + list_lr_tried
                elif learning_rate > max(list_lr_tried):
                    list_lr_tried = list_lr_tried + [learning_rate]
                else:
                    print('there is an error')
                    sys.exit()
                    
                    
        print('list_lr_tried, best_lr')
        print(list_lr_tried, best_lr)
        
        tune_lr_output = {}
        tune_lr_output['best_lr'] = best_lr
        tune_lr_output['list_lr_tried'] = list_lr_tried
    else:


        for learning_rate in args['list_lr']:
            args['alpha'] = learning_rate
            name_result, data_, _ = train(args)
            
            data_ = None
            torch.cuda.empty_cache()
            
        tune_lr_output = None


#     return data_
    return tune_lr_output

    
    

def get_if_stop(args, i, iter_per_epoch, timesCPU):
    if args['if_max_epoch']:
        # stop by epoch
        
#         print('i')
#         print(i)
        
#         print('int(args[max_epoch/time] * iter_per_epoch)')
#         print(int(args['max_epoch/time'] * iter_per_epoch))
        
#         sys.exit()
        
        if i < int(args['max_epoch/time'] * iter_per_epoch):
            return False
        else:
            return True
    else:
        # stop by time
        
#         print('timesCPU[-1]')
#         print(timesCPU[-1])
        
#         print('args[max_epoch/time]')
#         print(args['max_epoch/time'])
        
        if timesCPU[-1] < args['max_epoch/time']:
            return False
        else:
            return True

def get_erf(p):
    sign_ = []
    for l in range(len(p)):
        sign_l = {}
        for key in p[l]:
            sign_l[key] = torch.erf(p[l][key])
        sign_.append(sign_l)
    return sign_

def get_erf_approx(p):
    sign_ = []
    for l in range(len(p)):
        sign_l = {}
        for key in p[l]:
            sign_l[key] = 1 / (1 + p[l][key]**2)
        sign_.append(sign_l)
    return sign_

def get_reciprocal(p):
    sign_ = []
    for l in range(len(p)):
        sign_l = {}
        for key in p[l]:
            sign_l[key] = 1 / p[l][key]
        sign_.append(sign_l)
    return sign_

def get_sign(p):
    sign_ = []
    for l in range(len(p)):
        sign_l = {}
        for key in p[l]:
            sign_l[key] = np.sign(p[l][key])
        sign_.append(sign_l)
    return sign_


def get_sign_torch(p):
    sign_ = []
    for l in range(len(p)):
        sign_l = {}
        for key in p[l]:
            sign_l[key] = torch.sign(p[l][key])
        sign_.append(sign_l)
    return sign_

'''
def get_zero(params):
    layers_params = params['layers_params']
    
    delta = []
    for l in range(len(layers_params)):
        delta_l = {}
        if layers_params[l]['name'] == 'fully-connected':
            delta_l['W'] = np.zeros((layers_params[l]['output_size'], layers_params[l]['input_size']))
            delta_l['b'] = np.zeros(layers_params[l]['output_size'])
        elif layers_params[l]['name'] == 'conv':
            delta_l['W'] = np.zeros((layers_params[l]['conv_out_channels'],
                                     layers_params[l]['conv_in_channels'],
                                     layers_params[l]['conv_kernel_size'],
                                     layers_params[l]['conv_kernel_size']))
            delta_l['b'] = np.zeros(layers_params[l]['conv_out_channels'])
        elif layers_params[l]['name'] == '1d-conv':
            delta_l['W'] = np.zeros((layers_params[l]['conv_out_channels'],
                                     layers_params[l]['conv_in_channels'],
                                     layers_params[l]['conv_kernel_size']))
            delta_l['b'] = np.zeros(layers_params[l]['conv_out_channels'])
        else:
            print('Error: layers unsupported when get zero for ' + layers_params[l]['name'])
            sys.exit()
        delta.append(delta_l)
        
    return delta
'''

def get_zero_torch(params):
    layers_params = params['layers_params']
    device = params['device']
    
    delta = []
    for l in range(len(layers_params)):
        delta_l = {}
        if layers_params[l]['name'] == 'fully-connected':
            delta_l['W'] = torch.zeros(layers_params[l]['output_size'], layers_params[l]['input_size'], device=device)
            delta_l['b'] = torch.zeros(layers_params[l]['output_size'], device=device)
        elif layers_params[l]['name'] in ['conv',
                                          'conv-no-activation',
                                          'conv-no-bias-no-activation']:
            delta_l['W'] = torch.zeros(layers_params[l]['conv_out_channels'],
                                     layers_params[l]['conv_in_channels'],
                                     layers_params[l]['conv_kernel_size'],
                                     layers_params[l]['conv_kernel_size'], device=device)
            if layers_params[l]['name'] in ['conv',
                                            'conv-no-activation',]:
                delta_l['b'] = torch.zeros(layers_params[l]['conv_out_channels'], device=device)
        elif layers_params[l]['name'] == '1d-conv':
            delta_l['W'] = torch.zeros(layers_params[l]['conv_out_channels'],
                                     layers_params[l]['conv_in_channels'],
                                     layers_params[l]['conv_kernel_size'],
                                       device=device)
            delta_l['b'] = torch.zeros(layers_params[l]['conv_out_channels'], device=device)
        elif layers_params[l]['name'] == 'BN':
            
#             print('layers_params[l][num_features]')
#             print(layers_params[l]['num_features'])
            
#             sys.exit()
            
            delta_l['W'] = torch.zeros(layers_params[l]['num_features'], device=device)
            
            delta_l['b'] = torch.zeros(layers_params[l]['num_features'], device=device)
            
            
        else:
            print('Error: layers unsupported when get zero for ' + layers_params[l]['name'])
            sys.exit()
        delta.append(delta_l)
        
    return delta


def get_full_grad(model, x, t, params):
    N1 = params['N1']
    reduction = 'mean'
    i = 0
    while (i+1) * N1 <= len(x):
        loss = get_regularized_loss_from_x(model, x[i*N1: (i+1)*N1], t[i*N1: (i+1)*N1], reduction)
        model.zero_grad()
        loss.backward()
        grad_i = get_model_grad(model, params)
        if i == 0:
            full_grad = grad_i
        else:
            full_grad = get_plus_torch(full_grad, grad_i)
        i += 1
    full_grad = get_multiply_scalar(1./i, full_grad)

    
    
    return full_grad

def get_regularized_loss_from_x_no_grad(model, x, t, reduction, tau):
    with torch.no_grad():
        z, _, _ = model.forward(x)
#     return get_loss_from_z(model, z, t, reduction)
    return get_regularized_loss_from_z(model, z, t, reduction, tau)



def get_acc_whole_dataset(model, params, x, np_t):
    N1 = params['N1']
    N1 = np.minimum(N1, len(x))
    
    i = 0
    list_acc = []
    model.eval()
    
    while i + N1 <= len(x):
        with torch.no_grad():
            list_acc.append(get_acc_from_x(model, params, x[i: i+N1], np_t[i: i+N1]))
            # z, _, _ = model.forward(x[i: i+N1])

        i += N1
    model.train()
    return sum(list_acc) / len(list_acc)

def get_regularized_loss_from_x_whole_dataset(model, x, t, reduction, params):
    N1 = params['N1']
    device = params['device']
    
    
    i = 0
    list_loss = []
    model.eval()
    
    
    while i + N1 <= len(x):
        with torch.no_grad():
            z, _, _ = model.forward(torch.from_numpy(x[i: i+N1]).to(device))
        
        list_loss.append(
            get_regularized_loss_from_z(model, z, torch.from_numpy(t[i: i+N1]).to(device), reduction).item())
        

        
        i += N1
        

        
        
    model.train()
    return sum(list_loss) / len(list_loss)

def get_regularized_loss_and_acc_from_x_whole_dataset_with_generator(model, generator, reduction, params):
    N1 = params['N1']
#     N1 = np.minimum(N1, len(x))
    
    
    device = params['device']
    
    list_loss = []
    list_unregularized_loss = []
    list_acc = []
    
    model.eval()
    
    for (X_mb, t_mb) in generator:
        
#         print('X_mb.size()')
#         print(X_mb.size())
        
        X_mb, t_mb = X_mb.to(device), t_mb.to(device)
        
        if len(X_mb) != N1:
            break
        
#         with torch.no_grad():
        z, _, _ = model.forward(X_mb)
            
        
        
        loss_i, unregularized_loss_i =\
        get_regularized_loss_from_z(model, z, t_mb, reduction, params['tau'])
        
        list_loss.append(loss_i.item())
        
        list_unregularized_loss.append(unregularized_loss_i.item())
        
        list_acc.append(
            get_acc_from_z(model, params, z, t_mb))
        
    model.train()
    
    return sum(list_loss) / len(list_loss), sum(list_unregularized_loss) / len(list_unregularized_loss), sum(list_acc) / len(list_acc)

def get_regularized_loss_and_acc_from_x_whole_dataset(model, x, t, reduction, params):
    N1 = params['N1']
    N1 = np.minimum(N1, len(x))
    
    
    i = 0
    device = params['device']
    
    list_loss = []
    list_unregularized_loss = []
    list_acc = []
    
    model.eval()
    
    while i + N1 <= len(x):
        
        X_mb = torch.from_numpy(x[i: i+N1]).to(device)
        t_mb = torch.from_numpy(t[i: i+N1]).to(device)
        
#         with torch.no_grad():
        z, _, _ = model.forward(X_mb)
            
        
        
        loss_i, unregularized_loss_i =\
        get_regularized_loss_from_z(model, z, t_mb, reduction, params['tau'])
        
        list_loss.append(loss_i.item())
        
        list_unregularized_loss.append(unregularized_loss_i.item())
        
        list_acc.append(
            get_acc_from_z(model, params, z, t_mb))
        
        i += N1
    model.train()
    
    return sum(list_loss) / len(list_loss), sum(list_unregularized_loss) / len(list_unregularized_loss), sum(list_acc) / len(list_acc)

def get_loss_from_x(model, x, t, reduction):
    # with torch.no_grad():
    z, _, _ = model.forward(x)
    return get_loss_from_z(model, z, t, reduction)

def get_acc_from_x(model, params, x, np_t):
    
    z, _ , _= model.forward(x)

    return get_acc_from_z(model, params, z, np_t)

def get_regularized_loss_from_z(model, z, t, reduction, tau):
    
#     loss = get_loss_from_z(model, z, t, reduction)
    unregularized_loss = get_loss_from_z(model, z, t, reduction)
    
#     loss += 0.5 * tau *\
#     get_dot_product_torch(model.layers_weight, model.layers_weight)
    loss = unregularized_loss + 0.5 * tau *\
    get_dot_product_torch(model.layers_weight, model.layers_weight)
    
    return loss, unregularized_loss


def get_loss_from_z(model, z, t, reduction):
    if model.name_loss == 'multi-class classification':
        # since this is multi-calss cross entropy loss,
        # there is no ambiguity between "average" and "sum"
        
        # common bug: size of z does not match max of t
        

        
#         print('z.size()')
#         print(z.size())
        
#         print('t.size()')
#         print(t.size())

        loss = F.cross_entropy(z, t, reduction = reduction)


    elif model.name_loss == 'binary classification':
        # since this is binary-calss cross entropy loss,
        # there is no ambiguity between "average" and "sum"
        

        loss = torch.nn.BCEWithLogitsLoss(reduction = reduction)(z, t.float().unsqueeze_(1))


        if reduction == 'none':
            loss = loss.squeeze(1)
            
#         sys.exit()
        

    elif model.name_loss == 'logistic-regression':
        # use of cross-entropy endorsed by Hinton and Salakhutdinov 2006 and 
        # Hessian-free code
        
        # if reduction == 'mean', the following gives the sum of loss / #data / #feature
        # i.e. for a single data point, the loss on all pixels are averaged      
        

        if reduction == 'none':
            loss = torch.nn.BCEWithLogitsLoss(reduction = reduction)(z, t.float())
            loss = torch.sum(loss, dim=1)
        elif reduction == 'mean':
            loss = torch.nn.BCEWithLogitsLoss(reduction = 'sum')(z, t.float())
            
            loss = loss / z.size(0) / z.size(1)
#             loss = loss / z.size(0)
            
        elif reduction == 'sum':
            loss = torch.nn.BCEWithLogitsLoss(reduction = reduction)(z, t.float())
            
    elif model.name_loss == 'logistic-regression-sum-loss':
        # use of cross-entropy endorsed by Hinton and Salakhutdinov 2006 and 
        # Hessian-free code
        
        # if reduction == 'mean', the following gives the sum of loss / #data
        # i.e. for a single data point, the loss on all pixels are summed      
        

        if reduction == 'none':
            loss = torch.nn.BCEWithLogitsLoss(reduction = reduction)(z, t.float())
            loss = torch.sum(loss, dim=1)
        elif reduction == 'mean':
            loss = torch.nn.BCEWithLogitsLoss(reduction = 'sum')(z, t.float())
            
#             loss = loss / z.size(0) / z.size(1)
            loss = loss / z.size(0)
            
        elif reduction == 'sum':
            loss = torch.nn.BCEWithLogitsLoss(reduction = reduction)(z, t.float())

    elif model.name_loss == 'linear-regression-half-MSE':
        
        if reduction == 'mean':
            loss = torch.nn.MSELoss(reduction = 'sum')(z, t) / 2
            
            
            # only average by mini-batch, not by #feature
            loss = loss / z.size(0)
            
            # averaged by #mini-bath * #feature
#             loss = loss / z.size(0) / z.size(1)
            
            # averaged by #mini-bath
#             loss = loss / z.size(0) / z.size(1)
        elif reduction == 'none':
            loss = torch.nn.MSELoss(reduction = 'none')(z, t) / 2
            loss = torch.sum(loss, dim=1)
        else:
            print('reduction')
            print(reduction)
            print('error: unknown reduction')
            sys.exit()
            
            sys.exit()
            
    elif model.name_loss == 'linear-regression':
        
        if reduction == 'mean':
            loss = torch.nn.MSELoss(reduction = 'sum')(z, t)
            
            
            # only average by mini-batch, not by #feature
            loss = loss / z.size(0)
            
            # averaged by #mini-bath * #feature
#             loss = loss / z.size(0) / z.size(1)
            
            # averaged by #mini-bath
#             loss = loss / z.size(0) / z.size(1)
        elif reduction == 'none':
            loss = torch.nn.MSELoss(reduction = 'none')(z, t)
            loss = torch.sum(loss, dim=1)
        else:
            print('reduction')
            print(reduction)
            print('error: unknown reduction')
            sys.exit()
            
            sys.exit()
    
    else:
        print('Error: loss function not specified.')
        sys.exit()
    
    return loss



def get_acc_from_z(model, params, z, torch_t):
    
    if model.name_loss == 'multi-class classification':
        
#         np_t = torch_t.cpu().data.numpy()
        
        y = z.argmax(dim=1)
        
#         acc = np.mean(y.cpu().numpy() == np_t)
        
#         print('acc')
#         print(acc)
        
#         print('y == torch_t')
#         print(y == torch_t)
        
#         print('(y == torch_t).float()')
#         print((y == torch_t).float())
        
#         print('torch.mean((y == torch_t).float())')
#         print(torch.mean((y == torch_t).float()))
        
        acc = torch.mean((y == torch_t).float())
        
#         print('np_t should be tensor')
#         sys.exit()
    
    elif model.name_loss == 'binary classification':
        
        print('np_t should be tensor')
        sys.exit()
        
        z_1 = torch.sigmoid(z)
        
        y = (z_1 > 0.5)
        
        y = y[:, 0]
        
        acc = np.mean(y.cpu().data.numpy() == np_t)
    elif model.name_loss in ['logistic-regression',
                             'logistic-regression-sum-loss']:
        # the MSE is the sum of mse / #data / #feature
        
#         print('np_t should be tensor logistic-regression')
#         sys.exit()

        z_sigmoid = torch.sigmoid(z)
    
#         print('z[0][0].item()')
#         print(z[0][0].item())

        criterion = nn.MSELoss(reduction = 'mean')
        acc = criterion(z_sigmoid, torch_t)
        
#         print('z_sigmoid.size()')
#         print(z_sigmoid.size())
#         print('torch_t.size()')
#         print(torch_t.size())
        
#         print('z_sigmoid[0][0].item()')
#         print(z_sigmoid[0][0].item())
        
        
#         sys.exit()
        
#         print('z_sigmoid.size()')
#         print(z_sigmoid.size())
#         print('torch.from_numpy(np_t).to(params[device]).size()')
#         print(torch.from_numpy(np_t).to(params['device']).size())
        
#         test_torch_t = torch.from_numpy(np_t).to(params['device'])
        
#         print('torch.sum((z_sigmoid - test_torch_t)**2) / 256 /625')
#         print(torch.sum((z_sigmoid - test_torch_t)**2) / 256 /625)
        
#         print('acc in logistic-regression')
#         print(acc.item())
#         sys.exit()
        
        
    elif model.name_loss in ['linear-regression',
                             'linear-regression-half-MSE']:
        
#         criterion = nn.MSELoss(reduction = 'mean')

#         criterion = nn.MSELoss(reduction = 'sum')
#         acc = criterion(z, torch_t)
        
#         acc = nn.MSELoss(reduction = 'sum')(z, torch_t)
#         acc = acc / z.size(0)
        
        acc = nn.MSELoss(reduction = 'mean')(z, torch_t)
#         acc = acc / z.size(0)
        
        
#         acc = acc.item()
    else:
        print('Error: unkwoen name_loss')
        sys.exit()
    acc = acc.item()
    
#     print('acc')
#     print(acc)
    
    return acc
    
    


    
    
def compute_sum_J_transpose_V_backp(v, data_, params):
    # use backpropagation
    algorithm = params['algorithm']
    N2 = params['N2']
    numlayers = params['numlayers']
    
    model = data_['model']
    X_mb_N2 = data_['X_mb_N2']
    
    
    
    z, _, _ = model.forward(X_mb_N2)
    
    # if algorithm in ['SMW-Fisher-different-minibatch',
                    #  'SMW-Fisher-batch-grad-momentum-exponential-decay',
                    #  'ekfac-EF',
                    #  'kfac',
                    #  'SMW-Fisher',
                    #  'SMW-Fisher-momentum',
                    #  'SMW-Fisher-D_t-momentum',
                    #  'SMW-Fisher-momentum-D_t-momentum',
                    #  'GI-Fisher',
                    #  'SMW-Fisher-BD',
                    #  'RMSprop-individual-grad-no-sqrt-LM',
                    #  'SMW-Fisher-batch-grad',
                    #  'SMW-Fisher-batch-grad-momentum',
                    #  'matrix-normal-LM-momentum-grad']:
    if params['matrix_name'] == 'Fisher':
                         
                         
        t_mb_N2 = data_['t_mb_pred_N2'] # note that t_mb will be correspond to either EF or Fisher
    
#     model_new = Model()
    
#     model_new.W = model.W 


#     model_new = Model_2()
    
#     print('model_new.W[1]): ', model_new.W[1])
#     print('model.W[1]): ', model.W[1])
    
#     model_new.load_state_dict(model.state_dict())

#     for l in range(numlayers):
#         model_new.W[l].data = model.W[l].data
#     print('test')
    
#     weighted_loss = torch.dot(loss, v)
        
        
#         loss = F.cross_entropy(torch.squeeze(z), t_mb[N2_index], reduction = 'none')
        reduction = 'none'
        loss = get_loss_from_z(model, z, t_mb_N2, reduction)
        
#         print('error: should change to device')
#         sys.exit()
        
#         loss = torch.dot(loss, v.to(params['device']))
        loss = torch.dot(loss, v)
        
    elif algorithm == 'SMW-GN':
        
        m_L = params['m_L']
        
        v = v.view(N2, m_L)
        
#         print('print(z.dtype): ', z.dtype)
        
#         print('print(v.dtype): ', v.dtype)
        
        loss = torch.sum(z * v.data)
        
    else:
        print('Error! 1500')
        sys.exit()


    model.zero_grad()

    # print('loss')
    # print(loss)

    loss.backward()
    
#     print('test 10:28')
    
#     print('model_1.W[1].size():', model_1.W[1].size())

    delta = get_model_grad(model, params)

    

    
    
    


    

    model.zero_grad()
    
    
    return delta

    
def get_D_t(data_, params):
    algorithm = params['algorithm'] 
    N2 = params['N2']
    numlayers = params['numlayers']
    
    if algorithm == 'SMW-Fisher-different-minibatch' or\
    algorithm == 'SMW-Fisher':
        a_grad_momentum = data_['a_grad_N2']
        h_momentum = data_['h_N2']
    
        lambda_ = params['lambda_']
        
        print('error: should change to device')
        sys.exit()

        # compute D_t 
        D_t = lambda_ * torch.eye(N2).to(params['device'])

        for l in range(numlayers):
            # @ == torch.mm in this case, speed also similar
            # D_t += 1 / N2 * (a_grad_momentum[l] @ a_grad_momentum[l].t()) * (h_momentum[l] @ h_momentum[l].t() + 1)
            D_t += 1 / N2 * torch.mm(a_grad_momentum[l], a_grad_momentum[l].t()) *\
            (torch.mm(h_momentum[l], h_momentum[l].t()) + 1)

        # D_t = D_t.cpu().data.numpy()
        torch_D_t = D_t
    elif algorithm == 'SMW-Fisher-momentum' or\
    algorithm == 'SMW-Fisher-D_t-momentum' or\
    algorithm == 'GI-Fisher':
        a_grad_momentum = data_['a_grad_momentum']
        h_momentum = data_['h_momentum']
    
        lambda_ = params['lambda_']
        
        

        # compute D_t 
        D_t = lambda_ * torch.eye(N2, device=params['device'])

        for l in range(numlayers):
            # @ == torch.mm in this case, speed also similar
            # D_t += 1 / N2 * (a_grad_momentum[l] @ a_grad_momentum[l].t()) * (h_momentum[l] @ h_momentum[l].t() + 1)
            D_t += 1 / N2 * torch.mm(a_grad_momentum[l], a_grad_momentum[l].t()) *\
            (torch.mm(h_momentum[l], h_momentum[l].t()) + 1)

        # D_t = D_t.cpu().data.numpy()
        torch_D_t = D_t


    elif algorithm == 'SMW-Fisher-momentum-D_t-momentum':

        a_grad_momentum = data_['a_grad_for_D_t']
        h_momentum = data_['h_for_D_t']
    
        lambda_ = params['lambda_']
        
        

        # compute D_t 
        D_t = lambda_ * torch.eye(N2)
    
        for l in range(numlayers):
        
            D_t += 1 / N2 * torch.mm(a_grad_momentum[l], a_grad_momentum[l].t()) *\
            (torch.mm(h_momentum[l], h_momentum[l].t()) + 1)
        
        D_t = D_t.data.numpy()
    elif algorithm == 'SMW-GN':
        # a_grad[l]: N2, m_L, m_l
        
    
        GN_cache = data_['GN_cache']
        h = GN_cache['h']
        a_grad = GN_cache['a_grad']
        
        m_L = params['m_L']
        lambda_ = params['lambda_']
        
        
        # D_t = np.zeros((m_L * N2, m_L * N2))
        torch_D_t = torch.zeros(m_L * N2, m_L * N2, device=params['device'])
        
        
        
        model = data_['model']
        
#         start_time = time.time()
        
        
        for l in range(numlayers):

            # np_a_grad_l = a_grad[l].cpu().data.numpy()
            # h_l = h[l].cpu().data.numpy()

            torch_a_grad_1 = a_grad[l]
            torch_h_l = h[l]
            
            # h[l]: N2 * m[l]
            
            # a_grad[l]
            
    
            

            
            # permuted_a_grad_l =\
            # np.reshape(
                # np.swapaxes(np_a_grad_l,0,1), (m_L * N2, data_['model'].layersizes[l+1]))
            
            torch_permuted_a_grad_l =\
            torch_a_grad_1.permute(1,0,2).view(m_L * N2, data_['model'].layersizes[l+1])
            
            
            # print('torch.max(\
                # torch_permuted_a_grad_l - torch.from_numpy(permuted_a_grad_l).cuda())')
            # print(torch.max(
                # torch_permuted_a_grad_l - torch.from_numpy(permuted_a_grad_l).cuda()))
            # print('torch.min(\
                # torch_permuted_a_grad_l - torch.from_numpy(permuted_a_grad_l).cuda())')
            # print(torch.min(
                # torch_permuted_a_grad_l - torch.from_numpy(permuted_a_grad_l).cuda()))

            

            

            
            # h_l_h_l_t = np.matmul(h_l, np.transpose(h_l)) + 1 # + 1 for b
            torch_h_l_h_l_t = torch.mm(torch_h_l, torch_h_l.t()) + 1 # + 1 for b
            
            # h_kron = np.kron(h_l_h_l_t, np.ones((m_L, m_L)))
            torch_h_kron = get_kronecker_torch(
                torch_h_l_h_l_t, torch.ones(m_L, m_L, device=params['device']))
            
            # print('torch.max(\
                # torch_h_kron - torch.from_numpy(h_kron).cuda())')
            # print(torch.max(
                # torch_h_kron - torch.from_numpy(h_kron).cuda()))
            # print('torch.min(\
                # torch_h_kron - torch.from_numpy(h_kron).cuda())')
            # print(torch.min(
                # torch_h_kron - torch.from_numpy(h_kron).cuda()))





   
            # D_t += np.multiply(h_kron, np.matmul(permuted_a_grad_l, np.transpose(permuted_a_grad_l)))
            torch_D_t += torch.mul(
                torch_h_kron, torch.mm(torch_permuted_a_grad_l, torch_permuted_a_grad_l.t()))
            
            # print('torch.max(torch_D_t - torch.from_numpy(D_t).cuda())')
            # print(torch.max(torch_D_t - torch.from_numpy(D_t).cuda()))
            # print('torch.min(torch_D_t - torch.from_numpy(D_t).cuda())')
            # print(torch.min(torch_D_t - torch.from_numpy(D_t).cuda()))


            
        
#         print('time for compute J J transpose: ', time.time() - start_time)
        
        # add the H term
        
#         start_time = time.time()
        
        if model.name_loss == 'binary classification':
            # print('need to check')
            # sys.exit()

            torch_D_t = 1 / N2 * torch_D_t
        
#         H = get_H(data_, params)
#         for i in range(N2):
#             D_t[i * m_L: (i+1) * m_L, i * m_L: (i+1) * m_L] += lambda_ * H[i]

            torch_D_t = torch_D_t + lambda_ * torch.eye(m_L * N2, device=params['device'])
        elif model.name_loss == 'multi-class classification':
            # D_t = get_JH(D_t, data_, params)
            torch_D_t = get_JH(torch_D_t, data_, params)

            torch_D_t = 1 / N2 * torch_D_t
        
#         H = get_H(data_, params)
#         for i in range(N2):
#             D_t[i * m_L: (i+1) * m_L, i * m_L: (i+1) * m_L] += lambda_ * H[i]
            torch_D_t = torch_D_t + lambda_ * torch.eye(m_L * N2, device=params['device'])
        else:
            
            print('Error: unknown loss')
            sys.exit()
        
        # H_test = get_JH(np.eye(len(D_t)), data_, params)
        
        # print('H_test')
        # print(H_test)
        
#         print('np.linalg.norm(H_test)')
#         print(np.linalg.norm(H_test))
        
#         print('np.linalg.norm(D_t)')
#         print(np.linalg.norm(D_t))


# ========================================
        
        
        
#         D_t = np.transpose(D_t)
        
#         for i in range(N2 * m_L):
#             D_t[:, i] = get_HV(D_t[:, i], data_, params)
        
#         D_t = np.transpose(D_t)
        
#         print('time for compute H: ', time.time() - start_time)
        
        
        
        # torch_D_t = 1 / N2 * torch_D_t
        
#         H = get_H(data_, params)
#         for i in range(N2):
#             D_t[i * m_L: (i+1) * m_L, i * m_L: (i+1) * m_L] += lambda_ * H[i]
        # torch_D_t = torch_D_t + lambda_ * np.eye(m_L * N2)
        
    else:
        print('Error! 1501')
        sys.exit()
    return torch_D_t

def get_kronecker_torch(A, B):
    return torch.einsum("ab,cd->acbd", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))

def get_JH(torch_D_t, data_, params):
    y = data_['y']
    N2 = params['N2']
    m_L = params['m_L']

    # D_t = torch_D_t.cpu().data.numpy()
    
    # D_t_1
    
    diag_y = y.view(m_L * N2)
    
    diag_y = diag_y.repeat(N2 * m_L, 1)
    
    # D_t_1 = D_t * diag_y.cpu().data.numpy()
    torch_D_t_1 = torch_D_t * diag_y
    
    # D_t_2
    
    # D_t_3 = np.zeros((m_L * N2, m_L * N2))
    torch_D_t_3 = torch.zeros(m_L * N2, m_L * N2, device=params['device'])
    for i in range(N2):
#         D_t_2[:, i] = D_t[:, i * m_L : (i+1) * m_L] @ y[i]

        # y_i = y[i].cpu().data.numpy()[:, np.newaxis]
        torch_y_i = torch.unsqueeze(y[i], -1)

        # print('torch.max(torch_y_i - torch.from_numpy(y_i).cuda())')
        # print(torch.max(torch_y_i - torch.from_numpy(y_i).cuda()))
        # print('torch.min(torch_y_i - torch.from_numpy(y_i).cuda())')
        # print(torch.min(torch_y_i - torch.from_numpy(y_i).cuda()))
    
    

        # D_t_3[:, i * m_L : (i+1) * m_L] = np.matmul(np.matmul(D_t[:, i * m_L : (i+1) * m_L], y_i), np.transpose(y_i))
        torch_D_t_3[:, i * m_L : (i+1) * m_L] =\
        torch.mm(torch.mm(torch_D_t[:, i * m_L : (i+1) * m_L], torch_y_i), torch_y_i.t())
    
    # D_t = D_t_1 - D_t_3
    torch_D_t = torch_D_t_1 - torch_D_t_3

    # print('torch.max(torch_D_t - torch.from_numpy(D_t).cuda())')
    # print(torch.max(torch_D_t - torch.from_numpy(D_t).cuda()))
    # print('torch.min(torch_D_t - torch.from_numpy(D_t).cuda())')
    # print(torch.min(torch_D_t - torch.from_numpy(D_t).cuda()))

    return torch_D_t
    

def get_H(data_, params):
    model = data_['model']
    
    if model.name_loss == 'multi-class classification':
        print('wrong')
        sys.exit()
        
        # N2_index = params['N2_index']
        m_L = params['m_L']
        N2 = params['N2']

        z_N2 = data_['z_N2']
        

        z_data = z_N2.data.numpy()
        
        H = np.zeros((N2, m_L, m_L))
        for i in range(N2):
            H[i] -= np.outer(z_data[i], z_data[i])
            H[np.diag_indices(m_L)] += z_data[i]
    elif model.name_loss == 'binary classification':
        y = data_['y']
        y = y.data.numpy()
        tilde_y = y * (1-y)
        H = np.diag(y)
    else:
        sys.exit()
    
    
    return H

def get_HV(torch_V, data_, params):
    model = data_['model']

    # y = data_['y']
    # y = y.cpu().data.numpy()
    torch_y = data_['y']

    # V = torch_V.cpu().data.numpy()

    if model.name_loss == 'multi-class classification':
    
        N2 = params['N2']
        m_L = params['m_L']
        
        torch_V = torch_V.view(N2, m_L)
        # V = np.reshape(V, (N2, m_L))
        
        
        
        
        
        # HV = np.multiply(y, V)
        torch_HV = torch.mul(torch_y, torch_V)
        
        
        # sum_HV = np.sum(HV, 1) # length N2
        torch_sum_HV = torch.sum(torch_HV, dim=1) # length N2
        
        
        # HV = HV - sum_HV[:, None] * y
        torch_HV = torch_HV - torch_sum_HV[:, None] * torch_y

        # print('torch.max(torch_HV - torch.from_numpy(HV).cuda())')
        # print(torch.max(torch_HV - torch.from_numpy(HV).cuda()))
        # print('torch.min(torch_HV - torch.from_numpy(HV).cuda())')
        # print(torch.min(torch_HV - torch.from_numpy(HV).cuda()))
            
        
        # HV = np.reshape(HV, m_L * N2)
        torch_HV = torch_HV.view(m_L * N2)

    elif model.name_loss == 'binary classification':
        y = np.squeeze(y, axis=1)
        # print('test no squeeze')

        # print('V.shape')
        # print(V.shape)

        # print('np.multiply(y, V).shape')
        # print(np.multiply(y, V).shape)

        HV = np.multiply(y, V)
    else:
        sys.exit()
    
    return torch_HV

def compute_JV(V, data_, params):
    algorithm = params['algorithm']
    
    numlayers = params['numlayers']
    N2 = params['N2']
    
    # if algorithm in ['SMW-Fisher-different-minibatch',
                    #  'SMW-Fisher-batch-grad-momentum-exponential-decay',
                    #  'SMW-Fisher',
                    #  'SMW-Fisher-momentum',
                    #  'ekfac-EF',
                    #  'kfac',
                    #  'SMW-Fisher-D_t-momentum',
                    #  'SMW-Fisher-momentum-D_t-momentum',
                    #  'GI-Fisher',
                    #  'SMW-Fisher-BD',
                    #  'RMSprop-individual-grad-no-sqrt-LM',
                    #  'SMW-Fisher-batch-grad',
                    #  'SMW-Fisher-batch-grad-momentum',
                    #  'matrix-normal-LM-momentum-grad']:
    if params['matrix_name'] == 'Fisher':

        # if algorithm == 'SMW-Fisher' or\
        # algorithm == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
        # algorithm == 'RMSprop-individual-grad-no-sqrt-LM' or\
        # algorithm == 'SMW-Fisher-batch-grad' or\
        # algorithm == 'kfac' or\
        # algorithm == 'SMW-Fisher-batch-grad-momentum':
            # 1
        # else:
            # print('Error: need to use current minibatch.')
            # sys.exit()
            # a_grad_momentum = data_['a_grad_momentum']
            # h_momentum = data_['h_momentum']
    
        
        
    
        v = torch.zeros(N2)
        if params['if_gpu']:
            v = v.cuda()

        # a_N2 = data_['a_N2']
        a_grad_N2 = data_['a_grad_N2']
        h_N2 = data_['h_N2']
    
        for l in range(numlayers):
    
            torch_V_W_l = V[l]['W']
            torch_V_b_l = V[l]['b']
            # torch_V_W_l = torch.from_numpy(V[l]['W']).float()
            # torch_V_b_l = torch.from_numpy(V[l]['b']).float()
            # if params['if_gpu']:
                # torch_V_W_l = torch_V_W_l.cuda()
                # torch_V_b_l = torch_V_b_l.cuda()
            
            v += torch.sum(torch.mm(a_grad_N2[l], torch_V_W_l) * h_N2[l], dim = 1)
        
            
            v += torch.sum(torch_V_b_l * a_grad_N2[l], dim=1)


            # test_start_time = time.process_time()

            # test_ans_1 = torch.sum(V['b'][l] * a_grad_momentum[l], dim=1)

            # test_time = time.process_time() - test_start_time

            # print('time for 1')
            # print(test_time)

            # test_start_time = time.process_time()

            # test_ans_2 = torch.mv(a_grad_momentum[l], V['b'][l])

            # test_time = time.process_time() - test_start_time

            # print('time for 2')
            # print(test_time)

            # print('torch.max(test_ans_2 - test_ans_1)')
            # print(torch.max(test_ans_2 - test_ans_1))

            # print('torch.min(test_ans_2 - test_ans_1)')
            # print(torch.min(test_ans_2 - test_ans_1))

            # print('test minibatch')
            # sys.exit()
            
            
#         print('v:', v)
        v = v.data
        
        
#     print('V[1]: ', V[1])
#     print('1/N2 * h_momentum[1].t() @ a_grad_momentum[1]:', 1/N2 * h_momentum[1].t() @ a_grad_momentum[1])
    elif algorithm == 'SMW-GN':
        
        GN_cache = data_['GN_cache']
        
        m_L = params['m_L']
        
        a_grad = GN_cache['a_grad'] # a_grad[l]: N2, m_L, m_l
        h = GN_cache['h']

        # a[l][N2_index] @ model.W[l] # N2 * m[l]
    # (a[l][N2_index] @ model.W[l]) * h[l][N2_index] # N2 * m[l]
    #  torch.sum((a[l][N2_index] @ model.W[l]) * h[l][N2_index], dim = 1)
    
    # a[l].grad: size N1 * m[l+1], it has a coefficient 1 / N1, which should be first compensate
    # h[l]: size N1 * m[l]
    # model.W[l]: size m[l+1] * m[l]
        
        
        
        '''
        test_start_time_cpu = time.process_time()
        
        v = torch.zeros(m_L, N2)
        
        
        
        for l in range(numlayers):
        
            a_grad_l = a_grad[l]
            for i in range(m_L):

    
                
                v[i] += torch.sum(torch.mm(a_grad_l[:, i, :], V['W'][l]) * h[l], dim = 1)
                v[i] += torch.mv(a_grad_l[:, i, :], V['b'][l])
        
        v = v.view(m_L * N2)

        print('time for non-parallel')
        print(time.process_time() - test_start_time_cpu)
        '''

        v = torch.zeros(N2, m_L, device=params['device'])
        for l in range(numlayers):
            a_grad_l = a_grad[l]

            # print('a_grad_l')
            # print(a_grad_l)
            # print('V[l][W]')
            # print(V[l]['W'])
            # print('h[l]')
            # print(h[l])



            # v += torch.sum(torch.matmul(a_grad_l, torch.from_numpy(V['W'][l])) * h[l][:, None, :], dim=2)
            v += torch.sum(
                torch.matmul(a_grad_l, V[l]['W']) * h[l][:, None, :], dim=2)
            
            # v += torch.matmul(a_grad_l, torch.from_numpy(V['b'][l]))
            v += torch.matmul(a_grad_l, V[l]['b'])

        v = v.permute(1, 0)
        # v = np.swapaxes(v,0,1)

        
        v = torch.reshape(v, (m_L * N2,))
        # v = np.reshape(v, (m_L * N2,))

        # print('time for parallel')
        # print(time.process_time() - test_start_time_cpu)

        # print('test parallel')
        
    else:
        print('Error! 1502')
        sys.exit()
    
    return v

def get_cache_momentum(data_, params):
    algorithm = params['algorithm']
    N2 = params['N2']
    
    if algorithm == 'SMW-GN':
        
    
        

        a_grad_momentum = data_['a_grad']
        h_momentum = data_['h']
        
        GN_cache = {}
        GN_cache['a_grad'] = a_grad_momentum
        GN_cache['h'] = h_momentum
        
        
        data_['GN_cache'] = GN_cache

    elif algorithm == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
    algorithm == 'SMW-Fisher-batch-grad-momentum':

        # N_iters = 30
        N_iters = params['N_iters']

        batch_grads_i = data_['model_regularized_grad_N2']
        batch_grads_test = data_['batch_grads_test']
        if len(data_['batch_grads']) == 0:

            

            batch_grads_test = {}
            batch_grads_test['W'] = []
            batch_grads_test['b'] = []
            for l in range(data_['model'].numlayers):
                batch_grads_test['W'].append(batch_grads_i['W'][l][np.newaxis,:])
                batch_grads_test['b'].append(batch_grads_i['b'][l][np.newaxis,:])

            
        elif len(data_['batch_grads']) < N_iters:
            for l in range(data_['model'].numlayers):
                # print('batch_grads_test')
                # print(batch_grads_test)
                
                batch_grads_test['W'][l] = np.concatenate(
                    (batch_grads_test['W'][l], batch_grads_i['W'][l][np.newaxis,:]), axis=0)
                batch_grads_test['b'][l] = np.concatenate(
                    (batch_grads_test['b'][l], batch_grads_i['b'][l][np.newaxis,:]), axis=0)
            
        else:
            # print('params[i]')
            # print(params['i'])

            # print('params[i] % N_iters')
            # print(params['i'] % N_iters)

            # replace_index = params['i'] % N_iters
            replace_index = 0
            swap_indices = np.asarray(
                list(range(replace_index+1, N_iters)) + list(range(0, replace_index+1)))

            replace_index = params['i'] % N_iters
            for l in range(data_['model'].numlayers):
                # batch_grads_test['W'][l][replace_index] = batch_grads_i['W'][l]
                # batch_grads_test['b'][l][replace_index] = batch_grads_i['b'][l]
                batch_grads_test['W'][l][0] = batch_grads_i['W'][l]
                batch_grads_test['b'][l][0] = batch_grads_i['b'][l]
                batch_grads_test['W'][l] = batch_grads_test['W'][l][swap_indices]
                batch_grads_test['b'][l] = batch_grads_test['b'][l][swap_indices]

            
        data_['batch_grads_test'] = batch_grads_test

        if len(data_['batch_grads']) == N_iters:
            data_['batch_grads'].popleft()
            data_['batch_grads_a_grad'].popleft()
            data_['batch_grads_h'].popleft()
        elif len(data_['batch_grads']) > N_iters:
            print('Error: len > N_iters')
            sys.exit()

        data_['batch_grads'].append(data_['model_regularized_grad_N2'])
        data_['batch_grads_a_grad'].append(data_['a_grad_N2'])
        data_['batch_grads_h'].append(data_['h_N2'])

        
        
        
        
        
        
    else:
    
        
        a_grad_N2 = data_['a_grad_N2']
        h_N2 = data_['h_N2']
    
    
    
    
        N1 = params['N1']
        
        i = params['i']
        
        numlayers = params['numlayers']
            
    
    
#         a = []
#         h = [X_mb] + h
#         for ii in range(len(cache)):
#             if ii % 2 == 0:
#                 a.append(cache[ii])
#             else:
#                 h.append(cache[ii])        
#         a.append(z)

    
    
    
    
    
    # Update running estimates
        if algorithm == 'SMW-Fisher-momentum':
            
            a_grad_momentum = data_['a_grad_momentum']
            h_momentum = data_['h_momentum']
            
            rho = min(1 - 1/(i+1), 0.95)
        
            for l in range(numlayers):
                a_grad_momentum[l] = rho * a_grad_momentum[l] + (1-rho) * a_grad_N2[l]
                h_momentum[l] = rho * h_momentum[l] + (1-rho) * h_N2[l]
        elif algorithm == 'SMW-Fisher-momentum-D_t-momentum':
            
            a_grad_momentum = data_['a_grad_momentum']
            h_momentum = data_['h_momentum']
            
            rho = min(1 - 1/(i+1), 0.95)
        
            for l in range(numlayers):
                a_grad_momentum[l] = rho * a_grad_momentum[l] + (1-rho) * a_grad_N2[l]
                h_momentum[l] = rho * h_momentum[l] + (1-rho) * h_N2[l]
                
            a_grad_for_D_t = []
            h_for_D_t = []
            for l in range(numlayers):
                a_grad_for_D_t.append(a_grad_N2[l])
                h_for_D_t.append(h_N2[l])
                
            data_['a_grad_for_D_t'] = a_grad_for_D_t
            data_['h_for_D_t'] = h_for_D_t
        
        elif algorithm == 'Fisher-block' or\
        algorithm == 'SMW-Fisher-D_t-momentum' or\
        algorithm == 'GI-Fisher' or\
        algorithm == 'SMW-Fisher-BD':
            print('Error: no need to get momentum.')
            sys.exit()
            # a_grad_momentum = []
            # h_momentum = []
            # for l in range(numlayers):
                # a_grad_momentum.append(a_grad_N2[l])
                # h_momentum.append(h_N2[l])
            
    
        
        
        else:
            print('Error! 1503')
            sys.exit()
        
        data_['a_grad_momentum'] = a_grad_momentum
        data_['h_momentum'] = h_momentum


    return data_



def get_subtract(model_grad, delta, params):
    diff_p = get_zero(params)
    for l in range(params['numlayers']):
        for key in diff_p[l]:
            diff_p[l][key] = np.subtract(model_grad[l][key], delta[l][key])
    return diff_p

def get_subtract_torch(model_grad, delta):
    # diff_p = get_zero(params)
    diff_p = []
    for l in range(len(model_grad)):
        diff_p_l = {}
        for key in model_grad[l]:
            diff_p_l[key] = torch.sub(model_grad[l][key], delta[l][key])
        diff_p.append(diff_p_l)
    return diff_p



def get_plus(model_grad, delta):
    sum_p = []
    for l in range(len(model_grad)):
        sum_p_l = {}
        for key in model_grad[l]:
            sum_p_l[key] = np.add(model_grad[l][key], delta[l][key])
        sum_p.append(sum_p_l)
    return sum_p

def get_plus_torch(model_grad, delta):
    sum_p = []
    for l in range(len(model_grad)):
        sum_p_l = {}
        for key in model_grad[l]:
            sum_p_l[key] = model_grad[l][key] + delta[l][key]
        sum_p.append(sum_p_l)
    return sum_p

def get_if_nan(p):
    for l in range(len(p)):
        for key in p[l]:
            if torch.sum(p[l][key] != p[l][key]):
                return True
    return False



def get_torch_tensor(p, params):
    p_torch = []
    for l in range(len(p)):
        p_torch_l = {}
        for key in p[l]:
            p_torch_l[key] = torch.from_numpy(p[l][key]).to(params['device'])
        p_torch.append(p_torch_l)
    return p_torch

def get_plus_scalar(alpha, model_grad):
    sum_p = []

    # numlayers = params['numlayers']
    for l in range(len(model_grad)):
        sum_p_l = {}
        for key in model_grad[l]:
            sum_p_l[key] = model_grad[l][key] + alpha
        sum_p.append(sum_p_l)
    return sum_p

def get_multiply_scalar(alpha, delta):
    alpha_p = []
    for l in range(len(delta)):
        alpha_p_l = {}
        for key in delta[l]:
            alpha_p_l[key] = alpha * delta[l][key]
        alpha_p.append(alpha_p_l)
    return alpha_p

def get_multiply_scalar_no_grad(alpha, delta):
    alpha_p = []
    for l in range(len(delta)):
        alpha_p_l = {}
        for key in delta[l]:
            alpha_p_l[key] = alpha * delta[l][key].data
        alpha_p.append(alpha_p_l)
    return alpha_p

def get_multiply_scalar_blockwise(alpha, delta, params):
    alpha_p = []
    for l in range(params['numlayers']):
        alpha_p_l = {}
        for key in delta[l]:
            alpha_p_l[key] = alpha[l] * delta[l][key]
        alpha_p.append(alpha_p_l)
    return alpha_p

def get_multiply_torch(alpha, delta):
    alpha_p = []
    for l in range(len(delta)):
        alpha_p_l = {}
        for key in delta[l]:
            alpha_p_l[key] = torch.mul(alpha[l][key], delta[l][key])
        alpha_p.append(alpha_p_l)
    return alpha_p

def get_multiply(alpha, delta):
    alpha_p = []
    for l in range(len(delta)):
        alpha_p_l = {}
        for key in delta[l]:
            alpha_p_l[key] = np.multiply(alpha[l][key], delta[l][key])
        alpha_p.append(alpha_p_l)
    return alpha_p

def get_weighted_sum_batch(hat_v, batch_grads_test, params):
    alpha_p = get_zero(params)
    for l in range(params['numlayers']):

        # print('hat_v.shape')
        # print(hat_v.shape)
        # print('batch_grads_test[W][l].shape')
        # print(batch_grads_test['W'][l].shape)
        # print('(hat_v * batch_grads_test[W][l]).shape')
        # print((hat_v * batch_grads_test['W'][l]).shape)

        # print('np.sum(hat_v * batch_grads_test[W][l], axis=0).shape')
        # print(np.sum(hat_v * batch_grads_test['W'][l], axis=0).shape)

        alpha_p['W'][l] = np.sum(hat_v[:, None, None] * batch_grads_test['W'][l], axis=0)
        alpha_p['b'][l] = np.sum(hat_v[:, None] * batch_grads_test['b'][l], axis=0)
    return alpha_p

def get_opposite(delta):
    numlayers = len(delta)
    
    

    p = []
    for l in range(numlayers):
        # if params['layers_params'][l]['name'] == 'fully-connected':
        p_l = {}
        for key in delta[l]:
            p_l[key] = -delta[l][key]
        # else:
            # print('Error: layer unsupported')
            # sys.exit()
        p.append(p_l)
        
    return p


def SMW_GN_update(data_, params):
    # a[l].grad: size N1 * m[l+1], it has a coefficient 1 / N1, which should be first compensate
    # h[l]: size N1 * m[l]
    # model.W[l]: size m[l+1] * m[l]
    
    model_grad = data_['model_regularized_grad_used_torch']
    model = data_['model']
    
    
    
    N1 = params['N1']
    N2 = params['N2']
    lambda_ = params['lambda_']
    
    m_L = data_['model'].layersizes[-1]

    params['m_L'] = m_L
    m_L = params['m_L']
    
    
    
    
    
    
    
#     start_time = time.time()
    
    data_ = get_cache_momentum(data_, params)
    # print('test get_cache_momentum')

#     print('time for get cache momentum: ', time.time() - start_time)
    
#     start_time = time.time()
    
    z_N2 = data_['z_N2']
    z_data = z_N2
    y = F.softmax(z_data, dim = 1)
    data_['y'] = y
    # print('test no y')
    
#     print('time for compute y: ', time.time() - start_time)


    
    
    
    
    
        
    # compute the vector after D_t    
    

    
#     start_time = time.time()
    
    v = compute_JV(model_grad, data_, params)
    # v = torch.ones(N2 * m_L)
    # print('test compute_JV')
    
    
#     print('v of compute JV: ', v)
    
#     print('time for compute JV: ', time.time() - start_time)
    
    
    # compute hat_v
    

#     start_time = time.time()
        
    D_t = get_D_t(data_, params)

    # print(D_t)
    
#     print('time for get D_t: ', time.time() - start_time)
    
#     start_time = time.time()
    

    


    # v = torch.unsqueeze(v, -1)
    # D_t_cho = torch.cholesky(D_t.data)
    # hat_v = torch.cholesky_solve(v.data, D_t_cho)
    # hat_v = torch.squeeze(hat_v, dim=1)

    v = torch.unsqueeze(v, -1)
    hat_v, _ = torch.solve(v.data, D_t.data)
    hat_v = torch.squeeze(hat_v, dim=1)

    # theoretically, cholesky should be faster than torch.solve()
    # however, this only happens in practice when the size of matrix is large
    # if matrix is small, torch.solve() is faster
    # since we always want to deal with small matricex, we choose to use torch.solve()

    '''
    #========
    d = D_t.size()[0]
    A = np.random.rand(d, d)
    A = np.matmul(A, np.transpose(A)) + np.eye(d)

    v = np.random.rand(d,1)

    A = torch.from_numpy(A).cuda()
    v = torch.from_numpy(v).cuda()

    start_time_cpu = time.process_time()
    start_time_wall = time.time()


    A_cho = torch.cholesky(A.data)
    torch.cholesky_solve(v.data, A_cho)
    
    print(time.process_time() - start_time_cpu)
    print(time.time() - start_time_wall)
    
    # A = np.random.rand(d, d)
    # A = np.matmul(A, np.transpose(A)) + np.eye(d)

    # v = np.random.rand(d,1)

    # A = torch.from_numpy(A).cuda()
    # v = torch.from_numpy(v).cuda()
    
    start_time_cpu = time.process_time()
    start_time_wall = time.time()
    
    torch.solve(v.data, A.data)

    print(time.process_time() - start_time_cpu)
    print(time.time() - start_time_wall)

    print('d')
    print(d)
    print('test 2 method')

    '''
    
    

    
    
    if model.name_loss == 'binary classification':
        1
    elif model.name_loss == 'multi-class classification':
        hat_v = get_HV(hat_v, data_, params)
    else:
        print('Error: unknown loss.')
        sys.exit()
    
    # hat_v = np.float32(hat_v)
    
    # hat_v = torch.from_numpy(hat_v)
    
#     hat_v = hat_v.long()
    
#     print('time for solve linear system: ', time.time() - start_time)
    
    # print('hat_v: ', hat_v)

#     hat_v = torch.ones(N2)
    
#     print('hat_v: ', hat_v)
#     print('1 - hat_v: ', 1 - hat_v)

    # compute natural gradient
    

    
#     start_time = time.time()
    
    
    
    delta = compute_sum_J_transpose_V_backp(hat_v, data_, params)
    # delta = model_grad
    # print('test compute_sum_J_transpose_V_backp')
    
#     print('time for compute J transpose V: ', time.time() - start_time)
    
#     print('\n')
    

    
    
        

        
        

        

    delta = get_multiply_scalar(1 / N2, delta)
    
    # print('delta')
    # print(delta)
    
    
    
    delta = get_subtract_torch(model_grad, delta)
    
    delta = get_multiply_scalar(1 / lambda_, delta)
    
        
    p = get_opposite(delta)
   
    data_['p_torch'] = p
        
    return data_



def compute_sum_J_transpose_V(v, data_, params):
    a_grad_momentum = data_['a_grad_momentum'] # N2 * m[l+1]
    h_momentum = data_['h_momentum'] # N2 * m[l]
    
    numlayers = params['numlayers']
    
    delta = []
    # delta['W'] = list(range(numlayers))
    # delta['b'] = list(range(numlayers))

    
    # test_start_time = time.process_time()
    

    # print('time for method 1')
    # print(time.process_time() - test_start_time)

    # ===========

    # test_start_time = time.process_time()

    for l in range(numlayers):
        delta_l = {}
        delta_l['b'] = torch.mv(a_grad_momentum[l].t(), v)
        delta_l['W'] = torch.mm(
            (v[:, None] * a_grad_momentum[l]).t(), h_momentum[l])
        delta.append(delta_l)

    # print('time for method 2')
    # print(time.process_time() - test_start_time)

    # print('test new method')
    # sys.exit()
    
    
    return delta





def update_lambda(p, data_, params):
    true_algorithm = params['algorithm']
    if params['algorithm'] in ['SMW-Fisher-signVAsqrt-p',
                               'SMW-Fisher-VA-p',
                               'SMW-Fisher-momentum-p-sign',
                               'SMW-Fisher-momentum-p',
                               'SMW-Fisher-momentum',
                               'SMW-Fisher-sign']:
        params['algorithm'] = 'SMW-Fisher'
    elif params['algorithm'] in ['kfac-momentum-grad',
                                 'kfac-EF',
                                 'kfac-TR',
                                 'kfac-momentum-grad-TR',
                                 'kfac-CG',
                                 'kfac-momentum-grad-CG',]:
        params['algorithm'] = 'kfac'

    model = data_['model']
    X_mb_N1 = data_['X_mb_N1']
    t_mb_N1 = data_['t_mb_N1']
    loss_N1 = data_['regularized_loss']
    
#     model_grad = data_['model_regularized_grad_used_torch']
    model_grad = data_['model_grad_used_torch']
    
    numlayers = params['numlayers']
    lambda_ = params['lambda_']
    boost = params['boost']
    drop = params['drop']
    
    algorithm = params['algorithm']
    
    
    # compute rho
      

#     [ll_chunk, ~] =...
#             computeLL(paramsp + test_p, indata, outdata, numchunks, targetchunk)

#     print('model.W[1].grad: ', model.W[1].grad)


        
    ll_chunk = get_new_loss(model, p, X_mb_N1, t_mb_N1, params)
    oldll_chunk = loss_N1



   
    
        
        
    if oldll_chunk - ll_chunk < 0:
        rho = float("-inf")
    else:
        if algorithm in ['SMW-Fisher-different-minibatch',
                         'SMW-Fisher',
                         'SMW-GN',
                         'GI-Fisher',
                         'matrix-normal-same-trace',
                         'matrix-normal',
                         'Kron-BFGS-LM',
                         'Kron-BFGS-LM-sqrt']:
            denom = - 0.5 * get_dot_product_torch(model_grad, p)
        elif algorithm in ['SMW-Fisher-batch-grad-momentum-exponential-decay',
                           'ekfac-EF',
                           'kfac',
                           'kfac-test',
                           'kfac-no-max',
                           'kfac-NoMaxNoSqrt',
                           'SMW-Fisher-momentum',
                           'SMW-Fisher-D_t-momentum',
                           'SMW-Fisher-momentum-D_t-momentum',
                           'SMW-Fisher-BD',
                           'RMSprop-individual-grad-no-sqrt-LM',
                           'SMW-Fisher-batch-grad',
                           'SMW-Fisher-batch-grad-momentum']:
#             print('error: should use grad on N1')
#             sys.exit()
            
            denom = computeFV(p, data_, params)
                
            denom = get_dot_product_torch(p, denom)
            
            
            denom = -0.5 * denom
            denom = denom - get_dot_product_torch(model_grad, p)
                
        else:
            print('algorithm')
            print(algorithm)
            print('Error! 1504')
            sys.exit()
        
        rho = (oldll_chunk - ll_chunk) / denom
    
    
    # update lambda   
    if rho < 0.25:
        lambda_ = lambda_ * boost
    elif rho > 0.75:
        lambda_ = lambda_ * drop

    if true_algorithm in ['SMW-Fisher-signVAsqrt-p',
                          'SMW-Fisher-VA-p',
                          'SMW-Fisher-momentum-p-sign',
                          'SMW-Fisher-momentum-p',
                          'SMW-Fisher-momentum',
                          'SMW-Fisher-sign',
                          'kfac-momentum-grad',
                          'kfac-TR',
                          'kfac-momentum-grad-TR',
                          'kfac-EF',
                          'kfac-CG',
                          'kfac-momentum-grad-CG',]:
        params['algorithm'] = true_algorithm
        
    return lambda_

def GI_Fisher_update(data_, params):
    model_grad = data_['model_grad_used']

    # if algorithm == 'SMW-Fisher-momentum':
        # a_grad_momentum = data_['a_grad_momentum']
        # h_momentum = data_['h_momentum']

    # print('model_grad[b][1]')
    # print(model_grad['b'][1])
        
        
    
    N1 = params['N1']
    N2 = params['N2']
    # i = params['i']
    # lambda_ = params['lambda_']
    # numlayers = params['numlayers']
    
    
    
    
    # N2_index = np.random.permutation(N1)[:N2]
    # N2_index = params['N2_index']

    data_ = get_cache_momentum(data_, params)

    # compute 1 / n * J * J^T := G and do cho fac

    # params['lambda_'] = 10**(-8)
    # params['lambda_'] = 10**(-5)
    # params['lambda_'] = 10**(-3)
    G = get_D_t(data_, params)

    # G_cho_fac = scipy.linalg.cho_factor(G)
    G_cho = torch.cholesky(G.data)
    

    # compute J * g
    v = compute_JV(model_grad, data_, params)
    

    # compute G^{-1} * (J * g)
    # hat_v = scipy.linalg.cho_solve(G_cho_fac, v)

    # hat_v, _ = torch.solve(v.data, G.data)
    hat_v = torch.cholesky_solve(v.data, G_cho)
    

    # compute G^{-1} * (G^{-1} * J * g)
    # hat_v = scipy.linalg.cho_solve(G_cho_fac, hat_v)


    # hat_v = np.float32(hat_v)
    # hat_v = torch.from_numpy(hat_v)

    # hat_v, _ = torch.solve(hat_v.data, G.data)
    hat_v = torch.cholesky_solve(hat_v.data, G_cho)



    # compute J^T * (G^{-1} * G^{-1} * J * g)
    delta = compute_sum_J_transpose_V_backp(hat_v, data_, params)

    # dividing by n
    delta = get_multiply_scalar(1 / N2, delta)

    # get minus
    p = get_opppsite(delta, params)
    data_['p'] = p
    return data_, params
   

    
    



def SMW_Fisher_batch_grad_update(data_, params):
    model_grad = data_['model_grad_used']
    
    N_iters = params['N_iters']
    N2 = params['N2']
    lambda_ = params['lambda_']
    numlayers = params['numlayers']

    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
    params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':
        data_ = get_cache_momentum(data_, params)
    elif params['algorithm'] == 'SMW-Fisher-batch-grad':
        1
    else:
        print('Error: need more on cache')
        sys.exit()

    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay':
        # print('params[rho_kfac]')
        # print(params['rho_kfac'])

        rho_kfac = params['rho_kfac']
        N_current = len(data_['batch_grads'])
        c_weights = np.asarray(list(range(N_current)))
        c_weights = N_current - 1 - c_weights
        c_weights = np.power(rho_kfac, c_weights)
        c_weights = c_weights * (1 - rho_kfac) / (1 - (rho_kfac**N_current))

        # print('c_weights')
        # print(c_weights)
        # print('np.sum(c_weights)')
        # print(np.sum(c_weights))

        # print('test weight')

    # compute J * g
    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
    params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':
        batch_grads = data_['batch_grads']

        # test_start_time = time.process_time()

        v = np.zeros(len(batch_grads))
        for i in range(len(batch_grads)):
            v[i] = get_dot_product(model_grad, batch_grads[i])

        if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay':
            v = np.sqrt(c_weights) * v

        # test_time = time.process_time() - test_start_time
        # print('time for method 1')
        # print(test_time)

        # ======

        # batch_grads_test = data_['batch_grads_test']

        # test_start_time = time.process_time()

        # v_test = get_dot_product_batch(model_grad, batch_grads_test, params)

        # test_time = time.process_time() - test_start_time
        # print('time for method 2')
        # print(test_time)

        # ================

        # test_start_time = time.process_time()

        # v_test = np.zeros(len(batch_grads))
        # for i in range(len(batch_grads)):
            # batch_grads_a_grad_i = data_['batch_grads_a_grad'][i]
            # batch_grads_h_i = data_['batch_grads_h'][i]
            # for l in range(numlayers):

                
                # v_test[i] += 1 / N2 * torch.sum(
                    # torch.mm(
                        # batch_grads_a_grad_i[l],
                        # torch.from_numpy(model_grad['W'][l]).float()) * batch_grads_h_i[l])
        
            
                # v_test[i] += 1 / N2 * torch.sum(
                    # torch.from_numpy(model_grad['b'][l]).float() * batch_grads_a_grad_i[l])
                
        # print('max(v_test - v)')
        # print(max(v_test - v))
        # print('min(v_test - v)')
        # print(min(v_test - v))

        # test_time = time.process_time() - test_start_time
        # print('time for method 2')
        # print(test_time)

        

        # print('test new J g')
        # sys.exit()
    elif params['algorithm'] == 'SMW-Fisher-batch-grad':
        model_grad_N2 = data_['model_regularized_grad_N2']
        v = get_dot_product(model_grad, model_grad_N2)
    else:
        print('Error: need more J')
        sys.exit()
        

    # compute D_t
    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
    params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':
        N_iters = params['N_iters']

        # delete old
        if len(data_['D_t_minus_lambda']) == N_iters:
            data_['D_t_minus_lambda'] = data_['D_t_minus_lambda'][1:, 1:]

        # add new
        if len(batch_grads) == 1:
            data_['D_t_minus_lambda'] =\
            np.ones((1,1)) * get_dot_product(batch_grads[0], batch_grads[0])
        else:

            # test_start_time = time.process_time()

            # D_t_i = np.zeros((len(batch_grads), 1))
            # for i in range(len(batch_grads)):
                # D_t_i[i, 0] = get_dot_product(batch_grads[-1], batch_grads[i])

            # test_time = time.process_time() - test_start_time
            # print('time for method 1')
            # print(test_time)

            # ================

            # test_start_time = time.process_time()

            D_t_i = np.zeros((len(batch_grads), 1))
            batch_grads_a_grad_j = data_['batch_grads_a_grad'][-1]
            batch_grads_h_j = data_['batch_grads_h'][-1]
            for i in range(len(batch_grads)):
                batch_grads_a_grad_i = data_['batch_grads_a_grad'][i]
                batch_grads_h_i = data_['batch_grads_h'][i]
                for l in range(numlayers):

                    D_t_i[i, 0] += 1 / (N2**2) * (
                        torch.mul(torch.mm(batch_grads_a_grad_j[l], batch_grads_a_grad_i[l].t()),
                                    torch.mm(batch_grads_h_j[l], batch_grads_h_i[l].t()) + 1)).sum()
                    
            # test_time = time.process_time() - test_start_time
            # print('time for method 2')
            # print(test_time)
            
            # print('max(D_t_i_test - D_t_i)')
            # print(max(D_t_i_test - D_t_i))
            # print('min(D_t_i_test - D_t_i)')
            # print(min(D_t_i_test - D_t_i))
            # print('test new D_t')
            # sys.exit()

            data_['D_t_minus_lambda'] = np.concatenate((data_['D_t_minus_lambda'], D_t_i[:-1]), axis=1)
            data_['D_t_minus_lambda'] = np.concatenate(
                (data_['D_t_minus_lambda'], np.transpose(D_t_i)), axis=0)
            
        if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay':
            D_t = data_['D_t_minus_lambda'] * np.outer(np.sqrt(c_weights), np.sqrt(c_weights))
        elif params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':
            D_t = 1 / len(data_['D_t_minus_lambda']) * data_['D_t_minus_lambda']
        D_t = D_t + lambda_ * np.eye(len(data_['D_t_minus_lambda']))
            
    elif params['algorithm'] == 'SMW-Fisher-batch-grad':
        D_t = lambda_
        D_t += get_dot_product(model_grad_N2, model_grad_N2)
    else:
        print('Error: need more D_t')
        sys.exit()
        


    # compute D_t^{-1} * (J * g)
    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
    params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':# D_t_cho_fac = scipy.linalg.cho_factor(D_t)
        # hat_v = scipy.linalg.cho_solve(D_t_cho_fac, v)

        # hat_v = torch.solve(v.data, D_t.data).data

        D_t_cho = torch.cholesky(D_t.data)
        hat_v = torch.cholesky_solve(v.data, D_t_cho)
    elif params['algorithm'] == 'SMW-Fisher-batch-grad':
        hat_v = v / D_t
    else:
        print('Error: need more solve')
        sys.exit()
        

    # compute J^T * (D_t^{-1} * (J * g))
    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
    params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':
        if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay':
            hat_v = hat_v * np.sqrt(c_weights)

        
        batch_grads_test = data_['batch_grads_test']

        # test_start_time = time.process_time()

        p = get_weighted_sum_batch(hat_v, batch_grads_test, params)

        # test_time = time.process_time() - test_start_time
        # print('time for method 2')
        # print(test_time)

        # print('max(test_p[W][0] - p[W][0])')
        # print((test_p['W'][0] - p['W'][0]).max())
        # print('min(test_p[W][0] - p[W][0])')
        # print((test_p['W'][0] - p['W'][0]).min())
        # print('max(test_p[b][1] - p[b][1])')
        # print((test_p['b'][1] - p['b'][1]).max())
        # print('min(test_p[b][1] - p[b][1])')
        # print((test_p['b'][1] - p['b'][1]).min())

        # print('test new J^T')
    elif params['algorithm'] == 'SMW-Fisher-batch-grad':
        p = get_multiply_scalar(hat_v, model_grad_N2)
    else:
        print('Error: need more transpose')
        sys.exit()
        
    
    # rest of SMW
    if params['algorithm'] == 'SMW-Fisher-batch-grad-momentum':
        p = get_multiply_scalar(1 / N_iters, p)

    p = get_subtract(model_grad, p, params)
    
    p = get_multiply_scalar(1 / lambda_, p)

    
    p = get_opposite(p)
    data_['p'] = p
    return data_, params

def get_new_loss(model, p, x, t, params):
    
    model_new = copy.deepcopy(model)

    device = params['device']
    
    for l in range(model_new.numlayers):
        for key in model_new.layers_weight[l]:
            model_new.layers_weight[l][key].data += p[l][key].data
            
            
    

    reduction = 'mean'
    loss = get_regularized_loss_from_x_no_grad(
        model_new, x, t, reduction, params['tau'])

    return loss

def get_dot_product(delta_1, delta_2):
    dot_product = 0
    for l in range(len(delta_1)):
        for key in delta_1[l]:
            dot_product += np.sum(np.multiply(delta_1[l][key], delta_2[l][key]))
    return dot_product

def get_dot_product_blockwise(delta_1, delta_2):
    dot_product = []
    for l in range(len(delta_1)):
        dot_product_l = 0
        for key in delta_1[l]:
            dot_product_l += np.sum(np.multiply(delta_1[l][key], delta_2[l][key]))
        dot_product.append(dot_product_l)
    return dot_product

def get_dot_product_torch(delta_1, delta_2):
#     dot_product = 0
#     for delta_1_l, delta_2_l in zip(delta_1, delta_2):
#         dot_product += sum([torch.sum(torch.mul(delta_1_l[key], delta_2_l[key])).item() for key in delta_1_l])
    
    dot_product = sum(
        [
            sum(
                [
                    torch.sum(torch.mul(delta_1_l[key], delta_2_l[key])).item() for key in delta_1_l
                ]
            ) for delta_1_l, delta_2_l in zip(delta_1, delta_2)
        ]
    )
            
    return dot_product

def get_dot_product_blockwise_torch(delta_1, delta_2):
    dot_product = []
    for l in range(len(delta_1)):
        dot_product_l = 0
        for key in delta_1[l]:
            dot_product_l += torch.sum(torch.mul(delta_1[l][key], delta_2[l][key]))
        dot_product.append(dot_product_l)
    return dot_product

def get_dot_product_batch(model_grad, batch_grads_test, params):
    # numlayers = params['numlayers']
    
    dot_product = np.zeros(len(batch_grads_test['W'][0]))
    for l in range(params['numlayers']):
        dot_product += np.sum(
            np.sum(np.multiply(model_grad['W'][l][None, :], batch_grads_test['W'][l]), axis=-1), axis=-1)
        dot_product += np.sum(np.multiply(model_grad['b'][l][None, :], batch_grads_test['b'][l]), axis=-1)
    
    return dot_product

def get_square(delta_1):
    numlayers = len(delta_1)
    sqaure_p = []
    for l in range(numlayers):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = np.square(delta_1[l][key])
        sqaure_p.append(sqaure_p_l)  
    return sqaure_p

def get_square_torch(delta_1):
    numlayers = len(delta_1)
    sqaure_p = []
    for l in range(numlayers):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = torch.mul(delta_1[l][key], delta_1[l][key])
        sqaure_p.append(sqaure_p_l)  
    return sqaure_p

def get_sqrt(delta_1):
    sqaure_p = []
    for l in range(len(delta_1)):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = np.sqrt(delta_1[l][key])
        sqaure_p.append(sqaure_p_l) 
    return sqaure_p

def get_sqrt_torch(delta_1):
    sqaure_p = []
    for l in range(len(delta_1)):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = torch.sqrt(delta_1[l][key])
        sqaure_p.append(sqaure_p_l) 
    return sqaure_p

def get_max_with_0(delta_1):
    sqaure_p = []
    for l in range(len(delta_1)):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = F.relu(delta_1[l][key])
        sqaure_p.append(sqaure_p_l) 
    return sqaure_p

def get_divide(delta_1, delta_2):
    numlayers = len(delta_1)
    sqaure_p = []
    for l in range(numlayers):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = np.divide(delta_1[l][key], delta_2[l][key])
        sqaure_p.append(sqaure_p_l)
    return sqaure_p

def get_divide_torch(delta_1, delta_2):
    numlayers = len(delta_1)
    sqaure_p = []
    for l in range(numlayers):
        sqaure_p_l = {}
        for key in delta_1[l]:
            sqaure_p_l[key] = torch.div(delta_1[l][key], delta_2[l][key])
        sqaure_p.append(sqaure_p_l)
    return sqaure_p

"""
def get_mean(delta, params):
#     import torch
    numlayers = params['numlayers']
    for l in range(numlayers):
        delta[l] = torch.mean(delta[l], dim=0)
    return delta
"""


def computeFV(delta, data_, params):
    
    
    model = data_['model']
    
    N1 = params['N1']
    N2 = params['N2']

    # N2_index = params['N2_index']
    
    algorithm = params['algorithm']
    
#     a_grad_momentum = data_['a_grad_momentum']
#     h_momentum = data_['h_momentum']
    
    
#     import time
#     start_time = time.time()

    
    v = compute_JV(delta, data_, params)
    
#     print('time for FV 1/2: ', time.time() - start_time)

    if algorithm == 'SMW-GN':
        # v = v.data.numpy()
        v = get_HV(v, data_, params)
        # v = torch.from_numpy(v)
    
    

    delta = compute_sum_J_transpose_V_backp(v, data_, params)
    
    
    #############
#     N2 = params['N2']
#     m_L = params['m_L']
#     test_v = torch.zeros(m_L * N2)
#     test_v[0] = 1
    
#     print('print(compute_sum_J_transpose_V_backp(test_v, data_, params)): ', compute_sum_J_transpose_V_backp(test_v, data_, params))
    
#     print('test')
    
    

    
    

    
#     print('delta[1].size(): ', delta[1].size())
    
    
    
#     delta = get_mean(delta, params)
    
    delta = get_multiply_scalar(1 / N2, delta)

#     delta += 
    
    return delta




    
def get_homo_grad(model_grad_N1, params):
    device = params['device']

    homo_model_grad_N1 = []
    for l in range(params['numlayers']):
        if params['layers_params'][l]['name'] == 'fully-connected':
            
            homo_model_grad_N1_l = torch.cat(
                (model_grad_N1[l]['W'], model_grad_N1[l]['b'].unsqueeze(1)), dim=1)
        elif params['layers_params'][l]['name'] in ['conv',
                                                    'conv-no-activation']:
            # take Fashion-MNIST as an example
            # model_grad_N1[l]['W']: 32 * 1 * 5 * 5
            # model_grad_N1[l]['b']: 32
            # 32: conv_out_channels
            # 1: conv_in_channels
            # 5 * 5: conv_kernel_size
            
            homo_model_grad_N1_l = torch.cat(
                (
                    model_grad_N1[l]['W'].flatten(start_dim=1),
                    model_grad_N1[l]['b'].unsqueeze(dim=1)
                ),
                dim=1
            )
            
        elif params['layers_params'][l]['name'] in ['conv-no-bias-no-activation']:
            
            homo_model_grad_N1_l = model_grad_N1[l]['W'].flatten(start_dim=1)
            
        elif params['layers_params'][l]['name'] == 'BN':
            
            homo_model_grad_N1_l = torch.cat(
                (model_grad_N1[l]['W'], model_grad_N1[l]['b'])
            )
            
        else:
            print('Error: unsupported layer when homo grad for ' + params['layers_params'][l]['name'])
            sys.exit()
        homo_model_grad_N1.append(homo_model_grad_N1_l)

    return homo_model_grad_N1  
    





def Kron_SGD_update(data_, params):
    numlayers = params['numlayers']
    
    model_grad = data_['model_regularized_grad_used_torch']
    
    delta = []
    for l in range(numlayers):
        delta_l = {}
        
        mean_a_grad_l = torch.mean(data_['a_grad_N2'][l], dim=0)
        mean_h_l = torch.mean(data_['h_N2'][l], dim=0)
        
        delta_l['W'] = torch.ger(mean_a_grad_l, mean_h_l)
        delta_l['b'] = model_grad[l]['b']
        
#         print('delta_l[W].size()')
#         print(delta_l['W'].size())
        
        delta.append(delta_l)
        
    p = get_opposite(delta)
    data_['p_torch'] = p
        
    
    return data_, params








def HessianAction_scaled_BFGS_update(Kron_BFGS_matrices_l, l, data_, params):
    
    assert params['Kron_BFGS_action_h'] == 'HessianAction-scaled-BFGS'
    
    mean_h_l = torch.mean(data_['h_N2'][l], dim=0).data

    if params['Kron_BFGS_if_homo']:
        mean_h_l = torch.cat(
(
    mean_h_l, 
    torch.ones(1, device=params['device'])
),
dim=0
)

    H_l_h = Kron_BFGS_matrices_l['H']['h']
    s_l_h = torch.mv(H_l_h, mean_h_l)

#     if action_h == 'HessianAction-scaled-BFGS':
    beta_ = params['Kron_BFGS_A_decay']
    s_l_h = s_l_h / beta_
    y_l_h = torch.mv(Kron_BFGS_matrices_l['A_LM'], s_l_h)






    beta_ = params['Kron_BFGS_A_decay']
    H_l_h = H_l_h / beta_

    Kron_BFGS_matrices_l['H']['h'], update_status =\
get_BFGS_formula_v2(H_l_h, s_l_h, y_l_h, mean_h_l, False)
    
    print('torch.norm(Kron_BFGS_matrices_l[H][h])')
    print(torch.norm(Kron_BFGS_matrices_l['H']['h']))

    if update_status != 0:


        sys.exit()
    return Kron_BFGS_matrices_l








def get_BFGS_PowellB0Damping(s_l_a, y_l_a, params):
    
    print('need to move')
    sys.exit()
    
    # B_0 = 1 / gamma * I
    
    delta = params['Kron_BFGS_H_epsilon']
    
    s_T_y = torch.dot(s_l_a, y_l_a)
    y_T_y = torch.dot(y_l_a, y_l_a)
    
    gamma = (y_T_y / s_T_y).item()
    
#     gamma = torch.max(gamma, delta)
    if gamma < delta:
        gamma = delta
    
#     print('gamma')
#     print(gamma)
    
    
    
    alpha = params['Kron_BFGS_H_epsilon']

    s_T_s = torch.dot(s_l_a, s_l_a)
#     s_T_y = torch.dot(s_l_a, y_l_a)

    s_B_s = s_T_s/gamma

    if s_T_y / s_B_s > alpha:
        1
    else:
        theta =  (1-alpha) * s_B_s / (s_B_s - s_T_y)
        y_l_a = theta * y_l_a + (1-theta) * s_l_a/gamma
        
#     sys.exit()
    
    return s_l_a, y_l_a




def get_BFGS_DoubleDamping(s_l_a, y_l_a, l, data_, params):
    
    print('need to move')
    sys.exit()
    
    # DD merged into one step
    
    
    alpha = params['Kron_BFGS_H_epsilon']
        
    Kron_BFGS_matrices_l = data_['Kron_BFGS_matrices'][l]

    if params['Kron_BFGS_action_a'] == 'LBFGS':
        Hy = LBFGS_Hv(
            y_l_a,
            data_['Kron_LBFGS_s_y_pairs']['a_grad'][l],
            params,
            False
        )
    elif params['Kron_BFGS_action_a'] == 'BFGS':
        H_l_a_grad = Kron_BFGS_matrices_l['H']['a_grad']
        Hy = torch.mv(H_l_a_grad ,y_l_a)
    else:
        print('error: not implemented for ' + params['Kron_BFGS_action_a'])
        sys.exit()


    

#     if params['Kron_BFGS_action_a'] == 'LBFGS':

        
#     elif params['Kron_BFGS_action_a'] == 'BFGS':
        
#     else:
#         print('error: not implemented for ' + params['Kron_BFGS_action_a'])
#         sys.exit()
        
    s_T_y = torch.dot(s_l_a, y_l_a)

    yHy = torch.dot(y_l_a, Hy)
    
    s_T_s = torch.dot(s_l_a, s_l_a)
    
    sigma = max(yHy.item(), s_T_s.item())
    
#     print('sigma')
#     print(sigma)
#     sys.exit()

#     if s_T_y / yHy > alpha:
    if s_T_y / sigma > alpha:
        1
    else:
        if yHy >= s_T_s:
            theta =  ((1-alpha) * yHy / (yHy - s_T_y)).item()

#             original_s_l_a = s_l_a

            s_l_a = theta * s_l_a + (1-theta) * Hy
        else:
            theta =  (1-alpha) * s_T_s / (s_T_s - s_T_y)
            y_l_a = theta * y_l_a + (1-theta) * s_l_a
    
    return s_l_a, y_l_a
    




    


def get_block_BFGS_formula(H, s, y):
    
    D = s
    
    D_t_y_inv = torch.mm(D.t(), y).inverse()
    I = torch.eye(H.size()[0]).cuda()
    
    H = torch.mm(torch.mm(D, D_t_y_inv), D.t()) +\
    torch.mm(
        torch.mm(
            I - torch.mm(torch.mm(D, D_t_y_inv), y.t()), H),
        I - torch.mm(torch.mm(y, D_t_y_inv), D.t()))
    
    
    return H

def get_BFGS_formula(H, s, y, g_k):
    
    s = s.data
    y = y.data

    # ger(a, b) = a b^T
    rho_inv = torch.dot(s, y)

    if rho_inv <= 0:
#     if rho_inv <= 10**(-3):
#         print('BFGS not updated (case 1).')
#         print('rho_inv')
#         print(rho_inv)
    
        return H, 1
    elif rho_inv <= 10**(-4) * torch.dot(s, s) * np.sqrt(torch.dot(g_k, g_k).item()):
#         print('BFGS not updated (case 2).')
        return H, 2
    
#     print('rho_inv / (torch.dot(s, s) * np.sqrt(torch.dot(g_k, g_k).item()))')
#     print(rho_inv / (torch.dot(s, s) * np.sqrt(torch.dot(g_k, g_k).item())))

    # sHs = torch.dot(s, torch.mv(H, s))
    # if rho_inv < 0.25 * sHs:
        # theta = (0.75 * sHs) / (sHs - rho_inv)



    rho = 1 / rho_inv

    # s = s / np.sqrt(rho_inv.item())
    # y = y / np.sqrt(rho_inv.item())
    # rho = 1

    Hy = torch.mv(H, y)
    H_new = H.data + (rho**2 * torch.dot(y, torch.mv(H, y)) + rho) * torch.ger(s, s) -\
    rho * (torch.ger(s, Hy) + torch.ger(Hy, s))
    

    
    if torch.norm(H_new) > 2 * torch.norm(H):
#     if torch.norm(H_new) > 5 * torch.norm(H) or\
#     torch.max(torch.isinf(H_new)):
#         print('BFGS not updated (case 3).')
        return H, 3
#         H = H_new
    elif torch.max(torch.isinf(H_new)):
        return H, 4
    else:
        H = H_new

    if torch.max(torch.isinf(H)):
        print('inf in H')
        print('s')
        print(s)
        print('y')
        print(y)
        sys.exit()

    return H, 0








def get_CG(func_A, b, x, max_iter, data_):
    # solve A x = b

    # input:
    # x: initial point

    # https://gist.github.com/sfujiwara/b135e0981d703986b6c2

    # print('x.size()')
    # print(x.size())
    # print('b.size()')
    # print(b.size())
    # print('func_A(x).size()')
    # print(func_A(x).size())

    r = func_A(x) - b
    p = - r
    r_k_norm = torch.sum(r * r)
    i = 0
    while i < max_iter:
        Ap = func_A(p)
        alpha = r_k_norm / torch.sum(p * Ap)

        # if alpha != alpha:
            # print('nan in alpha')
            # print('r_k_norm')
            # print(r_k_norm)
            # print('torch.sum(p * Ap)')
            # print(torch.sum(p * Ap))
            # print('torch.sum(p * p)')
            # print(torch.sum(p * p))
            # print('p')
            # print(p)
            # print('i')
            # print(i)
            # print('torch.sum(Ap * Ap)')
            # print(torch.sum(Ap * Ap))
            # sys.exit()

        x += alpha * p

        # if torch.sum(x != x):
            # print('nan in x')
            # print('x')
            # print(x)
            # print('p')
            # print(p)
            # sys.exit()

        r += alpha * Ap
        r_kplus1_norm = torch.sum(r * r)
        beta = r_kplus1_norm / r_k_norm
        r_k_norm = r_kplus1_norm

        if r_kplus1_norm < 1e-10:
        # if torch.sum(x * func_A(x)) - torch.sum(x * b) < 1e-10:
            break
        p = beta * p - r
        i += 1

    # if torch.sum(x != x):
        # print('nan in x')
        # print('x')
        # print(x)
        # sys.exit()

    return x


def get_safe_division(x, y):
    # if y == 0:
        # return 1e16
    if x == 0 and y == 0:
        print('Error: x = 0 and y = 0 in safe division')
        sys.exit()
    elif x == 0 and y !=0:
        return 0
    elif x != 0 and y == 0:
        return 1e16
    else:
        # print('np.log(x)')
        # print(np.log(x))
        # print('np.log(y)')
        # print(np.log(y))
        # print('np.log(x) - np.log(y)')
        # print(np.log(x) - np.log(y))
        if np.log(x) - np.log(y) < np.log(1e16):
            return np.exp(np.log(x) - np.log(y))
        else:
            return 1e16


def SGD_update(data_, params):
    true_algorithm = params['algorithm']
    if params['algorithm'] in ['SGD-yura-MA',
                               'SGD-yura',
                               'SGD-momentum-yura',
                               'SGD-VA',
                               'SGD-signVAsqrt',
                               'SGD-signVAerf',
                               'SGD-signVA',
                               'SGD-sign']:
        params['algorithm'] = 'SGD'
    elif params['algorithm'] == 'SGD-yura-old':
        params['algorithm'] = 'SGD-yura'

#     model_grad = data_['model_regularized_grad_used_torch']
    model_grad = data_['model_grad_used_torch']
        
    p = get_opposite(model_grad)
    # p_torch = get_opposite(model_grad_torch, params)

    if params['algorithm'] == 'SGD-yura' or\
    params['algorithm'] == 'SGD-yura-BD':
        # alpha = 2
        alpha = 1
        
        print('check whether we should use alpha or alpha_current')
        sys.exit()
        

        if params['i'] == 0:
            if params['algorithm'] == 'SGD-yura':
                lambda_0 = 1
                lambda_k = lambda_0
                theta_k = 10**10
            elif params['algorithm'] == 'SGD-yura-BD':
                lambda_0 = [1] * params['numlayers']
                lambda_k = lambda_0
                theta_k = [10**10] * params['numlayers']
                # print('test')
        else:
            lambda_k_minus_1 = params['yura_lambda']
            theta_k_minus_1 = params['yura_theta']
            weights_k_minus_1 = params['yura_weights']
            grad_k_minus_1 = params['yura_grad']

            
            # get previous grad
            model_new = copy.deepcopy(data_['model'])
            # model_new = get_model(params)


            device = params['device']
            for l in range(model_new.numlayers):
                for key in model_new.layers_weight[l]:

                    # model_new.layers_weight[l][key].data = torch.from_numpy(weights_k_minus_1[l][key]).float().to(device)
                    model_new.layers_weight[l][key].data = weights_k_minus_1[l][key].data
            

            reduction = 'mean'
            loss = get_regularized_loss_from_x(model_new, data_['X_mb'], data_['t_mb'], reduction)

            model_new.zero_grad()

            loss.backward()

            grad_k_minus_1_torch = get_model_grad(model_new, params)

            

            



            # diff_grad = get_subtract(model_grad, grad_k_minus_1, params)
            diff_grad_torch = get_subtract_torch(model_grad_torch, grad_k_minus_1_torch)

            if params['algorithm'] == 'SGD-yura':

                # diff_weights = get_subtract_torch(data_['model'].layers_weight, weights_k_minus_1)
                # norm_sqaure_diff_weights = get_dot_product_torch(diff_weights, diff_weights).cpu().data.numpy()
                norm_sqaure_diff_weights =\
                get_dot_product_torch(data_['p_torch'], data_['p_torch']) * (params['alpha']**2)
                norm_sqaure_diff_weights_np = norm_sqaure_diff_weights.cpu().data.numpy()

                # norm_sqaure_diff_grad = get_dot_product(diff_grad, diff_grad)

                norm_sqaure_diff_grad_torch = get_dot_product_torch(diff_grad_torch, diff_grad_torch)
                norm_sqaure_diff_grad_np = norm_sqaure_diff_grad_torch.cpu().data.numpy()

                # print('norm_sqaure_diff_grad')
                # print(norm_sqaure_diff_grad)
                # print('norm_sqaure_diff_grad_torch')
                # print(norm_sqaure_diff_grad_torch)
                # sys.exit()

                L_k_inv = np.sqrt(get_safe_division(norm_sqaure_diff_weights_np,
                    norm_sqaure_diff_grad_np))
                lambda_k = min(np.sqrt(1 + theta_k_minus_1 / 10) * lambda_k_minus_1, L_k_inv / alpha)

                if lambda_k == 0:
                    print('Warning: lambda_k == 0')
                    lambda_k = lambda_k_minus_1
                    # print('norm_sqaure_diff_grad_np')
                    # print(norm_sqaure_diff_grad_np)
                    # print('norm_sqaure_diff_grad_torch')
                    # print(norm_sqaure_diff_grad_torch)
                    # print('norm_sqaure_diff_weights_np')
                    # print(norm_sqaure_diff_weights_np)
                    # print('norm_sqaure_diff_weights')
                    # print(norm_sqaure_diff_weights)
                    # sys.exit()

                theta_k = lambda_k / lambda_k_minus_1

                # print('lambda_k')
                # print(lambda_k)
                # print('np.sqrt(1 + theta_k_minus_1 / 10) * lambda_k_minus_1')
                # print(np.sqrt(1 + theta_k_minus_1 / 10) * lambda_k_minus_1)
                # print('L_k_inv / alpha')
                # print(L_k_inv / alpha)
                # print('L_k_inv')
                # print(L_k_inv)
                # print('norm_sqaure_diff_weights_np')
                # print(norm_sqaure_diff_weights_np)
                # print('norm_sqaure_diff_grad_np')
                # print(norm_sqaure_diff_grad_np)
            elif params['algorithm'] == 'SGD-yura-BD':
                lambda_k = []
                theta_k = []

                norm_sqaure_diff_weights =\
                get_dot_product_blockwise_torch(data_['p_torch'], data_['p_torch']) * (params['alpha']**2)
                norm_sqaure_diff_weights = [element_.cpu().data.numpy() for element_ in norm_sqaure_diff_weights]

                norm_sqaure_diff_grad = get_dot_product_blockwise(diff_grad, diff_grad)

                for l in range(params['numlayers']):

                    



                    L_k_inv = np.sqrt(get_safe_division(norm_sqaure_diff_weights[l], norm_sqaure_diff_grad[l]))
                    lambda_k_l = min(
                        np.sqrt(1 + theta_k_minus_1[l] / 10) * lambda_k_minus_1[l], L_k_inv / alpha)
                    
                    if lambda_k_l == 0:
                        print('Error: lambda_k_l == 0')
                        print('l')
                        print(l)
                        print('lambda_k_l')
                        print(lambda_k_l)
                        print('np.sqrt(1 + theta_k_minus_1[l] / 10) * lambda_k_minus_1[l]')
                        print(np.sqrt(1 + theta_k_minus_1[l] / 10) * lambda_k_minus_1[l])
                        print('L_k_inv / alpha')
                        print(L_k_inv / alpha)
                        print('L_k_inv')
                        print(L_k_inv)
                        print('norm_sqaure_diff_weights[l]')
                        print(norm_sqaure_diff_weights[l])
                        sys.exit()

                    if lambda_k_minus_1[l] == 0:
                        print('Error: lambda_k_minus_1[l] == 0')
                        print('l')
                        print(l)
                        print('lambda_k_minus_1[l]')
                        print(lambda_k_minus_1[l])
                        sys.exit()

                    theta_k_l = lambda_k_l / lambda_k_minus_1[l]

                    lambda_k.append(lambda_k_l)
                    theta_k.append(theta_k_l)
                    # print('test')

                

        if params['algorithm'] == 'SGD-yura':    
            p = get_multiply_scalar(lambda_k, p)
        elif params['algorithm'] == 'SGD-yura-BD':
            p = get_multiply_scalar_blockwise(lambda_k, p)

        params['yura_lambda'] = lambda_k
        params['yura_theta'] = theta_k
        params['yura_weights'] = copy.deepcopy(data_['model'].layers_weight)
        params['yura_grad'] = copy.deepcopy(model_grad)
    elif params['algorithm'] in ['SGD']:
        1
    else:
        print('Error: unkown algo when yura')
        sys.exit()
    
    if params['algorithm'] in ['SGD-LRdecay']:
        
        print('error: should not reach here')
        
        sys.exit()
        
#         if params['epoch'] > 0 and params['epoch'] % params['num_epoch_to_decay'] == 0 and params['i'] % params['iter_per_epoch'] == 0:
#             params['alpha_current'] *= params['lr_decay_rate']
        params['alpha_current'] =\
    params['alpha'] *\
    (params['lr_decay_rate'] ** (params['epoch'] // params['num_epoch_to_decay']))
    elif params['algorithm'] in ['SGD']:
        pass
    else:
        print('params[algorithm]')
        print(params['algorithm'])
        sys.exit()


    data_['p_torch'] = p
    

    if true_algorithm in ['SGD-yura', 
                          'SGD-yura-MA', 
                          'SGD-momentum-yura',
                          'SGD-momentum',
                          'SGD-VA',
                          'SGD-signVA',
                          'SGD-signVAerf',
                          'SGD-signVAsqrt',
                          'SGD-sign']:
        params['algorithm'] = true_algorithm
    elif true_algorithm == 'SGD-yura-old':
        params['algorithm'] = true_algorithm
    
    
        
    return data_

def RMSprop_update(data_, params):
    
#     print('move to another file')
    
#     model_grad = data_['model_grad_used']
    model_grad = data_['model_grad_used_torch']

    algorithm = params['algorithm']
    
    if algorithm in ['Adam',
                     'Adam-noWarmStart']:
        beta_1 = params['momentum_gradient_rho']
        
        assert params['momentum_gradient_rho'] == params['momentum_gradient_dampening']
        
        i = params['i']
        
        model_grad = get_multiply_scalar(1 / (1 - beta_1**(i+1)), model_grad)
        
    elif algorithm in ['RMSprop',
                       'RMSprop-warmStart']:
        1
    else:
        print('error: check if bias correction for grad for ' + algorithm)
        sys.exit()

    if algorithm == 'RMSprop-individual-grad-no-sqrt-LM':
        epsilon = params['lambda_']
    elif algorithm in ['RMSprop-individual-grad-no-sqrt-Fisher',
                       'RMSprop-individual-grad-no-sqrt',
                       'RMSprop-individual-grad',
                       'RMSprop-no-sqrt',
                       'RMSprop',
                       'RMSprop-warmStart',
                       'RMSprop-test',
                       'Adam',
                       'Adam-test',
                       'Adam-noWarmStart']:
        epsilon = params['RMSprop_epsilon']
    else:
        print('Error: undefined epsilon.')
        sys.exit()
    
#     print('params.keys()')
#     print(params.keys())
    
#     print('RMSprop_beta_2 in params')
#     print('RMSprop_beta_2' in params)
    
#     print('params[RMSprop_beta_2]')
#     print(params['RMSprop_beta_2'])
    
#     sys.exit()
    
#     beta_2 = 0.9
    beta_2 = params['RMSprop_beta_2']
    
#     print('beta_2')
#     print(beta_2)
    
#     sys.exit()
        
    
    if algorithm in ['RMSprop',
                     'RMSprop-warmStart',
                     'RMSprop-test',
                     'Adam',
                     'Adam-test',
                     'Adam-noWarmStart',
                     'RMSprop-no-sqrt']:
        data_['RMSprop_momentum_2'] =\
        get_plus_torch(
            get_multiply_scalar(beta_2, data_['RMSprop_momentum_2']), 
            get_multiply_scalar(1-beta_2, get_square_torch(model_grad)))
    elif algorithm == 'RMSprop-individual-grad' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-Fisher' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-LM':
        a_grad_N2 = data_['a_grad_N2']
        h_N2 = data_['h_N2']

        model = data_['model']

        N2 = params['N2']

        for l in range(model.numlayers):
            if params['layers_params'][l]['name'] == 'fully-connected':

                h_l_square = torch.mul(h_N2[l], h_N2[l])
                a_grad_l_square = torch.mul(a_grad_N2[l], a_grad_N2[l]) # N2 * m_l

                W_l_square = torch.mm(h_l_square.t(), a_grad_l_square) / N2

                data_['RMSprop_momentum_2'][l]['W'] =\
                beta_2 * data_['RMSprop_momentum_2'][l]['W'] +\
                (1-beta_2) * W_l_square.t().cpu().data.numpy()

                data_['RMSprop_momentum_2'][l]['b'] =\
                beta_2 * data_['RMSprop_momentum_2'][l]['b'] +\
                (1-beta_2) * torch.mean(a_grad_l_square, dim=0).cpu().data.numpy()
            elif params['layers_params'][l]['name'] == 'conv':
                print('h_N2[l].size')
                print(h_N2[l].size())
                print('a_grad_N2[l].size')
                print(a_grad_N2[l].size())
                print('model_grad[l][W].shape()')
                print(model_grad[l]['W'].shape)

                h_N2_l_pad = F.pad(h_N2[l], (2,2,2,2))

                print('h_N2_l_pad.size()')
                print(h_N2_l_pad.size())

                # print('range(model_grad[l][W].size()[0])')
                # print(range(model_grad[l]['W'].size()[0]))

                for i in range(model_grad[l]['W'].shape[0]):
                    for j in range(model_grad[l]['W'].shape[1]):
                        for test_h in range(model_grad[l]['W'].shape[2]):
                            for test_w in range(model_grad[l]['W'].shape[3]):
                                print('i, j, test_h, test_w')
                                print(i, j, test_h, test_w)

                                # print('model_grad[l][W][i, j, test_h, test_w]')
                                # print(model_grad[l]['W'][i, j, test_h, test_w])

                                # print('torch.from_numpy(model_grad[l][W][i, j, test_h, test_w])')
                                # print(torch.from_numpy(model_grad[l]['W'][i, j, test_h, test_w]))

                                # print('torch.from_numpy(model_grad[l][W][i, j, test_h, test_w]).float()')
                                # print(torch.from_numpy(model_grad[l]['W'][i, j, test_h, test_w]).float())

                                # print('torch.from_numpy(model_grad[l][W][i, j, test_h, test_w]).float().cuda()')
                                # print(torch.from_numpy(model_grad[l]['W'][i, j, test_h, test_w]).float().cuda())

                                print('torch.sum(torch.mean(torch.mul(a_grad_N2[l][:, i], h_N2_l_pad[:, j, test_h: test_h+28, test_w: test_w+28]), dim=0)) -\
                                      torch.from_numpy(model_grad[l][W][i, j, test_h, test_w]).float().cuda()')
                                print(torch.sum(torch.mean(torch.mul(a_grad_N2[l][:, i], h_N2_l_pad[:, j, test_h: test_h+28, test_w: test_w+28]), dim=0)) -\
                                      model_grad[l]['W'][i, j, test_h, test_w])

                                # sys.exit()




                for i in range(len(h_N2[l])):
                    print('h_N2_l_pad[i].size()')
                    print(h_N2_l_pad[i].size())
                    print('a_grad_N2[l][i].size()')
                    print(a_grad_N2[l][i].size())

                    h_N2_l_pad_i_expand = torch.unsqueeze(h_N2_l_pad[i], 0)

                    print('h_N2_l_pad_i_expand.size()')
                    print(h_N2_l_pad_i_expand.size())

                    sys.exit()
            else:
                print('Error: unknown layer when update rmsprop')
                sys.exit()
    else:
        print('Error: unsupported algorithm.')
        sys.exit()
        
    if algorithm in ['Adam',
                     'Adam-test',
                     'Adam-noWarmStart']:
        
        i = params['i']
        
        model_grad_second_moment = get_multiply_scalar(1 / (1 - beta_2**(i+1)), data_['RMSprop_momentum_2'])
        
    elif algorithm in ['RMSprop',
                       'RMSprop-warmStart']:
        model_grad_second_moment = data_['RMSprop_momentum_2']
    else:
        print('error: check if bias correction for grad for ' + algorithm)
        sys.exit()
    
        


    if algorithm in ['RMSprop',
                     'RMSprop-warmStart',
                     'RMSprop-test',
                     'Adam',
                     'Adam-test',
                     'Adam-noWarmStart',
                     'RMSprop-individual-grad']:
        # epsilon = 10**(-8)

        # epsilon = 10**(-4)
        
        

        p = get_divide_torch(
            model_grad, 
            get_plus_scalar(epsilon, get_sqrt_torch(model_grad_second_moment)))
    elif algorithm == 'RMSprop-individual-grad-no-sqrt' or\
    algorithm == 'RMSprop-no-sqrt' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-Fisher' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-LM':

        # epsilon = 10**(-4)
        # print('test epsilon')

        
        p = get_divide(
            model_grad, 
            get_plus_scalar(epsilon, model_grad_second_moment))

    else:
        print('Error: unsupported algorithm 2.')
        sys.exit()
    
    p = get_opposite(p)
    
    

    


    data_['p_torch'] = p
    return data_

def update_parameter(p_torch, model, params):
    numlayers = params['numlayers']
    alpha = params['alpha_current']
    device = params['device']

    
    for l in range(numlayers):
        
#         print('params[layers_params][l][name]')
#         print(params['layers_params'][l]['name'])
        
        for name_variable in model.layers_weight[l].keys():
            
            if params['weight_decay'] != 0:
                model.layers_weight[l][name_variable].data *= (1 - alpha*params['weight_decay'])
                
#             if params['layers_params'][l]['name'] == 'conv-no-activation' and name_variable == 'b':
                
#                 print('name_variable')
#                 print(name_variable)
            
#                 print('torch.norm(model.layers_weight[l][name_variable]).item()')
#                 print(torch.norm(model.layers_weight[l][name_variable]).item())
            
#                 print('torch.norm(p_torch[l][name_variable]).item()')
#                 print(torch.norm(p_torch[l][name_variable]).item())
                
#                 print('need to change back')
#                 continue
                
            
            model.layers_weight[l][name_variable].data += alpha * p_torch[l][name_variable].data
            
#             print('name_variable')
#             print(name_variable)
            
#             print('torch.norm(model.layers_weight[l][name_variable]).item()')
#             print(torch.norm(model.layers_weight[l][name_variable]).item())
            
#             print('torch.norm(p_torch[l][name_variable]).item()')
#             print(torch.norm(p_torch[l][name_variable]).item())
        

        
    return model

# input_data.py

"""Functions for downloading and reading MNIST data."""

import os
# import urllib
# import urllib.request
import numpy as np


import sys












    
    
def dense_to_one_hot(labels_dense, num_classes=10):
    """Convert class labels from scalars to one-hot vectors."""
    num_labels = labels_dense.shape[0]
    index_offset = np.arange(num_labels) * num_classes
    labels_one_hot = np.zeros((num_labels, num_classes))
    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
    return labels_one_hot



    
def load_subsampled_imagenet(train_dir):
    
    train_path = train_dir + '/' + 'YiRen_imagenet_sample/train/'
    transform = transforms.Compose(
        [transforms.Resize((256,256)), transforms.ToTensor()]
    )

    # in case you want to normalize images
    # normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
    #                                      std=[0.229, 0.224, 0.225])
    # transform = transforms.Compose(
    #     [transforms.Resize((256,256)), transforms.ToTensor(), normalize]
    # )

    imagenet_data = datasets.ImageFolder(train_path, transform=transform)
    data_loader = torch.utils.data.DataLoader(
        imagenet_data,
#         batch_size=4373,
        batch_size=200,
        shuffle=True,
        num_workers=0
    )
    
    return data_loader
    








def get_post_activation(pre_, activation):
    if activation == 'relu':
        post_ = F.relu(pre_)
    elif activation == 'sigmoid':
        post_ = torch.sigmoid(pre_)
    elif activation == 'tanh':
        post_ = torch.tanh(pre_)
    elif activation == 'linear':
        post_ = pre_
    else:
        print('Error: unsupported activation for ' + activation)
        sys.exit()
    return post_

def get_layer_forward(input_, layer_, activation_, layer_params):
    if layer_params['name'] == 'fully-connected':
            
        a_ = layer_(input_)
        h_ = get_post_activation(a_, activation_)
        a_.retain_grad()
        
        output_ = h_
        pre_ = a_
        
    elif layer_params['name'] in ['conv',
                                  '1d-conv']:
        
        
        a_ = layer_(input_)
        
#         print('a_.size()')
#         print(a_.size())
        
#         print('torch.norm(a_)')
#         print(torch.norm(a_))
        
        
        
        
        
        
        h_ = get_post_activation(a_, activation_)
        

        a_.retain_grad()
        
        output_ = h_
        pre_ = a_
        
    elif layer_params['name'] in ['conv-no-activation',
                                  'conv-no-bias-no-activation']:
        a_ = layer_(input_)
        
#         print('a_.size()')
#         print(a_.size())
        
#         print('torch.norm(a_)')
#         print(torch.norm(a_))
        
        
        a_.retain_grad()
        output_ = a_
        pre_ = a_
        
    elif layer_params['name'] in ['BN']:
        a_ = layer_(input_)
        a_.retain_grad()
        output_ = a_
        pre_ = a_
        
    else:
        print('layer_params[name]')
        print(layer_params['name'])
        print('Error: unkown layer')
        sys.exit()
    

    return output_, pre_
    # return a_, h_

def get_layers_params(name_model, layersizes, activations, params):
    if name_model == 'fully-connected':
        layers_ = []
        for l in range(len(layersizes) - 1):
            layer_i = {}
            layer_i['name'] = 'fully-connected'
            layer_i['input_size'] = layersizes[l]
            layer_i['output_size'] = layersizes[l+1]
            layer_i['activation'] = activations[l]
            layers_.append(layer_i)
    elif name_model == 'simple-CNN':
        # https://arxiv.org/pdf/1910.05446.pdf
        # https://arxiv.org/pdf/1811.03600.pdf

        # "same" padding:
        # i.e. H_in = H_out
        # by https://pytorch.org/docs/master/nn.html#torch.nn.Conv2d
        # H_out = H_in + 2 * padding - dilation * (kernel_size - 1)
        # (when stride = 1)
        # Hence, since dilation = 1, H_in = H_out => padding = (kernel_size - 1) / 2
        # this is also endorsed by https://discuss.pytorch.org/t/same-convolution-in-pytorch/19937
        
#         print('need to accomadate GAP')
#         sys.exit()

        layersizes = [32, 64, 1024]

        layers_ = []

        layer_1 = {}
        layer_1['name'] = 'conv'
        
        
        if params['name_dataset'] == 'Subsampled-ImageNet-simple-CNN':
            layer_1['conv_in_channels'] = 3
        elif params['name_dataset'] in ['Fashion-MNIST',
                                        'Fashion-MNIST-N1-60',
                                        'Fashion-MNIST-N1-60-no-regularization',
                                        'Fashion-MNIST-N1-256-no-regularization',
                                        'Fashion-MNIST-GAP-N1-60-no-regularization']:
            layer_1['conv_in_channels'] = 1
        else:
            print('error: need to check conv_in_channels for ' + params['name_dataset'])
            sys.exit()
            
        
        layer_1['conv_out_channels'] = layersizes[0]
        layer_1['conv_kernel_size'] = 5
        layer_1['conv_stride'] = 1
        layer_1['conv_padding'] = int((layer_1['conv_kernel_size'] - 1)/2)
        
        
#         layer_1['activation'] = activations[0]
        layer_1['activation'] = 'relu'
    
        layers_.append(layer_1)
        
        layer_1 = {}
        layer_1['name'] = 'max_pool'
        layer_1['max_pool_kernel_size'] = 2
        layer_1['max_pool_stride'] = 2
        
        layers_.append(layer_1)
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = layersizes[0]
        layer_2['conv_out_channels'] = layersizes[1]
        layer_2['conv_kernel_size'] = 5
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = int((layer_2['conv_kernel_size'] - 1)/2)
        
#         layer_2['activation'] = activations[1]
        layer_2['activation'] = 'relu'
        
        layers_.append(layer_2)
        
        if params['name_dataset'] == 'Fashion-MNIST-GAP-N1-60-no-regularization':
            # https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html
            
            layer_ = {}
            layer_['name'] = 'global_average_pooling'
#             layer_['max_pool_kernel_size'] = 2
#             layer_['max_pool_stride'] = 2

            layers_.append(layer_)
    
        elif params['name_dataset'] in ['Fashion-MNIST-N1-60-no-regularization',
                                        'Fashion-MNIST-N1-256-no-regularization']:
            
            layer_2 = {}
            layer_2['name'] = 'max_pool'
            layer_2['max_pool_kernel_size'] = 2
            layer_2['max_pool_stride'] = 2

            layers_.append(layer_2)


            layer_5 = {}
            layer_5['name'] = 'flatten'

            layers_.append(layer_5)
    
        else:
            print('error: need to check for ' + params['name_dataset'])
            sys.exit()
        
            
        

        layer_3 = {}
        layer_3['name'] = 'fully-connected'
        
        if params['name_dataset'] == 'Subsampled-ImageNet-simple-CNN':
            layer_3['input_size'] = 64 * 64 * layersizes[1]
        elif params['name_dataset'] in ['Fashion-MNIST',
                                        'Fashion-MNIST-N1-60',
                                        'Fashion-MNIST-N1-60-no-regularization',
                                        'Fashion-MNIST-N1-256-no-regularization']:
            layer_3['input_size'] = 7 * 7 * layersizes[1]
        elif params['name_dataset'] in ['Fashion-MNIST-GAP-N1-60-no-regularization']:
            layer_3['input_size'] = layersizes[1]
        else:
            print('error: need to check input_size for ' + params['name_dataset'])
            sys.exit()
            
        
        layer_3['output_size'] = layersizes[2]
        
        layer_3['activation'] = 'relu'
#         layer_3['activation'] = activations[2]

        layers_.append(layer_3)
        

        layer_4 = {}
        layer_4['name'] = 'fully-connected'
        layer_4['input_size'] = layer_3['output_size']
        
        if params['name_dataset'] == 'Subsampled-ImageNet-simple-CNN':
            layer_4['output_size'] = 200
        elif params['name_dataset'] in ['Fashion-MNIST',
                                        'Fashion-MNIST-N1-60',
                                        'Fashion-MNIST-N1-60-no-regularization',
                                        'Fashion-MNIST-N1-256-no-regularization',
                                        'Fashion-MNIST-GAP-N1-60-no-regularization']:
            layer_4['output_size'] = 10
        else:
            print('error: need to check output_size for ' + params['name_dataset'])
            sys.exit()
            
        
#         layer_4['activation'] = activations[3]
        layer_4['activation'] = 'linear'

        layers_.append(layer_4)
        
        
    elif name_model == 'CNN':
#         layersizes = [96, 192, 192, 10]
        layersizes = [96, 192, 192, 100]
        
        layers_ = []
        
        for l in range(3):
        
            layer_l = {}
            layer_l['name'] = 'conv'
            
            if l == 0:
                layer_l['conv_in_channels'] = 3
            else:
                layer_l['conv_in_channels'] = layersizes[l-1]
            
            
            layer_l['conv_out_channels'] = layersizes[l]
            
            layer_l['conv_kernel_size'] = 5
            layer_l['conv_stride'] = 1
            layer_l['conv_padding'] = int((layer_l['conv_kernel_size'] - 1)/2)
            
#             layer_l['activation'] = activations[l]
            layer_l['activation'] = 'relu'
    
            layers_.append(layer_l)
            
            layer_l = {}
            layer_l['name'] = 'max_pool'
            layer_l['max_pool_kernel_size'] = 2
            layer_l['max_pool_stride'] = 2
            layers_.append(layer_l)
            
        layer_5 = {}
        layer_5['name'] = 'flatten'
        layers_.append(layer_5)
            
        layer_4 = {}
        layer_4['name'] = 'fully-connected'
        layer_4['input_size'] = layersizes[2] * 4 * 4
        layer_4['output_size'] = layersizes[3]
        
#         layer_4['activation'] = activations[3]
        layer_4['activation'] = 'linear'
        
#         layer_4['if_flatten'] = True
        
        layers_.append(layer_4)
    elif name_model == '1d-CNN':
        # https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/
        
#         model = Sequential()

#         model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))

#         model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
#         model.add(Dropout(0.5))
#         model.add(MaxPooling1D(pool_size=2))

#         model.add(Flatten())
#         model.add(Dense(100, activation='relu'))

#         model.add(Dense(n_outputs, activation='softmax'))

#         model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        

        
        layersizes = [64, 64, 100, 6]
        
        layers_ = []
        
        for l in range(2):
        
            layer_l = {}
            layer_l['name'] = '1d-conv'
            
            if l == 0:
                layer_l['conv_in_channels'] = 1
            else:
                layer_l['conv_in_channels'] = layersizes[l-1]
            
            
            layer_l['conv_out_channels'] = layersizes[l]
            
            layer_l['conv_kernel_size'] = 3
            layer_l['conv_stride'] = 1
            layer_l['conv_padding'] = 0
            
#             if l == 0: 
#                 layer_l['if_max_pool'] = False
#             elif l == 1:
#                 layer_l['if_max_pool'] = True
#                 layer_l['max_pool_kernel_size'] = 2
#                 layer_l['max_pool_stride'] = 0
            
            
            layer_l['activation'] = activations[l]
            
            layers_.append(layer_l)
            
        layer_6 = {}
        layer_6['name'] = 'max_pool_1d'
        layer_6['max_pool_kernel_size'] = 2
        layer_6['max_pool_stride'] = 0
        layers_.append(layer_6)
            
        layer_5 = {}
        layer_5['name'] = 'flatten'
        layers_.append(layer_5)
            
        layer_3 = {}
        layer_3['name'] = 'fully-connected'
        layer_3['input_size'] = layersizes[1] * 278
        layer_3['output_size'] = layersizes[2]
        layer_3['activation'] = activations[2]
        
        layers_.append(layer_3)
        
        layer_4 = {}
        layer_4['name'] = 'fully-connected'
        layer_4['input_size'] = layersizes[2]
        layer_4['output_size'] = layersizes[3]
        layer_4['activation'] = activations[3]
#         layer_4['if_flatten'] = False
        
        layers_.append(layer_4)
        
    elif name_model == 'AllCNNC':
        # https://arxiv.org/pdf/1412.6806.pdf
        # https://github.com/mateuszbuda/ALL-CNN/blob/master/ALL_CNN_C.png
        
        # except that in the following, for the last 3 * 3 conv in Table 1,
        # we use padding = 1
        
        # see also:
        # 'CNN' in https://arxiv.org/pdf/1910.05446.pdf
        
        layers_ = []
        
        layers_ = add_conv_block(layers_, 3, 96, 3, 1, 1, params)
        layers_ = add_conv_block(layers_, 96, 96, 3, 1, 1, params)
        layers_ = add_conv_block(layers_, 96, 96, 3, 2, 1, params)
        
        layers_ = add_conv_block(layers_, 96, 192, 3, 1, 1, params)
        layers_ = add_conv_block(layers_, 192, 192, 3, 1, 1, params)
        layers_ = add_conv_block(layers_, 192, 192, 3, 2, 1, params)
        
        layers_ = add_conv_block(layers_, 192, 192, 3, 1, 1, params)
        
        layers_ = add_conv_block(layers_, 192, 192, 1, 1, 0, params)
        

        
        
        if params['name_dataset'] == 'CIFAR-100-onTheFly-AllCNNC':
            layers_ = add_conv_block(layers_, 192, 100, 1, 1, 0, params)
        elif params['name_dataset'] in ['CIFAR-10-AllCNNC',
                                        'CIFAR-10-N1-128-AllCNNC',
                                        'CIFAR-10-N1-512-AllCNNC']:
            layers_ = add_conv_block(layers_, 192, 10, 1, 1, 0, params)
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])
            sys.exit()
        
            
        
        layer_ = {}
        layer_['name'] = 'global_average_pooling'
        layers_.append(layer_)
        
    elif name_model == 'ConvPoolCNNC':
        # https://arxiv.org/pdf/1412.6806.pdf
        
        layers_ = []
        
        layers_ = add_conv_block(layers_, 3, 96, params)
        
        layers_ = add_conv_block(layers_, 96, 96, params)
        
        layers_ = add_conv_block(layers_, 96, 96, params)
        
        
        
        sys.exit()
        
    elif name_model in ['ResNet32']:
        
        # ResNet 32 in sec 4.2 and Table 6 of https://arxiv.org/pdf/1512.03385.pdf
        
        # see also:
        # https://pytorch.org/vision/stable/models.html
        
        # see also:
        # https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py
        
        # see also:
        # https://github.com/km1414/CNN-models/blob/master/resnet-32/resnet-32.py
        # (this one seems to have bias)
        
        # in the NoBias mode, conv layers don't have bias, including the first conv
        # see e.g. https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html#resnet34
        # (this webpage is no longer available)
        
        
        
        
        if params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BN',
                                      'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                                      'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                                      'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            if_BNNoAffine = False
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                                        'CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',
                                        'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',]:
            if_BNNoAffine = True
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])
            sys.exit()
            
        if params['name_dataset'] in ['CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            shortcut_type = 'padding'
            if_BN_shortcut = None
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',]:
            shortcut_type = 'conv'
            if_BN_shortcut = True
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',]:
            shortcut_type = 'conv'
            if_BN_shortcut = False
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])
            sys.exit()
            
            if params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',]:
                if_BN_shortcut = True
            elif params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                                            'CIFAR-10-onTheFly-ResNet32-BN']:
                if_BN_shortcut = False
            else:
                print('params[name_dataset]')
                print(params['name_dataset'])

                sys.exit()
            
        if params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                                      'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            if_downsample_only = True
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',]:
            if_downsample_only = False
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])

            sys.exit()
            
            
        if params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',
                                      'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                      'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                      'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            if_conv_bias = False
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly']:
            if_conv_bias = True
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])

            sys.exit()
            
        

        
        
        layers_ = []
        
        layers_ = add_conv_block(layers_, 3, 16, 3, 1, 1, params)
        
        layers_ = add_res_block(layers_, 16, 16, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 16, 16, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 16, 16, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 16, 16, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 16, 16, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        layers_ = add_res_block(layers_, 16, 32, 2, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 32, 32, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 32, 32, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 32, 32, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 32, 32, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        layers_ = add_res_block(layers_, 32, 64, 2, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        layer_ = {}
        layer_['name'] = 'global_average_pooling'
        layers_.append(layer_)
        
        layer_ = {}
        layer_['name'] = 'fully-connected'
        layer_['input_size'] = 64
        
        if params['name_dataset'] in ['CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            layer_['output_size'] = 100
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',]:
            layer_['output_size'] = 10
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])
            sys.exit()
        
            
        
        layer_['activation'] = 'linear'
        layers_.append(layer_)
        
    elif name_model in ['ResNet34']:
        
        # ResNet 34 in Fig 3 of https://arxiv.org/pdf/1512.03385.pdf
        
        # see also
        # https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/resnet.py
        
        if params['name_dataset'] == 'CIFAR-100-onTheFly-ResNet34-BNNoAffine':
            if_BNNoAffine = True
            shortcut_type = 'conv'
            if_BN_shortcut = False
            if_downsample_only = False
            if_conv_bias = True
        elif params['name_dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN':
            if_BNNoAffine = False
            shortcut_type = 'conv'
            if_BN_shortcut = False
            if_downsample_only = False
            if_conv_bias = True
        elif params['name_dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut':
            if_BNNoAffine = False
            shortcut_type = 'conv'
            if_BN_shortcut = True
            if_downsample_only = False
            if_conv_bias = True
        elif params['name_dataset'] == 'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly':
            if_BNNoAffine = False
            shortcut_type = 'conv'
            if_BN_shortcut = True
            if_downsample_only = True
            if_conv_bias = True
        elif params['name_dataset'] in ['CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                                        'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',]:
            if_BNNoAffine = False
            shortcut_type = 'conv'
            if_BN_shortcut = True
            if_downsample_only = True
            if_conv_bias = False
        elif params['name_dataset'] == 'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias':
            if_BNNoAffine = False
            shortcut_type = 'padding'
            if_BN_shortcut = None
            if_downsample_only = True
            if_conv_bias = False
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])
            sys.exit()
        
        
        layers_ = []
        
        layers_ = add_conv_block(layers_, 3, 64, 3, 1, 1, params)
        
        
        
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 64, 64, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        layers_ = add_res_block(layers_, 64, 128, 2, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 128, 128, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 128, 128, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 128, 128, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        layers_ = add_res_block(layers_, 128, 256, 2, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 256, 256, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 256, 256, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 256, 256, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 256, 256, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 256, 256, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        layers_ = add_res_block(layers_, 256, 512, 2, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 512, 512, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        layers_ = add_res_block(layers_, 512, 512, 1, if_BNNoAffine, shortcut_type, if_BN_shortcut, if_downsample_only, if_conv_bias)
        
        
        
        layer_ = {}
        layer_['name'] = 'global_average_pooling'
        layers_.append(layer_)
        
#         sys.exit()
        
        layer_ = {}
        layer_['name'] = 'fully-connected'
        layer_['input_size'] = 512
        layer_['output_size'] = 100
        layer_['activation'] = 'linear'
        layers_.append(layer_)
    
    elif name_model == 'vgg16':
        # referece:
        # import torchvision.models as models
        # vgg16 = models.vgg16()
        # print(vgg16.eval())
        
        # model D in https://arxiv.org/pdf/1409.1556.pdf
        
        # when use BN (or BNNoAffine), bias term is NOT omitted in conv
        # see e.g. https://pytorch.org/docs/stable/_modules/torchvision/models/vgg.html#vgg16_bn
        # see also https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py
        
        
        if params['name_dataset'] == 'Subsampled-ImageNet-vgg16':
            print('error: need to check all below')
            sys.exit()
            
        if params['name_dataset'] == 'CIFAR-10-vgg16':
            print('error: not supported anymore')
            sys.exit()
            
            
        
        
        if params['name_dataset'] == 'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool':
            print('error: not supported anymore')
            sys.exit()
        
        
        
        layers_ = []
        
        layers_ = add_conv_block(layers_, 3, 64, 3, 1, 1, params)
        
        layers_ = add_conv_block(layers_, 64, 64, 3, 1, 1, params)
        
#     (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
        layers_ = add_conv_block(layers_, 64, 128, 3, 1, 1, params)
        
        layers_ = add_conv_block(layers_, 128, 128, 3, 1, 1, params)
        
#         sys.exit()
        
#     (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
        layers_ = add_conv_block(layers_, 128, 256, 3, 1, 1, params)
        
        layers_ = add_conv_block(layers_, 256, 256, 3, 1, 1, params)
        
        layers_ = add_conv_block(layers_, 256, 256, 3, 1, 1, params)
        
#         sys.exit()
        
#     (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)

#         layer_2 = {}
#         layer_2['name'] = 'conv-no-activation'
#         layer_2['conv_in_channels'] = 256
#         layer_2['conv_out_channels'] = 512
#         layer_2['conv_kernel_size'] = 3
#         layer_2['conv_stride'] = 1
#         layer_2['conv_padding'] = 1
#         layer_2['activation'] = None
#         layers_.append(layer_2)
        
#         layer_2 = {}
#         layer_2['name'] = 'relu'
#         layers_.append(layer_2)
        
        layers_ = add_conv_block(layers_, 256, 512, 3, 1, 1, params)
        
#         sys.exit()

#         layer_2 = {}
#         layer_2['name'] = 'conv-no-activation'
#         layer_2['conv_in_channels'] = 512
#         layer_2['conv_out_channels'] = 512
#         layer_2['conv_kernel_size'] = 3
#         layer_2['conv_stride'] = 1
#         layer_2['conv_padding'] = 1
#         layer_2['activation'] = None
#         layers_.append(layer_2)
        
#         layer_2 = {}
#         layer_2['name'] = 'relu'
#         layers_.append(layer_2)
        
        layers_ = add_conv_block(layers_, 512, 512, 3, 1, 1, params)
        
#         sys.exit()

#         layer_2 = {}
#         layer_2['name'] = 'conv-no-activation'
#         layer_2['conv_in_channels'] = 512
#         layer_2['conv_out_channels'] = 512
#         layer_2['conv_kernel_size'] = 3
#         layer_2['conv_stride'] = 1
#         layer_2['conv_padding'] = 1
#         layer_2['activation'] = None
#         layers_.append(layer_2)
        
#         layer_2 = {}
#         layer_2['name'] = 'relu'
#         layers_.append(layer_2)
        
        layers_ = add_conv_block(layers_, 512, 512, 3, 1, 1, params)
        
#         sys.exit()
        
#     (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)

#         layer_2 = {}
#         layer_2['name'] = 'conv-no-activation'
#         layer_2['conv_in_channels'] = 512
#         layer_2['conv_out_channels'] = 512
#         layer_2['conv_kernel_size'] = 3
#         layer_2['conv_stride'] = 1
#         layer_2['conv_padding'] = 1
#         layer_2['activation'] = None
#         layers_.append(layer_2)
        
#         layer_2 = {}
#         layer_2['name'] = 'relu'
#         layers_.append(layer_2)
        
        layers_ = add_conv_block(layers_, 512, 512, 3, 1, 1, params)
        
#         sys.exit()
        
#         layer_2 = {}
#         layer_2['name'] = 'conv-no-activation'
#         layer_2['conv_in_channels'] = 512
#         layer_2['conv_out_channels'] = 512
#         layer_2['conv_kernel_size'] = 3
#         layer_2['conv_stride'] = 1
#         layer_2['conv_padding'] = 1
#         layer_2['activation'] = None
#         layers_.append(layer_2)
        
#         layer_2 = {}
#         layer_2['name'] = 'relu'
#         layers_.append(layer_2)
        
        layers_ = add_conv_block(layers_, 512, 512, 3, 1, 1, params)
        
        layers_ = add_conv_block(layers_, 512, 512, 3, 1, 1, params)
        
#         sys.exit()
        
#     (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
#         if params['name_dataset'] in ['CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
#                                       'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
#                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
#                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
#                                       'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
#                                       'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
#                                       'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
#                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
#                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
#                                       'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
#                                       'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine']:
#             1
#         elif params['name_dataset'] == 'CIFAR-10-vgg16':
#             layer_ = {}
#             layer_['name'] = 'AdaptiveAvgPool2d'
#             layer_['AdaptiveAvgPool2d_output_size'] = (7,7)
#             layers_.append(layer_)
#         else:
#             print('error: need to check for ' + params['name_dataset'])
#             sys.exit()
  
    

        
        layer_5 = {}
        layer_5['name'] = 'flatten'
        layers_.append(layer_5)
        

        
        if params['name_dataset'] == 'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization':
            layer_ = {}
            layer_['name'] = 'fully-connected'
            layer_['input_size'] = 512
            layer_['output_size'] = 100
            layer_['activation'] = 'linear'
            layers_.append(layer_)
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization',
                                        'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization',]:
            layer_ = {}
            layer_['name'] = 'fully-connected'
            layer_['input_size'] = 512
            layer_['output_size'] = 10
            layer_['activation'] = 'linear'
            layers_.append(layer_)
        elif params['name_dataset'] in ['CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                        'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',]:
        
        

            layer_ = {}
            layer_['name'] = 'fully-connected'

            layer_['input_size'] = 512
            layer_['output_size'] = 4096
            layer_['activation'] = 'relu'
            layers_.append(layer_)

    #         if params['name_dataset'] in ['CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
    #                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
    #                                       'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
    #                                       'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
    #                                       'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
    #                                       'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
    #                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
    #                                       'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
    #                                       'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
    #                                       'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine']:
    #             1
    #         elif params['name_dataset'] in ['CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool']:
    #             layer_ = {}
    #             layer_['name'] = 'dropout'
    #             layer_['dropout_p'] = 0.5
    #             layers_.append(layer_)

    #         else:
    #             print('error: need to check for ' + params['name_dataset'])
    #             sys.exit()



            layer_ = {}
            layer_['name'] = 'fully-connected'
            layer_['input_size'] = 4096
            layer_['output_size'] = 4096
            layer_['activation'] = 'relu'
            layers_.append(layer_)




            layer_ = {}
            layer_['name'] = 'fully-connected'
            layer_['input_size'] = 4096

            if params['name_dataset'] in ['CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
                                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                          'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                          'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                          'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                                          'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',]:
                layer_['output_size'] = 10
            elif params['name_dataset'] in ['CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                                            'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                            'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                                            'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                            'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                                            'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                                            'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                                            'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                                            'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                                            'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine']:
                layer_['output_size'] = 100
            else:
                print('error: need to check for ' + params['name_dataset'])
                sys.exit()
                layer_['output_size'] = 1000


            layer_['activation'] = 'linear'
            layers_.append(layer_)
            
        else:
            print('params[name_dataset]')
            print(params['name_dataset'])
            sys.exit()
        
    elif name_model in ['vgg11']:
        
        
        
        # referece:
        # https://arxiv.org/pdf/1409.1556.pdf
        # Table 1, model A
        
        
        
        layers_ = []
        
        
        
        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 3
        layer_2['conv_out_channels'] = 64
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        
        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
        # working here
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 64
        layer_2['conv_out_channels'] = 128
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
        # working here
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 128
        layer_2['conv_out_channels'] = 256
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 256
        layer_2['conv_out_channels'] = 256
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
        # working here
        
        
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 256
        layer_2['conv_out_channels'] = 512
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 512
        layer_2['conv_out_channels'] = 512
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        
        # start to not work here
        
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 512
        layer_2['conv_out_channels'] = 512
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_2 = {}
        layer_2['name'] = 'conv'
        layer_2['conv_in_channels'] = 512
        layer_2['conv_out_channels'] = 512
        layer_2['conv_kernel_size'] = 3
        layer_2['conv_stride'] = 1
        layer_2['conv_padding'] = 1
        layer_2['activation'] = 'relu'
        layers_.append(layer_2)
        

        layer_ = {}
        layer_['name'] = 'max_pool'
        layer_['max_pool_kernel_size'] = 2
        layer_['max_pool_stride'] = 2
        layers_.append(layer_)
        

        
#         if params['name_dataset'] == 'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout':
#             1
#         elif params['name_dataset'] == 'CIFAR-10-vgg16':
#             layer_ = {}
#             layer_['name'] = 'AdaptiveAvgPool2d'
#             layer_['AdaptiveAvgPool2d_output_size'] = (7,7)
#             layers_.append(layer_)
#         else:
#             print('error: need to check for ' + params['name_dataset'])
#             sys.exit()
  
    
        
        
        layer_5 = {}
        layer_5['name'] = 'flatten'
        layers_.append(layer_5)
        
#         sys.exit()

#         layer_ = {}
#         layer_['name'] = 'dropout'
#         layer_['dropout_p'] = 0.5
#         layers_.append(layer_)
        

        layer_ = {}
        layer_['name'] = 'fully-connected'
        layer_['input_size'] = 512
#         layer_['input_size'] = 3072
#         layer_['input_size'] = 16384
#         layer_['input_size'] = 8192
#         layer_['input_size'] = 4096
#         layer_['input_size'] = 2048
        layer_['output_size'] = 4096
#         layer_['output_size'] = 512
        layer_['activation'] = 'relu'
        layers_.append(layer_)
        
#         if params['name_dataset'] == 'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout':
#             1
#         elif params['name_dataset'] == 'CIFAR-10-vgg16':
#             layer_ = {}
#             layer_['name'] = 'dropout'
#             layer_['dropout_p'] = 0.5
#             layers_.append(layer_)
            
#         else:
#             print('error: need to check')
#             sys.exit()

#         layer_ = {}
#         layer_['name'] = 'dropout'
#         layer_['dropout_p'] = 0.5
#         layers_.append(layer_)

        layer_ = {}
        layer_['name'] = 'fully-connected'
        layer_['input_size'] = 4096
#         layer_['input_size'] = 512
        layer_['output_size'] = 4096
#         layer_['output_size'] = 512
        layer_['activation'] = 'relu'
        layers_.append(layer_)
        
#         if params['name_dataset'] == 'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout':
#             1
#         elif params['name_dataset'] == 'CIFAR-10-vgg16':
#             layer_ = {}
#             layer_['name'] = 'dropout'
#             layer_['dropout_p'] = 0.5
#             layers_.append(layer_)
#         else:
#             print('error: need to check')
#             sys.exit()
        


        layer_ = {}
        layer_['name'] = 'fully-connected'
        layer_['input_size'] = 4096
#         layer_['input_size'] = 512
        layer_['output_size'] = 10
        layer_['activation'] = 'linear'
        layers_.append(layer_)
        
        
    
        
        
  

    
    else:
        print('Error: unknown model name in get_layers_params for ' + name_model)
        sys.exit()
    return layers_



class Model_3(nn.Module):
    def __init__(self, params):

        name_dataset = params['name_dataset']


        super(Model_3, self).__init__()
    
        self.name_loss = params['name_loss']

        if name_dataset in ['MNIST-autoencoder',
                            'MNIST-autoencoder-no-regularization',
                            'MNIST-autoencoder-N1-1000',
                            'MNIST-autoencoder-N1-1000-no-regularization',
                            'MNIST-autoencoder-N1-1000-sum-loss-no-regularization',
                            'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization',
                            'MNIST-autoencoder-relu-N1-1000-sum-loss',
                            'MNIST-autoencoder-relu-N1-100-sum-loss',
                            'MNIST-autoencoder-relu-N1-500-sum-loss',
                            'MNIST-autoencoder-relu-N1-1-sum-loss',
                            'MNIST-autoencoder-reluAll-N1-1-sum-loss',
                            'MNIST-autoencoder-N1-1000-sum-loss',
                            'CURVES-autoencoder',
                            'CURVES-autoencoder-no-regularization',
                            'CURVES-autoencoder-sum-loss-no-regularization',
                            'CURVES-autoencoder-sum-loss',
                            'CURVES-autoencoder-relu-sum-loss-no-regularization',
                            'CURVES-autoencoder-relu-sum-loss',
                            'CURVES-autoencoder-relu-N1-100-sum-loss',
                            'CURVES-autoencoder-relu-N1-500-sum-loss',
                            'CURVES-autoencoder-Botev',
                            'CURVES-autoencoder-Botev-sum-loss-no-regularization',
                            'CURVES-autoencoder-shallow',
                            'FACES-autoencoder',
                            'FACES-autoencoder-no-regularization',
                            'FACES-autoencoder-sum-loss-no-regularization',
                            'FACES-autoencoder-relu-sum-loss-no-regularization',
                            'FACES-autoencoder-relu-sum-loss',
                            'FACES-autoencoder-sum-loss',
                            'FacesMartens-autoencoder-relu',
                            'FacesMartens-autoencoder-relu-no-regularization',
                            'FacesMartens-autoencoder-relu-N1-500',
                            'FacesMartens-autoencoder-relu-N1-100',
                            'MNIST',
                            'MNIST-no-regularization',
                            'MNIST-N1-1000',
                            'MNIST-one-layer',
                            'DownScaledMNIST-no-regularization',
                            'DownScaledMNIST-N1-1000-no-regularization',
                            'webspam',
                            'CIFAR',
                            'CIFAR-deep',
                            'sythetic-linear-regression',
                            'sythetic-linear-regression-N1-1']:
            
            self.name_model = 'fully-connected'
            
        elif name_dataset in ['Fashion-MNIST',
                              'Fashion-MNIST-N1-60',
                              'Fashion-MNIST-N1-60-no-regularization',
                              'Fashion-MNIST-N1-256-no-regularization',
                              'Fashion-MNIST-GAP-N1-60-no-regularization',
                              'STL-10-simple-CNN',
                              'Subsampled-ImageNet-simple-CNN']:
            # https://arxiv.org/pdf/1910.05446.pdf
            self.name_model = 'simple-CNN'
        elif name_dataset in ['CIFAR-100',
                              'CIFAR-100-NoAugmentation']:
            self.name_model = 'CNN'
        elif name_dataset in ['CIFAR-10-AllCNNC',
                              'CIFAR-10-N1-128-AllCNNC',
                              'CIFAR-10-N1-512-AllCNNC',
                              'CIFAR-100-onTheFly-AllCNNC']:
            self.name_model = 'AllCNNC'
        elif name_dataset == 'CIFAR-10-ConvPoolCNNC':
            self.name_model = 'ConvPoolCNNC'
        elif name_dataset == 'UCI-HAR':
            self.name_model = '1d-CNN'
        elif name_dataset in ['CIFAR-10-vgg16',
                              'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-10-vgg16-GAP',
                              'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                              'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                              'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                              'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                              'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                              'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                              'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                              'CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization',
                              'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization',
                              'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                              'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                              'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                              'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                              'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                              'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                              'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                              'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                              'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                              'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                              'Subsampled-ImageNet-vgg16',]:
            self.name_model = 'vgg16'
        elif name_dataset in ['CIFAR-10-vgg11',
                              'CIFAR-10-NoAugmentation-vgg11']:
            self.name_model = 'vgg11'
        elif name_dataset in ['CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                              'CIFAR-10-onTheFly-ResNet32-BN',
                              'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                              'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                              'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                              'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                              'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                              'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                              'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                              'CIFAR-10-onTheFly-ResNet32-BNNoAffine-NoBias',
                              'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            self.name_model = 'ResNet32'
        elif name_dataset in ['CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                              'CIFAR-100-onTheFly-ResNet34-BN',
                              'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                              'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                              'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                              'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                              'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',]:
            self.name_model = 'ResNet34'
        else:
            print('Error: unkown dataset')
            sys.exit()



        if self.name_model == 'fully-connected':
            if name_dataset in ['MNIST',
                                'MNIST-no-regularization',
                                'MNIST-N1-1000']:
                layersizes = [784, 500, 10]
                self.activations_all = ['sigmoid', 'linear']
            elif name_dataset in ['DownScaledMNIST-no-regularization',
                                  'DownScaledMNIST-N1-1000-no-regularization']:
                # https://arxiv.org/pdf/1503.05671.pdf
                layersizes = [256, 20, 20, 20, 20, 20, 10]
                self.activations_all = ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'linear']
            elif name_dataset == 'MNIST-one-layer':
                layersizes = [784, 10]
#                 self.activations = ['linear']
                self.activations_all = ['linear']
            elif name_dataset in ['sythetic-linear-regression',
                                  'sythetic-linear-regression-N1-1']:
                layersizes = [100, 50] # http://proceedings.mlr.press/v70/zhou17a/zhou17a.pdf

                self.activations_all = ['linear']
            elif name_dataset == 'CIFAR':
                
                layersizes = [3072, 400, 400, 10]
                self.activations = ['sigmoid', 'sigmoid', 'linear']
                
            elif name_dataset == 'CIFAR-deep':
                
                layersizes = [3072, 128, 128, 128, 128, 10]
#                 self.activations = ['relu', 'relu', 'relu', 'relu', 'linear']
                self.activations_all = ['relu', 'relu', 'relu', 'relu', 'linear']
                
            elif name_dataset == 'Fashion-MNIST':
                # self.layersizes = [784, 400, 400, 10]
                layersizes = [784, 400, 400, 10]
                self.activations = ['sigmoid', 'sigmoid', 'linear']
            elif name_dataset == 'webspam':
                
                layersizes = [254, 400, 400, 1]
                self.activations = ['sigmoid', 'sigmoid', 'linear']
            elif name_dataset in ['MNIST-autoencoder',
                                  'MNIST-autoencoder-no-regularization',
                                  'MNIST-autoencoder-N1-1000',
                                  'MNIST-autoencoder-N1-1000-no-regularization',
                                  'MNIST-autoencoder-N1-1000-sum-loss-no-regularization',
                                  'MNIST-autoencoder-N1-1000-sum-loss']:
                # reference: https://arxiv.org/pdf/1301.3641.pdf,
                # https://www.cs.toronto.edu/~hinton/science.pdf
                
                layersizes = [784, 1000, 500, 250, 30, 250, 500, 1000, 784]
                self.activations_all =\
                ['sigmoid', 'sigmoid', 'sigmoid', 'linear', 'sigmoid', 'sigmoid', 'sigmoid', 'linear']
                
            elif name_dataset in ['MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization',
                                  'MNIST-autoencoder-relu-N1-1000-sum-loss',
                                  'MNIST-autoencoder-relu-N1-100-sum-loss',
                                  'MNIST-autoencoder-relu-N1-500-sum-loss',
                                  'MNIST-autoencoder-relu-N1-1-sum-loss']:
                
                layersizes = [784, 1000, 500, 250, 30, 250, 500, 1000, 784]
                self.activations_all =\
                ['relu', 'relu', 'relu', 'linear', 'relu', 'relu', 'relu', 'linear']
                
            elif name_dataset in ['MNIST-autoencoder-reluAll-N1-1-sum-loss']:
                
                layersizes = [784, 1000, 500, 250, 30, 250, 500, 1000, 784]
                self.activations_all =\
                ['relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'linear']
            
            elif name_dataset in ['CURVES-autoencoder',
                                  'CURVES-autoencoder-no-regularization',
                                  'CURVES-autoencoder-sum-loss-no-regularization',
                                  'CURVES-autoencoder-sum-loss']:
                # https://www.cs.toronto.edu/~hinton/science.pdf
                # https://arxiv.org/pdf/1301.3641.pdf
                
                layersizes =\
                [784, 400, 200, 100, 50, 25, 6, 25, 50, 100, 200, 400, 784]
                self.activations_all =\
                ['sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid',
                 'linear',
                 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'linear']
                
            elif name_dataset in ['CURVES-autoencoder-relu-sum-loss-no-regularization',
                                  'CURVES-autoencoder-relu-sum-loss',
                                  'CURVES-autoencoder-relu-N1-100-sum-loss',
                                  'CURVES-autoencoder-relu-N1-500-sum-loss']:
                
                layersizes =\
                [784, 400, 200, 100, 50, 25, 6, 25, 50, 100, 200, 400, 784]
                self.activations_all =\
                ['relu', 'relu', 'relu', 'relu', 'relu',
                 'linear',
                 'relu', 'relu', 'relu', 'relu', 'relu', 'linear']
                
            elif name_dataset in ['CURVES-autoencoder-Botev',
                                  'CURVES-autoencoder-Botev-sum-loss-no-regularization']:
                # https://arxiv.org/pdf/1706.03662.pdf
                layersizes = [784, 1000, 500, 250, 30, 250, 500, 1000, 784]
                self.activations_all =\
                ['sigmoid', 'sigmoid', 'sigmoid', 'linear', 'sigmoid', 'sigmoid', 'sigmoid', 'linear']
                
                
            elif name_dataset == 'CURVES-autoencoder-shallow':
                # https://www.cs.toronto.edu/~hinton/science.pdf
                
                layersizes = [784, 532, 6, 532, 784]
                self.activations =\
                ['sigmoid', 'linear', 'sigmoid', 'linear']
                
            elif name_dataset in ['FACES-autoencoder',
                                  'FACES-autoencoder-no-regularization',
                                  'FACES-autoencoder-sum-loss-no-regularization',
                                  'FACES-autoencoder-sum-loss']:
                # https://www.cs.toronto.edu/~hinton/science.pdf
                layersizes = [625, 2000, 1000, 500, 30,
                             500, 1000, 2000, 625]
                self.activations_all =\
                ['sigmoid', 'sigmoid', 'sigmoid', 'linear',
                 'sigmoid', 'sigmoid', 'sigmoid', 'linear']
                
            elif name_dataset in ['FACES-autoencoder-relu-sum-loss-no-regularization',
                                  'FACES-autoencoder-relu-sum-loss',
                                  'FacesMartens-autoencoder-relu',
                                  'FacesMartens-autoencoder-relu-no-regularization',
                                  'FacesMartens-autoencoder-relu-N1-500',
                                  'FacesMartens-autoencoder-relu-N1-100']:
                layersizes = [625, 2000, 1000, 500, 30,
                             500, 1000, 2000, 625]
                self.activations_all =\
                ['relu', 'relu', 'relu', 'linear',
                 'relu', 'relu', 'relu', 'linear']
                
                
                
            else:
                print('Dateset not supported!')
                sys.exit()
            
        elif self.name_model == 'simple-CNN':
            layersizes = []
#             self.activations_all = ['relu', '', 'relu', '', '', 'relu', 'linear']
            self.activations_all = []
        elif self.name_model == 'CNN':
            layersizes = []
#             self.activations = ['relu', 'relu', 'relu', 'linear']
#             self.activations_all = ['relu', '',  'relu', '',  'relu', '', '', 'linear']
            self.activations_all = []
        elif self.name_model == 'AllCNNC':
            layersizes = []
            self.activations_all = []
        elif self.name_model == 'ConvPoolCNNC':
            layersizes = []
            self.activations_all = []
        elif self.name_model == '1d-CNN':
            layersizes = []
#             self.activations_all = ['relu', 'relu', '', '', 'relu', 'linear']
            self.activations_all = ['relu', 'relu', 'relu', 'linear']
        elif self.name_model == 'vgg16':
            layersizes = []
            self.activations_all = []
        elif self.name_model == 'vgg11':
            layersizes = []
            self.activations_all = []
        elif self.name_model == 'ResNet32':
            layersizes = []
            self.activations_all = []
        elif self.name_model == 'ResNet34':
            layersizes = []
            self.activations_all = []
        else:
            print('Error: model name not supported for ' + self.name_model)
            sys.exit()
            
        self.layersizes = layersizes

        layers_params = get_layers_params(self.name_model, layersizes, self.activations_all, params)
        
        # self.layers: with weights
        # self.layers_all
        # self.layers_no_weights: to be deprecated
        

#         self.numlayers = len(layers_params)

        
#         self.layers = list(range(self.numlayers))
        self.layers_all = []
        
#         self.layers_no_weights = list(range(self.numlayers))
#         self.layers_no_weights = []

#         for l in range(self.numlayers):
        for l in range(len(layers_params)):
            
            if layers_params[l]['name'] == 'fully-connected':
#                 self.layers[l] =\
                self.layers_all.append(
                nn.Linear(layers_params[l]['input_size'], layers_params[l]['output_size'], bias=True)
                )
    
#                 self.layers_no_weights.append([])
                
            elif layers_params[l]['name'] in ['conv-no-bias-no-activation']:

                self.layers_all.append(
                    nn.Conv2d(
                        in_channels=layers_params[l]['conv_in_channels'],
                        out_channels=layers_params[l]['conv_out_channels'],
                        kernel_size=layers_params[l]['conv_kernel_size'],
                        stride=layers_params[l]['conv_stride'],
                        padding=layers_params[l]['conv_padding'],
                        bias=False
                    )
                )
                
            elif layers_params[l]['name'] in ['conv',
                                              'conv-no-activation']:

                self.layers_all.append(
                nn.Conv2d(
                    in_channels=layers_params[l]['conv_in_channels'],
                    out_channels=layers_params[l]['conv_out_channels'],
                    kernel_size=layers_params[l]['conv_kernel_size'],
                    stride=layers_params[l]['conv_stride'],
                    padding=layers_params[l]['conv_padding'])
                )
                
            elif layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                              'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                              'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                              'ResBlock-BN',
                                              'ResBlock-BN-BNshortcut',
                                              'ResBlock-BN-identityShortcut',
                                              'ResBlock-BN-identityShortcut-NoBias',
                                              'ResBlock-BN-BNshortcut-NoBias',
                                              'ResBlock-BN-PaddingShortcut-NoBias',]:
                
                self.layers_all.append([])
                
                # conv
                self.layers_all[-1].append(
                    nn.Conv2d(
                        in_channels=layers_params[l]['conv1']['conv_in_channels'],
                        out_channels=layers_params[l]['conv1']['conv_out_channels'],
                        kernel_size=layers_params[l]['conv1']['conv_kernel_size'],
                        stride=layers_params[l]['conv1']['conv_stride'],
                        padding=layers_params[l]['conv1']['conv_padding'],
                        bias=layers_params[l]['conv1']['conv_bias']
                    )
                )
                
                # BN or BNNoAffine
                if layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                                'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                'ResBlock-BNNoAffine-PaddingShortcut-NoBias',]:
                    self.layers_all[-1].append(
                        nn.BatchNorm2d(layers_params[l]['BNNoAffine1']['num_features'], affine=False).to(params['device'])
                    )
                elif layers_params[l]['name'] in ['ResBlock-BN',
                                                  'ResBlock-BN-BNshortcut',
                                                  'ResBlock-BN-identityShortcut',
                                                  'ResBlock-BN-identityShortcut-NoBias',
                                                  'ResBlock-BN-BNshortcut-NoBias',
                                                  'ResBlock-BN-PaddingShortcut-NoBias']:
                    self.layers_all[-1].append(
                        nn.BatchNorm2d(layers_params[l]['BN1']['num_features'], affine=True).to(params['device'])
                    )
                else:
                    print('layers_params[l][name]')
                    print(layers_params[l]['name'])
                    sys.exit()
                
                # conv
                self.layers_all[-1].append(
                    nn.Conv2d(
                        in_channels=layers_params[l]['conv2']['conv_in_channels'],
                        out_channels=layers_params[l]['conv2']['conv_out_channels'],
                        kernel_size=layers_params[l]['conv2']['conv_kernel_size'],
                        stride=layers_params[l]['conv2']['conv_stride'],
                        padding=layers_params[l]['conv2']['conv_padding'],
                        bias=layers_params[l]['conv2']['conv_bias']
                    )
                )
                
                # BN or BNNoAffine
                
                
                if layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                                'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                'ResBlock-BNNoAffine-PaddingShortcut-NoBias',]:
                    self.layers_all[-1].append(
                        nn.BatchNorm2d(layers_params[l]['BNNoAffine2']['num_features'], affine=False).to(params['device'])
                    )
                elif layers_params[l]['name'] in ['ResBlock-BN',
                                                  'ResBlock-BN-BNshortcut',
                                                  'ResBlock-BN-identityShortcut',
                                                  'ResBlock-BN-identityShortcut-NoBias',
                                                  'ResBlock-BN-BNshortcut-NoBias',
                                                  'ResBlock-BN-PaddingShortcut-NoBias']:
                    self.layers_all[-1].append(
                        nn.BatchNorm2d(layers_params[l]['BN2']['num_features'], affine=True).to(params['device'])
                    )
                else:
                    print('layers_params[l][name]')
                    print(layers_params[l]['name'])
                    sys.exit()
                
                if layers_params[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                'ResBlock-BN-identityShortcut',
                                                'ResBlock-BN-identityShortcut-NoBias',
                                                'ResBlock-BN-PaddingShortcut-NoBias',]:
                    1
                elif layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                                  'ResBlock-BN',
                                                  'ResBlock-BN-BNshortcut',
                                                  'ResBlock-BN-BNshortcut-NoBias']:
                    # 1*1 conv
                    self.layers_all[-1].append(
                        nn.Conv2d(
                            in_channels=layers_params[l]['conv3']['conv_in_channels'],
                            out_channels=layers_params[l]['conv3']['conv_out_channels'],
                            kernel_size=layers_params[l]['conv3']['conv_kernel_size'],
                            stride=layers_params[l]['conv3']['conv_stride'],
                            padding=layers_params[l]['conv3']['conv_padding'],
                            bias=layers_params[l]['conv3']['conv_bias']
                        )
                    )

                    if layers_params[l]['name'] in ['ResBlock-BN-BNshortcut',
                                                    'ResBlock-BN-BNshortcut-NoBias']:
                        self.layers_all[-1].append(
                            nn.BatchNorm2d(layers_params[l]['BN3']['num_features'], affine=True).to(params['device'])
                        )
                else:
                    print('layers_params[l][name]')
                    print(layers_params[l]['name'])
                    sys.exit()
                
                    

                    
            elif layers_params[l]['name'] == '1d-conv':
#                 self.layers[l] =\
                self.layers_all.append(
                nn.Conv1d(
                    in_channels=layers_params[l]['conv_in_channels'],
                    out_channels=layers_params[l]['conv_out_channels'],
                    kernel_size=layers_params[l]['conv_kernel_size'],
                    stride=layers_params[l]['conv_stride'],
                    padding=layers_params[l]['conv_padding'])
                )
#                 if layers_params[l]['if_max_pool']:
#                     self.layers_no_weights.append(
#                     nn.MaxPool1d(
#                         kernel_size=layers_params[l]['max_pool_kernel_size'],
#                         stride=layers_params[l]['max_pool_stride'])
#                     )
#                 else:
#                 self.layers_no_weights.append([])
                
            elif layers_params[l]['name'] == 'flatten':
                self.layers_all.append(nn.Flatten())
#                 self.layers_no_weights.append(
#                 []
#                 )
                
            elif layers_params[l]['name'] == 'max_pool_1d':
                self.layers_all.append(
                    nn.MaxPool1d(
                        kernel_size=layers_params[l]['max_pool_kernel_size'],
                        stride=layers_params[l]['max_pool_stride'])
                )
#                 self.layers_no_weights.append(
#                 []
#                 )
                
            elif layers_params[l]['name'] == 'max_pool':
                self.layers_all.append(
                    nn.MaxPool2d(
                        kernel_size=layers_params[l]['max_pool_kernel_size'],
                        stride=layers_params[l]['max_pool_stride'])
                )

            elif layers_params[l]['name'] == 'AdaptiveAvgPool2d':
                self.layers_all.append(
                    nn.AdaptiveAvgPool2d(
                        output_size=layers_params[l]['AdaptiveAvgPool2d_output_size']
                    )
                )
            
            elif layers_params[l]['name'] == 'dropout':
                self.layers_all.append(
                    nn.Dropout(
                        p=layers_params[l]['dropout_p'], 
                        inplace=False
                    )
                    )
                
            elif layers_params[l]['name'] == 'global_average_pooling':
                self.layers_all.append([])
                
            elif layers_params[l]['name'] == 'relu':
                self.layers_all.append([])
                
            elif layers_params[l]['name'] == 'BN':
                self.layers_all.append(
                    nn.BatchNorm2d(layers_params[l]['num_features'])
                )
                
            elif layers_params[l]['name'] == 'BNNoAffine':
                
                self.layers_all.append(
                    nn.BatchNorm2d(layers_params[l]['num_features'], affine=False).to(params['device'])
                )
                
#                 sys.exit()
                
                    
            else:
                print('Error: layer unsupported for ' + layers_params[l]['name'])
                sys.exit()
                
                
#         print('self.layers_all')
#         print(self.layers_all)
        
#         sys.exit()

        

        self.layers_weight = []
#         for l in range(self.numlayers):
        for l in range(len(layers_params)):
            
            if layers_params[l]['name'] in ['fully-connected',
                                            'conv',
                                            'conv-no-activation',
                                            '1d-conv']:
                layers_weight_l = {}
                layers_weight_l['W'] = self.layers_all[l].weight
                layers_weight_l['b'] = self.layers_all[l].bias
                self.layers_weight.append(layers_weight_l)
                
            elif layers_params[l]['name'] in ['conv-no-bias-no-activation']:
                layers_weight_l = {}
                layers_weight_l['W'] = self.layers_all[l].weight
                
#                 print('self.layers_all[l].bias')
#                 print(self.layers_all[l].bias)
                
#                 sys.exit()
                
#                 layers_weight_l['b'] = self.layers_all[l].bias
                self.layers_weight.append(layers_weight_l)
                
            elif layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                              'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                              'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                              'ResBlock-BN',
                                              'ResBlock-BN-BNshortcut',
                                              'ResBlock-BN-identityShortcut',
                                              'ResBlock-BN-identityShortcut-NoBias',
                                              'ResBlock-BN-BNshortcut-NoBias',
                                              'ResBlock-BN-PaddingShortcut-NoBias',]:
                
                layers_weight_l = {}
                layers_weight_l['W'] = self.layers_all[l][0].weight
                
                if layers_params[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                'ResBlock-BN-identityShortcut-NoBias',
                                                'ResBlock-BN-BNshortcut-NoBias',
                                                'ResBlock-BN-PaddingShortcut-NoBias']:
                    pass
                elif layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                                  'ResBlock-BN',
                                                  'ResBlock-BN-identityShortcut',
                                                  'ResBlock-BN-BNshortcut']:
                    layers_weight_l['b'] = self.layers_all[l][0].bias
                else:
                    print('layers_params[l][name]')
                    print(layers_params[l]['name'])
                    sys.exit()
                
                    
                    
                self.layers_weight.append(layers_weight_l)
                
                if layers_params[l]['name'] in ['ResBlock-BN',
                                                'ResBlock-BN-BNshortcut',
                                                'ResBlock-BN-identityShortcut',
                                                'ResBlock-BN-identityShortcut-NoBias',
                                                'ResBlock-BN-BNshortcut-NoBias',
                                                'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_weight_l = {}
                    layers_weight_l['W'] = self.layers_all[l][1].weight
                    layers_weight_l['b'] = self.layers_all[l][1].bias
                    self.layers_weight.append(layers_weight_l)
                
                layers_weight_l = {}
                layers_weight_l['W'] = self.layers_all[l][2].weight
                
                if layers_params[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                'ResBlock-BN-identityShortcut-NoBias',
                                                'ResBlock-BN-BNshortcut-NoBias',
                                                'ResBlock-BN-PaddingShortcut-NoBias']:
                    pass
                elif layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                                  'ResBlock-BN',
                                                  'ResBlock-BN-identityShortcut',
                                                  'ResBlock-BN-BNshortcut']:
                    layers_weight_l['b'] = self.layers_all[l][2].bias
                else:
                    print('layers_params[l][name]')
                    print(layers_params[l]['name'])
                    sys.exit()
                    
                self.layers_weight.append(layers_weight_l)
                
                if layers_params[l]['name'] in ['ResBlock-BN',
                                                'ResBlock-BN-BNshortcut',
                                                'ResBlock-BN-identityShortcut',
                                                'ResBlock-BN-identityShortcut-NoBias',
                                                'ResBlock-BN-BNshortcut-NoBias',
                                                'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_weight_l = {}
                    layers_weight_l['W'] = self.layers_all[l][3].weight
                    layers_weight_l['b'] = self.layers_all[l][3].bias
                    self.layers_weight.append(layers_weight_l)
                    
                    
                if layers_params[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                'ResBlock-BN-identityShortcut',
                                                'ResBlock-BN-identityShortcut-NoBias',
                                                'ResBlock-BN-PaddingShortcut-NoBias']:
                    1
                else:
                
                    layers_weight_l = {}
                    layers_weight_l['W'] = self.layers_all[l][4].weight
                    if layers_params[l]['name'] == 'ResBlock-BN-BNshortcut-NoBias':
                        pass
                    elif layers_params[l]['name'] in ['ResBlock-BNNoAffine',
                                                      'ResBlock-BN',
                                                      'ResBlock-BN-BNshortcut']:
                        layers_weight_l['b'] = self.layers_all[l][4].bias
                    else:
                        print('layers_params[l][name]')
                        print(layers_params[l]['name'])
                        sys.exit()
                        
                    self.layers_weight.append(layers_weight_l)


                    if layers_params[l]['name'] in ['ResBlock-BN-BNshortcut',
                                                    'ResBlock-BN-BNshortcut-NoBias']:
                        layers_weight_l = {}
                        layers_weight_l['W'] = self.layers_all[l][5].weight
                        
#                         if layers_params[l]['name'] == 'ResBlock-BN-BNshortcut-NoBias':
#                             pass
#                         else:
#                             sys.exit()
                        layers_weight_l['b'] = self.layers_all[l][5].bias
                        self.layers_weight.append(layers_weight_l)
                
            elif layers_params[l]['name'] == 'BN':
#                 print('self.layers_all[l]')
#                 print(self.layers_all[l])
                
#                 print('self.layers_all[l].weight.size()')
#                 print(self.layers_all[l].weight.size())
                
#                 print('self.layers_all[l].bias.size()')
#                 print(self.layers_all[l].bias.size())
#                 sys.exit()
                
                layers_weight_l = {}
                layers_weight_l['W'] = self.layers_all[l].weight
                layers_weight_l['b'] = self.layers_all[l].bias
                self.layers_weight.append(layers_weight_l)
            elif layers_params[l]['name'] in ['flatten',
                                              'max_pool',
                                              'max_pool_1d',
                                              'AdaptiveAvgPool2d',
                                              'dropout',
                                              'global_average_pooling',
                                              'relu',
                                              'BNNoAffine']:
                1
            else:
                print('layers_params[l][name]')
                print(layers_params[l]['name'])
                print('Error: layer unsupported when define weight for ' + layers_params[l]['name'])
                sys.exit()
            




        
        

                
        
        # filter out the layers with no weights
        self.layers_params_all = layers_params
        layers_params = []
        self.layers = []
        for l in range(len(self.layers_params_all)):
            if self.layers_params_all[l]['name'] in ['fully-connected',
                                                     'conv',
                                                     'conv-no-activation',
                                                     'conv-no-bias-no-activation',
                                                     '1d-conv',
                                                     'BN']:
                
#                 print('self.layers_params_all[l]')
#                 print(self.layers_params_all[l])
                
#                 sys.exit()
                
                layers_params.append(self.layers_params_all[l])
                self.layers.append(self.layers_all[l])
                
            elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine',
                                                       'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                       'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                       'ResBlock-BN',
                                                       'ResBlock-BN-BNshortcut',
                                                       'ResBlock-BN-identityShortcut',
                                                       'ResBlock-BN-identityShortcut-NoBias',
                                                       'ResBlock-BN-BNshortcut-NoBias',
                                                       'ResBlock-BN-PaddingShortcut-NoBias',]:
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                         'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-BNshortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_params.append(
                        {**{'name': 'conv-no-bias-no-activation'}, **self.layers_params_all[l]['conv1']}
                    )
                elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine',
                                                           'ResBlock-BN',
                                                           'ResBlock-BN-identityShortcut',
                                                           'ResBlock-BN-BNshortcut']:
                    
                    layers_params.append(
                        {**{'name': 'conv-no-activation'}, **self.layers_params_all[l]['conv1']}
                    )
                else:
                    print('self.layers_params_all[l][name]')
                    print(self.layers_params_all[l]['name'])
                    sys.exit()
            
    
                    
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BN',
                                                         'ResBlock-BN-BNshortcut',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-BNshortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_params.append(
                        {**{'name': 'BN'}, **self.layers_params_all[l]['BN1']}
                    )
                

                if self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                         'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-BNshortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_params.append(
                        {**{'name': 'conv-no-bias-no-activation'}, **self.layers_params_all[l]['conv2']}
                    )
                elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine',
                                                           'ResBlock-BN',
                                                           'ResBlock-BN-identityShortcut',
                                                           'ResBlock-BN-BNshortcut',
                                                           'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_params.append(
                        {**{'name': 'conv-no-activation'}, **self.layers_params_all[l]['conv2']}
                    )
                else:
                    print('self.layers_params_all[l][name]')
                    print(self.layers_params_all[l]['name'])
                    sys.exit()
                    
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BN',
                                                         'ResBlock-BN-BNshortcut',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-BNshortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    layers_params.append(
                        {**{'name': 'BN'}, **self.layers_params_all[l]['BN2']}
                    )
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                         'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    1
                else:
                    if self.layers_params_all[l]['name'] == 'ResBlock-BN-BNshortcut-NoBias':
                        layers_params.append(
                            {**{'name': 'conv-no-bias-no-activation'}, **self.layers_params_all[l]['conv3']}
                        )
                    elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine',
                                                               'ResBlock-BN',
                                                               'ResBlock-BN-BNshortcut']:
                        layers_params.append(
                            {**{'name': 'conv-no-activation'}, **self.layers_params_all[l]['conv3']}
                        )
                    else:
                        print('self.layers_params_all[l][name]')
                        print(self.layers_params_all[l]['name'])
                        sys.exit()
                        

                    if self.layers_params_all[l]['name'] in ['ResBlock-BN-BNshortcut',
                                                             'ResBlock-BN-BNshortcut-NoBias']:
                        layers_params.append(
                            {**{'name': 'BN'}, **self.layers_params_all[l]['BN3']}
                        )

                self.layers.append(self.layers_all[l][0])
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BN',
                                                         'ResBlock-BN-BNshortcut',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-BNshortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    self.layers.append(self.layers_all[l][1])
                
                self.layers.append(self.layers_all[l][2])
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BN',
                                                         'ResBlock-BN-BNshortcut',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-BNshortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    self.layers.append(self.layers_all[l][3])
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                         'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias',
                                                         'ResBlock-BN-PaddingShortcut-NoBias']:
                    1
                else:
                    self.layers.append(self.layers_all[l][4])

                    if self.layers_params_all[l]['name'] in ['ResBlock-BN-BNshortcut',
                                                             'ResBlock-BN-BNshortcut-NoBias']:
                        self.layers.append(self.layers_all[l][5])
                
            elif self.layers_params_all[l]['name'] in ['flatten',
                                                       'max_pool',
                                                       'max_pool_1d',
                                                       'AdaptiveAvgPool2d',
                                                       'dropout',
                                                       'global_average_pooling',
                                                       'relu',
                                                       'BNNoAffine']:
                1
            else:
                print('error: unkown layers_params_all[l][name]: ' + self.layers_params_all[l]['name'])
                sys.exit()
                

                
        
        import torch.nn.init as init
        
        for l in range(len(layers_params)):
            
            if layers_params[l]['name'] == 'fully-connected':
                
                if params['initialization_pkg'] == 'numpy':
                
                    np_W_l =\
                    (np.random.uniform(size=self.layers_weight[l]['W'].size()) * 2 - 1) *\
                    np.sqrt(2 / (layers_params[l]['input_size'] + layers_params[l]['output_size']))

                    np_W_l = torch.from_numpy(np_W_l)

                    self.layers_weight[l]['W'].data = np_W_l.type(torch.FloatTensor)
                
                elif params['initialization_pkg'] == 'torch':
                    
                    self.layers_weight[l]['W'].data =\
                    (
                        torch.distributions.uniform.Uniform(low=0, high=1).sample(
                            sample_shape=self.layers_weight[l]['W'].size()
                        )
                        * 2 - 1
                    ) *\
                    np.sqrt(
                        2 / (layers_params[l]['input_size'] + layers_params[l]['output_size'])
                    )
                    
                elif params['initialization_pkg'] in ['default',
                                                      'normal']:
                    # U(-sqrt(k), sqrt(k)), where k = 1 / in_featurest
                    
                    pass
                    
                elif params['initialization_pkg'] == 'kaiming_normal':
                    
#                     print('torch.norm(self.layers_weight[l][W])')
#                     print(torch.norm(self.layers_weight[l]['W']))
                    
                    init.kaiming_normal_(self.layers_weight[l]['W'])
                    
#                     print('torch.norm(self.layers_weight[l][W])')
#                     print(torch.norm(self.layers_weight[l]['W']))
                    
#                     sys.exit()
                    
                else:
                    print('error: unknown params[initialization_pkg] for ' + params['initialization_pkg'])
                    sys.exit()
                

            elif layers_params[l]['name'] in ['conv',
                                              'conv-no-activation',
                                              'conv-no-bias-no-activation']:
                
                if params['initialization_pkg'] == 'normal':
            
                    # https://github.com/chengyangfu/pytorch-vgg-cifar10
                    
                    if layers_params[l]['name'] == 'conv-no-bias-no-activation':
                        pass
                    elif layers_params[l]['name'] in ['conv-no-activation',
                                                      'conv']:
                        self.layers_weight[l]['b'].data.zero_()
                    else:
                        print('layers_params[l][name]')
                        print(layers_params[l]['name'])
                        sys.exit()

                    self.layers_weight[l]['W'].data.normal_(
                        0, 
                        math.sqrt(2. / (layers_params[l]['conv_kernel_size']**2 * layers_params[l]['conv_out_channels']))
                    )
                elif params['initialization_pkg'] == 'default':
                    # use default initialization
                    1
                elif params['initialization_pkg'] == 'kaiming_normal':
                    
                    # the default arguments are equivalent to relu
                    
                    init.kaiming_normal_(self.layers_weight[l]['W'])
                    
                else:
                    print('error: need to check for ' + params['initialization_pkg'])
                    sys.exit()
                    
                    
            elif layers_params[l]['name'] in ['ResBlock-BNNoAffine']:
                
                print('should not reach here')
                
                sys.exit()
                
                print('need to check how to initialize')
                    
                    
                
            elif layers_params[l]['name'] in ['1d-conv']:
                
                sys.exit()
                
                # use default initialization
                1

            elif layers_params[l]['name'] in ['BN']:
                1

            else:
                print('layers_params[l][name]')
                print(layers_params[l]['name'])
                print('Error: layer unsupported when initialization.')
                sys.exit()
                
                
#         sys.exit()
        
        
        self.layers_params = layers_params
        
        self.numlayers = len(layers_params)
        self.numlayers_all = len(self.layers_params_all)
        
        self.layers = nn.ModuleList(self.layers)

        
            

    
    def forward(self, x):
        
        
#         a = list(range(self.numlayers))
        a = []
        
#         h = list(range(self.numlayers))
        h = []

        input_ = x


        for l in range(self.numlayers_all):
            
#             print('input_.size()')
#             print(input_.size())
            
#             print('self.layers_params_all[l][name]')
#             print(self.layers_params_all[l]['name'])
            
            if self.layers_params_all[l]['name'] in ['fully-connected',
                                                     'conv',
                                                     'conv-no-activation',
                                                     'conv-no-bias-no-activation',
                                                     '1d-conv',
                                                     'BN']:

#                 input_, a_l =\
#                 get_layer_error: need to accomadate GAP(
#                     input_, self.layers_all[l], self.activations_all[l], self.layers_params_all[l])
            
                if self.layers_params_all[l]['name'] in ['conv-no-activation',
                                                         'conv',
                                                         'conv-no-bias-no-activation',
                                                         'fully-connected']:

                    h.append(input_)
                
                elif self.layers_params_all[l]['name'] == 'BN':
                    
                    h.append([])
                    
#                     print('input_.size()')
#                     print(input_.size())
                    
#                     print('self.layers_all[l](input_).size()')
#                     print(self.layers_all[l](input_).size())
                    
#                     test_output_affine_true = self.layers_all[l](input_)
                    
#                     print('torch.norm(self.layers_all[l](input_))')
#                     print(torch.norm(self.layers_all[l](input_)))
                    
                    
                    
#                     print('self.layers_all[l].affine')
#                     print(self.layers_all[l].affine)

#                     print('self.layers_all[l].weight.size()')
#                     print(self.layers_all[l].weight.size())
                    
#                     print('torch.norm(self.layers_all[l](input_))')
#                     print(torch.norm(self.layers_all[l](input_)))
                    
#                     print('torch.norm(self.layers_all[l](input_) - test_output_affine_true)')
#                     print(torch.norm(self.layers_all[l](input_) - test_output_affine_true))
                    
#                     print('self.layers_all[l].weight[0].item()')
#                     print(self.layers_all[l].weight[0].item())
                    
#                     if np.abs(self.layers_all[l].weight[0].item() - 1.0) > 1e-3:
                        
#                         print('np.abs(self.layers_all[l].weight[0].item() - 1.0)')
#                         print(np.abs(self.layers_all[l].weight[0].item() - 1.0))
                    
#                         sys.exit()
                    
                else:
                    print('error: need to check for ' + self.layers_params_all[l]['name'])
                    sys.exit()
                
                
                input_, a_l =\
                get_layer_forward(
                    input_, self.layers_all[l], self.layers_params_all[l]['activation'], self.layers_params_all[l])

                a.append(a_l)
                
                
            elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine',
                                                       'ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                       'ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                       'ResBlock-BN',
                                                       'ResBlock-BN-BNshortcut',
                                                       'ResBlock-BN-identityShortcut',
                                                       'ResBlock-BN-identityShortcut-NoBias',
                                                       'ResBlock-BN-BNshortcut-NoBias',
                                                       'ResBlock-BN-PaddingShortcut-NoBias',]:
                
                h.append([])
                h.append([])
                
                a.append([])
                a.append([])
                
                if self.layers_params_all[l]['name'] == 'ResBlock-BN':
                    h.append([])
                    h.append([])

                    a.append([])
                    a.append([])
                    
                    h.append([])

                    a.append([])
                    
                    index_mapping = [-1, -5, -4, -3, -2]
                elif self.layers_params_all[l]['name'] in ['ResBlock-BN-BNshortcut',
                                                           'ResBlock-BN-BNshortcut-NoBias']:
                    h.append([])
                    h.append([])
                    h.append([])

                    a.append([])
                    a.append([])
                    a.append([])
                    
                    h.append([])

                    a.append([])
                    
                    index_mapping = [-2, -6, -5, -4, -3]
                elif self.layers_params_all[l]['name'] in ['ResBlock-BN-identityShortcut',
                                                           'ResBlock-BN-identityShortcut-NoBias',
                                                           'ResBlock-BN-PaddingShortcut-NoBias']:
                    h.append([])

                    a.append([])
                    
                    h.append([])

                    a.append([])
                    
                    index_mapping = [None, -4, -3, -2, -1]
                elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                           'ResBlock-BNNoAffine-PaddingShortcut-NoBias',]:
                
                    index_mapping = [None, -2, None, -1, None]
                    
                elif self.layers_params_all[l]['name'] == 'ResBlock-BNNoAffine':
                    index_mapping = [-1, -3, None, -2, None]
                    
                
                if self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-identityShortcut-NoBias',
                                                         'ResBlock-BN-identityShortcut',
                                                         'ResBlock-BN-identityShortcut-NoBias']:
                    input_shortcut = input_
                elif self.layers_params_all[l]['name'] in ['ResBlock-BNNoAffine-PaddingShortcut-NoBias',
                                                           'ResBlock-BN-PaddingShortcut-NoBias',]:

                    input_shortcut = F.pad(input_[:, :, ::2, ::2], (0,0,0,0,input_.size(1)//2,input_.size(1)//2))
                    
                else:

                    h[index_mapping[0]] = input_
                    input_shortcut, a_l = get_layer_forward(
                        input_, self.layers_all[l][4], None, {'name':'conv-no-activation'})
                    a[index_mapping[0]] = a_l

                    if self.layers_params_all[l]['name'] in ['ResBlock-BN-BNshortcut',
                                                             'ResBlock-BN-BNshortcut-NoBias']:
                        input_shortcut, a_l =\
                        get_layer_forward(
                            input_shortcut, self.layers_all[l][5], None, {'name':'BN'})
                        a[-1] = a_l
                
                
                # conv
                h[index_mapping[1]] = input_
                input_, a_l =\
                get_layer_forward(
                    input_, self.layers_all[l][0], None, {'name':'conv-no-activation'})
                # no-bias is not needed here
                a[index_mapping[1]] = a_l
                
                # BN or BNNoAffine
                if index_mapping[2] == None:
                    input_ = self.layers_all[l][1](input_)
                else:
                    input_, a_l =\
                    get_layer_forward(
                        input_, self.layers_all[l][1], None, {'name':'BN'})
                    a[index_mapping[2]] = a_l
                
                # relu
                input_ = F.relu(input_)
                
                # conv
                h[index_mapping[3]] = input_
                input_, a_l =\
                get_layer_forward(
                    input_, self.layers_all[l][2], None, {'name':'conv-no-activation'})
                a[index_mapping[3]] = a_l
                
                # BN or BNNoAffine
                if index_mapping[4] == None:
                    input_ = self.layers_all[l][3](input_)
                else:
                    input_, a_l =\
                    get_layer_forward(
                        input_, self.layers_all[l][3], None, {'name':'BN'})
                    a[index_mapping[4]] = a_l
                
#                 if self.layers_params_all[l]['name'] in ['ResBlock-BN-PaddingShortcut-NoBias']:
#                     sys.exit()
                
                # add
                input_ = input_ + input_shortcut
                
                # relu
                input_ = F.relu(input_)
                
            elif self.layers_params_all[l]['name'] in ['flatten',
                                                       'max_pool',
                                                       'max_pool_1d',
                                                       'AdaptiveAvgPool2d',
                                                       'dropout',
                                                       'BNNoAffine']:

                

                input_ = self.layers_all[l](input_)
                
            elif self.layers_params_all[l]['name'] == 'global_average_pooling':
                
#                 print('input_.size()')
#                 print(input_.size())
                
                input_ = torch.mean(input_, dim=(2,3))
                
#                 print('input_.size()')
#                 print(input_.size())
                
#                 sys.exit()
    
            elif self.layers_params_all[l]['name'] == 'relu':
                input_ = F.relu(input_)
            else:
                print('error: unknown self.layers_params_all[l][name]: ' + self.layers_params_all[l]['name'])
                sys.exit()
                
#             print('input_.size() in forward')
#             print(input_.size())
                
            
            
            
            # max pooling

#             if not self.layers_params_all[l]['name'] == 'fully-connected':
#                 if 'if_max_pool' in self.layers_params_all[l] and\
#                 self.layers_params_all[l]['if_max_pool']:
#                     input_ = self.layers_no_weights[l](input_)
                    



        # a[l]: pre-activation of the current layer (the one that requires grad)
        # h[l]: input of the current layer (post-activation of the previous layer in the fully-connected case)
        
#         sys.exit()
        
        return input_, a, h

    
    

def get_model(params):
    model = Model_3(params)
    if params['if_gpu']:
        model.to(params['device'])
    return model


def get_A_A_T_kfac(a, h, l, params):
    
    layers_params = params['layers_params']
    device = params['device']
    
    kernel_size = layers_params[l]['conv_kernel_size']
    padding = layers_params[l]['conv_padding']


    if layers_params[l]['name'] == '1d-conv':
        size_test_2_A_j = kernel_size *\
layers_params[l]['conv_in_channels']
    elif layers_params[l]['name'] == 'conv':
        
        size_test_2_A_j = kernel_size**2 *\
layers_params[l]['conv_in_channels']
    
    if params['Kron_BFGS_if_homo']:
        size_test_2_A_j += 1


    test_2_A_j = torch.zeros(
        size_test_2_A_j, size_test_2_A_j, device=device
    )

    if layers_params[l]['name'] == '1d-conv':
        # 1d-conv: a[l]: M * I * |T|
        h_l_padded = F.pad(h[l], (padding, padding), "constant", 0)
        
        h_homo_ones = torch.ones(h_l_padded.size(0), 1 ,device=device)
        
        

        for t in range(a[l].size(2)):
            # a[l].size(2) = |T|
            
            h_l_t = h_l_padded[:, :, t:t+kernel_size].data

            # in the flatten, delta changes the fastest
            h_l_t_flatten = h_l_t.flatten(start_dim=1)
            
            if params['Kron_BFGS_if_homo']:
                h_l_t_flatten = torch.cat((h_l_t_flatten, h_homo_ones), dim=1)

            test_2_A_j += torch.mm(h_l_t_flatten.t(), h_l_t_flatten)
    elif layers_params[l]['name'] == 'conv':
        # 2d-conv: a[l]: M * I * |T|, where |T| has two dimensions
        h_l_padded = F.pad(
            h[l], (padding, padding, padding, padding), "constant", 0
        )
        
        h_homo_ones = torch.ones(h_l_padded.size(0), 1 ,device=device)

        for t1 in range(a[l].size(2)):
            for t2 in range(a[l].size(3)):
                
#                 print('h_l_padded.size()')
#                 print(h_l_padded.size())
                
                h_l_t =\
h_l_padded[:, :, t1:t1+kernel_size, t2:t2+kernel_size].data

#                 print('h_l_t.size()')
#                 print(h_l_t.size())

                h_l_t_flatten = h_l_t.flatten(start_dim=1)
                
                if params['Kron_BFGS_if_homo']:
                    h_l_t_flatten = torch.cat((h_l_t_flatten, h_homo_ones), dim=1)
                
#                 if l == 1:
#                     sys.exit()
    
                test_2_A_j += torch.mm(h_l_t_flatten.t(), h_l_t_flatten)
                
                
    return test_2_A_j




def get_A_A_T_kron_bfgs_v5(h, l, params):
    
    layers_params = params['layers_params']
    device = params['device']
                        
    in_channels = layers_params[l]['conv_in_channels']
    kernel_size = layers_params[l]['conv_kernel_size']
    padding = layers_params[l]['conv_padding']

    if layers_params[l]['name'] == '1d-conv':

        test_5_A_j = torch.zeros(
            in_channels,
            kernel_size,
            in_channels,
            kernel_size,
            device=device
        )

        h_l_padded = F.pad(h[l].data, (padding, padding), "constant", 0)
        
        T = h_l_padded.size(2)-kernel_size+1
        
        weight_conv1d = torch.ones(1, 1, T, device=device)

        for delta_2 in range(-kernel_size+1, kernel_size):
            
            
#             h_l_diff_1 = h_l_padded[:, :, np.maximum(0,delta_2):
#                     np.minimum(h_l_padded.size(2),h_l_padded.size(2)+delta_2)]
            
            if delta_2 > 0:
                h_l_diff_1 = h_l_padded[:, :, delta_2:h_l_padded.size(2)]
                h_l_diff_2 = h_l_padded[:, :, 0:h_l_padded.size(2)-delta_2]
            else:
                h_l_diff_1 = h_l_padded[:, :, 0:h_l_padded.size(2)+delta_2]
                h_l_diff_2 = h_l_padded[:, :, -delta_2:h_l_padded.size(2)]
            
            h_l_square = torch.einsum('ijl,ikl->jkl', h_l_diff_1.data, h_l_diff_2.data)
            
#             weight_conv1d = torch.ones(1, 1, T, device=device)
            
            h_l_square_conv = F.conv1d(
                h_l_square.view(h_l_square.size(0) * h_l_square.size(1), 1, -1),
                weight_conv1d
            ).view(h_l_square.size(0), h_l_square.size(1), -1)
            
            if delta_2 > 0:
                start_index = [0, delta_2]
            else:
                start_index = [-delta_2, 0]

            for delta in range(kernel_size-np.abs(delta_2)):
                test_5_A_j[
                    :, start_index[0] + delta, :, start_index[1] + delta
                ] = h_l_square_conv[:, :, delta].t()

            



#         print('test_5_A_j.size()')
#         print(test_5_A_j.size())
                
        test_5_A_j = test_5_A_j.view(
        test_5_A_j.size(0) * test_5_A_j.size(1),
        test_5_A_j.size(2) * test_5_A_j.size(3),
    )
        
#         print('test_5_A_j.size()')
#         print(test_5_A_j.size())
        
        if params['Kron_BFGS_if_homo']:
            homo_test_5_A_j = torch.zeros(
                test_5_A_j.size(0)+1, test_5_A_j.size(1)+1, device=device
            )
            
            homo_test_5_A_j[:-1, :-1] = test_5_A_j

            sum_h_l_padded = torch.sum(h_l_padded, dim=0)
            sum_h_l_padded = sum_h_l_padded.unsqueeze(1)
            weight_conv1d = torch.ones(1, 1, T, device=device) 
            homo_test_5_A_j[-1, :-1] =\
            F.conv1d(sum_h_l_padded, weight_conv1d).view(-1)
            
            homo_test_5_A_j[:-1, -1] = homo_test_5_A_j[-1, :-1]
            
            homo_test_5_A_j[-1, -1] = T * h_l_padded.size(0)
            
#             print('homo_test_5_A_j.requires_grad')
#             print(homo_test_5_A_j.requires_grad)
            
            test_5_A_j = homo_test_5_A_j

    elif layers_params[l]['name'] == 'conv':
        test_5_A_j = torch.zeros(
            in_channels,
            kernel_size,
            kernel_size,
            in_channels,
            kernel_size,
            kernel_size,
            device=device
        )

        h_l_padded = F.pad(
            h[l].data, (padding, padding, padding, padding), "constant", 0
        )
        
        
#         print('h_l_padded.size()')
#         print(h_l_padded.size())
#         sys.exit()
        T0 = h_l_padded.size(2)-kernel_size+1
        T1 = h_l_padded.size(3)-kernel_size+1
        
        
        weight_conv1d = torch.ones(1, 1, h_l_padded.size(2)-kernel_size+1, h_l_padded.size(3)-kernel_size+1, device=device)
        
        
        for delta_2_0 in range(-kernel_size+1, kernel_size):
            for delta_2_1 in range(-kernel_size+1, kernel_size):
                
                
                if delta_2_0 > 0:
                    h_l_diff_1 = h_l_padded[:, :, delta_2_0:h_l_padded.size(2)]
                    h_l_diff_2 = h_l_padded[:, :, 0:h_l_padded.size(2)-delta_2_0]
                else:
                    h_l_diff_1 = h_l_padded[:, :, 0:h_l_padded.size(2)+delta_2_0]
                    h_l_diff_2 = h_l_padded[:, :, -delta_2_0:h_l_padded.size(2)]
                    
#                 print('h_l_diff_1.size()')
#                 print(h_l_diff_1.size())
                    
                if delta_2_1 > 0:
                    h_l_diff_1 = h_l_diff_1[:, :, :, delta_2_1:h_l_padded.size(3)]
                    h_l_diff_2 = h_l_diff_2[:, :, :, 0:h_l_padded.size(3)-delta_2_1]
                else:
                    h_l_diff_1 = h_l_diff_1[:, :, :, 0:h_l_padded.size(3)+delta_2_1]
                    h_l_diff_2 = h_l_diff_2[:, :, :, -delta_2_1:h_l_padded.size(3)]

                
                
                h_l_square = torch.einsum(
                    'ijlt,iklt->jklt', h_l_diff_1.data, h_l_diff_2.data
                )
                
#                 print('h_l_diff_1.size()')
#                 print(h_l_diff_1.size())
                
                
                t1=0
                t2=0
                kfac_h_l_t =\
h_l_padded[:, :, t1:t1+kernel_size, t2:t2+kernel_size].data



#                 kfac_h_l_t_flatten = kfac_h_l_t.flatten(start_dim=1)
#                 torch.mm(kfac_h_l_t_flatten.t(), kfac_h_l_t_flatten)
                
#                 h_l_diff_1 = h_l_diff_1.reshape(h_l_diff_1.size(0), h_l_diff_1.size(1), h_l_diff_1.size(2) * h_l_diff_1.size(3))
                
#                 h_l_diff_2 = h_l_diff_2.reshape(h_l_diff_2.size(0), h_l_diff_2.size(1), h_l_diff_2.size(2) * h_l_diff_2.size(3))
                
#                 h_l_diff_1 = h_l_diff_1.permute(2,1,0)
#                 h_l_diff_2 = h_l_diff_2.permute(2,0,1)
                
#                 h_l_diff_1 = h_l_diff_1.data
#                 h_l_diff_2 = h_l_diff_2.data
                
#                 torch.bmm(h_l_diff_1, h_l_diff_2)
                
#                 sys.exit()



                
#                 weight_conv1d = torch.ones(1, 1, h_l_padded.size(2)-kernel_size+1, h_l_padded.size(3)-kernel_size+1, device=device)
                
                h_l_square_conv = F.conv1d(
                    h_l_square.view(h_l_square.size(0) * h_l_square.size(1), 1, *(h_l_square.size()[2:])),
                    weight_conv1d
                )
                
                
                h_l_square_conv = h_l_square_conv.view(h_l_square.size(0), h_l_square.size(1), *(h_l_square_conv.size()[2:]))
                
#                 print('h_l_square_conv.size()')
#                 print(h_l_square_conv.size())
            
                if delta_2_0 > 0:
                    start_index_0 = [0, delta_2_0]
                else:
                    start_index_0 = [-delta_2_0, 0]

                if delta_2_1 > 0:
                    start_index_1 = [0, delta_2_1]
                else:
                    start_index_1 = [-delta_2_1, 0]

                for delta_0 in range(kernel_size-np.abs(delta_2_0)):
                    for delta_1 in range(kernel_size-np.abs(delta_2_1)):
                        test_5_A_j[
                            :, start_index_0[0] + delta_0, start_index_1[0] + delta_1,
                            :, start_index_0[1] + delta_0, start_index_1[1] + delta_1
                        ] = h_l_square_conv[:, :, delta_0, delta_1].t()

        test_5_A_j = test_5_A_j.view(
        test_5_A_j.size(0) * test_5_A_j.size(1) * test_5_A_j.size(2),
        test_5_A_j.size(3) * test_5_A_j.size(4) * test_5_A_j.size(5),
    )  

        
#         print('test_5_A_j.requires_grad before homo')
#         print(test_5_A_j.requires_grad)
        
        
        if params['Kron_BFGS_if_homo']:
            homo_test_5_A_j = torch.zeros(
                test_5_A_j.size(0)+1, test_5_A_j.size(1)+1, device=device
            )
            
            homo_test_5_A_j[:-1, :-1] = test_5_A_j
            
#             print('h_l_padded.size()')
#             print(h_l_padded.size())

            sum_h_l_padded = torch.sum(h_l_padded, dim=0)
            
#             print('sum_h_l_padded.size()')
#             print(sum_h_l_padded.size())
            
            sum_h_l_padded = sum_h_l_padded.unsqueeze(1)

#             print('homo_test_5_A_j.requires_grad before conv1d')
#             print(homo_test_5_A_j.requires_grad)
            
            weight_conv1d = torch.ones(1, 1, T0, T1, device=device) 
            
#             print('sum_h_l_padded.requires_grad')
#             print(sum_h_l_padded.requires_grad)
#             print('weight_conv1d.requires_grad')
#             print(weight_conv1d.requires_grad)
            
            homo_test_5_A_j[-1, :-1] =\
            F.conv1d(sum_h_l_padded, weight_conv1d).view(-1)
            
#             print('homo_test_5_A_j.requires_grad after conv1d')
#             print(homo_test_5_A_j.requires_grad)
            
            homo_test_5_A_j[:-1, -1] = homo_test_5_A_j[-1, :-1]
            
            homo_test_5_A_j[-1, -1] = T0 * T1 * h_l_padded.size(0)
            
            test_5_A_j = homo_test_5_A_j
        
#     print('test_5_A_j.requires_grad')
#     print(test_5_A_j.requires_grad)
        
        
    return test_5_A_j








def get_A_A_T_kron_bfgs(h, l, params):
    
    layers_params = params['layers_params']
    device = params['device']
                        
    in_channels = layers_params[l]['conv_in_channels']
    kernel_size = layers_params[l]['conv_kernel_size']
    padding = layers_params[l]['conv_padding']

    if layers_params[l]['name'] == '1d-conv':

        test_5_A_j = torch.zeros(
            in_channels,
            kernel_size,
            in_channels,
            kernel_size,
            device=device
        )

        h_l_padded = F.pad(h[l], (padding, padding), "constant", 0)

        for delta_2 in range(-kernel_size+1, kernel_size):
            h_l_diff = h_l_padded[
                :,
                :,
                np.remainder(np.asarray(range(h[l].size(2)))-delta_2,h[l].size(2))
                                 ]


            cutting_indices_2 = np.arange(
                    np.maximum(0,delta_2),
                    np.minimum(h_l_padded.size(2),h_l_padded.size(2)+delta_2)
                )


            cutting_indices =\
            np.ix_(
                np.asarray(range(h_l_padded.size(0))),
                np.asarray(range(h_l_padded.size(1))),
                cutting_indices_2
            )

            h_l_diff_1 = h_l_padded[cutting_indices]
            h_l_diff_2 = h_l_diff[cutting_indices]
            einsum_ = torch.einsum('ijl,ikl->ijkl', h_l_diff_1.data, h_l_diff_2.data)


            h_l_square = torch.sum(einsum_, dim=0)

            sum_h_l_square = torch.sum(h_l_square, dim=-1)
            for delta in range(kernel_size-np.abs(delta_2)):

                # the length of minus part is:
                # kernel_size-np.abs(delta_2) - 1

                indices_dim_2 = np.arange(0, h_l_square.size(2))
                indcies_dim_2_included =\
                np.arange(0, h_l_square.size(2) - (kernel_size-np.abs(delta_2) - 1)) + delta

                indcies_dim_2_excludes =\
                np.setdiff1d(indices_dim_2, indcies_dim_2_included)

                if delta_2 > 0:
                    start_index = [0, delta_2]
                else:
                    start_index = [-delta_2, 0]

                test_5_A_j[
                    :, start_index[0] + delta, :, start_index[1] + delta
                ] = (
                    sum_h_l_square-\
                    torch.sum(
                        h_l_square[
                            :, :, indcies_dim_2_excludes
                        ],
                        dim=-1
                    )
                ).t()

        test_5_A_j = test_5_A_j.view(
        test_5_A_j.size(0) * test_5_A_j.size(1),
        test_5_A_j.size(2) * test_5_A_j.size(3),
    )

    elif layers_params[l]['name'] == 'conv':
        test_5_A_j = torch.zeros(
            in_channels,
            kernel_size,
            kernel_size,
            in_channels,
            kernel_size,
            kernel_size,
            device=device
        )

        h_l_padded = F.pad(
            h[l], (padding, padding, padding, padding), "constant", 0
        )
        for delta_2_0 in range(-kernel_size+1, kernel_size):
            for delta_2_1 in range(-kernel_size+1, kernel_size):

                diff_indices =\
                np.ix_(
                    np.asarray(range(h_l_padded.size(0))),
                    np.asarray(range(h_l_padded.size(1))),
                    np.remainder(
                        np.asarray(range(h_l_padded.size(2)))-delta_2_0,
                        h_l_padded.size(2)
                    ),
                    np.remainder(
                        np.asarray(range(h_l_padded.size(3)))-delta_2_1,
                        h_l_padded.size(3)
                    )
                )

                h_l_diff = h_l_padded[diff_indices]

                cutting_indices_2 = np.arange(
                    np.maximum(0,delta_2_0),
                    np.minimum(h_l_padded.size(2),h_l_padded.size(2)+delta_2_0)
                )

                cutting_indices_3 = np.arange(
                    np.maximum(0,delta_2_1),
                    np.minimum(h_l_padded.size(3),h_l_padded.size(3)+delta_2_1)
                )


                cutting_indices =\
                np.ix_(
                    np.asarray(range(h_l_padded.size(0))),
                    np.asarray(range(h_l_padded.size(1))),
                    cutting_indices_2,
                    cutting_indices_3
                )

                h_l_diff_1 = h_l_padded[cutting_indices]

                h_l_diff_2 = h_l_diff[cutting_indices]

                einsum_ = torch.einsum(
                    'ijlt,iklt->ijklt', h_l_diff_1.data, h_l_diff_2.data
                )

                h_l_square = torch.sum(einsum_, dim=0)
                
#                 print('h_l_square.size()')
#                 print(h_l_square.size())
#                 sys.exit()
                
                sum_h_l_square_dim_2 = torch.sum(h_l_square, dim=[2]) # dim 3 remains open
                sum_h_l_square_dim_3 = torch.sum(h_l_square, dim=[3])

                sum_h_l_square = torch.sum(h_l_square, dim=[-1, -2])


                for delta_0 in range(kernel_size-np.abs(delta_2_0)):
                    for delta_1 in range(kernel_size-np.abs(delta_2_1)):

                        if delta_2_0 > 0:
                            start_index_0 = [0, delta_2_0]
                        else:
                            start_index_0 = [-delta_2_0, 0]

                        if delta_2_1 > 0:
                            start_index_1 = [0, delta_2_1]
                        else:
                            start_index_1 = [-delta_2_1, 0]


                        indices_dim_2 = np.arange(0, h_l_square.size(2))
                        indcies_dim_2_included =\
                        np.arange(0, h_l_square.size(2) - (kernel_size-np.abs(delta_2_0) - 1)) + delta_0

                        indcies_dim_2_excludes =\
                        np.setdiff1d(indices_dim_2, indcies_dim_2_included)

                        indices_dim_3 = np.arange(0, h_l_square.size(3))
                        indcies_dim_3_included =\
                        np.arange(0, h_l_square.size(3) - (kernel_size-np.abs(delta_2_1) - 1)) + delta_1

                        indcies_dim_3_excludes =\
                        np.setdiff1d(indices_dim_3, indcies_dim_3_included)


                        # this should be used for sum_h_l_square_dim_2
                        slicing_indices_minus_1 =\
                np.ix_(
                    np.asarray(range(h_l_square.size(0))),
                    np.asarray(range(h_l_square.size(1))),
                    indcies_dim_3_excludes
                )

                        # this should be used for sum_h_l_square_dim_3
                        slicing_indices_minus_2 =\
                np.ix_(
                    np.asarray(range(h_l_square.size(0))),
                    np.asarray(range(h_l_square.size(1))),
                    indcies_dim_2_excludes
                )

                        slicing_indices_plus =\
                np.ix_(
                    np.asarray(range(h_l_square.size(0))),
                    np.asarray(range(h_l_square.size(1))),
                    indcies_dim_2_excludes,
                    indcies_dim_3_excludes
                )
                        
#                         print('sum_h_l_square_dim_2.size()')
#                         print(sum_h_l_square_dim_2.size())
                        
#                         print('sum_h_l_square_dim_3.size()')
#                         print(sum_h_l_square_dim_3.size())
                        
#                         sys.exit()


                        test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ] = sum_h_l_square            
        
                        test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ] -=\
                        torch.sum(sum_h_l_square_dim_2[slicing_indices_minus_1], dim=[-1])
                   
        
                        test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ] -=\
                        torch.sum(sum_h_l_square_dim_3[slicing_indices_minus_2], dim=[-1])
                        
                
            
                        test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ] +=\
                        torch.sum(h_l_square[slicing_indices_plus], dim=[-2, -1])
                
                        
    
                        test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ] =\
                        copy.deepcopy(test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ].t())
        
                        '''
                        if l == 1:
                            
                        
                            print('(\
                        sum_h_l_square\
                       -torch.sum(sum_h_l_square_dim_2[slicing_indices_minus_1], dim=[-1])\
                       -torch.sum(sum_h_l_square_dim_3[slicing_indices_minus_2], dim=[-1])\
                       +torch.sum(h_l_square[slicing_indices_plus], dim=[-2, -1])\
                    ).t() -\
                            test_5_A_j[\
                                :, \
                                start_index_0[0]+delta_0,\
                                start_index_1[0]+delta_1,\
                                :,\
                                start_index_0[1]+delta_0,\
                                start_index_1[1]+delta_1\
                            ]')
                            
                            print((
                        sum_h_l_square\
                       -torch.sum(sum_h_l_square_dim_2[slicing_indices_minus_1], dim=[-1])\
                       -torch.sum(sum_h_l_square_dim_3[slicing_indices_minus_2], dim=[-1])\
                       +torch.sum(h_l_square[slicing_indices_plus], dim=[-2, -1])
                    ).t() -\
                            test_5_A_j[
                                :, 
                                start_index_0[0]+delta_0,
                                start_index_1[0]+delta_1,
                                :,
                                start_index_0[1]+delta_0,
                                start_index_1[1]+delta_1
                            ])
                            
                            print('test_5_A_j[\
                                :, \
                                start_index_0[0]+delta_0,\
                                start_index_1[0]+delta_1,\
                                :,\
                                start_index_0[1]+delta_0,\
                                start_index_1[1]+delta_1\
                            ].size()')
                            print(test_5_A_j[
                                :, 
                                start_index_0[0]+delta_0,
                                start_index_1[0]+delta_1,
                                :,
                                start_index_0[1]+delta_0,
                                start_index_1[1]+delta_1
                            ].size())
                            
                            sys.exit()
                            


                        test_5_A_j[
                            :, 
                            start_index_0[0]+delta_0,
                            start_index_1[0]+delta_1,
                            :,
                            start_index_0[1]+delta_0,
                            start_index_1[1]+delta_1
                        ] = (
                    sum_h_l_square\
                   -torch.sum(sum_h_l_square_dim_2[slicing_indices_minus_1], dim=[-1])\
                   -torch.sum(sum_h_l_square_dim_3[slicing_indices_minus_2], dim=[-1])\
                   +torch.sum(h_l_square[slicing_indices_plus], dim=[-2, -1])
                ).t()
                '''
                
    
#         if l == 1:
#             sys.exit()

        test_5_A_j = test_5_A_j.view(
        test_5_A_j.size(0) * test_5_A_j.size(1) * test_5_A_j.size(2),
        test_5_A_j.size(3) * test_5_A_j.size(4) * test_5_A_j.size(5),
    )  
        
    return test_5_A_j



def train_initialization(data_, params, args):
    algorithm = params['algorithm']
    
    if params['N2'] > params['N1']:
        print('Error! 1432')
        sys.exit()
        
    params['if_grafting'] = args['if_grafting']
    
    params['weight_decay'] = args['weight_decay']
        
    if params['if_lr_decay']:
        params['num_epoch_to_decay'] = args['num_epoch_to_decay']
        params['lr_decay_rate'] = args['lr_decay_rate']
        
#     import utils_git.utils_kbfgs as utils_kbfgs
    import utils_git.utils_shampoo as utils_shampoo
    import utils_git.utils_kfac as utils_kfac

    if algorithm in utils_kfac.list_algorithm:
        
        params['kfac_if_svd'] = args['kfac_if_svd']
        
        params['kfac_if_update_BN'] = args['kfac_if_update_BN']
        params['kfac_if_BN_grad_direction'] = args['kfac_if_BN_grad_direction']
        
        if params['kfac_if_update_BN'] == False and params['weight_decay'] != 0:
            print('error: only work if weight_decay == 0')
            sys.exit()
        
        if algorithm in ['kfac-no-max-no-LM',
                         'kfac-warmStart-no-max-no-LM',
                         'kfac-correctFisher-warmStart-no-max-no-LM',
                         'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                         'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                         'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                         'kfac-warmStart-lessInverse-no-max-no-LM',
                         'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM']:
            params['Kron_BFGS_if_homo'] = True
        
        if algorithm in ['ekfac-EF-VA',
                         'ekfac-EF',
                         'kfac-EF']:
            print('error: need to check warm start')
            sys.exit()

        if params['algorithm'] in ['kfac-TR',
                                   'kfac-momentum-grad-TR']:
            params['TR_max_iter'] = args['TR_max_iter']
        if params['algorithm'] in ['kfac-CG',
                                   'kfac-momentum-grad-CG']:
            params['CG_max_iter'] = args['CG_max_iter']
            
        if params['algorithm'] in ['kfac-no-max-no-LM',
                                   'kfac-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                   'kfac-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                   'kfac-NoMaxNoSqrt-no-LM']:
            params['kfac_damping_lambda'] = args['kfac_damping_lambda']
            
        if params['algorithm'] == 'kfac-no-max-epsilon-A-G-no-LM':
            params['kfac_A_epsilon'] = args['kfac_A_epsilon']
            params['kfac_G_epsilon'] = args['kfac_G_epsilon']
            
        

        device = params['device']
        layersizes = params['layersizes']
        numlayers = params['numlayers']
        
        layers_params = params['layers_params']

        A = []  # KFAC A
        G = []  # KFAC G


        for l in range(numlayers):
            if params['layers_params'][l]['name'] == 'fully-connected':
                
                input_size = params['layers_params'][l]['input_size']
                output_size = params['layers_params'][l]['output_size']
#                 assert layersizes[l] == input_size
#                 assert layersizes[l+1] == output_size
            
#                 A.append(torch.zeros(layersizes[l] + 1, layersizes[l] + 1, device=device))
                
                A.append(torch.zeros(input_size + 1, input_size + 1, device=device))
                
                
#                 G.append(torch.zeros(layersizes[l+1], layersizes[l+1], device=device))
                G.append(torch.zeros(output_size, output_size, device=device))
                
            elif params['layers_params'][l]['name'] in ['conv',
                                                        'conv-no-activation',
                                                        'conv-no-bias-no-activation',]:
            
                size_A = layers_params[l]['conv_in_channels'] *\
                layers_params[l]['conv_kernel_size']**2
                
                if params['layers_params'][l]['name'] in ['conv',
                                                          'conv-no-activation',]:
                
                    size_A += 1
            
                A.append(torch.zeros(size_A, size_A, device=device))
                
                size_G = layers_params[l]['conv_out_channels']
                
                G.append(torch.zeros(size_G, size_G, device=device))
            elif params['layers_params'][l]['name'] in ['BN']:
                
#                 print('layers_params[l]')
#                 print(layers_params[l])
                
#                 sys.exit()
                
                A.append([])
                
                size_G = layers_params[l]['num_features'] * 2
                
                G.append(torch.zeros(size_G, size_G, device=device))
            else:
                print('Error: unsupported layer when initial cache for ' + params['layers_params'][l]['name'])
                sys.exit()

        data_['A'] = A
        data_['G'] = G


        if params['algorithm'] in ['kfac',
                                   'kfac-no-max',
                                   'kfac-NoMaxNoSqrt',
                                   'kfac-NoMaxNoSqrt-no-LM',
                                   'kfac-no-max-no-LM',
                                   'kfac-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                   'kfac-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                   'kfac-no-max-epsilon-A-G-no-LM']:
            
#             print('params[kfac_if_svd]')
#             print(params['kfac_if_svd'])
            
#             sys.exit()
            
            if params['kfac_if_svd']:
                
                U_A, U_G = numlayers * [0], numlayers * [0]
                
                data_['U_A'] = U_A
                data_['U_G'] = U_G
                
                s_A, s_G = numlayers * [0], numlayers * [0]
                
                data_['s_A'] = s_A
                data_['s_G'] = s_G
                
            else:
            
                A_inv, G_inv = numlayers * [0], numlayers * [0]

                data_['A_inv'] = A_inv
                data_['G_inv'] = G_inv

        if params['algorithm'] == 'ekfac-EF-VA' or\
        params['algorithm'] == 'ekfac-EF':
            U_A, U_G = model.numlayers * [0], model.numlayers * [0]
            data_['U_A'] = U_A
            data_['U_G'] = U_G

            data_['ekfac_s'] = get_zero_torch(params)

            if params['algorithm'] == 'ekfac-EF-VA':
                data_['ekfac_m'] = get_zero_torch(params)

        params['kfac_inverse_update_freq'] = args['kfac_inverse_update_freq']
        params['kfac_cov_update_freq'] = args['kfac_cov_update_freq']
        params['kfac_rho'] = args['kfac_rho']
        
        get_warm_start(data_, params)
        
    elif algorithm in ['RMSprop',
                       'RMSprop-warmStart',
                       'RMSprop-test',
                       'Adam',
                       'Adam-test',
                       'Adam-noWarmStart',
                       'RMSprop-no-sqrt',
                       'RMSprop-individual-grad',
                       'RMSprop-individual-grad-no-sqrt',
                       'RMSprop-individual-grad-no-sqrt-Fisher',
                       'RMSprop-individual-grad-no-sqrt-LM']:
        
        params['RMSprop_epsilon'] = args['RMSprop_epsilon']
        
        params['RMSprop_beta_2'] = args['RMSprop_beta_2']
        
        data_['RMSprop_momentum_2'] = get_zero_torch(params)
        
        if algorithm in ['RMSprop-warmStart',
                         'Adam',
                         'Adam-test']:
        
            N1 = params['N1']
            device = params['device']
            model = data_['model']
            if N1 < params['num_train_data']:
                # i.e. stochastic setting

                i = 0 # position of training data
                j = 0 # position of mini-batch

                while i + N1 <= params['num_train_data']:



                    X_mb, t_mb = data_['dataset'].train.next_batch(N1)
                    X_mb = torch.from_numpy(X_mb).to(device)
                    t_mb = torch.from_numpy(t_mb).to(device)



                    z, a, h = model.forward(X_mb)
                    loss = get_loss_from_z(model, z, t_mb, reduction='mean') # not regularized

                    model.zero_grad()
                    loss.backward()

                    model_grad = get_model_grad(model, params)

                    if params['if_regularized_grad'] and params['tau'] != 0:


                        model_grad = get_plus_torch(
                        model_grad,
                        get_multiply_scalar_no_grad(params['tau'], model.layers_weight)
                        )
                    else:
                        1

                    i += N1
                    j += 1




    #                 sys.exit()

    #                 for l in range(numlayers):
                        # bar_A_j = 1 / j * (A_1 + ... + A_j)
                        # bar_A_j = (j-1) / j * bar_A_{j-1} + 1 / j * A_j

    #                     homo_h_l = torch.cat((h[l], torch.ones(N1, 1, device=device)), dim=1)

    #                     A_j = 1/N1 * torch.mm(homo_h_l.t(), homo_h_l).data

    #                     data_['A'][l] *= (j-1)/j
                    data_['RMSprop_momentum_2'] = get_multiply_scalar(
                            (j-1)/j, data_['RMSprop_momentum_2']
                        )

    #                     data_['A'][l] += 1/j * A_j
                    data_['RMSprop_momentum_2'] = get_plus_torch(
                            data_['RMSprop_momentum_2'],
                            get_multiply_scalar(1/j, get_square_torch(model_grad))
                        )

    
                
            

        
        
    elif params['algorithm'] in utils_shampoo.list_algorithm:
        
        params['shampoo_if_coupled_newton'] = args['shampoo_if_coupled_newton']
        
        params['if_Hessian_action'] = args['if_Hessian_action']

        params['shampoo_inverse_freq'] = args['shampoo_inverse_freq']
    
        params['shampoo_update_freq'] = args['shampoo_update_freq']
        
        params['shampoo_decay'] = args['shampoo_decay']
        params['shampoo_weight'] = args['shampoo_weight']
        
        if params['algorithm'] in ['matrix-normal-allVariables-warmStart-MaxEigDamping',
                                   'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                   'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',]:
            pass
        elif params['algorithm'] in ['shampoo-allVariables-warmStart',
                                     'shampoo-allVariables-warmStart-lessInverse',
                                     'shampoo-allVariables-filterFlattening-warmStart',
                                     'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-same-trace-allVariables-warmStart',
                                     'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                     'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
            params['shampoo_epsilon'] = args['shampoo_epsilon']
        else:
            print('params[algorithm]')
            print(params['algorithm'])
            sys.exit()
        
            
        
        numlayers = params['numlayers']
        
        data_['shampoo_H'] = []
        for l in range(numlayers):
            data_['shampoo_H'].append({})
    
        data_['shampoo_H_LM_minus_2k'] = []
        for l in range(numlayers):
            data_['shampoo_H_LM_minus_2k'].append({})
    
        data_['shampoo_H_trace'] = []
        for l in range(numlayers):
            data_['shampoo_H_trace'].append({})
            
        if params['algorithm'] in ['matrix-normal-allVariables-warmStart',
                                   'matrix-normal-allVariables-warmStart-MaxEigDamping',
                                   'matrix-normal-allVariables-warmStart-noPerDimDamping',
                                   'matrix-normal-same-trace-warmStart',
                                   'matrix-normal-same-trace-warmStart-noPerDimDamping',
                                   'matrix-normal-same-trace-allVariables-warmStart',
                                   'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                   'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                                   'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                   'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                   'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                                   'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                   'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                   'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                   'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                   'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                                   'shampoo-allVariables-warmStart',
                                   'shampoo-allVariables-warmStart-lessInverse',
                                   'shampoo-allVariables-filterFlattening-warmStart',
                                   'shampoo-allVariables-filterFlattening-warmStart-lessInverse',]:
            params['if_warm_start'] = True
        elif params['algorithm'] in ['matrix-normal-allVariables',
                                     'matrix-normal-same-trace',
                                     'matrix-normal-same-trace-allVariables',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                     'shampoo-allVariables']:
            params['if_warm_start'] = False
        else:
            print('error: need to check for ' + params['algorithm'])
            sys.exit()
            
        
        if params['if_warm_start']:
            get_warm_start(data_, params)
            
    elif algorithm in ['Fisher-BD',]:
        
        params['Fisher_BD_damping'] = args['Fisher_BD_damping']
        
        data_['block_Fisher'] = []
        for l in range(params['numlayers']):
            data_['block_Fisher'].append([])
        
        get_warm_start(data_, params)

    elif params['algorithm'] in ['SMW-Fisher-batch-grad-momentum-exponential-decay',
                                 'SMW-Fisher-batch-grad-momentum']:
        params['N_iters'] = 30

        from collections import deque
        data_['batch_grads'] = deque()
        data_['batch_grads_a_grad'] = deque()
        data_['batch_grads_h'] = deque()

        print('test batch_grads_a_grad')

        data_['batch_grads_test'] = []

        print('test batch_grads_test')


        data_['D_t_minus_lambda'] = []

    elif params['algorithm'] == 'SMW-Fisher-momentum':
        a_grad_momentum = []
        h_momentum = []

        layersizes = data_['model'].layersizes

        for l in range(model.numlayers):
            a_grad_momentum.append(
                torch.zeros(N2, layersizes[l+1], device=params['device']))
            h_momentum.append(torch.zeros(N2, layersizes[l], device=params['device']))

        data_['a_grad_momentum'] = a_grad_momentum
        data_['h_momentum'] = h_momentum

        D_t_inv = np.zeros((N2, N2))
        data_['D_t_inv'] = D_t_inv

    elif params['algorithm'] == 'SMW-Fisher-D_t-momentum':
        data_['J_J_transpose'] = np.float32(np.zeros((N2, N2)))

    elif params['algorithm'] == 'SMW-Fisher-momentum-D_t-momentum':
        a_grad_momentum = []
        h_momentum = []

        layersizes = params['layersizes']

        for l in range(model.numlayers):
            a_grad_momentum.append(torch.zeros(N2, layersizes[l+1]))
            h_momentum.append(torch.zeros(N2, layersizes[l]))

        data_['a_grad_momentum'] = a_grad_momentum
        data_['h_momentum'] = h_momentum

        data_['J_J_transpose'] = np.float32(np.zeros((N2, N2)))

    elif algorithm in ['SMW-Fisher-signVAsqrt-p',
                       'SMW-Fisher-VA-p',
                       'SMW-Fisher-momentum-p-sign',
                       'SMW-Fisher-momentum-p',
                       'SMW-Fisher-sign',
                       'SMW-Fisher-different-minibatch',
                       'SMW-Fisher',
                       'SGD-VA',
                       'SGD-yura-BD',
                       'SGD-yura-old',
                       'SGD-yura',
                       'SGD-sign',
                       'SGD-signVAerf',
                       'SGD-signVA',
                       'SGD-signVAsqrt',
                       'SGD-momentum-yura',
                       'SGD-momentum',
                       'SGD',
                       'SMW-GN',
                       'GI-Fisher',
                       'SMW-Fisher-BD',
                       'Kron-SGD',
                       'BFGS',
                       'BFGS-homo']:
        
        pass
    
        
    elif algorithm == 'Kron-BFGS-1st-layer-only':
        print('Error: this algo is abandoned')
        sys.exit()
        
    elif algorithm == 'SGD-yura-MA':
        params['yura_lambda_second_term_MA'] = 0
        params['yura_lambda_second_term_MA_weight'] =\
        args['yura_lambda_second_term_MA_weight']
    elif params['algorithm'] == 'SMW-Fisher-batch-grad':
        params['N_iters'] = 30
    else:
        print('Error: momentum variables not defined for ' + params['algorithm'])
        sys.exit()
        
        
    return data_, params





def train(args):
    
    print('\n')
    print('learning rate = {}'.format(args['alpha']))
    
    assert os.path.isdir(args['home_path'])
    
#     import datetime
#     current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
#     with open(args['home_path'] + 'notebooks/lr_record_' + current_time + '.txt', 'w') as fp:
#         fp.write('algorithm = {}\n'.format(args['algorithm']))
#         fp.write('learning rate = {}\n'.format(args['alpha']))
        

        

    print('args')
    print(args)
    
    from utils_git.utils_kfac import kfac_update
#     from utils_git.utils_kbfgs import Kron_BFGS_update_v2
    from utils_git.utils_shampoo import shampoo_update
    
#     from utils_git.utils_kbfgs import list_algorithm as kbfgs_list_algorithm
    
    params = {}
    
    from utils_git.utils_shampoo import list_algorithm as list_algorithm_shampoo
    
    params['list_algorithm_shampoo'] = list_algorithm_shampoo
    
    from utils_git.utils_kfac import list_algorithm as list_algorithm_kfac
    
    params['list_algorithm_kfac'] = list_algorithm_kfac
    
    torch.cuda.empty_cache()
    
    params['initialization_pkg'] = args['initialization_pkg']
    
#     seed_number = 9999
    seed_number = args['seed_number']
    
    
    params['seed_number'] = seed_number

    np.random.seed(seed_number)
    torch.manual_seed(seed_number)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    num_threads = args['num_threads']
    params['num_threads'] = num_threads
    
#     print('num_threads')
#     print(num_threads)
    
#     print('num_threads == float(inf)')
#     print(num_threads == float('inf'))
    
    if num_threads == float('inf'):
        1
    else:
    
        torch.set_num_threads(num_threads)
        assert torch.get_num_threads() == num_threads
        
#     print('torch.get_num_threads()')
#     print(torch.get_num_threads())
    
    
    
#     sys.exit()


    

    params['home_path'] = args['home_path']
    params['if_gpu'] = args['if_gpu']
    params['if_test_mode'] = args['if_test_mode']

    if params['if_gpu'] and torch.cuda.is_available():  
        dev = "cuda:0" 
    else:  
        dev = "cpu"  
    device = torch.device(dev)  
    params['device'] = device
    

    params['algorithm'] = args['algorithm']

    
    
    
    
    matrix_name = args['matrix_name']
    params['matrix_name'] = matrix_name
    
    params['if_records'] = {}

    params['if_record_sgd_norm'] = args['if_record_sgd_norm']
    params['if_record_p_norm'] = args['if_record_p_norm']
#     params['if_record_kron_bfgs_cosine'] = args['if_record_kron_bfgs_cosine']
    params['if_record_kfac_p_norm'] = args['if_record_kfac_p_norm']
    params['if_record_kfac_p_cosine'] = args['if_record_kfac_p_cosine']
    params['if_record_res_grad_norm'] = args['if_record_res_grad_norm']
    params['if_record_res_grad_random_norm'] = args['if_record_res_grad_random_norm']
    params['if_record_res_grad_grad_norm'] = args['if_record_res_grad_grad_norm']
    params['if_record_res_grad_norm_per_iter'] = args['if_record_res_grad_norm_per_iter']
    params['if_record_sgn_norm'] = args['if_record_sgn_norm']
    
    if params['if_test_mode']:
        if 'if_record_kron_bfgs_update_status' in args:
            params['if_record_kron_bfgs_update_status'] =\
            args['if_record_kron_bfgs_update_status']
        if 'if_record_kron_bfgs_matrix_norm_per_iter' in args:
            params['if_record_kron_bfgs_matrix_norm_per_iter'] =\
            args['if_record_kron_bfgs_matrix_norm_per_iter']
#         if 'if_record_loss_per_iter' in args:
#             params['if_record_loss_per_iter'] =\
#             args['if_record_loss_per_iter']
        if 'if_record_kfac_G_inv_norm_per_iter' in args:
            params['if_record_kfac_G_inv_norm_per_iter'] =\
            args['if_record_kfac_G_inv_norm_per_iter']
        if 'if_record_kfac_G_inv_norm_per_epoch' in args:
            params['if_record_kfac_G_inv_norm_per_epoch'] =\
            args['if_record_kfac_G_inv_norm_per_epoch']
            
        if 'if_record_kfac_G_norm_per_epoch' in args:
            params['if_records']['if_record_kfac_G_norm_per_epoch'] =\
            args['if_record_kfac_G_norm_per_epoch']
        else:
            params['if_records']['if_record_kfac_G_norm_per_epoch'] = False
            
        if 'if_record_kfac_G_twoNorm_per_epoch' in args:
            params['if_records']['if_record_kfac_G_twoNorm_per_epoch'] =\
            args['if_record_kfac_G_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_kfac_G_twoNorm_per_epoch'] = False
            
        if 'if_record_kfac_A_twoNorm_per_epoch' in args:
            params['if_records']['if_record_kfac_A_twoNorm_per_epoch'] =\
            args['if_record_kfac_A_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_kfac_A_twoNorm_per_epoch'] = False
            
        if 'if_record_kron_bfgs_A_twoNorm_per_epoch' in args:
            params['if_records']['if_record_kron_bfgs_A_twoNorm_per_epoch'] =\
            args['if_record_kron_bfgs_A_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_kron_bfgs_A_twoNorm_per_epoch'] = False
            
        if 'if_record_kron_bfgs_G_LM_twoNorm_per_epoch' in args:
            params['if_records']['if_record_kron_bfgs_G_LM_twoNorm_per_epoch'] =\
            args['if_record_kron_bfgs_G_LM_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_kron_bfgs_G_LM_twoNorm_per_epoch'] = False
            
        if 'if_record_kron_bfgs_Hg_twoNorm_per_epoch' in args:
            params['if_records']['if_record_kron_bfgs_Hg_twoNorm_per_epoch'] =\
            args['if_record_kron_bfgs_Hg_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_kron_bfgs_Hg_twoNorm_per_epoch'] = False
            
        if 'if_record_kron_bfgs_Ha_twoNorm_per_epoch' in args:
            params['if_records']['if_record_kron_bfgs_Ha_twoNorm_per_epoch'] =\
            args['if_record_kron_bfgs_Ha_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_kron_bfgs_Ha_twoNorm_per_epoch'] = False
            
        if 'if_record_kron_bfgs_matrix_norm' in args:
            params['if_records']['if_record_kron_bfgs_matrix_norm'] =\
            args['if_record_kron_bfgs_matrix_norm']
        else:
            params['if_records']['if_record_kron_bfgs_matrix_norm'] = False
            
        if 'if_record_layerWiseHessian_twoNorm_per_epoch' in args:
            params['if_records']['if_record_layerWiseHessian_twoNorm_per_epoch'] =\
            args['if_record_layerWiseHessian_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_layerWiseHessian_twoNorm_per_epoch'] = False
            
        if 'if_record_inverseLayerWiseHessian_twoNorm_per_epoch' in args:
            params['if_records']['if_record_inverseLayerWiseHessian_twoNorm_per_epoch'] =\
            args['if_record_inverseLayerWiseHessian_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_inverseLayerWiseHessian_twoNorm_per_epoch'] = False
            
        if 'if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch' in args:
            params['if_records']['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch'] =\
            args['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch'] = False
            
        if 'if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch' in args:
            params['if_records']['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch'] =\
            args['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch']
        else:
            params['if_records']['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch'] = False
            
        if 'if_record_kfac_F_twoNorm_per_epoch' in args:
            params['if_record_kfac_F_twoNorm_per_epoch'] =\
            args['if_record_kfac_F_twoNorm_per_epoch']
        if 'if_record_kron_bfgs_norm_s_y_per_iter' in args:
            params['if_record_kron_bfgs_norm_s_y_per_iter'] =\
            args['if_record_kron_bfgs_norm_s_y_per_iter']
        if 'if_record_kron_bfgs_sTy_per_iter' in args:
            params['if_record_kron_bfgs_sTy_per_iter'] =\
            args['if_record_kron_bfgs_sTy_per_iter']
        if 'if_record_kron_bfgs_damping_status' in args:
            params['if_record_kron_bfgs_damping_status'] =\
            args['if_record_kron_bfgs_damping_status']
        if 'if_record_kron_bfgs_check_damping' in args:
            params['if_record_kron_bfgs_check_damping'] =\
            args['if_record_kron_bfgs_check_damping']
            
            
            
    
    
    
    
    
    
    
    
    

    
    params['if_max_epoch'] = args['if_max_epoch']
    params['max_epoch/time'] = args['max_epoch/time']

    if_max_epoch = args['if_max_epoch'] # 0 means max_time
    if if_max_epoch:
        max_epoch = args['max_epoch/time']
    else:
        max_time = args['max_epoch/time']
        
    

    record_epoch = args['record_epoch']
    
    
    name_dataset = args['dataset']
    params['name_dataset'] = name_dataset

    params['name_loss'] = args['name_loss']
    

    params['momentum_gradient_rho'] = args['momentum_gradient_rho']
    params['momentum_gradient_dampening'] = args['momentum_gradient_dampening']



    

    # Model
    model = get_model(params)

    

    params['name_model'] = model.name_model
    params['layersizes'] = model.layersizes

    print('name_loss:')
    print(model.name_loss)

    

    print('Model created.')


    params['layers_params'] = model.layers_params
    
    params['N1'] = args['N1']
    params['N2'] = args['N2']


    data_ = {}


    if name_dataset in ['CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                        'CIFAR-10-onTheFly-vgg16-NoLinear-no-regularization',
                        'CIFAR-10-onTheFly-vgg16-NoLinear-BN-no-regularization',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                        'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                        'CIFAR-10-onTheFly-ResNet32-BN',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                        'CIFAR-10-AllCNNC',
                        'CIFAR-10-N1-128-AllCNNC',
                        'CIFAR-10-N1-512-AllCNNC',
                        'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                        'CIFAR-100-onTheFly-ResNet34-BN',
                        'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                        'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                        'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                        'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                        'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-no-regularization',
                        'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                        'CIFAR-100-onTheFly-AllCNNC']:
        params['if_dataset_onTheFly'] = True
    elif name_dataset in ['Fashion-MNIST-N1-60-no-regularization',
                          'Fashion-MNIST-N1-256-no-regularization',
                          'CIFAR-100',
                          'CIFAR-100-NoAugmentation',
                          'CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-NoAugmentation-vgg11',
                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                          'DownScaledMNIST-N1-1000-no-regularization',
                          'MNIST-autoencoder-relu-N1-1000-sum-loss',
                          'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization',
                          'MNIST-autoencoder-relu-N1-100-sum-loss',
                          'CURVES-autoencoder-relu-sum-loss',
                          'CURVES-autoencoder-relu-sum-loss-no-regularization',
                          'CURVES-autoencoder-relu-N1-100-sum-loss',
                          'FacesMartens-autoencoder-relu',
                          'FacesMartens-autoencoder-relu-no-regularization',
                          'FacesMartens-autoencoder-relu-N1-100',]:
        params['if_dataset_onTheFly'] = False
    else:
        print('error: need to check if on the fly for ' + name_dataset)
        sys.exit()

    from utils_git.utils_data import read_data_sets

    
    
    if not params['if_dataset_onTheFly']:
        
        dataset = read_data_sets(name_dataset, params['name_model'], params['home_path'], one_hot=False)
    
#     sys.exit()

#         dataset = read_data_sets(name_dataset, params['name_model'], params['home_path'], one_hot=False)
    
    
    



        
        
        
        
        X_train = dataset.train.images
        t_train = dataset.train.labels

        print('For X_train:')
        get_statistics(X_train)

        

        print('X_train.shape')
        print(X_train.shape)
        print('t_train.shape')
        print(t_train.shape)

        




        params['num_train_data'] = len(t_train)
        
        X_test = dataset.test.images
        t_test = dataset.test.labels

#     print('For X_test:')
#     get_statistics(X_test)

#     print('X_test.shape')
#     print(X_test.shape)
#     print('t_test.shape')
#     print(t_test.shape)
    
    
    if params['if_dataset_onTheFly']:
        
#         print('dataset')
#         print(dataset)
#         sys.exit()
        
        from utils_git.utils_data import read_data_sets_v2
        
#         dataset = read_data_sets_v2(name_dataset, dataset, params)
        dataset = read_data_sets_v2(name_dataset, params)
    


        params['num_train_data'] = dataset.num_train_data
    
    
        
        

    
    
    
    data_['dataset'] = dataset
    
    

    params['alpha'] = args['alpha']
    params['alpha_current'] = params['alpha']
    
    
    params['numlayers'] = model.numlayers






    

    data_['model'] = model

    params = get_params(params, args)
    
    params['tau'] = args['tau']
    

    
    data_, params = train_initialization(data_, params, args)
    
#     print('N1 in params')
#     print('N1' in params)
#     sys.exit()
    
    
#     if params['algorithm'] in ['Kron-BFGS',
#                                'Kron-BFGS-no-norm-gate',
#                                'Kron-BFGS-Hessian-action',
#                                'Kron-BFGS-wrong',
#                                'Kron-BFGS-wrong-Hessian-action',
#                                'Kron-BFGS-no-norm-gate-damping',
#                                'Kron-BFGS-no-norm-gate-Shiqian-damping',
#                                'Kron-BFGS-no-norm-gate-momentum-s-y-damping']:
#         params['if_record_kron_bfgs_matrix_norm'] = True

    if params['if_momentum_gradient']:
#         data_['model_regularized_grad_momentum'] = get_zero_torch(params)
        
        data_['model_grad_momentum'] = get_zero_torch(params)
    
    if params['if_Adam']:
        
        print('error: should not reach here')
        sys.exit()

        params['Adam_beta_1'] = 0.9
        params['Adam_beta_2'] = 0.999
        params['Adam_epsilon'] = 10**(-8)

        data_['model_grad_Adam_momentum_1'] = get_zero(params)
        
        data_['model_grad_Adam_momentum_2'] = get_zero(params)

    

    

    

    

    if params['if_yura']:
        params['yura_lambda_0'] = 1

    if params['if_momentum_p']:
        # data_['p_momentum'] = get_zero(params)
        data_['p_momentum_torch'] = get_zero_torch(params)

    if params['if_VA_p'] or\
    params['if_signVAsqrt'] or\
    params['if_signVA'] or\
    params['if_signVAerf']:
        data_['p_momentum_1_torch'] = get_zero_torch(params)
        data_['p_momentum_2_torch'] = get_zero_torch(params)

    if params['if_LM']:
            
        boost = 1.01
        drop = 1 / 1.01
        params['boost'] = boost
        params['drop'] = drop




    # Visualization stuffs
    # len_record = int(max_epoch / record_epoch)

    epochs = [0]
    timesCPU = [0]
    timesWallClock = [0]
    
    if not params['if_dataset_onTheFly']:
        train_losses = []
        train_unregularized_losses = []
        train_acces = []
    
    train_unregularized_minibatch_losses = []
    train_minibatch_acces = []

    test_acces = []
    test_losses = []
    reduction = 'mean'
    
    
    
    if params['if_dataset_onTheFly']:
            test_loss_0, _, test_acc_0 = get_regularized_loss_and_acc_from_x_whole_dataset_with_generator(
                model, dataset.test_generator, reduction, params
            )
    else:
        test_loss_0, _, test_acc_0 = get_regularized_loss_and_acc_from_x_whole_dataset(
            model, X_test, t_test, reduction, params
        )
    
#     print('test_loss_0, test_acc_0')
#     print(test_loss_0, test_acc_0)
    
#     sys.exit()
    
    test_losses.append(test_loss_0)
    test_acces.append(test_acc_0)
    
    
    print('test_loss_0, test_acc_0')
    print(test_loss_0, test_acc_0)

    if params['if_LM']:
        lambdas = []
        lambdas.append(params['lambda_'])
    if params['if_yura']:
        yura_lambdas = []
        yura_lambdas.append(params['yura_lambda_0'])
    
    if params['if_test_mode']:
        if params['if_record_sgd_norm']:
            sgd_norms = []
        if params['if_record_p_norm']:
            p_norms = []
        if params['if_record_kfac_p_norm']:
            kfac_p_norms = []
            data_['kfac_p_norms'] = kfac_p_norms
        if params['if_record_kfac_p_cosine']:
            kfac_p_cosines = []
            data_['kfac_p_cosines'] = kfac_p_cosines
#         if params['if_record_kron_bfgs_cosine']:
#             kron_bfgs_cosines = []
#             data_['kron_bfgs_cosines'] = kron_bfgs_cosines
        if params['if_record_res_grad_norm']:
            res_grad_norms = []
            data_['res_grad_norms'] = res_grad_norms
        if params['if_record_res_grad_random_norm']:
            res_grad_random_norms = []
            data_['res_grad_random_norms'] = res_grad_random_norms
        if params['if_record_res_grad_grad_norm']:
            res_grad_grad_norms = []
            data_['res_grad_grad_norms'] = res_grad_grad_norms
        if params['if_record_res_grad_norm_per_iter']:
            res_grad_norms_per_iter = []
            data_['res_grad_norms_per_iter'] = res_grad_norms_per_iter
        if 'if_record_kron_bfgs_update_status' in params and\
        params['if_record_kron_bfgs_update_status']:
            data_['kron_bfgs_update_status'] = []
        if 'if_record_kron_bfgs_matrix_norm_per_iter' in params and\
        params['if_record_kron_bfgs_matrix_norm_per_iter']:
            data_['kron_bfgs_matrix_norms_per_iter'] = []
#         if 'if_record_loss_per_iter' in params and\
#             params['if_record_loss_per_iter'] == True:
#             data_['losses_per_iter'] = []
        if 'if_record_kfac_G_inv_norm_per_iter' in params and\
            params['if_record_kfac_G_inv_norm_per_iter'] == True:
            data_['kfac_G_inv_norms_per_iter'] = []
        if 'if_record_kfac_G_inv_norm_per_epoch' in params and\
            params['if_record_kfac_G_inv_norm_per_epoch'] == True:
            data_['kfac_G_inv_norms_per_epoch'] = []

        if params['if_records']['if_record_kfac_G_norm_per_epoch']:
            data_['kfac_G_norms_per_epoch'] = []
        if params['if_records']['if_record_kfac_G_twoNorm_per_epoch']:
            data_['kfac_G_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_kfac_A_twoNorm_per_epoch']:
            data_['kfac_A_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_kron_bfgs_A_twoNorm_per_epoch']:
            data_['kron_bfgs_A_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_kron_bfgs_G_LM_twoNorm_per_epoch']:
            data_['kron_bfgs_G_LM_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_kron_bfgs_Hg_twoNorm_per_epoch']:
            data_['kron_bfgs_Hg_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_kron_bfgs_Ha_twoNorm_per_epoch']:
            data_['kron_bfgs_Ha_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_layerWiseHessian_twoNorm_per_epoch'] == True:
            data_['layerWiseHessian_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_inverseLayerWiseHessian_twoNorm_per_epoch'] == True:
            data_['inverseLayerWiseHessian_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch'] == True:
            data_['inverseLayerWiseHessian_LM_twoNorms_per_epoch'] = []
        if params['if_records']['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch'] == True:
            data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch'] = []
            
        if 'if_record_kfac_F_twoNorm_per_epoch' in params and\
            params['if_record_kfac_F_twoNorm_per_epoch'] == True:
            data_['kfac_F_twoNorms_per_epoch'] = []
        if 'if_record_kron_bfgs_norm_s_y_per_iter' in params and\
            params['if_record_kron_bfgs_norm_s_y_per_iter'] == True:
            data_['kron_bfgs_norms_s_y_per_iter'] = {}
            data_['kron_bfgs_norms_s_y_per_iter']['s'] = []
            data_['kron_bfgs_norms_s_y_per_iter']['y'] = []
        if 'if_record_kron_bfgs_sTy_per_iter' in params and\
            params['if_record_kron_bfgs_sTy_per_iter'] == True:
            data_['kron_bfgs_sTy_per_iter'] = []
        if 'if_record_kron_bfgs_damping_status' in params and\
            params['if_record_kron_bfgs_damping_status'] == True:
            data_['kron_bfgs_damping_statuses'] = {}
        if 'if_record_kron_bfgs_check_damping' in params and\
            params['if_record_kron_bfgs_check_damping'] == True:
            data_['kron_bfgs_check_dampings'] = []
            
        if params['if_records']['if_record_kron_bfgs_matrix_norm'] == True:
            data_['kron_bfgs_matrix_norms'] = []

    if params['if_record_sgn_norm']:
        sgn_norms = []
        data_['model_grad_full'] = get_full_grad(model, X_train, t_train, params)
        
    print('params[if_dataset_onTheFly]')
    print(params['if_dataset_onTheFly'])
    
    if not params['if_dataset_onTheFly']:
        
        reduction = 'mean'
        loss_0, unregularized_loss_0, acc_0 = get_regularized_loss_and_acc_from_x_whole_dataset(model, X_train, t_train, reduction, params)
        

        
        train_losses.append(loss_0)
        train_unregularized_losses.append(unregularized_loss_0)
        train_acces.append(acc_0)
    
        print('loss_0, unregularized_loss_0, acc_0')
        print(loss_0, unregularized_loss_0, acc_0)


    N1 = params['N1']
    
#     print('len(t_train)')
#     print(len(t_train))
    
#     print('params[num_train_data]')
#     print(params['num_train_data'])
    
#     sys.exit()
        

#     iter_per_epoch = int(len(t_train) / N1)
    iter_per_epoch = int(params['num_train_data'] / N1)
    
    params['iter_per_epoch'] = iter_per_epoch

#     iter_per_record = int(np.floor(len(t_train) * record_epoch / N1))
    iter_per_record = int(np.floor(params['num_train_data'] * record_epoch / N1))
    
    
#     if params['if_test_mode']:
#         i = 0
#         while i + N1 <= params['num_train_data']:
#             data_['dataset'].train.next_batch(N1)
#             i += N1

    # Training
    print('Begin training...')
    epoch = -1
    i = -1
    
#     model_test = copy.deepcopy(model)
    
    while not get_if_stop(args, i+1, iter_per_epoch, timesCPU):

        i += 1
        params['i'] = i
        
#         print('i')
#         print(i)
        
#         if params['if_test_mode']:
        
#             torch.save(
#                 {'model_state_dict': data_['model'].state_dict()},
#                 'saved_model/saved_model_start_' + str(params['i'])
#             )
            
#             print('save model (start) to disk')
        
#         if params['if_test_mode']:
        
#             checkpoint = torch.load(
#                 'saved_model/saved_model_start_' + str(params['i']),
#                 map_location=torch.device(device)
#             )
    
#             data_['model'].load_state_dict(checkpoint['model_state_dict'])
        
#             print('load model (start) from disk')

        if i % iter_per_record == 0:
            start_time_wall_clock = time.time()
            start_time_cpu = time.process_time()
            epoch += 1
            params['epoch'] = epoch

        # get minibatch
        X_mb, t_mb = dataset.train.next_batch(N1)
        
#         X_mb = torch.from_numpy(X_mb).to(device)
        
        if not params['if_dataset_onTheFly']:
            X_mb = torch.from_numpy(X_mb)
        X_mb = X_mb.to(device)
        

        

        
#         t_mb = torch.from_numpy(t_mb).to(device)
        
        if not params['if_dataset_onTheFly']:
            t_mb = torch.from_numpy(t_mb)
        t_mb = t_mb.to(device)

    
    
        

        # Forward
        z, a, h = model.forward(X_mb)
        
        reduction = 'mean'
#         loss = get_regularized_loss_from_z(
#             model, z, t_mb, reduction, params['tau'])
        loss = get_loss_from_z(
            model, z, t_mb, reduction)
    
#         print('loss.item()')
#         print(loss.item())
    
        params['unregularized_minibatch_loss_i_no_MA'] = loss.item()
        
        if i == 0:
            
            unregularized_minibatch_loss_i = loss.item()
        
            train_unregularized_minibatch_losses.append(
            unregularized_minibatch_loss_i)
            
            minibatch_acc_i = get_acc_from_z(model, params, z, t_mb)
            
            train_minibatch_acces.append(minibatch_acc_i)
            
        else:
            
            minibatch_acc_i =\
            0.9 * minibatch_acc_i + 0.1 * get_acc_from_z(model, params, z, t_mb)
            
            unregularized_minibatch_loss_i =\
            0.9 * unregularized_minibatch_loss_i + 0.1 * loss.item()
    
#         if params['if_test_mode']:
#             if 'if_record_loss_per_iter' in params and\
#                 params['if_record_loss_per_iter'] == True:
                
#                 tau = params['tau']
                
#                 if tau == 0:
#                     data_['losses_per_iter'].append(loss.item())
#                 else:

#                     data_['losses_per_iter'].append(
#                         loss.item() +\
#                     0.5 * tau *\
#     get_dot_product_torch(model.layers_weight, model.layers_weight).item()
#                     )


        # backward and gradient
        model.zero_grad()
        
        if params['if_second_order_algorithm'] and params['matrix_name'] == 'Fisher-correct':
            loss.backward(retain_graph=True)
        else:
            loss.backward()
        
        
        model_grad_torch = get_model_grad(model, params)
        
        data_['model_grad_torch_unregularized'] = model_grad_torch
        
        model_grad_torch =\
        from_unregularized_grad_to_regularized_grad(model_grad_torch, data_, params)
        
        
        if torch.sum(z != z):
            print('Error: nan in z')
            # print('torch.sum(X_mb != X_mb)')
            # print(torch.sum(X_mb != X_mb))
            print('i')
            print(i)
            print('get_if_nan(model.layers_weight)')
            print(get_if_nan(model.layers_weight))
            for l in range(len(model.layers_weight)):
                for key in model.layers_weight[l]:
                    print('torch.max(model.layers_weight[l][key])')
                    print(torch.max(model.layers_weight[l][key]))
                    print('torch.min(model.layers_weight[l][key])')
                    print(torch.min(model.layers_weight[l][key]))
            for l in range(len(a)):
                print('torch.max(a[l])')
                print(torch.max(a[l]))
                print('torch.min(a[l])')
                print(torch.min(a[l]))
#             for l in range(len(h)):
                
#                 print('h[l]')
#                 print(h[l])
                
#                 print('torch.max(h[l])')
#                 print(torch.max(h[l]))
#                 print('torch.min(h[l])')
#                 print(torch.min(h[l]))
            break
        

        data_['model_grad_torch'] = model_grad_torch # regularized
        
#         print('torch.norm(X_mb)')
#         print(torch.norm(X_mb))
        
#         print('data_[model_grad_torch][-1][b][:10] usual backprop')
#         print(data_['model_grad_torch'][-1]['b'][:10])
        
#         print('get_dot_product_torch(data_[model_grad_torch], data_[model_grad_torch])')
#         print(get_dot_product_torch(data_['model_grad_torch'], data_['model_grad_torch']))
        
        
        # add regularization
#         model_grad_torch = get_plus_scalar(params['tau'], model_grad_torch)
        
        

        if get_if_nan(model_grad_torch):
            print('Error: nan in model_grad_torch')
            for l in range(len(model_grad_torch)):
                for key in model_grad_torch[l]:
                    print('torch.max(model_grad_torch[l][key])')
                    print(torch.max(model_grad_torch[l][key]))
                    print('torch.min(model_grad_torch[l][key])')
                    print(torch.min(model_grad_torch[l][key]))
            for l in range(len(model.layers_weight)):
                for key in model.layers_weight[l]:
                    print('torch.max(model.layers_weight[l][key])')
                    print(torch.max(model.layers_weight[l][key]))
                    print('torch.min(model.layers_weight[l][key])')
                    print(torch.min(model.layers_weight[l][key]))

            break




        
        

        
        if params['if_test_mode']:
            if params['if_record_sgd_norm']:
                sgd_norms.append(
                    np.sqrt(get_dot_product_torch(model_regularized_grad_torch, model_regularized_grad_torch).item()))
        
        if params['if_record_sgn_norm']:
            sgn_ = get_subtract_torch(model_regularized_grad_torch, data_['model_grad_full'])
            sgn_norms.append(np.sqrt(get_dot_product_torch(sgn_, sgn_).item()))
            
        if params['if_momentum_gradient']:
            # rho = min(1-1/(i+1), 0.9)
            rho = params['momentum_gradient_rho']
            
            # mimic torch.optim.SGD
#             dampening = rho
            dampening = params['momentum_gradient_dampening']
            
            data_['model_grad_momentum'] =\
            get_plus_torch(
                get_multiply_scalar(rho, data_['model_grad_momentum']),
                get_multiply_scalar(1 - dampening, model_grad_torch)) # regularized

        if params['if_momentum_gradient']:
            data_['model_grad_used_torch'] =\
            data_['model_grad_momentum']
        else:
        
            data_['model_grad_used_torch'] = model_grad_torch
            
#         if params['if_test_mode']:
            
#             assert params['if_momentum_gradient']

#             if not os.path.exists('saved_model/' + params['algorithm']):
#                 os.makedirs('saved_model/' + params['algorithm'])
            
#             torch.save(
#                 {'model_grad_momentum': data_['model_grad_momentum']},
#                 'saved_model/' + params['algorithm'] + '/saved_grad_' + str(params['i'])
#             )
        
#             print('save grad to disk')


        # get second order caches
        if params['if_second_order_algorithm']:
            data_['X_mb'] = X_mb
            data_['t_mb'] = t_mb
            
#             if i % 10 == 0:
            
            data_ = get_second_order_caches(z, a, h, data_, params)
        

        

        model = data_['model']

        if params['if_LM']:
            data_['regularized_loss'] = loss
            data_['t_mb_N1'] = t_mb
            lambda_minus_tau = params['lambda_']
            params['lambda_'] = params['lambda_'] + params['tau']
            
#         print('params[if_lr_decay]')
#         print(params['if_lr_decay'])
        
#         sys.exit()
        
        if params['if_lr_decay']:
            params['alpha_current'] =\
            params['alpha'] *\
            (params['lr_decay_rate'] ** (params['epoch'] // params['num_epoch_to_decay']))

        algorithm = params['algorithm']

        if algorithm in ['ekfac-EF-VA',
                         'ekfac-EF',
                         'kfac-TR',
                         'kfac-momentum-grad-TR',
                         'kfac-CG',
                         'kfac-momentum-grad-CG',
                         'kfac',
                         'kfac-no-max',
                         'kfac-NoMaxNoSqrt',
                         'kfac-NoMaxNoSqrt-no-LM',
                         'kfac-no-max-no-LM',
                         'kfac-warmStart-no-max-no-LM',
                         'kfac-warmStart-lessInverse-no-max-no-LM',
                         'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                         'kfac-correctFisher-warmStart-no-max-no-LM',
                         'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                         'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                         'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                         'kfac-no-max-epsilon-A-G-no-LM',
                         'kfac-EF',
                         'Fisher-block']:    
            data_, params = kfac_update(data_, params)
            
            if params['kfac_svd_failed']:
                
                print('i')
                print(i)
                
                print('error: kfac_svd_failed')
                
                break

        elif algorithm == 'SMW-Fisher-signVAsqrt-p' or\
        algorithm == 'SMW-Fisher-VA-p' or\
        algorithm == 'SMW-Fisher-momentum-p-sign' or\
        algorithm == 'SMW-Fisher-momentum-p' or\
        algorithm == 'SMW-Fisher-sign' or\
        algorithm == 'SMW-Fisher-different-minibatch' or\
        algorithm == 'SMW-Fisher' or\
        algorithm == 'SMW-Fisher-momentum' or\
        algorithm == 'SMW-Fisher-D_t-momentum' or\
        algorithm == 'SMW-Fisher-momentum-D_t-momentum':

            data_, params = SMW_Fisher_update(data_, params)
        elif algorithm in ['shampoo',
                           'shampoo-allVariables',
                           'shampoo-allVariables-warmStart',
                           'shampoo-allVariables-warmStart-lessInverse',
                           'shampoo-allVariables-filterFlattening-warmStart',
                           'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                           'shampoo-no-sqrt',
                           'shampoo-no-sqrt-Fisher',
                           'matrix-normal',
                           'matrix-normal-allVariables',
                           'matrix-normal-allVariables-warmStart',
                           'matrix-normal-allVariables-warmStart-MaxEigDamping',
                           'matrix-normal-allVariables-warmStart-noPerDimDamping',
                           'matrix-normal-same-trace',
                           'matrix-normal-same-trace-warmStart',
                           'matrix-normal-same-trace-warmStart-noPerDimDamping',
                           'matrix-normal-same-trace-allVariables',
                           'matrix-normal-same-trace-allVariables-warmStart',
                           'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                           'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                           'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                           'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                           'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                           'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                           'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                           'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                           'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
            data_, params = shampoo_update(data_, params)
        elif algorithm in ['Fisher-BD']:
            data_, params = Fisher_BD_update(data_, params)
        elif algorithm in ['SMW-Fisher-batch-grad-momentum-exponential-decay',
                           'SMW-Fisher-batch-grad',
                           'SMW-Fisher-batch-grad-momentum']:
            data_, params = SMW_Fisher_batch_grad_update(data_, params)
        elif algorithm in ['SGD-VA',
                           'SGD-signVAsqrt',
                           'SGD-signVAerf',
                           'SGD-signVA',
                           'SGD-yura-BD',
                           'SGD-yura-old',
                           'SGD-yura',
                           'SGD-yura-MA',
                           'SGD-sign',
                           'SGD-momentum-yura',
                           'SGD-momentum',
                           'SGD',]:
            data_ = SGD_update(data_, params)
        elif algorithm in ['RMSprop',
                           'RMSprop-warmStart',
                           'RMSprop-test',
                           'Adam',
                           'Adam-test',
                           'Adam-noWarmStart',
                           'RMSprop-no-sqrt',
                           'RMSprop-individual-grad',
                           'RMSprop-individual-grad-no-sqrt',
                           'RMSprop-individual-grad-no-sqrt-Fisher',
                           'RMSprop-individual-grad-no-sqrt-LM']:
            data_ = RMSprop_update(data_, params)
        elif algorithm == 'SMW-GN':
            data_ = SMW_GN_update(data_, params)
        elif algorithm == 'GI-Fisher':
            data_, params = GI_Fisher_update(data_, params)
        elif algorithm == 'SMW-Fisher-BD':
            data_, params = SMW_Fisher_BD_update(data_, params)
        elif algorithm in kbfgs_list_algorithm:

            # abandoned
            assert algorithm not in ['Kron-BFGS-block']
            
            data_, params = Kron_BFGS_update_v2(data_, params)

            if params['nan_detected']:
                break

        elif algorithm == 'Kron-SGD':
            data_, params = Kron_SGD_update(data_, params)
        elif algorithm in ['BFGS',
                           'BFGS-homo']:
            data_, params = BFGS_update(data_, params)
        else:
            print('Error: updating direction not defined for ' + algorithm)
            sys.exit()

        if params['if_LM']:
            params['lambda_'] = lambda_minus_tau


        p_torch = data_['p_torch']
        
#         if params['if_test_mode']:

#             if not os.path.exists('saved_model/' + params['algorithm']):
#                 os.makedirs('saved_model/' + params['algorithm'])
            
#             torch.save(
#                 {'p_torch': p_torch},
#                 'saved_model/' + params['algorithm'] + '/saved_p_' + str(params['i'])
#             )
        
#             print('save p to disk')
        

        if params['if_test_mode']:
            if params['if_record_p_norm']:
                p_norms.append(
                    np.sqrt(get_dot_product_torch(p_torch, p_torch).item()))

        if params['if_LM']:

            lambda_ = update_lambda(p_torch, data_, params)
            params['lambda_'] = lambda_

        

        if params['if_momentum_p']:
            rho_momentum_p = 0.9
            data_['p_momentum_torch'] = get_plus_torch(\
                                           get_multiply_scalar(rho_momentum_p, data_['p_momentum_torch']),
                                           get_multiply_scalar(1 - rho_momentum_p, p_torch))
            # p = data_['p_momentum']
            p_torch = data_['p_momentum_torch']

        if params['if_VA_p']:
            rho_momentum_p = 0.9
            data_['p_momentum_1'] = get_plus(\
                                           get_multiply_scalar(rho_momentum_p, data_['p_momentum_1']),
                                           get_multiply_scalar(1 - rho_momentum_p, p))
            data_['p_momentum_2'] = get_plus(\
                                           get_multiply_scalar(rho_momentum_p, data_['p_momentum_2']),
                                           get_multiply_scalar(1 - rho_momentum_p, get_square(p)))
            p = get_divide(\
                           get_multiply(data_['p_momentum_1'], get_square(data_['p_momentum_1'])),
                           get_plus_scalar(10**(-8), data_['p_momentum_2']))
            
        if params['if_signVAsqrt'] or\
        params['if_signVA'] or\
        params['if_signVAerf']:
            rho_momentum_p = 0.9
            data_['p_momentum_1_torch'] = get_plus_torch(\
                                           get_multiply_scalar(rho_momentum_p, data_['p_momentum_1_torch']),
                                           get_multiply_scalar(1 - rho_momentum_p, p_torch))
            data_['p_momentum_2_torch'] = get_plus_torch(\
                                           get_multiply_scalar(rho_momentum_p, data_['p_momentum_2_torch']),
                                           get_multiply_scalar(1 - rho_momentum_p, get_square_torch(p_torch)))
            if params['if_signVAsqrt']:
                p_torch = get_divide_torch(\
                            data_['p_momentum_1_torch'],
                            get_sqrt_torch(get_plus_scalar(10**(-8), data_['p_momentum_2_torch'])))
            elif params['if_signVA']:
                p_torch = get_divide(\
                           get_multiply(get_sign(data_['p_momentum_1']), get_square(data_['p_momentum_1'])),
                           get_plus_scalar(10**(-8), data_['p_momentum_2']))
            elif params['if_signVAerf']:
                if params['i'] > 100:
                    relative_variance = get_plus_scalar(
                        -1,
                        get_divide_torch(
                            data_['p_momentum_2_torch'],
                            get_plus_scalar(10**(-8), get_square_torch(data_['p_momentum_1_torch']))))
                    

                    
                    # print('data_[p_momentum_2_torch][-1][W]')
                    # print(data_['p_momentum_2_torch'][-1]['W'])

                    p_torch = get_multiply_torch(
                        get_sign_torch(data_['p_momentum_1_torch']),
                        get_erf(
                            get_reciprocal(
                                get_multiply_scalar(
                                    np.sqrt(2),
                                    get_sqrt_torch(get_max_with_0(relative_variance)))))
                    )
                else:
                    p_torch = data_['p_momentum_1_torch']

                
            
        

        if params['if_sign']:
            p = get_sign(p)
            

        if params['if_Adam']:
            
            print('error: deprecated, must be false')
            sys.exit()

            p, data_ = get_Adam_direction(p, data_, params)

        if params['if_yura']:
            p_torch = get_yura(p_torch, data_, params)
            
#         print('get_dot_product_torch(data_[model_grad_torch_unregularized], data_[model_grad_torch_unregularized])')
#         print(get_dot_product_torch(data_['model_grad_torch_unregularized'], data_['model_grad_torch_unregularized']))
            
#         print('get_dot_product_torch(p_torch, p_torch)')
#         print(get_dot_product_torch(p_torch, p_torch))

        model = update_parameter(p_torch, model, params)
        
#         l_debug = 1
        
#         print('p_torch[l_debug][W]')
#         print(p_torch[l_debug]['W'])
        
#         print('model.layers_weight[l_debug][W]')
#         print(model.layers_weight[l_debug]['W'])
        
#         l_debug = 3
#         print('model.layers_weight[l_debug][W]')
#         print(model.layers_weight[l_debug]['W'])
        
#         if params['if_test_mode']:
        
#             torch.save(
#                 {'model_state_dict': data_['model'].state_dict()},
#                 'saved_model/saved_model_end_' + str(params['i'])
#             )
            
#             print('save model (end) to disk')
        
#         if params['if_test_mode']:
        
#             checkpoint = torch.load(
#                 'saved_model/saved_model_end_' + str(params['i']),
#                 map_location=torch.device(device)
#             )
    
#             data_['model'].load_state_dict(checkpoint['model_state_dict'])
        
#             print('load model (end) from disk')

        if get_if_nan(model.layers_weight):
        
            print('get_if_nan(p_torch)')
            print(get_if_nan(p_torch))
            
            for l in range(len(p_torch)):
                for key in p_torch[l]:
                    
                    print('key')
                    print(key)
                    
                    print('torch.sum(p_torch[l][key] != p_torch[l][key])')
                    print(torch.sum(p_torch[l][key] != p_torch[l][key]))
                    
                    
                    
                    if torch.sum(p_torch[l][key] != p_torch[l][key]):
                        
                        print('p_torch[l][key].size()')
                        print(p_torch[l][key].size())
                        
                        print('p_torch[l][key]')
                        print(p_torch[l][key])
        
            print('Error: nan in model.layers_weight')
            break

    #     print('time 7/8: ', time.time() - start_time)
    
#         print('(i+1) % iter_per_record')
#         print((i+1) % iter_per_record)
#         print('iter_per_record')
#         print(iter_per_record)

        if (i+1) % iter_per_record == 0:
        
            if params['if_test_mode']:
                if 'if_record_kfac_G_inv_norm_per_epoch' in params and\
                params['if_record_kfac_G_inv_norm_per_epoch']:
                    
                    data_['kfac_G_inv_norms_per_epoch'].append(
                        [torch.norm(G_inv_l).item() for G_inv_l in data_['G_inv']]
                    )
                    # torch.norm() is Fro-norm here
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kfac_G_norm_per_epoch']:
                    
                    # data_['G'] is without LM
                    data_['kfac_G_norms_per_epoch'].append(
                        [torch.norm(G_l).item() for G_l in data_['G']]
                    )
                    
#                     print('[print(torch.norm(G_l).item()) for G_l in data_[G]]')
#                     [print(torch.norm(G_l).item()) for G_l in data_['G']]
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kfac_G_twoNorm_per_epoch']:
                    
                    # data_['G'] is without LM
                    data_['kfac_G_twoNorms_per_epoch'].append(
                        [np.linalg.norm(G_l.cpu().data.numpy(), ord=2) for G_l in data_['G']]
                    )
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kfac_A_twoNorm_per_epoch']:
                    
                    # data_['A'] is without LM
                    data_['kfac_A_twoNorms_per_epoch'].append(
                        [np.linalg.norm(A_l.cpu().data.numpy(), ord=2) for A_l in data_['A']]
                    )
                    
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kron_bfgs_A_twoNorm_per_epoch']:
                   
                    
                    # without LM
                    data_['kron_bfgs_A_twoNorms_per_epoch'].append(
                        [
                            np.linalg.norm(
                                Kron_BFGS_matrices_l['A'].cpu().data.numpy(), ord=2
                            ) for Kron_BFGS_matrices_l in data_['Kron_BFGS_matrices']
                        ]
                    )
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kron_bfgs_G_LM_twoNorm_per_epoch']:
                    
                    if params['algorithm'] == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping':
                        data_['kron_bfgs_G_LM_twoNorms_per_epoch'].append([])
                        for l in range(len(data_['Kron_LBFGS_s_y_pairs']['a_grad'])):
                            size_identity_v =\
                            params['layers_params'][l]['output_size']
 
                            identity_v = torch.eye(size_identity_v).cuda()
                    
                            data_['kron_bfgs_G_LM_twoNorms_per_epoch'][-1].append(
                                np.linalg.norm(
                                    LBFGS_Hv(identity_v, data_['Kron_LBFGS_s_y_pairs']['a_grad'][l], params, False).inverse().cpu().data.numpy(),
                                    ord=2
                                )
                            )
                    else:
                        print('error: not implemented')
                        sys.exit()
                    
                        data_['kron_bfgs_G_LM_twoNorms_per_epoch'].append(
                        [
                                 np.linalg.norm(
                                Kron_BFGS_matrices_l['H']['a_grad'].inverse().cpu().data.numpy(), ord=2
                            )
                            for Kron_BFGS_matrices_l in data_['Kron_BFGS_matrices']
                        ]
                    )
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kron_bfgs_Hg_twoNorm_per_epoch']:
                    
                    if params['algorithm'] == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping':
                        data_['kron_bfgs_Hg_twoNorms_per_epoch'].append([])
                        for l in range(len(data_['Kron_LBFGS_s_y_pairs']['a_grad'])):
                            size_identity_v =\
                            params['layers_params'][l]['output_size']
 
                            identity_v = torch.eye(size_identity_v).cuda()
                    
                            data_['kron_bfgs_Hg_twoNorms_per_epoch'][-1].append(
                                np.linalg.norm(
                                    LBFGS_Hv(identity_v, data_['Kron_LBFGS_s_y_pairs']['a_grad'][l], params, False).cpu().data.numpy(),
                                    ord=2
                                )
                            )
#                             data_['kron_bfgs_Hg_twoNorms_per_epoch'][-1].append(
#                                 np.linalg.norm(
#                                     LBFGS_Hv(identity_v, data_['Kron_LBFGS_s_y_pairs']['a'][l], params, False).cpu().data.numpy(),
#                                     ord=2
#                                 )
#                             )
                    else:
                        print('error: not implemented')
                        sys.exit()
                    
                        data_['kron_bfgs_Hg_twoNorms_per_epoch'].append(
                        [
                                 np.linalg.norm(Kron_BFGS_matrices_l['H']['a_grad'].cpu().data.numpy(), ord=2)
                            for Kron_BFGS_matrices_l in data_['Kron_BFGS_matrices']
                        ]
                    )
                    
            if params['if_test_mode']:
                if params['if_records']['if_record_kron_bfgs_Ha_twoNorm_per_epoch']:
                    
#                     print('data_[Kron_BFGS_matrices][0][H].keys()')
#                     print(data_['Kron_BFGS_matrices'][0]['H'].keys())
                    
#                     sys.exit()
                    
                    data_['kron_bfgs_Ha_twoNorms_per_epoch'].append(
                        [np.linalg.norm(Kron_BFGS_matrices_l['H']['h'].cpu().data.numpy(), ord=2) for Kron_BFGS_matrices_l in data_['Kron_BFGS_matrices']]
                    )
                    
                    
            if params['if_test_mode']:
                if 'if_record_kfac_F_twoNorm_per_epoch' in params and\
                params['if_record_kfac_F_twoNorm_per_epoch']:
                    
                    # without LM
                    # see https://math.stackexchange.com/questions/2342156/matrix-norm-of-kronecker-product
                    data_['kfac_F_twoNorms_per_epoch'].append(
                         [np.linalg.norm(A_l.cpu().data.numpy(), ord=2) * np.linalg.norm(G_l.cpu().data.numpy(), ord=2) for A_l, G_l in zip(data_['A'], data_['G'])]
                    )
#                     data_['kfac_F_twoNorms_per_epoch'].append(
#                          [torch.norm(A_l, p=2).item() * torch.norm(G_l, p=2).item()for A_l, G_l in zip(data_['A'], data_['G'])]
#                     )
            
    
            if params['if_test_mode'] and params['if_records']['if_record_kron_bfgs_matrix_norm'] == True:
                
                kron_bfgs_matrix_norms_i = []
                
                for l in range(len(data_['Kron_BFGS_matrices'])):
                    kron_bfgs_matrix_norms_i_l = {}
                    
#                     kron_bfgs_matrix_norms_i_l['a_grad'] =\
#                     torch.norm(data_['Kron_BFGS_matrices'][l]['H']['a_grad']).item()
                    
                    kron_bfgs_matrix_norms_i_l['a_grad'] = {}
                    kron_bfgs_matrix_norms_i_l['a_grad']['fro'] =\
                    torch.norm(data_['Kron_BFGS_matrices'][l]['H']['a_grad'], p='fro').item()
                    kron_bfgs_matrix_norms_i_l['a_grad']['2'] =\
                    np.linalg.norm(data_['Kron_BFGS_matrices'][l]['H']['a_grad'].cpu().numpy(), ord=2)
                    
                    try:
                        kron_bfgs_matrix_norms_i_l['a_grad']['max_eig'] =\
                    torch.symeig(data_['Kron_BFGS_matrices'][l]['H']['a_grad'])[0][-1].item()
                    except:
                        kron_bfgs_matrix_norms_i_l['a_grad']['max_eig'] = float('nan')
                    
                    kron_bfgs_matrix_norms_i.append(kron_bfgs_matrix_norms_i_l)
                    
                data_['kron_bfgs_matrix_norms'].append(kron_bfgs_matrix_norms_i)

    
            if params['if_test_mode'] and\
        (params['if_records']['if_record_layerWiseHessian_twoNorm_per_epoch'] or\
                params['if_records']['if_record_inverseLayerWiseHessian_twoNorm_per_epoch'] or\
        params['if_records']['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch'] or\
        params['if_records']['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch']):
            
#                 print('len(data_[inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch])')
#                 print(len(data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch']))
                    
#                 sys.exit()

                from utils_git.utils_hessian import compute_hessian

                true_layer_wise_hessian = compute_hessian(X_mb, t_mb, data_, params)
                
                if params['if_records']['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch']:
                    
                    print('torch.norm(X_mb)')
                    print(torch.norm(X_mb))
                    
                    if len(data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch']) == 0:
                        true_layer_wise_hessian_MA = true_layer_wise_hessian
                    else:
#                         print('len(true_layer_wise_hessian_MA)')
#                         print(len(true_layer_wise_hessian_MA))
                        
#                         print('len(true_layer_wise_hessian)')
#                         print(len(true_layer_wise_hessian))
                        
                        assert len(true_layer_wise_hessian_MA) == len(true_layer_wise_hessian)
                        
                        for l in range(len(true_layer_wise_hessian_MA)):
                            true_layer_wise_hessian_MA[l] =\
                        0.9 * true_layer_wise_hessian_MA[l] +\
                        0.1 * true_layer_wise_hessian[l]
                    
                    
                    
#                         sys.exit()
                        
                    data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch'].append([])
                        
                    for B_l in true_layer_wise_hessian_MA:
                        
                        lambda_hessian_LM =\
                        params['Kron_BFGS_A_LM_epsilon'] * params['Kron_BFGS_H_epsilon']
                        
                        B_l_LM = B_l + lambda_hessian_LM * np.eye(B_l.shape[0])
                        
                        data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch'][-1].append(
                        np.linalg.norm(np.linalg.inv(B_l_LM), ord=2)
                        )
                    
        
                

                if params['if_records']['if_record_layerWiseHessian_twoNorm_per_epoch']:
                    data_['layerWiseHessian_twoNorms_per_epoch'].append(
                    [np.linalg.norm(B_l, ord=2) for B_l in true_layer_wise_hessian]
                )
                
                if params['if_records']['if_record_inverseLayerWiseHessian_twoNorm_per_epoch']:
                    data_['inverseLayerWiseHessian_twoNorms_per_epoch'].append(
                    [np.linalg.norm(np.linalg.inv(B_l), ord=2) for B_l in true_layer_wise_hessian]
                )
                    
                if params['if_records']['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch']:
                    data_['inverseLayerWiseHessian_LM_twoNorms_per_epoch'].append([])
                    
                    for B_l in true_layer_wise_hessian:
                        
                        lambda_hessian_LM =\
                        params['Kron_BFGS_A_LM_epsilon'] * params['Kron_BFGS_H_epsilon']
                        
                        B_l_LM = B_l + lambda_hessian_LM * np.eye(B_l.shape[0])
                        
                        data_['inverseLayerWiseHessian_LM_twoNorms_per_epoch'][-1].append(
                        np.linalg.norm(np.linalg.inv(B_l_LM), ord=2)
                        )
                    
#                     data_['inverseLayerWiseHessian_LM_twoNorms_per_epoch'].append(
#                     [np.linalg.norm(np.linalg.inv(B_l), ord=2) for B_l in true_layer_wise_hessian]
#                 )
                    
            
            

            import datetime
            import pytz
            my_date = datetime.datetime.now(pytz.timezone('US/Eastern'))
            my_date = my_date.strftime("%d/%m/%Y %H:%M:%S")
            print("date and time =", my_date)

            timesCPU_i = time.process_time() - start_time_cpu
            timesWallClock_i = time.time() - start_time_wall_clock
            
            


            
            if not params['if_dataset_onTheFly']:
                
                reduction = 'mean'
                loss_i, unregularized_loss_i, acc_i = get_regularized_loss_and_acc_from_x_whole_dataset(model, X_train, t_train, reduction, params)
            

            
            
            if params['if_dataset_onTheFly']:    
                if math.isnan(unregularized_minibatch_loss_i):
                    print('Warning: unregularized_minibatch_loss_i is NAN.')
                    break
            else:
                
                if math.isnan(loss_i):
                    print('Warning: loss_i is NAN.')
                    break

            timesCPU.append(timesCPU_i)
            timesWallClock.append(timesWallClock_i)
            if epoch > 0:
                timesCPU[-1] = timesCPU[-1] + timesCPU[-2]
                timesWallClock[-1] = timesWallClock[-1] + timesWallClock[-2]
            
            if not params['if_dataset_onTheFly']:
                train_losses.append(loss_i)
                train_unregularized_losses.append(unregularized_loss_i)
                train_acces.append(acc_i)
            
            train_unregularized_minibatch_losses.append(
                unregularized_minibatch_loss_i
            )
#             sys.exit()
            train_minibatch_acces.append(minibatch_acc_i)
            
            

            reduction = 'mean'
            
            if params['if_dataset_onTheFly']:
                test_loss_i, test_unregularized_loss_i, test_acc_i =\
                get_regularized_loss_and_acc_from_x_whole_dataset_with_generator(model, dataset.test_generator, reduction, params)
            else:
                test_loss_i, test_unregularized_loss_i, test_acc_i =\
                get_regularized_loss_and_acc_from_x_whole_dataset(model, X_test, t_test, reduction, params)
            
            test_losses.append(test_loss_i)
            test_acces.append(test_acc_i)
            
            if params['if_LM']:
                lambdas.append(params['lambda_'])
                print('lambda = ', lambdas[-1])
            if params['if_yura']:
                yura_lambdas.append(params['yura_lambda'])
                print('yura-lambda = ', yura_lambdas[-1])
            epochs.append((epoch + 1) * record_epoch)
            



            
#             print('Learning rate: {0:.5f}'.format(params['alpha']))
            print('Current learning rate: {0:.5f}'.format(params['alpha_current']))
            
            print('Iter-{0:.3f}'.format(epochs[-1]))
            
            if not params['if_dataset_onTheFly']:
                print('Training loss: {0:.3f}'.format(train_losses[-1]))
                print('Training unregularized loss: {0:.3f}'.format(train_unregularized_losses[-1]))
                print('Training accuracy: {0:.3f}'.format(train_acces[-1]))
                
#                 print('train_unregularized_losses[-1]')
#                 print(train_unregularized_losses[-1])
            
            print('Training unregularized minibatch loss: {0:.3f}'.format(train_unregularized_minibatch_losses[-1]))
            print('Training minibatch acc: {0:.3f}'.format(train_minibatch_acces[-1]))
            
            

#             print('Testing loss: {0:.3f}'.format(test_losses[-1]))
            print('Testing unregularized loss: {0:.3f}'.format(test_unregularized_loss_i))
            print('Testing accuracy: {0:.3f}'.format(test_acces[-1]))
            
#             print('train_acces[-1]')
#             print(train_acces[-1])
            
            



            if epoch > 0:
                print('elapsed cpu time: ', timesCPU[-1] - timesCPU[-2])
                print('elapsed wall-clock time: ', timesWallClock[-1] - timesWallClock[-2])
            else:
                print('elapsed cpu time: ', timesCPU[-1])
                print('elapsed wall-clock time: ', timesWallClock[-1])

            


            
            # values_virtual_memory = psutil.virtual_memory()
            # print('total (GB):')
            # print(values_virtual_memory.total >> 30)
            # print('available (GB):')
            # print(values_virtual_memory.available >> 30)
            # print('percent (%):')
            # print(values_virtual_memory.percent)

            import gc
            gc.collect()

            torch.cuda.empty_cache()

            
            values_virtual_memory = psutil.virtual_memory()

            print('total (GB): {}, available (GB): {}, percent (%): {}'.format(
                values_virtual_memory.total >> 30, values_virtual_memory.available >> 30,\
                values_virtual_memory.percent
            ))
            


#             if params['if_gpu']:
            if params['device'] == 'cuda:0':
                
                print_gpu_usage(params)
                
                

            if 0:
                if params['algorithm'] == 'SMW-Fisher-different-minibatch' or\
                params['algorithm'] == 'SMW-Fisher' or\
                params['algorithm'] == 'SMW-Fisher-batch-grad-momentum-exponential-decay' or\
                params['algorithm'] == 'SMW-Fisher-batch-grad-momentum' or\
                params['algorithm'] == 'SMW-Fisher-batch-grad' or\
                params['algorithm'] == 'SGD' or\
                params['algorithm'] == 'ekfac-EF-VA' or\
                params['algorithm'] == 'ekfac-EF' or\
                params['algorithm'] == 'kfac':
                    1
                elif params['algorithm'] == 'RMSprop-individual-grad-no-sqrt-LM' or\
                params['algorithm'] == 'RMSprop-individual-grad-no-sqrt-Fisher' or\
                params['algorithm'] == 'RMSprop-individual-grad-no-sqrt' or\
                params['algorithm'] == 'RMSprop-individual-grad' or\
                params['algorithm'] == 'RMSprop-no-sqrt' or\
                params['algorithm'] == 'RMSprop':
                    for l in range(model.numlayers):
                        if params['layers_params'][l]['name'] == 'fully-connected' or\
                        params['layers_params'][l]['name'] == 'conv':
                            if np.max(data_['RMSprop_momentum_2'][l]['W']) < 10**(-100):
                                print('Warning: values too small.')
                                print('max value:')
                                print(np.max(data_['RMSprop_momentum_2'][l]['W']))
                            else:
                                fig, ax = plt.subplots()

                                # plt.ticklabel_format(style='sci', axis='x')
                                ax.xaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))

                                ax.hist(data_['RMSprop_momentum_2'][l]['W'].flatten())
                                plt.xticks(rotation=90)

                                # ax.ticklabel_format(style='sci')
                            
                                plt.show()
                        else:
                            print('Error: unknown layer when show v_t')
                            sys.exit()

                    print('test v_t')
                else:
                    print('Error: need more v_t')
                    sys.exit()
                    

            print('\n')



        # model = get_model_grad_zerod(model)

    print('Begin saving results...')
    
    params['algorithm'] = params['true_algorithm']

    

    name_algorithm_with_params = get_name_algorithm_with_params(params)

    name_result = name_dataset + '/' + name_algorithm_with_params + '/'



    epochs = np.asarray(epochs)
    timesCPU = np.asarray(timesCPU)
    timesWallClock = np.asarray(timesWallClock)
    
    if not params['if_dataset_onTheFly']:
        train_losses = np.asarray(train_losses)
        train_unregularized_losses = np.asarray(train_unregularized_losses)
        train_acces = np.asarray(train_acces)
    
    train_unregularized_minibatch_losses = np.asarray(train_unregularized_minibatch_losses)
    train_minibatch_acces = np.asarray(train_minibatch_acces)
    
    
    test_losses = np.asarray(test_losses)
    test_acces = np.asarray(test_acces)
    dict_result = {'train_unregularized_minibatch_losses': train_unregularized_minibatch_losses,
                   'train_minibatch_acces': train_minibatch_acces,
                   'test_losses': test_losses,
                   'test_acces': test_acces,
                   'timesCPU': timesCPU,
                   'timesWallClock': timesWallClock,
                   'epochs': epochs}
    if not params['if_dataset_onTheFly']:
        dict_result.update(
        {'train_losses': train_losses,
         'train_unregularized_losses': train_unregularized_losses,
         'train_acces': train_acces}
    )
    if params['if_LM']:
        lambdas = np.asarray(lambdas)
        dict_result['lambdas'] = lambdas
    if params['if_yura']:
        yura_lambdas = np.asarray(yura_lambdas)
        dict_result['yura_lambdas'] = yura_lambdas
        

    

    if params['if_test_mode']:
        if params['if_record_sgd_norm']:
            sgd_norms = np.asarray(sgd_norms)
            dict_result['sgd_norms'] = sgd_norms
        if params['if_record_p_norm']:
            p_norms = np.asarray(p_norms)
            dict_result['p_norms'] = p_norms
#         if params['if_record_kron_bfgs_cosine']:
#             kron_bfgs_cosines = data_['kron_bfgs_cosines']
#             dict_result['kron_bfgs_cosines'] = kron_bfgs_cosines
        if params['if_record_kfac_p_norm']:
            kfac_p_norms = data_['kfac_p_norms']
            kfac_p_norms = np.asarray(kfac_p_norms)
            dict_result['kfac_p_norms'] = kfac_p_norms
        if params['if_record_kfac_p_cosine']:
            kfac_p_cosines = data_['kfac_p_cosines']
            kfac_p_cosines = np.asarray(kfac_p_cosines)
            dict_result['kfac_p_cosines'] = kfac_p_cosines
        if params['if_record_res_grad_norm']:
            res_grad_norms = data_['res_grad_norms']
            res_grad_norms = np.asarray(res_grad_norms)
            dict_result['res_grad_norms'] = res_grad_norms
        if params['if_record_res_grad_random_norm']:
            res_grad_random_norms = data_['res_grad_random_norms']
            res_grad_random_norms = np.asarray(res_grad_random_norms)
            dict_result['res_grad_random_norms'] = res_grad_random_norms
        if params['if_record_res_grad_grad_norm']:
            res_grad_grad_norms = data_['res_grad_grad_norms']
            res_grad_grad_norms = np.asarray(res_grad_grad_norms)
            dict_result['res_grad_grad_norms'] = res_grad_grad_norms
        if params['if_record_res_grad_norm_per_iter']:
            res_grad_norms_per_iter = data_['res_grad_norms_per_iter']
            res_grad_norms_per_iter = np.asarray(res_grad_norms_per_iter)
            dict_result['res_grad_norms_per_iter'] = res_grad_norms_per_iter
        if 'if_record_kron_bfgs_matrix_norm_per_iter' in params and\
            params['if_record_kron_bfgs_matrix_norm_per_iter'] == True:
            dict_result['kron_bfgs_matrix_norms_per_iter'] = data_['kron_bfgs_matrix_norms_per_iter']
            
        if 'if_record_kron_bfgs_damping_status' in params and\
            params['if_record_kron_bfgs_damping_status'] == True:
            dict_result['kron_bfgs_damping_statuses'] = data_['kron_bfgs_damping_statuses']
            
        if 'if_record_kron_bfgs_check_damping' in params and\
            params['if_record_kron_bfgs_check_damping'] == True:
            dict_result['kron_bfgs_check_dampings'] = data_['kron_bfgs_check_dampings']
        
        
            
        if 'if_record_kron_bfgs_update_status' in params and\
            params['if_record_kron_bfgs_update_status'] == True:
            dict_result['kron_bfgs_update_status'] = data_['kron_bfgs_update_status']
            
#         if 'if_record_loss_per_iter' in params and\
#             params['if_record_loss_per_iter'] == True:
#             dict_result['losses_per_iter'] = data_['losses_per_iter']
            
        if 'if_record_kfac_G_inv_norm_per_iter' in params and\
        params['if_record_kfac_G_inv_norm_per_iter']:
            dict_result['kfac_G_inv_norms_per_iter'] = data_['kfac_G_inv_norms_per_iter']
            
        if 'if_record_kfac_G_inv_norm_per_epoch' in params and\
        params['if_record_kfac_G_inv_norm_per_epoch']:
            dict_result['kfac_G_inv_norms_per_epoch'] = data_['kfac_G_inv_norms_per_epoch']
            

        if params['if_records']['if_record_kfac_G_norm_per_epoch']:
            dict_result['kfac_G_norms_per_epoch'] = data_['kfac_G_norms_per_epoch']
        if params['if_records']['if_record_kfac_G_twoNorm_per_epoch']:
            dict_result['kfac_G_twoNorms_per_epoch'] = data_['kfac_G_twoNorms_per_epoch']
        if params['if_records']['if_record_kfac_A_twoNorm_per_epoch']:
            dict_result['kfac_A_twoNorms_per_epoch'] = data_['kfac_A_twoNorms_per_epoch']
        if params['if_records']['if_record_kron_bfgs_A_twoNorm_per_epoch']:
            dict_result['kron_bfgs_A_twoNorms_per_epoch'] = data_['kron_bfgs_A_twoNorms_per_epoch']
        if params['if_records']['if_record_kron_bfgs_G_LM_twoNorm_per_epoch']:
            dict_result['kron_bfgs_G_LM_twoNorms_per_epoch'] = data_['kron_bfgs_G_LM_twoNorms_per_epoch']
        if params['if_records']['if_record_kron_bfgs_Hg_twoNorm_per_epoch']:
            dict_result['kron_bfgs_Hg_twoNorms_per_epoch'] = data_['kron_bfgs_Hg_twoNorms_per_epoch']
        if params['if_records']['if_record_kron_bfgs_Ha_twoNorm_per_epoch']:
            dict_result['kron_bfgs_Ha_twoNorms_per_epoch'] = data_['kron_bfgs_Ha_twoNorms_per_epoch']
        if params['if_records']['if_record_layerWiseHessian_twoNorm_per_epoch']:
            dict_result['layerWiseHessian_twoNorms_per_epoch'] = data_['layerWiseHessian_twoNorms_per_epoch']
        if params['if_records']['if_record_inverseLayerWiseHessian_twoNorm_per_epoch']:
            dict_result['inverseLayerWiseHessian_twoNorms_per_epoch'] = data_['inverseLayerWiseHessian_twoNorms_per_epoch']
        if params['if_records']['if_record_inverseLayerWiseHessian_LM_twoNorm_per_epoch']:
            dict_result['inverseLayerWiseHessian_LM_twoNorms_per_epoch'] = data_['inverseLayerWiseHessian_LM_twoNorms_per_epoch']
        if params['if_records']['if_record_inverseLayerWiseHessian_LM_MA_twoNorm_per_epoch']:
            dict_result['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch'] = data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch']
        if params['if_records']['if_record_kron_bfgs_matrix_norm'] == True:
            dict_result['kron_bfgs_matrix_norms'] = data_['kron_bfgs_matrix_norms']
            
        if 'if_record_kfac_F_twoNorm_per_epoch' in params and\
        params['if_record_kfac_F_twoNorm_per_epoch']:
            dict_result['kfac_F_twoNorms_per_epoch'] = data_['kfac_F_twoNorms_per_epoch']
            
        if 'if_record_kron_bfgs_norm_s_y_per_iter' in params and\
        params['if_record_kron_bfgs_norm_s_y_per_iter']:
            dict_result['kron_bfgs_norms_s_y_per_iter'] = data_['kron_bfgs_norms_s_y_per_iter']
            
        if 'if_record_kron_bfgs_sTy_per_iter' in params and\
        params['if_record_kron_bfgs_sTy_per_iter']:
            dict_result['kron_bfgs_sTy_per_iter'] = data_['kron_bfgs_sTy_per_iter']

    if params['if_record_sgn_norm']:
        sgn_norms = np.asarray(sgn_norms)
        dict_result['sgn_norms'] = sgn_norms
        

    params_saved = {}
    for key_ in params['keys_params_saved']:
        params_saved[key_] = params[key_]
    dict_result['params'] = params_saved

    


    path_to_goolge_drive_dir = params['home_path'] + 'result/'
        
    
    os.makedirs(path_to_goolge_drive_dir + name_result, exist_ok = True)

    fake_args = {}
    fake_args['algorithm_dict'] = {}
    fake_args['algorithm_dict']['name'] = params['algorithm']
    # for key in dict_result['params']:
    fake_args['algorithm_dict']['params'] = dict_result['params']
    fake_args['home_path'] = params['home_path']
    fake_args['N1'] = params['N1']
    fake_args['N2'] = params['N2']
    fake_args['if_gpu'] = params['if_gpu']
    fake_args['dataset'] = name_dataset
    fake_args['name_loss'] = params['name_loss']
    
    fake_args['list_lr'] = [params['alpha']]
    
#     fake_args['tuning_criterion'] = 'train_loss'
    fake_args['tuning_criterion'] = 'test_acc'
    # does not matter because 
    # presumably, there will be at most 1 old pkl
    
    _, _, old_pkl_name = get_best_params(fake_args, if_plot=False)

    if old_pkl_name != None:
        print('Remove old result:')
        
#         print(path_to_goolge_drive_dir + name_result + old_pkl_name)
        print(name_result + old_pkl_name)
        
        os.remove(path_to_goolge_drive_dir + name_result + old_pkl_name)

    import datetime        

    filename_result_with_time =\
    'result_' + datetime.datetime.now().strftime("%Y-%m-%d-%H:%M:%S") +'.pkl'
    
    print('dict_result.keys()')
    print(dict_result.keys())
    
    print('dict_result[params].keys()')
    print(dict_result['params'].keys())
    
    print('dict_result[params')
    print(dict_result['params'])
    
    with open(path_to_goolge_drive_dir + name_result + filename_result_with_time, 'wb') as output_result:
        pickle.dump(dict_result, output_result)


#     print('Saved at ' + path_to_goolge_drive_dir + name_result + filename_result_with_time)
    print('Saved at ' + name_result + filename_result_with_time)

    return name_result, data_, dict_result['params']


def print_gpu_usage(params):
    device = params['device']
    
#     print('device')
#     print(device)
    
#     sys.exit()

    gpu_total_memory = torch.cuda.get_device_properties(device).total_memory
    gpu_cached = torch.cuda.memory_reserved(device)
    gpu_allocated = torch.cuda.memory_allocated(device)
    # f = c-a  # free inside cache

    print('total GPU memory: {0:.3f} GB, cached: {1:.3f} GB, allocated: {2:.3f} GB'.format(
        gpu_total_memory * 1e-9, gpu_cached * 1e-9, gpu_allocated * 1e-9))



def get_sort_profile():

    filepath = 'lprof0.txt'
    with open(filepath) as fp:
        line = fp.readline()
        cnt = 1
        list_percent_time = []
        list_line_1 = []
        while line:
            if cnt <=9:
                print(line)
            
            line_1 = line.strip().split()
            if len(line_1) > 5 and\
            line_1[0].replace('.','',1).isdigit() and\
            line_1[1].replace('Error: check if need to save shampoo.','',1).isdigit() and\
            line_1[2].replace('.','',1).isdigit() and\
            line_1[3].replace('.','',1).isdigit() and\
            line_1[4].replace('.','',1).isdigit():
                list_percent_time.append(float(line_1[4]))
                list_line_1.append(line)
            
            # print(line_1[0])
            line = fp.readline()
            cnt += 1


    list_percent_time = np.asarray(list_percent_time)

    argsort_list_percent_time = np.argsort(-list_percent_time)
    for i in argsort_list_percent_time:
        print(list_line_1[i])


File Path: utils_git/utils_data.py
Content:
import sys
import torch
import os
import pickle
import numpy as np

import torchvision.transforms as transforms
import torchvision.datasets as datasets

import gzip

import urllib.request

def load_downScaledMNIST(batch_size, test_batch_size, train_dir):
    
    
    
    
    print('==> Preparing data..')

    transform_train = transforms.Compose([
#         transforms.RandomCrop(32, padding=4),
#         transforms.RandomHorizontalFlip(),
        transforms.Resize([16, 16]),
        transforms.ToTensor(),
#         transforms.Normalize((0.,), (255.0,)),
        transforms.Normalize((0.,), (1.0,)),
    ])

    # Normalize the test set same as training set without augmentation
    transform_test = transforms.Compose([
        transforms.Resize([16, 16]),
        transforms.ToTensor(),
#         transforms.Normalize((0.,), (255.0,)),
        transforms.Normalize((0.,), (1.0,)),
    ])
    
    
    
    root_dir = train_dir
    
#     sys.exit()

    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST(root=root_dir, train=True, transform=transform_train, download=True),
        batch_size=batch_size,
        shuffle=True,
#         num_workers=1,
        pin_memory=True)

    test_loader = torch.utils.data.DataLoader(
        datasets.MNIST(root=root_dir, train=False, transform=transform_test,download=True),
        batch_size=test_batch_size,
        shuffle=False,
#         num_workers=1,
        pin_memory=True)
    
    return train_loader, test_loader

# def load_cifar100(name_dataset, batch_size = 128, test_batch_size = 1000):
def load_cifar100(name_dataset, home_path, batch_size, test_batch_size):
    
    print('==> Preparing data..')
    
    if name_dataset == 'CIFAR-100-NoAugmentation':
        transform_train = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]),
    ])
    elif name_dataset in ['CIFAR-100',
                          'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                          'CIFAR-100-onTheFly-ResNet34-BN',
                          'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                          'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                          'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                          'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                          'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                          'CIFAR-100-onTheFly-AllCNNC']:
        transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]),
    ])
    else:
        print('error: need to check for ' + name_dataset)
        sys.exit()

        

    # Normalize the test set same as training set without augmentation
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]),
    ])

    train_loader = torch.utils.data.DataLoader(
        datasets.CIFAR100(root=home_path + 'data/' + name_dataset + '_data', train=True, transform=transform_train, download=True),
        batch_size=batch_size, shuffle=True,
        num_workers=4, pin_memory=True, drop_last=True)

    test_loader = torch.utils.data.DataLoader(
        datasets.CIFAR100(root=home_path + 'data/' + name_dataset + '_data', train=False, transform=transform_test,download=True),
        batch_size=test_batch_size, shuffle=False, num_workers=0, pin_memory=True)
    
    return train_loader, test_loader

# def load_cifar(name_dataset, batch_size = 512, test_batch_size = 1000):
def load_cifar(name_dataset, home_path, batch_size, test_batch_size):
    
    print('==> Preparing data..')

    if name_dataset in ['CIFAR-10-NoAugmentation-vgg11',
                        'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout']:
        # https://arxiv.org/pdf/1811.03600.pdf
        
        transform_train = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
#     elif name_dataset in ['CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout']:
#         transform_train = transforms.Compose([
#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
#     ])
    elif name_dataset in ['CIFAR',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                          'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                          'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                          'CIFAR-10-onTheFly-ResNet32-BN',
                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                          'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                          'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                          'CIFAR-10-AllCNNC',
                          'CIFAR-10-N1-128-AllCNNC',
                          'CIFAR-10-N1-512-AllCNNC']:
        # RandomCrop and RandomHorizontalFlip do NOT affect normalization,
        # because their order can be exchanged
        
        
    
        transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
        
    else:
        print('error: need to check for ' + name_dataset)
        sys.exit()
        
        
    
    # (0.2023, 0.1994, 0.2010): AVERAGE std per image (degress of freedom = 1),
    # NOT std of the tensor of size (50000, 32, 32)
    
    # other normalization:
    # normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
    #                                  std=[0.229, 0.224, 0.225])
    # see https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/trainer.py

    # Normalize the test set same as training set without augmentation
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    train_loader = torch.utils.data.DataLoader(
        datasets.CIFAR10(root=home_path + 'data/' + name_dataset + '_data', train=True, transform=transform_train, download=True),
        batch_size=batch_size, shuffle=True,
        num_workers=4, pin_memory=True, drop_last=True)

    test_loader = torch.utils.data.DataLoader(
        datasets.CIFAR10(root=home_path + 'data/' + name_dataset + '_data', train=False, transform=transform_test,download=True),
        batch_size=test_batch_size, shuffle=False, num_workers=0, pin_memory=True)
    
    return train_loader, test_loader

def extract_labels(filename, one_hot=False):
    """Extract the labels into a 1D uint8 numpy array [index]."""
    print('Extracting', filename)
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if magic != 2049:
            raise ValueError(
                'Invalid magic number %d in MNIST label file: %s' %
                (magic, filename))
        num_items = _read32(bytestream)
        buf = bytestream.read(num_items)
        labels = np.frombuffer(buf, dtype=np.uint8)
        if one_hot:
            return dense_to_one_hot(labels)
        return labels

def _read32(bytestream):
    dt = np.dtype(np.uint32).newbyteorder('>')
    return np.frombuffer(bytestream.read(4), dtype=dt)[0]

def extract_images(filename):
    """Extract the images into a 4D uint8 numpy array [index, y, x, depth]."""
    print('Extracting', filename)
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if magic != 2051:
            raise ValueError(
                'Invalid magic number %d in MNIST image file: %s' %
                (magic, filename))
        num_images = _read32(bytestream)
        rows = _read32(bytestream)
        cols = _read32(bytestream)
        buf = bytestream.read(rows * cols * num_images)
        data = np.frombuffer(buf, dtype=np.uint8)
        
#         print('print(shape(data))', np.shape(data))
        
        data = data.reshape(num_images, rows, cols, 1)
        
#         print('print(shape(data))', np.shape(data))
        
        return data

def maybe_download(SOURCE_URL, filename, work_directory):
    """Download the data from Yann's website, unless it's already here."""
    
#     print('current path', os.getcwd())
    
#     print('work_directory', work_directory)
    
    if not os.path.exists(work_directory):
        os.makedirs(work_directory)
        
    filepath = os.path.join(work_directory, filename)
    
#     print('filepath', filepath)
    
    if not os.path.exists(filepath):
        
        
        
        
        
        
        
#         filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)
        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)
        statinfo = os.stat(filepath)
        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')
    return filepath

class DataSet(object):

    def __init__(self, images, labels, if_autoencoder, input_reshape, fake_data=False):
        if fake_data:
            self._num_examples = 10000
        else:
            assert images.shape[0] == labels.shape[0], (
                "images.shape: %s labels.shape: %s" % (images.shape,
                                                       labels.shape))
            self._num_examples = images.shape[0]
            
            # Convert shape from [num examples, rows, columns, depth]
            # to [num examples, rows*columns] (assuming depth == 1)
#             assert images.shape[3] == 1

            

            if input_reshape == 'fully-connected':
            
#                 print('images.shape')
#                 print(images.shape)
                
                # make sure that the channel changes the slowest
                images = np.swapaxes(images, 2, 3)
                images = np.swapaxes(images, 1, 2)
                
#                 print('images.shape')
#                 print(images.shape)
#                 sys.exit()
            
                images = images.reshape(images.shape[0],
                                    images.shape[1] * images.shape[2] * images.shape[3])
            elif input_reshape == '2d-CNN':
                # shape of images before moveaxis: N, H, W, C_in
                # input of conv2d: (N, C_in, H, W)
                
#                 print('images.shape')
#                 print(images.shape)
#                 sys.exit()

                
                
                images = np.moveaxis(images, [1, 2, 3], [2, 3, 1])
        
            elif input_reshape == '1d-CNN':
#                 print('images.shape')
#                 print(images.shape)
                
                images = images.squeeze(-1)
                
#                 print('images.shape')
#                 print(images.shape)
                
                images = np.swapaxes(images, 1, 2)
                
#                 print('images.shape')
#                 print(images.shape)
                
#                 sys.exit()


            images = images.astype(np.float32)

            if if_autoencoder:
                labels = images
            

        self._images = images
        self._labels = labels
        self._epochs_completed = 0
        self._index_in_epoch = 0

    @property
    def images(self):
        return self._images

    @property
    def labels(self):
        return self._labels

    @property
    def num_examples(self):
        return self._num_examples

    @property
    def epochs_completed(self):
        return self._epochs_completed

    def next_batch(self, batch_size, fake_data=False):
        """Return the next `batch_size` examples from this data set."""
        if fake_data:
            fake_image = [1.0 for _ in range(784)]
            fake_label = 0
            return [fake_image for _ in range(batch_size)], [
                fake_label for _ in range(batch_size)]
        start = self._index_in_epoch
        self._index_in_epoch += batch_size
        if self._index_in_epoch > self._num_examples:
            # Finished epoch
            self._epochs_completed += 1
            # Shuffle the data
            perm = np.arange(self._num_examples)
            np.random.shuffle(perm)
            self._images = self._images[perm]
            self._labels = self._labels[perm]
            # Start next epoch
            start = 0
            self._index_in_epoch = batch_size
            assert batch_size <= self._num_examples
        end = self._index_in_epoch
        return self._images[start:end], self._labels[start:end]

def read_data_sets(name_dataset, name_model, home_path, fake_data=False, one_hot=False):
    
    if name_dataset in ['MNIST-N1-1000',
                        'MNIST-no-regularization',
                        'MNIST-one-layer']:
        name_dataset = 'MNIST'
    elif name_dataset in ['DownScaledMNIST-N1-1000-no-regularization']:
        name_dataset = 'DownScaledMNIST-no-regularization'
    elif name_dataset in ['MNIST-autoencoder-no-regularization',
                          'MNIST-autoencoder-N1-1000',
                          'MNIST-autoencoder-N1-1000-no-regularization',
                          'MNIST-autoencoder-N1-1000-sum-loss-no-regularization',
                          'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization',
                          'MNIST-autoencoder-relu-N1-1000-sum-loss',
                          'MNIST-autoencoder-relu-N1-100-sum-loss',
                          'MNIST-autoencoder-relu-N1-500-sum-loss',
                          'MNIST-autoencoder-relu-N1-1-sum-loss',
                          'MNIST-autoencoder-reluAll-N1-1-sum-loss',
                          'MNIST-autoencoder-N1-1000-sum-loss']:
        name_dataset = 'MNIST-autoencoder'
    elif name_dataset in ['FACES-autoencoder-no-regularization',
                          'FACES-autoencoder-sum-loss-no-regularization',
                          'FACES-autoencoder-relu-sum-loss-no-regularization',
                          'FACES-autoencoder-relu-sum-loss',
                          'FACES-autoencoder-sum-loss']:
        name_dataset = 'FACES-autoencoder'
    elif name_dataset in ['FacesMartens-autoencoder-relu-no-regularization',
                          'FacesMartens-autoencoder-relu-N1-500',
                          'FacesMartens-autoencoder-relu-N1-100',]:
        name_dataset = 'FacesMartens-autoencoder-relu'
    elif name_dataset in ['CIFAR-deep',
                          'CIFAR-10-vgg16',
                          'CIFAR-10-vgg11',
                          'CIFAR-10-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                          'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                          'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                          'CIFAR-10-onTheFly-ResNet32-BN',
                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                          'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                          'CIFAR-10-AllCNNC',
                          'CIFAR-10-N1-128-AllCNNC',
                          'CIFAR-10-N1-512-AllCNNC']:
        name_dataset = 'CIFAR'
    elif name_dataset in ['CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-10-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine']:  
        name_dataset = 'CIFAR-10-NoAugmentation-vgg11'
    elif name_dataset in ['CIFAR-100-NoAugmentation-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-NoAugmentation-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout']:
        name_dataset = 'CIFAR-100-NoAugmentation'
    elif name_dataset in ['CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                          'CIFAR-100-onTheFly-AllCNNC']:
        name_dataset = 'CIFAR-100'
    elif name_dataset in ['CURVES-autoencoder-no-regularization',
                          'CURVES-autoencoder-sum-loss-no-regularization',
                          'CURVES-autoencoder-relu-sum-loss-no-regularization',
                          'CURVES-autoencoder-relu-sum-loss',
                          'CURVES-autoencoder-relu-N1-100-sum-loss',
                          'CURVES-autoencoder-relu-N1-500-sum-loss',
                          'CURVES-autoencoder-sum-loss',
                          'CURVES-autoencoder-shallow',
                          'CURVES-autoencoder-Botev',
                          'CURVES-autoencoder-Botev-sum-loss-no-regularization']:
        name_dataset = 'CURVES-autoencoder'
    elif name_dataset == 'Subsampled-ImageNet-vgg16':
        name_dataset = 'Subsampled-ImageNet-simple-CNN'
    elif name_dataset == 'sythetic-linear-regression-N1-1':
        name_dataset = 'sythetic-linear-regression'
    elif name_dataset in ['Fashion-MNIST-N1-60',
                          'Fashion-MNIST-N1-60-no-regularization',
                          'Fashion-MNIST-N1-256-no-regularization',
                          'Fashion-MNIST-GAP-N1-60-no-regularization']:
        name_dataset = 'Fashion-MNIST'
        
    
    class DataSets(object):
        pass
    data_sets = DataSets()
    if fake_data:
        data_sets.train = DataSet([], [], fake_data=True)
        data_sets.validation = DataSet([], [], fake_data=True)
        data_sets.test = DataSet([], [], fake_data=True)
        return data_sets
    
#     train_dir = '../data/' + name_dataset + '_data'
    
    train_dir = home_path + 'data/' + name_dataset + '_data'
    
    VALIDATION_SIZE = 0
    
    if name_dataset in ['MNIST',
                        'Fashion-MNIST']:
        if_autoencoder = False
        
        if name_dataset in ['MNIST',
                            'MNIST-one-layer']:
            SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
        elif name_dataset == 'Fashion-MNIST':
            SOURCE_URL =\
            'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'
            # https://github.com/zalandoresearch/fashion-mnist
        
        
        TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'
        TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'
        TEST_IMAGES = 't10k-images-idx3-ubyte.gz'
        TEST_LABELS = 't10k-labels-idx1-ubyte.gz'
#         VALIDATION_SIZE = 5000
    
        local_file = maybe_download(SOURCE_URL, TRAIN_IMAGES, train_dir)
        train_images = extract_images(local_file)
        local_file = maybe_download(SOURCE_URL, TRAIN_LABELS, train_dir)
        train_labels = extract_labels(local_file, one_hot=one_hot)
        local_file = maybe_download(SOURCE_URL, TEST_IMAGES, train_dir)
        test_images = extract_images(local_file)
        local_file = maybe_download(SOURCE_URL, TEST_LABELS, train_dir)
        test_labels = extract_labels(local_file, one_hot=one_hot)

        train_images = np.multiply(train_images, 1.0 / 255.0)
        test_images = np.multiply(test_images, 1.0 / 255.0)
        
        train_labels = np.int64(train_labels)
        test_labels = np.int64(test_labels)
        
        test_train_images = train_images.reshape((train_images.shape[0], -1))
        
#         print('np.matmul(np.transpose(test_train_images), test_train_images).shape')
#         print(np.matmul(np.transpose(test_train_images), test_train_images).shape)
        
        test_A = np.matmul(np.transpose(test_train_images), test_train_images)
        
        print('np.amin(np.diag(test_A))')
        print(np.amin(np.diag(test_A)))
        
#         print('np.linalg.matrix_rank(test_A)')
#         print(np.linalg.matrix_rank(test_A))
        
#         print('np.linalg.matrix_rank(test_train_images)')
#         print(np.linalg.matrix_rank(test_train_images))
        
    

    elif name_dataset == 'STL-10-simple-CNN':
        
        SOURCE_URL = 'http://ai.stanford.edu/~acoates/stl10/'
        TRAIN_IMAGES = 'stl10_binary.tar.gz'
        
        local_file = maybe_download(SOURCE_URL, TRAIN_IMAGES, train_dir)
        
        extract_images(local_file)
        
        sys.exit()

    elif name_dataset == 'UCI-HAR':
        # https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/
        
        if_autoencoder = False
        
        if os.path.isfile(train_dir + '/' + 'UCI_HAR_data_np.pkl'):
            with open(train_dir + '/' + 'UCI_HAR_data_np.pkl', 'rb') as filename_pkl:
                data_np = pickle.load(filename_pkl)
                
                train_images = data_np['train_images']
                train_labels = data_np['train_labels']
                test_images = data_np['test_images']
                test_labels = data_np['test_labels']
        else:
        
            SOURCE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/'
            IMAGES_ = 'UCI%20HAR%20Dataset.zip'

            local_file = maybe_download(SOURCE_URL, IMAGES_, train_dir)

#             print('local_file')
#             print(local_file)

            import zipfile
            with zipfile.ZipFile(local_file,"r") as zip_ref:
                zip_ref.extractall(train_dir)

            train_images = np.loadtxt(train_dir + '/' + 'UCI HAR Dataset/' + 'train/' + 'X_train.txt')
            train_labels = np.loadtxt(train_dir + '/' + 'UCI HAR Dataset/' + 'train/' + 'y_train.txt')
            test_images = np.loadtxt(train_dir + '/' + 'UCI HAR Dataset/' + 'test/' + 'X_test.txt')
            test_labels = np.loadtxt(train_dir + '/' + 'UCI HAR Dataset/' + 'test/' + 'y_test.txt')

            UCI_HAR_data_np = {}
            UCI_HAR_data_np['train_images'] = train_images
            UCI_HAR_data_np['train_labels'] = train_labels
            UCI_HAR_data_np['test_images'] = test_images
            UCI_HAR_data_np['test_labels'] = test_labels
            with open(train_dir + '/' + 'UCI_HAR_data_np.pkl', 'wb') as filename_pkl:
                pickle.dump(UCI_HAR_data_np, filename_pkl)
                
            os.remove(local_file)
            import shutil
            shutil.rmtree(train_dir + '/' + 'UCI HAR Dataset')

        train_images = train_images[:, :, np.newaxis, np.newaxis]
        test_images = test_images[:, :, np.newaxis, np.newaxis]
        
        train_labels -= 1
        test_labels -= 1
        
        train_labels = np.int64(train_labels)
        test_labels = np.int64(test_labels)
        
    elif name_dataset == 'MNIST-autoencoder':
        
        print('Begin laoding data...')
        
        if_autoencoder = True
        
        SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
        
        TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'
        TEST_IMAGES = 't10k-images-idx3-ubyte.gz'

#         VALIDATION_SIZE = 5000
    
        local_file = maybe_download(SOURCE_URL, TRAIN_IMAGES, train_dir)
        train_images = extract_images(local_file)
        
        

        local_file = maybe_download(SOURCE_URL, TEST_IMAGES, train_dir)
        test_images = extract_images(local_file)

        
        # see "Reducing the Dimensionality of Data with Neural Networks"
        train_images = np.multiply(train_images, 1.0 / 255.0)
        test_images = np.multiply(test_images, 1.0 / 255.0)

        train_labels = train_images
        test_labels = test_images
        
    elif name_dataset == 'FACES-autoencoder':
        
        if_autoencoder = True
        
        with open(train_dir + '/' + 'faces.pkl', 'rb') as filename_pkl:
            data_np = pickle.load(filename_pkl)
        
        # data_np[0], 103500, train
        # data_np[1], 20700, validation
        # data_np[2], 41400, test

        train_images = data_np[0][0]
        test_images = data_np[2][0]
        
        train_images = train_images[:, :, np.newaxis, np.newaxis]
        test_images = test_images[:, :, np.newaxis, np.newaxis]
        
        train_labels = train_images
        test_labels = test_images
        
        
    elif name_dataset == 'CURVES-autoencoder':
        if_autoencoder = True
        
        SOURCE_URL = 'http://www.cs.toronto.edu/~jmartens/'
        TRAIN_IMAGES = 'digs3pts_1.mat'
        
        local_file = maybe_download(SOURCE_URL, TRAIN_IMAGES, train_dir)

#         import mat4py
    
#         images_ = mat4py.loadmat(local_file)
        

        import scipy.io as sio
        
        images_ = sio.loadmat(local_file)
        
#         sys.exit()
            
        train_images = np.asarray(images_['bdata'])
        test_images = np.asarray(images_['bdatatest'])
        
        train_images = train_images[:, :, np.newaxis, np.newaxis]
        test_images = test_images[:, :, np.newaxis, np.newaxis]
        
        train_labels = train_images
        test_labels = test_images
        
    elif name_dataset == 'FacesMartens-autoencoder-relu':
        
        
        
        if_autoencoder = True
        
        SOURCE_URL = 'http://www.cs.toronto.edu/~jmartens/'
        TRAIN_IMAGES = 'newfaces_rot_single.mat'
        
        local_file = maybe_download(SOURCE_URL, TRAIN_IMAGES, train_dir)
        
        

#         import mat4py
    
#         images_ = mat4py.loadmat(local_file)
        
        import scipy.io as sio
        
        images_ = sio.loadmat(local_file)
        
#         print('images_.keys()')
#         print(images_.keys())
        
#         print('scipy_images_.keys()')
#         print(scipy_images_.keys())
        
#         sys.exit()
        
        
            
        images_ = np.asarray(images_['newfaces_single'])
#         test_images = np.asarray(images_['bdatatest'])

        images_ = np.transpose(images_)
        
        train_images = images_[:103500]
        test_images = images_[-41400:]
        
        train_images = train_images[:, :, np.newaxis, np.newaxis]
        test_images = test_images[:, :, np.newaxis, np.newaxis]
        
        train_labels = train_images
        test_labels = test_images
        
    elif name_dataset == 'Subsampled-ImageNet-simple-CNN':
        if_autoencoder = False
        
        # need to move YiRen_imagenet_sample/ to train_dir manually

        data_loader = load_subsampled_imagenet(train_dir)
        
        print('Extracting from data loader...')
        
        X_Y = next(iter(data_loader))
        
        print('Done Extracting.')
        
        X = X_Y[0]
        Y = X_Y[1]
        
        
        
        X = X.data.numpy()
        Y = Y.data.numpy()
        
#         print('X.shape')
#         print(X.shape)
        
#         print('Y.shape')
#         print(Y.shape)
        
        X = np.moveaxis(X, [1, 2, 3], [3, 1, 2])
#         Y = np.moveaxis(Y, [1, 2, 3], [2, 3, 1])
        

        
#         train_images = X[:4000]
#         train_labels = Y[:4000]
#         test_images = X[4000:]
#         test_labels = Y[4000:]
        
        train_images = X[:100]
        train_labels = Y[:100]
        test_images = X[100:]
        test_labels = Y[100:]
        
    elif name_dataset in ['DownScaledMNIST-no-regularization']:
        
        
        
        if_autoencoder = False
        
        if os.path.isfile(train_dir + '/' + 'data_np.pkl'):
            with open(train_dir + '/' + 'data_np.pkl', 'rb') as filename_pkl:
                data_np = pickle.load(filename_pkl)
                
                train_images = data_np['train_images']
                train_labels = data_np['train_labels']
                test_images = data_np['test_images']
                test_labels = data_np['test_labels']
                
            
        else:
        
            train_, test_ = load_downScaledMNIST(batch_size=60000, test_batch_size=10000, train_dir=train_dir)

            

            train_ = next(iter(train_))
            test_ = next(iter(test_))
            
            


            train_images = train_[0].data.numpy()
            train_labels = train_[1].data.numpy()
            test_images = test_[0].data.numpy()
            test_labels = test_[1].data.numpy()

#             print('train_images.shape')
#             print(train_images.shape)
            
#             print('test_images.shape')
#             print(test_images.shape)
            
            
            

            train_images = np.swapaxes(train_images, 1, 2)
            train_images = np.swapaxes(train_images, 2, 3)
            test_images = np.swapaxes(test_images, 1, 2)
            test_images = np.swapaxes(test_images, 2, 3)
            
#             print('train_images.shape')
#             print(train_images.shape)
            
#             print('test_images.shape')
#             print(test_images.shape)
            
            

            data_np = {}
            data_np['train_images'] = train_images
            data_np['train_labels'] = train_labels
            data_np['test_images'] = test_images
            data_np['test_labels'] = test_labels
            
            
            
            import shutil
            
            shutil.rmtree(train_dir + '/')
            
#             print('delete download data')

            
            
            os.mkdir(train_dir + '/')

            with open(train_dir + '/' + 'data_np.pkl', 'wb') as filename_pkl:
                pickle.dump(data_np, filename_pkl)
        
    
    elif name_dataset in ['CIFAR',
                          'CIFAR-10-NoAugmentation-vgg11',
                          'CIFAR-100',
                          'CIFAR-100-NoAugmentation']:
        
        
        
        if_autoencoder = False
        
        if os.path.isfile(train_dir + '/' + 'data_np.pkl'):
            with open(train_dir + '/' + 'data_np.pkl', 'rb') as filename_pkl:
                data_np = pickle.load(filename_pkl)
                
                train_images = data_np['train_images']
                train_labels = data_np['train_labels']
                test_images = data_np['test_images']
                test_labels = data_np['test_labels']
                
#             print('np.mean(train_images, axis=(0,1,2))')
#             print(np.mean(train_images, axis=(0,1,2)))
            
#             sys.exit()
        else:
            
            
        
            if name_dataset == 'CIFAR':
                train_, test_ = load_cifar(name_dataset, home_path, batch_size=50000, test_batch_size=10000)
                
                
            elif name_dataset == 'CIFAR-10-NoAugmentation-vgg11':
                
                train_, test_ = load_cifar(name_dataset, home_path, batch_size=50000, test_batch_size=10000)
            elif name_dataset in ['CIFAR-100',
                                  'CIFAR-100-NoAugmentation']:
        
                train_, test_ = load_cifar100(name_dataset, home_path, batch_size=50000, test_batch_size=10000)

            train_ = next(iter(train_))
            test_ = next(iter(test_))


            train_images = train_[0].data.numpy()
            train_labels = train_[1].data.numpy()
            test_images = test_[0].data.numpy()
            test_labels = test_[1].data.numpy()
            

            train_images = np.swapaxes(train_images, 1, 2)
            train_images = np.swapaxes(train_images, 2, 3)
            test_images = np.swapaxes(test_images, 1, 2)
            test_images = np.swapaxes(test_images, 2, 3)
            
#             print('train_images.shape')
#             print(train_images.shape)
            
#             print('np.mean(train_images, axis=(0,1,2))')
#             print(np.mean(train_images, axis=(0,1,2)))
            
            

#             sys.exit()

            data_np = {}
            data_np['train_images'] = train_images
            data_np['train_labels'] = train_labels
            data_np['test_images'] = test_images
            data_np['test_labels'] = test_labels
            
            import shutil
            
            shutil.rmtree(train_dir + '/')
            os.mkdir(train_dir + '/')

            with open(train_dir + '/' + 'data_np.pkl', 'wb') as filename_pkl:
                pickle.dump(data_np, filename_pkl)
        
        '''
        
        import tarfile
#         import numpy as np
        
        SOURCE_URL = 'https://www.cs.toronto.edu/~kriz/'
        if name_dataset == 'CIFAR':
            file_name = 'cifar-10-python.tar.gz'
        elif name_dataset == 'CIFAR-100':
            file_name = 'cifar-100-python.tar.gz'
        
        local_file = maybe_download(SOURCE_URL, file_name, train_dir)
        
        print('local_file', local_file)
        
#         sys.exit()
        
        
        tf = tarfile.open(train_dir + '/' + file_name)
        tf.extractall(train_dir)

        if name_dataset == 'CIFAR':
            working_dir = train_dir + '/cifar-10-batches-py/'

            for i in range(5):
                with open(working_dir + 'data_batch_' + str(i+1), 'rb') as fo:
                    dict_ = pickle.load(fo, encoding='bytes')

                    if i == 0:
                        train_images = dict_['data'.encode('UTF-8')]
                        train_labels = dict_['labels'.encode('UTF-8')]
                    else:
                        train_images = np.concatenate((train_images, dict_['data'.encode('UTF-8')]))
                        train_labels = np.concatenate((train_labels, dict_['labels'.encode('UTF-8')]))

            with open(working_dir + 'test_batch', 'rb') as fo:
                dict_ = pickle.load(fo, encoding='bytes')
                test_images = dict_['data'.encode('UTF-8')]
                test_labels = dict_['labels'.encode('UTF-8')]
                test_labels = np.asarray(test_labels)
                
        elif name_dataset == 'CIFAR-100':
            working_dir = train_dir + '/cifar-100-python/'
            
            with open(working_dir + 'train', 'rb') as fo:
                dict_ = pickle.load(fo, encoding='bytes')
                
            train_images = dict_[b'data']
            train_labels = dict_[b'fine_labels']
            
            with open(working_dir + 'test', 'rb') as fo:
                dict_ = pickle.load(fo, encoding='bytes')
                
            test_images = dict_[b'data']
            test_labels = dict_[b'fine_labels']
            
        
        train_images = train_images[:, :, np.newaxis, np.newaxis]
        test_images = test_images[:, :, np.newaxis, np.newaxis]
        
        train_images = np.multiply(train_images, 1.0 / 255.0)
        test_images = np.multiply(test_images, 1.0 / 255.0)
        
        
        '''
        
    elif name_dataset == 'webspam':
        if_autoencoder = False
        
        
#         if os.path.isfile('../data/webspam_data' + '/webspam_data_np.pkl'):
        if os.path.isfile(train_dir + '/webspam_data_np.pkl'):
#             with open('../data/webspam_data' + '/webspam_data_np.pkl', 'rb') as filename_pkl:
            with open(train_dir + '/webspam_data_np.pkl', 'rb') as filename_pkl:
                webspam_data_np = pickle.load(filename_pkl)
                x = webspam_data_np['x']
                lines_y = webspam_data_np['lines_y']
        else:
        
            SOURCE_URL = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/'
            file_name = 'webspam_wc_normalized_unigram.svm.bz2'

            local_file = maybe_download(SOURCE_URL, file_name, train_dir)

            #import bz2

            #bz_file = bz2.BZ2File(train_dir + '/' + "webspam_wc_normalized_unigram.svm.bz2")
            #line_list = bz_file.readlines()

            import bz2,shutil

            with bz2.BZ2File(train_dir + '/' + "webspam_wc_normalized_unigram.svm.bz2") as fr, open(train_dir + '/' + "output.bin","wb") as fw:
                shutil.copyfileobj(fr,fw)
                
            os.remove(train_dir + '/webspam_wc_normalized_unigram.svm.bz2')



#             with open('../data/webspam_data' + '/output.bin', "r") as file:
            with open(train_dir + '/output.bin', "r") as file:

                lines = file.readlines()
                
#             os.remove('../data/webspam_data' + '/output.bin')
            os.remove(train_dir + '/output.bin')

            lines_1 = [line.split() for line in lines]

            lines_y = [line[0] for line in lines_1]
            lines_x = [line[1:] for line in lines_1]

            from tqdm import tqdm



            x = np.zeros((len(lines_x), 254))
            i = 0

            #set_feature = set()

            for line in tqdm(lines_x):
                #print(line)

                np_line = np.asarray([line_i.split(':') for line_i in line])

                #np_line[:, 1] = np_line[:, 1].astype(np.float)



                #print(np_line[:, 0])

                x[i, np_line[:, 0].astype(np.int) - 1] = np_line[:, 1].astype(np.float)

                #set_feature = set_feature.union(set(list(np_line[:, 0].astype(np.int))))

                i +=1

            lines_y = np.asarray(lines_y)
            lines_y = lines_y.astype(np.int)
            lines_y = (lines_y + 1) / 2

            webspam_data_np = {}
            webspam_data_np['x'] = x
            webspam_data_np['lines_y'] = lines_y
#             with open('../data/webspam_data' + '/webspam_data_np.pkl', 'wb') as filename_pkl:
            with open(train_dir + '/webspam_data_np.pkl', 'wb') as filename_pkl:
                pickle.dump(webspam_data_np, filename_pkl)
        
        train_images = x[:300000]
        test_images = x[300000:]
        train_labels = lines_y[:300000]
        test_labels = lines_y[300000:]
        
        
        
        
        
        
        
        """
          from scipy.io import loadmat
        import scipy.io as io
        x = io.loadmat('gdrive/My Drive/Gauss_Newton/data/webspam/webspam_wc_normalized_unigram.svm.mat')
        
        print(x)
        """
        
        
        """
        with open(home_path + 'data/webspam_data/' + 'webspam_wc_normalized_unigram.pkl', 'rb') as f:
            dict_webspam = pickle.load(f)

        import numpy as np

        for key in dict_webspam:
            dict_webspam[key] = np.asarray(dict_webspam[key])
        train_images = dict_webspam['indata']
        train_labels = dict_webspam['outdata']
        test_images = dict_webspam['intest']
        test_labels = dict_webspam['outtest']
        """
        
        print('train_images.shape')
        print(train_images.shape)
        
        
        """
        SOURCE_URL = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/'
        file_name = 'webspam_wc_normalized_unigram.svm.bz2'
        
        
        local_file = maybe_download(SOURCE_URL, file_name, train_dir)
        
        from sklearn.datasets import load_svmlight_file

        data = load_svmlight_file(train_dir + '/' + file_name)
        
        images = data[0]
        labels = data[1]
        
        """
        
        
        #         % compute mean and std from train
        mean_train = np.mean(train_images, axis=0)

        std_train = np.std(train_images, axis=0)
        
        
        
#         [m, ~] = size(indata);
        std_train = np.maximum(std_train, 1 / np.sqrt(len(train_images[0]))) # https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization


    
#         print('train_images[:10] before pre-process')
#         print(train_images[:10])
        
        print('np.linalg.norm(train_images, ord=1, axis=-1)[:10]')
        print(np.linalg.norm(train_images, ord=1, axis=-1)[:10])
    
#         % normalize train
        train_images = train_images - mean_train[None, :]

#         [~, N_train] = size(indata);
        train_images = train_images / std_train[None, :]

#         % normalize test
        test_images = test_images - mean_train[None, :]

#         [~, N_test] = size(intest);
        test_images = test_images / std_train[None, :]
        
        train_images = train_images[:, :, np.newaxis, np.newaxis]
        test_images = test_images[:, :, np.newaxis, np.newaxis]
        
    elif name_dataset == 'sythetic-linear-regression':
        if_autoencoder = False
        # http://proceedings.mlr.press/v70/zhou17a/zhou17a.pdf
        p = 100
        q = 50
        N = 1000
        
        np.random.seed(127)
        
        # images_: N * p
        mean = np.zeros(p)
        rho = 0.5
        cov = (1-rho**2) * np.eye(p) + rho**2 * np.ones((p, p))
        images_ = np.random.multivariate_normal(mean, cov, size=N)
        
        # coefficient
        beta_ = np.random.rand(p, q)
        
        # labes_: N * q
        
        labels_ = np.matmul(images_, beta_)
        labels_ += np.random.normal(size=labels_.shape)
        
        labels_ = np.float32(labels_)
        images_ = np.float32(images_)
        
        images_ = images_[:, :, np.newaxis, np.newaxis]
        
        train_images = images_[:int(0.9*N)]
        train_labels = labels_[:int(0.9*N)]
        test_images = images_[int(0.9*N):]
        test_labels = labels_[int(0.9*N):]
        
    else:
        print('error: Dataset not supported for ' + name_dataset)
        sys.exit()
        

    validation_images = train_images[:1]
    validation_labels = train_labels[:1]
    # fake
    
    train_images = train_images[VALIDATION_SIZE:]
    train_labels = train_labels[VALIDATION_SIZE:]

    if name_model in ['simple-CNN',
                      'CNN',
                      'vgg16',
                      'vgg11',
                      'ResNet32',
                      'ResNet34',
                      'AllCNNC']:
        input_reshape = '2d-CNN'
    elif name_model in ['1d-CNN']:
        input_reshape = '1d-CNN'
    elif name_model == 'fully-connected':
        input_reshape = 'fully-connected'
    else:
        print('Error: unknown model name for ' + name_model)
        sys.exit()

    data_sets.train = DataSet(train_images, train_labels, if_autoencoder, input_reshape)
    data_sets.validation = DataSet(validation_images, validation_labels, if_autoencoder, input_reshape)
    data_sets.test = DataSet(test_images, test_labels, if_autoencoder, input_reshape)
        
    
    return data_sets



class DataSet_v2():
    
    def __init__(self, train_loader):
        self.train_loader = train_loader
        self.batch_iterator = iter(train_loader)
    
    def next_batch(self, batch_size):
        try:
            return next(self.batch_iterator)
        except:
            self.batch_iterator = iter(self.train_loader)
            return next(self.batch_iterator)

# def read_data_sets_v2(name_dataset, dataset_notOnTheFly, params):
def read_data_sets_v2(name_dataset, params):
    
    class DataSets_v2(object):
        pass
    
    dataset  = DataSets_v2()
    
    if name_dataset in ['CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                        'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                        'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                        'CIFAR-10-onTheFly-ResNet32-BN',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                        'CIFAR-10-AllCNNC',
                        'CIFAR-10-N1-128-AllCNNC',
                        'CIFAR-10-N1-512-AllCNNC',
                        'CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                        'CIFAR-100-onTheFly-ResNet34-BN',
                        'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                        'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                        'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                        'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                        'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                        'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                        'CIFAR-100-onTheFly-AllCNNC']:
    
        dataset.num_train_data = 50000
    else:
        print('error: need to check for ' + name_dataset)
        sys.exit()
        
    N1 = params['N1']
        
    if name_dataset in ['CIFAR-10-NoAugmentation-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-N1-256-vgg16-NoAdaptiveAvgPool',
                        'CIFAR-10-onTheFly-N1-512-vgg16-NoAdaptiveAvgPoolNoDropout',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-NoBias',
                        'CIFAR-10-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                        'CIFAR-10-onTheFly-ResNet32-BNNoAffine',
                        'CIFAR-10-onTheFly-ResNet32-BN',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcut',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly',
                        'CIFAR-10-onTheFly-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BNNoAffine-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-BNshortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                        'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization',
                        'CIFAR-10-AllCNNC',
                        'CIFAR-10-N1-128-AllCNNC',
                        'CIFAR-10-N1-512-AllCNNC']:
    
        train_loader, test_loader = load_cifar(name_dataset, params['home_path'], N1, N1)
    elif name_dataset in ['CIFAR-100-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-ResNet34-BNNoAffine',
                          'CIFAR-100-onTheFly-ResNet34-BN',
                          'CIFAR-100-onTheFly-ResNet34-BN-BNshortcut',
                          'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly',
                          'CIFAR-100-onTheFly-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-N1-128-ResNet34-BN-BNshortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-N1-128-ResNet34-BN-PaddingShortcutDownsampleOnly-NoBias',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine-no-regularization',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN',
                          'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization',
                          'CIFAR-100-onTheFly-vgg16-NoLinear-BN-no-regularization',
                          'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout',
                          'CIFAR-100-onTheFly-N1-256-vgg16-NoAdaptiveAvgPoolNoDropout-BNNoAffine',
                          'CIFAR-100-onTheFly-AllCNNC']:
        train_loader, test_loader = load_cifar100(name_dataset, params['home_path'], N1, N1)
    else:
        print('error: need to check for ' + name_dataset)
        sys.exit()
    
    dataset.train = DataSet_v2(train_loader)
    
#     dataset.test = dataset_notOnTheFly.test
    
    dataset.test_generator = test_loader
    
    return dataset


File Path: utils_git/utils_get_params.py
Content:
def get_params(params, args):
    params['true_algorithm'] = params['algorithm']
    
    algorithm = params['algorithm']
    


    
    if algorithm == 'SGD-LRdecay-momentum':
        params['if_lr_decay'] = True
        params['algorithm'] = 'SGD-momentum'
    elif algorithm == 'Adam-noWarmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'Adam-noWarmStart-momentum-grad'
        
    elif algorithm == 'kfac-warmStart-lessInverse-no-max-no-LM-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'kfac-warmStart-lessInverse-no-max-no-LM-momentum-grad'
        
    elif algorithm == 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad'
        
    elif algorithm == 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM-momentum-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-unregularized-grad-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-unregularized-grad-momentum-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad'
        
    elif algorithm == 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad'
        
    elif algorithm == 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-warmStart-momentum-grad'
    elif algorithm == 'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
    
    elif algorithm == 'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-momentum-grad'
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad'
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping-momentum-grad'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad'
        
    elif algorithm == 'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay':
        params['if_lr_decay'] = True
        params['algorithm'] = 'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart-momentum-grad'
        
    elif algorithm in ['SGD-momentum',
                       'Adam-noWarmStart-momentum-grad',
                       'Fisher-BD',
                       'Fisher-BD-momentum-grad',
                       'kfac-warmStart-no-max-no-LM-momentum-grad',
                       'kfac-warmStart-lessInverse-no-max-no-LM-momentum-grad',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM-momentum-grad',
                       'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM-momentum-grad',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM-momentum-grad',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt-regularized-grad-momentum-grad',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT-regularized-grad-momentum-grad',
                       'shampoo-allVariables-warmStart-momentum-grad',
                       'shampoo-allVariables-warmStart-lessInverse-momentum-grad',
                       'shampoo-allVariables-filterFlattening-warmStart-momentum-grad',
                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad',
                       'matrix-normal-allVariables-warmStart-MaxEigDamping-momentum-grad',
                       'matrix-normal-same-trace-allVariables-warmStart-momentum-grad',
                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping-momentum-grad',
                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping-momentum-grad',
                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart-momentum-grad',
                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad',
                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-momentum-grad',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping-momentum-grad',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping-momentum-grad',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping-momentum-grad',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-momentum-grad',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad',
                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart-momentum-grad',]:
        params['if_lr_decay'] = False
    else:
        print('algorithm')
        print(algorithm)
        sys.exit()
    
    # get rid of momentum-grad
    algorithm = params['algorithm']
    if algorithm == 'Kron-BFGS-LM-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-LM-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-LM-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-LM-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-LM-sqrt-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-LM-sqrt-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-momentum-s-y-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-momentum-s-y-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-momentum-s-y-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-momentum-s-y-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-momentum-s-y-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-momentum-s-y-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-Shiqian-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-Shiqian-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-Shiqian-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-Shiqian-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Shiqian-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Shiqian-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-PowellBDamping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-PowellBDamping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep-regularized-grad'
        
    elif algorithm == 'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-doubleGrad-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-doubleGrad-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping-regularized-grad'
        
    elif algorithm == 'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-identity-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-identity-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-identity-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-identity-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-damping-unregularized-grad'
    
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-damping-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-damping-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-damping-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-damping-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-wrong-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-wrong-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-regularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-regularized-grad'
        
    elif algorithm == 'Kron-BFGS-homo-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-homo-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-Hessian-action-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-Hessian-action-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-wrong-Hessian-action-unregularized-grad-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-wrong-Hessian-action-unregularized-grad'
        
    elif algorithm == 'Kron-BFGS-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Kron-BFGS-LM'
        
    elif algorithm == 'Fisher-BD-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Fisher-BD'
        
    elif algorithm == 'kfac-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac'
        
    elif algorithm == 'kfac-momentum-grad-test':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-test'
        
    elif algorithm == 'kfac-no-max-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-no-max'
        
    elif algorithm == 'kfac-NoMaxNoSqrt-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-NoMaxNoSqrt'
        
    elif algorithm == 'kfac-NoMaxNoSqrt-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-NoMaxNoSqrt-no-LM'
        
    elif algorithm == 'kfac-no-max-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-no-max-no-LM'
        
    elif algorithm == 'kfac-warmStart-no-max-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-warmStart-no-max-no-LM'
        
    elif algorithm == 'kfac-warmStart-lessInverse-no-max-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-warmStart-lessInverse-no-max-no-LM'
        
    elif algorithm == 'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM'
        
    elif algorithm == 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM'
        
    elif algorithm == 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM'
        
    elif algorithm == 'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM'
        
    elif algorithm == 'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM'
        
    elif algorithm == 'kfac-no-max-epsilon-A-G-no-LM-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'kfac-no-max-epsilon-A-G-no-LM'
        
    elif algorithm == 'SGD-momentum':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'SGD'
    
    elif algorithm == 'SGD-LRdecay-momentum':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'SGD-LRdecay'
        
    elif algorithm == 'RMSprop-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'RMSprop'
        
    elif algorithm == 'RMSprop-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'RMSprop-warmStart'
        
    elif algorithm == 'RMSprop-momentum-grad-test':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'RMSprop-test'
        
    elif algorithm == 'Adam-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Adam'
        
    elif algorithm == 'Adam-momentum-grad-test':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Adam-test'
        
    elif algorithm == 'Adam-noWarmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'Adam-noWarmStart'
        
    elif algorithm == 'shampoo-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo'
        
    elif algorithm == 'shampoo-allVariables-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-allVariables'
        
    elif algorithm == 'shampoo-allVariables-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-allVariables-warmStart'
        
    elif algorithm == 'shampoo-allVariables-warmStart-lessInverse-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-allVariables-warmStart-lessInverse'
        
    elif algorithm == 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart'
        
    elif algorithm == 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse'
        
    elif algorithm == 'shampoo-no-sqrt-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-no-sqrt'
        
    elif algorithm == 'shampoo-no-sqrt-Fisher-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-no-sqrt-Fisher'
        
    elif algorithm == 'shampoo-no-sqrt-Fisher-momentum-grad-test':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'shampoo-no-sqrt-Fisher-test'
        
    elif algorithm == 'matrix-normal-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal'
        
    elif algorithm == 'matrix-normal-allVariables-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-allVariables'
        
    elif algorithm == 'matrix-normal-allVariables-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-allVariables-warmStart'
        
    elif algorithm == 'matrix-normal-allVariables-warmStart-MaxEigDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-allVariables-warmStart-MaxEigDamping'
        
    elif algorithm == 'matrix-normal-allVariables-warmStart-noPerDimDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-allVariables-warmStart-noPerDimDamping'
        
    elif algorithm == 'matrix-normal-same-trace-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace'
        
    elif algorithm == 'matrix-normal-same-trace-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-warmStart'
        
    elif algorithm == 'matrix-normal-same-trace-warmStart-noPerDimDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-warmStart-noPerDimDamping'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-warmStart'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-filterFlattening-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-filterFlattening-warmStart'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart'
        
    elif algorithm == 'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping'
        
    elif algorithm == 'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse'
        
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart'
        
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse'
        
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping'
        
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping'
        
    elif algorithm == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart'
        
    elif algorithm == 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse'
        
    elif algorithm == 'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart-momentum-grad':
        params['if_momentum_gradient'] = True
        params['algorithm'] = 'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart'
        
    else:   
        if algorithm in ['SGD',
                         'SGD-yura-MA',
                         'SGD-yura',
                         'SMW-Fisher',
                         'SMW-Fisher-momentum',
                         'SMW-GN',
                         'shampoo',
                         'matrix-normal-same-trace',
                         'kfac-TR',
                         'kfac-CG',
                         'kfac',
                         'kfac-test',
                         'kfac-no-max',
                         'kfac-no-max-no-LM',
                         'ekfac-EF',
                         'Kron-BFGS',
                         'Kron-BFGS-no-norm-gate-regularized-grad',
                         'Kron-BFGS-no-norm-gate-momentum-s-y-regularized-grad',
                         'Kron-BFGS-no-norm-gate-momentum-s-y-damping-regularized-grad',
                         'Kron-BFGS-no-norm-gate-damping-regularized-grad',
                         'Kron-BFGS-no-norm-gate-Shiqian-damping-regularized-grad',
                         'Kron-BFGS-unregularized-grad',
                         'Kron-BFGS-wrong-unregularized-grad',
                         'Kron-BFGS-regularized-grad',
                         'Kron-BFGS-homo-regularized-grad',
                         'Kron-BFGS-LM',
                         'Kron-BFGS-LM-unregularized-grad',
                         'Kron-BFGS-LM-regularized-grad',
                         'Kron-BFGS-LM-sqrt-regularized-grad',
                         'Kron-BFGS-Hessian-action',
                         'Kron-BFGS-Hessian-action-unregularized-grad',
                         'Kron-BFGS-1st-layer-only',
                         'Kron-BFGS-block',
                         'Kron-SGD',
                         'Kron-SGD-test',
                         'BFGS',
                         'BFGS-homo']:
            params['if_momentum_gradient'] = False
        elif algorithm in ['matrix-normal-LM-momentum-grad',
                           'matrix-normal-same-trace-LM-momentum-grad',
                           'kfac-momentum-grad-CG',
                           'kfac-momentum-grad-TR',
                           'Kron-BFGS-momentum-grad',
                           'Kron-BFGS-Hessian-action-momentum-grad']:
            params['if_momentum_gradient'] = True
        else:
            print('Error: unkown if momentum gradient for ' + algorithm)
            sys.exit()
    
    # get rid of (un)regularized grad
    algorithm = params['algorithm']
    if algorithm == 'Kron-BFGS-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-momentum-s-y-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-momentum-s-y'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-momentum-s-y-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-momentum-s-y-damping'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-momentum-s-y-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-momentum-s-y-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-damping'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-Shiqian-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-Shiqian-damping'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-Shiqian-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-Shiqian-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Shiqian-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Shiqian-damping'
    
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-PowellBDamping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-PowellBDamping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-H-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep'
        
    elif algorithm == 'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-doubleGrad-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-doubleGrad'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping'
        
    elif algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping'
        
    elif algorithm == 'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping'
        
    elif algorithm == 'Kron-BFGS-homo-identity-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-identity'
        
    elif algorithm == 'Kron-BFGS-homo-identity-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-identity'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-no-norm-gate-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-damping-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-damping'
        
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-damping-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-damping'
        
    elif algorithm == 'Kron-BFGS-no-norm-gate-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-no-norm-gate'
        
    elif algorithm == 'Kron-BFGS-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS'
        
    elif algorithm == 'Kron-BFGS-wrong-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-wrong'
        
    elif algorithm == 'Kron-BFGS-homo-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo'
        
    elif algorithm == 'Kron-BFGS-homo-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-homo'
        
    elif algorithm == 'Kron-BFGS-LM-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-LM'
        
    elif algorithm == 'Kron-BFGS-LM-sqrt-regularized-grad':
        params['if_regularized_grad'] = True
        params['algorithm'] = 'Kron-BFGS-LM-sqrt'
        
    elif algorithm == 'Kron-BFGS-Hessian-action-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-Hessian-action'
        
    elif algorithm == 'Kron-BFGS-wrong-Hessian-action-unregularized-grad':
        params['if_regularized_grad'] = False
        params['algorithm'] = 'Kron-BFGS-wrong-Hessian-action'
    else:
        if algorithm in ['Kron-BFGS-LM-unregularized-grad']:
            params['if_regularized_grad'] = False
        elif algorithm in ['SGD',
                           'RMSprop',
                           'RMSprop-warmStart',
                           'Adam',
                           'Adam-test',
                           'Adam-noWarmStart',
                           'shampoo',
                           'shampoo-allVariables',
                           'shampoo-allVariables-warmStart',
                           'shampoo-allVariables-warmStart-lessInverse',
                           'shampoo-allVariables-filterFlattening-warmStart',
                           'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                           'shampoo-no-sqrt',
                           'shampoo-no-sqrt-Fisher',
                           'matrix-normal',
                           'matrix-normal-allVariables',
                           'matrix-normal-allVariables-warmStart',
                           'matrix-normal-allVariables-warmStart-MaxEigDamping',
                           'matrix-normal-allVariables-warmStart-noPerDimDamping',
                           'matrix-normal-same-trace',
                           'matrix-normal-same-trace-warmStart',
                           'matrix-normal-same-trace-warmStart-noPerDimDamping',
                           'matrix-normal-same-trace-allVariables',
                           'matrix-normal-same-trace-allVariables-warmStart',
                           'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                           'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                           'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                           'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                           'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                           'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                           'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                           'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                           'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                           'BFGS',
                           'BFGS-homo',
                           'Fisher-BD',
                           'kfac',
                           'kfac-no-max',
                           'kfac-NoMaxNoSqrt',
                           'kfac-NoMaxNoSqrt-no-LM',
                           'kfac-no-max-no-LM',
                           'kfac-warmStart-no-max-no-LM',
                           'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                           'kfac-correctFisher-warmStart-no-max-no-LM',
                           'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                           'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                           'kfac-warmStart-lessInverse-no-max-no-LM',
                           'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                           'kfac-no-max-epsilon-A-G-no-LM']:
            params['if_regularized_grad'] = True
        else:
            print('Error: unkown if_regularized_grad for ' + algorithm)
            sys.exit()
            
    algorithm = params['algorithm']  

    if algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-doubleGrad':
        params['if_double_grad'] = True
        params['algorithm'] = 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping'
    elif algorithm in ['SGD',
                       'RMSprop-warmStart',
                       'Adam-noWarmStart',
                       'Fisher-BD',
                       'kfac',
                       'kfac-no-max-no-LM',
                       'kfac-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-warmStart-lessInverse-no-max-no-LM',
                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-epsilon-A-G-no-LM',
                       'shampoo',
                       'shampoo-allVariables',
                       'shampoo-allVariables-warmStart',
                       'shampoo-allVariables-warmStart-lessInverse',
                       'shampoo-allVariables-filterFlattening-warmStart',
                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                       'shampoo-no-sqrt',
                       'shampoo-no-sqrt-Fisher',
                       'matrix-normal',
                       'matrix-normal-allVariables',
                       'matrix-normal-allVariables-warmStart',
                       'matrix-normal-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace',
                       'matrix-normal-same-trace-warmStart',
                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace-allVariables',
                       'matrix-normal-same-trace-allVariables-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep',
                       'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping']:
        params['if_double_grad'] = False
    else:
        print('error: need to check if_double_grad for ' + algorithm)
        sys.exit()
    
    algorithm = params['algorithm']

    if algorithm in ['SGD-VA',
                     'SGD-yura-BD',
                     'SGD-yura-old',
                     'SGD-yura-MA',
                     'SGD-yura',
                     'SGD-signVAsqrt',
                     'SGD-signVAerf',
                     'SGD-signVA',
                     'SGD-sign',
                     'SGD-momentum-yura',
                     'SGD-momentum',
                     'SGD',
                     'shampoo',
                     'shampoo-allVariables',
                     'shampoo-allVariables-warmStart',
                     'shampoo-allVariables-warmStart-lessInverse',
                     'shampoo-allVariables-filterFlattening-warmStart',
                     'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                     'shampoo-no-sqrt',
                     'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                     'RMSprop',
                     'RMSprop-warmStart',
                     'Adam',
                     'Adam-test',
                     'Adam-noWarmStart',
                     'RMSprop-no-sqrt',
                     'BFGS',
                     'BFGS-homo']:
        params['if_second_order_algorithm'] = False
    elif algorithm in ['SMW-Fisher-signVAsqrt-p',
                       'SMW-Fisher-VA-p',
                       'SMW-Fisher-momentum-p-sign',
                       'SMW-Fisher-momentum-p',
                       'SMW-Fisher-sign',
                       'SMW-Fisher-different-minibatch',
                       'SMW-Fisher',
                       'SMW-Fisher-batch-grad-momentum-exponential-decay',
                       'SMW-Fisher-momentum',
                       'ekfac-EF-VA',
                       'ekfac-EF',
                       'Fisher-BD',
                       'kfac-TR',
                       'kfac-CG',
                       'kfac-momentum-grad-CG',
                       'kfac-momentum-grad-TR',
                       'kfac',
                       'kfac-momentum-grad',
                       'kfac-no-max',
                       'kfac-NoMaxNoSqrt',
                       'kfac-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-no-LM',
                       'kfac-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-warmStart-lessInverse-no-max-no-LM',
                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-epsilon-A-G-no-LM',
                       'kfac-no-max-momentum-grad',
                       'kfac-EF',
                       'shampoo-no-sqrt-Fisher',
                       'shampoo-no-sqrt-Fisher-test',
                       'matrix-normal',
                       'matrix-normal-allVariables',
                       'matrix-normal-allVariables-warmStart',
                       'matrix-normal-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-momentum-grad',
                       'matrix-normal-LM-momentum-grad',
                       'matrix-normal-same-trace',
                       'matrix-normal-same-trace-warmStart',
                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace-allVariables',
                       'matrix-normal-same-trace-allVariables-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                       'SMW-Fisher-momentum-D_t-momentum',
                       'GI-Fisher',
                       'SMW-GN',
                       'SMW-Fisher-BD',
                       'RMSprop-individual-grad',
                       'RMSprop-individual-grad-no-sqrt',
                       'RMSprop-individual-grad-no-sqrt-Fisher',
                       'RMSprop-individual-grad-no-sqrt-LM',
                       'SMW-Fisher-batch-grad-momentum',
                       'SMW-Fisher-batch-grad',
                       'Kron-BFGS',
                       'Kron-BFGS-no-norm-gate',
                       'Kron-BFGS-no-norm-gate-momentum-s-y',
                       'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-no-norm-gate-damping',
                       'Kron-BFGS-homo-no-norm-gate-damping',
                       'Kron-BFGS-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Powell-H-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellBDamping',
                       'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep',
                       'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping',
                       'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping',
                       'Kron-BFGS-homo-identity',
                       'Kron-BFGS-wrong',
                       'Kron-BFGS-homo',
                       'Kron-BFGS-momentum-grad',
                       'Kron-BFGS-unregularized-grad',
                       'Kron-BFGS-unregularized-grad-momentum-grad',
                       'Kron-BFGS-Hessian-action',
                       'Kron-BFGS-wrong-Hessian-action',
                       'Kron-BFGS-Hessian-action-unregularized-grad',
                       'Kron-BFGS-Hessian-action-momentum-grad',
                       'Kron-BFGS-LM',
                       'Kron-BFGS-LM-sqrt',
                       'Kron-BFGS-LM-unregularized-grad',
                       'Kron-BFGS-LM-momentum-grad',
                       'Kron-BFGS-1st-layer-only',
                       'Kron-BFGS-block',
                       'Kron-SGD']:
        params['if_second_order_algorithm'] = True
    else:
        print('Error: unknown if_second_order_algorithm for ' + algorithm)
        sys.exit()

    if algorithm in ['SMW-Fisher-signVAsqrt-p',
                     'SMW-Fisher-VA-p',
                   'SMW-Fisher-momentum-p-sign',
                   'SMW-Fisher-momentum-p',
                   'SMW-Fisher-sign',
                   'SMW-Fisher-different-minibatch',
                   'SMW-Fisher',
                   'SMW-Fisher-batch-grad-momentum-exponential-decay',
                   'SMW-Fisher-momentum',
                   'SMW-Fisher-momentum-D_t-momentum',
                   'GI-Fisher',
                   'kfac-TR',
                   'kfac-CG',
                   'kfac-momentum-grad-CG',
                   'kfac-momentum-grad-TR',
                   'kfac-momentum-grad',
                   'kfac',
                   'kfac-test',
                   'kfac-no-max',
                   'kfac-NoMaxNoSqrt',
                   'kfac-no-max-momentum-grad',
                   'kfac-EF',
                   'SMW-GN',
                   'SMW-Fisher-BD',
                   'RMSprop-individual-grad-no-sqrt-LM',
                   'SMW-Fisher-batch-grad-momentum',
                   'SMW-Fisher-batch-grad',
                   'matrix-normal-LM-momentum-grad',
                   'matrix-normal-same-trace-LM-momentum-grad',]:
        params['if_LM'] = True
    elif algorithm in ['Kron-BFGS-LM',
                       'Kron-BFGS-LM-sqrt',
                       'Kron-BFGS-LM-unregularized-grad',
                       'Kron-BFGS-LM-momentum-grad']:
        
        print('error: should use Kron_BFGS_LM')
        
        sys.exit()
        
        params['if_LM'] = True
    elif algorithm in ['ekfac-EF-VA',
                       'ekfac-EF',
                       'SGD-VA',
                       'SGD-yura-BD',
                       'SGD-yura-old',
                       'SGD-yura-MA',
                       'SGD-yura',
                       'SGD-signVAsqrt',
                       'SGD-signVAerf',
                       'SGD-signVA',
                       'SGD-sign',
                       'SGD-momentum-yura',
                       'SGD-momentum',
                       'SGD',
                       'shampoo',
                       'shampoo-allVariables',
                       'shampoo-allVariables-warmStart',
                       'shampoo-allVariables-warmStart-lessInverse',
                       'shampoo-allVariables-filterFlattening-warmStart',
                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                       'shampoo-no-sqrt',
                       'shampoo-no-sqrt-Fisher',
                       'matrix-normal',
                       'matrix-normal-allVariables',
                       'matrix-normal-allVariables-warmStart',
                       'matrix-normal-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace',
                       'matrix-normal-same-trace-warmStart',
                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace-allVariables',
                       'matrix-normal-same-trace-allVariables-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                       'RMSprop',
                       'RMSprop-warmStart',
                       'Adam',
                       'Adam-test',
                       'Adam-noWarmStart',
                       'RMSprop-no-sqrt',
                       'RMSprop-individual-grad',
                       'Fisher-BD',
                       'kfac-no-max-no-LM',
                       'kfac-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-warmStart-lessInverse-no-max-no-LM',
                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-epsilon-A-G-no-LM',
                       'Kron-BFGS',
                       'Kron-BFGS-no-norm-gate',
                       'Kron-BFGS-no-norm-gate-momentum-s-y',
                       'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-no-norm-gate-damping',
                       'Kron-BFGS-homo-no-norm-gate-damping',
                       'Kron-BFGS-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Powell-H-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellBDamping',
                       'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep',
                       'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping',
                       'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping',
                       'Kron-BFGS-homo-identity',
                       'Kron-BFGS-wrong',
                       'Kron-BFGS-homo',
                       'Kron-BFGS-momentum-grad',
                       'Kron-BFGS-unregularized-grad',
                       'Kron-BFGS-unregularized-grad-momentum-grad',
                       'Kron-BFGS-Hessian-action',
                       'Kron-BFGS-wrong-Hessian-action',
                       'Kron-BFGS-Hessian-action-unregularized-grad',
                       'Kron-BFGS-Hessian-action-momentum-grad',
                       'Kron-BFGS-1st-layer-only',
                       'Kron-BFGS-block',
                       'Kron-SGD',
                       'BFGS',
                       'BFGS-homo']:
        params['if_LM'] = False
    elif algorithm == 'RMSprop-individual-grad-no-sqrt' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-Fisher':
        params['if_LM'] = False
    else:
        print('Error: unknown if_LM')
        sys.exit()

    if algorithm in ['SMW-Fisher-batch-grad-momentum-exponential-decay',
                     'SMW-Fisher-batch-grad',
                     'SMW-Fisher-batch-grad-momentum',
                     'shampoo-no-sqrt-Fisher',
                     'shampoo-no-sqrt-Fisher-test',
                     'matrix-normal',
                     'matrix-normal-allVariables',
                     'matrix-normal-allVariables-warmStart',
                     'matrix-normal-allVariables-warmStart-MaxEigDamping',
                     'matrix-normal-allVariables-warmStart-noPerDimDamping',
                     'matrix-normal-LM-momentum-grad',
                     'matrix-normal-same-trace',
                     'matrix-normal-same-trace-warmStart',
                     'matrix-normal-same-trace-warmStart-noPerDimDamping',
                     'matrix-normal-same-trace-allVariables',
                     'matrix-normal-same-trace-allVariables-warmStart',
                     'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                     'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                     'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                     'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                     'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                     'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',]:
        params['if_model_grad_N2'] = True
    elif algorithm in ['SGD-VA',
                       'SGD-signVAsqrt',
                       'SGD-signVAerf',
                       'SGD-signVA',
                       'SGD-sign',
                       'SGD-momentum-yura',
                       'SGD-momentum',
                       'SGD',
                       'SGD-yura-BD',
                       'SGD-yura-old',
                       'SGD-yura-MA',
                       'SGD-yura',
                       'SMW-Fisher-signVAsqrt-p',
                       'SMW-Fisher-VA-p',
                       'SMW-Fisher-momentum-p-sign',
                       'SMW-Fisher-momentum-p',
                       'SMW-Fisher-sign',
                       'SMW-Fisher-different-minibatch',
                       'SMW-Fisher',
                       'SMW-Fisher-momentum',
                       'SMW-GN',
                       'shampoo',
                       'shampoo-allVariables',
                       'shampoo-allVariables-warmStart',
                       'shampoo-allVariables-warmStart-lessInverse',
                       'shampoo-allVariables-filterFlattening-warmStart',
                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                       'shampoo-no-sqrt',
                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                       'RMSprop-individual-grad-no-sqrt-Fisher',
                       'RMSprop-individual-grad-no-sqrt',
                       'RMSprop-individual-grad',
                       'RMSprop',
                       'RMSprop-warmStart',
                       'Adam',
                       'Adam-test',
                       'Adam-noWarmStart',
                       'Fisher-BD',
                       'kfac-TR',
                       'kfac-CG',
                       'kfac-momentum-grad-CG',
                       'kfac-momentum-grad-TR',
                       'kfac-momentum-grad',
                       'kfac',
                       'kfac-no-max',
                       'kfac-NoMaxNoSqrt',
                       'kfac-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-no-LM',
                       'kfac-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-warmStart-lessInverse-no-max-no-LM',
                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-epsilon-A-G-no-LM',
                       'kfac-no-max-momentum-grad',
                       'kfac-EF',
                       'ekfac-EF',
                       'ekfac-EF-VA',
                       'Kron-BFGS',
                       'Kron-BFGS-no-norm-gate',
                       'Kron-BFGS-no-norm-gate-momentum-s-y',
                       'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-no-norm-gate-damping',
                       'Kron-BFGS-homo-no-norm-gate-damping',
                       'Kron-BFGS-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Powell-H-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellBDamping',
                       'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep',
                       'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping',
                       'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping',
                       'Kron-BFGS-homo-identity',
                       'Kron-BFGS-wrong',
                       'Kron-BFGS-homo',
                       'Kron-BFGS-LM',
                       'Kron-BFGS-LM-sqrt',
                       'Kron-BFGS-LM-unregularized-grad',
                       'Kron-BFGS-momentum-grad',
                       'Kron-BFGS-unregularized-grad',
                       'Kron-BFGS-unregularized-grad-momentum-grad',
                       'Kron-BFGS-LM-momentum-grad',
                       'Kron-BFGS-Hessian-action',
                       'Kron-BFGS-wrong-Hessian-action',
                       'Kron-BFGS-Hessian-action-unregularized-grad',
                       'Kron-BFGS-Hessian-action-momentum-grad',
                       'Kron-BFGS-1st-layer-only',
                       'Kron-BFGS-block',
                       'Kron-SGD',
                       'BFGS',
                       'BFGS-homo']:
        params['if_model_grad_N2'] = False
    else:
        print('Error: check if need model_grad_N2')
        sys.exit()

    

    if algorithm == 'ekfac-EF-VA' or\
    algorithm == 'ekfac-EF' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-LM' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt-Fisher' or\
    algorithm == 'RMSprop-individual-grad-no-sqrt' or\
    algorithm == 'RMSprop-individual-grad' or\
    algorithm == 'RMSprop-no-sqrt':
        params['lambda_'] = args['RMSprop_epsilon']
#         params['tau'] = 0
    elif algorithm in ['SMW-Fisher-signVAsqrt-p',
                       'SMW-Fisher-VA-p',
                       'SMW-Fisher-momentum-p-sign',
                       'SMW-Fisher-momentum-p',
                       'SMW-Fisher-sign',
                       'SMW-Fisher-different-minibatch',
                       'SMW-Fisher',
                       'SMW-Fisher-momentum',
                       'SMW-Fisher-batch-grad-momentum-exponential-decay',
                       'SMW-Fisher-batch-grad-momentum',
                       'SMW-Fisher-batch-grad',
                       'SMW-GN',
                       'kfac-TR',
                       'kfac-CG',
                       'kfac-momentum-grad-CG',
                       'kfac-momentum-grad-TR',
                       'kfac-momentum-grad',
                       'kfac',
                       'kfac-test',
                       'kfac-no-max',
                       'kfac-NoMaxNoSqrt',
                       'kfac-no-max-momentum-grad',
                       'kfac-EF',
                       'matrix-normal-LM-momentum-grad',
                       'matrix-normal-same-trace-LM-momentum-grad',
                       'Kron-BFGS-LM',
                       'Kron-BFGS-LM-sqrt',
                       'Kron-BFGS-LM-unregularized-grad',
                       'Kron-BFGS-LM-momentum-grad']:
        params['lambda_'] = args['lambda_']
#         params['tau'] = args['tau']
    elif algorithm in ['SGD-VA',
                       'SGD-yura-BD',
                       'SGD-yura-old',
                       'SGD-yura-MA',
                       'SGD-yura',
                       'SGD-signVAsqrt',
                       'SGD-signVAerf',
                       'SGD-signVA',
                       'SGD-sign',
                       'shampoo',
                       'shampoo-allVariables',
                       'shampoo-allVariables-warmStart',
                       'shampoo-allVariables-warmStart-lessInverse',
                       'shampoo-allVariables-filterFlattening-warmStart',
                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                       'shampoo-no-sqrt',
                       'shampoo-no-sqrt-Fisher',
                       'matrix-normal',
                       'matrix-normal-allVariables',
                       'matrix-normal-allVariables-warmStart',
                       'matrix-normal-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace',
                       'matrix-normal-same-trace-warmStart',
                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace-allVariables',
                       'matrix-normal-same-trace-allVariables-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                       'SGD-momentum-yura',
                       'SGD-momentum',
                       'SGD',
                       'RMSprop',
                       'RMSprop-warmStart',
                       'RMSprop-test',
                       'Adam',
                       'Adam-test',
                       'Adam-noWarmStart',
                       'Fisher-BD',
                       'kfac-no-max-no-LM',
                       'kfac-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-warmStart-lessInverse-no-max-no-LM',
                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-epsilon-A-G-no-LM',
                       'Kron-BFGS',
                       'Kron-BFGS-no-norm-gate',
                       'Kron-BFGS-no-norm-gate-momentum-s-y',
                       'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Powell-H-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellBDamping',
                       'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep',
                       'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping',
                       'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping',
                       'Kron-BFGS-homo-identity',
                       'Kron-BFGS-no-norm-gate-damping',
                       'Kron-BFGS-homo-no-norm-gate-damping',
                       'Kron-BFGS-wrong',
                       'Kron-BFGS-homo',
                       'Kron-BFGS-momentum-grad',
                       'Kron-BFGS-unregularized-grad',
                       'Kron-BFGS-unregularized-grad-momentum-grad',
                       'Kron-BFGS-Hessian-action',
                       'Kron-BFGS-wrong-Hessian-action',
                       'Kron-BFGS-Hessian-action-unregularized-grad',
                       'Kron-BFGS-Hessian-action-momentum-grad',
                       'Kron-BFGS-1st-layer-only',
                       'Kron-BFGS-block',
                       'Kron-SGD',
                       'BFGS',
                       'BFGS-homo']:
        1
    else:
        print('Error: need check lambda')
        sys.exit()

    if algorithm == 'SMW-Fisher-different-minibatch':
        params['if_different_minibatch'] = True
    elif algorithm in ['RMSprop-individual-grad-no-sqrt-LM',
                       'RMSprop-individual-grad-no-sqrt-Fisher',
                       'RMSprop-individual-grad-no-sqrt-EF',
                       'RMSprop-individual-grad-no-sqrt',
                       'RMSprop-individual-grad',
                       'RMSprop-no-sqrt',
                       'RMSprop',
                       'RMSprop-warmStart',
                       'RMSprop-test',
                       'Adam',
                       'Adam-test',
                       'Adam-noWarmStart',
                       'SMW-Fisher-signVAsqrt-p',
                       'SMW-Fisher-VA-p',
                       'SMW-Fisher-sign',
                       'SMW-Fisher-momentum-p-sign',
                       'SMW-Fisher-momentum-p',
                       'SMW-Fisher',
                       'SMW-Fisher-momentum',
                       'SMW-GN',
                       'shampoo-no-sqrt-Fisher',
                       'shampoo-no-sqrt-Fisher-test',
                       'matrix-normal',
                       'matrix-normal-allVariables',
                       'matrix-normal-allVariables-warmStart',
                       'matrix-normal-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-allVariables-warmStart-noPerDimDamping',
                       'matrix-normal-LM-momentum-grad',
                       'matrix-normal-same-trace',
                       'matrix-normal-same-trace-warmStart',
                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                       'matrix-normal-same-trace-allVariables',
                       'matrix-normal-same-trace-allVariables-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                       'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                       'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                       'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                       'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                       'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                       'ekfac-EF-VA',
                       'ekfac-EF',
                       'Fisher-BD',
                       'kfac-TR',
                       'kfac-CG',
                       'kfac-momentum-grad-CG',
                       'kfac-momentum-grad-TR',
                       'kfac-momentum-grad',
                       'kfac',
                       'kfac-no-max',
                       'kfac-NoMaxNoSqrt',
                       'kfac-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-momentum-grad',
                       'kfac-EF',
                       'kfac-no-max-no-LM',
                       'kfac-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-no-max-no-LM',
                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-warmStart-lessInverse-no-max-no-LM',
                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                       'kfac-no-max-epsilon-A-G-no-LM',
                       'SGD-VA',
                       'SGD-signVAsqrt',
                       'SGD-signVA',
                       'SGD-sign',
                       'Kron-BFGS',
                       'Kron-BFGS-no-norm-gate',
                       'Kron-BFGS-no-norm-gate-momentum-s-y',
                       'Kron-BFGS-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-damping',
                       'Kron-BFGS-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Shiqian-damping',
                       'Kron-BFGS-homo-no-norm-gate-Powell-H-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellBDamping',
                       'Kron-BFGS-homo-no-norm-gate-PowellHDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-PowellDoubleDampingV2',
                       'Kron-BFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchANotDamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-Sqrt',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-SqrtT',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2-KFACSplitting',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2-extraStep',
                       'Kron-(L)BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-miniBatchA-HessianActionV2IdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchA-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS(L)-homo-no-norm-gate-miniBatchADamped-HessianActionV2-momentum-s-y-DDV2',
                       'Kron-BFGS-homo-no-norm-gate-scaledHessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-HessianActionIdentityInitial-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-PowellDoubleDampingSkip',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-DoubleDamping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-H-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-B0-damping',
                       'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Shiqian-damping',
                       'Kron-(L)BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping',
                       'Kron-LBFGS-homo-no-norm-gate-momentum-s-y-Powell-double-damping',
                       'Kron-BFGS-homo-no-norm-gate-Hessian-action-Powell-double-damping',
                       'Kron-BFGS-homo-identity',
                       'Kron-BFGS-no-norm-gate-damping',
                       'Kron-BFGS-homo-no-norm-gate-damping',
                       'Kron-BFGS-wrong',
                       'Kron-BFGS-homo',
                       'Kron-BFGS-LM',
                       'Kron-BFGS-LM-sqrt',
                       'Kron-BFGS-LM-unregularized-grad',
                       'Kron-BFGS-momentum-grad',
                       'Kron-BFGS-unregularized-grad',
                       'Kron-BFGS-unregularized-grad-momentum-grad',
                       'Kron-BFGS-LM-momentum-grad',
                       'Kron-BFGS-Hessian-action',
                       'Kron-BFGS-wrong-Hessian-action',
                       'Kron-BFGS-Hessian-action-unregularized-grad',
                       'Kron-BFGS-Hessian-action-momentum-grad',
                       'Kron-BFGS-1st-layer-only',
                       'Kron-BFGS-block',
                       'Kron-SGD']:
        params['if_different_minibatch'] = False
    elif algorithm in ['SGD-signVAerf',
                       'SGD-yura-BD',
                       'SGD-yura-old',
                       'SGD-yura-MA',
                       'SGD-yura',
                       'shampoo',
                       'shampoo-allVariables',
                       'shampoo-allVariables-warmStart',
                       'shampoo-allVariables-warmStart-lessInverse',
                       'shampoo-allVariables-filterFlattening-warmStart',
                       'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                       'shampoo-no-sqrt',
                       'SGD-momentum-yura',
                       'SGD-momentum',
                       'SGD',
                       'BFGS',
                       'BFGS-homo']:
        1 
    else:
        print('Error: need check different minibatch')
        sys.exit()
        
    if algorithm == 'SGD-sign' or\
    algorithm == 'SMW-Fisher-sign':
        print('error: not supported.')
        sys.exit()
    else:
        params['if_sign'] = False

    if algorithm in ['SGD-momentum-yura',
                     'SMW-Fisher-momentum-p-sign',
                     'SMW-Fisher-momentum-p']:
        
        print('error: not supported.')
        sys.exit()
        
        params['if_momentum_p'] = True
    else:
        params['if_momentum_p'] = False

    if algorithm in ['SGD-VA',
                     'SMW-Fisher-VA-p']:
        
        print('error: not supported.')
        sys.exit()
        
        params['if_VA_p'] = True
    else:
        params['if_VA_p'] = False

    if algorithm in ['SMW-Fisher-signVAsqrt-p',
                     'SGD-signVAsqrt']:
        
        print('error: not supported.')
        sys.exit()
        
        params['if_signVAsqrt'] = True
    else:
        params['if_signVAsqrt'] = False

    if algorithm in ['SGD-signVA']:
        
        print('error: not supported.')
        sys.exit()
        
        params['if_signVA'] = True
    else:
        params['if_signVA'] = False

    if algorithm in ['SGD-yura',
                     'SGD-yura-MA',
                     'SGD-momentum-yura']:
        
        print('error: not supported.')
        sys.exit()
        
        params['if_yura'] = True
    else:
        params['if_yura'] = False

    params['if_signVAerf'] = False


    params['if_Adam'] = False
    


    params['keys_params_saved'] = []

    params['keys_params_saved'].append('if_test_mode')
    params['keys_params_saved'].append('tau')
    params['keys_params_saved'].append('seed_number')
    params['keys_params_saved'].append('num_threads')
    
    params['keys_params_saved'].append('initialization_pkg')
    params['keys_params_saved'].append('N1')
    params['keys_params_saved'].append('N2')
    
    params['keys_params_saved'].append('if_max_epoch')
    params['keys_params_saved'].append('max_epoch/time')
    
    params['keys_params_saved'].append('momentum_gradient_rho')
    params['keys_params_saved'].append('momentum_gradient_dampening')
    
    params['keys_params_saved'].append('if_grafting')
    
    params['keys_params_saved'].append('weight_decay')
    
    if params['if_lr_decay']:
        params['keys_params_saved'].append('num_epoch_to_decay')
        params['keys_params_saved'].append('lr_decay_rate')

    if params['algorithm'] == 'SGD-yura-MA':
        params['keys_params_saved'].append('yura_lambda_second_term_MA_weight')
        
        

    

    
    if params['algorithm'] in ['RMSprop',
                               'RMSprop-warmStart',
                               'RMSprop-test',
                               'Adam',
                               'Adam-test',
                               'Adam-noWarmStart']:
        params['keys_params_saved'].append('RMSprop_epsilon')
        params['keys_params_saved'].append('RMSprop_beta_2')
        
    import utils_git.utils_kfac as utils_kfac
    
    if params['algorithm'] in utils_kfac.list_algorithm:
        params = utils_kfac.get_saved_params_kfac(params)
        

        
#     import utils_git.utils_kbfgs as utils_kbfgs
    
#     if params['algorithm'] in utils_kbfgs.list_algorithm:
#         params = utils_kbfgs.get_saved_params_kbfgs(params, args)
    
    
    
        

    
#     if params['algorithm'] in ['matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
#                                'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',]:
#         params['keys_params_saved'].append('if_Hessian_action')
#     else:
#         print('params[algorithm]')
#         print(params['algorithm'])
#         sys.exit()
        
        
    if params['algorithm'] in ['shampoo',
                               'shampoo-allVariables',
                               'shampoo-allVariables-warmStart',
                               'shampoo-allVariables-warmStart-lessInverse',
                               'shampoo-allVariables-filterFlattening-warmStart',
                               'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                               'shampoo-no-sqrt',
                               'shampoo-no-sqrt-Fisher',
                               'matrix-normal',
                               'matrix-normal-allVariables',
                               'matrix-normal-allVariables-warmStart',
                               'matrix-normal-allVariables-warmStart-MaxEigDamping',
                               'matrix-normal-allVariables-warmStart-noPerDimDamping',
                               'matrix-normal-LM-momentum-grad',
                               'matrix-normal-same-trace',
                               'matrix-normal-same-trace-warmStart',
                               'matrix-normal-same-trace-warmStart-noPerDimDamping',
                               'matrix-normal-same-trace-allVariables',
                               'matrix-normal-same-trace-allVariables-warmStart',
                               'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                               'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                               'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping']:
        
        params['keys_params_saved'].append('shampoo_inverse_freq')
        params['keys_params_saved'].append('shampoo_update_freq')
        
        params['keys_params_saved'].append('shampoo_decay')
        params['keys_params_saved'].append('shampoo_weight')
        
        params['keys_params_saved'].append('if_Hessian_action')
        
        params['keys_params_saved'].append('shampoo_if_coupled_newton')
    

    if params['algorithm'] in ['shampoo',
                               'shampoo-allVariables',
                               'shampoo-allVariables-warmStart',
                               'shampoo-allVariables-warmStart-lessInverse',
                               'shampoo-allVariables-filterFlattening-warmStart',
                               'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                               'shampoo-no-sqrt',
                               'shampoo-no-sqrt-Fisher',
                               'matrix-normal',
                               'matrix-normal-allVariables',
                               'matrix-normal-allVariables-warmStart',
                               'matrix-normal-allVariables-warmStart-noPerDimDamping',
                               'matrix-normal-same-trace',
                               'matrix-normal-same-trace-warmStart',
                               'matrix-normal-same-trace-warmStart-noPerDimDamping',
                               'matrix-normal-same-trace-allVariables',
                               'matrix-normal-same-trace-allVariables-warmStart',
                               'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                               'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
        params['keys_params_saved'].append('shampoo_epsilon')
        
        
#     print('params[algorithm]')
#     print(params['algorithm'])
    
#     sys.exit()
    
    if params['algorithm'] == 'Fisher-BD':
        params['keys_params_saved'].append('Fisher_BD_damping')
    

    if algorithm in ['kfac-momentum-grad',
                       'kfac-EF']:
        params['algorithm'] = 'kfac'
    elif algorithm == 'kfac-no-max-momentum-grad':
        params['algorithm'] = 'kfac-no-max'
    elif algorithm in ['Kron-BFGS-momentum-grad',
                       'Kron-BFGS-unregularized-grad',
                       'Kron-BFGS-unregularized-grad-momentum-grad']:
        params['algorithm'] = 'Kron-BFGS'
    elif algorithm in ['Kron-BFGS-Hessian-action-momentum-grad']:
        params['algorithm'] = 'Kron-BFGS-Hessian-action'
    elif algorithm in ['Kron-BFGS-LM-momentum-grad',
                       'Kron-BFGS-LM-unregularized-grad']:
        params['algorithm'] = 'Kron-BFGS-LM'
    


    return params
File Path: utils_git/utils_kfac.py
Content:
import torch
import torch.nn.functional as F
import sys
import math
import copy

from utils_git.utils import get_homo_grad
from utils_git.utils import get_opposite
from utils_git.utils import from_homo_to_weight_and_bias
from utils_git.utils import get_A_A_T

list_algorithm = ['ekfac-EF-VA',
                 'ekfac-EF',
                 'kfac-TR',
                 'kfac-momentum-grad-TR',
                 'kfac-CG',
                 'kfac-momentum-grad-CG',
                 'kfac',
                 'kfac-no-max',
                 'kfac-NoMaxNoSqrt',
                 'kfac-NoMaxNoSqrt-no-LM',
                 'kfac-no-max-no-LM',
                 'kfac-warmStart-no-max-no-LM',
                 'kfac-warmStart-lessInverse-no-max-no-LM',
                 'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                 'kfac-correctFisher-warmStart-no-max-no-LM',
                 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                 'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                 'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                 'kfac-no-max-epsilon-A-G-no-LM',
                 'kfac-EF']

def get_saved_params_kfac(params):
    
    params['keys_params_saved'].append('kfac_if_svd')
    
    params['keys_params_saved'].append('kfac_if_update_BN')
    params['keys_params_saved'].append('kfac_if_BN_grad_direction')
    
    if params['algorithm'] in ['kfac-TR',
                               'kfac-momentum-grad-TR']:
        params['keys_params_saved'].append('TR_max_iter')
        
    if params['algorithm'] in ['kfac-CG', 
                               'kfac-momentum-grad-CG']:
        params['keys_params_saved'].append('CG_max_iter')
        
    if params['algorithm'] in ['kfac',
                               'kfac-momentum-grad',
                               'kfac-no-max',
                               'kfac-NoMaxNoSqrt',
                               'kfac-NoMaxNoSqrt-no-LM',
                               'kfac-no-max-no-LM',
                               'kfac-warmStart-no-max-no-LM',
                               'kfac-correctFisher-warmStart-no-max-no-LM',
                               'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                               'kfac-warmStart-lessInverse-no-max-no-LM',
                               'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                               'kfac-no-max-epsilon-A-G-no-LM',
                               'kfac-no-max-momentum-grad']:
        params['keys_params_saved'].append('kfac_inverse_update_freq')
        params['keys_params_saved'].append('kfac_cov_update_freq')
        
    if params['algorithm'] in ['kfac-no-max-no-LM',
                               'kfac-warmStart-no-max-no-LM',
                               'kfac-correctFisher-warmStart-no-max-no-LM',
                               'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                               'kfac-warmStart-lessInverse-no-max-no-LM',
                               'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                               'kfac-NoMaxNoSqrt-no-LM']:
        params['keys_params_saved'].append('kfac_damping_lambda')
        
    if params['algorithm'] == 'kfac-no-max-epsilon-A-G-no-LM':
        params['keys_params_saved'].append('kfac_A_epsilon')
        params['keys_params_saved'].append('kfac_G_epsilon')
    
    return params

def get_g_g_T_BN(model, l, batch_size):
    # In kfac code (https://github.com/tensorflow/kfac), they use a "sum the squares estimator"
    # (see the class ScaleAndShiftFullFB in https://github.com/tensorflow/kfac/blob/master/kfac/python/ops/fisher_blocks.py)
    # For more detail, see ScaleAndShiftFactor in https://github.com/tensorflow/kfac/blob/master/kfac/python/ops/fisher_factors.py
    
    # However, it is difficult to cache the intermediate variable of BN layer in pytorch.
    # Hence, we decide to use a "square the sum estimator", for simplicity
    
    g = torch.cat((model.layers_weight[l]['W'].grad.data, model.layers_weight[l]['b'].grad.data))
                    
    # g is averaged over minibatch
    # should first * batch_size, take outer product, then / batch_size 
    # i.e. "square the sum estimator" in kfac code
    # which is equivalent to batch_size * (g g^T)
    G_j = batch_size * torch.outer(g, g)
    
    return G_j

def kfac_if_inverse(params):
    
    i = params['i']
    inverse_update_freq = params['kfac_inverse_update_freq']
    cov_update_freq = params['kfac_cov_update_freq']
    
    if params['algorithm'] in ['kfac-TR', 'kfac-CG']:
        return False
    
    if params['algorithm'] in ['kfac-warmStart-lessInverse-no-max-no-LM',
                               'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',]:
        
        if i % inverse_update_freq == 0:
            return True
        else:
            return False
        
    elif params['algorithm'] in ['kfac-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',]:
        if (i <= inverse_update_freq and i % cov_update_freq == 0) or i % inverse_update_freq == 0:
            return True
        else:
            return False
    else:
        print('error: need to check for ' + params['algorithm'])
        sys.exit()
    
        

def kfac_update(data_, params):
    true_algorithm = params['algorithm']
    if params['algorithm'] == 'kfac-EF':
        params['algorithm'] = 'kfac'
    elif params['algorithm'] == 'kfac-momentum-grad-TR':
        params['algorithm'] = 'kfac-TR'
    elif params['algorithm'] == 'kfac-momentum-grad-CG':
        params['algorithm'] = 'kfac-CG'
        
    i = params['i']
        
    if i == 0:
        params['kfac_svd_failed'] = False

    A = data_['A']
    G = data_['G']
    model = data_['model']
    
#     model_grad = data_['model_regularized_grad_used_torch']
    model_grad = data_['model_grad_used_torch']
    
#     model_grad_N1 = data_['model_regularized_grad_torch']
    model_grad_N1 = data_['model_grad_torch']

    if params['algorithm'] == 'ekfac-EF-VA':
        U_A = data_['U_A']
        U_G = data_['U_G']
        ekfac_s = data_['ekfac_s']
        ekfac_m = data_['ekfac_m']
    elif params['algorithm'] == 'ekfac-EF':
        U_A = data_['U_A']
        U_G = data_['U_G']
        ekfac_s = data_['ekfac_s']
    elif params['algorithm'] in ['kfac',
                                 'kfac-test',
                                 'kfac-no-max',
                                 'kfac-NoMaxNoSqrt',
                                 'kfac-NoMaxNoSqrt-no-LM',
                                 'kfac-no-max-no-LM',
                                 'kfac-warmStart-no-max-no-LM',
                                 'kfac-warmStart-lessInverse-no-max-no-LM',
                                 'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                 'kfac-correctFisher-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                 'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                 'kfac-no-max-epsilon-A-G-no-LM']:
        
        if params['kfac_if_svd']:
            U_A = data_['U_A']
            U_G = data_['U_G']
            s_A = data_['s_A']
            s_G = data_['s_G']
        else:
        
            A_inv = data_['A_inv']
            G_inv = data_['G_inv']
    elif params['algorithm'] in ['kfac-TR',
                                 'kfac-CG']:
        1
    else:
        print('Error: unknown algo in kfac_update')
        sys.exit()

    
    
    N1 = params['N1']
    N2 = params['N2']
    
    
    
    if params['algorithm'] in ['kfac',
                               'kfac-no-max',
                               'kfac-NoMaxNoSqrt']:
        lambda_ = params['lambda_']
        lambda_A = math.sqrt(lambda_)
        lambda_G = math.sqrt(lambda_)
        
    elif params['algorithm'] in ['kfac-no-max-no-LM',
                                 'kfac-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                 'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                 'kfac-warmStart-lessInverse-no-max-no-LM',
                                 'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                 'kfac-NoMaxNoSqrt-no-LM']:
        lambda_ = params['kfac_damping_lambda']
        lambda_A = math.sqrt(lambda_)
        lambda_G = math.sqrt(lambda_)
    elif params['algorithm'] == 'kfac-no-max-epsilon-A-G-no-LM':
#         lambda_ = params['kfac_damping_lambda']
        lambda_A = params['kfac_A_epsilon']
        lambda_G = params['kfac_G_epsilon']
    else:
        print('error: need to check lambda for ' + params['algorithm'])
        sys.exit()
        
    
    
#     alpha = params['alpha']
    numlayers = params['numlayers']
    kfac_rho = params['kfac_rho']

    device = params['device']
    
    
    
#     a_grad_N2 = data_['a_grad_N2']
    h_N2 = data_['h_N2']
    
    # h denotes the bar_a in kfac paper, a_grad denotes the g
    G_ = []
    A_ = []
    

        
    if params['algorithm'] in ['kfac-warmStart-no-max-no-LM',
                               'kfac-correctFisher-warmStart-no-max-no-LM',
                               'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                               'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                               'kfac-warmStart-lessInverse-no-max-no-LM',
                               'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',]:
        rho = kfac_rho
    elif params['algorithm'] == 'kfac-no-max-no-LM':
        rho = min(1-1/(i+1), kfac_rho)
    else:
        print('error: not implemented for ' + params['algorithm'])
        sys.exit()


    # used in ekfac
    homo_model_grad_N1 = get_homo_grad(model_grad_N1, params)
    
    if params['if_momentum_gradient']:
        homo_model_grad = get_homo_grad(model_grad, params)
    else:
        homo_model_grad = homo_model_grad_N1

    if params['algorithm'] in ['kfac-TR',
                               'kfac-CG']:
        if i > 0:
            minus_p_prev = get_opposite(data_['p_torch'])
        else:
            minus_p_prev = data_['model_regularized_grad_used_torch']
        homo_minus_p_prev = get_homo_grad(minus_p_prev, params)
        
    cov_update_freq = params['kfac_cov_update_freq']
        
    # Step
    delta = []
    for l in range(numlayers):
        
        if i % cov_update_freq == 0:
        
            if params['layers_params'][l]['name'] in ['fully-connected',
                                                      'conv',
                                                      'conv-no-activation',
                                                      'conv-no-bias-no-activation']:

                G_.append(get_g_g_T(data_['a_N2'], l, params))

                # no need to save A_, can be improved
                A_.append(get_A_A_T(data_['h_N2'], l, data_, params))

                # Update running estimates of KFAC
                A[l].data = rho*A[l].data + (1-rho)*A_[l].data
                G[l].data = rho*G[l].data + (1-rho)*G_[l].data

            elif params['layers_params'][l]['name'] in ['BN']:

                A_.append([])
                
                if params['kfac_if_update_BN'] and not params['kfac_if_BN_grad_direction']:

                    G_.append(get_g_g_T_BN(model, l, N2))

                    G[l].data = rho*G[l].data + (1-rho)*G_[l].data
                    
                else:
                    G_.append([])
            else:
                print('Error: unknown layer when compute A: ' + params['layers_params'][l]['name'])
                sys.exit()

            
        

        
        # Amortize the inverse. Only update inverses every now and then
        if kfac_if_inverse(params):

            # phi_ = np.sqrt( ( np.trace(A[l].cpu().data.numpy()) / A[l].shape[0] )\
                # / np.maximum( np.trace(G[l].cpu().data.numpy()) / G[l].shape[0], 10**(-3) ) )
            if params['algorithm'] in ['kfac']:
        
                phi_ = torch.sqrt(
                           ( torch.sum(torch.diag(A[l])) / A[l].size()[0] )\
                / max( torch.sum(torch.diag(G[l])) / G[l].size()[0], 10**(-3) ) )
            elif params['algorithm'] in ['kfac-no-max',
                                         'kfac-no-max-no-LM',
                                         'kfac-no-max-epsilon-A-G-no-LM',
                                         'kfac-warmStart-no-max-no-LM',
                                         'kfac-warmStart-lessInverse-no-max-no-LM',
                                         'kfac-correctFisher-warmStart-no-max-no-LM',
                                         'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',]:
                
                phi_ = 1
                
            elif params['algorithm'] in ['kfac-NoMaxNoSqrt',
                                         'kfac-NoMaxNoSqrt-no-LM',
                                         'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                         'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                         'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',]:
                if params['layers_params'][l]['name'] != 'BN':
                    phi_ = torch.sqrt(
                           (torch.sum(torch.diag(A[l])) / A[l].size()[0] )\
                            / torch.sum(torch.diag(G[l])) / G[l].size()[0] 
                           )
                
            else:
                print('error: not implemented')
                sys.exit()
                        
            
            if not params['kfac_if_svd']:

                if params['layers_params'][l]['name'] == 'BN':
                    if params['kfac_if_update_BN'] and not params['kfac_if_BN_grad_direction']:
                        G_l_LM = G[l] + (1 / lambda_) * torch.eye(G[l].size()[0], device=device)
                else:

                    A_l_LM = A[l] + (phi_ * lambda_A) * torch.eye(A[l].size()[0], device=device)

                    G_l_LM = G[l] + (1 / phi_ * lambda_G) * torch.eye(G[l].size()[0], device=device)

            if params['algorithm'] in ['kfac',
                                       'kfac-no-max',
                                       'kfac-NoMaxNoSqrt',
                                       'kfac-NoMaxNoSqrt-no-LM',
                                       'kfac-no-max-no-LM',
                                       'kfac-warmStart-no-max-no-LM',
                                       'kfac-correctFisher-warmStart-no-max-no-LM',
                                       'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                       'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                       'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                       'kfac-warmStart-lessInverse-no-max-no-LM',
                                       'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                       'kfac-no-max-epsilon-A-G-no-LM']:
                # inverse() is not working properly on gpu, so use cpu
                # A_l_LM_cpu = A_l_LM.cpu()
                # A_inv_l_cpu = A_l_LM_cpu.inverse()
                # A_inv[l] = A_inv_l_cpu.to(device)

                # G_l_LM_cpu = G_l_LM.cpu()
                # G_inv_l_cpu = G_l_LM_cpu.inverse()
                # G_inv[l] = G_inv_l_cpu.to(device)
                
                if params['kfac_if_svd']:
                    
                    if params['layers_params'][l]['name'] == 'BN':
                        
                        if params['kfac_if_update_BN'] and not params['kfac_if_BN_grad_direction']:
                            
                            try:
                                s_G[l], U_G[l] = torch.symeig(G[l].data, eigenvectors=True)
                            except:
        #                         s_G[l], U_G[l] = torch.symeig(G[l].data.cpu(), eigenvectors=True)
        #                         s_G[l], U_G[l] = s_G[l].to(device), U_G[l].to(device)

                                print('svd faild in G')

                                params['kfac_svd_failed'] = True
                        
                    else:
                    
                        # U * diag{s} * U.t() = A (or G)
    #                     s_A[l], U_A[l] = torch.symeig(A[l].data, eigenvectors=True)
    #                     s_G[l], U_G[l] = torch.symeig(G[l].data, eigenvectors=True)

                        try:
                            s_A[l], U_A[l] = torch.symeig(A[l].data, eigenvectors=True)

    #                         s_A[l] = s_A[l] * (s_A[l] > 1e-10).float()

                        except:
    #                         s_A[l], U_A[l] = torch.symeig(A[l].data.cpu(), eigenvectors=True)

    #                         s_A[l], U_A[l] = s_A[l].to(device), U_A[l].to(device)

                            print('l')
                            print(l)

                            print('svd faild in A')


                            params['kfac_svd_failed'] = True

                        try:
                            s_G[l], U_G[l] = torch.symeig(G[l].data, eigenvectors=True)
                        except:
    #                         s_G[l], U_G[l] = torch.symeig(G[l].data.cpu(), eigenvectors=True)
    #                         s_G[l], U_G[l] = s_G[l].to(device), U_G[l].to(device)

                            print('svd faild in G')

                            params['kfac_svd_failed'] = True
                    
                else:
                
                    if params['layers_params'][l]['name'] == 'BN':
                        if params['kfac_if_update_BN'] and not params['kfac_if_BN_grad_direction']:
                            G_inv[l] = G_l_LM.inverse()
                    else:
                        A_inv[l] = A_l_LM.inverse()
                        G_inv[l] = G_l_LM.inverse()
            elif params['algorithm'] == 'ekfac-EF-VA' or\
            params['algorithm'] == 'ekfac-EF':

                # symeig is used in ekfac's code:
                # https://github.com/Thrandis/EKFAC-pytorch/blob/master/ekfac.py
                # A = U_A * s * U_A.t()

                # use cpu to do eigen-decomp to match kfac
                # A_l_LM_cpu = A_l_LM.cpu()
                # _, U_A[l] = torch.symeig(A_l_LM_cpu, eigenvectors=True)

                # G_l_LM_cpu = G_l_LM.cpu()
                # _, U_G[l] = torch.symeig(G_l_LM_cpu, eigenvectors=True)

                # U_A[l] = U_A[l].to(device)
                # U_G[l] = U_G[l].to(device)

                _, U_A[l] = torch.symeig(A_l_LM, eigenvectors=True)
                _, U_G[l] = torch.symeig(G_l_LM, eigenvectors=True)

                
                
                
                
            else:
                print('Error: unknown algo when inverse')
                sys.exit()



        # update scaling
        if params['algorithm'] == 'ekfac-EF-VA' or\
        params['algorithm'] == 'ekfac-EF':
            if params['layers_params'][l]['name'] == 'fully-connected':
                # homo_s_l = torch.mm(torch.mm(U_G[l].t(), homo_model_grad), U_A[l].t())
                homo_s_l_N1 = torch.mm(torch.mm(U_G[l].t(), homo_model_grad_N1[l]), U_A[l].t())

                ekfac_s[l]['W'] = rho * ekfac_s[l]['W'] + (1 - rho) * (homo_s_l_N1[:, :-1]**2).data
                ekfac_s[l]['b'] = rho * ekfac_s[l]['b'] + (1 - rho) * (homo_s_l_N1[:, -1]**2).data

                if params['algorithm'] == 'ekfac-EF-VA':
                    ekfac_m[l]['W'] = rho * ekfac_m[l]['W'] + (1 - rho) * homo_s_l_N1[:, :-1].data
                    ekfac_m[l]['b'] = rho * ekfac_m[l]['b'] + (1 - rho) * homo_s_l_N1[:, -1].data

                # if np.isnan((ekfac_s[l]['W']).max()):
                    # sys.exit()
                    
                print('error: check why there is a to device')
                sys.exit()

                homo_s_l_N1 = homo_s_l_N1.to(device)

                # if torch.isnan(torch.max(homo_s_l)):
                    # sys.exit()
            else:
                print('Error: unsupported layer when update scaling')
                sys.exit()
            
        # compute kfac direction
        if params['algorithm'] in ['kfac',
                                   'kfac-no-max',
                                   'kfac-NoMaxNoSqrt',
                                   'kfac-NoMaxNoSqrt-no-LM',
                                   'kfac-no-max-no-LM',
                                   'kfac-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                   'kfac-warmStart-lessInverse-no-max-no-LM',
                                   'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                   'kfac-no-max-epsilon-A-G-no-LM']:
            
            if params['kfac_if_svd']:
                
                if params['layers_params'][l]['name'] == 'BN':
                    
                    if params['kfac_if_update_BN']:
                
                        if params['kfac_if_BN_grad_direction']:
                            homo_delta_l = copy.deepcopy(homo_model_grad[l])
                            
#                             if l == 1:
                            
#                                 print('homo_delta_l')
#                                 print(homo_delta_l)
                            
                        else:
                            print('error: not implemented')
                            sys.exit()
                        
                    else:
                        homo_delta_l = torch.zeros(homo_model_grad[l].size(), device=device)
                        
                    
                else:
                    homo_delta_l = torch.mm(
                        torch.mm(U_G[l].t(), homo_model_grad[l]),
                        U_A[l]
                    )

                    homo_delta_l = homo_delta_l / (torch.outer(s_G[l], s_A[l]) + params['kfac_damping_lambda'])

                    homo_delta_l = torch.mm(
                        torch.mm(U_G[l], homo_delta_l),
                        U_A[l].t()
                    )
                
                # G^{-1} * grad * A^{-1}
                # = (U_G * diag{s_G} * U_G.t())^{-1} * grad * (U_A * diag{s_A} * U_A.t())^{-1}
                # = U_G * diag{s_G}^{-1} * U_G.t() * grad * U_A * diag{s_A}^{-1} * U_A.t()
                
                
                
                
                
                
            else:
            
                if params['layers_params'][l]['name'] == 'BN':

                    if params['kfac_if_update_BN']:
                        
                        if params['kfac_if_BN_grad_direction']:
                            homo_delta_l = copy.deepcopy(homo_model_grad[l])
                        else:

                            homo_delta_l = torch.mv(G_inv[l], homo_model_grad[l])
                    else:
                        homo_delta_l = torch.zeros(homo_model_grad[l].size(), device=device)
                else:

                    homo_delta_l = torch.mm(torch.mm(G_inv[l], homo_model_grad[l]), A_inv[l])

                if params['if_test_mode']:
                    if 'if_record_kfac_G_inv_norm_per_iter' in params and\
                    params['if_record_kfac_G_inv_norm_per_iter']:

                        if l == 0:
                            data_['kfac_G_inv_norms_per_iter'].append([])

                        data_['kfac_G_inv_norms_per_iter'][-1].append(torch.norm(G_inv[l]).item())
            
            
        elif params['algorithm'] == 'kfac-TR':
            # objective function: 1 / 2 * x^T F x - dot(g, x)
            # constaint: ||x||^2 \le 1 / \lambda

            # use beck's method to compute global optimal
            # diferent initializations for easy case and hard case

            # objective function: (x.T).dot(hess).dot(x) / 2 + (grad.T).dot(x)

            # input:
            # hess: n * n
            # grad: n * 1

            # radius = 1. / lambda_
            radius = 1000. / lambda_
            
            # this configuration is for testing 
            # n = np.max(np.shape(hess))
            x_easy = homo_minus_p_prev[l]
            # x_hard = np.random.randn(n, 1)
            

            phi_ = torch.sqrt(\
                           ( torch.sum(torch.diag(A[l])) / A[l].size()[0] )\
                / max( torch.sum(torch.diag(G[l])) / G[l].size()[0], 10**(-3) ) )
            

            A_l_LM = A[l] + (phi_ * math.sqrt(lambda_)) * torch.eye(A[l].size()[0], device=device)
            G_l_LM = G[l] + (1 / phi_ * math.sqrt(lambda_)) * torch.eye(G[l].size()[0], device=device)

            # if torch.sum(G_l_LM != G_l_LM):
                # print('nan in G_l_LM')
                # print('G[l]')
                # print(G[l])
                # print('phi_')
                # print(phi_)
                # print('lambda_')
                # print(lambda_)
            
            iter_ = 0
            while iter_ < params['TR_max_iter']:
                
                iter_ = iter_ + 1

                # easy case
                # compute the gradient
                
                # temp_grad_easy = torch.mm(torch.mm(G[l], x_easy), A[l]) - homo_model_grad[l]
                temp_grad_easy =\
                torch.mm(torch.mm(G_l_LM, x_easy), A_l_LM) - homo_model_grad[l]

                if params['if_test_mode']:
                    if params['if_record_res_grad_norm_per_iter']:
                        if l == 0 and iter_ == 1:
                            data_['res_grad_norms_per_iter'].append([])
                        if iter_ == 1:
                            data_['res_grad_norms_per_iter'][-1].append([])

                        # print('data_[res_grad_norms_per_iter]')
                        # print(data_['res_grad_norms_per_iter'])

                        data_['res_grad_norms_per_iter'][-1][-1].append(
                            np.sqrt(torch.sum(temp_grad_easy**2).item())
                        )

                if params['if_test_mode']:
                    if torch.sum(temp_grad_easy != temp_grad_easy):
                        print('nan in temp_grad_easy')
                        print('G_l_LM')
                        print(G_l_LM)
                        print('x_easy')
                        print(x_easy)
                        print('A_l_LM')
                        print(A_l_LM)
                        print('homo_model_grad[l]')
                        print(homo_model_grad[l])
                    if torch.sum(temp_grad_easy == float('inf')):
                        print('inf in temp_grad_easy')
                        print('G_l_LM')
                        print(G_l_LM)
                        print('x_easy')
                        print(x_easy)
                        print('A_l_LM')
                        print(A_l_LM)
                        print('homo_model_grad[l]')
                        print(homo_model_grad[l])

                # lr_ = 1. / (iter_ + 10)
                lr_ = 1. / (torch.norm(G_l_LM) * torch.norm(A_l_LM))
                x_easy = x_easy - lr_ * temp_grad_easy
                
                
                # if l == 1:
                # print('l')
                # print(l)
                # print('iter_')
                # print(iter_)
                # print('1 / (iter_ + 10)')
                # print(1 / (iter_ + 10))
                # print('1 / (torch.norm(G_l_LM) * torch.norm(A_l_LM))')
                # print(1 / (torch.norm(G_l_LM) * torch.norm(A_l_LM)))
                # print('torch.norm(x_easy)')
                # print(torch.norm(x_easy))
                # print('torch.norm(temp_grad_easy)')
                # print(torch.norm(temp_grad_easy))
                # print('torch.sum(x_easy * torch.mm(torch.mm(G_l_LM, x_easy), A_l_LM))\
                        # - torch.sum(x_easy * homo_model_grad[l])')
                # print(torch.sum(x_easy * torch.mm(torch.mm(G_l_LM, x_easy), A_l_LM))\
                        # - torch.sum(x_easy * homo_model_grad[l]))

                if params['if_test_mode']:
                    if torch.sum(x_easy != x_easy):
                        print('nan in x_easy')
                        print('x_easy')
                        print(x_easy)
                        print('temp_grad_easy')
                        print(temp_grad_easy)

                '''
                temp_norm = torch.norm(x_easy)
                # projection
                if temp_norm > radius:
                    if temp_norm < 0.0001:
                        # restart if the norm is too small
                        x_easy = np.zeros((n, 1))
                    else:

                        x_easy = x_easy / temp_norm * radius
                '''

                

                # hard case
                # compute the gradient
                # temp_grad_hard = hess.dot(x_hard) + grad
                # x_hard = x_hard - temp_grad_hard / (iter + 10)
                # temp_norm = np.linalg.norm(x_hard)
                # if temp_norm > radius:
                    # x_hard = x_hard / temp_norm * radius
                
                # easy_value = (x_easy.T).dot(hess).dot(x_easy) / 2 + (grad.T).dot(x_easy)
                # hard_value = (x_hard.T).dot(hess).dot(x_hard) / 2 + (grad.T).dot(x_hard)
            
            # if easy_value < hard_value:
            homo_delta_l = x_easy
            # else:
                # return x_hard

            if params['if_test_mode']:
                if torch.sum(homo_delta_l!=homo_delta_l):
                    print('nan in homo_delta_l')
                    # sys.exit()

                if params['if_record_kfac_p_norm']:
                    kfac_homo_delta_l = torch.mm(
                        torch.mm(G_l_LM.cpu().inverse().to(device), homo_model_grad[l]),
                        A_l_LM.cpu().inverse().to(device))
                    if l == 0:
                        data_['kfac_p_norms'].append(
                            torch.sqrt(torch.sum(kfac_homo_delta_l**2)).item()
                        )
                    else:
                        data_['kfac_p_norms'][-1] =\
                        np.sqrt(data_['kfac_p_norms'][-1]**2 +\
                                torch.sum(kfac_homo_delta_l**2).item())
                if params['if_record_kfac_p_cosine']:
                    kfac_p_cosine = (torch.sum(kfac_homo_delta_l * homo_delta_l) /\
                            torch.norm(kfac_homo_delta_l) / torch.norm(homo_delta_l)).item()
                    if l == 0:
                        data_['kfac_p_cosines'].append([])
                    data_['kfac_p_cosines'][-1].append(kfac_p_cosine)
                if params['if_record_res_grad_norm']:
                    res_grad_norm = np.sqrt(torch.sum(temp_grad_easy**2).item())
                    if l == 0:
                        data_['res_grad_norms'].append([])
                    data_['res_grad_norms'][-1].append(res_grad_norm)
                if params['if_record_res_grad_random_norm']:
                    temp_grad_random = torch.mm(torch.mm(G_l_LM, torch.randn(x_easy.size(), device=device)), A_l_LM) - homo_model_grad[l]
                    res_grad_random_norm = np.sqrt(torch.sum(temp_grad_random**2).item())
                    if l == 0:
                        data_['res_grad_random_norms'].append([])
                    data_['res_grad_random_norms'][-1].append(res_grad_random_norm)
                if params['if_record_res_grad_grad_norm']:
                    temp_grad_grad = torch.mm(torch.mm(G_l_LM, homo_model_grad[l]), A_l_LM) - homo_model_grad[l]
                    res_grad_grad_norm = np.sqrt(torch.sum(temp_grad_grad**2).item())
                    if l == 0:
                        data_['res_grad_grad_norms'].append([])
                    data_['res_grad_grad_norms'][-1].append(res_grad_grad_norm)
                        

        elif params['algorithm'] == 'kfac-CG':
            phi_ = torch.sqrt(\
                           ( torch.sum(torch.diag(A[l])) / A[l].size()[0] )\
                / max( torch.sum(torch.diag(G[l])) / G[l].size()[0], 10**(-3) ) )
            

            A_l_LM = A[l] + (phi_ * math.sqrt(lambda_)) * torch.eye(A[l].size()[0], device=device)
            G_l_LM = G[l] + (1 / phi_ * math.sqrt(lambda_)) * torch.eye(G[l].size()[0], device=device)
            # A, G are without LM

            func_A = lambda x: torch.mm(torch.mm(G_l_LM, x), A_l_LM)
            # data_['G_l_LM'] = G_l_LM
            # data_['A_l_LM'] = A_l_LM
            homo_delta_l = get_CG(func_A, homo_model_grad[l], homo_minus_p_prev[l],
                                  params['CG_max_iter'], data_)




        elif params['algorithm'] == 'ekfac-EF-VA' or\
        params['algorithm'] == 'ekfac-EF':
            if params['if_momentum_gradient']:
                homo_s_l_used = homo_s_l_N1
            else:
                
                print('error: should U_A be transposed?')
                
                print('error: check why there is a to device')
                sys.exit()
                
                homo_s_l_used = torch.mm(torch.mm(U_G[l].t(), homo_model_grad[l]), U_A[l].t()).to(device)

            homo_delta_l = homo_s_l_used /\
            (torch.cat((ekfac_s[l]['W'], ekfac_s[l]['b'].unsqueeze(1)), dim=1) +\
             10**(-4))
            
            if params['algorithm'] == 'ekfac-EF-VA':
                homo_delta_l = homo_delta_l *\
                (torch.cat((ekfac_m[l]['W'],
                       ekfac_m[l]['b'].unsqueeze(1)), dim=1))**2

                       

            homo_delta_l = torch.mm(torch.mm(U_G[l], homo_delta_l), U_A[l])
        
        else:
            print('Error: unknown algo when compute direction')
            sys.exit()
            
        
        
        

#         # store the delta
#         delta_l = {}
#         if params['layers_params'][l]['name'] == 'fully-connected':
#             delta_l['W'] = homo_delta_l[:, :-1]
#             delta_l['b'] = homo_delta_l[:, -1]
#         elif params['layers_params'][l]['name'] == 'conv':
#             # take Fashion-MNIST as an example
#             # model_grad_N1[l]['W']: 32 * 1 * 5 * 5
#             # model_grad_N1[l]['b']: 32
#             # 32: conv_out_channels
#             # 1: conv_in_channels
#             # 5 * 5: conv_kernel_size
#             delta_l['b'] = homo_delta_l[:, -1]
#             delta_l['W'] = homo_delta_l[:, :-1].reshape(model_grad_N1[l]['W'].size())  
#         else:
#             print('Error: unsupported layer when store the data for ' + params['layers_params'][l]['name'])
#             sys.exit()
            
        delta_l = from_homo_to_weight_and_bias(homo_delta_l, l, params)
        
#         sys.exit()
        

            
            
        delta.append(delta_l)
        



    ##############
    algorithm = params['algorithm']
    if algorithm == 'Fisher-block':
        
        print('error: should not reach here; use SMW-Fisher-BD instead')
        sys.exit()
        
        delta = []
        for l in range(numlayers):
            
#             print('print(model_grad[l])', model_grad[l])

            params['algorithm'] = 'SMW-Fisher'
    
            data_test, _ = SMW_Fisher_update(data_, params)
    
#             print('SMW_Fisher_update(data_, params)', SMW_Fisher_update(data_, params))
            

            p = data_test['p']
        
            delta.append(-p)
        
            params['algorithm'] = 'Fisher-block'
        

    
    

    p = get_opposite(delta)
        

        
    
    
    

    
        
    data_['A'] = A
    data_['G'] = G 
    # A, G are without LM


    
    if params['algorithm'] == 'ekfac-EF-VA':
        data_['U_A'] = U_A
        data_['U_G'] = U_G
        data_['ekfac_s'] = ekfac_s
        data_['ekfac_m'] = ekfac_m
    elif params['algorithm'] == 'ekfac-EF':
        data_['U_A'] = U_A
        data_['U_G'] = U_G
        data_['ekfac_s'] = ekfac_s
    elif params['algorithm'] in ['kfac',
                                 'kfac-no-max',
                                 'kfac-NoMaxNoSqrt',
                                 'kfac-NoMaxNoSqrt-no-LM',
                                 'kfac-no-max-no-LM',
                                 'kfac-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-NoMaxNoSqrt-no-LM',
                                 'kfac-correctFisher-warmStart-lessInverse-no-max-no-LM',
                                 'kfac-correctFisher-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                 'kfac-warmStart-lessInverse-no-max-no-LM',
                                 'kfac-warmStart-lessInverse-NoMaxNoSqrt-no-LM',
                                 'kfac-no-max-epsilon-A-G-no-LM']:
        
        if params['kfac_if_svd']:
            
            data_['U_A'] = U_A
            data_['U_G'] = U_G
            data_['s_A'] = s_A
            data_['s_G'] = s_G
            
        else:
        
            data_['A_inv'] = A_inv
            data_['G_inv'] = G_inv
    elif params['algorithm'] in ['kfac-TR',
                                 'kfac-CG']:
        1
    else:
        print('Error: unknown algo when save momentum data')
        sys.exit()
    
    data_['p_torch'] = p

    if true_algorithm in ['kfac-EF',
                          'kfac-momentum-grad-CG',
                          'kfac-momentum-grad-TR']:
        params['algorithm'] = true_algorithm
        
        
    data_['a_grad_N2'] = None
    data_['a_N2'] = None
    data_['h_N2'] = None
    
    
        
    return data_, params

def get_g_g_T(a, l, params):
    # returns the AVERAGED g_g_T across a minibatch
    
    layers_params = params['layers_params']
    
    if layers_params[l]['name'] == 'fully-connected':
        
        size_minibatch = a[l].size(0)
        
        
        # we use "size_minibatch *", instead of "1/size_minibatch *"
        # because a[l].grad is actually "1/size_minibatch * a[l].grad"
        G_j = size_minibatch * torch.mm(a[l].grad.t(), a[l].grad).data
        
    elif layers_params[l]['name'] in ['conv',
                                      'conv-no-activation',
                                      'conv-no-bias-no-activation']:
        # take Fashion-MNIST as an example:
        # a[l]: 1000 * 32 * 28 * 28
        # 1000: size of minibatch
        # 32: # out-channel
        # 28 * 28: size of image
        
        # return 1 / |T| / size_minibatch * g_g_T
        

        
        size_minibatch = a[l].size(0)
        
        
        
        
        # a[l].grad is actually "1/size_minibatch * a_l_grad"
        a_l_grad = size_minibatch * a[l].grad
        
        a_l_grad_permuted = a_l_grad.permute(1, 0, 2, 3)
        
#         print('a_l_permuted.size()')
#         print(a_l_permuted.size())
        
        a_l_grad_flattened = torch.flatten(a_l_grad_permuted, start_dim=1)
        
        G_j = torch.mm(a_l_grad_flattened, a_l_grad_flattened.t()) / a_l_grad_flattened.size(1)
    else:
        print('error: not implemented for ' + layers_params[l]['name'])
        sys.exit()
        
    return G_j




File Path: utils_git/utils_plot.py
Content:
import pickle
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import datetime
import math



# from utils_git.utils import *
from utils_git.utils import get_name_loss
from utils_git.utils import get_best_params
from utils_git.utils import get_name_algorithm_with_params
from utils_git.utils import from_dataset_to_N1_N2

def get_mean_and_scaled_error(list_data, key, name_loss, if_max_epoch):
    
#     list_curves = []
    list_curves_raw = []
    for data_ in list_data:
        
        list_curves_raw.append(data_[key])
        
#     print('if_max_epoch')
#     print(if_max_epoch)
    
#     sys.exit()
    
    if if_max_epoch:
        
        sys.exit()
        
        len_ = max([len(curve) for curve in list_curves_raw])
        
    else:
        len_ = min([len(curve) for curve in list_curves_raw])
        
 
    
    
    print('len_')
    print(len_)
    
    list_curves = []
#     for i in range(len(list_curves_raw)):
    for curve in list_curves_raw:

        if if_max_epoch:
        
            sys.exit()

            if len(curve) == len_:
                list_curves.append(curve)
                
        else:
            list_curves.append(curve[:len_])
            
#             list_curves[i] = list_curves[i]
        
    
    for curve in list_curves:
        
        assert len(curve) == len(list_curves[0])
        
    list_curves = np.asarray(list_curves)
    
    print('key')
    print(key)
    
    if key in ['train_unregularized_minibatch_losses',
               'epochs',
               'timesCPU']:
        pass
    elif key == 'test_acces':
        

        
        if name_loss in ['logistic-regression-sum-loss',
                         'linear-regression-half-MSE',]:
            pass
        elif name_loss in ['multi-class classification',]:
            list_curves = 1 - list_curves
        else:
            print('name_loss')
            print(name_loss)
            sys.exit()
        
            
    else:
        sys.exit()
        
#     print('list_curves')
#     print(list_curves)
        
#     print('len(list_curves)')
#     print(len(list_curves))
        
#     print('math.sqrt(len(list_curves))')
#     print(math.sqrt(len(list_curves)))
    
#     sys.exit()
    
#     return np.mean(list_curves, axis=0), np.std(list_curves, axis=0)
    return np.mean(list_curves, axis=0), np.std(list_curves, axis=0) / math.sqrt(len(list_curves))

def get_plot_seed_result(z_value, dataset, algorithms, if_max_epoch, list_y, path_to_home, if_title):
    
#     plt.rcParams['xtick.labelsize']=20
#     plt.rcParams['ytick.labelsize']=20

    plt.rcParams.update({'font.size': 22})

    list_y_legend = []
    for key_y in list_y:        
        if key_y == 'train_unregularized_minibatch_losses':
            list_y_legend.append('train loss')
        elif key_y == 'test_acces':
            list_y_legend.append('val error')
        else:
            print('key_y')
            print(key_y)
        
            sys.exit()

    list_x = ['epochs', 'timesCPU']
    list_x_legend = ['epoch', 'process time (second)']

    # plt.figure(figsize=(30,40))

#     fig, axs = plt.subplots(len(list_y), len(list_x), figsize=(15,15))
    fig, axs = plt.subplots(len(list_y), len(list_x), figsize=(15,7.5*len(list_y)))

    fake_args = {'dataset': dataset}
    from_dataset_to_N1_N2(fake_args)

    N1 = fake_args['N1']
    N2 = fake_args['N2']

    for algorithm in algorithms:

    #     algorithm = 'Kron-BFGS-homo-no-norm-gate-miniBatchADamped-HessianActionV2-\
    #     momentum-s-y-DDV2-regularized-grad-momentum-grad'

        algorithm_name = algorithm['name']
        algorithm_name_legend = algorithm['name_legend']
        lr = algorithm['lr']






#         path_to_home = '/rigel/home/yr2322/gauss_newton/'

        path_to_dir = path_to_home + 'result/' +\
        dataset + '/' +\
        algorithm_name + '/if_gpu_True/alpha_' + str(lr) +\
        '/N1_' + str(N1) + '/N2_' + str(N2) + '/'

        # os.listdir(path_to_dir)

        list_data = []

        for file in os.listdir(path_to_dir):

            with open(path_to_dir + file, 'rb') as fp:
                data_ = pickle.load(fp)

#             if data_['params']['if_max_epoch'] == 0 and\
#             data_['params']['if_test_mode'] == False:
            if data_['params']['if_test_mode'] == False:
                
                flag = True
                
                if 'params' in algorithm:
                    for key in algorithm['params']:
                        if key not in data_['params']:
                            flag = False
                            break
                        if algorithm['params'][key] != data_['params'][key]:
                            flag = False
                            break
                    

                if flag:
                    
                    print('data_[params]')
                    print(data_['params'])
                    
                    list_data.append(data_)

        print('len(list_data)')
        print(len(list_data))





    #     assert len(list_data) > 0
        assert len(list_data) == 5

        for j in range(len(list_x)):

            key_x = list_x[j]
            
            name_loss = get_name_loss(dataset)

            mean_x, _ = get_mean_and_scaled_error(list_data, key_x, name_loss, if_max_epoch)

            for i in range(len(list_y)):

                # compute the mean curve and the error bar
                key_y = list_y[i]
                
                


#                 mean_y, error = get_mean_and_error(list_data, key_y, name_loss)
                mean_y, scaled_error = get_mean_and_scaled_error(list_data, key_y, name_loss, if_max_epoch)
                
                
                if key_y == 'test_acces':
                    
                    print('1 - np.min(mean_y)')
                    print(1 - np.min(mean_y))
            
#                 print('i, j')
#                 print(i, j)
                
#                 print('axs')
#                 print(axs)
                
                if len(list_y) == 1:
                    ax = axs[j]
                else:
                    ax = axs[i, j]

                # see https://www.mathsisfun.com/data/confidence-interval.html
                ax.plot(mean_x, mean_y, label=algorithm_name_legend)
                
#                 ax.fill_between(
#                     mean_x, 
#                     mean_y-error/math.sqrt(len(list_data)), 
#                     mean_y+error/math.sqrt(len(list_data)), 
#                     alpha=0.5
#                 )
                ax.fill_between(
                    mean_x, 
                    mean_y - z_value * scaled_error, 
                    mean_y + z_value * scaled_error, 
                    alpha=0.5
                )
                


                ax.set_yscale('log')

                ax.set_xlabel(list_x_legend[j])
                ax.set_ylabel(list_y_legend[i])

                
    plt.legend()
    
    if if_title:
        fig.suptitle(dataset)
        
    plt.tight_layout()
    
    
        
    path_to_dir = path_to_home + 'logs/plot_seed_result/' + dataset + '/'
    
    if not os.path.exists(path_to_dir):
        os.makedirs(path_to_dir)
    
    plt.savefig(
        path_to_dir + str(datetime.datetime.now().strftime('%Y-%m-%d-%X')) + '.pdf'
    )
    
    plt.show()

#     return 

def plot_matrices_norm_kfac(args):
    
    path_to_file = args['path_to_file']

    with open(path_to_file, 'rb') as fp:
        data_ = pickle.load(fp)

    print('data_.keys()')
    print(data_.keys())

    kfac_G_inv_norms_per_epoch =\
    np.asarray(data_['kfac_G_inv_norms_per_epoch'])
    kfac_G_norms_per_epoch = np.asarray(data_['kfac_G_norms_per_epoch'])
    kfac_F_twoNorms_per_epoch = np.asarray(data_['kfac_F_twoNorms_per_epoch'])
    
    kfac_A_twoNorms_per_epoch = np.asarray(data_['kfac_A_twoNorms_per_epoch'])
    kfac_G_twoNorms_per_epoch = np.asarray(data_['kfac_G_twoNorms_per_epoch'])
    layerWiseHessian_twoNorms_per_epoch = np.asarray(data_['layerWiseHessian_twoNorms_per_epoch'])



    num_epoch = kfac_A_twoNorms_per_epoch.shape[0]
    
    num_subplots = 6
    
    plt.figure(figsize=(8,8*num_subplots))
    
    plt.subplot(num_subplots, 1, 1)
    for l in range(kfac_A_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(np.arange(num_epoch) + 1, kfac_A_twoNorms_per_epoch[:, l], label='l = ' + str(l))
    plt.legend()
    plt.title('A (without LM)')
    
    plt.subplot(num_subplots, 1, 2)
    for l in range(kfac_G_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(np.arange(num_epoch) + 1, kfac_G_twoNorms_per_epoch[:, l], label='l = ' + str(l))
    plt.legend()
    plt.title('G (without LM)')
    
    plt.subplot(num_subplots, 1, 3)
    assert kfac_A_twoNorms_per_epoch.shape[1] ==\
    kfac_G_twoNorms_per_epoch.shape[1]
    for l in range(kfac_A_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                kfac_A_twoNorms_per_epoch[:, l] *\
                kfac_G_twoNorms_per_epoch[:, l], 
                label='l = ' + str(l)
            )
    plt.legend()
    plt.title('kfac_F (i.e. A kron G) (without LM)')
    
    
    
    
    plt.subplot(num_subplots, 1, 4)
    for l in range(layerWiseHessian_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(np.arange(num_epoch) + 1, layerWiseHessian_twoNorms_per_epoch[:, l], label='l = ' + str(l))
    plt.legend()
    plt.title('true Hessian (without LM)')
    
    plt.subplot(num_subplots, 1, 5)
    lambda_kfac = data_['params']['kfac_damping_lambda']
#     print('lambda_A')
#     print(lambda_A)
    assert kfac_A_twoNorms_per_epoch.shape[1] ==\
    kfac_G_twoNorms_per_epoch.shape[1]
    for l in range(kfac_A_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                (kfac_A_twoNorms_per_epoch[:, l] + np.sqrt(lambda_kfac)) *\
                (kfac_G_twoNorms_per_epoch[:, l] + np.sqrt(lambda_kfac)), 
                label='l = ' + str(l)
            )
    plt.legend()
    plt.title('kfac_F (with LM)')
    
    
    plt.subplot(num_subplots, 1, 6)

    for l in range(layerWiseHessian_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                layerWiseHessian_twoNorms_per_epoch[:, l] + lambda_kfac, 
                label='l = ' + str(l)
            )
    plt.legend()
    plt.title('true Hessian (with LM)')
    
    
    '''
    
    

    
    plt.subplot(num_subplots, 1, 7)

    lambda_G = data_['params']['Kron_BFGS_H_epsilon']

    lambda_ = lambda_A * lambda_G

    for l in range(layerWiseHessian_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                layerWiseHessian_twoNorms_per_epoch[:, l] + lambda_, 
                label='l = ' + str(l)
            )


    plt.legend()
    plt.title('true Hessian (with LM)')
    
    plt.subplot(num_subplots, 1, 8)
    for l in range(inverseLayerWiseHessian_LM_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                inverseLayerWiseHessian_LM_twoNorms_per_epoch[:, l], 
                label='l = ' + str(l)
            )
    plt.legend()
    plt.title('inverse of true Hessian (with LM)')
    
    plt.subplot(num_subplots, 1, 9)
    for l in range(inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch[:, l], 
                label='l = ' + str(l)
            )
    plt.legend()
    plt.title('inverse of true Hessian (MA) (with LM)')
    '''
    
    
    
    
    
    
    
    home_path = args['home_path']
    
#     print('path_to_file.split(result/)')
#     print(path_to_file.split('result/'))
    
    print('home_path + path_to_file.split(result/)[-1]')
    print(home_path + 'logs/plot_matrices_norm_kron_bfgs/' + path_to_file.split('result/')[-1] + '.pdf')
    
    saved_path_to_file = home_path + 'logs/plot_matrices_norm_kron_bfgs/' + path_to_file.split('result/')[-1] + '.pdf'
    
    if not os.path.exists(saved_path_to_file):
        os.makedirs(saved_path_to_file)
    if os.path.isdir(saved_path_to_file): 
        os.rmdir(saved_path_to_file)
    
    plt.savefig(saved_path_to_file)
    
    print('saved_path_to_file')
    print(saved_path_to_file)
    
    print('saved_path_to_file.split(result_)')
    print(saved_path_to_file.split('result_'))
    
    saved_path_to_dir = saved_path_to_file.split('result_')[0]
    
    name_pkl_file = 'result_' + path_to_file.split('result_')[1]
    
#     print('name_pkl_file')
#     print(name_pkl_file)
    
    
    
    shutil.copyfile(path_to_file, saved_path_to_dir + name_pkl_file)
    
    plt.show()


def plot_matrices_norm_kron_bfgs(args):
    
    path_to_file = args['path_to_file']

    with open(path_to_file, 'rb') as fp:
        data_ = pickle.load(fp)

#     print(data_.keys())

    kron_bfgs_A_twoNorms_per_epoch = np.asarray(data_['kron_bfgs_A_twoNorms_per_epoch'])
    kron_bfgs_G_LM_twoNorms_per_epoch = np.asarray(data_['kron_bfgs_G_LM_twoNorms_per_epoch'])
    kron_bfgs_Hg_twoNorms_per_epoch = np.asarray(data_['kron_bfgs_Hg_twoNorms_per_epoch'])
    kron_bfgs_Ha_twoNorms_per_epoch = np.asarray(data_['kron_bfgs_Ha_twoNorms_per_epoch'])
    layerWiseHessian_twoNorms_per_epoch = np.asarray(data_['layerWiseHessian_twoNorms_per_epoch'])
    inverseLayerWiseHessian_LM_twoNorms_per_epoch = np.asarray(data_['inverseLayerWiseHessian_LM_twoNorms_per_epoch'])
    inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch = np.asarray(data_['inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch'])



    num_epoch = kron_bfgs_A_twoNorms_per_epoch.shape[0]
    
    num_subplots = 5
    
    plt.figure(figsize=(8,8*num_subplots))
    
    plt.subplot(num_subplots, 1, 1)

    for l in range(kron_bfgs_A_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(np.arange(num_epoch) + 1, kron_bfgs_A_twoNorms_per_epoch[:, l], label='l = ' + str(l))


    plt.legend()
    plt.title('A (without LM)')
    
#     plt.subplot(num_subplots, 1, 2)
#     for l in range(kron_bfgs_Ha_twoNorms_per_epoch.shape[1]):
#         if l != 0 and l != 5:
#             plt.plot(np.arange(num_epoch) + 1, kron_bfgs_Ha_twoNorms_per_epoch[:, l], label='l = ' + str(l))
#     plt.legend()
#     plt.title('H_a (with LM)')
    
    plt.subplot(num_subplots, 1, 2)
    for l in range(kron_bfgs_Hg_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(np.arange(num_epoch) + 1, kron_bfgs_Hg_twoNorms_per_epoch[:, l], label='l = ' + str(l))
    plt.legend()
    plt.title('H_g (with LM)')
    
#     plt.subplot(num_subplots, 1, 4)
#     assert kron_bfgs_Ha_twoNorms_per_epoch.shape[1] ==\
#     kron_bfgs_Hg_twoNorms_per_epoch.shape[1]
#     for l in range(kron_bfgs_Ha_twoNorms_per_epoch.shape[1]):
#         if l != 0 and l != 5:
#             plt.plot(
#                 np.arange(num_epoch) + 1, 
#                 kron_bfgs_Ha_twoNorms_per_epoch[:, l] *\
#                 kron_bfgs_Hg_twoNorms_per_epoch[:, l], 
#                 label='l = ' + str(l)
#             )
#     plt.legend()
#     plt.title('H_a kron H_g')
    
    plt.subplot(num_subplots, 1, 3)
    for l in range(kron_bfgs_G_LM_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(np.arange(num_epoch) + 1, kron_bfgs_G_LM_twoNorms_per_epoch[:, l], label='l = ' + str(l))
    plt.legend()
    plt.title('G_LM (i.e. inverse of H_g)')
    
    plt.subplot(num_subplots, 1, 4)
    lambda_A = data_['params']['Kron_BFGS_A_LM_epsilon']
    print('lambda_A')
    print(lambda_A)
    assert kron_bfgs_A_twoNorms_per_epoch.shape[1] ==\
    kron_bfgs_G_LM_twoNorms_per_epoch.shape[1]
    for l in range(kron_bfgs_A_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                (kron_bfgs_A_twoNorms_per_epoch[:, l] + lambda_A) *\
                kron_bfgs_G_LM_twoNorms_per_epoch[:, l], 
                label='l = ' + str(l)
            )
    plt.legend()
    plt.title('KBFGS_Hessian (i.e. A_LM kron G_LM) (with LM)')

    
    plt.subplot(num_subplots, 1, 5)

    lambda_G = data_['params']['Kron_BFGS_H_epsilon']

    lambda_ = lambda_A * lambda_G

    for l in range(layerWiseHessian_twoNorms_per_epoch.shape[1]):
        if l != 0 and l != 5:
            plt.plot(
                np.arange(num_epoch) + 1, 
                layerWiseHessian_twoNorms_per_epoch[:, l] + lambda_, 
                label='l = ' + str(l)
            )


    plt.legend()
    plt.title('true Hessian (with LM)')
    
#     plt.subplot(num_subplots, 1, 8)
#     for l in range(inverseLayerWiseHessian_LM_twoNorms_per_epoch.shape[1]):
#         if l != 0 and l != 5:
#             plt.plot(
#                 np.arange(num_epoch) + 1, 
#                 inverseLayerWiseHessian_LM_twoNorms_per_epoch[:, l], 
#                 label='l = ' + str(l)
#             )
#     plt.legend()
#     plt.title('inverse of true Hessian (with LM)')
    
#     plt.subplot(num_subplots, 1, 9)
#     for l in range(inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch.shape[1]):
#         if l != 0 and l != 5:
#             plt.plot(
#                 np.arange(num_epoch) + 1, 
#                 inverseLayerWiseHessian_LM_MA_twoNorms_per_epoch[:, l], 
#                 label='l = ' + str(l)
#             )
#     plt.legend()
#     plt.title('inverse of true Hessian (MA) (with LM)')
    
    
    
    
    
    
    
    home_path = args['home_path']
    
#     print('path_to_file.split(result/)')
#     print(path_to_file.split('result/'))
    
    print('home_path + path_to_file.split(result/)[-1]')
    print(home_path + 'logs/plot_matrices_norm_kron_bfgs/' + path_to_file.split('result/')[-1] + '.pdf')
    
    saved_path_to_file = home_path + 'logs/plot_matrices_norm_kron_bfgs/' + path_to_file.split('result/')[-1] + '.pdf'
    
    if not os.path.exists(saved_path_to_file):
        os.makedirs(saved_path_to_file)
    if os.path.isdir(saved_path_to_file): 
        os.rmdir(saved_path_to_file)
    
    plt.savefig(saved_path_to_file)
    
    print('saved_path_to_file')
    print(saved_path_to_file)
    
    print('saved_path_to_file.split(result_)')
    print(saved_path_to_file.split('result_'))
    
    saved_path_to_dir = saved_path_to_file.split('result_')[0]
    
    name_pkl_file = 'result_' + path_to_file.split('result_')[1]
    
#     print('name_pkl_file')
#     print(name_pkl_file)
    
    
    
    shutil.copyfile(path_to_file, saved_path_to_dir + name_pkl_file)
    
    plt.show()


    

def plot_hyperparams_vs_loss_v2(args):
    
    from utils_git.utils_plot import plot_hyperparams_vs_loss_multiple_algos
    
    from scipy.interpolate import griddata

    
    hyperparams = plot_hyperparams_vs_loss_multiple_algos(args)

    hyperparams = hyperparams[0]
    
    algorithm = args['algorithms'][0]
    
    if algorithm['algorithm'] in ['Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                                  'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad']:
        hyperparams[:, 1] = hyperparams[:, 1]**2
    else:
        print('error: check if **2 for ' + algorithm['algorithm'])
        sys.exit()
    
#     print('hyperparams[:, 1]')
#     print(hyperparams[:, 1])
    
#     sys.exit()

    # Achraf's code

#     import pandas as pd
#     import pickle
    import matplotlib.pyplot as plt
    from matplotlib import ticker, cm
    
    
    
    import numpy as np
    import os
    import datetime
    
    plt.rcParams.update({'font.size': 18})
    plt.rc('font', family='serif')
    plt.style.use('seaborn-muted')


    hyperparams #: grid of results stored as a list of (lr, damping, value)
    
    x = []
    y = []
    z = []

    for x_t in hyperparams:
        x += [x_t[0]]
        y += [x_t[1]]
        z += [x_t[2]]

    step = 5
    
    if args['dataset'] in ['CURVES-autoencoder-relu-sum-loss',
                           'MNIST-autoencoder-relu-N1-1000-sum-loss']:
        min_level = 50
        max_level = 200
    elif args['dataset'] == 'FacesMartens-autoencoder-relu':
        min_level = 5
        max_level = 100
    else:
        print('error: min_level for ' + args['dataset'])
#         print('z')
#         print(z)
        print('max(z)')
        print(max(z))
        print('min(z)')
        print(min(z))
        sys.exit()
    
    levels = step*np.arange(min_level/step, max_level/step +1)

    

    xs,ys = np.meshgrid(np.logspace(np.log10(min(x)), np.log10(max(x)), num=50), np.logspace(np.log10(min(y)), np.log10(max(y)), num=50))
    
    resampled = griddata((x, y), z, (xs, ys), method = 'linear')
#     resampled = scipy.interpolate.griddata((x, y), z, (xs, ys), method = 'linear')

    
    

    # Name_tilte = 'K-BFGS(L)' + ', ' + 'Curves Autoencoder'
    Name_tilte = algorithm['algo_legend'] + ', ' + args['dataset_legend']

    fig = plt.figure(figsize = (12,10))
    cp = plt.contourf(xs, ys, resampled, levels = levels, cmap = cm.RdYlBu,  extend='max')
    fig.patch.set_facecolor('white')

    cp.cmap.set_over(cm.RdYlBu_r(1))
    cp.cmap.set_under('red')

    plt.xscale('log')
    plt.yscale('log')

    plt.colorbar(cp)

    plt.title(Name_tilte, fontsize = 25, fontweight='normal')
    plt.xlabel('learning rate')
    plt.ylabel('damping')
#     plt.show()
    
    saving_dir = args['home_path'] + 'logs/' + 'hyperparams_vs_loss_v2/' +\
    args['dataset'] + '/'
    
    if not os.path.exists(saving_dir):
        os.makedirs(saving_dir)

    plt.savefig(saving_dir +\
                str(datetime.datetime.now().strftime('%Y-%m-%d-%X')) + '.pdf', bbox_inches='tight')
    
    plt.show()


'''
def plot_hyperparams_vs_loss_v2(args):
    
    hyperparams = plot_hyperparams_vs_loss_multiple_algos(args)

    hyperparams = hyperparams[0]

    print('hyperparams.shape')
    print(hyperparams.shape)

    # Achraf's code

#     import pandas as pd
#     import pickle
#     import matplotlib.pyplot as plt
    from matplotlib import ticker, cm
    
    from scipy.interpolate import griddata
    
#     import numpy as np

    
    plt.rcParams.update({'font.size': 18})
    plt.rc('font', family='serif')
    plt.style.use('seaborn-muted')


    hyperparams #: grid of results stored as a list of (lr, damping, value)

    step = 5
    levels = step*np.arange(50/step, 200/step +1)

    x = []
    y = []
    z = []

    for x_t in hyperparams:
        x += [x_t[0]]
        y += [x_t[1]]
        z += [x_t[2]]

    xs,ys = np.meshgrid(np.logspace(np.log10(min(x)), np.log10(max(x)), num=50), np.logspace(np.log10(min(y)), np.log10(max(y)), num=50))
    
    resampled = griddata((x, y), z, (xs, ys), method = 'linear')
#     resampled = scipy.interpolate.griddata((x, y), z, (xs, ys), method = 'linear')
    

    # Name_tilte = 'K-BFGS(L)' + ', ' + 'Curves Autoencoder'
    Name_tilte = algorithm['algo_legend'] + ', ' + args['dataset_legend']

    fig = plt.figure(figsize = (12,10))
    cp = plt.contourf(xs, ys, resampled, levels = levels, cmap = cm.RdYlBu,  extend='max')
    fig.patch.set_facecolor('white')

    cp.cmap.set_over(cm.RdYlBu_r(1))
    cp.cmap.set_under('red')

    plt.xscale('log')
    plt.yscale('log')

    plt.colorbar(cp)

    plt.title(Name_tilte, fontsize = 25, fontweight='normal')
    plt.xlabel('learning rate')
    plt.ylabel('damping')
#     plt.show()
    
    saving_dir = args['home_path'] + 'logs/' + 'hyperparams_vs_loss_v2/' +\
    args['dataset'] + '/'
    
    if not os.path.exists(saving_dir):
        os.makedirs(saving_dir)

    plt.savefig(saving_dir +\
                str(datetime.datetime.now().strftime('%Y-%m-%d-%X')) + '.pdf', bbox_inches='tight')
    
    plt.show()
    '''

    

def plot_hyperparams_vs_loss_multiple_algos(args):

    num_algo = len(args['algorithms'])
    
    plt.figure(figsize=(3,1))

    fig, axs = plt.subplots(1, num_algo)
#     fig, axs = plt.subplots(1, num_algo+1)
    
    if num_algo == 1:
        axs = [axs]
    
    hyperparams = []
    
    index_subplot = 0
    for algo in args['algorithms']:
        
#         print('axs')
#         print(axs)
#         sys.exit()
        
        

        
#         ax = plt.subplot(1, len(args['algorithms']), index_subplot)
        ax = axs[index_subplot]
    
        index_subplot += 1
        
#         print('args.keys()')
#         print(args.keys())
#         sys.exit()
        

        
        args_1 = copy.deepcopy(args)
        args_1.pop('algorithms')
        
        args_1.update(algo)
        
#         print('args_1.keys()')
#         print(args_1.keys())
#         sys.exit()
        
        hyperparams_i = plot_hyperparams_vs_loss(index_subplot, axs, args_1)
        hyperparams.append(hyperparams_i)
        
    saving_dir = args['home_path'] + 'logs/' + 'hyperparams_vs_loss/' +\
    args['dataset'] + '/'
    
    if not os.path.exists(saving_dir):
        os.makedirs(saving_dir)

    plt.savefig(saving_dir +\
                str(datetime.datetime.now().strftime('%Y-%m-%d-%X')) + '.pdf', bbox_inches='tight')
    
    plt.show()
    
    return hyperparams

def plot_hyperparams_vs_loss(index_subplot, axs, args):
    
    
    
    import matplotlib.pyplot as plt
    import matplotlib.colors as colors
    
    import matplotlib.ticker as ticker
    
    ax = axs[index_subplot-1]
    
    


    home_path = args['home_path']
    name_dataset = args['dataset']
    algorithm = args['algorithm']
    list_lr = args['list_lr']
    

    args = from_dataset_to_N1_N2(args)
    N1 = args['N1']
    N2 = args['N2']
    
    fetched_data = []

    for lr in list_lr:
        working_dir = home_path + 'result/' + name_dataset + '/' + algorithm + '/' + 'if_gpu_True/' +\
                  'alpha_' + str(lr) + '/' +\
                  'N1_' + str(N1) + '/' +\
                  'N2_' + str(N2) + '/'



        os.chdir(working_dir)

        print(os.listdir())

        list_file = os.listdir()

        for file_ in list_file:
    #         print(file_)

            print('\n')
            print('len(fetched_data)')
            print(len(fetched_data))

            with open(file_, 'rb') as fp:
                data_ = pickle.load(fp)



            if data_['params']['if_test_mode'] == True:
                continue

            print('data_[params]')
            print(data_['params'])
            
            if algorithm in ['Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad',
                             'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad']:
                damping_keyword = 'Kron_BFGS_A_LM_epsilon'
            elif algorithm == 'kfac-no-max-no-LM-momentum-grad':
                damping_keyword = 'kfac_damping_lambda'
            else:
                print('error: unknown algorithm for ' + algorithm)
                sys.exit()
             

             # print('args[list_damping]')
#             print(args['list_damping'])
#             print('data_[params][damping_keyword]')
#             print(data_['params'][damping_keyword])
#             sys.exit()

            if data_['params'][damping_keyword] in args['list_damping']:
                fetched_data.append(
                    [lr, 
                     data_['params'][damping_keyword], 
                     np.min(data_['train_losses'])]
                )

    print('fetched_data')
    print(fetched_data)

    fetched_data = np.asarray(fetched_data)

    print('fetched_data.shape')
    print(fetched_data.shape)

    x = fetched_data[:, 0]
    y = fetched_data[:, 1]
    z = fetched_data[:, 2]
    
    
    cm = plt.cm.get_cmap('RdYlBu')

#     fig = plt.figure(figsize=(6,6))
#     ax = fig.add_subplot(111)

    ax.set_xscale('log')
    ax.set_yscale('log')

#     print('y')
#     print(y)
#     sys.exit()

#     plt.xlim(min(x) / 2, max(x) * 2)
#     plt.ylim(min(y) / 2, max(y) * 2)

    ax.set(xlim=(min(x) / 2, max(x) * 2))
    ax.set(ylim=(min(y) / 2, max(y) * 2))
    
#     plt.xlabel('learning rate')
    ax.set(xlabel='learning rate')
    
    if index_subplot == 1:
#     plt.ylabel('damping')
        ax.set(ylabel='damping')
        


#     plt.title(args['algo_legend'])
    ax.set(title=args['algo_legend'])
    

    # sc = plt.scatter(x, y, c=z, vmin=np.min(z), vmax=np.max(z), s=35, cmap=cm)
    
    if args['if_contour']:
        
#         if index_subplot == 1:
        
#             min_z = np.min(z)
#             max_z = np.max(z)
#         else:
#             min_z = np.minimum(min_z, np.min(z))
#             max_z = np.maximum(max_z, np.max(z))
    
#     sc = ax.tricontour(x, y, z, vmin=np.min(z), vmax=np.max(z), cmap=cm, norm=colors.LogNorm())


        levs = np.geomspace(np.min(z), np.max(z), num=100)
#         levs = np.linspace(np.min(z), np.max(z))
        
#         print('levs')
#         print(levs)

#         ax.tricontour(x, y, z, levs, cmap=cm, norm=colors.LogNorm(vmin=np.min(z), vmax=np.max(z)))
        sc = ax.tricontour(x, y, z, levs, cmap=cm, norm=colors.LogNorm())
#         sc = ax.tricontour(x, y, z, levs, cmap=cm)
#         sc = ax.tricontour(x, y, z, levs)

#         print('return_tricontour')
#         print(return_tricontour)
        
#         plt.setp(return_tricontour, norm=colors.LogNorm())
        
#         sys.exit()

#         sc = ax.tricontourf(x, y, z, levs, cmap=cm, norm=colors.LogNorm(vmin=np.min(z), vmax=np.max(z)))
        sc = ax.tricontourf(x, y, z, levs, cmap=cm, norm=colors.LogNorm())
#         sc = ax.tricontourf(x, y, z, levs, cmap=cm)

#         plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
#         if index_subplot == len(axs):
#             plt.colorbar(sc, ax=axs, norm=colors.LogNorm())
#             plt.colorbar(sc, ax=axs, spacing='proportional')
#             plt.colorbar(ax=axs)
            
#             plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=axs)
            
#             plt.colorbar.ColorbarBase(tick = levs)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if index_subplot == len(axs):
            
            min_z = np.min(z)
            max_z = np.max(z)
            
            unit = 10**np.floor(np.log10(min_z))
            multiplier = np.floor(min_z / unit)

            levs = []
            while 1:
                levs_i = unit * multiplier


                levs.append(levs_i)

                if multiplier == 9:
                    multiplier = 1
                    unit *= 10
                else:
                    multiplier += 1

                if levs_i > max_z:
                    break

            levs_colorbar = levs
            
            
            plt.colorbar(sc, ax=axs, spacing='proportional', ticks=levs_colorbar)
        
    else:
    
#         sc = ax.scatter(x, y, c=z, vmin=np.min(z), vmax=np.max(z), s=35, cmap=cm, norm=colors.LogNorm())
        sc = ax.scatter(
            x, y, c=z, s=35, cmap=cm, 
            norm=colors.LogNorm(vmin=np.min(z), vmax=np.max(z))
        )

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if index_subplot == len(axs):
            plt.colorbar(sc, ax=axs)
            
    return fetched_data
        
    
    
    
    
    



    


    



def get_subplot(ax, name_dataset, name_dataset_legend, algorithms, x_axis_name, y_axis_name, args):
    
    from utils_git.utils import get_name_algorithm
    
#     print('args')
#     print(args)

    plt.rcParams['xtick.labelsize']=20
    plt.rcParams['ytick.labelsize']=20
    
    ax.set_yscale('log')
    
#     ax.set_xscale('log')
#     ax.set_xscale('linear')
    ax.set_xscale(args['x_scale'])
    
    
    
    
    i = -1
    for algorithm_dict_try in algorithms:
        i += 1

        if isinstance(algorithm_dict_try, str):
            algorithm = algorithm_dict_try
            algorithm_dict = {}
            algorithm_dict['name'] = algorithm
            algorithm_dict['params'] = {}
            algorithm_dict['legend'] = algorithm
        else:
            algorithm = algorithm_dict_try['name']
            algorithm_dict = algorithm_dict_try
            if not 'legend' in algorithm_dict:
                algorithm_dict['legend'] = algorithm
                

        
        if 'if_test_mode' not in algorithm_dict['params']:
            algorithm_dict['params']['if_test_mode'] = args['if_test_mode']
        
        if 'if_max_epoch' not in algorithm_dict['params']:
            algorithm_dict['params']['if_max_epoch'] = args['if_max_epoch']
                

        path_to_google_drive_dir = args['home_path'] + 'result/'

        fake_params = {}
        # fake_params['if_Adam'] = args['if_Adam']
        # fake_params['if_momentum_gradient'] = if_momentum_gradient
        fake_params['algorithm'] = algorithm
        fake_params['if_gpu'] = args['if_gpu']

        name_algorithm, no_algorithm = get_name_algorithm(fake_params)
        
        name_result = name_dataset + '/' + name_algorithm + '/'



        

        # get best_alpha
        


        args['algorithm'] = algorithm
        args['dataset'] = name_dataset
        args['name_loss'] = get_name_loss(name_dataset)
        args['algorithm_dict'] = algorithm_dict

        best_alpha, _, best_name_result_pkl = get_best_params(args, if_plot=False)
    
        if best_name_result_pkl == None:
            print('Error: best_name_result_pkl == None for ' + args['algorithm'])
            sys.exit()


        fake_params = {}
        fake_params['alpha'] = best_alpha
        fake_params['N1'] = args['N1']
        fake_params['N2'] = args['N2']
        # fake_params['if_Adam'] = args['if_Adam']
        # fake_params['if_momentum_gradient'] = if_momentum_gradient
        fake_params['algorithm'] = algorithm
#         fake_params['RMSprop_epsilon'] = best_epsilon
        fake_params['if_gpu'] = args['if_gpu']

        name_algorithm_with_params = get_name_algorithm_with_params(fake_params)
        
        
        # name_result = name_dataset + '_' + name_algorithm
        name_result_with_params = name_dataset + '/' + name_algorithm_with_params + '/'

        with open(path_to_google_drive_dir + name_result_with_params +\
                    best_name_result_pkl, 'rb') as handle:
            record_result = pickle.load(handle)
    
        if y_axis_name == 'training loss':
            
            

            if 'train_losses' in record_result:
                y_data = record_result['train_losses']
            else:
                y_data = record_result['losses']
            
#             print('x_axis_name')
#             print(x_axis_name)
            
#             if x_axis_name == 'epoch':
#                 print('algorithm')
#                 print(algorithm)
#                 print('np.min(y_data)')
#                 print(np.min(y_data))
        
        elif y_axis_name == 'training unregularized loss':
            
            y_data = record_result['train_unregularized_losses']
            
        elif y_axis_name == 'training unregularized minibatch loss':
            
            y_data = record_result['train_unregularized_minibatch_losses']
            
        elif y_axis_name == 'training minibatch error':
            
#             print('get_name_loss(name_dataset)')
#             print(get_name_loss(name_dataset))
            
#             sys.exit()
        
            if get_name_loss(name_dataset) == 'multi-class classification':
                y_data = 1 - record_result['train_minibatch_acces']
            else:
                print('error: need to check')
                sys.exit()
                
        elif y_axis_name == 'training error':
            
#             print('get_name_loss(name_dataset)')
#             print(get_name_loss(name_dataset))
            
#             sys.exit()
            
            if get_name_loss(name_dataset) == 'multi-class classification':
                
                print('record_result.keys()')
                print(record_result.keys())
                
                assert 'train_acces' in record_result
                
                y_data = 1 - record_result['train_acces']
                
#                 sys.exit()
                
            else:
                print('error: need to check')
                sys.exit()

        elif y_axis_name == 'testing error':
            

            
            if get_name_loss(name_dataset) == 'multi-class classification':
                if 'test_acces' in record_result:
                    y_data = 1 - record_result['test_acces']
                else:
                    y_data = 1 - record_result['acces']
            elif get_name_loss(name_dataset) in ['logistic-regression-sum-loss',
                                                 'linear-regression-half-MSE']:
                if 'test_acces' in record_result:
                    y_data = record_result['test_acces']
                else:
                    y_data = record_result['acces']
            else:
                print('error: need to check for ' + get_name_loss(name_dataset))
                sys.exit()
            
                if name_dataset in ['MNIST-autoencoder',
                                    'MNIST-autoencoder-no-regularization',
                                    'MNIST-autoencoder-N1-1000',
                                    'MNIST-autoencoder-N1-1000-sum-loss',
                                    'MNIST-autoencoder-N1-1000-no-regularization',
                                    'MNIST-autoencoder-N1-1000-sum-loss-no-regularization',
                                    'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization',
                                    'MNIST-autoencoder-relu-N1-1000-sum-loss',
                                    'CURVES-autoencoder',
                                    'CURVES-autoencoder-no-regularization',
                                    'CURVES-autoencoder-sum-loss-no-regularization',
                                    'CURVES-autoencoder-relu-sum-loss-no-regularization',
                                    'CURVES-autoencoder-relu-sum-loss',
                                    'CURVES-autoencoder-sum-loss',
                                    'CURVES-autoencoder-Botev',
                                    'CURVES-autoencoder-Botev-sum-loss-no-regularization',
                                    'CURVES-autoencoder-shallow',
                                    'FACES-autoencoder',
                                    'FACES-autoencoder-no-regularization',
                                    'FACES-autoencoder-sum-loss-no-regularization',
                                    'FACES-autoencoder-relu-sum-loss-no-regularization',
                                    'FACES-autoencoder-relu-sum-loss',
                                    'FACES-autoencoder-sum-loss',
                                    'FacesMartens-autoencoder-relu',
                                    'sythetic-linear-regression']:
                    if 'test_acces' in record_result:
                        y_data = record_result['test_acces']
                    else:
                        y_data = record_result['acces']
                elif name_dataset in ['MNIST',
                                      'MNIST-no-regularization',
                                      'MNIST-N1-1000',
                                      'MNIST-one-layer',
                                      'DownScaledMNIST-no-regularization',
                                      'DownScaledMNIST-N1-1000-no-regularization',
                                      'webspam',
                                      'Fashion-MNIST',
                                      'Fashion-MNIST-N1-60',
                                      'Fashion-MNIST-N1-60-no-regularization',
                                      'CIFAR',
                                      'CIFAR-deep',
                                      'UCI-HAR']:
                    if 'test_acces' in record_result:
                        y_data = 1 - record_result['test_acces']
                    else:
                        y_data = 1 - record_result['acces']
                else:
                    print('Error: need check name_loss')
                    sys.exit()
                

            if name_dataset == 'MNIST':
                ax.set_ylim([0.01, 0.1])
        else:
            print('Error! need to check for ' + y_axis_name)
            sys.exit()

        
#         print('args[if_lr_in_legend]')
#         print(args['if_lr_in_legend'])
#         sys.exit()
        
        if args['if_lr_in_legend']:

            name_legend = algorithm_dict['legend'] +\
        ', lr = ' + str(best_alpha)
        else:
            name_legend = algorithm_dict['legend']

        if x_axis_name == 'cpu time':
#             time = record_result['timesCPU']
            x_data = record_result['timesCPU']
#             ax.plot(time, y_data, label=name_legend)
#             ax.plot(x_data, y_data, label=name_legend)
        elif x_axis_name == 'wall clock time':
#             time = record_result['timesWallClock']
            x_data = record_result['timesWallClock']
#             ax.plot(time, y_data, label=name_legend)
#             ax.plot(x_data, y_data, label=name_legend)
        elif x_axis_name == 'epoch':
#             epochs = record_result['epochs']
            x_data = record_result['epochs']
#             ax.plot(epochs, y_data, label=name_legend)
#             ax.plot(x_data, y_data, label=name_legend)
        else:
            print('Error.')
            sys.exit()
            
#         print('args[color]')
#         print(args['color'])
        
#         sys.exit()
        
        if args['color'] == None:
            ax.plot(x_data, y_data, label=name_legend)
        else:
            
#             print('i')
#             print(i)
            
#             sys.exit()
            
            
            ax.plot(x_data, y_data, args['color'][i], label=name_legend)
            
    if x_axis_name == 'cpu time':
        plt.xlabel('process time (second)', fontsize=20)
#         plt.xlabel('CPU times (second)')
    elif x_axis_name in ['epoch', 'wall clock time']:
        plt.xlabel(x_axis_name, fontsize=20)
    else:
        print('error: need to check x_axis_name for ' + x_axis_name)
        sys.exit()
        

    
        
    
    
    
    
    if y_axis_name == 'training unregularized minibatch loss':
        y_axis_name_legend = 'training loss'
    elif y_axis_name == 'testing error':
        y_axis_name_legend = 'testing error'
    else:
        print('y_axis_name')
        print(y_axis_name)

        sys.exit()
    
        

    plt.ylabel(y_axis_name_legend, fontsize=20)
    
#     print('args')
#     print(args)
    
#     print('args[if_title]')
#     print(args['if_title'])
    
#     sys.exit()
    
    if args['if_title']:
        plt.suptitle(name_dataset_legend)
    
    plt.grid(True)
    
    
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])

def get_plot(name_dataset, name_dataset_legend, algorithms, args):
    
    from utils_git.utils import from_dataset_to_N1_N2
    
#     print('args')
#     print(args)
    
    args['dataset'] = name_dataset
    args = from_dataset_to_N1_N2(args)
    
    
    list_x = args['list_x']

    list_y = args['list_y']
    
#     print('len(list_x)')
#     print(len(list_x))
    
#     print('len(list_y)')
#     print(len(list_y))
    
#     sys.exit()
    
    

#     plt.figure(figsize=(2*5,2*5))
    plt.figure(figsize=(len(list_x)*5,len(list_y)*5))

    index_subplot = 0
    for name_y in list_y:
        for name_x in list_x:
            index_subplot += 1
            ax = plt.subplot(len(list_y), len(list_x), index_subplot)
            get_subplot(ax, name_dataset, name_dataset_legend, algorithms, name_x, name_y, args)
            
#     print('args[if_show_legend]')
#     print(args['if_show_legend'])
#     sys.exit()

    if args['if_show_legend']:
        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20)
    
    path_to_func_dir = args['home_path'] + 'logs/get_subplot_' + args['tuning_criterion'] + '/'

    if not os.path.exists(path_to_func_dir):
            os.makedirs(path_to_func_dir)
    if not os.path.exists(path_to_func_dir + name_dataset + '/'):
            os.makedirs(path_to_func_dir + name_dataset + '/')

    

    plt.savefig(path_to_func_dir + name_dataset + '/' +\
                str(datetime.datetime.now().strftime('%Y-%m-%d-%X')) + '.pdf', bbox_inches='tight')

    plt.show()


def plot_damping_status(args):
    
    path_to_file = args['path_to_file']
    
    print('path_to_file')
    print(path_to_file)
    
    # get some params
    home_path = path_to_file.split('result')[0]
    name_result = path_to_file.split('result')[1]
    name_result = name_result.split('/')
    
    dataset = name_result[1]
    algorithm = name_result[2]
    lr = name_result[4]
    
    if dataset == 'CURVES-autoencoder-relu-sum-loss':
        dataset_legend = 'CURVES'
    elif dataset == 'MNIST-autoencoder-relu-N1-1000-sum-loss':
        dataset_legend = 'MNIST'
    elif dataset == 'FacesMartens-autoencoder-relu':
        dataset_legend = 'FACES'
    else:
        print('error: no dataset_legend for ' + dataset)
        sys.exit()
        
#     print('algorithm')
#     print(algorithm)
#     sys.exit()
    
    if algorithm == 'Kron-BFGS(L)-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        algorithm_legend = 'K-BFGS(L)'
    elif algorithm == 'Kron-BFGS-homo-no-norm-gate-HessianAction-momentum-s-y-Powell-double-damping-regularized-grad-momentum-grad':
        algorithm_legend = 'K-BFGS'
    else:
        print('error: algorithm_legend for ' + algorithm)
        sys.exit()

    


    min_index = 0
    max_index = 200000000000000

    with open(path_to_file, 'rb') as filename:
        data_ = pickle.load(filename)

    plt.plot(np.arange(len(data_['losses_per_iter']))[min_index: max_index],
                 data_['losses_per_iter'][min_index: max_index])

    plt.plot(np.linspace(
            0,
            len(data_['losses_per_iter']),
            len(data_['train_losses'])
                          )[min_index: max_index],
             data_['train_losses'][min_index: max_index])

    plt.yscale('log')
    plt.show()
    
    
#     list_damping = data_['kron_bfgs_damping_statuses'].keys()
#     list_damping = list(list_damping)
    list_damping = []
    
    if 'kron_bfgs_check_dampings' in data_:
#         num_subplot = len(list_damping)+1+1
        num_subplot = len(list_damping)+2
    else:
        num_subplot = len(list_damping)
        
#     print('num_subplot')
#     print(num_subplot)
#     sys.exit()
    

    plt.figure(figsize=(8,16))
    
    plt.figure(figsize=(8, 4 * num_subplot))
    
#     index_subplot = 0
    index_subplot = -1
    while 1:
        if index_subplot == num_subplot - 1:
            break
            
        
        
        index_subplot += 1
        
        print('num_subplot')
        print(num_subplot)
        print('index_subplot+1')
        print(index_subplot+1)
        
        ax = plt.subplot(num_subplot, 1, index_subplot+1)
        
#         print('index_subplot-1')
#         print(index_subplot-1)
        
#         sys.exit()
        
        if index_subplot < len(list_damping):
            
            name_damping = list_damping[index_subplot-1]

            np_kron_bfgs_damping_statuses =\
            data_['kron_bfgs_damping_statuses'][name_damping]
            
            np_kron_bfgs_damping_statuses =\
            np.asarray(np_kron_bfgs_damping_statuses)
        elif index_subplot == len(list_damping):
            
#             name_damping = 'ratio of inequality holds'
            name_damping = 'Fraction of Iters. inequality hold'
            
            np_kron_bfgs_damping_statuses =\
            data_['kron_bfgs_check_dampings']

            np_kron_bfgs_damping_statuses =\
            np.asarray(np_kron_bfgs_damping_statuses)

#             alpha = data_['params']['Kron_BFGS_H_epsilon']
            alpha = 0.2
            
            print('alpha')
            print(alpha)
            
            print('data_[params][Kron_BFGS_H_epsilon]')
            print(data_['params']['Kron_BFGS_H_epsilon'])

            np_kron_bfgs_damping_statuses = np_kron_bfgs_damping_statuses >= alpha / 2
            
        elif index_subplot == len(list_damping)+1:
            
#             name_damping = 'yHy/sy'
            name_damping = 'Average value of yHy/sy'
            
            np_kron_bfgs_damping_statuses =\
            data_['kron_bfgs_check_dampings'] # sy/yHy

            np_kron_bfgs_damping_statuses =\
            np.asarray(np_kron_bfgs_damping_statuses)
            
            np_kron_bfgs_damping_statuses = 1 / np_kron_bfgs_damping_statuses

            alpha = data_['params']['Kron_BFGS_H_epsilon']

#             np_kron_bfgs_damping_statuses = np_kron_bfgs_damping_statuses >= alpha / 2
            


    #     np_kron_bfgs_damping_statuses = np_kron_bfgs_damping_statuses['Powell-H-damping']


        L = np_kron_bfgs_damping_statuses.shape[1]

        if dataset in ['FACES-autoencoder-sum-loss-no-regularization',
                       'FacesMartens-autoencoder-relu']:
            iter_per_epoch = 103
        elif dataset in ['MNIST-autoencoder-N1-1000-sum-loss-no-regularization',
                         'MNIST-autoencoder-relu-N1-1000-sum-loss']:
            iter_per_epoch = 60
        elif dataset in ['CURVES-autoencoder-sum-loss-no-regularization',
                         'CURVES-autoencoder-relu-sum-loss-no-regularization',
                         'CURVES-autoencoder-relu-sum-loss']:
            iter_per_epoch = 20
        else:
            print('error: no iter_per_epoch for ' + dataset)
            sys.exit()

        np_kron_bfgs_damping_statuses =\
        np_kron_bfgs_damping_statuses.reshape(-1, iter_per_epoch, L)

        np_kron_bfgs_damping_statuses =\
            np.mean(np_kron_bfgs_damping_statuses, axis=1)

    #     print(np_kron_bfgs_damping_statuses.shape)


        average_np_kron_bfgs_damping_statuses =\
        np.mean(np_kron_bfgs_damping_statuses, axis=1)

    #     sys.exit()



        for l in range(L):
            if l > 1 and l < L-2:
                continue



            np_kron_bfgs_damping_statuses_l =\
            np_kron_bfgs_damping_statuses[:, l]




    #         np_kron_bfgs_damping_statuses_l =\
    #         np_kron_bfgs_damping_statuses_l.reshape(-1, 103)

    #         np_kron_bfgs_damping_statuses_l =\
    #         np.mean(np_kron_bfgs_damping_statuses_l, axis=1)

            ax.plot(
                np.arange(
                len(np_kron_bfgs_damping_statuses_l)
            )[min_index: max_index]+1,
                np_kron_bfgs_damping_statuses_l[min_index: max_index],
                label='l = {}'.format(l)
            )




        ax.plot(
                np.arange(
                len(average_np_kron_bfgs_damping_statuses)
            )[min_index: max_index]+1,
                average_np_kron_bfgs_damping_statuses[min_index: max_index],
                label='average'
            )

        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        ax.set_xlabel('epoch')
        
#         ax.set_ylabel('ratio of ' + name_damping)
        ax.set_ylabel(name_damping)




        # add title
#         text_title = dataset_legend + '\n' +\
#         algorithm[:int(len(algorithm)/2)] + '\n' + algorithm[int(len(algorithm)/2):] +\
#         '\n' + lr
        
        text_title = dataset_legend + ', ' + algorithm_legend

        if index_subplot == len(list_damping):
#         plt.suptitle(text_title)
            plt.title(text_title)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
#     plt.subplots_adjust(top=0.85)
    
    
    # save to disk
    saved_path = home_path + 'logs/'
    saved_path = path_to_file.replace('result/', 'logs/plot_damping_status/') + '.pdf'

    if not os.path.exists(saved_path):
        os.makedirs(saved_path)
    if os.path.isdir(saved_path): 
        os.rmdir(saved_path)
    
    plt.tight_layout()
    plt.savefig(saved_path)
    
    plt.show()
File Path: utils_git/utils_shampoo.py
Content:
import torch
import numpy as np
import sys
import os

from utils_git.utils import get_opposite, get_BFGS_formula_v2, coupled_newton

list_algorithm = [
    'shampoo',
    'shampoo-allVariables',
    'shampoo-allVariables-warmStart',
    'shampoo-allVariables-warmStart-lessInverse',
    'shampoo-allVariables-filterFlattening-warmStart',
    'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
    'shampoo-no-sqrt',
    'shampoo-no-sqrt-Fisher',
    'matrix-normal',
    'matrix-normal-allVariables',
    'matrix-normal-allVariables-warmStart',
    'matrix-normal-allVariables-warmStart-MaxEigDamping',
    'matrix-normal-allVariables-warmStart-noPerDimDamping',
    'matrix-normal-same-trace',
    'matrix-normal-same-trace-warmStart',
    'matrix-normal-same-trace-warmStart-noPerDimDamping',
    'matrix-normal-same-trace-allVariables',
    'matrix-normal-same-trace-allVariables-warmStart',
    'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
    'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
    'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
    'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
    'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping'
    'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
    'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
    'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
    'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
    'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
    'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
    'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
    'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
    'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
    'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
    'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
    'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',
]

def get_tensor_reshape_back(delta_l, l, name_variable, params):
    

    if get_tensor_reshape_option(l, name_variable, params) == 'filter-flattening':
        
        kernel_size = params['layers_params'][l]['conv_kernel_size']
        
#         import copy
        
#         delta_l_2 = copy.deepcopy(delta_l.view(delta_l.size(0), delta_l.size(1), kernel_size, kernel_size))
        
#         delta_l_3 = copy.deepcopy(delta_l_2.view(delta_l_2.size(0), delta_l_2.size(1), delta_l_2.size(2)*delta_l_2.size(3)))
        
#         print('torch.norm(delta_l_3 - delta_l)')
#         print(torch.norm(delta_l_3 - delta_l))
        
#         sys.exit()
        
        delta_l = delta_l.view(delta_l.size(0), delta_l.size(1), kernel_size, kernel_size)
        
    elif get_tensor_reshape_option(l, name_variable, params) == 'KFAC-reshaping':
        
        kernel_size = params['layers_params'][l]['conv_kernel_size']
        conv_in_channels = params['layers_params'][l]['conv_in_channels']
        
        delta_l = delta_l.view(delta_l.size(0), conv_in_channels, kernel_size, kernel_size)
        
    elif get_tensor_reshape_option(l, name_variable, params) == 'None':
        pass
        
    else:
        print('get_tensor_reshape_option(l, name_variable, params)')
        print(get_tensor_reshape_option(l, name_variable, params))
        sys.exit()
    
    return delta_l

# def get_if_tensor_reshape(l, name_variable, params):
def get_tensor_reshape_option(l, name_variable, params):
    
    
    
    if params['algorithm'] in ['shampoo-allVariables-filterFlattening-warmStart',
                               'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
        
        if params['layers_params'][l]['name'] in ['conv',
                                                  'conv-no-activation',
                                                  'conv-no-bias-no-activation']:
        
            if name_variable == 'W': 
                
                
                
                if params['algorithm'] in ['matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                           'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                           'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',]:
                    return 'KFAC-reshaping'
                elif params['algorithm'] in ['shampoo-allVariables-filterFlattening-warmStart',
                                             'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                                             'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                             'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                             'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                             'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                             'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
                    return 'filter-flattening'
                else:
                    print('params[algorithm]')
                    print(params['algorithm'])
                
                    sys.exit()
                
                    

            elif name_variable == 'b':
                return 'None'
                
            else:
                print('name_variable')
                print(name_variable)
                sys.exit()
            
            
        elif params['layers_params'][l]['name'] in ['fully-connected',
                                                    'BN']:
            return 'None'
            
        else:
            print('params[layers_params][l]')
            print(params['layers_params'][l])
            
            sys.exit()
            
    elif params['algorithm'] in ['matrix-normal-allVariables-warmStart-MaxEigDamping',
                                 'matrix-normal-same-trace-allVariables-warmStart',
                                 'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                 'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                                 'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                                 'shampoo-allVariables-warmStart',
                                 'shampoo-allVariables-warmStart-lessInverse',]:
        return 'None'
    else:
        
        print('params[algorithm]')
        print(params['algorithm'])
        sys.exit()

def get_tensor_reshape(g_W, l, name_variable, params):
    

    

    if get_tensor_reshape_option(l, name_variable, params) == 'filter-flattening':
        g_W = g_W.view(g_W.size(0), g_W.size(1), g_W.size(2) * g_W.size(3))
    elif get_tensor_reshape_option(l, name_variable, params) == 'KFAC-reshaping':
        
#         print('g_W.size()')
#         print(g_W.size())
        
#         sys.exit()
        
        g_W = g_W.view(g_W.size(0), g_W.size(1) * g_W.size(2) * g_W.size(3))
        
    elif get_tensor_reshape_option(l, name_variable, params) == 'None':
        1
    else:
        print('get_tensor_reshape_option(l, name_variable, params)')
        print(get_tensor_reshape_option(l, name_variable, params))
        sys.exit()
        
    return g_W
        
    
    
    '''
    if params['algorithm'] == 'matrix-normal-same-trace-allVariables-filterFlattening-warmStart':
        
        
        
        
        
        if params['layers_params'][l]['name'] == 'conv':
        
            if name_variable == 'W': 
                g_W = g_W.view(g_W.size(0), g_W.size(1), g_W.size(2) * g_W.size(3))

            elif name_variable == 'b':
        
                1
                
            else:
                
                print('name_variable')
                print(name_variable)
        
                sys.exit()
            
            
        elif params['layers_params'][l]['name'] == 'fully-connected':
            1
            
        else:
            print('params[layers_params][l]')
            print(params['layers_params'][l])
            
            sys.exit()
            
        
    else:
        
        print('params[algorithm]')
        print(params['algorithm'])
    
        sys.exit()
    
    
    
    return g_W
    '''

def get_if_shampoo_update(name_variable, params):
    
    if params['algorithm'] in ['matrix-normal',
                            'matrix-normal-same-trace',
                            'matrix-normal-same-trace-warmStart',
                            'matrix-normal-same-trace-warmStart-noPerDimDamping',]:
        print('error: only support allVariables mode now')
        sys.exit()
    
    if name_variable == 'W' or\
    (
        name_variable == 'b'
#         and\
#         params['algorithm'] in ['matrix-normal-allVariables',
#                                 'matrix-normal-allVariables-warmStart',
#                                 'matrix-normal-allVariables-warmStart-noPerDimDamping',
#                                 'matrix-normal-same-trace-allVariables',
#                                 'matrix-normal-same-trace-allVariables-warmStart',
#                                 'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
#                                 'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
#                                 'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
#                                 'shampoo-allVariables',
#                                 'shampoo-allVariables-warmStart']
    ):
        return True
#     elif name_variable == 'b' and\
#     params['algorithm'] in ['matrix-normal',
#                             'matrix-normal-same-trace',
#                             'matrix-normal-same-trace-warmStart',
#                             'matrix-normal-same-trace-warmStart-noPerDimDamping',]:
#         return False
    else:
        print('error: not implemented for ' + name_variable)
#         print('error: not implemented for ' + params['algorithm'])
        sys.exit()
    
def shampoo_kron_matrices_warm_start_per_variable(j, model_grad_N1, l, name_variable, data_, params):
    # model_grad_N1: used for 2nd-order estimate
    
    
    if not get_if_shampoo_update(name_variable, params):
        return
        
        
        





    g_W = model_grad_N1[l][name_variable]

#     print('g_W.size()')
#     print(g_W.size())
    
    g_W = get_tensor_reshape(g_W, l, name_variable, params)
    
#     print('g_W.size()')
#     print(g_W.size())


#         test_H_l = []
#         for ii in range(len(g_W.size())):
#             axes = list(range(len(g_W.size())))
#             axes.remove(ii)
#             test_H_l.append(
#                 torch.tensordot(g_W, g_W, dims=(axes, axes)).data
#             )

    test_H_l = shampoo_get_list_of_contractions(g_W)



#         print('j')
#         print(j)

#         if i == 0:
    if j == 1:

#             print('data_[shampoo_H][l]')
#             print(data_['shampoo_H'][l])






#             if params['algorithm'] in ['shampoo-allVariables',
#                                        'matrix-normal-allVariables']:

#             data_['shampoo_H'].append(test_H_l)
#             data_['shampoo_H'][l] = test_H_l
        data_['shampoo_H'][l][name_variable] = test_H_l

#             else:
#                 print('error: check the impact of warm start for ' + params['algorithm'])
#                 sys.exit()
    else:




        for ii in range(len(data_['shampoo_H'][l][name_variable])):
#                 data_['shampoo_H'][l][name_variable][ii] =\
#     decay_ * data_['shampoo_H'][l][name_variable][ii].data + weight_ * test_H_l[ii]
            data_['shampoo_H'][l][name_variable][ii] *= (j-1)/j
            data_['shampoo_H'][l][name_variable][ii] += 1/j * test_H_l[ii]
        
        
        
#     else:
#         1
    
def shampoo_get_list_of_contractions(g_W):
    
    test_H_l = []
        
    for ii in range(len(g_W.size())):
        axes = list(range(len(g_W.size())))

        axes.remove(ii)

        test_H_l.append(
            torch.tensordot(g_W, g_W, dims=(axes, axes)).data
        )
        
#         print('g_W.size()')
#         print(g_W.size())
        
#         print('test_H_l[-1].size()')
#         print(test_H_l[-1].size())
        
#         sys.exit()
        
    return test_H_l
    

def shampoo_kron_matrices_per_variable(model_grad_N1, l, name_variable, data_, params):
    # model_grad_N1: used for 2nd-order estimate
    
    if not get_if_shampoo_update(name_variable, params):
        return
        
    i = params['i']
    
    if i % params['shampoo_update_freq'] != 0:
        return

    decay_ = params['shampoo_decay']
    weight_ = params['shampoo_weight']

    if not params['if_warm_start']:
        weight_ = max(1/(i+1), weight_)




    g_W = model_grad_N1[l][name_variable]

    g_W = get_tensor_reshape(g_W, l, name_variable, params)






    test_H_l = shampoo_get_list_of_contractions(g_W)



    if i == 0:


        if not params['if_warm_start']:
            data_['shampoo_H'][l][name_variable] = test_H_l
    else:
        # L[l] = decay_ * L[l].data + weight_ * L_[l].data
        # R[l] = decay_ * R[l].data + weight_ * R_[l].data


        for ii in range(len(data_['shampoo_H'][l][name_variable])):
            data_['shampoo_H'][l][name_variable][ii] =\
decay_ * data_['shampoo_H'][l][name_variable][ii].data + weight_ * test_H_l[ii]
    
    


def shampoo_inversion_per_variable(model_grad_N1, l, name_variable, data_, params):
    
    if not get_if_shampoo_update(name_variable, params):
        return
        
    device = params['device']

    i = params['i']
    
#     if params['if_Hessian_action']:
#         inverse_freq = 1
#         inverse_freq = 20
#     else:
    
    inverse_freq = params['shampoo_inverse_freq']

    if params['algorithm'] in ['shampoo-allVariables-warmStart-lessInverse',
                               'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                               'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                               'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',]:
        pass
    elif params['algorithm'] in ['shampoo-allVariables-filterFlattening-warmStart',
                                 'matrix-normal-same-trace-allVariables-warmStart',
                                 'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                 'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                 'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
        if i < inverse_freq:
#             inverse_freq = 1
            inverse_freq = params['shampoo_update_freq']
    else:
        print('params[algorithm]')
        print(params['algorithm'])
        sys.exit()

        

    if i % inverse_freq == 0:
        # L[l] = torch.mm(torch.mm(L_l_U, torch.diag(L_l_S)), L_l_V.t())
        # L_l_U, L_l_S, L_l_V = torch.svd(L[l])
        # R_l_U, R_l_S, R_l_V = torch.svd(R[l])

        # L_l_U, L_l_S, L_l_V = torch.svd(L[l])



        if params['if_LM']:
            epsilon = params['lambda_']
        else:





            if params['algorithm'] in ['matrix-normal-allVariables-warmStart-noPerDimDamping',
                                       'matrix-normal-same-trace-warmStart-noPerDimDamping',
                                       'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping']:

#                     assert params['tau'] == 0



                epsilon = pow(params['shampoo_epsilon'] + params['tau'], 1 / len(data_['shampoo_H'][l][name_variable]))

            elif params['algorithm'] in ['shampoo-allVariables-warmStart',
                                         'shampoo-allVariables-warmStart-lessInverse',
                                         'shampoo-allVariables-filterFlattening-warmStart',
                                         'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                                         'matrix-normal-allVariables-warmStart',
                                         'matrix-normal-same-trace',
                                         'matrix-normal-same-trace-warmStart',
                                         'matrix-normal-same-trace-allVariables',
                                         'matrix-normal-same-trace-allVariables-warmStart',
                                         'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                         'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                         'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                         'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                         'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                         'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                         'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                         'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                         'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                         'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                         'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                         'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                         'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                         'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:

                # tau is ignored since we tune shampoo_epsilon

                epsilon = params['shampoo_epsilon']

            elif params['algorithm'] in ['matrix-normal-allVariables-warmStart-MaxEigDamping',
                                         'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                         'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',]:

                pass

            else:
                print('params[algorithm]')
                print(params['algorithm'])

                sys.exit()



        if params['algorithm'] in ['shampoo-no-sqrt',
                                   'shampoo-no-sqrt-Fisher']:
            power_preconditioner = 1
        elif params['algorithm'] in ['shampoo',
                                     'shampoo-allVariables',
                                     'shampoo-allVariables-warmStart',
                                     'shampoo-allVariables-warmStart-lessInverse',
                                     'shampoo-allVariables-filterFlattening-warmStart',
                                     'shampoo-allVariables-filterFlattening-warmStart-lessInverse',]:
            power_preconditioner = 0.5
        elif params['algorithm'] in ['matrix-normal',
                                     'matrix-normal-allVariables',
                                     'matrix-normal-allVariables-warmStart',
                                     'matrix-normal-allVariables-warmStart-MaxEigDamping',
                                     'matrix-normal-allVariables-warmStart-noPerDimDamping',
                                     'matrix-normal-same-trace',
                                     'matrix-normal-same-trace-warmStart',
                                     'matrix-normal-same-trace-warmStart-noPerDimDamping',
                                     'matrix-normal-same-trace-allVariables',
                                     'matrix-normal-same-trace-allVariables-warmStart',
                                     'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                     'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                                     'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                                     'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                     'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                     'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
            1
        else:
            print('Error: unkown algo for power_preconditioner for ' + params['algorithm'])
            sys.exit()



        H_l_LM_minus_2k = []
        H_l_trace = []

        H = data_['shampoo_H']

        if params['algorithm'] in ['matrix-normal',
                                   'matrix-normal-allVariables',
                                   'matrix-normal-allVariables-warmStart',
                                   'matrix-normal-allVariables-warmStart-MaxEigDamping',
                                   'matrix-normal-allVariables-warmStart-noPerDimDamping',
                                   'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',]:
            
            if params['shampoo_if_coupled_newton']:
                sys.exit()



            for ii in range(len(H[l][name_variable])):

                H_l_trace.append(torch.trace(H[l][name_variable][ii]).item())



#                     diag_ind = np.diag_indices(H[l][ii].shape[0])

#                     H_l_ii_LM = H[l][ii]

#                     H_l_ii_LM[diag_ind[0], diag_ind[1]] +=\
#                     epsilon * torch.ones(H[l][ii].shape[0], device=device)

                if params['algorithm'] == 'matrix-normal-allVariables-warmStart-MaxEigDamping':
                    epsilon = torch.linalg.norm(H[l][name_variable][ii].data, ord=2).item()
                elif params['algorithm'] == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping':
                    epsilon *= torch.linalg.norm(H[l][name_variable][ii].data, ord=2).item()
                elif params['algorithm'] == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping':
                    epsilon *= torch.trace(H[l][name_variable][ii].data) / H[l][name_variable][ii].size(0)
                elif params['algorithm'] == 'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping':
                    
                    epsilon *= torch.trace(H[l][name_variable][ii].data)
                
                if params['if_Hessian_action']:
        
                    g_W = model_grad_N1[l][name_variable]
                
                    g_W = get_tensor_reshape(g_W, l, name_variable, params)
                    
                    axes = list(range(len(g_W.size())))
                    axes.remove(ii)
                    
                    if len(axes):
                        s = torch.mean(g_W.data, dim=axes)
                    else:
                        s = g_W.data
        
                    y = torch.mv(H[l][name_variable][ii], s) + epsilon * s
            
#                     print('l')
#                     print(l)
                    
#                     print('name_variable')
#                     print(name_variable)

#                     print('data_[shampoo_H_LM_minus_2k][l][name_variable]')
#                     print(data_['shampoo_H_LM_minus_2k'][l][name_variable])
            
                    H_l_LM_minus_2k_ii, _ = get_BFGS_formula_v2(
                        data_['shampoo_H_LM_minus_2k'][l][name_variable][ii],
                        s, y, None, False
                    )
                    H_l_LM_minus_2k.append(H_l_LM_minus_2k_ii)
                
                    
                else:

                    H_l_ii_LM = H[l][name_variable][ii] + epsilon * torch.eye(H[l][name_variable][ii].shape[0], device=device)


                    H_l_LM_minus_2k.append(H_l_ii_LM.inverse())

        elif params['algorithm'] in ['matrix-normal-same-trace',
                                     'matrix-normal-same-trace-warmStart',
                                     'matrix-normal-same-trace-warmStart-noPerDimDamping',
                                     'matrix-normal-same-trace-allVariables',
                                     'matrix-normal-same-trace-allVariables-warmStart',
                                     'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                     'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                                     'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                     'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
            
            

            c_trace = torch.trace(H[l][name_variable][0])


            list_n = [H_l_ii.size(0) for H_l_ii in H[l][name_variable]]

            c_trace = c_trace / np.prod(list_n)
            c_trace = c_trace**(1/len(list_n))

            for ii in range(len(H[l][name_variable])):
                axes = list(range(len(list_n)))
                axes.remove(ii)

                H_l_same_trace = H[l][name_variable][ii] / c_trace**(len(list_n)-1) /\
                     np.prod(np.asarray(list_n)[axes])

                if params['algorithm'] == 'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping':
                    epsilon = torch.trace(H_l_same_trace) / H_l_same_trace.size(0)
                elif params['algorithm'] == 'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping':

#                         if H_l_same_trace.size(0) == 1:
#                             epsilon = H_l_same_trace.data[0][0].item()
#                         else:
#                             epsilon = torch.lobpcg(H_l_same_trace.data)[0].item()

                    epsilon = torch.linalg.norm(H_l_same_trace.data, ord=2).item()

                H_l_LM_same_trace =\
                H_l_same_trace + epsilon * torch.eye(list_n[ii], device=device)
                
            
            
                if params['shampoo_if_coupled_newton']:
                    H_l_LM_minus_2k.append(
                        coupled_newton(H_l_LM_same_trace, 1, 0, device, True,)
                    )
                else:
                    H_l_LM_minus_2k.append(H_l_LM_same_trace.inverse())

#                     H_l_LM_minus_2k.append(H_l_LM_same_trace.cpu().inverse().cuda())

#                     H_l_LM_same_trace = H_l_LM_same_trace.cpu()
#                     H_l_LM_same_trace = H_l_LM_same_trace.inverse()
#                     H_l_LM_same_trace = H_l_LM_same_trace.cuda()
#                     H_l_LM_minus_2k.append(H_l_LM_same_trace)



        elif params['algorithm'] in ['shampoo',
                                     'shampoo-allVariables',
                                     'shampoo-allVariables-warmStart',
                                     'shampoo-allVariables-warmStart-lessInverse',
                                     'shampoo-allVariables-filterFlattening-warmStart',
                                     'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                                     'shampoo-no-sqrt',
                                     'shampoo-no-sqrt-Fisher']:

            for ii in range(len(H[l][name_variable])):
            
                if params['shampoo_if_coupled_newton']:
            
                    H_l_LM_minus_2k.append(
                        coupled_newton(
                            H[l][name_variable][ii], 
                            len(H[l][name_variable]) / power_preconditioner,
                            epsilon, 
                            device,
                            False,
                        )
                    )
                
#                     print('H_l_LM_minus_2k[-1].size()')
#                     print(H_l_LM_minus_2k[-1].size())
                    
#                     print('H_l_LM_minus_2k[-1]')
#                     print(H_l_LM_minus_2k[-1])
                    
#                     print('H[l][name_variable][ii]')
#                     print(H[l][name_variable][ii])
                    
#                     print('torch.mm(H_l_LM_minus_2k[-1], H[l][name_variable][ii]) + epsilon * H_l_LM_minus_2k[-1]')
#                     print(torch.mm(H_l_LM_minus_2k[-1], H[l][name_variable][ii]) + epsilon * H_l_LM_minus_2k[-1])
                    
#                     sys.exit()
                    
                else:
                    # this is the default of params['shampoo_if_coupled_newton']

                    H_l_ii_LM = H[l][name_variable][ii] + epsilon * torch.eye(H[l][name_variable][ii].shape[0], device=device)

    #                 if_np_svd = False
    #                 if_cpu_svd = False

    #                     try:
    #                         H_l_U, H_l_S, H_l_V = torch.svd(H_l_ii_LM)
    #                     except:

    #                         np.save('gpu_svd.npy', H_l_ii_LM.detach().cpu().numpy())

    #                         import pickle
    #                         with open('gpu_svd.pkl', 'wb') as fp:
    #                             pickle.dump(H_l_ii_LM.detach().cpu().numpy(), fp)


    #                     H_l_U, H_l_S, H_l_V = torch.svd(H_l_ii_LM)

    #                 H_l_U, H_l_S, H_l_V = get_svd_by_cpu(H_l_ii_LM, params)


                    try:
                        H_l_U, H_l_S, H_l_V = torch.svd(H_l_ii_LM)

                        if torch.sum(H_l_S != H_l_S) or\
                        torch.sum(H_l_U != H_l_U) or\
                        torch.sum(H_l_V != H_l_V):
                            H_l_U, H_l_S, H_l_V = get_svd_by_cpu(H_l_ii_LM, params)
                    except:
                        H_l_U, H_l_S, H_l_V = get_svd_by_cpu(H_l_ii_LM, params)





                    power_H_l_LM_minus_2k = power_preconditioner / len(H[l][name_variable])

                    H_l_LM_minus_2k.append(
                        torch.mm(
                            torch.mm(
                                H_l_U, 
                                torch.diag(
                                    1/(H_l_S**power_H_l_LM_minus_2k)
                                )
                            ), 
                            H_l_V.t()
                        )
                    )

        else:
            print('Error: unkown algo in svd for ' + params['algorithm'])
            sys.exit()

        data_['shampoo_H_LM_minus_2k'][l][name_variable] = H_l_LM_minus_2k
        data_['shampoo_H_trace'][l][name_variable] = H_l_trace

def shampoo_compute_direction_per_variable(model_grad, l, name_variable, data_, params):
    # model_grad: 1st-order estimate
    
    if get_if_shampoo_update(name_variable, params):
        
        H_l_LM_minus_2k = data_['shampoo_H_LM_minus_2k'][l][name_variable]
        H_l_trace = data_['shampoo_H_trace'][l][name_variable]
        
        delta_l = model_grad[l][name_variable]
        
        delta_l = get_tensor_reshape(delta_l, l, name_variable, params)
        
        
        for ii in range(len(H_l_LM_minus_2k)):
            
            delta_l = torch.tensordot(delta_l, H_l_LM_minus_2k[ii], dims=([0], [0]))
        
        
        delta_l = get_tensor_reshape_back(delta_l, l, name_variable, params)
        
        # re-scaling for matrix-normal
        if params['algorithm'] in ['matrix-normal',
                                   'matrix-normal-allVariables',
                                   'matrix-normal-allVariables-warmStart',
                                   'matrix-normal-allVariables-warmStart-MaxEigDamping',
                                   'matrix-normal-allVariables-warmStart-noPerDimDamping',
                                   'matrix-normal-correctFisher-allVariables-filterFlattening-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-lessInverse',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-MaxEigWithEpsilonDamping',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-AvgEigWithEpsilonDamping',
                                   'matrix-normal-correctFisher-allVariables-KFACReshaping-warmStart-TraceWithEpsilonDamping',]:
            delta_l = delta_l * (np.asarray(H_l_trace).prod())**(1 - 1/len(delta_l.size()))


        elif params['algorithm'] in ['shampoo',
                                     'shampoo-allVariables',
                                     'shampoo-allVariables-warmStart',
                                     'shampoo-allVariables-warmStart-lessInverse',
                                     'shampoo-allVariables-filterFlattening-warmStart',
                                     'shampoo-allVariables-filterFlattening-warmStart-lessInverse',
                                     'shampoo-no-sqrt',
                                     'shampoo-no-sqrt-Fisher',
                                     'matrix-normal-same-trace',
                                     'matrix-normal-same-trace-warmStart',
                                     'matrix-normal-same-trace-warmStart-noPerDimDamping',
                                     'matrix-normal-same-trace-allVariables',
                                     'matrix-normal-same-trace-allVariables-warmStart',
                                     'matrix-normal-same-trace-allVariables-warmStart-AvgEigDamping',
                                     'matrix-normal-same-trace-allVariables-warmStart-MaxEigDamping',
                                     'matrix-normal-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-same-trace-allVariables-warmStart-noPerDimDamping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-lessInverse',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart',
                                     'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-lessInverse',
                                     'matrix-normal-EF-same-trace-allVariables-filterFlattening-warmStart',]:
            1
        else:
            print('Error: unknown algo in tensor dot for ' + params['algorithm'])
            sys.exit()
            
        return delta_l
    else:
        return model_grad[l][name_variable]

def get_svd_by_cpu(H_l_ii_LM, params):
    device = params['device']

    if_cpu_svd = True
    H_l_cpu = H_l_ii_LM.cpu()

    try:
        H_l_U_cpu, H_l_S_cpu, H_l_V_cpu = torch.svd(H_l_cpu)
    except:
        
#         print('should not reach here')
#         sys.exit()
        
        if_np_svd = True
        np_H_l_U_cpu, np_H_l_S_cpu, np_H_l_V_cpu = np.linalg.svd(H_l_cpu.data.numpy())
        np_H_l_V_cpu = np.transpose(np_H_l_V_cpu)
        H_l_U_cpu, H_l_S_cpu, H_l_V_cpu = \
        torch.from_numpy(np_H_l_U_cpu),\
        torch.from_numpy(np_H_l_S_cpu), torch.from_numpy(np_H_l_V_cpu)

    H_l_U, H_l_S, H_l_V =\
    H_l_U_cpu.to(device), H_l_S_cpu.to(device), H_l_V_cpu.to(device)

    return H_l_U, H_l_S, H_l_V



def shampoo_update(data_, params):
    
    true_algorithm = params['algorithm']


#     model_grad = data_['model_regularized_grad_used_torch']
    model_grad = data_['model_grad_used_torch']
    
    
    
    if params['matrix_name'] in ['Fisher',
                                 'Fisher-correct']:
        model_grad_N1 = data_['model_grad_N2'] # unregularized
    elif params['matrix_name'] == 'None':
        # None is also EF
        model_grad_N1 = data_['model_grad_torch']
    else:
        print('params[matrix_name]')
        print(params['matrix_name'])
        sys.exit()
        

    i = params['i']

    

    # alpha = params['alpha']
    numlayers = params['numlayers']

    device = params['device']
    
#     weight_ = max(1/(i+1), weight_)

    
        
        
    # Step
    delta = []
    for l in range(numlayers):

        for name_variable in data_['model'].layers_weight[l].keys():
            shampoo_kron_matrices_per_variable(model_grad_N1, l, name_variable, data_, params)
        

    




        for name_variable in data_['model'].layers_weight[l].keys():
            shampoo_inversion_per_variable(model_grad_N1, l, name_variable, data_, params)
            


        
        
        
        

        # store the delta
        dict_delta_l = {}
        
        for name_variable in data_['model'].layers_weight[l].keys():
            dict_delta_l[name_variable] = shampoo_compute_direction_per_variable(model_grad, l, name_variable, data_, params)

            
        delta.append(dict_delta_l)



    p = get_opposite(delta)


    
    
    
    data_['p_torch'] = p

    if true_algorithm == 'matrix-normal-LM-momentum-grad':
        params['algorithm'] = true_algorithm

    return data_, params
Output:
{
    "experimental_code": "def train_model(home_path = '/home/jupyter/',dataset_name = 'CIFAR-10',algorithm = 'TNT',lr = 1e-4,damping_value = 0.01,weight_decay = 0,):args = {}args['list_lr'] = [lr]args['weight_decay'] = weight_decayif dataset_name == 'CIFAR-10':args['dataset'] = 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'args['initialization_pkg'] = 'kaiming_normal'elif dataset_name == 'CIFAR-100':args['dataset'] = 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization'args['initialization_pkg'] = 'normal'elif dataset_name == 'MNIST':args['dataset'] = 'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization'args['initialization_pkg'] = 'normal'elif dataset_name == 'FACES':args['dataset'] = 'FacesMartens-autoencoder-relu-no-regularization'args['initialization_pkg'] = 'normal'else:print('dataset_name')print(dataset_name)sys.exit()if dataset_name in ['MNIST']:args['if_max_epoch'] = 0args['max_epoch/time'] = 500elif dataset_name in ['FACES']:args['if_max_epoch'] = 0args['max_epoch/time'] = 2000elif dataset_name in ['CIFAR-10', 'CIFAR-100']:if algorithm in ['SGD-m', 'Adam']:args['if_max_epoch'] = 1args['max_epoch/time'] = 200args['num_epoch_to_decay'] = 60args['lr_decay_rate'] = 0.1elif algorithm in ['TNT', 'Shampoo', 'KFAC']:args['if_max_epoch'] = 1args['max_epoch/time'] = 100args['num_epoch_to_decay'] = 40args['lr_decay_rate'] = 0.1else:print('algorithm')print(algorithm)sys.exit()else:print('dataset_name')print(dataset_name)sys.exit()args['momentum_gradient_rho'] = 0.9if algorithm == 'SGD-m':args['momentum_gradient_dampening'] = 0if dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'SGD-momentum'elif dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'SGD-LRdecay-momentum'else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm == 'Adam':args['RMSprop_epsilon'] = damping_valueargs['RMSprop_beta_2'] = 0.999args['momentum_gradient_dampening'] = 0.9if dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'Adam-noWarmStart-momentum-grad-LRdecay'elif dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'Adam-noWarmStart-momentum-grad'else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm in ['TNT', 'Shampoo']:if algorithm in ['Shampoo']:args['shampoo_if_coupled_newton'] = Trueelif algorithm in ['TNT']:args['shampoo_if_coupled_newton'] = Falseargs['shampoo_epsilon'] = damping_valueargs['if_Hessian_action'] = Falseargs['shampoo_decay'] = 0.9args['shampoo_weight'] = 0.1args['momentum_gradient_dampening'] = 0if dataset_name in ['CIFAR-10', 'CIFAR-100']:if algorithm == 'TNT':args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'elif algorithm == 'Shampoo':args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'args['shampoo_update_freq'] = 10args['shampoo_inverse_freq'] = 100elif dataset_name in ['MNIST', 'FACES']:if algorithm == 'TNT':args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'elif algorithm == 'Shampoo':args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'args['shampoo_update_freq'] = 1args['shampoo_inverse_freq'] = 20else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm == 'KFAC':args['kfac_if_update_BN'] = Trueargs['kfac_if_BN_grad_direction'] = Trueargs['kfac_rho'] = 0.9args['kfac_damping_lambda'] = damping_valueargs['momentum_gradient_dampening'] = 0if dataset_name in ['FACES', 'MNIST']:args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad'args['kfac_if_svd'] = Falseargs['kfac_cov_update_freq'] = 1args['kfac_inverse_update_freq'] = 20elif dataset_name in ['CIFAR-100', 'CIFAR-10']:args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay'args['kfac_if_svd'] = Falseargs['kfac_cov_update_freq'] = 10args['kfac_inverse_update_freq'] = 100else:print('dataset_name')print(dataset_name)sys.exit()else:print('algorithm')print(algorithm)sys.exit()args['record_epoch'] = 1args['seed_number'] = 9999args['num_threads'] = 8args['home_path'] = home_pathargs['if_gpu'] = Trueargs['if_test_mode'] = Falseargs['if_auto_tune_lr'] = Falseargs['if_grafting'] = False_ = tune_lr(args)return",
    "experimental_info": "Method: TNT\nAlgorithm name: 'TNT'\nLearning rate (lr): 1e-4\nDamping value: 0.01\nWeight decay: 0\nMomentum gradient rho: 0.9\nMomentum gradient dampening: 0\n\nDataset-specific settings for CIFAR-10:\n  Dataset argument: 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'\n  Initialization package: 'kaiming_normal'\n  Max epoch/time: 100 epochs\n  Learning rate decay: True\n  Number of epochs to decay LR: 40\n  LR decay rate: 0.1\n  Algorithm argument: 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'\n  Shampoo update frequency: 10\n  Shampoo inverse frequency: 100\n  Tau (regularization parameter, from from_dataset_to_N1_N2): 0\n\nDataset-specific settings for MNIST/FACES:\n  Algorithm argument: 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'\n  Shampoo update frequency: 1\n  Shampoo inverse frequency: 20\n\nCommon settings for TNT:\n  Shampoo if coupled newton: False\n  Shampoo epsilon: 0.01 (damping_value)\n  If Hessian action: False\n  Shampoo decay: 0.9\n  Shampoo weight: 0.1\n\nGeneral experimental settings:\n  Record epoch: 1\n  Seed number: 9999\n  Number of threads: 8\n  Home path: '/home/jupyter/' (default)\n  If GPU: True\n  If test mode: False\n  If auto tune LR: False\n  If grafting: False\n  If LR decay: True (handled by get_params, e.g., for CIFAR-10/100 setup)\n  If momentum gradient: True (handled by get_params)"
}
