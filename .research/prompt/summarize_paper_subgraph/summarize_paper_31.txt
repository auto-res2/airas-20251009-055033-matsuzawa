
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Published as a conference paper at ICLR 2024 IDENTIFYING POLICY GRADIENT SUBSPACES Jan Schneider1∗ Pierre Schumacher1,2 Simon Guist1 Le Chen1 Daniel H¨aufle2,3 Bernhard Sch¨olkopf1 Dieter B¨uchler1 1Max Planck Institute for Intelligent Systems, T¨ubingen, Germany 2Hertie Institute for Clinical Brain Research, T¨ubingen, Germany 3Institute for Computer Engineering, University of Heidelberg, Germany ABSTRACT Policy gradient methods hold great potential for solving complex continuous con- trol tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various sim- ulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to rein- forcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization. 1 I NTRODUCTION Deep reinforcement learning (RL) has marked significant achievements in numerous challeng- ing problems, ranging from Atari games (Mnih et al., 2013) to various real robotic challenges, such as contact-rich manipulation (Gu et al., 2017; Kalashnikov et al., 2018), complex plan- ning problems (Everett et al., 2018; Ao et al., 2022), and hard-to-control dynamic tasks (Cheng et al., 2023; Kaufmann et al., 2023). Despite these notable successes, deep RL methods are often brittle due to the use of function approximators with large numbers of parameters and persistently changing data distributions – a setting notoriously hard for optimization. Deep RL, in its vanilla form, operates under limited prior knowledge and structural information about the problem, conse- quently requiring large numbers of interactions with the environment to reach good performance. For supervised learning (SL), Gur-Ari et al. (2018) demonstrated that the gradients utilized for neu- ral network optimization reside in a low-dimensional, slowly-changing subspace. Based on this insight, recent works introduce more structured optimization procedures for SL by identifying and harnessing these gradient subspaces. Exploiting this structure enables the optimization to be carried out in a reduced-dimensional subspace, yielding enhanced efficiency with minimal, if any, loss in performance (Li et al., 2018; Gressmann et al., 2020; Larsen et al., 2021; Li et al., 2022a). Despite the benefits of subspace methods in SL, their adoption in deep RL has remained limited. A straightforward way to transfer these principles is to find lower-dimensional subspaces in policy gradient approaches (Peters & Schaal, 2008). Policy gradient (PG) methods estimate the gradient of the RL objective ∇θJ(θ) to update the policy’s parameters θ using some form of stochastic gradient descent (SGD). Since most SL approaches using subspaces operate at the level of the SGD optimization, PG algorithms would be a natural choice to leverage the knowledge about subspaces from SL in the RL context. Nevertheless, in RL, such methods have been explored primarily within the realm of evolutionary strategies (Maheswaranathan et al., 2019), representation learning (Le Lan et al., 2023), and transfer learning (Gaya et al., 2022). A possible explanation is the constantly changing data distribution of RL due to continual exploration that intuitively seems to hinder the identification of gradient subspaces. The limited body of studies using subspaces in PG algorithms underlines the need for a more profound discussion in this domain. ∗Correspondence to jan.schneider@tuebingen.mpg.de 1 arXiv:2401.06604v3  [cs.LG]  18 Mar 2024Published as a conference paper at ICLR 2024 This paper conducts a comprehensive empirical evaluation of gradient subspaces in the context of PG algorithms, assessing their properties across various simulated RL benchmarks. Our experi- ments reveal several key findings: (i) there exist parameter-space directions that exhibit significantly larger curvature compared to other parameter-space directions, (ii) the gradients live in the subspace spanned by these directions, and (iii) the subspace remains relatively stable throughout the RL train- ing. Additionally, we analyze the gradients of the critic – an integral part of the PG estimation in actor-critic methods – and observe that the critic subspace often exhibits less variability and retains a larger portion of its gradient compared to the actor subspace. We also test the robustness of PG subspaces regarding mini-batch approximations of the gradient that are used in practice during train- ing and evaluate a similar mini-batch approximation of the Hessian. Lastly, we explore the extent to which the variation in the data distribution influences the aforementioned subspace analysis by conducting experiments with both an on-policy as well as an off-policy algorithm, the latter of which reuses previously collected data for training. By shedding light on gradient subspaces in deep RL, this paper provides insights that can potentially enhance RL performance by advancing parameter-space exploration or enabling second-order opti- mization. We begin by reviewing existing literature on subspace approaches in Section 2, followed by a recapitulation of the RL preliminaries in Section 3 as a foundation for the analysis of gradient subspaces in RL in Section 4. Section 5 concludes this work with a discussion of the results and implications of our work. The code for our experiments is available on the project website. 2 R ELATED WORK Numerous works have studied the use of gradient subspaces in SL. These works can be roughly divided into informed and random subspace approaches. In the following, we give an overview of these papers and highlight works that investigate related concepts in RL. Informed subspaces in supervised learning Informed subspace methods identify the subspace from some part of the training process. For instance, Gur-Ari et al. (2018) show that the gradients used for neural network optimization lie in a low-dimensional subspace of the parameter-space di- rections with the highest curvature in the loss. Furthermore, they show that this subspace changes slowly throughout the training. Recent works have highlighted numerous benefits of these insights. Li et al. (2022a) apply principal component analysis on the network parameters at previous check- points and use the top principal components as subspace. They apply SGD and BFGS algorithms in the subspace and demonstrate superior learning performance compared to training in the original parameter space. Similarly, Tuddenham et al. (2020) propose to use a BFGS method in a low- dimensional subspace defined by the full-batch gradients with respect to individual classification labels. Sohl-Dickstein et al. (2014) construct a second-order Taylor approximation of individual loss terms efficiently in a low-dimensional subspace that captures recently observed gradients. With this approximation, the overall loss is a sum of quadratic functions that can be optimized in closed form. Chen et al. (2022) utilize insights about the low-dimensional subspaces to learn an optimizer that scales to high-dimensional parameter spaces. Traditionally, learned optimizers are limited in scale by the high output dimensionality of the network. By outputting parameter updates in the low-dimensional subspace, Chen et al. circumvent this challenge and efficiently learn to optimize large neural networks. Gauch et al. (2022) apply the knowledge of low-dimensional gradient sub- space to few-shot adaptation by identifying a suitable subspace on the training task and utilizing it for efficient finetuning on the test task. Likewise, Zhou et al. (2020) tackle differential privacy by identifying a gradient subspace on a public dataset and projecting gradients calculated from the pri- vate dataset into this subspace. Adding noise in the subspace instead of the parameter space lowers the error bound of differential privacy to be linear in the number of subspace dimensions instead of the parameter dimensions. Li et al. (2022b) tackle an overfitting problem of adversarial training by identifying a gradient subspace in an early training phase before overfitting occurs. By projecting later gradients into this subspace, they enforce that the training dynamics remain similar to the early training phase, which mitigates the overfitting problem. Random subspaces in supervised learning Gressmann et al. (2020) describe different modifica- tions to improve neural network training in random subspaces, like resampling the subspace reg- ularly and splitting the network into multiple components with individual subspaces. They utilize these subspaces to reduce the communication overhead in a distributed learning setting by send- 2Published as a conference paper at ICLR 2024 ing the parameter updates as low-dimensional subspace coefficients. Li et al. (2018) quantify the difficulty of a learning task by its intrinsic dimensionality, i.e., the dimensionality of the random subspace needed to reach good performance. Aghajanyan et al. (2021) apply a similar analysis to language models and find that the effectiveness of fine-tuning language models is tied to a low in- trinsic dimensionality. Based on these findings, Hu et al. (2021) propose a method for fine-tuning large models efficiently via low-rank parameter updates. Larsen et al. (2021) compare informed and random subspaces and find that with informed subspaces, typically significantly fewer dimensions are needed for training. Furthermore, they note the benefit of first training in the original space for some time before starting subspace training, as this increases the probability of the subspace intersecting with low-loss regions in the parameter space. Subspace approaches to reinforcement learning Only a handful of works use subspace methods for RL. Tang et al. (2023) identify a parameter subspace through singular value decomposition of the policy parameters’ temporal evolution. They propose to cut off the components of the update step that lie outside this subspace and additionally elongate the update in the directions corresponding to the largest singular values. In contrast to our work, they focus solely on the policy network, consider only off-policy learning algorithms, and do not establish a connection to the curvature of the learning objective. Li et al. (2018) quantify the difficulty of RL tasks by their intrinsic dimensionality. To that end, the authors optimize the value function of Deep Q-Networks (DQN) (Mnih et al., 2013) as well as the population distribution – a distribution over the policy parameters – of evolutionary strategies (ES) (Salimans et al., 2017) in random subspaces. Maheswaranathan et al. (2019) define a search distribution for ES that is elongated along an informed subspace spanned by surrogate gradients. While the authors do not evaluate their approach for RL, they highlight the relevance to RL due to the method’s ability to use surrogate gradients generated from learned models, similar to the use of the critic in actor-critic methods. Gaya et al. (2022) enable improved generalization in online RL by learning a subspace of policies. Recently, they extended their method to scale favorably to continual learning settings (Gaya et al., 2023). Both methods explicitly learn a subspace of policy parameters, while our work investigates the natural training dynamics of PG methods. 3 P RELIMINARIES This section introduces the mathematical background and notation used throughout the paper. Fur- thermore, we briefly describe the two RL algorithms that we will analyze in Section 4. 3.1 M ATHEMATICAL BACKGROUND AND NOTATION For a given objective functionJ(θ), we useg = ∂ ∂θ J(θ) ∈ Rn to denote the gradient of a model with respect to its parameters θ and H = ∂2 ∂2θ J(θ) ∈ Rn×n to denote the corresponding Hessian matrix. We use vi to denote the ith largest eigenvector of H. Note that we use “ ith largest eigenvector” as shorthand for “eigenvector with respect to the ith largest eigenvalue”. Since H is symmetric, all eigenvectors are orthogonal to each other, and we assume ||vi|| = 1. In this work, we investigate projections of gradients into lower-dimensional subspaces, i.e., map- pings from Rn to Rk with k ≪ n. These mappings are defined by a projection matrix P ∈ Rk×n. ˆg = P g∈ Rk denotes the projection of g into the subspace and ˜g = P+ˆg ∈ Rn is the mapping of ˆg back to the original dimensionality that minimizes the projection error ||g − ˜g||. Here, P+ denotes the pseudoinverse of P. If the projection matrix is semi-orthogonal, i.e., the columns are orthogonal and norm one, the pseudoinverse simplifies to the transpose P+ = PT . The matrix of the k largest eigenvectors P = (v1, . . . , vk)T is one example of such a semi-orthogonal matrix. 3.2 R EINFORCEMENT LEARNING We consider tasks formulated as Markov decision processes (MDPs), defined by the tuple (S, A, p, r). Here, S and A are the state and action spaces, respectively. The transition dynam- ics p: S × A × S →[0, ∞) define the probability density of evolving from one state to another. At each timestep the agent receives a scalar reward r: S × A →R. A stochastic policy, πθ(at | st), defines a mapping from state st to a probability distribution over actions at. RL aims to find an optimal policy π∗, maximizing the expected cumulative return, discounted by γ ∈ [0, 1]. 3Published as a conference paper at ICLR 2024 The value function V π(s) represents the expected (discounted) cumulative reward from state s following policy π, and the action value function Qπ(s, a) denotes the expected (discounted) cu- mulative reward for taking action a in state s and then following π. The advantage function Aπ(s, a) = Qπ(s, a) − V π(s) quantifies the relative benefit of taking an action a in state s over the average action according to policy π. RL algorithms generally can be divided into two styles of learning. On-policy methods, like Prox- imal Policy Optimization (PPO) (Schulman et al., 2017), only use data generated from the current policy for updates. In contrast, off-policy algorithms, such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018) leverage data collected from different policies, such as old iterations of the policy. 3.2.1 P ROXIMAL POLICY OPTIMIZATION On-policy PG methods typically optimize the policy πθ via an objective such as J(θ) = Eπθ h πθ(at | st) ˆAt i = Eπold h rt(θ) ˆAt i with rt(θ) = πθ(at|st) πold(at|st) and ˆAt being an estimator of the advantage function at timestep t and πold denoting the policy before the update (Kakade & Lang- ford, 2002). However, optimizing this objective can result in excessively large updates, leading to instabilities and possibly divergence. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an on-policy actor-critic algorithm designed to address this issue by clipping the probability ratio rt(θ) to the interval [1−ϵ, 1 +ϵ], which removes the incentive for movingrt(θ) outside the interval, resulting in the following actor loss. JPPO actor(θ) = Eπold h min  rt(θ) ˆAt, clip (rt(θ), 1 − ϵ, 1 + ϵ) ˆAt i (1) The advantage estimation ˆAt = δt + (γλ)δt+1 +··· + (γλ)T−t+1δT−1 with δt = rt +γVϕ(st+1)− Vϕ(st) uses a learned value function Vϕ, which acts as a critic. The hyperparameter λ determines the trade-off between observed rewards and estimated values. The critic is trained to minimize the mean squared error between the predicted value and the discounted sum of future episode rewards V target t = Pt+T τ=t γτ−trτ . JPPO critic(ϕ) = E  (Vϕ(st) − V target t )2 (2) 3.2.2 S OFT ACTOR -CRITIC Soft Actor-Critic (SAC) (Haarnoja et al., 2018) is a policy gradient algorithm that integrates the maximum entropy reinforcement learning framework with the actor-critic approach. As such, it op- timizes a trade-off between the expected return and the policy’s entropy. It is an off-policy algorithm and, as such, stores transitions in a replay buffer D, which it samples from during optimization. To that end, SAC modifies the targets for the learned Q-function Qϕ to include a term that incen- tivizes policies with large entropy ˆQt = rt + γ(Qϕ(st+1, at+1) − log πθ(at+1 | st+1)), resulting in the following critic loss. JSAC critic(ϕ) = ED,πθ 1 2  Qϕ(st, at) − ˆQt 2 (3) Note that SAC, in its original formulation, trains an additional value function and a second Q- function, but we omitted these details for brevity. The algorithm then trains the actor to minimize the KL-divergence between the policy and the exponential of the learned Q-function. JSAC actor(θ) = ED  DKL  πθ(· |st)  exp(Qϕ(st, ·)) Zϕ(st)  (4) Zϕ(st) denotes the normalization to make the right side of the KL-divergence a proper distribution. Optimizing this loss increases the probability of actions with high value under the Q-function. 4 G RADIENT SUBSPACES IN POLICY GRADIENT ALGORITHMS In Section 2, we have highlighted several works from SL that utilize low-dimensional gradient sub- spaces for improving the learning performance. Naturally, we would like to transfer these benefits 4Published as a conference paper at ICLR 2024 to policy gradient algorithms. However, the training in RL is significantly less stationary than in the supervised setting (Bjorck et al., 2022). As the RL agent changes, the data distribution shifts since the data is generated by the agent’s interactions with its environment. Furthermore, the value of a state also depends on the agent’s behavior in future states. Thus, the targets for the actor and critic networks change constantly. These crucial differences between SL and RL underscore the need to analyze to which extent insights about gradient subspaces transfer between these settings. The analysis presented in this section focuses on two policy gradient algorithms: PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018), which are popular instantiations of on-policy and off-policy RL. We apply the algorithms to twelve benchmark tasks from OpenAI Gym (Brockman et al., 2016), Gym Robotics (Plappert et al., 2018a), and the DeepMind Control Suite (Tunyasuvu- nakool et al., 2020). Our code builds upon the algorithm implementations ofStable Baselines3 (Raf- fin et al., 2021). The learning curves are displayed in Appendix A. We ran each experiment for 10 random seeds and plot the mean and standard deviation for the results in Sections 4.2 and 4.3. Due to space constraints, we show the analysis results only for selected tasks and present detailed results for all twelve tasks in Appendix B. Moreover, we conduct an evaluation of the impact of the RL algorithm’s hyperparameters on the gradient subspace in Appendix C. For the following analyses, we calculate Hessian eigenvectors of the loss with respect to the net- work parameters via the Lanczos method (Lehoucq et al., 1998) since it is an efficient method for estimating the top eigenvectors that avoids explicitly constructing the Hessian matrix. Since we can only estimate the Hessian from data, we use a large number of state-action pairs to obtain precise estimates for the eigenvectors of the true Hessian, similar to how Ilyas et al. (2020) approximate the true policy gradient. For PPO, we collect 106 on-policy samples. This would, however, not be faithful to the diverse distribution of off-policy data that SAC uses for training. To match this data distribution for the analysis, we save the replay buffer during training and use the data of the complete replay buffer for estimating the Hessian. Note that the replay buffer also has a capacity of 106 samples but is not completely filled at the beginning of training. As mentioned in Section 3, SAC and PPO each train two different networks, an actor and a critic. We, therefore, conduct our analysis for each network individually. To verify that there exist high- curvature directions spanning a subspace that stays relatively stable throughout the training and that contains the gradient, we check three conditions: i) Some parameter-space directions exhibit significantly larger curvature in the actor/critic loss than other directions. ii) The actor/critic gradient mainly lies in the subspace spanned by these directions. iii) The subspaces for the actor and critic networks change slowly throughout the training. 4.1 T HE LOSS CURVATURE IS LARGE IN A FEW PARAMETER -SPACE DIRECTIONS The Hessian matrix describes the curvature of a function, with the eigenvectors being the directions of maximum and minimum curvature. The corresponding eigenvalues describe the magnitude of the curvature along these directions. Therefore, we verify condition i) by plotting the spectrum of Hessian eigenvalues for the actor and critic loss of PPO with respect to the network parameters in Figure 1. The plots show that there are a few large eigenvalues for both the actor and critic loss. All remaining eigenvalues are distributed close to zero. These plots confirm that there are a few directions with significantly larger curvature; in other words, the problem is ill-conditioned. 4.2 T HE GRADIENT LIES IN THE HIGH -CURVATURE SUBSPACE To verify condition ii) that the high-curvature subspace contains the gradients of the respective loss, we measure how well these gradients can be represented in the subspace. Let Pk ∈ Rk×n be the semi-orthogonal matrix that projects into the high-curvature subspace. Pk consists row-wise of the k largest Hessian eigenvectors. We compute the relative projection error, i.e., the relative difference between the original gradient g ∈ Rn and the projected gradient ˜g = P+ k Pkg that is the result of mapping into the high-curvature subspace and back into the original space. The fraction of the gradient that can be represented in the subspace is then given by Sfrac(Pk, g) = 1 − ||˜g − g||2 ||g||2 , (5) 5Published as a conference paper at ICLR 2024 0 2000 4000 Index 25 0 25 Eigenvalue (a) Finger-spin, actor 0 2000 4000 Index 1000 0 1000 (b) Finger-spin, critic 0 2000 4000 Index 100 0 100 (c) Walker2D, actor 0 2000 4000 Index 500 0 500 (d) Walker2D, critic Figure 1: The spectrum of the Hessian eigenvalues for PPO on the tasks Finger-spin (a, b) and Walker2D (c, d). The Hessian is estimated from 106 state-action pairs. For both the actor (a, c) and critic (b, d) loss, there is a small number of large eigenvalues, while the bulk of the eigenvalues is close to zero. This finding shows that there is a small number of high-curvature directions in the loss landscapes, which is in accordance with results from SL. which simplifies to the following gradient subspace fraction criterion of Gur-Ari et al. (2018). Sfrac(Pk, g) = ||Pk g||2 ||g||2 (6) We derive this equality in Appendix D. Note that0 ≤ Sfrac(Pk, g) ≤ 1 holds, where Sfrac(Pk, g) = 1 implies that the subspace perfectly contains the gradient, while Sfrac(Pk, g) = 0 means that the gradient lies entirely outside the subspace. Due to the normalization by||g||, this criterion is invariant to the scale of the gradient, which enables comparing gradient subspaces of different models and models at different stages of the training. To evaluate how the gradient subspace fraction evolves over time, we evaluate the criterion at check- points every 50,000 steps during the RL training. To compactly visualize this data, we split the training into three phases: initial, training, and convergence, and for each phase, we average the results of all timesteps of that phase. Since the algorithms require different numbers of timesteps for solving each of the tasks and reach different performance levels, we define the following heuristic criterion for the training phases. We first smooth the learning curves by averaging over a sliding window and compute the maximum episode return Rmax over the smoothed curve. Next, we calcu- late the improvement relative to the episode returnRinit of the initial policy at each timestept of the smoothed learning curve as ∆R(t) = R(t) − Rinit Rmax − Rinit . (7) We then define the end of the initial phase as the first timestep at which the agent reaches 10% of the total improvement, i.e., ∆R(t) ≥ 0.1. Similarly, we define the start of the convergence phase as the first timestep at which the agent reaches 90% of the total improvement, i.e., ∆R(t) ≥ 0.9. We choose k = 100 as subspace dimensionality since this subspace already largely captures the gradients, and the largest 100 eigenvectors can still be calculated reasonably efficiently with the Lanczos method. Appendix B displays results for different subspace sizes. With the tuned hyperpa- rameters from RL Baselines3 Zoo that we use for training, the PPO actor and critic usually contain around 5,000 parameters, and the SAC actor and critic around 70,000 and 140,000 parameters (2 Q-networks `a 70,000 parameters), respectively. Hence, the subspace dimensionality is around 2% the size of the parameters for PPO and around 0.14% and 0.07% for SAC. We consider a precise approximation of the true gradient computed with 106 state-action pairs for PPO and the full replay buffer for SAC. We denote this approximation as precise gradient and the low-sample gradient used during regular RL training as mini-batch gradient. In a similar manner, we denote the Hessian estimated on the large dataset as precise Hessian and the estimate from 2048 samples as mini-batch Hessian. We choose 2048 samples for the mini-batch Hessian since that is the amount of data that PPO with default hyperparameters collects for the policy updates. Hence, this is a realistic setting for estimating the subspace during training. Figure 2 shows the value of the gradient subspace fraction Sfrac(Pk, g) for PPO and SAC on four different tasks, divided into the three training phases. Note that for an uninformed random pro- jection, the gradient subspace fraction would be k/n in expectation, i.e., the ratio of the original 6Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 1.0 Gradient subspace fractionPPO SAC Ant PPO SAC Finger-spin PPO SAC LunarLanderContinuous PPO SAC Walker2D Initial phase Precise gradient, precise Hessian Training phase Mini-batch gradient, precise Hessian Convergence phase Mini-batch gradient, mini-batch Hessian 0.0 0.2 0.4 0.6 0.8 1.0 Gradient subspace fraction PPO SAC Ant PPO SAC Finger-spin PPO SAC LunarLanderContinuous PPO SAC Walker2D (a) Actor 0.0 0.2 0.4 0.6 0.8 1.0 Gradient subspace fraction PPO SAC Ant PPO SAC Finger-spin PPO SAC LunarLanderContinuous PPO SAC Walker2D (b) Critic Figure 2: The fraction Sfrac of the gradient that lies within the high-curvature subspace spanned by the 100 largest Hessian eigenvectors. Results are displayed for the actor (top) and critic (bottom) of PPO and SAC on the Ant, Finger-spin, LunarLanderContinuous, and Walker2D tasks. The results demonstrate that a significant fraction of the gradient lies within the high-curvature subspace, but the extent to which the gradient is contained in the subspace depends on the algorithm, task, and training phase. For both algorithms, the gradient subspace fraction is significantly higher for the critic than for the actor. Furthermore, the quantity is also often larger for SAC’s actor than for PPO’s, particularly in the early stages of the training. Even with mini-batch estimates for the gradient and Hessian, the gradient subspace fraction is considerable. and subspace dimensionalities (0.02 for PPO and 0.0014 for SAC’s actor and 0.0007 for its critic). The results in Figure 2 show a significantly higher gradient subspace fraction, which means that the gradients computed by PPO and SAC lie to a large extent in the high-curvature subspace. We observe that the fraction of the gradient in the subspace is considerably higher for the critic than for the actor. Furthermore, the gradient subspace fraction is also often higher for SAC’s actor than for PPO’s. This finding is particularly significant since the subspace size corresponds to a significantly lower percentage of the parameter dimensionality for SAC than for PPO. We hypothesize that the effect is caused by the off-policy nature of SAC. In the off-policy setting, the training distribution for the networks changes slowly since the optimization reuses previous data. In this regard, SAC is closer than PPO to the supervised learning setting, where the data distribution is fixed and for which Gur-Ari et al. (2018) report high gradient subspace fractions. Still, the subspace fraction for PPO is significant, considering that the dimensionality of the subspace is merely 2% of the origi- nal parameter space. Furthermore, for PPO, the subspace fraction often improves after the initial phase. Similarly, Gur-Ari et al. (2018) report for the supervised learning setting that the gradient starts evolving in the subspace only after some initial steps. However, for the SAC actor, this trend appears to be reversed, with the gradient subspace fraction being highest in the initial steps. Moreover, the precise gradient, computed with a large number of samples, tends to lie better in the subspace than the mini-batch gradient. The noise resulting from the low-sample approximation seems to perturb the gradient out of the subspace. However, since the difference is typically small, the gradient subspace is still valid for the low-sample gradient estimates used during RL training. Lastly, even the subspace identified with the mini-batch Hessian captures the gradient to a significant extent. This property is crucial since it implies that we do not need access to the precise Hessian, which is costly to compute and might require additional data. Instead, we can already obtain a reasonable gradient subspace from the mini-batch Hessian. 7Published as a conference paper at ICLR 2024 t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap PPO, actor PPO, critic SAC, actor SAC, critic t1 t1 0  1  2  3  4  5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (a) Ant t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (b) Finger-spin t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (c) LunarLanderCont. t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (d) Walker2D Figure 3: Evolution of the overlap between the high-curvature subspace identified at an early timestep t1 = 100 ,000 and later timesteps for the actor and critic of PPO and SAC. While the overlap between the subspaces degrades as the networks are updated, it remains considerable even after 400,000 timesteps, indicating that the subspace remains similar, even under significant changes in the network parameters and the data distribution. This finding implies that information about the gradient subspace at earlier timesteps can be reused at later timesteps. 4.3 T HE HIGH -CURVATURE SUBSPACE CHANGES SLOWLY THROUGHOUT THE TRAINING So far, we have verified that the gradients of the actor and critic losses optimized by PPO and SAC lie to a large extent in the subspace spanned by the top eigenvectors of the Hessian with respect to the current parameters. However, even though there are relatively efficient methods for computing the top Hessian eigenvectors without explicitly constructing the Hessian matrix, calculating these eigenvectors at every step would be computationally expensive. Ideally, we would like to identify a subspace once that remains constant throughout the training. In practice, however, the gradient subspace will not stay exactly the same during the training, but if it changes slowly, it is possible to reuse knowledge from earlier timesteps and update the subspace at a lower frequency. To that end, we investigate condition iii) by calculating the subspace overlap, defined by Gur-Ari et al. (2018). The subspace overlap between timesteps t1 and t2 is defined as Soverlap  P(t1) k , P(t2) k  = 1 k kX i=1 P(t1) k v(t2) i  2 , (8) where v(t) i is the ith largest eigenvector at timestep t. P(t) k =  v(t) 1 , . . . , v(t) k T denotes the pro- jection matrix from the full parameter space to the high-curvature subspace, identified at timestep t. Similar to Equation (5), the criterion measures how much of the original vector is preserved during the projection into the subspace. For the subspace overlap, however, we use the projection matrix at timestep t1 not to project the gradient but rather project the Hessian eigenvectors that span the high-curvature subspace identified at a later timestep t2 of the training. This criterion, thus, mea- sures how much the gradient subspace changes between these timesteps. Note that we assume the eigenvectors v(t) i to be normalized to one and therefore do not normalize by their length. Gur-Ari et al. (2018) showed in the supervised setting that the gradient subspace stabilizes only after some initial update steps. Therefore, we choose the timestep t1 at which we initially identify the subspace as t1 = 100 ,000 since this is still relatively early in the training, but the gradient subspace should already have stabilized reasonably well. We evaluate the subspace overlap criterion every 10,000 timesteps until timestep 500,000. This interval covers a significant portion of the training and showcases the extent to which the subspace changes under significant differences in the network parameters and the data distribution. For the sake of completeness and to further highlight the influence of the data distribution on the subspace, we showcase the subspace overlap over the entire duration of the training in Appendix E. As in Section 4.2, we use k = 100 as subspace dimensionality and refer to Appendix B for the evaluation of different subspace sizes. The analysis results in Figure 3 show that the subspace overlap reduces the further apart the two timesteps t1 and t2 are, but in all cases, the subspace overlap remains significantly above zero, implying that information of previous subspaces can be reused at later timesteps. If the two timesteps are close to each other, the overlap is considerable. Similar to the gradient subspace fraction in Section 4.2, the subspace overlap is often more pronounced for the critic than the actor, particularly for SAC. 8Published as a conference paper at ICLR 2024 5 C ONCLUSION In this work, we showed that findings from the SL literature about gradient subspaces transfer to the RL setting. Despite the continuously changing data distribution inherent to RL, the gradients of the actor and critic networks of PPO and SAC lie in a low-dimensional, slowly-changing subspace of high curvature. We demonstrated that this property holds for both on-policy and off-policy learning, even though the distribution shift in the training data is particularly severe in the on-policy setting. 5.1 H IGH -CURVATURE SUBSPACES EXPLAIN CLIFFS IN REWARD LANDSCAPES Sullivan et al. (2022) investigate visualizations of the reward landscapes around policies optimized by PPO. Reward landscapes describe the resulting cumulative rewards over the space of policy pa- rameters. They observe empirically that these landscapes feature “cliffs” in policy gradient direction. When changing the parameters in this direction, the cumulative reward increases for small steps but drops sharply beyond this increase. In random directions, these cliffs do not seem to occur. The results from Section 4.2 constitute a likely explanation of this phenomenon. The cliffs that the authors describe can be interpreted as signs of large curvature in the reward landscape. Our analysis demonstrates that the policy gradient is prone to lie in a high-curvature direction of the policy loss. Sullivan et al. investigate the cumulative reward, which is different from the policy loss that we analyze in this work. However, one of the fundamental assumptions of policy gradient methods is that there is a strong link between the policy loss and the cumulative reward. Therefore, high curvature in the loss likely also manifests in the cumulative reward. There is no such influence for random directions, so the curvature in gradient direction is larger than in random directions. 5.2 P OTENTIAL OF GRADIENT SUBSPACES IN REINFORCEMENT LEARNING Leveraging properties of gradient subspaces has proven beneficial in numerous works in SL, e.g., (Li et al., 2022a; Chen et al., 2022; Gauch et al., 2022; Zhou et al., 2020; Li et al., 2022b). The analyses in this paper demonstrate that similar subspaces can be found in popular policy gradient algorithms. In the following, we outline two opportunities for harnessing the properties of gradient subspaces and bringing the discussed benefits to RL. Optimization in the subspace While the network architectures used in reinforcement learning are often small compared to the models used in other fields of machine learning, the dimensionality of the optimization problem is still considerable. Popular optimizers, like Adam (Kingma & Ba, 2014), typically rely only on gradient information, as computing the Hessian at every timestep would be computationally very demanding in high dimensions. However, in Section 4.1, we have seen that the optimization problem is ill-conditioned. Second-order methods, like Newton’s method, are known to be well-suited for ill-conditioned problems (Nocedal & Wright, 1999). With the insights of this paper, it seems feasible to reduce the dimensionality of the optimization problems in RL algorithms by optimizing in the low-dimensional subspace instead of the original parameter space. The low dimensionality of the resulting optimization problems would enable computing and inverting the Hessian matrix efficiently and make second-order optimization methods feasible. Guiding parameter-space exploration The quality of the exploration actions significantly im- pacts the performance of RL algorithms (Amin et al., 2021). Most RL algorithms explore by ap- plying uncorrelated noise to the actions produced by the policy. However, this often leads to ineffi- cient exploration, particularly in over-actuated systems, where correlated actuation is crucial (Schu- macher et al., 2022). A viable alternative is to apply exploration noise to the policy parameters instead (R ¨uckstiess et al., 2010; Plappert et al., 2018b). This approach results in a more directed exploration and can be viewed as exploring strategies similar to the current policy. In Section 4, we observed that the gradients utilized by policy gradient methods predominantly lie within a small subspace of all parameter-space directions. As typical parameter-space exploration does not consider the properties of the training gradient when inducing parameter noise, only a small fraction of it might actually push the policy parameters along directions that are relevant to the task. Considering that the optimization mostly occurs in a restricted subspace, it might be advantageous to limit exploration to these directions. Sampling parameter noise only in the high-curvature subspace constitutes one possible way of focusing exploration on informative parameter-space directions. 9Published as a conference paper at ICLR 2024 6 R EPRODUCIBILITY We applied our analyses to proven and publicly available implementations of the RL algorithms from Stable-Baselines3 (Raffin et al., 2021) on well-known, publicly available benchmark tasks from (Brockman et al., 2016; Plappert et al., 2018b; Tunyasuvunakool et al., 2020). Further exper- imental details like the learning curves of the algorithms and the fine-grained analysis results for the entire training are displayed in Appendices A and B, respectively. To facilitate reproducing our results, we make our code, as well as the raw analysis data, including hyperparmeter settings and model checkpoints, publically available on the project website. REFERENCES Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the ef- fectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Papers), pp. 7319–7328, 2021. Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A survey of exploration methods in reinforcement learning. arXiv preprint arXiv:2109.00157, 2021. Yunke Ao, Le Chen, Florian Tschopp, Michel Breyer, Roland Siegwart, and Andrei Cramariuc. Uni- fied data collection for visual-inertial calibration via deep reinforcement learning. InInternational Conference on Robotics and Automation, pp. 1646–1652. IEEE, 2022. Johan Bjorck, Carla P Gomes, and Kilian Q Weinberger. Is high variance unavoidable in RL? A case study in continuous control. In International Conference on Learning Representations, 2022. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. Xuxi Chen, Tianlong Chen, Yu Cheng, Weizhu Chen, Ahmed Awadallah, and Zhangyang Wang. Scalable learning to optimize: A learned optimizer can train big models. InEuropean Conference on Computer Vision, pp. 389–405. Springer, 2022. Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme parkour with legged robots. arXiv preprint arXiv:2309.14341, 2023. Michael Everett, Yu Fan Chen, and Jonathan P How. Motion planning among dynamic, decision- making agents with deep reinforcement learning. In International Conference on Intelligent Robots and Systems, pp. 3052–3059. IEEE, 2018. Martin Gauch, Maximilian Beck, Thomas Adler, Dmytro Kotsur, Stefan Fiel, Hamid Eghbal-zadeh, Johannes Brandstetter, Johannes Kofler, Markus Holzleitner, Werner Zellinger, et al. Few-shot learning by dimensionality reduction in gradient space. In Conference on Lifelong Learning Agents, pp. 1043–1064. PMLR, 2022. Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for online adaptation in reinforcement learning. In International Conference of Learning Representations, 2022. Jean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, and Roberta Raileanu. Building a subspace of policies for scalable continual learning. In International Con- ference of Learning Representations, 2023. Frithjof Gressmann, Zach Eaton-Rosen, and Carlo Luschi. Improving neural network training in low dimensional random bases. Advances in Neural Information Processing Systems, 33:12140– 12150, 2020. Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In International Conference on Robotics and Automation, pp. 3389–3396. IEEE, 2017. 10Published as a conference paper at ICLR 2024 Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.arXiv preprint arXiv:1812.04754, 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Confer- ence on Machine Learning, pp. 1861–1870. PMLR, 2018. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. A closer look at deep policy gradients. In International Con- ference on Learning Representations, 2020. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, pp. 267–274, 2002. Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018. Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias M ¨uller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature, 620:982–987, 2023. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Brett W Larsen, Stanislav Fort, Nic Becker, and Surya Ganguli. How many degrees of freedom do we need to train deep networks: A loss landscape perspective. In International Conference on Learning Representations, 2021. Charline Le Lan, Joshua Greaves, Jesse Farebrother, Mark Rowland, Fabian Pedregosa, Rishabh Agarwal, and Marc G Bellemare. A novel stochastic gradient descent algorithm for learning principal subspaces. In International Conference on Artificial Intelligence and Statistics , pp. 1703–1718. PMLR, 2023. Richard B Lehoucq, Danny C Sorensen, and Chao Yang. ARPACK users’ guide: Solution of large- scale eigenvalue problems with implicitly restarted Arnoldi methods. SIAM, 1998. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018. Tao Li, Lei Tan, Zhehao Huang, Qinghua Tao, Yipeng Liu, and Xiaolin Huang. Low dimensional trajectory hypothesis is true: DNNs can be trained in tiny subspaces. Transactions on Pattern Analysis and Machine Intelligence, 45(3):3411–3420, 2022a. Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, and Xiaolin Huang. Subspace adversarial training. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 13409–13418, 2022b. Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided evolutionary strategies: Augmenting random search with surrogate gradients. In International Conference on Machine Learning, pp. 4264–4273. PMLR, 2019. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier- stra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999. Supratik Paul, Vitaly Kurin, and Shimon Whiteson. Fast efficient hyperparameter tuning for policy gradient methods. Advances in Neural Information Processing Systems, 32, 2019. 11Published as a conference paper at ICLR 2024 Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682–697, 2008. Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow- ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce- ment learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018a. Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In International Conference on Learning Representations, 2018b. Antonin Raffin. RL Baselines3 Zoo. https://github.com/DLR-RM/ rl-baselines3-zoo, 2020. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dor- mann. Stable-Baselines3: Reliable reinforcement learning implementations. The Journal of Ma- chine Learning Research, 22(1):12348–12355, 2021. Thomas R ¨uckstiess, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi Sun, and J ¨urgen Schmidhuber. Exploring parameter space in reinforcement learning. Paladyn, Journal of Behavioral Robotics, 1:14–24, 2010. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Pierre Schumacher, Daniel Haeufle, Dieter B ¨uchler, Syn Schmitt, and Georg Martius. DEP-RL: Embodied exploration for reinforcement learning in overactuated and musculoskeletal systems. In International Conference on Learning Representations, 2022. Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli. Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods. In International Conference on Machine Learn- ing, pp. 604–612. PMLR, 2014. Ryan Sullivan, Jordan K Terry, Benjamin Black, and John P Dickerson. Cliff diving: Exploring reward surfaces in reinforcement learning environments. InInternational Conference on Machine Learning, pp. 20744–20776. PMLR, 2022. Hongyao Tang, Min Zhang, and Jianye Hao. The ladder in chaos: A simple and effective im- provement to general DRL algorithms by policy path trimming and boosting. arXiv preprint arXiv:2303.01391, 2023. Mark Tuddenham, Adam Pr ¨ugel-Bennett, and Jonathan Hare. Quasi-Newton’s method in the class gradient defined high-curvature subspace. arXiv preprint arXiv:2012.01938, 2020. Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private SGD with gradient subspace identification. In International Conference on Learning Representations, 2020. 12Published as a conference paper at ICLR 2024 A L EARNING CURVES 0.0 0.2 0.4 0.6 0.8 1.0 Environment steps ×106 0 250 500 750 1000 Cumulative reward PPO SAC 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 2000 4000 6000 (a) Ant 0.00 0.25 0.50 0.75 1.00 0 500 1000 (b) Ball in cup 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 200 400 (c) BipedalWalker 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.1 0.2 0.3 0.4 0.5 40 20 0 (d) FetchReach 0.00 0.25 0.50 0.75 1.00 0 500 1000 (e) Finger-spin 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 5000 10000 15000 (f) HalfCheetah 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.5 1.0 1.5 0 2000 4000 (g) Hopper 0.00 0.25 0.50 0.75 1.00 250 0 250 (h) LunarLanderContinuous 0.0 0.1 0.2 0.3 1500 1000 500 0 (i) Pendulum 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.1 0.2 0.3 0.4 0.5 60 40 20 0 0.0  0.5  1.0  1.5  2.0  2.5  3.0 Environment steps (millions) 0 1500 3000 4500 6000 (j) Reacher 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 200 400 0.0  0.5  1.0  1.5  2.0  2.5  3.0 Environment steps (millions) 0 1500 3000 4500 6000  (k) Swimmer 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 2000 4000 6000 0.0  0.5  1.0  1.5  2.0  2.5  3.0 Environment steps (millions) 0 1500 3000 4500 6000  (l) Walker2D Figure 4: Learning curves for PPO and SAC on tasks from OpenAI Gym (Brockman et al., 2016), Gym Robotics (Plappert et al., 2018a), and the DeepMind Control Suite (Tunyasuvunakool et al., 2020). We use the algorithm implementations of Stable Baselines3 (Raffin et al., 2021) with tuned hyperparameters from RL Baselines3 Zoo (Raffin, 2020) for the Gym tasks and hyperparam- eters tuned by random search over 50 configurations for the Gym Robotics and DeepMind Control Suite tasks. Results are averaged over ten random seeds; shaded areas represent the standard devia- tion across seeds. 13Published as a conference paper at ICLR 2024 B D ETAILED ANALYSIS RESULTS FOR ALL TASKS 0.0 0.2 0.4 0.6 0.8 1.0 Environment steps ×106 0.0 0.2 0.4 0.6 0.8 1.0 Gradient fraction in subspace Precise gradient; 1 EV Mini-batch gradient; 1 EV Precise gradient; 10 EVs Mini-batch gradient; 10 EVs Precise gradient; 100 EVs Mini-batch gradient; 100 EVs 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (a) Ant, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (b) Ant, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (c) Ball in cup, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (d) Ball in cup, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (e) Bip.Walker, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (f) Bip.Walker, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (g) FetchReach, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (h) FetchReach, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (i) Finger-spin, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (j) Finger-spin, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (k) HalfCheetah, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (l) HalfCheetah, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (m) Hopper, actor 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (n) Hopper, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (o) LunarLander, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (p) LunarLander, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (q) Pendulum, actor 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (r) Pendulum, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (s) Reacher, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (t) Reacher, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0 (u) Swimmer, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (v) Swimmer, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (w) Walker2D, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (x) Walker2D, critic Figure 5: The evolution of the fraction of the gradient that lies within the high-curvature subspace throughout the training for PPO on all tasks. Evaluation of gradient subspaces with different num- bers of eigenvectors. Results for the actor and critic. 14Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 1.0 Environment steps ×106 0.0 0.2 0.4 0.6 0.8 1.0 Gradient fraction in subspace Precise gradient; 1 EV Mini-batch gradient; 1 EV Precise gradient; 10 EVs Mini-batch gradient; 10 EVs Precise gradient; 100 EVs Mini-batch gradient; 100 EVs 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (a) Ant, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (b) Ant, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (c) Ball in cup, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (d) Ball in cup, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (e) Bip.Walker, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (f) Bip.Walker, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (g) FetchReach, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (h) FetchReach, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (i) Finger-spin, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (j) Finger-spin, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (k) HalfCheetah, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (l) HalfCheetah, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (m) Hopper, actor 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (n) Hopper, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (o) LunarLander, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (p) LunarLander, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (q) Pendulum, actor 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (r) Pendulum, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (s) Reacher, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (t) Reacher, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0 (u) Swimmer, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (v) Swimmer, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (w) Walker2D, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (x) Walker2D, critic Figure 6: The evolution of the fraction of the gradient that lies within the high-curvature subspace throughout the training for SAC on all tasks. Evaluation of gradient subspaces with different num- bers of eigenvectors. Results for the actor and critic. 15Published as a conference paper at ICLR 2024 t1 0.0 0.2 0.4 0.6 0.8 1.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap 1 EV 10 EVs 100 EVs t1 t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (a) Ant, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (b) Ant, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (c) Ball in cup, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (d) Ball in cup, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (e) Bip.Walker, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (f) Bip.Walker, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (g) FetchReach, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (h) FetchReach, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (i) Finger-spin, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (j) Finger-spin, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (k) HalfCheetah, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (l) HalfCheetah, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (m) Hopper, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (n) Hopper, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (o) LunarLander, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (p) LunarLander, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (q) Pendulum, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (r) Pendulum, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (s) Reacher, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (t) Reacher, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (u) Swimmer, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (v) Swimmer, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (w) Walker2D, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (x) Walker2D, critic Figure 7: The evolution of the overlap between the high-curvature subspace identified at timestep t1 = 100 ,000 and later timesteps for PPO on all tasks. Evaluation of gradient subspaces with different numbers of eigenvectors. Results for the actor and critic. 16Published as a conference paper at ICLR 2024 t1 0.0 0.2 0.4 0.6 0.8 1.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap 1 EV 10 EVs 100 EVs t1 t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (a) Ant, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (b) Ant, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (c) Ball in cup, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (d) Ball in cup, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (e) Bip.Walker, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (f) Bip.Walker, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (g) FetchReach, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (h) FetchReach, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (i) Finger-spin, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (j) Finger-spin, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (k) HalfCheetah, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (l) HalfCheetah, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (m) Hopper, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (n) Hopper, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (o) LunarLander, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (p) LunarLander, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (q) Pendulum, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (r) Pendulum, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (s) Reacher, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (t) Reacher, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (u) Swimmer, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (v) Swimmer, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (w) Walker2D, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (x) Walker2D, critic Figure 8: The evolution of the overlap between the high-curvature subspace identified at timestep t1 = 100 ,000 and later timesteps for SAC on all tasks. Evaluation of gradient subspaces with different numbers of eigenvectors. Results for the actor and critic. 17Published as a conference paper at ICLR 2024 C I MPACT OF SUBOPTIMAL HYPERPARAMETERS Hyperparameters typically have a significant impact on the learning performance of policy gradient algorithms (Paul et al., 2019). In Section 4.2, we analyzed the gradient subspace for training runs with tuned hyperparameter configurations. However, which hyperparameters work well for a given problem is typically not known a priori. Finding good hyperparameters often involves running nu- merous RL trainings, which might be infeasible when training on real-world tasks. For our insights to be valuable for such real-world settings, it is crucial that suitable gradient subspaces can also be identified for training runs with suboptimal hyperparameters. Therefore, in this section, we investi- gate the robustness of the gradient subspace with respect to suboptimal hyperparameters. To get a set of hyperparameters that are realistic but potentially suboptimal, we sample hyperparameter con- figurations from the ranges that are found frequently in the tuned hyperparameters ofRL Baselines3 Zoo (Raffin, 2020). Particularly, we draw the samples from the sets given in Table 1. Note that these are also the bounds we use when sampling configurations for hyperparameter tuning. Algorithm Hyperparameter Values Logscale PPO learning rate [10−5, 10−1] yes batch size {32, 64, 128, 256} no n steps {256, 512, 1024, 2048, 4096} no n epochs {5, 10, 20} no gamma [0.9, 1] no gae lambda [0.9, 1] no clip range {0.1, 0.2, 0.3} no ent coef [10−8, 10−2] yes net arch {(64, 64), (128, 128), (256, 256)} no SAC learning rate [10−5, 10−1] yes batch size {32, 64, 128, 256} no train freq {1, 4, 16, 32, 64} no gamma [0.9, 1] no tau {0.005, 0.01, 0.02} no learning starts {100, 1000, 10000} no net arch {(64, 64), (128, 128), (256, 256)} no Table 1: Sets from which we draw random hyperparameter configurations for PPO and SAC. We refer to each hyperparameter by its name in Stable Baselines3 (Raffin et al., 2021). Logscale means that we first transform the interval into logspace and draw the exponent uniformly from the trans- formed range. All other hyperparameters are drawn uniformly. These sets reflect common hyperpa- rameter choices from RL Baselines3 Zoo (Raffin, 2020). We sampled a total of 100 hyperparameter configurations per algorithm and task. Figure 9 dis- plays the distribution over the maximum cumulative reward that these hyperparameter configurations achieve. As expected, the variance in the performance is large across the hyperparameter configura- tions. While some configurations reach performance levels comparable to the tuned configuration, most sampled configurations converge to suboptimal behaviors. To display the values of the gradient subspace fraction and the subspace overlap compactly for all configurations, we compress the results for each training run to a single scalar. For the gradient subspace fraction, we compute the mean of the criterion over the timesteps. Regarding the subspace overlap, we choose the early timestept1 = 100,000 in accordance to the experiments in Section 4.3. However, taking the mean over the remaining timesteps would not be faithful to how the subspace would typically be utilized in downstream applications. Such methods would likely update the gradient subspace after a given number of gradient steps instead of identifying it once and using it for the rest of the training. The rate of this update would likely depend on the exact way that the 18Published as a conference paper at ICLR 2024 Finger-spin (PPO) Finger-spin (SAC) 0 200 400 600 800 1000Maximum reward T uned configurations (mean) Walker2D (PPO) Walker2D (SAC) 0 2000 4000 6000Maximum reward T uned configurations (mean) Figure 9: Violin plot of the maximum cumulative rewards achieved by 100 agents trained with random hyperparameters on Finger-spin (left) and Walker2D (right). The black dots mark the per- formance of the individual random configurations, and the red cross signifies the performance of the tuned hyperparameters averaged over 10 random seeds. The random hyperparameters result in agents of vastly different maximum performance. While some of the random hyperparameters achieve performance comparable to the tuned agents, the bulk of random hyperparameters stagnate at suboptimal performance levels. Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Gradient subspace fraction T uned configurations (mean) (a) Actor Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Gradient subspace fraction T uned configurations (mean) (b) Critic Figure 10: Violin plot of the mean gradient subspace fraction throughout the training of 100 agents with random hyperparameters. The red cross marks the same quantity for the high-performing hyperparameters used throughout the paper (averaged over 10 random seeds). For PPO’s actor, the variance in the gradient subspace fraction is significantly higher than for SAC’s. The critic’s gradient subspace fraction is very high across all hyperparameter configurations. 19Published as a conference paper at ICLR 2024 Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap (t2 t1 = 50,000) T uned configurations (mean) (a) Actor Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap (t2 t1 = 50,000) T uned configurations (mean) (b) Critic Figure 11: Violin plot of the mean subspace overlap fort1 = 100,000 and t2 = 150,000 of 50 agents with random hyperparameters. The red cross marks the same quantity for the high-performing hyperparameters used throughout the paper (averaged over 10 random seeds). For both tasks and networks, the variance of the subspace overlap is large across the hyperparameter configurations. subspace is utilized. For this analysis, we choose t2 = 150,000, so that the timestep difference of t2 −t1 = 50,000 steps is significantly shorter than the total length of the training, but the number of gradient steps is large enough that the subspace could change. The results for the gradient subspace fraction and the subspace overlap for random hyperparam- eters are visualized in Figure 10 and Figure 11, respectively. Figure 10a shows for SAC’s actor that the gradient is well contained in the subspace for most random hyperparameter configurations. For PPO’s actor, the spread in the gradient subspace fraction is significantly higher. As shown in Figure 10b, the gradient subspace fraction for the critic is very high for all hyperparameter config- urations. These results suggest that gradient subspaces can be utilized in SAC independently of the hyperparameter configuration, while PPO’s actor might require more considerations. Figure 11 shows that there is a significant spread in the subspace overlap for the random hyperpa- rameter configurations, which indicates that potential downstream applications might need to update the gradient subspace more frequently, depending on the hyperparameters. 20Published as a conference paper at ICLR 2024 D D ERIVATION OF THE GRADIENT SUBSPACE FRACTION OBJECTIVE In this section, we derive Equation (6). Recall that Pk = (v1, . . . , vk)T ∈ Rk×n is the matrix of the k largest Hessian eigenvectors, which is semi-orthogonal. We use ˜g = P+ k Pk g ∈ Rn to denote the projection of the gradient g into the subspace and back to its original dimensionality. In the following derivation, we drop the subscript k of matrix Pk for ease of notation. Sfrac(P, g) = 1 − ||˜g − g||2 ||g||2 (9) = 1 − ||P+Pg − g||2 ||g||2 (10) = 1 − ||PT Pg − g||2 ||g||2 (11) = 1 − (PT Pg − g)T (PT Pg − g) gT g (12) = 1 − (gT PT P − gT )(PT Pg − g) gT g (13) = 1 − gT PT PP T Pg − 2gT PT Pg + gT g gT g (14) = 1 − gT PT Pg − 2gT PT Pg + gT g gT g (15) = 1 − gT g − gT PT Pg gT g (16) = gT g − gT g + gT PT Pg gT g (17) = gT PT Pg gT g (18) = (Pg)T Pg gT g (19) = ||Pg||2 ||g||2 (20) Note that we used the fact that the pseudo-inverse of a semi-orthogonal matrix P is equal to its transpose P+ = PT in step (11). Furthermore, we used the property PP T = I of semi-orthogonal matrices in step (15). E S UBSPACE OVERLAP FOR THE ENTIRE TRAINING In Section 4.3, we showed the subspace overlap for a range of 400,000 timesteps. Figure 12 visual- izes the subspace overlap criterion with t1 = 100,000 for all future timesteps on tasks that require training for 3 million steps. While in practical applications of gradient subspaces, the subspace would likely be updated multiple times during training, this visualization highlights the influence of the data distribution on the subspace. The plots show a small drop in the subspace overlap for SAC at 1.1 million steps. Since the replay buffer has a size of 1 million samples, this marks the point at which the original data from timestep t1 is completely replaced by new data collected by updated policies. Since the networks’ training data is sampled from the replay buffer, this drop indicates that this change in the data distribution has a negative effect on the subspace overlap. The effect is gen- erally more pronounced for the critic than the actor because the actor’s subspace overlap degrades faster and is already at a relatively low level at the mark of 1.1 million timesteps. For PPO, there is no such drop in the subspace overlap since the algorithm does not use experience replay and instead collects new data for every update. 21Published as a conference paper at ICLR 2024 t1 0.0 0.2 0.4 0.6 0.8 1.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap PPO, actor PPO, critic SAC, actor SAC, critic t1 t1 0.0  0.5  1.0  1.5  2.0  2.5  3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 0.0 0.5 1.0 1.5 2.0 2.5 3.0 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (a) Ant 0.0 0.5 1.0 1.5 2.0 2.5 3.0 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (b) HalfCheetah t1 0.0  0.5  1.0  1.5  2.0  2.5  3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 0.0 0.5 1.0 1.5 2.0 2.5 3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (c) Swimmer 0.0 0.5 1.0 1.5 2.0 2.5 3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (d) Walker2D Figure 12: Evolution of the subspace overlap between the early timestep t1 = 100,000 (marked by the dashed gray line) and all future timesteps of the training. Results for the actor and critic of PPO and SAC. For SAC, a small drop in the subspace overlap is visible in all plots at around 1.1 million timesteps. This marks the timestep at which the data in the replay buffer is replaced completely by new data, indicating that the data distribution affects the subspace overlap. 22
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper addresses the inefficiency of deep reinforcement learning (RL) due to persistently changing data distributions and a large number of parameters. Drawing inspiration from supervised learning (SL) where gradients reside in low-dimensional, slowly-changing subspaces, this work empirically evaluates if similar gradient subspaces exist in deep policy gradient (PG) methods. The main contributions are demonstrating that: (i) policy gradients for actor and critic networks in PPO and SAC indeed live in low-dimensional, high-curvature subspaces; (ii) these subspaces remain relatively stable throughout RL training despite continuous data distribution shifts; and (iii) the critic's subspace generally exhibits less variability and retains a larger fraction of its gradient. The findings highlight the potential for enhanced RL performance through improved parameter-space exploration or second-order optimization.",
    "methodology": "The study conducts a comprehensive empirical evaluation of gradient subspaces using two popular PG algorithms: Proximal Policy Optimization (PPO) (on-policy) and Soft Actor-Critic (SAC) (off-policy). It assesses three conditions: (i) the existence of high-curvature directions by analyzing the spectrum of Hessian eigenvalues (estimated via the Lanczos method); (ii) how well the gradients lie in the high-curvature subspace using the gradient subspace fraction Sfrac(Pk, g) = ||Pk g||^2 / ||g||^2; and (iii) the stability of these subspaces over time using the subspace overlap Soverlap(P(t1)k, P(t2)k). Hessian and gradient estimates were computed using 10^6 on-policy samples for PPO and the full replay buffer for SAC, with further robustness tests employing mini-batch estimates (2048 samples).",
    "experimental_setup": "Experiments were performed using PPO and SAC on twelve benchmark tasks from OpenAI Gym, Gym Robotics, and the DeepMind Control Suite. The algorithm implementations from Stable Baselines3 were utilized, with hyperparameters sourced from RL Baselines3 Zoo for Gym tasks and tuned via random search for others. Each experiment was run for 10 random seeds. Analysis of gradient subspace fraction and subspace overlap was conducted at checkpoints every 50,000 steps. Training progress was divided into initial, training, and convergence phases based on performance improvement (10% and 90% of total improvement). A subspace dimensionality of k=100 was chosen. Additionally, robustness to suboptimal hyperparameters was assessed by evaluating 100 randomly sampled configurations.",
    "limitations": "While the paper demonstrates the existence and stability of gradient subspaces, a direct limitation for immediate practical application is the computational expense of calculating Hessian eigenvectors at every training step, even with efficient methods like Lanczos. The study notes that low-sample mini-batch gradient approximations slightly perturb the gradient out of the precise subspace. Furthermore, the effectiveness and stability of gradient subspaces, as measured by gradient subspace fraction and subspace overlap, can vary significantly with suboptimal hyperparameter configurations, particularly for PPO's actor, suggesting that subspace updates might need to be more frequent depending on hyperparameters.",
    "future_research_directions": "The findings suggest two main avenues for future research: (1) Optimization in the subspace: Develop new RL optimization methods that operate within the identified low-dimensional, high-curvature subspace. This could enable efficient computation and inversion of the Hessian, making second-order optimization feasible for deep RL and addressing ill-conditioned problems. (2) Guiding parameter-space exploration: Utilize the knowledge of high-curvature subspaces to guide parameter-space exploration by sampling exploration noise predominantly in these relevant directions, potentially leading to more efficient and directed learning in RL. Additionally, further analysis could explore the implications of the critic's greater subspace stability for specialized optimization strategies."
}
