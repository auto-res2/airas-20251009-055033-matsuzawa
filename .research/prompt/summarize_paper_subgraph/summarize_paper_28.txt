
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Directional Smoothness and Gradient Methods: Convergence and Adaptivity Aaron Mishkin∗ Stanford University amishkin@cs.stanford.edu Ahmed Khaled∗ Princeton University ahmed.khaled@princeton.edu Yuanhao Wang Princeton University yuanhaoa@princeton.edu Aaron Defazio FAIR, Meta AI adefazio@meta.com Robert M. Gower CCM, Flatiron Institute gowerrobert@gmail.com Abstract We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness. 1 Introduction Gradient methods for differentiable functions are typically analyzed under the assumption that f is L-smooth, meaning ∇f is L-Lipschitz continuous. This condition implies f is upper-bounded by a quadratic and guarantees that gradient descent (GD) with step-size η <2/L decreases the optimality gap at each iteration (Bertsekas, 1997). However, experience shows that GD can still decrease the objective when f is not L-smooth, particularly for deep neural networks (Bengio, 2012; Z. Li et al., 2020; J. Cohen et al., 2021). Even for functions verifying smoothness, convergence rates are often pessimistic and fail to predict optimization speed in practice (Paquette et al., 2023). One alternative to global smoothness is local Lipschitz continuity of the gradient (“local smoothness”). Local smoothness assumes different Lipschitz constants hold for different neighbourhoods, which avoids global assumptions and improves rates. However, such analyses typically rely on boundedness of the iterates and then use local smoothness to obtain L-smoothness over a compact set (Malitsky and Mishchenko, 2020). Boundedness is guaranteed in several ways: Junyu Zhang and Hong (2020) break optimization into stages, Patel and Berahas (2022) use stopping-times, and Lu and S. Mei (2023) employ a line-search. Unfortunately, these approaches modify the underlying optimization algorithm, require local smoothness oracles (Park et al., 2021), or rely on highly complex arguments. In contrast, we prove simple rates for GD without global smoothness by deriving bounds of the form, f(xk+1) ≤ f(xk) + ⟨∇f(xk), xk+1 − xk⟩ + M(xk+1, xk) 2 ∥xk+1 − xk∥2 2, (1) ∗Equal contribution. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2403.04081v2  [cs.LG]  13 Jan 202550 100 150 200 Iterations 10−2 100 102 Optimality Gap ionosphere 10 20 30 40 Iterations 10−10 10−6 10−2 mammographic 1/M(xk+1, xk) Polyak Bound (L-Smooth) Bound (1/M(xk+1, xk)) Bound (Polyak) Figure 1: Comparison of actual (solid lines) and theoretical (dashed lines) convergence rates for GD with (i) step-sizes strongly adapted to the directional smoothness (ηk = 1/M(xk+1, xk)) and (ii) the Polyak step-size. Both problems are logistic regressions on UCI repository datasets (Asuncion and Newman, 2007). Our bounds using directional smoothness are tighter than those based on global L- smoothness of f and adapt to the optimization path. For example, on mammographic our theoretical rate for the Polyak step-size concentrates rapidly exactly when the optimizer shows fast convergence. where the directional smoothness function M(xk+1, xk) depends only on properties of f along the chord between xk and xk+1. Our sub-optimality bounds provide a path-dependent perspective on GD and are tighter than conventional analyses when the step-size sequence is adapted to the directional smoothness, meaning ηk < 2/M(xk+1, xk). See Figure 1 for two real-data examples highlighting our improvement over classical rates. We summarize all our contributions as follows. Directional Smoothness. We introduce three constructive directional smoothness functions M(x, y). The first, point-wise smoothness, depends only on the end-points x, yand is easily computed, while the second, path-wise smoothness, yields a tighter bound, but depends on the chord C = {αx + (1 − α)y : α ∈ [0, 1]}. The last function, which we call the optimal point-wise smoothness, is both easy-to-evaluate and provides the tightest possible quadratic upper bound. Sub-optimality bounds. We leverage directional smoothness functions to prove new sub-optimality bounds for GD on convex functions. Our bounds are localized to the GD trajectory, hold for any step-size sequence, and are tighter than the classic analysis using L-smoothness. They are also more general since we do not need to assume that f is globally L-smooth to show progress; all we require is a sequence of step-sizes adapted to the directional smoothness function. Furthermore, our approach extends naturally to acceleration, allowing us to prove optimal rates for (strongly)-convex functions. Adaptive Step-Sizes in the Quadratic Case. In the general setting, computing step-sizes which are adapted to the directional smoothness requires solving a challenging non-linear root-finding problem. For quadratic problems, we show that the ideal step-size that satisfies ηk = 1/M(xk+1, xk) is the Rayleigh quotient and is connected to the hedging algorithm (Altschuler and Parrilo, 2023). Exponential Search. Moving beyond quadratics, we prove that the equation ηk = 1/M(xk+1, xk) admits a solution under mild conditions, meaning ideal step-sizes can be computed using Newton’s method. Since computing these step-sizes is typically impractical, we adapt exponential search (Carmon and Hinder, 2022) to obtain similar path-dependent complexities up to a log-log penalty. Polyak and Normalized GD.More importantly, we show that the Polyak step-size (Polyak, 1987) and normalized GD achieve fast, path-dependent rates without knowledge of the directional smoothness. Our analysis reveals that the Polyak step-size adapts to any directional smoothness to obtain the tightest possible convergence rate. This property is not shared by constant step-size GD and may explain the superiority of the Polyak step-size in many practical settings. 1.1 Additional Related Work Directional smoothness is a relaxation of non-uniform smoothness (J. Mei et al., 2021), which restricts the smoothness function M to depend only on x, the origin point. J. Mei et al. (2021) leverage non-uniform smoothness and a non-uniform Łojasiewicz inequality to break lower-bounds for first-order optimization. Similarly, Berahas et al. (2023) show that a weak local smoothness oracle can break lower bounds for gradient methods. A major advantage of our work over such oracle-based approaches is that we construct explicit directional smoothness functions that are easy to evaluate. 2Similar to non-uniform smoothness, Grimmer (2019) and Orabona (2023) consider Hölder-type growth conditions with constants that depend on a neighbourhood of x. Since directional smoothness is stronger than and implies these Hölder error bounds, our M functions can be leveraged to make their results fully explicit (the Hölder bounds are non-constructive). Finally, while they also analyze normalized GD, our rates are anytime and do not use online-to-batch reductions like Orabona (2023). Directional smoothness is also related to(L0, L1)-smoothness (Jingzhao Zhang et al., 2020; B. Zhang et al., 2020), which can be interpreted as a directional smoothness function with exponential depen- dence on the distance between x and y. The extension of (L0, L1)-smoothness to (r, l)-smoothness by H. Li et al. (2023) shows how to bound sequences of such directional smoothness functions, even for accelerated methods. These approaches are complementary to ours and showcase a setting where directional smoothness leads to concrete convergence rates. Our work is most closely connected to that by Malitsky and Mishchenko (2020), who use a smoothed version of M(x, y) to set the step-size. Vladarean et al. (2021) apply a similar smoothed step-size scheme to primal-dual hybrid gradient methods, while Zhao and Huang (2024) relate directional smoothness to Barzilai-Borwein updates (Barzilai and Borwein, 1988) and Vainsencher et al. (2015) use local smoothness over neighbourhoods of the global minimizer to set the step-size for SVRG. Finally, we note that adaptivity to directional smoothness is different from adaptivity to the sequence of observed gradients obtained by methods such as Adagrad (Duchi et al., 2010; Streeter and McMahan, 2010). Adagrad and its variants are most useful when the gradients are bounded, such as in Lipschitz optimization, although they can also be used to obtain rates for smooth functions (Levy, 2017). We do not address adaptivity to gradients in this work. 2 Directional Smoothness We say that a convex functionf is L-smooth if for all x, y∈ Rd, f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + L 2 ∥y − x∥2 2. (2) Minimizing this quadratic upper bound in y gives the classical GD update with step-size ηk = 1/L. However, this viewpoint leads to rates which depend on the global, worst-case growth off. This is both counter-intuitive and undesirable because the iterates of GD, xk+1 = xk − ηk∇f(xk), depend only on local properties of f. Ideally, the analysis should also depend only on the local conditioning along the path {x1, x2, . . .}. Towards this end, we generalize the smoothness upper-bound as follows. Definition 2.1. We call M : Rd,d → R+ a directional smoothness function for f if for all x, y∈ Rd, f(y) ≤ f(x)+⟨∇f(x), y−x⟩ + M(x, y) 2 ∥y−x∥2. (3) If a function is L-smooth, then M(x, y) = L is a trivial choice of directional smoothness function. In the rest of this section, we construct different M functions that provide tighter bounds on f while still being possible to evaluate. The first is the point-wise directional smoothness, D(x, y) := 2∥∇f(y) − ∇f(x)∥2 ∥y − x∥2 . (4) Point-wise smoothness is a directional estimate of L and satisfies D(x, y) ≤ 2L. Indeed, L can be equivalently defined as the supremum ofD(x, y)/2 over the domain of f (Beck, 2017). If f is convex and differentiable, then D(x, y) is a directional smoothness function according to Definition 2.1. Lemma 2.2. If f is convex and differentiable, then the point-wise directional smoothness satisfies, f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + D(x, y) 2 ∥y − x∥2 2. (5) See Appendix A (we defer all proofs to the relevant appendices). In the worst-case, the point-wise directional smoothness D is weaker than the standard upper-bound M(x, y) = L by a factor of two. This is not an artifact of the analysis and is generally unavoidable, as the next proposition shows. Proposition 2.3. There exists a convex, differentiablef and x, y∈ Rd such that if t <2, then f(y) > f(x) + ⟨∇f(x), y− x⟩ + t∥∇f(x) − ∇f(y)∥ 2∥y − x∥2 ∥y − x∥2 2. (6) 3xk xk+1 x∗ f(x) Actual Progress L-Smooth Mk-Smooth Figure 2: Illustration of GD with ηk = 1 /L. Even though this step-size exactly minimizes the upper-bound from L-smoothness, Mk directional smoothness better predicts the progress of the gradient step because Mk ≪ L. Our rates improve on L-smoothness because of this tighter bound. While the point-wise smoothness is easy to compute, this additional factor of two can make Equa- tion (5) looser than L-smoothness — on isotropic quadratics, for example. As an alternative, we define the path-wise directional smoothness, A(x, y) := sup t∈[0,1] ⟨∇f(x+t(y−x))−∇f(x), y−x⟩ t∥y−x∥2 , (7) and show it verifies the quadratic upper-bound and satisfies Definition 2.1 even without convexity. Lemma 2.4. For any differentiable functionf, the path-wise smoothness (7) satisfies f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + A(x, y) 2 ∥y − x∥2 2. (8) Path smoothness is tighter than point-wise smoothness since A(x, y) ≤ D(x, y), but hard to compute because it depends on the chord between x and y. That is, it depends on the properties off on the line {tx + (1 − t)y : t ∈ [0, 1]} rather than solely on the points x and y like the point-wise smoothness. Point-wise and path-wise smoothness are constructive, but they may not yield the tightest bounds in all situations. The tightest directional smoothness function, which we call the optimal point-wise smoothness, is the smallest number for which the quadratic upper bound holds, H(x, y) = |f(y) − f(x) − ⟨∇f(x), y− x⟩| 1 2 ∥y − x∥2 (9) By definition, H is the tightest possible directional smoothness function; it lower bounds any constant C that satisfies the quadratic bound (2). Thus, H(x, y) ≤ M(x, y) for any smoothness function M. The directional smoothness functions introduced in this section represent different trade-offs between computability and tightness. The optimal point-wise smoothness H(x, y) requires access to both the function and gradient values, whereas the point-wise directional-smoothness D(x, y) requires only access to the gradients and convexity. In contrast, the path-wise direction smoothnessA(x, y) satisfies Lemma 2.4 with or without convexity, but may be hard to evaluate. 3 Path-Dependent Sub-Optimality Bounds Using directional smoothness, we obtain a descent lemma which depends only on local geometry, f(xk+1) ≤ f(xk) −  ηk − η2 kM(xk, xk+1) 2  ∥∇f(xk)∥2 2. (10) See Lemma A.1. If ηk < 2/M(xk, xk+1), then GD is guaranteed to decrease the function value and we call ηk adapted to M(xk, xk+1). However, computing adapted step-sizes is not always straight- forward. For instance, finding ηk = 1/M(xk, xk+1(ηk)) requires solving a non-linear equation. The rest of this section leverages directional smoothness to derive new guarantees for GD with arbi- trary step-sizes. We emphasize that these results are sub-optimality bounds, rather than convergence rates; a sequence of adapted step-sizes is required to convert our propositions into a convergence theory. As a trade-off, our bounds reflect the locality of GD, rather than treating it as a global method. 4We start with the case when f has lower curvature. Instead of using strong convexity or the PL- condition (Karimi et al., 2016), we propose the directional strong convexity constant, µ(x, y)= inf t∈[0,1] ⟨∇f(x+t(y−x))−∇f(x), y−x⟩ t∥y − x∥2 2 . (11) If f is convex, then µ(x, y) ≥ 0 and it verifies the standard lower-bound from strong convexity, f(y) ≥ f(x) + ⟨∇f(x), y− x⟩ + µ(x, y) 2 ∥y − x∥2 2. (12) Moreover, we have µ(x, y) ≥ µ when f is µ–strongly convex. We prove two bounds for convex functions using directional strong convexity. For brevity, we denote Mi := M(xi, xi+1), µi := µi(xi, x∗), δi = f(xi) − f(x∗), and ∆i = ∥xi − x∗∥2 2, where x∗ is a minimizer of f. Proposition 3.1. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, δk ≤ "Y i∈G (1 + ηiλiµi) # δ0 + X i∈B   Y j>i,j∈G (1 + ηjλjµj)   ηiλi 2 ∥∇f(xi)∥2 2, (13) where λi =ηiMi−2, G = {i : ηi <2/Mi}, and B = [k]\G. The analysis splits iterations into good steps G, where ηk is adapted to the directional smoothness, and bad steps B, where the step-size is too large and GD may increase the optimality gap. When f is L-smooth and µ-strongly convex, using the step-size sequence ηk = 1/L gives f(xk+1) − f(x∗) ≤ " kY i=0  1 − µi (2 − Mi/L) L # (f(x0) − f(x∗)) (14) where µi (2 − Mi/L) ≥ µ. Thus, Equation (13) gives at least as tight a rate as standard assumptions by localizing to the convergence path using any directional smoothness M. When Mi < L, the gap in constants yields a strictly improved rate (see Figure 2). We also prove a more elegant bound. Proposition 3.2. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, ∆k ≤ " kY i=0 |1 − µiηi| 1 + µi+1ηi # ∆0 + kX i=0  Y j>i |1 − µjηj| 1 + µj+1ηj     Miη3 i − η2 i  1 + µi+1ηi ∥∇f(xi)∥2 2. (15) Unlike Proposition 3.1, this analysis shows linear progress at each iteration and does not divide k into good steps and bad steps. In exchange, the second term in Equation (15) reflects how much convergence is degraded when ηk is not adapted to the directional smoothness function M. We conclude this section with a bound for when there is no lower curvature, meaning µi = 0. Proposition 3.3. Let xk =Pk i=0 ηixi+1/Pk i=0 ηi. If f is convex and differentiable, then GD satisfies, f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk i=0 ηi + Pk i=0 η2 i (ηiMi − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . (16) Eq. (16) is faster than standard analyses whenever Mi < L; it will be a key tool in the next sections. 3.1 Path-Dependent Acceleration Now we show that directional smoothness can also be used to derive path-dependent sub-optimality bounds for accelerated algorithms — that is, methods obtaining optimal rates for smooth, convex optimization. In particular, we study Nesterov’s accelerated gradient descent (AGD) (Nesterov, 1983) and prove that directional smoothness leads to tighter rates given adapted step-sizes. Throughout this section we assume that f is µ-strongly convex with µ = 0 when f is merely convex. Although our analysis uses estimating sequences (Nesterov et al., 2018), we state AGD in the following “momentum” formulation, where yk is the momentum and αk the momentum parameter, xk+1 = yk − ηk∇f(yk) α2 k+1 = (1 − αk+1)α2 k ηk+1 ηk + ηk+1αk+1µ yk+1 = xk+1 + αk(1 − αk) α2 k + αk+1 (xk+1 − xk) . (17) 5If ηk ≤ 1/M(xk, xk+1), then Equation (10) combined with 1 − ηkM(xk, xk+1)/2 ≥ 1/2 implies, f(xk+1) ≤ f(yk) − ηk 2 ∥∇f(yk)∥2 2. (18) Our analysis leverages the fact that this descent condition for xk+1 is the only connection between the smoothness of f and the convergence rate of AGD. Since Equation (18) depends only on the step-size ηk, we can replace L within the analysis of AGD with a sequence of adapted step-sizes. The following theorem controls the effect of these step-sizes to obtain path-dependent bounds. Theorem 3.4. Suppose f is differentiable, µ–strongly convex and AGD is run with adapted step-sizes ηk ≤ 1/Mk. If µ >0 and α0 = √η0µ, then AGD obtains the following accelerated rate: f(xk+1) − f(x∗) ≤ kY i=0 (1 − √µηi) h f(x0) − f(x∗) + µ 2 ∥x0 − x∗∥2 2 i . (19) Let ηmin = mini∈[k] ηi. If µ ≥ 0 and α0 ∈ (√µη0, c), where c is the maximum value of α0 for which γ0 = α2 0−η0α0µ η0(1−α0) satisfies γ0 < 3/ηmin + µ, then AGD obtains the following rate: f(xk+1) − f(x∗) ≤ 4 ηmin(γ0 − µ)(k + 1)2 h f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 i . (20) If ηk = 1/Mk > 1/L, then these rates are strictly faster than those obtained under L-smoothness and Theorem 3.4 shows that AGD provably benefits from taking the largest possible steps given the local geometry of f. However, obtaining accelerated rates when µ = 0 requires prior knowledge of the minimum step-size; while this is straightforward for L-smooth functions, it is not clear how to extend such result to non-strongly convex acceleration with locally Lipschitz gradients. For example, while H. Li et al. (2023) show that the (r, l)-smoothness (a valid directional smoothness function) is bounded over the iterate trajectory, their rate does not adapt to the optimization path. 4 Adaptive Learning Rates Converting our sub-optimality bounds into convergence rates requires adapted step-sizes satisfying ηk < 2/M(xk, xk+1). Given an adapted step-size, the directional descent lemma (Equation (10)) implies GD decreases f and we can obtain fast rates if the step-sizes are bounded below. However, xk+1 is itself a function of ηk, meaning adapted step-sizes are not straightforward to compute. For L-smooth f, the different directional smoothness functions M introduced in Section 2 satisfy M(xk, xk+1) ≤ 2L. This implies ηk < 1 L is trivially adapted. As such step-sizes don’t capture local properties of f, we introduce the notion of strongly adapted step-sizes, which satisfy ηk = 1/M(xk+1(ηk), xk). (21) Equation (10) implies GD with a strongly adapted step-size makes guaranteed progress as, f(xk+1) ≤ f(xk) − [2M(xk+1, xk)]−1 ∥∇f(xk)∥2 2. (22) This progress is greater than that guaranteed by L-smoothness when M(xk, xk+1) < Land holds even when f is not L-smooth. However, it is not clear a priori if (i) strongly adapted step-sizes exist or if (ii) any iterative method achieves the progress in Eq.(21). Surprisingly, we provide a positive answer to both questions. Strongly adapted ηk are computable and we also prove GD with the Polyak step-size adapts to any choice of directional smoothness, including the optimal point-wise smoothness. Before presenting this strong result, we consider the illustrative case of quadratic minimization. 4.1 Adaptivity in Quadratics Now we show that step-sizes adapted to both the point-wise smoothness M and the path-wise smoothness A exist when f is quadratic. Let f(x) = x⊤Bx/2 − c⊤x, where B is positive semi- definite. Assuming {ηk} is strongly adapted to the directional smoothness, Equation (16) implies f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk i=0 ηi = ∥x0 − x∗∥2 2 2 Pk i=0 1 M(xi,xi+1) ≤ ∥x0 − x∗∥2 2 2(k + 1) Pk i=0 M(xi, xi+1) k + 1 , (23) 6101 103 Iteration 104 105 106 Optimality Gap 101 103 Iteration 102 107 Point-Wise Smoothness 101 103 Iteration 10 3 2 × 10 3 3 × 10 3 4 × 10 3 Adapted Step-Sizes 1/L 1/Dk 1/Ak 2/L Figure 3: Performance of GD with different step-size rules for a synthetic quadratic problem. We run GD for 20,000 steps on 20 random quadratic problems with L = 1000 and Hessian skew. Left- to-right, the first plot shows the optimality gap f(xk) − f(x∗), the second shows the point-wise directional smoothness D(xk, xk+1), and the third shows step-sizes used by the different methods. where we used ηiMi = 1 as well as Jensen’s inequality. This guarantee depends solely on the average directional smoothness along the optimization trajectory {x0, x1, . . .}. When f is quadratic, we can exactly compute these smoothness constants. In particular, the point-wise directional smoothness is, D(xi, xi+1) = 2∥B∇f(xi)∥2/∥∇f(xi)∥2. Notably, D(xi, xi+1) has no dependence on xi+1 and the corresponding strongly adapted step-size is given by ηi = ∥∇f(xi)∥2/(2∥B∇f(xi)∥2) — see Lemma C.1. Remarkably, this expression recovers the step-size proposed by Dai and Yang (2006), who show it approximates the Cauchy step-size and converges to the “edge-of-stability” (J. Cohen et al., 2021) at 2/L as k → ∞. Combining this simple expression with Equation (23) gives a fast, non-asymptotic convergence rate for GD and new theoretical justification for their work. We can also compute the path-wise directional smoothness in closed form. As Lemma C.2 shows, A(xi, xi+1) = ∇f(xi)⊤B∇f(xi)/∇f(x)⊤∇f(x), and ηi = ∇f(xi)⊤∇f(xi)/[∇f(xi)⊤B∇f(xi)] is the well-known Cauchy step-size. Path-wise directional smoothness thus provides another interpretation (and convergence guarantee) for the Cauchy step-size, which is traditionally derived by minimizing f(x − η∇f(x)) in η. 4.2 Adaptivity for Convex Functions In the last subsection, we proved that strongly adapted step-sizes for the point-wise and path-wise directional smoothness functions have closed-form expressions when f is quadratic. Moreover, these step-sizes recover two classical schemes from the optimization literature, giving them new justification and fast convergence rates. Now we consider the existence of strongly adapted step-sizes for general convex functions. Our first result gives simple conditions for Equation (21) to have at least one solution when M is the point-wise directional smoothness. Proposition 4.1. If f is convex and continuously differentiable, then either (i) f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/D(x, x− η∇f(x)). The next proposition uses a similar argument with slightly stronger conditions to show existence of strongly adapted step-sizes for the path-wise smoothness. Proposition 4.2. If f is convex and twice continuously differentiable, then either (i)f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/A(x, x− η∇f(x)). Propositions 4.1 and 4.2 do not assume the global smoothness; although neither proof is constructive, it is possible to compute strongly adapted step-sizes for the point-wise directional smoothness using root-finding methods. We show in Section 5 that iff is twice differentiable, then strongly adapted step-sizes can be found via Newton’s method using only Hessian-vector products, ∇2f(x)∇f(x). 4.2.1 Exponential Search Now we show that the exponential search algorithm developed by Carmon and Hinder (2022) can be used to find step-sizes that adapt on average to the directional smoothness. Consider a fixed 7optimization horizon k and denote by xi(η) the sequence of iterates obtained by running GD from x0 using a fixed step-size η. Define the criterion function, ψ(η) = Pk i=0 ∥∇f(xi(η))∥2 2 Pk i=0 M(xi(η), xi+1(η))∥∇f(xi(η))∥2 2 , (24) and suppose that we have a step-size η that satisfies ψ(η)/2 ≤ η ≤ ψ(η). Using these bounds in Proposition 3.3 yields the following convergence rate, f(xk) − f∗ ≤ h k Pk i=0 M(xi,xi+1)∥∇f(xi)∥2 2Pk i=0 ∥∇f(xi)∥2 2 i ∥x0 − x∗∥2 2. (25) While η does not adapt to each directional smoothness M(xi, xi+1) along the path, it adapts to a weighted average of the directional smoothness constants, where the weights are the observed squared gradient norms. This is always smaller than the maximum directional smoothness along the trajectory and can be much smaller than the global smoothness. Furthermore, we have reduced our problem to finding η ∈ [ψ(η)/2, ψ(η)], which is similar to the problem Carmon and Hinder (2022) solve with exponential search. We adopt their approach as Algorithm 1 and give a convergence guarantee. Theorem 4.3. Assume f is convex and L-smooth. Then Algorithm 1 with η0 > 0 requires at most 2K(log log(2η0/L) ∨ 1) iterations of GD and in the last run it outputs a step-size η and point xK = 1 K PK−1 i=0 xi(η) such that exactly one of the following holds: Case 1: η = η0 and f(xK) − f(x∗) ≤ ∥x0 − x∗∥2 2 2Kη0 Case 2: η ̸= η0 and f(xK) − f∗ ≤ ∥x0 − x∗∥2 2 2K "Pk i=0 Mi∥∇f(x′ i)∥2 2 Pk i=0 ∥∇f(x′ i)∥2 2 # , where Mi def = M(x′ i, x′ i+1) and x′ i are the iterates generated by GD with step-size η′ ∈ [η, 2η]. Theorem 4.3 requires f to be L-smooth, but has only a log log dependence on the global smoothness constant. Moreover, the rate scales with the weighted average of smoothness constants along a very close trajectory {x′ 1, x′ 2, . . .}. In the next section, we give convergence bounds that depend on the unweighted average of the directional smoothness constants along the actual optimization trajectory. 4.2.2 Polyak’s Step-Size Rule Our theory so-far suggests using strongly adapted step-sizes, but neither root-finding nor exponential search are practical methods for large-scale optimization. Thus, we now consider other step-size selection rules which may leverage directional smoothness. In particular, the Polyak step-size sets, ηk = γ (f(xk) − f(x∗)) /∥∇f(xk)∥2 2, (26) for some γ >0, which is optimal for smooth and non-smooth optimization (Hazan and Kakade, 2019) given knowledge of f(w∗). Surprisingly, we show that GD with the Polyak step-size also achieves the same guarantee as strongly adapted step-sizes without knowledge of the directional smoothness. Theorem 4.4. Suppose that f is convex and differentiable and let M be any directional smoothness function for f. Let ∆0 := ∥x0 − x∗∥2 2. Then GD with the Polyak step-size and γ ∈ (1, 2) satisfies f(xk) − f(x∗) ≤ c(γ)∆0 2 Pk−1 i=0 M(xi, xi+1)−1 , (27) where c(γ) = γ/(2 − γ)(γ − 1) and xk = Pk−1 i=0  M(xi, xi+1)−1xi  / Pk−1 i=0 M(xi, xi+1)−1  . Theorem 4.4 measures sub-optimality at an average iterate obtained using the directional smoothness. However, it also holds for the best iterate, ˆxk = arg mini∈[k] f(xi), meaning no knowledge of the directional smoothness is required to obtain the guarantee. We prove an alternative guarantee for the Polyak step-size in Theorem D.2, where the progress depends on the sum of step-sizes rather than on the average directional smoothness. This shows that the step-size in Equation (26) can itself be viewed as a measure of local smoothness, albeit without formal justification. 8Compared with the standard guarantee for the Polyak step-size underL-smoothness, f(xk)−f(x∗) ≤ 2L∆0/k (Hazan and Kakade, 2019), our analysis in Theorem 4.4 with the choice γ = 1.5 yields f(xk) − f(x∗) ≤ 3∆0 Pk−1 i=0 M(xi, xi+1)−1 ≤ 3∆0 k Pk−1 k=0 M(xi, xi+1) k , where the second bound follows from Jensen’s inequality and shows that the convergence depends on the average directional smoothness along the trajectory, rather than on L. If f is L-smooth, then M(xk, xk+1) ≤ L immediately recovers the classic rate for Polyak’s method up to a 3/2 constant factor. If f is not L-smooth, but M(xk, xk+1) is bounded, then Equation (27) generalizes the O(1/k) rate proved concurrently by Takezawa et al. (2024), but for any choice of directional smoothness (of which (L0, L1)-smoothness (Jingzhao Zhang et al., 2020) is but one). Comparison with strongly adapted step-sizes.As we saw for quadratics, strongly adapted step-sizes for any directional smoothness function allow us to obtain the following convergence rate, f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk−1 i=0 M(xi, xi+1)−1 . This is matches the guarantee given by Equation (27) up to constant factors. As a result, we give a positive answer to the question posed earlier in this section: GD with the Polyak step-size achieves the same convergence for any smoothness function M as GD with step-sizes strongly adapted to M. Application to the optimal directional smoothness. Theorem 4.4 holds for every directional smoothness function M. Therefore we can specialize Equation (27) with the optimal point-wise directional smoothness H (as defined in Equation (4)) and γ = 1.5 to get the guarantee, min i∈[k−1] [f(xi) − f(x∗)] ≤ 3∥x0 − x∗∥2 2Pk−1 i=0 H(xi, xi+1)−1 . (28) This rate requires computing the iterate with the minimum function value, but that is easy to track during optimization. Unlike our previous results, Equation (28) requires no access to the optimal point-wise smoothness, yet obtains a dependence on the tightest constant possible. 4.3 Normalized Gradient Descent Now we change directions slightly and study normalized GD, whose convergence also depends on the directional smoothness. Normalized GD uses step-sizes which are divided by the gradient magnitude, xk+1 = xk − ηk ∥∇f(xk)∥2 ∇f(xk). (29) Our next theorem shows that normalized GD obtains a guarantee which depends solely on the average of the point-wise directional smoothness Dk := D(xk, xk+1) despite no explicit knowledge of Dk. Theorem 4.5. Suppose that f is convex and differentiable. Let D be the point-wise directional smoothness defined by Equation (4) and ∆0 := ∥x0 − x∗∥2 2. Then normalized GD with a sequence of non-increasing step-sizes ηk satisfies f(ˆxk) − f(x∗) ≤ ∆0 + Pk−1 i=0 η2 i 2k2 f(x0) η2 0 − f(x∗) η2 k−1  + ∆0 + Pk−1 i=0 η2 i 2k k−1X i=0 M(xi, xi+1) k , (30) where ˆxk = arg mini∈[k−1] f(xi). If maxi∈[k−1] M(xi, xi+1) is bounded for all k (i.e. f is L- smooth), then for ηi = 1/ √ i we have f(ˆxk) − f(x∗) ∈ O(1/k) and for ηi = 1/ √ i we get the anytime result f(ˆxk) − f(x∗) ∈ O(log(k)/k). Theorem 4.5 gives a rate for normalized GD which is valid for any convex f without any dependence on global smoothness. However, does not adapt to any smoothness function like the Polyak step-size. 5 Experiments We evaluate the practical improvement of our convergence rates over those using L-smoothness on two logistic regression problems taken from the UCI repository (Asuncion and Newman, 2007). 90 100 200 Iterations 10□3 10□2 10□1 Optimality Gap ionosphere 0 100 200 Iterations 10□14 10□11 10□8 10□5 10□2 horse-colic 0 100 200 Iterations 10□2 10□1 ozone GD (1/L) GD (1/Dk) Polyak Norm. GD AdGD Figure 4: Comparison of GD withηk = 1/L, step-sizes strongly adapted to the point-wise smoothness (ηk = 1/D(xk, xk+1)), and the Polyak step-size against normalized GD (Norm. GD) and the AdGD method on three logistic regression problems. AdGD uses a smoothed version of the point-wise directional smoothness from the previous iteration to set ηk. We find that GD methods with adaptive step-sizes consistently outperform GD with ηk = 1/L and even obtain a linear rate on horse-colic. Figure 1 compares GD with strongly adapted step-sizes η = 1/Mk, where Mk is the point-wise smoothness, against GD with the Polyak step-size. We also plot the exact convergence rates for each method, Equation (16) and Equation (27), respectively, and compare against the classical guarantee for both methods. Our convergence rates are an order of magnitude tighter on the ionosphere dataset and display a remarkable ability to adapt to the path of optimization on mammographic. Figure 3 compares the performance of GD with strongly adapted step-sizes and with the fixed step- size ηk = 1/L for a synthetic quadratic with Hessian skew (R. Pan et al., 2022). Results are averaged over twenty random problems. We find that strongly adapted step-sizes lead to significantly faster convergence. Since Ak, Dk ≪ L, the adapted step-sizes are larger than 2/L, especially at the start of training; they eventually converge to2/L, indicating these methods operate at the edge-of-stability (J. Cohen et al., 2021; J. M. Cohen et al., 2022). This is consistent with Ahn et al. (2022) and Y . Pan and Y . Li (2023), who show local smoothness is correlated with edge-of-stability behavior. We conclude with a comparison of empirical convergence rates on three additional logistic regression problems from the UCI repository. We compare GD with ηk = 1/L, GD with step-sizes strongly adapted to the point-wise smoothness ( ηk = 1/Dk), GD with the Polyak step-size (Polyak), and normalized GD (Norm. GD) against the AdGD method (Malitsky and Mishchenko, 2020). The Polyak step-size performs best on every dataset butozone, where GD with ηk = 1/Dk solves the problem to high accuracy in just a few iterations. Thus, although Polyak step-sizes have the optimal dependence on directional smoothness, computing strongly adapted step-sizes can still be advantageous. 6 Conclusion We present new sub-optimality bounds for GD under novel measures of local gradient variation which we call directional smoothness functions. Our results hold for any step-sizes, improve over standard analyses when ηk is adapted to the choice of directional smoothness, and depend only on properties of f local to the optimization path. For convex quadratics, we show that computing step-sizes strongly adapted to directional smoothness functions is straightforward and recovers two well-known step-size schemes, including the Cauchy step-size. In the general case, we prove that an algorithm based on exponential search gives a weighted-version of the path-dependent convergence rate with no need for adapted step-sizes. We also show that GD with the Polyak step-size and normalized GD both obtain fast rates with no dependence on the global smoothness parameter. Crucially, the Polyak step-size adapts to any choice of directional smoothness, including the tightest possible parameter. 10Acknowledgements Aaron Mishkin was supported by NSF Grant DGE-1656518, by NSERC Grant PGSD3-547242-2020, and by an internship at the Center for Computational Mathematics, Flatiron Institute. We thank Si Yi Meng for insightful discussions during the preparation of this work and Fabian Schaipp for use of the step-back code. We also thank the anonymous reviewers for comments leading to improvements in Proposition 3.2 and the addition of Theorem D.2. References Ahn, Kwangjun, Jingzhao Zhang, and Suvrit Sra (2022). “Understanding the unstable convergence of gradient descent”. In: International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA. V ol. 162. Proceedings of Machine Learning Research, pp. 247–257. Altschuler, Jason M. and Pablo A. Parrilo (2023). “Acceleration by Stepsize Hedging I: Multi-Step Descent and the Silver Stepsize Schedule”. In: CoRR abs/2309.07879. Asuncion, Arthur and David Newman (2007). UCI machine learning repository. Barzilai, Jonathan and Jonathan M Borwein (1988). “Two-point step size gradient methods”. In: IMA journal of numerical analysis 8.1, pp. 141–148. Beck, Amir (2017). First-order methods in optimization. MOS-SIAM series on optimization. Philadel- phia : Philadelphia: Society for Industrial and Applied Mathematics ; Mathematical Optimization Society. ISBN : 978-161-197-4-9-9-7. Bengio, Yoshua (2012). “Practical Recommendations for Gradient-Based Training of Deep Archi- tectures”. In: Neural Networks: Tricks of the Trade - Second Edition. V ol. 7700. Lecture Notes in Computer Science, pp. 437–478. Berahas, Albert S., Lindon Roberts, and Fred Roosta (2023). “Non-Uniform Smoothness for Gradient Descent”. In: arXiv preprint arXiv:2311.08615 abs/2311.08615. Bertsekas, Dimitri P (1997). “Nonlinear programming”. In: Journal of the Operational Research Society 48.3, pp. 334–334. Bubeck, Sébastien et al. (2015). “Convex optimization: Algorithms and complexity”. In: Foundations and Trends® in Machine Learning 8.3-4, pp. 231–357. Carmon, Yair and Oliver Hinder (2022). “Making SGD Parameter-Free”. In:Conference on Learn- ing Theory, 2-5 July 2022, London, UK. V ol. 178. Proceedings of Machine Learning Research, pp. 2360–2389. Cohen, Jeremy et al. (2021). “Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability”. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Cohen, Jeremy M. et al. (2022). “Adaptive Gradient Methods At the Edge of Stability”. In: arXiv preprint arXiv:2207.14484 abs/2207.14484. Dai, Y . H. and X. Q. Yang (2006). “A New Gradient Method with an Optimal Stepsize Property”. In: Computational Optimization and Applications 33.1, pp. 73–88. Duchi, John C., Elad Hazan, and Yoram Singer (2010). “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”. In: COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pp. 257–269. Fernández-Delgado, Manuel et al. (2014). “Do we need hundreds of classifiers to solve real world classification problems?” In: The journal of machine learning research 15.1, pp. 3133–3181. Grimmer, Benjamin (2019). “Convergence Rates for Deterministic and Stochastic Subgradient Methods without Lipschitz Continuity”. In: SIAM J. Optim. 29.2, pp. 1350–1365. Hazan, Elad and Sham Kakade (2019). “Revisiting the Polyak step size”. In: arXiv preprint arXiv:1905.00313. He, Kaiming et al. (2015). “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification”. In: Proceedings of the IEEE international conference on computer vision, pp. 1026–1034. Hogan, William W (1973). “Point-to-set maps in mathematical programming”. In: SIAM review 15.3, pp. 591–603. Karimi, Hamed, Julie Nutini, and Mark Schmidt (2016). “Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition”. In: Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16. Springer, pp. 795–811. 11Levy, Kfir Y . (2017). “Online to Offline Conversions, Universality and Adaptive Minibatch Sizes”. In: Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1613–1622. Li, Haochuan et al. (2023). “Convex and Non-convex Optimization Under Generalized Smoothness”. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural In- formation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Li, Zhiyuan, Kaifeng Lyu, and Sanjeev Arora (2020). “Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate”. In: Advances in Neural Informa- tion Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Liu, Dong C and Jorge Nocedal (1989). “On the limited memory BFGS method for large scale optimization”. In: Mathematical programming 45.1-3, pp. 503–528. Lu, Zhaosong and Sanyou Mei (2023). “Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient”. In: SIAM Journal on Optimization 33.3, pp. 2275–2310. Malitsky, Yura and Konstantin Mishchenko (2020). “Adaptive Gradient Descent without Descent”. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event. V ol. 119. Proceedings of Machine Learning Research, pp. 6702–6712. Mei, Jincheng et al. (2021). “Leveraging Non-uniformity in First-order Non-convex Optimization”. In: Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. V ol. 139. Proceedings of Machine Learning Research, pp. 7555–7564. Mishkin, Aaron, Mert Pilanci, and Mark Schmidt (2024). “Faster Convergence of Stochastic Acceler- ated Gradient Descent under Interpolation”. In: arXiv preprint arXiv:2404.02378. Nesterov, Yurii (1983). “A method for solving the convex programming problem with convergence rate O (1/k2)”. In: Dokl akad nauk Sssr. V ol. 269, p. 543. Nesterov, Yurii et al. (2018).Lectures on convex optimization. V ol. 137. Springer. Orabona, Francesco (2023). “Normalized Gradients for All”. In: arXiv preprint arXiv:2308.05621 abs/2308.05621. Pan, Rui, Haishan Ye, and Tong Zhang (2022). “Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums”. In: ICLR. Pan, Yan and Yuanzhi Li (2023). “Toward understanding why adam converges faster than sgd for transformers”. In: arXiv preprint arXiv:2306.00204. Paquette, Courtney et al. (2023). “Halting time is predictable for large models: A universality property and average-case analysis”. In: Foundations of Computational Mathematics 23.2, pp. 597–673. Park, Jea-Hyun, Abner J Salgado, and Steven M Wise (2021). “Preconditioned accelerated gradient descent methods for locally Lipschitz smooth objectives with applications to the solution of nonlinear PDEs”. In: Journal of Scientific Computing 89.1, p. 17. Paszke, Adam et al. (2019). “Pytorch: An imperative style, high-performance deep learning library”. In: Advances in neural information processing systems 32. Patel, Vivak and Albert S. Berahas (2022). “Gradient descent in the absence of global Lipschitz conti- nuity of the gradients: Convergence, divergence and limitations of its continuous approximation”. In: arXiv preprint arXiv:2210.02418. Polyak, Boris T (1987). “Introduction to optimization”. In. Streeter, Matthew and H. Brendan McMahan (2010). “Less Regret Via Online Conditioning”. In: arXiv preprint arXiv:1002.4862. Takezawa, Yuki et al. (2024). “Polyak Meets Parameter-free Clipped Gradient Descent”. In:CoRR abs/2405.15010. DOI : 10 . 48550 / ARXIV . 2405 . 15010. arXiv: 2405 . 15010. URL : https : //doi.org/10.48550/arXiv.2405.15010. Vainsencher, Daniel, Han Liu, and Tong Zhang (2015). “Local Smoothness in Variance Reduced Optimization”. In: Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2179–2187. Virtanen, Pauli et al. (2020). “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python”. In: Nature Methods 17, pp. 261–272. Vladarean, Maria-Luiza, Yura Malitsky, and V olkan Cevher (2021). “A first-order primal-dual method with adaptivity to local smoothness”. In: Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 6171–6182. 12Zhang, Bohang et al. (2020). “Improved Analysis of Clipping Algorithms for Non-convex Optimiza- tion”. In: Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Zhang, Jingzhao et al. (2020). “Why Gradient Clipping Accelerates Training: A Theoretical Justifica- tion for Adaptivity”. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Zhang, Junyu and Mingyi Hong (2020). “First-order algorithms without Lipschitz gradient: A sequential local optimization approach”. In: arXiv preprint arXiv:2010.03194. Zhao, Weijing and He Huang (2024). “Adaptive stepsize estimation based accelerated gradient descent algorithm for fully complex-valued neural networks”. In:Expert Systems with Applications 236, p. 121166. 13A Proofs for Section 2 Lemma 2.2. If f is convex and differentiable, then the point-wise directional smoothness satisfies, f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + D(x, y) 2 ∥y − x∥2 2. (5) Proof. By the convexity of f we have f(x) + ⟨∇f(x), y− x⟩ ≤f(y). Rearranging and then using Cauchy-Schwarz we get f(x) ≤ f(y) + ⟨∇f(x), x− y⟩ = f(y) + ⟨∇f(y), x− y⟩ + ⟨∇f(x) − ∇f(y), x− y⟩ ≤ f(y) + ⟨∇f(y), x− y⟩ + ∥∇f(x) − ∇f(y)∥∥x − y∥ = f(y) + ⟨∇f(y), x− y⟩ + D(x, y) 2 ∥x − y∥2. Lemma 2.4. For any differentiable functionf, the path-wise smoothness (7) satisfies f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + A(x, y) 2 ∥y − x∥2 2. (8) Proof. Starting from the fundamental theorem of calculus, f(y) − f(x) − ⟨∇f(x), y− x⟩ = Z 1 0 ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩dt ≤ Z 1 0 A(x, y)t∥x − y∥2 2dt = A(x, y) 2 ∥y − x∥2 2. which completes the proof. Proposition 2.3. There exists a convex, differentiablef and x, y∈ Rd such that if t <2, then f(y) > f(x) + ⟨∇f(x), y− x⟩ + t∥∇f(x) − ∇f(y)∥ 2∥y − x∥2 ∥y − x∥2 2. (6) Proof. Let Hf denote the optimal pointwise directional smoothness associated with some convex and differentiable function f : Rd → R (as defined in Equation (4)), and Df denote the pointwise directional smoothness associated with f (as defined in Equation (4)). For any t, the statement of (6) is equivalent to saying Hf > t∥∇f(x)−∇f(y)∥ ∥x−y∥ for all x, y∈ Rd and convex, differentiablef. Observe that Lemma 2.2 already shows that for all convex and differentiable functions f : Rd → R Hf (x, y) ≤ Df (x, y) = 2∥∇f(x) − ∇f(y)∥ ∥x − y∥ for all x, y∈ Rd. In order to show that this is tight, we suppose by the way of contradiction that there exists some 2 > t≥ 0 such that for all convex and differentiable functions f : Rd → R Hf (x, y) ≤ t · ∥∇f(x) − ∇f(y)∥ ∥x − y∥ (31) for all x, y∈ R. We shall show that no sucht exists by showing for each such t there exists a function ft such that Equation (31) does not hold. Consider fϵ(x) = √ x2 + ϵ2 for ϵ ≤ 1. The function f is differentiable. Moreover f′ ϵ(x) = x√ x2 + ϵ2 , f ′′ ϵ (x) = ϵ2 (ϵ2 + x2) 3 2 ≥ 0. 14Therefore f is convex. Let g(x) = |x|. Fix x = 1 and y = 0, we have |g(x) − g(y) − sign(y) · (x − y)| ≤ |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| + |g(x) − fϵ(x)| + |g(y) − fϵ(y)| + |(f′ ϵ(y) − sign(y)) · (x − y)| = |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| + 1 − p 1 + ϵ2  + 0 − √ ϵ2  + |(0 − 0) · (1 − 0)| ≤ |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| + 2ϵ. Now observe that g(x) − g(y) − sign(y) · (x − y) = |1| − |0| −0 · (1 − 0) = 1. Therefore |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| ≥1 − 2ϵ. (32) By definition we have 1 2 ∥x − y∥2 = 1 2 , therefore Hf (x, y) = |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| 1 2 ∥x − y∥2 ≥ 2 − 4ϵ. (33) But by our starting assumption we have that there exists some t < 2 such that Hf (x, y) ≤ t∥f′(x)−f′(y)∥ |x−y| for all differentiable and convex functions f. Applying this to f = fϵ we get Hfϵ(1, 0) ≤ t|f′ ϵ(1) − f′ ϵ(0)| |1| = t · 1√ 1 + ϵ2 ≤ t. (34) Combining Equations (33) and (34) we have 2 − 4ϵ ≤ Hfϵ(1, 0) ≤ t Rearranging we get 2 − t ≤ 4ϵ Choosing ϵ = 2−t 8 > 0 we get a contradiction. It follows that the minimal t such that H(x, y) ≤ t|f′(x)−f′(y)| |x−y| for all convex and differentiable f is t = 2. Lemma A.1. One step of gradient descent with step-size ηk > 0 makes progress as f(xk+1) ≤ f(xk) − ηk  1 − ηkM(xk, xk+1) 2  ∥∇f(xk)∥2 2. Proof. Starting from Equation (4), we have f(xk+1) ≤ f(xk) + ⟨∇f(xk), xk+1 − xk⟩ + M(xk, xk+1) 2 ∥xk+1 − xk∥2 2 = f(xk) − ηk∥∇f(xk)∥2 2 + η2 kM(xk, xk+1) 2 ∥∇f(xk)∥2 2 = f(xk) − ηk 1 − ηkM(xk, xk+1) 2  ∥∇f(xk)∥2 2. B Proofs for Section 3 Lemma B.1. If f is convex, then for any x, y∈ Rd, f(y) ≥ f(x) + ⟨∇f(x), y− x⟩ + µ(x, y) 2 ∥y − x∥2 2. (35) If f is µ strongly convex, then µ(x, y) ≥ µ. 15Proof. The fundamental theorem of calculus implies f(x) − ⟨∇f(x), y− x⟩ = Z 1 0 ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩dt ≥ Z 1 0 µ(x, y)t∥x − y∥2 2dt = µ(x, y) 2 ∥y − x∥2 2. Note that we have implicitly used convexity to verify the inequality in the second line in the case where µ(x, y) = 0. Now assume that f is µ strongly convex. As a standard consequence of strong-convexity, we obtain: ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩ t∥x − y∥2 2 = ⟨∇f(x + t(y − x)) − ∇f(x), x+ t(y − x) − x⟩ t2∥x − y∥2 2 ≥ µ∥x − t(y − x) − x∥2 2 t2∥y − x∥2 2 = µ. Proposition 3.1. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, δk ≤ "Y i∈G (1 + ηiλiµi) # δ0 + X i∈B   Y j>i,j∈G (1 + ηjλjµj)   ηiλi 2 ∥∇f(xi)∥2 2, (13) where λi =ηiMi−2, G = {i : ηi <2/Mi}, and B = [k]\G. Proof. First note that λi < 0 for i ∈ Gand λi ≥ 0 for i ∈ B. We start from Equation (10), f(xk+1) ≤ f(xk) + ηk ηkM(xk, xk+1) 2 − 1  ∥∇f(xk)∥2 2 = f(xk) + 1 k∈G · ηkλk 2 ∥∇f(xk)∥2 2  + 1 k∈B · ηkλk 2 ∥∇f(xk)∥2 2  ≤ f(xk) + 1 k∈G · [ηkλkµk (f(xk) − f(x∗))] + 1 k∈B · ηkλk 2 ∥∇f(xk)∥2 2  , where we used that directional strong convexity gives ∥∇f(xk)∥2 2 ≥ 2µk (f(xk) − f(x∗)) . Subtracting f(x∗) from both sides and then recursively applying the inequality gives the result. Proposition 3.2. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, ∆k ≤ " kY i=0 |1 − µiηi| 1 + µi+1ηi # ∆0 + kX i=0  Y j>i |1 − µjηj| 1 + µj+1ηj     Miη3 i − η2 i  1 + µi+1ηi ∥∇f(xi)∥2 2. (15) Proof. Let ∆k = ∥xk − x∗∥2 2 and observe ∆k = ∥xk − xk+1 + xk+1 − x∗∥2 2 = ∆k+1 + ∥xk − xk+1∥2 2 + 2⟨xk − xk+1, xk+1 − x∗⟩. Using this expansion in ∆k+1 − ∆k, we obtain ∆k+1 − ∆k = −∥xk − xk+1∥2 2 − 2 ⟨xk − xk+1, xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − xk⟩ −2ηk ⟨∇f(xk), xk − x∗⟩. 16Now we control the inner-products with directional strong convexity and directional smoothness. ≤ −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − xk⟩ + 2ηk h f(x∗) − f(xk) − µk 2 ∆k i ≤ −η2 k∥∇f(xk)∥2 2 + 2ηk  f(xk) − f(xk+1) + M(xk, xk+1)η2 k 2 ∥∇f(xk)∥2 2  + 2ηk h f(x∗) − f(xk) − µk 2 ∆k i = η2 k (M(xk, xk+1)ηk − 1) ∥∇f(xk)∥2 2 + 2ηk [f(x∗) − f(xk+1)] − µkηk∆k ≤ η2 k (M(xk, xk+1)ηk − 1) ∥∇f(xk)∥2 2 − ηkµk+1∆k+1 − µkηk∆k, where the last inequality follows from µk+1 strong convexity between xk+1 and x∗. Re-arranging this expression allows us to deduce a rate with error terms depending on the local smoothness, =⇒ (1 + µk+1ηk)∆k+1 ≤ (1 − µkηk) ∆k + η2 k (M(xk, xk+1)η − 1) ∥∇f(xk)∥2 2 ≤ |1 − µkηk|∆k + η2 k (M(xk, xk+1)η − 1) ∥∇f(xk)∥2 2 =⇒ ∆k+1 ≤ |1 − µkηk| 1 + µk+1ηk ∆k + η2 k (M(xk, xk+1)η − 1) 1 + µk+1ηk ∥∇f(xk)∥2 2 ≤ " kY i=0 |1 − µiηi| 1 + µi+1ηi # ∆0 + kX i=0   kY j=i+1 |1 − µjηj| 1 + µj+1ηj   η2 i (M(xi, xi+1)ηi − 1) 1 + µi+1ηi ∥∇f(xi)∥2 2. Proposition 3.3. Let xk =Pk i=0 ηixi+1/Pk i=0 ηi. If f is convex and differentiable, then GD satisfies, f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk i=0 ηi + Pk i=0 η2 i (ηiMi − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . (16) Proof. Let ∆k = ∥xk − x∗∥2 2 and observe ∆k = ∥xk − xk+1 + xk+1 − x∗∥2 2 = ∆k+1 + ∥xk − xk+1∥2 2 + 2⟨xk − xk+1, xk+1 − x∗⟩. Using this expansion in ∆k+1 − ∆k, we obtain ∆k+1 − ∆k = −∥xk − xk+1∥2 2 − 2 ⟨xk − xk+1, xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − xk⟩ −2ηk ⟨∇f(xk), xk − x∗⟩. Now we use convexity and directional smoothness to control the two inner-products as follows: ∆k+1 − ∆k ≤ −η2 k∥∇f(xk)∥2 2 − 2ηk (f(xk) − f(x∗)) − 2ηk ⟨∇f(xk), xk+1 − xk⟩ ≤ −η2 k∥∇f(xk)∥2 2 − 2ηk (f(xk) − f(x∗)) + 2ηk(f(xk) − f(xk+1)) + η3 kM(xk, xk+1)∥∇f(xk)∥2 2 = η2 k(ηkM(xk, xk+1) − 1)∥∇f(xk)∥2 2 − 2ηk (f(xk+1) − f(x∗)) . Re-arranging this equation and summing over iterations implies the following sub-optimality bound: kX i=0 ηi Pk i=0 ηi (f(xi+1) − f(x∗)) ≤ ∆0 + Pk i=0 η2 i (ηiM(xi, xi+1) − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . Convexity of f and Jensen’s inequality now imply the final result, =⇒ f(xk) − f(x∗) ≤ ∆0 + Pk i=0 η2 i (ηiM(xi, xi+1) − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . 17B.1 Path-Dependent Acceleration: Proofs This section proves Theorem 3.4 using estimating sequences. Throughout this section, we assume µ ≥ 0 is the global strong convexity parameter, where µ = 0 covers the non-strongly convex case. We start from the estimating sequences version of Equation (17), which is given as follows: α2 k = ηk(1 − αk)γk + ηkαkµ γk+1 = (1 − αk)γk + αkµ yk = 1 γk + αkµ [αkγkvk + γk+1xk] xk+1 = yk − ηk∇f(yk) vk+1 = 1 γk+1 [(1 − αk)γkvk + αkµyk − αk∇f(yk)] . (36) The algorithm is initialized with x0 = v0 and some γ0 > 0. Note that y0 = x0 = v0 since yk is a convex combination of xk and vk. First we prove that this scheme is equivalent to the one given in Equation (17). Lemma B.2. Equation (36) and Equation (17) lead to equivalent updates for the yk, xk, and αk sequences. Moreover, given initializationγ0 > 0, the corresponding initialization for α0 is, α0 = η0 2  (µ − γ0) + p (γ0 − µ)2 + 4γ0/η0  . (37) Proof. The proof follows Nesterov et al. (2018, Theorem 2.2.3). Expanding the definition of vk+1, we obtain vk+1 = 1 γk+1 (1 − αk) αk [(γk + αkµ)yk − γk+1xk] + αkµyk − αk∇f(yk)  = 1 γk+1 (1 − αk)γk αk yk + µyk  − (1 − αk) αk xk − αk γk+1 ∇f(yk) = xk − ηk αk ∇f(yk) + 1 αk (yk − xk) = xk + 1 αk (xk+1 − xk) . Plugging this back into the expression for yk+1, yk+1 = 1 γk+1 + αk+1µ [αk+1γk+1vk+1 + γk+2xk+1] = 1 γk+1 + αk+1µ  αk+1γk+1(xk + 1 αk (xk+1 − xk)) + γk+2xk+1  = αk+1γk1 + αk(1 − αk+1)γk+1 + αkαk+1µ αk(γk+1 + αk+1µ) xk+1 − αk+1γk+1(1 − αk) αk(γk+1 + αk+1µ)xk = xk+1 + αk+1γk+1(1 − αk) αk(γk+1 + αk+1µ)(xk+1 − xk) = xk+1 + αk+1γk+1(1 − αk) αk   γk+1 + α2 k+1/ηk − (1 − αk+1)γk+1 (xk+1 − xk) = xk+1 + αk(1 − αk) (αk+1 + α2 k)(xk+1 − xk). Note that this update is consistent with Equation (17). Since γk = α2 k/ηk, we can write, α2 k+1 = ηk+1(1 − αk+1)γk + ηk+1αk+1µ = ηk+1 ηk (1 − αk+1)α2 k + ηk+1αk+1µ, 18which is also consistent with Equation (17). Finally, the initialization for α0 is determined by γ0 in Equation (36) as, α2 0 = η0(1 − α0)γ0 + η0α0µ. The quadratic formula now implies, α0 = η0 2  (µ − γ0) + p (γ0 − µ)2 + 4γ0/η0  . This completes the proof. As mentioned, our proof uses the concept of estimating sequences. Definition B.3. Two sequences λk, ϕk are estimating sequences for f if λl ≥ 0 for all k ∈ N, limk→∞ λk = 0, and, ϕk(x) ≤ (1 − λk)f(x) + λkϕ0(x), (38) for all x ∈ Rd. We use the same estimating sequences as developed by Nesterov et al. (2018). Letλ0 = 1, ϕ0(x) = f(x0) + γ0 2 ∥x − x0∥2 2, and define the updates, λk+1 = (1 − αk)λk ϕk+1(x) = (1 − αk)ϕk(x) + αk  f(yk) + ⟨∇f(yk), x− yk⟩ + µ 2 ∥x − yk∥2 2  , (39) where µ ≥ 0 is the strong convexity parameter, withµ = 0 when f is merely convex. It is straight- forward to differentiate ϕk+1 to see that vk+1 of Equation (36) is the minimizer. Indeed, Nesterov et al. (2018, Lemma 2.2.3) shows that this choice for the estimating sequences obeys the following canonical form: ϕk+1(x) = min z ϕk+1(z) + γk+1 2 ∥x − vk+1∥2 2, (40) where γk+1 and vk+1 are given by Equation (36) and the minimum value is, min z ϕk+1(z) = (1 − αk) min z ϕk(z) + αkf(yk) − α2 k 2γk+1 ∥∇f(yk)∥2 2 + αk(1 − αk)γk γk+1 µ 2 ∥yk − vk∥2 2 + ⟨∇f(yk), vk − yk⟩  . (41) Before we can prove our main theorem, we must show that these choices for λk and ϕk yield a valid estimating sequence. The following proofs build on (Nesterov et al., 2018) and (Mishkin et al., 2024). Lemma B.4. Assume αk ∈ (0, 1] for all k ∈ N. If µ >0 and γ0 = µ, then λk = k−1Y i=0 (1 − √ηkµ). (42) If µ ≥ 0 and γ0 ∈ (µ, µ+ 3/ηmin), then, λk ≤ 4 ηmin(γ0 − µ)(k + 1)2 . (43) Proof. Assume γ0 = µ >0. Then γk = µ for all k and, α2 k = (1 − αk)ηkµ + αkηkµ = ηkµ. As a consequence, λk = k−1Y i=0 (1 − √ηkµ), as claimed. 19Now assume γ0 ∈ (µ, 3L + µ). Modifying the proof by Nesterov et al. (2018, Lemma 2.2.4), we compute as follows: γk+1 − µ = (1 − αk)γk + (αk − 1)µ = (1 − αk)(γk − µ) Recursing on this equality implies γk+1 = (γ0 − µ) kY i=0 (1 − αk) = λk+1(γ0 − µ). If αk = 1 or λk = 0, then using λk+1 = (1 − αk)λk implies λk+1 = 0 and the result trivially holds. Otherwise, recall α2 k/γk+1 = ηk to obtain, 1 − λk+1 λk = αk = (γk+1ηk)1/2 = (ηkµ + ηkλk+1(γ0 − µ))1/2 =⇒ 1 λk+1 − 1 λk = 1 λ1/2 k+1  ηkµ λk+1 + ηk(γ0 − µ) 1/2 ≥ 1 λ1/2 k+1 ηminµ λk+1 + ηmin(γ0 − µ) 1/2 . Finally, this implies 2 λ1/2 k+1   1 λ1/2 k+1 − 1 λ1/2 k ! ≥   1 λ1/2 k+1 − 1 λ1/2 k !  1 λ1/2 k+1 + 1 λ1/2 k ! ≥ 1 λ1/2 k+1 ηminµ λk+1 + ηmin(γ0 − µ) 1/2 . Moreover, this bound holds uniformly for all k ∈ N. We have now exactly reached Eq. 2.2.11 of Nesterov et al. (2018, Lemma 2.2.4) with L replaced by 1/ηmin. Applying that Lemma with this modification, we obtain λk ≤ 4 ηmin(γ0 − µ)(k + 1)2 , which completes the proof. Lemma B.5. If f is strongly convex with parameter µ ≥ 0 and ηk ≤ 1/µ for all k ∈ N, then λk and ϕk are estimating sequences. Proof. Using the quadratic formula, we find αk = µ − γk ± p (µ − γk)2 + 4ˆγk/ηk 2/ηk . Thus, (µ − γk) +   (µ − γk)2 + 4ˆγk/ηk 1/2 > 0. is sufficient for αk > 0. This holds if µ ≥ γk. Otherwise, we require, (µ − γk)2 + 4ˆγk/ηk > (µ − γk)2, which holds if and only if ηk, γk > 0. On the other hand, we also need αk ≤ 1, which is satisfied when, 4 + 4ηk(γk − µ) + η2 k(µ − γk)2 ≤ η2 k(µ − γk)2 + 4ηkγk ⇐⇒ ηk ≤ 1 µ, as claimed. Recall λ0 = 1 and λk+1 = (1 − αk)λk. Since αk ∈ (0, 1], λk ≥ 0 holds by induction. It remains to show that λk tends to zero, which holds by Lemma B.4 since we have shown αk ∈ (0, 1] for all k. Now we establish the last piece, ϕk(x) ≤ (1 − λk)f(x) + λkϕ0(x). But this follows immediately by Nesterov et al. (2018, Lemma 2.2.2). 20Now we can prove the last major lemma before our convergence result. Lemma B.6. Suppose f is strongly convex with parameterµ ≥ 0 and ηk is a sequence of adapted step-sizes, meaning ηk ≤ 1/M(xk, xk+1). Then for every k ∈ N, min z ϕk(z) ≥ f(xk). Proof. We use an inductive proof again. The inductive assumption is min z ϕk(z) ≥ f(xk), It is easy to see this holds at k = 0 since, ϕ0(x) = f(x0) + γ0 2 ∥x − v0∥2 2, implies minz ϕ0(z) = f(x0). Using Equation (41), we obtain min z ϕk+1(z) = (1 − αk) min z ϕk(z) + αkf(yk) − α2 k 2γk+1 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 µ 2 ∥yk − vk∥2 + ⟨∇f(yk, zk), vk − yk⟩  ≥ (1 − αk)f(xk) + αkf(yk) − α2 k 2γk+1 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩  , where the inequality holds by the inductive assumption. Using convexity off and recalling α2 k γk+1 = ηk from the definition of the update (Equation (36)), min z ϕk+1(z) ≥ (1 − αk) (f(yk) + ⟨∇f(yk), xk − yk⟩) + αkf(yk) − ηk 2 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩  = f(yk) + (1− αk) ⟨∇f(yk), xk − yk⟩ −ηk 2 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩  Using the fact that the step-sizes are adapted and invoking the directional descent lemma (i.e. Equation (18)) now implies min z ϕk+1(z) ≥ f(xk+1) + (1− αk)  ⟨∇f(yk), xk − yk⟩ + αkγk γk+1 µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩  . The remainder of the proof is largely unchanged from the analysis in Nesterov et al. (2018). The definition of yk gives xk − yk = αkγk γk+1 (yk − vk), which we use to obtain min z ϕk+1(z) ≥ f(xk+1) + (1− αk) αkγk γk+1 ⟨∇f(yk), yk − vk⟩ + αkγk γk+1 µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩  = f(xk+1) + µαk(1 − αk)γk 2γk+1 ∥yk − vk∥2 ≥ f(xk+1), since µαk(1−αk)γk 2γk+1 ≥ 0. We conclude the desired result by induction. 21The main accelerated result now follows almost immediately. Theorem 3.4. Suppose f is differentiable, µ–strongly convex and AGD is run with adapted step-sizes ηk ≤ 1/Mk. If µ >0 and α0 = √η0µ, then AGD obtains the following accelerated rate: f(xk+1) − f(x∗) ≤ kY i=0 (1 − √µηi) h f(x0) − f(x∗) + µ 2 ∥x0 − x∗∥2 2 i . (19) Let ηmin = mini∈[k] ηi. If µ ≥ 0 and α0 ∈ (√µη0, c), where c is the maximum value of α0 for which γ0 = α2 0−η0α0µ η0(1−α0) satisfies γ0 < 3/ηmin + µ, then AGD obtains the following rate: f(xk+1) − f(x∗) ≤ 4 ηmin(γ0 − µ)(k + 1)2 h f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 i . (20) Proof. We analyze the equivalent formulation given in Equation (36). See Lemma B.2 for a formal proof that these two schemes produce the same xk, yk, and αk iterates. Note that our proof follows Nesterov et al. (2018) and Mishkin et al. (2024) closely; while their results are very similar, we are not aware of pre-existing works which adapt them to our specific setting. First, observe that M(xk, xk+1) ≥ µ for all k ∈ N. Since the step-sizes ηk are assumed to satisfy ηk ≤ 1/M(xk, xk+1), we also have that ηk ≤ 1/µ for every k. Thus, Lemma B.4 and Lemma B.5 apply. Using the definition of an estimating sequence and Lemma B.6, we obtain, f(xk) ≤ min z ϕk(z) ≤ min z (1 − λk)f(z) + λkϕ0(z) ≤ (1 − λk)f(x∗) + λkϕ0(x∗). Re-arranging this equation and expanding the definition ϕ0 (Equation (39)), we deduce the following: f(xk) − f(x∗) ≤ λk (ϕ0(x∗) − f(x∗)) = λk  f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2  . We see that the rate of convergence of AGD is entirely controlled by the convergence of the sequence λk. If µ >0 and γ0 = µ, then Lemma B.4 implies f(xk) − f(x∗) ≤ k−1Y i=0 (1 − √µηi) h f(x0) − f(x∗) + µ 2 ∥x0 − x∗∥2 2 i . By Lemma B.2, this initialization is equivalent to choosing α0 = √η0µ, which is the setting claimed in the theorem. Alternatively, if µ ≥ 0 and γ0 ∈ (µ, µ+ 3/ηmin), then, f(xk) − f(x∗) ≤ 4 ηmin(γ0 − µ)k2 h f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 i , where the equality, γ0 = α2 0 − η0α0µ η0(1 − α0) , holds by Lemma B.2. Since α0 ≤ 1 for η0 ≤ 1/µ, η0 is an increasing function of γ0. Thus, an upper-bound c on α0 can be deduced from that on γ0 using the quadratic formula: c = − 3η0 2ηmin + η0 2  9 (ηmin)2 + 43ηmin + µ η0 1/2 = 3η0 2ηmin " 1 + 4(ηmin)2 3ηmin + µ 9η0 1/2 − 1 # . 22C Proofs for Section 4.1 Lemma C.1. Let B be a positive semi-definite matrix and suppose that f(x) = 1 2x⊤Bx − c⊤x. Let xi+1 = xi − η∇f(xi). Then for any η >0, the pointwise directional smoothness between the gradient descent iterates xi, xi+1 is given by 1 2D(xi, xi+1) = ∥B∇f(xi)∥2 ∥∇f(xi)∥2 . Proof. We have by straightforward algebra, 1 2D(xi, xi+1) = ∥∇f(xi+1) − ∇f(xi)∥2 ∥xi+1 − xi∥2 = ∥[Bxi+1 − c] − [Bxi − c] ∥2 ∥xi+1 − xi∥2 = ∥B [xi+1 − xi] ∥2 ∥xi+1 − xi∥2 = ∥B [−η∇f(xi)] ∥2 ∥ −η∇f(xi)∥2 = ∥B∇f(xi)∥2 ∥∇f(xi)∥2 . Lemma C.2. Let B be a positive semi-definite matrix and suppose that f(x) = 1 2x⊤Bx − c⊤x. Let xi+1 = xi − η∇f(xi). Then for any η >0, the path-wise directional smoothness between the gradient descent iterates xi, xi+1 is given by by A(xi, xi+1) = ∇f(xi)⊤B∇f(xi) ∇f(xi)⊤∇f(xi) . Proof. Let At(x, y) = ⟨∇f(x+t(y−x))−∇f(x),y−x⟩ t∥x−y∥2 2 . We have At(x, y) = ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩ t∥x − y∥2 2 = ⟨(B(x + t(y − x))) − c − [Bx − c] , y− x⟩ t∥x − y∥2 2 = ⟨t · B(y − x), y− x⟩ t∥x − y∥2 2 = (y − x)⊤B(y − x) ∥x − y∥2 2 . The path-wise directional smoothness A is therefore A(x, y) = sup t∈[0,1] At(x, y) = sup t∈[0,1] (y − x)⊤B(y − x) ∥x − y∥2 2 = (y − x)⊤B(y − x) ∥x − y∥2 2 . 23Plugging in y = x − η∇f(x) = x − η[Bx − c] in the above gives A(x, x− η∇f(x)) = (−η [Bx − c]) B(−η) [Bx − c] ∥η[Bx − c]∥2 2 = (Bx − c)⊤B(Bx − c) ∥Bx − c∥2 2 = (Bx − c)⊤B(Bx − c) ∥Bx − c∥2 2 = ∇f(x)⊤B∇f(x) ∇f(x)⊤∇f(x) . D Proofs for Section 4.2 Proposition 4.1. If f is convex and continuously differentiable, then either (i) f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/D(x, x− η∇f(x)). Proof. Let I = {η : ∇f(x − η∇f(x)) = ∇f(x)}. For every η ∈ I, it holds that −⟨∇f(x − η∇f(x)), ∇f(x)⟩ = −∥∇f(x)∥2 2. However, since f is convex, the directional derivative −⟨∇f(x − η′∇f(x)), ∇f(x)⟩, is monotone non-decreasing in η′. We deduce that I must be an interval of form [0, η]. If η is not bounded, then f is linear along −∇f(x) and is minimized by taking η → ∞. Therefore, we may assume η is finite. Let η >η. Then we have the following: x − η∇f(x) = x − 2∥x − η∇f(x) − x∥2 ∥∇f(x − η∇f(x)) − ∇f(x)∥2 ∇f(x) ⇐⇒ ∇f(x) = 2∥∇f(x)∥2 ∥∇f(x − η∇f(x)) − ∇f(x)∥2 ∇f(x), from which we deduce ∥∇f(x − η∇f(x)) − ∇f(x)∥2 = 2∥∇f(x)∥2, is sufficient for the implicit equation to hold. Squaring both sides and multiplying by 1/2, we obtain the following alternative root-finding problem: h(η) := 1 2∥∇f(x − η∇f(x))∥2 2 − ⟨∇f(x − η∇f(x)), ∇f(x)⟩ −1 2∥∇f(x)∥2 2 = 0. (44) Because f is C1, h is a continuous function and it suffices to show that there exists an interval in which h crosses 0. From the display above, we see h(η) = −∥∇f(x)∥2 2 < 0. Continuity now implies ∃η′ > η such that h(η′) < 0. Now, supposeh(η) ≤ 0 for all η ≥ η′. Working backwards, we see that this can only occur when η ≤ 2∥x − η∇f(x) − x∥2 ∥∇f(x − η∇f(x)) − ∇f(x)∥2 = 1 D(x(η), x− η∇f(x)) for all η ≥ η′. The directional descent lemma (Equation (10)) now implies f(x − η∇f(x)) ≤ f(x) − η  1 − ηD(x, x− η∇f(x)) 2  ∥∇f(x)∥2 2 ≤ f(x) − η 2∥∇f(x)∥2 2, Taking limits on both sides as η → ∞implies f(x − η∇f(x)) is minimized along the ray x(η) = x − η∇f(x). Thus, we deduce that either there exists η′′ > η′ such that h(η′′) > 0 exists, or f is minimized along the gradient direction as claimed. 24Proposition 4.2. If f is convex and twice continuously differentiable, then either (i)f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/A(x, x− η∇f(x)). Proof. Let J =  η : ⟨∇f(x − η∇f(x)), ∇f(x)⟩ = ∥∇f(x)∥2 2 	 . Since f is convex, the directional derivative −⟨∇f(x − η′∇f(x)), ∇f(x)⟩, is monotone non-decreasing in η′. We deduce that J must be an interval of form [0, η]. If η is not bounded, then convexity implies lim η→∞ f(x − η∇f(x)) ≤ lim η→∞ f(x) − η ⟨∇f(x − η∇f(x)), ∇f(x)⟩ = −∞, meaning f is minimized along −∇f(x). Therefore, we may assume η is finite. We have x − η∇f(x) = x − 1 A(x, x− η∇f(x))∇f(x) ⇐⇒ η = inf t∈[0,1] tη∥∇f(x)∥2 2 ⟨∇f(x) − ∇f(x − tη∇f(x)), ∇f(x)⟩. Thus, for η >η, the equation we must solve reduces to h(η) := η − inf t∈[0,1] tη∥∇f(x)∥2 2 ⟨∇f(x) − ∇f(x − tη∇f(x)), ∇f(x)⟩ = 0. Since f is C2, h is continuous (see, e.g. Hogan (1973, Theorem 7)) and it suffices to show that there exists an interval over which h crosses 0. Using Taylor’s theorem, we can re-write this expression as h(η) = η − inf t∈[0,1] ∥∇f(x)∥2 2 ⟨∇f(x), ∇2f(x − α(tη)∇f(x))∇f(x)⟩, where for some α(tη) ∈ [0, tη]. Examining the denominator, we find that, Z t 0 ∇f(x)⊤∇2f(x − tη∇f(x))∇f(x)dt = ⟨∇f(x − η∇f(x)) − ∇f(x), ∇f(x)⟩ = 0, which, since f is convex, implies ∇f(x)⊤∇2f(x − α∇f(x))∇f(x) = 0, for every α ∈ [0, η]. By continuity of the Hessian, for every ϵ >0, there exists δ > 0 such that η′ ∈ [η, η + δ] guarantees, ∇f(x)⊤∇2f(x − η′∇f(x))∇f(x) < ϵ. Substituting this into our expression for h, h(η′) = η′ − inf t∈[0,1] ∥∇f(x)∥2 2 ⟨∇f(x), ∇2f(x − α(tη′)∇f(x))∇f(x)⟩ < η + δ − ∥∇f(x)∥2 2 ϵ < 0, for ϵ, δsufficiently small. Thus, there exists η′ > η for which h(η′) < 0. Now let us show that h(η′′) > 0 for some η′′. For convenience, define g(η) = inf t∈[0,1] t∥∇f(x)∥2 2 ⟨∇f(x) − ∇f(x − tη∇f(x)), ∇f(x)⟩, 25Algorithm 1 Gradient Descent with Exponential Search 1: Procedure ExponentialSearch(x, η0) 2: for k = 1, 2, 3, . . .do 3: ηout ← RootFindingBisection  x, 2−2k η0, η0  . 4: if ηout < ∞ then 5: Return ηout 6: end if 7: end for 8: End Procedure 9: Procedure RootFindingBisection(x, ηlo, ηhi) 10: Define ϕ(η) = η − ψ(η) where ψ(η) is given in (24) \\ One access to ϕ requires T descent steps. 11: if ϕ(ηhi) ≤ 0 then 12: Return ηhi 13: end if 14: if ϕ(ηlo) > 0 then 15: Return ∞ 16: end if 17: while ηhi > 2ηlo do 18: ηmid = √ηloηhi 19: if ϕ(ηmid) > 0 then 20: ηhi = ηmid 21: else 22: ηlo = ηmid 23: end if \\ Invariant: ϕ(ηhi) > 0, and ϕ(ηlo) ≤ 0. 24: end while 25: Return ηlo 26: End Procedure which is a continuous and monotone non-increasing function. Take η → ∞and let lim η→∞ g(η) = c, where the limit exists, but may be −∞. Indeed, it must hold that c <∞ since, lim η→∞ g(η) < g(η′) < ∞. If c <0, then taking η′′ large enough that g(η′′) ≤ 0 suffices. Alternatively, if c ≥ 0, then there exists ˜η such that g(η) ≤ c + ϵ for every η ≥ ˜η. Choosing η′′ > max {˜η, c} + ϵ yields h(η′′) = η′′ − g(η′′) > c+ ϵ − c − ϵ = 0. This completes the proof. Theorem 4.3. Assume f is convex and L-smooth. Then Algorithm 1 with η0 > 0 requires at most 2K(log log(2η0/L) ∨ 1) iterations of GD and in the last run it outputs a step-size η and point xK = 1 K PK−1 i=0 xi(η) such that exactly one of the following holds: Case 1: η = η0 and f(xK) − f(x∗) ≤ ∥x0 − x∗∥2 2 2Kη0 Case 2: η ̸= η0 and f(xK) − f∗ ≤ ∥x0 − x∗∥2 2 2K "Pk i=0 Mi∥∇f(x′ i)∥2 2 Pk i=0 ∥∇f(x′ i)∥2 2 # , where Mi def = M(x′ i, x′ i+1) and x′ i are the iterates generated by GD with step-size η′ ∈ [η, 2η]. 26Proof of Theorem 4.3. This analysis follows (Carmon and Hinder, 2022). First, instantiate Equa- tion (16) from Proposition 3.3 with ηi = η for all i to obtain f(xk) − f∗ ≤ ∥x0 − x∗∥2 2ηk + η h η Pk i=0 M(xi, xi+1)∥∇f(xi)∥2 − Pk i=0 ∥∇f(xi)∥2 i 2k . (45) Now, observe that if we get a “Lucky strike” andϕ(ηhi) = ϕ(η0) ≤ 0, then specializing Equation (45) for η = η0 we get f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2η0k + η0 2k " η0 kX i=0 M(xi, xi+1)∥∇f(xi)∥2 2 − kX i=0 ∥∇f(xi)∥2 2 # = ∥x0 − x∗∥2 2 2η0k + η0 Pk i=0 M(xi, xi+1)∥∇f(xi)∥2 2 2k · ϕ(η0) ≤ ∥x0 − x∗∥2 2 2η0k . This covers the first case of Theorem 4.3. With the first case out of the way, we may assume thatϕ(ηhi) > 0. This implies that ηhi > 1 L , since if η ≤ 1 L we have ϕ(η) ≤ 0. Now observe that when ηlo = 22−k η0 ≤ 1 L , we have that ϕ(ηlo) ≤ 0, therefore it takes at most k = ⌈log log η0 L−1 ⌉ to find such an ηlo. From here on, we suppose that ϕ(ηhi) > 0 and ϕ(ηlo) ≤ 0. Now observe that the algorithm’s main loop always maintains the invariant ϕ(ηhi) > 0 and ϕ(ηlo) ≤ 0, and every iteration of the loop halves log ηhi ηlo , therefore we make at most ⌈log logη0L⌉ loop iterations. The output step-size ηlo satisfies ηhi 2 ≤ ηlo ≤ ηhi and ϕ(ηlo) ≤ 0. Specializing Equation (45) for η = η0 and using that ϕ(ηlo) ≤ 0 we get f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2ηlok + ηlo Pk i=0 M(xi(ηlo), xi+1(ηlo))∥∇f(xi(ηlo))∥2 2 2k · ϕ(ηlo) ≤ ∥x0 − x∗∥2 2 2ηlok . (46) By the loop invariant ϕ(ηhi) > 0 we have ϕ(ηhi) > 0 ⇔ ηhi > PK i=0 ∥∇f(xi(ηhi))∥2 2PK i=0 ∥∇f(xi(ηhi))∥2 2M(xi(ηhi), xi+1(ηhi)) By the loop termination condition we have ηlo ≥ ηhi 2 , combining this with the last equation we get ηlo ≥ ηhi 2 ≥ 1 2 PK i=0 ∥∇f(xi(ηhi))∥2 2PK i=0 ∥∇f(xi(ηhi))∥2 2M(xi(ηhi), xi+1(ηhi)) . Plugging this into Equation (46) we obtain f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 k · PK i=0 ∥∇f(xi(ηhi))∥2 2M(xi(ηhi), xi+1(ηhi)) PK i=0 ∥∇f(xi(ηhi))∥2 2 It remains to notice that ηhi ∈ [ηlo, 2ηlo]. Theorem 4.4. Suppose that f is convex and differentiable and let M be any directional smoothness function for f. Let ∆0 := ∥x0 − x∗∥2 2. Then GD with the Polyak step-size and γ ∈ (1, 2) satisfies f(xk) − f(x∗) ≤ c(γ)∆0 2 Pk−1 i=0 M(xi, xi+1)−1 , (27) where c(γ) = γ/(2 − γ)(γ − 1) and xk = Pk−1 i=0  M(xi, xi+1)−1xi  / Pk−1 i=0 M(xi, xi+1)−1  . For the proof of this theorem, we will need the following proposition: 27Proposition D.1. Let x ∈ Rd. Define ηx = γ f(x)−f(x∗) ∥∇f(x)∥2 for some γ ∈ (1, 2) and let ˜x = x − ηx∇f(x). Then, f(x) − f(x∗) ≥ γ − 1 γ2 2 M(x, ˜x)∥∇f(x)∥2 2. Proof. Observe f(x) − f(x∗) = f(x) − f(˜x) + f(˜x) − f(x∗) ≥ f(x) − f(˜x). (47) By smoothness we have f(˜x) ≤ f(x) + ⟨∇f(x), ˜x − x⟩ + M(x, ˜x) 2 ∥˜x − x∥2 = f(x) − ηx∥∇f(x)∥2 2 + η2 xM(x, ˜x) 2 ∥∇f(x)∥2 2. Plugging back into Equation (47) we get f(x) − f(x∗) ≥ ηx∥∇f(x)∥2 2 − η2 xM(x, ˜x) 2 ∥∇f(x)∥2 2. Let us now use the definition of ηx = γ f(x)−f(x∗) ∥∇f(x)∥2 2 to get f(x) − f(x∗) ≥ γ(f(x) − f(x∗)) − γηxM(x, ˜x) 2 (f(x) − f(x∗)). Assuming that f(x) ̸= f(x∗) then we get by cancellation 1 ≥ γ − γηxM(x, ˜x) 2 . Using the definition of ηx again 1 − γ ≥ −γ2 M(x, ˜x) 2 f(x) − f(x∗) ∥∇f(x)∥2 2 Rearranging we get f(x) − f(x∗) ≥ γ − 1 γ2 2 M(x, ˜x)∥∇f(x)∥2 2. If f(x) = f(x∗) then ∥∇f(x)∥2 2 = 0 , both sides are identically zero and the statement holds trivially. Now we can prove our theorem on the convergence of GD with Polyak step-sizes: Proof of Theorem 4.4. We start by considering the distance to the optimum and expanding the square ∥xk+1 − x∗∥2 2 = ∥xk − x∗∥2 2 + 2⟨xk+1 − xk, xk − x∗⟩ + ∥xk+1 − xk∥2 2 = ∥xk − x∗∥2 2 − 2ηk ⟨∇f(xk), xk − x∗⟩ + η2 k∥∇f(xk)∥2 2. (48) Let δk = f(xk) − f(x∗). By convexity we have f(x∗) ≥ f(xk) + ⟨∇f(xk), x∗ − xk⟩. Therefore we can upper bound Equation (48) as ∥xk+1 − x∗∥2 2 ≤ ∥xk − x∗∥2 2 − 2ηkδk + η2 k∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − 2ηkδk + ηk   γ δk ∥∇f(xk)∥2 2 ! ∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − (2 − γ)ηkδk, (49) 28where in the second line we used the definition of ηk. By Proposition D.1 we have δk ≥ γ − 1 γ 2 M(xk, xk+1)∥∇f(xk)∥2 2. (50) Using this in Equation (49) gives ∥xk+1 − x∗∥2 2 ≤ ∥xk − x∗∥2 2 − (2 − γ)ηk γ − 1 γ2 2 M(xk, xk+1)∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − (2 − γ)γ − 1 γ2 2 M(xk, xk+1)   γ δk ∥∇f(xk)∥2 2 ! ∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − 2(2 − γ)(γ − 1) γM (xk, xk+1) δk. Rearranging we get 2(2 − γ)(γ − 1) γM (xk, xk+1) δk ≤ ∥xk − x∗∥2 2 − ∥xk+1 − x∗∥2 2. Summing up and telescoping we get k−1X i=0 2(2 − γ)(γ − 1) γM (xi, xi+1) δi ≤ ∥x0 − x∗∥2 2. Let xk = 1Pk−1 i=0 M(xi,xi+1)−1 Pk−1 i=0 M(xi, xi+1)−1xi, then by the convexity of f and Jensen’s inequality we have f(xk) − f(x∗) ≤ 1 Pk−1 i=0 M(xi, xi+1)−1 k−1X i=0 M(xi, xi+1)−1δi ≤ γ 2(2 − γ)(γ − 1) 1 Pk−1 i=0 M(xi, xi+1)−1 ∥x0 − x∗∥2 2. Theorem D.2. If f is convex and differentiable, then GD with the Polyak step-size andγ <2 satisfies, f(xk) − f(x∗) ≤ 1 (2 − γ) Pk i=0 ηi ∥x0 − x∗∥2 2, (51) where xk = Pk−1 i=0 ηixi/ Pk−1 i=0 ηi  . Proof. The proof begins in the same manner as that for Theorem 4.4, ∥xk+1 − x∗∥2 2 = ∥xk − x∗∥2 2 + 2⟨xk+1 − xk, xk − x∗⟩ + ∥xk+1 − xk∥2 2 = ∥xk − x∗∥2 2 − 2ηk ⟨∇f(xk), xk − x∗⟩ + η2 k∥∇f(xk)∥2 2 ≤ ∥xk − x∗∥2 2 − 2ηkδk + η2 k∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − 2ηkδk + ηk   γ δk ∥∇f(xk)∥2 2 ! ∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − (2 − γ)ηkδk. Re-arranging, summing from i = 0 to k − 1, and dividing by Pk−1 i=0 ηi, =⇒ k−1X i=0 ηi Pk i=0 ηi (f(xi) − f(w∗)) ≤ 1 (2 − γ) Pk i=0 ηi  ∥x0 − x∗∥2 2 − ∥xk − x∗∥2 2  =⇒ f(xk) − f(x∗) ≤ 1 (2 − γ) Pk i=0 ηi ∥x0 − x∗∥2 2, which completes the proof. 29Lemma D.3. Normalized GD with step-sizes ηk satisfies − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ ≤η2 kM(xk, xk+1) − ηk∥∇f(xk)∥2. (52) Proof. By convexity we have f(xk+1) ≤ f(xk) + ⟨xk+1 − xk, ∇f(xk+1)⟩ = f(xk) − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ (53) Now note that − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ = ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk) − ∇f(xk+1)⟩ (54) − ηk∥∇f(xk)∥2 ≤ ηk∥∇f(xk) − ∇f(xk+1)∥ −ηk∥∇f(xk)∥2, (55) where we used Cauchy-Schwarz. Recalling the definition of directional smoothness M(xk, xk+1) def = ∥∇f(xk) − ∇f(xk+1)∥ ∥xk − xk+1∥ = ∥∇f(xk) − ∇f(xk+1)∥ ηk in Equation (55) gives − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ ≤η2 kM(xk, xk+1) − ηk∥∇f(xk)∥2. Theorem 4.5. Suppose that f is convex and differentiable. Let D be the point-wise directional smoothness defined by Equation (4) and ∆0 := ∥x0 − x∗∥2 2. Then normalized GD with a sequence of non-increasing step-sizes ηk satisfies f(ˆxk) − f(x∗) ≤ ∆0 + Pk−1 i=0 η2 i 2k2 f(x0) η2 0 − f(x∗) η2 k−1  + ∆0 + Pk−1 i=0 η2 i 2k k−1X i=0 M(xi, xi+1) k , (30) where ˆxk = arg mini∈[k−1] f(xi). If maxi∈[k−1] M(xi, xi+1) is bounded for all k (i.e. f is L- smooth), then for ηi = 1/ √ i we have f(ˆxk) − f(x∗) ∈ O(1/k) and for ηi = 1/ √ i we get the anytime result f(ˆxk) − f(x∗) ∈ O(log(k)/k). Proof. Here we will first establish that for any non-increasing sequence of step-sizes ηk > 0 we have that min i∈[k−1] f(xi) − f(x∗) ≤ 1 2 ∆0 + Pk−1 i=0 η2 i k   f(x0) kη2 0 − f(x∗) kη2 k−1 + k−1X i=0 M(xi, xi+1) k ! . (56) The specialized results follow by assuming that Pk−1 i=0 M(xi,xi+1) k is bounded, which it is the case of L–Lipschitz gradients. In particular the mini∈[k−1] f(xi) − f(x∗) ∈ O(1/T) result follows by plugging in ηi = 1/ √ k and using that k−1X i=0 η2 i = k−1X i=0 1 k = 1 f(x0) kη2 0 − f(x∗) kη2 k−1 = f(x0) − f(x∗). Alternatively we get mini∈[k−1] f(xi) − f(x∗) ∈ O(log(T)/T) by plugging in ηi = 1/√i + 1 and using that k−1X i=0 η2 i = k−1X i=0 1 i + 1 ≤ log(k) f(x0) kη2 0 − f(x∗) kη2 k−1 = f(x0) k − f(x∗). 30With this in mind, let us prove Equation (56). By convexity, f(xk+1) ≤ f(xk) − ηk ∥∇f(xk)∥2 ∇f(xk)⊤∇f(xk+1) ≤ f(xk) − ηk∥∇f(xk)∥2 + η2 kM(xk, xk+1). (Using (52)) Re-arranging, dividing through by η2 k, and then summing over i = 0, ··· , k− 1 gives k−1X i=0 ∥∇f(xi)∥2 ηi ≤ f(x0) η2 0 + k−2X i=1 f(xi)  1 η2 i − 1 η2 i−1  − f(x∗) η2 k−1 + k−1X i=0 M(xi, xi+1) ≤ f(x0) η2 0 − f(x∗) η2 k−1 + k−1X i=0 M(xi, xi+1), (57) where we used that ηi−1 ≤ ηi =⇒ 1 η2 i − 1 η2 i−1 ≤ 0. Using Jensen’s inequality over the mapa 7→ 1/a, which is convex for a positive, gives k−1X i=0 ηi ∥∇f(xk)∥2 ≥ k2 Pk−1 i=0 ∥∇f(xk)∥2/ηi (57) ≥ k2 f(x0) η2 0 − f(x∗) η2 k−1 + Pk−1 i=0 M(xi, xi+1) . (58) Meanwhile, recall our notation ∆i = ∥xi − x∗∥2 2. Expanding the squares and using that f(x) is convex, we have that ∆i+1 = ∆i − 2 ηi ∥∇f(xi)∥2 ∇f(xi)⊤(xi − x∗) + η2 i ≤ ∆i − 2ηi f(xi) − f(x∗) ∥∇f(xk)∥2 + η2 i . As before, we use δi := f(xi) − f(x∗). Re-arranging, summing both sides of the above over i = 0, . . . , k− 1 and using telescopic cancellation gives k−1X i=0 ηi δi ∥∇f(xi)∥ ≤ ∆0 + Pt−1 i=0 η2 i 2 . Using the above along with (58) gives, min i∈[k−1] δi ≤ 1 Pk−1 i=0 ηi ∥∇f(xi)∥2 k−1X i=0 ηi δi ∥∇f(xi)∥2 ≤ 1 2 ∆0 + Pk−1 i=0 η2 iPk−1 i=0 ηi ∥∇f(xi)∥ ≤ 1 2 ∆0 + Pk−1 i=0 η2 i k   f(x0) kη2 0 − f(x∗) kη2 k−1 + k−1X i=0 M(xi, xi+1) k ! E Experimental Details In this section we provide additional details necessary to reproduce our experiments. We run our logistic regression experiments using PyTorch (Paszke et al., 2019). For the UCI datasets, we use the pre-processed version of the data provided by Fernández-Delgado et al. (2014), although we do not use their evaluation procedure as it is known have test-set leakage. Instead, we randomly perform an 80–20 train-test split and use the test set for validation. Unless otherwise stated, all methods are initialized using the Kaiming initialization (He et al., 2015), which is standard in PyTorch. 31In order to compute the strongly adapted step-sizes, we run the SciPy (Virtanen et al., 2020) imple- mentation of Newton method on Equation (44). In general, we find this procedure is surprisingly robust, although it can be slow. Figure 1: We pick two datasets from the UCI repository to showcase different behaviors of the upper-bounds. We compute a tight-upper bound on L as follows. Recall that for logistic regression problems the Hessian is given by ∇2f(x) = A⊤Diag  1 σ(−y · Ax) + 2 +σ(y · Ax)  A, where A is the data matrix and σ(z) = 1 1+exp(z) is the sigmoid function. A short calculation shows that the diagonal matrix Diag  1 σ(−y · Ax) + 2 +σ(y · Ax)  ⪯ 1 4I, which is tight when x = 0. As a result, L = λmax(A⊤A)/4. We compute this manually. We also compute the optimal value for the logistic regression problem using the SciPy implementation of BFGS (Liu and Nocedal, 1989). We use this value for f(x∗) to compute the Polyak step-size and when plotting sub-optimality. It turns out that the upper-bound based on L-smoothness for both GD with the Polyak step-size (Hazan and Kakade, 2019) and standard GD (Bubeck et al., 2015) is f(xk) − f(x∗) ≤ 2L∥x0 − x∗∥2 2 k . Figure 3: We run these experiments using vanilla NumPy. As mentioned in the text, we generate a quadratic optimization problem min x 1 2x⊤Ax − b⊤x, where the eigenvalues of A were generated to follow power law distribution with parameter α = 3. We scaled the eigenvalues to ensureL = 1000. The dimension of the problem we create is d = 300. We repeat the experiment for 20 random trials and plot the mean and standard deviations. Figure 4: We pick three different datasets from the UCI repository to showcase the possible con- vergence behavior of the optimization methods. We compute L and f(w∗) as described above for Figure 1. For normalized GD, we use the step-size schedule ηk = η0/ √ k as suggested by our theory. To pickη0, we run a grid search on the grid generated by np.logspace(-8, 1, 20). We implement AdGD from scratch and use a starting step-size of η0 = 10−3. We use the same procedure to compute the strongly adapted step-sizes as described above. F Computational Details The experiments in Figure 3 were run on a MacBook Pro (16 inch, 2019) with a 2.6 GHz 6-Core Intel i7 CPU and 16GB of memory. All other experiments were run on a Slurm cluster with several different node configurations. Our experiments on the cluster were run with nodes using (i) Nvidia A100 GPUs (80GB or 40GB memory) or Nvidia H100-80GB GPUs with Icelake CPUs, or (ii) Nvidia V100-32GB or V100-16GB GPUs with Skylake CPUs. All jobs were allocated a single GPU and 24GB of RAM. 32
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper introduces new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the optimization path, rather than on global, worst-case L-smoothness constants. It develops the concept of directional smoothness, a measure of gradient variation, to construct tighter upper-bounds on the objective. Key findings include: new path-dependent sub-optimality bounds for GD, demonstration that strongly adapted step-sizes recover classical schemes for quadratic problems (e.g., Cauchy step-size), and proof that the Polyak step-size and normalized GD achieve fast, path-dependent rates without explicit knowledge of directional smoothness. The research also shows that these convergence guarantees are tighter than classical L-smoothness theory in experiments.",
    "methodology": "The core methodology revolves around defining and leveraging 'directional smoothness' functions M(x, y) which quantify gradient variation along the chord between points x and y. Three constructive directional smoothness functions are introduced: point-wise smoothness D(x,y), path-wise smoothness A(x,y), and optimal point-wise smoothness H(x,y). These functions are used to derive a local descent lemma and new sub-optimality bounds for convex functions, including the case with directional strong convexity. The paper explores 'strongly adapted step-sizes' (ηk = 1/M(xk+1, xk)) which require solving non-linear root-finding problems for general functions, but are shown to be equivalent to well-known step-sizes for quadratics. For general convex functions, existence proofs for strongly adapted step-sizes are provided, and an exponential search algorithm is adapted to find approximate adaptive step-sizes. Additionally, the paper analyzes the Polyak step-size and normalized GD, demonstrating their inherent adaptivity to directional smoothness.",
    "experimental_setup": "The experimental evaluation was performed on logistic regression problems from the UCI repository (ionosphere, mammographic, horse-colic, ozone datasets) and synthetic quadratic problems. For UCI datasets, a pre-processed version was used with an 80-20 train-test split for validation. The optimal value f(x*) for logistic regression was computed using SciPy's BFGS implementation. Methods were initialized using Kaiming initialization. For synthetic quadratic problems, eigenvalues were generated to follow a power-law distribution with L=1000. Benchmarks included GD with a fixed step-size (1/L), GD with step-sizes strongly adapted to point-wise smoothness (1/Dk), Polyak step-size GD, normalized GD (with a grid-searched η0 and schedule ηk = η0/√k), and the AdGD method. Strongly adapted step-sizes were computed using SciPy's Newton method on Equation (44). Experiments were run on a MacBook Pro or a Slurm cluster with Nvidia A100/H100/V100 GPUs.",
    "limitations": "The general computation of strongly adapted step-sizes for non-quadratic functions involves solving challenging non-linear root-finding problems (e.g., using Newton's method), which can be impractical for large-scale optimization. While exponential search can find approximate adaptive step-sizes, it introduces a log-log penalty and requires the function to be L-smooth. Obtaining accelerated rates for non-strongly convex functions (µ=0) requires prior knowledge of the minimum step-size, which is not straightforward when dealing with locally Lipschitz gradients. The provided rate for normalized GD (Theorem 4.5) does not adapt to any smoothness function like the Polyak step-size does.",
    "future_research_directions": "The paper lays the groundwork for further exploration of path-dependent optimization. One implicit direction is improving the practical computation of strongly adapted step-sizes for general non-quadratic and non-convex functions. Further research could also focus on extending the analysis of directional smoothness to other optimization algorithms beyond GD and AGD, particularly those used in deep learning. Investigating how the insights from directional smoothness can be integrated into adaptive gradient methods (like Adagrad variants) is another potential area. Additionally, developing anytime rates for non-strongly convex acceleration with locally Lipschitz gradients without requiring prior knowledge of minimum step-sizes remains an open problem."
}
