
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
A Bayesian Perspective on Training Speed and Model Selection Clare Lyle † Lisa Schut† Binxin Ru† Yarin Gal† Mark van der Wilk‡ Abstract We take a Bayesian perspective to illustrate a connection between training speed and the marginal likelihood in linear models. This provides two major insights: ﬁrst, that a measure of a model’s training speed can be used to estimate its marginal likelihood. Second, that this measure, under certain conditions, predicts the relative weighting of models in linear model combinations trained to minimize a regression loss. We verify our results in model selection tasks for linear models and for the inﬁnite-width limit of deep neural networks. We further provide encouraging empirical evidence that the intuition developed in these settings also holds for deep neural networks trained with stochastic gradient descent. Our results suggest a promising new direction towards explaining why neural networks trained with stochastic gradient descent are biased towards functions that generalize well. 1 Introduction Choosing the right inductive bias for a machine learning model, such as convolutional structure for an image dataset, is critical for good generalization. The problem of model selection concerns itself with identifying good inductive biases for a given dataset. In Bayesian inference, the marginal likelihood (ML) provides a principled tool for model selection. In contrast to cross-validation, for which computing gradients is cumbersome, the ML can be conveniently maximised using gradients when its computation is feasible. Unfortunately, computing the marginal likelihood for complex models such as neural networks is typically intractable. Workarounds such as variational inference suffer from expensive optimization of many parameters in the variational distribution and differ signiﬁcantly from standard training methods for Deep Neural Networks (DNNs), which optimize a single parameter sample from initialization. A method for estimating the ML that closely follows standard optimization schemes would pave the way for new practical model selection procedures, yet remains an open problem. A separate line of work aims to perform model selection by predicting a model’s test set performance. This has led to theoretical and empirical results connecting training speed and generalization error [17, 21]. This connection has yet to be fully explained, as most generalization bounds in the literature depend only on the ﬁnal weights obtained by optimization, rather than on the trajectory taken during training, and therefore are unable to capture this relationship. Understanding the link between training speed, optimization and generalization thus presents a promising step towards developing a theory of generalization which can explain the empirical performance of neural networks. In this work, we show that the above two lines of inquiry are in fact deeply connected. We investigate the connection between the log ML and the sum of predictive log likelihoods of datapoints, condi- tioned on preceding data in the dataset. This perspective reveals a family of estimators of the log †OATML Group, University of Oxford. Correspondence toclare.lyle@cs.ox.ac.uk ‡Imperial College London 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.14499v1  [cs.LG]  27 Oct 2020ML which depend only on predictions sampled from the posterior of an iterative Bayesian updating procedure. We study the proposed estimator family in the context of linear models, where we can conclusively analyze its theoretical properties. Leveraging the fact that gradient descent can produce exact posterior samples for linear models [31] and the inﬁnite-width limit of deep neural networks [7, 26], we show that this estimator can be viewed as the sum of a subset of the model’s training losses in an iterative optimization procedure. This immediately yields an interpretation of marginal likelihood estimation as measuring a notion of training speed in linear models. We further show that this notion of training speed is predictive of the weight assigned to a model in a linear model combination trained with gradient descent, hinting at a potential explanation for the bias of gradient descent towards models that generalize well in more complex settings. We demonstrate the utility of the estimator through empirical evaluations on a range of model selection problems, conﬁrming that it can effectively approximate the marginal likelihood of a model. Finally, we empirically evaluate whether our theoretical results for linear models may have explanatory power for more complex models. We ﬁnd that an analogue of our estimator for DNNs trained with stochastic gradient descent (SGD) is predictive of both ﬁnal test accuracy and the ﬁnal weight assigned to the model after training a linear model combination. Our ﬁndings in the deep learning setting hint at a promising avenue of future work in explaining the empirical generalization performance of DNNs. 2 Background and Related Work 2.1 Bayesian Parameter Inference A Bayesian model Mis deﬁned by a prior distribution over parameters θ, P(θ|M), and a prediction map from parameters θto a likelihood over the dataD, P(D|θ,M). Parameter ﬁtting in the Bayesian framework entails ﬁnding the posterior distribution P(θ|D), which yields robust and principled uncertainty estimates. Though exact inference is possible for certain models like Gaussian processes (GPs) [38], it is intractable for DNNs. Here approximations such as variational inference [4] are used [14, 5, 27, 16, 9], to improve robustness and obtain useful uncertainty estimates. Variational approximations require optimisation over the parameters of the approximate posterior distribution. This optimization over distributions changes the loss landscape, and is signiﬁcantly slower than the pointwise optimization used in standard DNNs. Pointwise optimization methods inspired by Bayesian posterior sampling can produce similar variation and uncertainty estimates as variational inference, while improving computational efﬁciency [45, 30, 29]. An appealing example of this is ensembling [25], which works by training a collection models in the usual pointwise manner, starting from kindependently initialized points. In the case of linear models, this is exactly equivalent to Bayesian inference, as this sample-then- optimize approach yields exact posterior samples [ 31, 36]. He et al. [18] extend this approach to obtain posterior samples from DNNs in the inﬁnite-width limit. 2.2 Bayesian Model Selection In addition to ﬁnding model parameters, Bayesian inference can also perform model selection over different inductive biases, which are speciﬁed through both model structure (e.g. convolutional vs fully connected) and the prior distribution on parameters. The Bayesian approach relies on ﬁnding the posterior over models P(M|D), which uses the marginal likelihood (ML) as its likelihood function: P(D|M) = ∫ θ P(D|θ)P(θ|Mi)dθ= EP(θ|M)P(D|θ) . (1) Instead of computing the full posterior, it is common to select the model with the highest marginal likelihood. This is known as type-II maximum likelihood [ 27, 28] and is less prone to overﬁtting than performing maximum likelihood over the parameters and model combined. This is because the marginal likelihood is able to trade off between model ﬁt and model complexity [39]. Maximising the ML is standard procedure when it is easy to compute. For example, in Gaussian processes it used to set simple model parameters like smoothness [38], while recent work has demonstrated that complex inductive biases in the form of invariances can also be learned [44]. For many deep models, computing Equation 1 is intractable, and obtaining approximations that are accurate enough for model selection and that scale to complex models is an active area of research [23]. In general, variational lower bounds that scale are too loose when applied to DNNs [5]. Deep 2Gaussian processes provide a case where the bounds do work [6, 8], but heavy computational load holds performance several years behind deep learning. While ensembling methods provide useful uncertainty estimates and improve the computational efﬁciency of the variational approach, they have not yet provided a solution for Bayesian model selection. 2.3 Generalization and Risk Minimization Bayesian model selection addresses a subtly different problem from the risk minimization framework used in many learning problems. Nonetheless, the two are closely related; Germain et al. [15] show that in some cases optimizing a PAC-Bayesian risk bound is equivalent to maximizing the marginal likelihood of a Bayesian model. In practice, maximizing an approximation of the marginal likelihood in DNNs trained with SGD can improve generalization performance [41]. More recently, Arora et al. [1] computed a data-dependent complexity measure which resembles the data-ﬁt term in the marginal likelihood of a Bayesian model and which relates to optimization speed, hinting at a potential connection between the two. At the same time, generalization in deep neural networks (DNNs) remains mysterious, with classical learning-theoretic bounds failing to predict the impressive generalization performance of DNNs [47, 33]. Recent work has shown that DNNs are biased towards functions that are ‘simple’, for various deﬁnitions of simplicity [22, 13, 43, 42]. PAC-Bayesian generalization bounds, which can quantify a broad range of deﬁnitions of complexity, can attain non-vacuous values [32, 10, 11], but nonetheless exhibit only modest correlation with generalization error [21]. These bounds depend only on the ﬁnal distribution over parameters after training; promising alternatives consider properties of the trajectory taken by a model during optimization [17, 35]. This trajectory-based perspective is a promising step towards explaining the correlation between the number of training steps required for a model to minimize its objective function and its ﬁnal generalization performance observed in a broad range of empirical analyses [21, 3, 34, 40]. 3 Marginal Likelihood Estimation with Training Statistics In this section, we investigate the equivalence between the marginal likelihood (ML) and a notion of training speed in models trained with an exact Bayesian updating procedure. For linear models and inﬁnitely wide neural networks, exact Bayesian updating can be done using gradient descent optimisation. For these cases, we derive an estimator of the marginal likelihood which 1) is related to how quickly a model learns from data, 2) only depends on statistics that can be measured during pointwise gradient-based parameter estimation, and 3) becomes tighter for ensembles consisting of multiple parameter samples. We also investigate how gradient-based optimization of a linear model combination can implicitly perform approximate Bayesian model selection in Section 3.3. 3.1 Training Speed and the Marginal Likelihood Let Ddenote a dataset of the form D = ( Di)n i=1 = ( xi,yi)n i=1, and let D<i = ( Dj)i−1 j=1 with D<1 = ∅. We will abbreviate P(D|M) := P(D) when considering a single model M. Observe that P(D) = ∏n i=1 P(Di|D<i) to get the following form of the log marginal likelihood: log P(D) = log n∏ i=1 P(Di|D<i) = n∑ i=1 log P(Di|D<i) = n∑ i=1 log[EP(θ|D<i)P(Di|θ)]. (2) If we deﬁne training speed as the number of data points required by a model to form an accurate posterior, then models which train faster – i.e. whose posteriors assign high likelihood to the data after conditioning on only a few data points – will obtain a higher marginal likelihood. Interpreting the negative log posterior predictive probability log P(Di|D<i) of each data point as a loss function, the log ML then takes the form of the sum over the losses incurred by each data point during training, i.e. the area under a training curve deﬁned by a Bayesian updating procedure. 3.2 Unbiased Estimation of a Lower Bound In practice, computing log P(Di|D<i) may be intractable, necessitating approximate methods to estimate the model evidence. In our analysis, we are interested in estimators of log P(D) computed 3by drawing ksamples of θ ∼P(θ|D<i) for each i = 1,...,n . We can directly estimate a lower bound L(D) = ∑n i=1 E[log P(Di|D<i) using the log likelihoods of these samples ˆL(D) = n∑ i=1 1 k k∑ j=1 log P(Di|θi j). (3) This will produce a biased estimate of the log marginal likelihood due to Jensen’s inequality. We can get a tighter lower bound by ﬁrst estimating E[log P(Di|θ)] using our posterior samples before applying the logarithm, obtaining ˆLk(D) = n∑ i=1 log 1 k k∑ j=1 P(Di|θi j). (4) Proposition 3.1. Both ˆLand ˆLk as deﬁned in Equation 4 are estimators of lower bounds on the log marginal likelihood; that is E[ ˆL(D)] = L(D) ≤log P(D) and E[ ˆLk(D)] = Lk(D) ≤log P(D) . (5) Further, the bias term in Lcan be quantiﬁed as follows. L(D) = log P(D) − n∑ i=1 KL(P(θ|D<i)||P(θ|D<i+1)) (6) We include the proof of this and future results in Appendix A. We observe that both lower bound estimators exhibit decreased variance when using multiple posterior samples; however, ˆLk also exhibits decreasing bias (with respect to the log ML) as kincreases; each kdeﬁnes a distinct lower bound Lk = E[ ˆLk] on log P(D). The gap induced by the lower bound L(D) is characterized by the information gain each data point provides to the model about the posterior, as given by the Kullback-Leibler (KL) divergence [24] between the posterior at time iand the posterior at time i+ 1. Thus, while Lhas a Bayesian interpretation it is arguably more closely aligned with the minimum description length notion of model complexity [19]. When the posterior predictive distribution of our model is Gaussian, we consider a third approach which, unlike the previous two methods, also applies to noiseless models. Let D= (Xi,Yi)n i=1, and (θi j)k j=1 be kparameter samples from P(θ|D<i). We assume a mapping f : Θ ×X →Y such that sampling parameters θ and computing f(θ,Xi) is equivalent to sampling from the posterior P(·|D<i,Xi). We can then obtain the following estimator of a lower bound on log P(D). Proposition 3.2. Let P(Yi|D<i,Xi) = N(µi,σ2 i) for some µi,σ2 i. Deﬁne the standard mean and variance estimators ˆµi = 1 N ∑N j=1 f(θi j,xi) and ˆσ2 i = 1 N−1 ∑(f(θi j,xi) −ˆµ)2. Then the estimator ˆLS(D) = n∑ i=1 log P(Yi|ˆµi,ˆσ2 i) (7) is a lower bound on the log ML: i.e. E[ ˆLS(D)] ≤log P(D). We provide an empirical evaluation of the rankings provided by the different estimators in Section 4. We ﬁnd that ˆLS exhibits the least bias in the presence of limited samples from the posterior, though we emphasize its limitation to Gaussian posteriors; for more general posterior distributions, ˆLk minimizes bias while still estimating a lower bound. 3.2.1 Lower bounds via gradient descent trajectories The bounds on the marginal likelihood we introduced in the previous section required samples from the sequence of posteriors as data points were incrementally added p(θ|D<i). Ensembles of linear models trained with gradient descent yield samples from the model posterior. We now show that we can use these samples to estimate the log ML using the estimators introduced in the previous section. We will consider the Bayesian linear regression problem of modelling dataD= (Xi,Yi)n i=1 assumed to be generated by the process Y = θ⊤Φ(X) + ϵ∼N(0,σ2 NI) for some unknown θ, known σ2 N, 4and feature map Φ. Typically, a Gaussian prior is placed onθ; this prior is then updated as data points are seen to obtain a posterior over parameters. In the overparmeterised, noiseless linear regression setting, Matthews et al. [31] show that the distribution over parameters θobtained by sampling from the prior on θ0 and running gradient descent to convergence on the dataD<i is equivalent to sampling from the posterior conditioned on D<i. Osband et al. [36] extend this result to posteriors which include observation noise σ2 N ̸= 0 under the assumption that the targets Yi are themselves noiseless observations. Algorithm 1: Marginal Likelihood Estimation for Linear Models Input: A dataset D= (xi,yi)n i=1, parameters µ0,σ2 0,σ2 N Result: An estimate of L(D) θt ←θ0 ∼N(µ0,σ2 0); ˜Y ←Y + ϵ∼N(0,σ2 N); sumLoss ←0 ; ℓ(D≤i,w) ←∥˜Y≤i −θ⊤X≤i∥2 2 + σ2 N θ2 0 ∥θ−θ0∥2 2; for Di ∈D do sumLoss = sumLoss + (θ⊤ t xi−yi)2 2σ2 N ; θt ←GradientDescent(ℓ,θt,D≤i) ; end return sumLoss We can use this procedure to obtain posterior samples for our estimators by iteratively running sample- then-optimize on the sets D<i. Algorithm 1 outlines our approach, which uses sample-then-optimize on iterative subsets of the data to obtain the necessary posterior samples for our estimator. Theorem 3.3 shows that this procedure yields an unbiased estimate of L(D) when a single prior sample is used, and an unbiased estimate of Lk(D) when an ensemble of kmodels are trained in parallel. Theorem 3.3. Let D= (Xi,Yi)n i=1 and let (θi j)n,J i,j=1 be generated by the procedure outlined above. Then the estimators ˆL, ˆLS,and ˆLk, applied to the collection (θi j), are lower bounds on log P(D). Further, expressing −log P(Di|θ) as the ℓ2 regression loss plus a constant, we then obtain log P(D) ≥ n∑ i=1 Eθi∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,θi) + c= L(D) (8) We highlight that Theorem 3.3 precisely characterizes the lower bound on the marginal likelihood as a sum of ‘training losses’ based on the regression lossℓ2(Di,θi). 3.2.2 From Linear Models to Inﬁnite Neural Networks Beyond linear models, our estimators can further perform model selection in the inﬁnite-width limit of neural networks. Using the optimization procedure described by He et al. [18], we can obtain an exact posterior sample from a GP given by the neural tangent kernel [20]. The iterative training procedure described in Algorithm 1 will thus yield a lower bound on the marginal likelihood of this GP using sampled losses from the optimization trajectory of the neural network. We evaluate this bound in Section 4, and formalize this argument in the following corollary. Corollary 3.4. Let Dbe a dataset indexed by our standard notation. Let f0 be sampled from an inﬁnitely wide neural network architecture Funder some initialization distribution, and let fi ∞be the limiting solution under the training dynamics deﬁned by He et al. [18] applied to the initialization f0 and using data D<i. Let K∞denote the neural tangent kernel for F, and M= GP(0,K∞) the induced Gaussian Process. Then fi ∞∼P(f|D<i,M), and in the limit of inﬁnite training time, the iterative sample-then-optimize procedure yields an unbiased estimate of L(D|M). Letting ℓ2 denote the scaled squared ℓ2 regression loss and cbe a constant, we obtain as a direct corollary of Theorem 3.3 P(D) ≥Efi∞∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,fi) + c= L(D) . (9) This result provides an additional view on the link between training speed and generalisation in wide neural networks noted by Arora et al. [1], who analysed the convergence of gradient descent. They 5compute a PAC generalization bound which a features the data complexity term equal to that in the marginal likelihood of a Gaussian process Rasmussen [38]. This term provides a bound on the rate of convergence of gradient descent, whereas our notion of training speed is more closely related to sample complexity and makes the connection to the marginal likelihood more explicit. It is natural to ask if such a Bayesian interpretation of the sum over training losses can be extended to non-linear models trained with stochastic gradient descent. Although SGD lacks the exact posterior sampling interpretation of our algorithm, we conjecture a similar underlying mechanism connecting the sum over training losses and generalization. Just as the marginal likelihood measures how well model updates based on previous data points generalize to a new unseen data point, the sum of training losses measures how well parameter updates based on one mini-batch generalize to the rest of the training data. If the update generalizes well, we expect to see a sharper decrease in the training loss, i.e. for the model to train more quickly and exhibit a lower sum over training losses. This intuition can be related to the notion of ‘stiffness’ proposed by Fort et al.[12]. We provide empirical evidence supporting our hypothesis in Section 4.2. 3.3 Bayesian Model Selection and Optimization The estimator L(D) reveals an intriguing connection between pruning in linear model combinations and Bayesian model selection. We assume a data set D= (Xi,Yi)n i=1 and a collection of kmodels M1,..., Mk. A linear regressor wis trained to ﬁt the posterior predictive distributions of the models to the target Yi; i.e. to regress on the dataset (Φ,Y ) = ( φi = ( ˆYi 1 ,..., ˆYi n),Yi )n i=1 with ˆYi j ∼P( ˆY|D<i,Xi,Mj). (10) The following result shows that the optimal linear regressor on this data generating distribution assigns the highest weight to the model with the highest L(D) whenever the model errors are independent. This shows that magnitude pruning in a linear model combination is equivalent to approximate Bayesian model selection, under certain assumptions on the models. Proposition 3.5. Let M1,..., Mk be Bayesian linear regression models with ﬁxed noise variance σ2 N and Gaussian likelihoods. Let Φ be a (random) matrix of posterior prediction samples, of the form Φ[i,j] = ˆyj i ∼P(yj|D<j,xj,Mi). Suppose the following two conditions on the columns of Φ are satisﬁed: E⟨Φ[:,i],y⟩= E⟨Φ[:,j],y⟩for all i,j, and E⟨Πy⊥φi,Πy⊥φj⟩= 0. Let w∗denote the least-squares solution to the regression problem minwEΦ∥Φw−y∥2. Then the following holds arg max i w∗ i = arg max i L(D|Mi) ∀w∗= arg min w E∥Φw−y∥2 . (11) The assumption on the independence of model errors is crucial in the proof of this result: families of models with large and complementary systematic biases may not exhibit this behaviour. We observe in Section 4 that the conditions of Proposition 1 are approximately satisﬁed in a variety of model comparison problems, and running SGD on a linear combination of Bayesian models still leads to solutions that approximate Bayesian model selection. We conjecture that analogous phenomena occur during training within a neural network. The proof of Proposition 3.5 depends on the observation that, given a collection of features, the best least-squares predictor will assign the greatest weight to the feature that best predicts the training data. While neural networks are not linear ensembles of ﬁxed models, we conjecture that, especially for later layers of the network, a similar phenomenon will occur wherein weights from nodes that are more predictive of the target values over the course of training will be assigned higher magnitudes. We empirically investigate this hypothesis in Section 4.2. 4 Empirical Evaluation Section 3 focused on two key ideas: that training statistics can be used as an estimator for a Bayesian model’s marginal likelihood (or a lower bound thereof), and that gradient descent on a linear ensemble implicitly arrives at the same ranking as this estimator in the inﬁnite-sample, inﬁnite-training-time limit. We further conjectured that similar phenomena may also hold for deep neural networks. We now illustrate these ideas in a range of settings. Section 4.1 provides conﬁrmation and quantiﬁcation of our results for linear models, the model class for which we have theoretical guarantees, while Section 4.2 provides preliminary empirical conﬁrmation that the mechanisms at work in linear models also appear in DNNs. 6Figure 1: Left: ranking according to log P(D), L(D) with exact posterior samples, and L(D) computed on samples generated by gradient descent. Right: gap between true marginal likelihood and Lk(D) estimator shrinks as a function of kfor both exact and gradient descent-generated samples. 4.1 Bayesian Model Selection While we have shown that our estimators correspond to lower bounds on the marginal likelihood, we would also like the relative rankings of models given by our estimator to correlate with those assigned by the marginal likelihood. We evaluate this correlation in a variety of linear model selection problems. We consider three model selection problems; for space we focus on one, feature dimension selection, and provide full details and evaluations on the other two tasks in Appendix B.1. For the feature dimension selection task, we construct a synthetic dataset inspired by Wilson and Izmailov [46] of the form (X,y), where xi = (yi+ ϵ1,yi+ ...,y i+ ϵ15,ϵ16,...,ϵ 30), and consider a set of models {Mk}with feature embeddings φk(xi) = xi[1,...,k ]. The optimal model in this setting is the one which uses exactly the set of ‘informative’ featuresx[1,..., 15]. We ﬁrst evaluate the relative rankings given by the true marginal likelihood with those given by our estimators. We compare LS, Land Lk; we ﬁrst observe that all methods agree on the optimal model: this is a consistent ﬁnding across all of the model selection tasks we considered. While all methods lower bound the log marginal likelihood,Lk(D) and LS(D) exhibit a reduced gap compared to the naive lower bound. In the rightmost plot of Figure 1, we further quantify the reduction in the bias of the estimator Lk(D) described in Section 3. We use exact posterior samples (which we denote in the ﬁgure simply as posterior samples) and approximate posterior samples generated by the gradient descent procedure outlined in Algorithm 1 using a ﬁxed step size and thus inducing some approximation error. We ﬁnd that both sampling procedures exhibit decreasing bias as the number of samples kis increased, with the exact sampling procedure exhibiting a slightly smaller gap than the approximate sampling procedure. We next empirically evaluate the claims of Proposition 3.5 in settings with relaxed assumptions. We compare the ranking given by the true log marginal likelihood, the estimated L(D), and the weight assigned to each model by the trained linear regressor. We consider three variations on how sampled predictions from each model are drawn to generate the features φi: sampling the prediction for point ˆYi from P( ˆYi|D<i) (‘concurrent sampling’ – this is the setting of Proposition 3.5), as well as two baselines: the posterior P( ˆYi|D) (‘posterior sampling’), and the priorP( ˆYi) (‘prior sampling’). We ﬁnd that the rankings of the marginal likelihood, its lower bound, and of the ranking given by concurrent optimization all agree on the best model in all three of the model selection problems outlined previously, while the prior and posterior sampling procedure baselines do not exhibit a consistent ranking with the log ML. We visualize these results for the feature dimension selection problem in Figure 2; full results are shown in Figure 5. We further illustrate how the L(D) estimator can select inductive biases in the inﬁnite-width neural network regime in Figure 2. Here we evaluate the relative change in the log ML of a Gaussian Process induced by a fully-connected MLP (MLP-NTK-GP) and a convolutional neural network (Conv-NTK-GP) which performs regression on the MNIST dataset. The fully-connected model sees a consistent decrease in its log ML with each additional data point added to the dataset, whereas the convolutional model sees the incremental change in its log ML become less negative as more data 7Figure 2: Left: Relative rankings given by optimize-then-prune, ML, and estimated L(D) on the feature selection problem. Right: visualizing the interpretation of L(D) as the ‘area under the curve’ of training losses: we plot the relative change in the estimator L(D≤i) −L(D<i) for convolutional and fully-connected NTK-GP models, and shade their area. Figure 3: Linear combinations of DNNs on FashionMNIST trained. Left: ensemble weights versus the test loss for concurrent training. Middle: sum over training losses (SOTL), standardized by the number of training samples, versus test loss for parallel training. Right: training curves for the different models trained in parallel. All results are averaged over 10 runs, and standard deviations are shown by the shaded regions around each observation. The model parameters, given in the parentheses, are the number of layers (l), nodes per layer (n) and kernel size (k), respectively. points are added as a result of its implicit bias, as well as a much higher incremental change in its log ML from the start of training. This leads to the Conv-NTK-GP having a higher value for L(D) than the MLP-NTK-GP. We provide an analogous plot evaluatinglog P(D) in the appendix. 4.2 Training Speed, Ensemble Weight, and Generalization in DNNs We now address our conjectures from Section 3, which aim to generalize our results for linear models to deep neural networks trained with SGD. Recall that our hypothesis involves translatingiterative posterior samples to minibatch training losses over an SGD trajectory, and bayesian model evidence to generalization error; we conjectured that just as the sum of the log posterior likelihoods is useful for Bayesian model selection, the sum of minibatch training losses will be useful to predict generalization error. In this section, we evaluate whether this conjecture holds for a simple convolutional neural network trained on the FashionMNIST dataset. Our results provide preliminary evidence in support of this claim, and suggest that further work investigating this relationship may reveal valuable insights into how and why neural networks generalize. 4.2.1 Linear Combination of DNN Architectures We ﬁrst evaluate whether the sum over training losses (SOTL) obtained over an SGD trajectory correlates with a model’s generalization error, and whether SOTL predicts the weight assigned to a model by a linear ensemble. To do so, we train a linear combination of DNNs with SGD to determine whether SGD upweights NNs that generalize better. Further details of the experiment can be found in Appendix B.2. Our results are summarized in Figure 3. 8Figure 4: Weight assigned to subnetwork by SGD in a deep neural network (x-axis) versus the subnet- work performance (estimated by the sum of cross-entropy, on the y-axis) for different FashionMNIST classes. The light blue ovals denote depict 95% conﬁdence intervals, estimated over 10 seeds (i.e. 2σ for both the weight and SOTL). The orange line depicts the general trend. We observe a strong correlation between SOTL and average test cross-entropy (see Figure 3 middle column), validating that the SOTL is correlated with generalization. Further, we ﬁnd that architectures with lower test error (when trained individually) are given higher weight by the linear ensembling layer – as can be seen from the left plot in Figure 3. This supports our hypothesis that SGD favours models that generalize well. 4.2.2 Subnetwork Selection in Neural Networks Finally, we evaluate whether our previous insights apply to submodels within a neural network, suggesting a potential mechanism which may bias SGD towards parameters with better generalization performance. Based on the previous experiments, we expect that nodes that have a lower sum over training errors (if evaluated as a classiﬁer on their own) are favoured by gradient descent and therefore have a larger ﬁnal weight than those which are less predictive of the data. If so, we can then view SGD followed by pruning (in the ﬁnal linear layer of the network) as performing an approximation of a Bayesian model selection procedure. We replicate the model selection problem of the previous setting, but replace the individual models with the activations of the penultimate layer of a neural network, and replace the linear ensemble with the ﬁnal linear layer of the network. Full details on the experimental set-up can be found in Appendix B.3. We ﬁnd that our hypotheses hold here: SGD assigns larger weights to subnetworks that perform well, as can be seen in Figure 4. This suggests that SGD is biased towards functions that generalize well, even within a network. We ﬁnd the same trend holds for CIFAR-10, which is shown in Appendix B.3. 5 Conclusion In this paper, we have proposed a family of estimators of the marginal likelihood which illustrate the connection between training speed and Bayesian model selection. Because gradient descent can produce exact posterior samples in linear models, our result shows that Bayesian model selection can be done by training a linear model with gradient descent and tracking how quickly it learns. This approach also applies to the inﬁnite-width limit of deep neural networks, whose dynamics resemble those of linear models. We further highlight a connection between magnitude-based pruning and model selection, showing that models for which our lower bound is high will be assigned more weight by an optimal linear model combination. This raises the question of whether similar mechanisms exist in ﬁnitely wide neural networks, which do not behave as linear models. We provide preliminary empirical evidence that the connections shown in linear models have predictive power towards explaining generalization and training dynamics in DNNs, suggesting a promising avenue for future work. 96 Broader Impact Due to the theoretical nature of this paper, we do not foresee any immediate applications (positive or negative) that may arise from our work. However, improvement in our understanding of generalization in deep learning may lead to a host of downstream impacts which we outline brieﬂy here for completeness, noting that the marginal effect of this paper on such broad societal and environmental impacts is likely to be very small. 1. Safety and robustness. Developing a stronger theoretical understanding of generalization will plausibly lead to training procedures which improve the test-set performance of deep neural networks. Improving generalization performance is crucial to ensuring that deep learning systems applied in practice behave as expected based on their training performance. 2. Training efﬁciency and environmental impacts. In principle, obtaining better estimates of model and sub-model performance could lead to more efﬁcient training schemes, thus potentially reducing the carbon footprint of machine learning research. 3. Bias and Fairness. The setting of our paper, like much of the related work on generalization, does not consider out-of-distribution inputs or training under constraints. If the training dataset is biased, then a method which improves the generalization performance of the model under the i.i.d. assumption will be prone to perpetuating this bias. Acknowledgements Lisa Schut was supported by the Accenture Labs and Alan Turing Institute. 10References [1] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019. [2] D. Basu. On statistics independent of a complete sufﬁcient statistic. Sankhy¯a: The Indian Journal of Statistics (1933-1960), 15(4):377–380, 1955. ISSN 00364452. URL http://www. jstor.org/stable/25048259. [3] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018. [4] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017. [5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pages 1613–1622, 2015. [6] Andreas Damianou and Neil Lawrence. Deep gaussian processes. volume 31 of Proceedings of Machine Learning Research, pages 207–215, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR. URL http://proceedings.mlr.press/v31/damianou13a.html. [7] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=H1-nGgWC-. [8] Vincent Dutordoir, Mark van der Wilk, Artem Artemev, and James Hensman. Bayesian image classiﬁcation with deep convolutional gaussian processes. volume 108 of Proceedings of Machine Learning Research, pages 1529–1539, Online, 26–28 Aug 2020. PMLR. URL http://proceedings.mlr.press/v108/dutordoir20a.html. [9] David Duvenaud, Dougal Maclaurin, and Ryan Adams. Early stopping as nonparametric variational inference. In Artiﬁcial Intelligence and Statistics, pages 1070–1077, 2016. [10] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. [11] Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent PAC-Bayes priors via differential privacy. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, NeurIPS 31, pages 8430–8441. 2018. [12] Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019. [13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=rJl-b3RcF7. [14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Ininternational conference on machine learning, pages 1050–1059, 2016. [15] Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. PAC-Bayesian theory meets Bayesian inference. In Advances in Neural Information Processing Systems, pages 1884–1892, 2016. [16] Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors,Advances in Neural Information Process- ing Systems 24, pages 2348–2356. Curran Associates, Inc., 2011. URL http://papers.nips. cc/paper/4329-practical-variational-inference-for-neural-networks.pdf . [17] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent, 2015. [18] Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the neural tangent kernel. arXiv preprint arXiv:2007.05864, 2020. 11[19] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5–13, 1993. [20] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571–8580, 2018. [21] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to ﬁnd them, 2019. [22] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. In Advances in Neural Information Processing Systems, pages 3491–3501, 2019. [23] Mohammad Emtiyaz E Khan, Alexander Immer, Ehsan Abedi, and Maciej Ko- rzepa. Approximate inference turns deep networks into gaussian processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32 , pages 3094– 3104. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/ 8573-approximate-inference-turns-deep-networks-into-gaussian-processes. pdf. [24] S. Kullback and R. A. Leibler. On information and sufﬁciency. Ann. Math. Statist., 22(1): 79–86, 03 1951. doi: 10.1214/aoms/1177729694. URL https://doi.org/10.1214/aoms/ 1177729694. [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems, pages 6402–6413, 2017. [26] Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z. [27] David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992. [28] David JC MacKay.Information theory, inference and learning algorithms. Cambridge university press, 2003. [29] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems, pages 13132–13143, 2019. [30] Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873–4907, 2017. [31] Alexander G de G Matthews, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Sample- then-optimize posterior sampling for bayesian linear models. Neural Information Processing Systems, 2017. [32] David A. McAllester. Some PAC-Bayesian Theorems. Machine Learning, 37(3):355–363, 1999. [33] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 11615–11626. Curran Associates, Inc., 2019. [34] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019. [35] Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy. Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances in Neural Information Processing Systems, pages 11015–11025, 2019. 12[36] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep rein- forcement learning. In Advances in Neural Information Processing Systems, pages 8617–8629, 2018. [37] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177–1184, 2008. [38] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine Learning, pages 63–71. Springer, 2003. [39] Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. In Advances in neural information processing systems, pages 294–300, 2001. [40] Binxin Ru, Clare Lyle, Lisa Schut, Mark van der Wilk, and Yarin Gal. Revisiting the train loss: an efﬁcient performance estimator for neural architecture search, 2020. [41] Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2017. [42] Samuel L. Smith and Quoc V . Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=BJij4yg0Z. [43] Guillermo Valle-Pérez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018. [44] M. van der Wilk, M. Bauer, S. John, and J. Hensman. Learning Invariances using the Marginal Likelihood. arXiv e-prints, August 2018. _eprint: 1808.05563. [45] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11) , pages 681–688, 2011. [46] Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. arXiv preprint arXiv:2002.08791, 2020. [47] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. 13A Proofs of Theoretical Results Proposition 3.1. Both ˆLand ˆLk as deﬁned in Equation 4 are estimators of lower bounds on the log marginal likelihood; that is E[ ˆL(D)] = L(D) ≤log P(D) and E[ ˆLk(D)] = Lk(D) ≤log P(D) . (5) Further, the bias term in Lcan be quantiﬁed as follows. L(D) = log P(D) − n∑ i=1 KL(P(θ|D<i)||P(θ|D<i+1)) (6) Proof. The result for Lfollows from a straightforward derivation: L(D) = ∑∫ log P(Di|θ)dP(θ|D<i) (12) = ∑∫ log[P(Di|θ)P(θ|D<i)P(Di|D<i) P(θ|D<i)P(Di|D<i) ]dP(θ|D<i) (13) = ∑∫ log P(θ|D≤i)) P(θ|D<i) dP(θ|D<i) + ∑ log P(Di|D<i) (14) = ∑( log P(Di|D<i) −KL(P(θ|D<i)||P(θ|D≤i)) ) (15) = log P(D) − n∑ i=1 KL(P(θ|D<i)||P(θ|D≤i)). (16) The result for ˆLk follows immediately from Jensen’s inequality, yielding ∑ E[log k∑ j=1 1 kp(Di|θj)] ≤ ∑ log E[ k∑ j=1 1 kp(Di|θj)] = ∑ log E[p(Di|θj)] = log P(D) . (17) Because Lk applies Jensen’s inequality to a random variable with decreasing variance as a function of k, we expect the bias of Lk to decrease as kgrows, an observation characterized in Section 4. Proposition 3.2. Let P(Yi|D<i,Xi) = N(µi,σ2 i) for some µi,σ2 i. Deﬁne the standard mean and variance estimators ˆµi = 1 N ∑N j=1 f(θi j,xi) and ˆσ2 i = 1 N−1 ∑(f(θi j,xi) −ˆµ)2. Then the estimator ˆLS(D) = n∑ i=1 log P(Yi|ˆµi,ˆσ2 i) (7) is a lower bound on the log ML: i.e. E[ ˆLS(D)] ≤log P(D). Proof. To show that the sum of the estimated log likelihoods is a lower bound on the log marginal likelihood, it sufﬁces to show that each term in the sum of the estimates is a lower bound on the corresponding term in log marginal likelihood expression. Thus, without loss of generality we consider a single data point Di = (x,y) and posterior distribution p(y|x,D<i) = N(µ,σ2). Let y∈R, ˆµ,ˆσthe standard estimators for sample mean and variance given sample ˆY ∈Rk sampled from N(µ,σ2). We want to show EˆY∼N(µ,σ2)[ln p(y|ˆµ,ˆσ2)] ≤ln p(y|µ,σ2). (18) We ﬁrst note that ˆµ( ˆY) ⊥ˆσ( ˆY) for ˆY a collection of i.i.d. Gaussian random variables [ 2]. We also take advantage of the fact that the log likelihood of a Gaussian is concave with respect to itsµ parameter and its σ2 parameter. Notably, the log likelihood is not concave w.r.t. the joint pair (µ,σ2), but because the our estimators are independent, this will not be a problem for us. We proceed as follows by ﬁrst decomposing the expectation over the samples ˆY into an expectation over ˆµand ˆσ2 EX∼N(µ,σ2)[ln p(y|ˆµ,ˆσ2)] = Eˆµ,Y2,...,YN ln p(y|ˆµ,ˆσ2) (19) = EˆµEˆσ2 ln p(y|ˆµ,ˆσ2) (20) 14We apply Jensen’s inequality ﬁrst to the inner expectation, then to the outer. ≤Eˆµln p(y|ˆµ,E[ˆσ2]) = Eˆµln p(y|ˆµ,σ2) (21) ≤ln p(y|µ,σ2) (22) So we obtain our lower bound. Theorem 3.3. Let D= (Xi,Yi)n i=1 and let (θi j)n,J i,j=1 be generated by the procedure outlined above. Then the estimators ˆL, ˆLS,and ˆLk, applied to the collection (θi j), are lower bounds on log P(D). Further, expressing −log P(Di|θ) as the ℓ2 regression loss plus a constant, we then obtain log P(D) ≥ n∑ i=1 Eθi∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,θi) + c= L(D) (8) Proof. The heavy lifting for this result has largely been achieved by Propositions 3.1 and 3.2, which state that provided the samples θi j are distributed according to the posterior, the inequalities will hold. It therefore remains only to show that the sample-then-optimize procedure yields samples from the posterior. The proof of this result can be found in Lemma 3.8 of Osband et al. [36], who show that the optimum for the gradient descent procedure described in Algorithm 1 does indeed correspond to the posterior distribution for each subset D<i. Finally, it is straightforward to express the lower bound estimator ˆLas the sum of regression losses. We obtain this result by showing that the inequality holds for each term log P(Di|θi) in the summation. log P(Di|θ) = log[exp ( −(θ⊤xi −yi)2 2σ2 ) 1√ 2πσ] (23) = −(θ⊤xi −yi)2 2σ2 −1 2 log(2πσ2) (24) = c1ℓ2(Di,θ) + c2 (25) We note that in practice, the solutions found by gradient descent for ﬁnite step size and ﬁnite number of steps will not necessarily correspond to the exact local optimum. However, it is straightforward to bound the error obtained from this approximate sampling in terms of the distance of θfrom the optimum θ∗. Denoting the difference |θ−θ∗|by δ, we get |log P(Di|θ∗) −log P(Di|θ)|= |((θ∗)⊤xi −yi)2 2σ2 −((θ)⊤xi −yi)2 2σ2 | (26) ≤ 1 2σ2 |(θ∗)⊤xi −θ⊤xi|2 (27) ≤|((θ∗)⊤xi)2 −(θ⊤xi)2|+ |2y||θ⊤x−(θ∗)⊤x| (28) ≤|(θ∗−θ)⊤x+ 2((θ∗)⊤x)((θ∗−θ)⊤x)|+ |2y||θ⊤x−(θ∗)⊤x| (29) ≤|θ∗−θ||x|+ 2|θ∗x||θ∗−θ||x|+ |2y||x||θ−θ∗| (30) and so the error in the estimate of log P(D|θ) will be proportional to the distance |θ−θ∗|induced by the approximate optimization procedure. Corollary 3.4. Let Dbe a dataset indexed by our standard notation. Let f0 be sampled from an inﬁnitely wide neural network architecture Funder some initialization distribution, and let fi ∞be the limiting solution under the training dynamics deﬁned by He et al. [18] applied to the initialization f0 and using data D<i. Let K∞denote the neural tangent kernel for F, and M= GP(0,K∞) the induced Gaussian Process. Then fi ∞∼P(f|D<i,M), and in the limit of inﬁnite training time, the iterative sample-then-optimize procedure yields an unbiased estimate of L(D|M). Letting ℓ2 denote the scaled squared ℓ2 regression loss and cbe a constant, we obtain as a direct corollary of Theorem 3.3 P(D) ≥Efi∞∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,fi) + c= L(D) . (9) 15Proof. Follows immediately from the results of He et al. [18] stating that the the limiting distribution of fk ∞is precisely P(f|Dn ≤k,M). We therefore obtain the same result as for Theorem 3.3, plugging in the kernel gradient descent procedure on f for the parameter-space gradient descent procedure on θ. The following Lemma will be useful in order to prove Proposition 3.5. Intuitively, this result states that in a linear regression problem in which each feature φi is ‘normalized’ (the dot product ⟨φi,y⟩= ⟨φj,y⟩= αfor some αand all i,j) and ‘independent’ (i.e.⟨Πy⊥φi,Πy⊥φj⟩= 0), then the optimal linear regression solution assigns highest weight to the feature which obtains the least error in predicting yon its own. Lemma A.1. Let y∈Rn, and Φ ∈Rd×d be a design matrix such that Φ[:,j] = αy+ ϵj∀jfor some ﬁxed α ≥0, with ϵ ∈y⊥, and ϵ⊤ i ϵj = 0 for all i ̸= j. Let w∗be the solution to the least squares regression problem on Φ and y. Then min i wi = min i ∥fi(x) −y∥2 = max i L(Mi) (31) Proof. We express the minimization problem as follows. We letφ(x) = (f1(x),...,f k(x)), where fi(x) = αy+ ϵi, with ϵi ⊥ϵj. We denote by 1 the vector containing all ones (of length k). We observe that we can decompose the design matrixΦ into one component whose columns are parallel to y, denoted Φy, and one component whose columns are orthogonal to y, denoted Φ⊥. Let σ2 i = ∥ϵi∥2. By assumption, Φy = αy1 ⊤, and Φ⊤ ⊥Φ⊥= diag(σ2 1,...,σ 2 n) = Σ. We then observe the following decomposition of the squared error loss of a weight vector w, denoted ℓ(w). ℓ(w) = ∥Φw−y∥2 = (Φw−y)⊤(Φw−y) = ((Φy + Φ⊥)w−y)⊤((Φy + Φ⊥)w−y) = (Φyw−y)⊤(Φyw−y) + w⊤Φ⊤ ⊥Φ⊥w = ∥y∥2∥1 −α1 ⊤w∥2 + ∑ σ2 iwi In particular, the loss decomposes into a term which depends on the sum of the wi, and another term which will depend on the norm of the component of each model’s predictions orthogonal to the targets y. As this is a quadratic optimization problem, it is clear that an optimal wexists, and so w⊤1 will take some ﬁnite value, say β. We will show that for any ﬁxed β, the solution to the minimization problem min w ∑ wiσ2 i : w⊤1 = β (32) is such that the argmax over i of wi is equal to that of the minimum variance. This follows by applying the method of Lagrange multipliers to obtain that the optimal wsatisﬁes w∗ i = α∑σ−2 i 1 σ2 i . (33) In particular, w∗ i is inversely proportional to the variance of fi, and so is maximized for i = arg miniE∥fi(x) −y∥2. Proposition 3.5. Let M1,..., Mk be Bayesian linear regression models with ﬁxed noise variance σ2 N and Gaussian likelihoods. Let Φ be a (random) matrix of posterior prediction samples, of the form Φ[i,j] = ˆyj i ∼P(yj|D<j,xj,Mi). Suppose the following two conditions on the columns of Φ are satisﬁed: E⟨Φ[:,i],y⟩= E⟨Φ[:,j],y⟩for all i,j, and E⟨Πy⊥φi,Πy⊥φj⟩= 0. Let w∗denote the least-squares solution to the regression problem minwEΦ∥Φw−y∥2. Then the following holds arg max i w∗ i = arg max i L(D|Mi) ∀w∗= arg min w E∥Φw−y∥2 . (11) 16Proof. We ﬁrst clarify the independence assumptions as they pertain to the assumptions of the previous lemma: writing Φ[:,i] as fi(x) + ζi = αy+ ϵi + ζi with ζi ∼N (0,Σi) corresponding to the noise from the posterior distribution and fi its mean, the ﬁrst independence assumption is equivalent to the requirement that fi = α′y+ ϵi with ϵi ⊥yfor all i. The second independence assumption is an intuitive expression of the constraint that ϵi ⊥ϵj in the linear-algebraic sense of independence, and that ζj i is sampled independently (in the probabilistic sense) for all iand j. We note that our lower bound for each model in the linear regression setting is equal to E∑N i=1 ∥fk(xi) + ζi −yi∥2 + c where c is a ﬁxed normalizing constant. By the previous Lemma, we know that the linear regression solution w∗ based on the posterior means satisﬁes, maxiw∗ i = maxiL(Mi). It is then straightforward to extend this result to the noisy setting. E[∥Φw−y∥2] = E[∥(Φy + Φ⊥+ ζ)w−y∥2] (34) = E[((Φy + Φ⊥+ ζ)w−y)⊤((Φy + Φ⊥+ ζ)w−y)] (35) = ∥Φyw−y∥2 + w⊤Φ⊤ ⊥Φ⊥w+ E[w⊤ζ⊤ζw] (36) = (w⊤1 −α)2∥y∥2 + w⊤Φ⊤ ⊥Φ⊥w+ E[w⊤ζ⊤ζw] (37) = (w⊤1 −α)2∥y∥2 + ∑ w2 i(∥Φ⊥[:,i]∥2 + ∥ζi∥2) (38) We again note via the same reasoning as in the previous Lemma that the model with the greatest lower bound will be the one which minimizes ∥Φ⊥[:,i]∥2 + ∥ζi∥2, and that the weight given to index iwill be inversely proportional to this term. It only remains to show that for each model i, the model which maximizes L(Mi) will also minimize ∥Φ⊥[:,i]∥2 + ∥ζi∥2. This follows precisely from the Gaussian likelihood assumption. As we showed previously L(D|Mi) = E[ ∑ log P(yi|D<i)] ∝− ∑ E[ℓ2(yi −ˆyi] (39) = [∥y−µ∥2 + E[∥ˆy−µ∥2] (40) = α∥y∥2 + ∥Φ⊥[:,i]∥2 + E[∥ζi∥2] (41) and so ﬁnding the model Mi which maximizes L(D,Mi) is equivalent to picking the maximal index iof w∗which optimizes the expected loss of the least squares regression problem. 17Figure 5: Relative rankings given by optimize-then-prune, ML, and estimated L(D). Left: feature selection. Middle: prior variance selection. Right: RFF frequency selection. Rankings are consistent with what our theoretical results predict. Results are averaged over 5 runs. B Experiments B.1 Experimental details: Model Selection using Trajectory Statistics We consider 3 model selection settings in which to evaluate the practical performance of our estimators. In prior variance selection we evaluate a set of BLR models on a synthetic linear regression data set. Each model Mi has a prior distribution over the dparameters of the form w∼N(0,σ2 iId) for some σ2 i, and the goal is to select the optimal prior variance (in other words, the optimal regularization coefﬁcient). We additionally evaluate an analogous initialization variance selection method on an NTK network trained on a toy regression dataset. In frequency (lengthscale) selection we use as input a subset of the handwritten digits dataset MNIST given by all inputs labeled with a 0 or a 1. We compute random Fourier features (RFF) of the input to obtain the features for a Bayesian linear regression model, and perform model selection over the frequency of the features (full details on this in the appendix). This is equivalent to obtaining the lengthscale of an approximate radial basis function kernel. In feature dimension selection, we use a synthetic dataset [46] of the form (X,y), where xi = (yi + ϵ1,yi + ...,y i + ϵ15,ϵ16,...,ϵ 30). We then consider a set of models {Mk}with feature embeddings φk(xi) = xi[1,...,k ]. The optimal model in this setting is the one which uses exactly the set of ‘informative’ featuresx[1,..., 15]. The synthetic data simulation used in this experiment is identical to that used in [ 46]. Below, we provide the details. Let k be the number of informative features and d the total number of features. We generate a datapoint Di = {xi,yi}as follows: 1. Sample yi: yi ∼U([0,1]) 2. Sample kinformative features: xi,j ∼N(yi,σ0) ∀j ∈1,...k 3. Sample max(d−k,0) noise features: xi,k+j ∼N(0,σ1) ∀j ∈1,...d −k 4. Concatenate the features: Xi = [xi,1,...x i,d] We set σ0 = σ1 = 1, k= 15, n= 30, and let dvary from 5 to n. We then run our estimators on the Bayesian linear regression problem for each feature dimension, and ﬁnd that all estimators agree on the optimal number of features, k. To compute the random fourier features used for MNIST classiﬁcation, we vectorize the MNIST input images and follow the procedure outlined by [37] (Algorithm 1) to produce RFF features, which are then used for standard Bayesian linear regression against the binarized labels. The frequency parameter (which can also be interpreted as a transformation of the lengthscale of the RBF kernel approximated by the RFF model) is the parameter of interest for model selection. Results can be found in Figure 5. We additionally provide an analogue to our evaluation of model selection in NTK-GPs, with the change in the log marginal likelihood plotted instead of L(D). We obtain analogous results, as can be seen in Figure 6. 18Figure 6: Evaluation of change in log ML after data point iis added for NTK-GPs on a random subset of MNIST. B.2 Experimental details: Bayesian model comparison Here we provide further detail of the experiment in Section 4.2.1. The goal of the experiment is to determine whether the connection between sum-over-training losses (SOTL) and model evidence observed in the linear regression setting extends to DNNs. In particular, the two sub-questions are: 1. Do models with a lower SOTL generalize better? 2. Are these models favoured by SGD? To answer these questions, we train a linear combination of NNs. We can answer subquestion [1] by plotting the correlation between SOTL and test performance of an individual model. Further, we address subquestion [2] by considering the correlation between test loss and linear weights assigned to each model. Below we explain the set-up of the linear combination in more detail. We train a variety of deep neural networks along with a linear ‘ensemble’ layer that performs a linear transformation of the concatenated logit outputs3 of the classiﬁcation models. Let hm(xi) be logit output of model mfor input xi, ℓ(yi,hi) be the loss for point i(where hi is a logit) and wm,t be the weight corresponding to model mat time step t. We consider two training strategies: we ﬁrst train models individually using the cross-entropy loss between each model’s prediction and the true label, only cross-entropy loss of the ﬁnal ensemble prediction to train the linear weights. Mathematically, we update the models using the gradients δ δθm ℓ(yi,hm(xi)), (42) and the ‘ensemble’ weights using δ δwm ℓ(yi, ∑ m wmhm(xi)). (43) We refer to this training scheme as Parallel Trainingas the models are trained in parallel. We also consider the setting in which the models are trained using the cross entropy loss from the ensemble prediction backpropagated through the linear ensemble layer, i.e. the model parameters are now updated using: δ δθm ℓ(yi, ∑ m wmhm(xi)). (44) 3These are pre-softmax outputs. To obtain the predicted probability of a class, they are fed through a softmax function. 19We refer to this scheme as the Concurrent Training. We train a variety of different MLPs (with varying layers,and nodes) and convolutional neural networks (with varying layers, nodes and kernels) on FashionMNIST using SGD until convergence. B.3 Experimental Details: SGD upweights submodels that perform well Below we provide further details of the experiment in Section 4.2.2. The goal of the experiment is to determine whether SGD upweights sub-models that ﬁt the data better. We train a MLP network (with units 200,200,10) on FashionMMIST using SGD until convergence. After training is completed, for every class of y, we rank all nodes in the penultimate layer by the norm of their absolute weight (in the ﬁnal dense layer). We group the points into submodels according to their ranking – the knodes with the highest weights are grouped together, next the k+ 1,... 2k ranked nodes are grouped, etc. We set k= 10. We determine the performance of a submodels by training a simple logistic classiﬁer to predict the class of an input, based on the output of the submodel. To measure the performance of the classiﬁer, we use the cross-entropy loss. To capture the equivalent notion of the AUC, we estimate the performance of the sub-models throughout training, and sum over the estimated cross-entropy losses. Below, we show additional plots for theparallel and concurrent training schemes. The results are the same to those presented in the main text, and we observe [1] a negative correlation between test performance and ensemble weights and [2] a strong correlation between SOTL and average test cross-entropy. Figure 7: Linear combinations of DNNs on FashionMNIST. Left: ensemble weights versus the test loss for parallel training; we observe a negative correlation. Middle: SOTL (standardized by the number of training samples) versus test loss for concurrent and concurrent training. We observe a strong correlation indicating that the SOTL generalizes well. Right: training curves for the different models in concurrent training schemes. All results are averaged over 10 runs, and standard deviations are shown by the shaded regions around each observation. The model parameters, given in the parentheses, are the number of layers (l), nodes per layer (n) and kernel size (k), respectively. However, similarly to the linear setting, the difference in assigned weights is magniﬁed in the concurrent training scheme. Here we ﬁnd that in the concurrent training scheme, the ensemble focuses on training the CNNs (as can be seen from the training curve in Figure 3 in the main text). This is likely because CNNs are able to learn more easily, leading to larger weights earlier on. 20Figure 8: Weight assigned to subnetwork by SGD in a deep neural network (x-axis) versus the subnet- work performance (estimated by the sum of cross-entropy, on the y-axis) for different FashionMNIST classes. The light blue ovals denote depict 95% conﬁdence intervals, estimated over 10 seeds (i.e. 2σ for both the weight and SOTL). The orange line depicts the general trend. Figure 9: Weight assigned to subnetwork by SGD in a deep neural network (x-axis) versus the subnetwork performance (estimated by the sum of cross-entropy, on the y-axis) for different CIFAR- 10 classes. The light blue ovals denote depict 95% conﬁdence intervals, estimated over 10 seeds (i.e. 2σfor both the weight and SOTL). The orange line depicts the general trend. Above, we show additional plots to those shown in Figure 4, Section 4.2.2. Figure 8 shows the results for the all FashionMNIST classes, and Figure 9 shows the results for experiment on CIFAR-10. From both, we see that SGD assigns higher weights to subnetworks that perform better. 21
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "This paper establishes a connection between a model's training speed and its marginal likelihood, offering a new method for model selection and insights into generalization. It shows that a measure of training speed can estimate marginal likelihood and predict model weightings in linear combinations. The core contribution is a family of marginal likelihood estimators derived from sums of predictive log likelihoods (interpreted as training losses) from an iterative Bayesian updating procedure. For linear models and infinite-width neural networks, where gradient descent provides exact posterior samples, these estimators can be computed directly from optimization trajectories. Empirical evidence suggests this intuition extends to deep neural networks (DNNs) trained with stochastic gradient descent (SGD), indicating a promising direction for explaining why SGD-trained neural networks generalize well. The study also highlights a link between magnitude-based pruning in linear model combinations and approximate Bayesian model selection.",
    "methodology": "The methodology revolves around a Bayesian perspective on marginal likelihood (ML) estimation. The log ML is re-expressed as the sum of log posterior predictive probabilities of data points, conditioned on preceding data (sum of 'training losses' over a Bayesian updating curve). Three estimators for lower bounds on the log ML are proposed: L(D) (based on expected log likelihoods), Lk(D) (a tighter bound using k posterior samples), and LS(D) (for Gaussian posteriors, using estimated mean and variance). For linear models and infinite-width neural networks (approximated by Neural Tangent Kernel GPs), gradient descent is leveraged to produce exact posterior samples iteratively, allowing these estimators to be computed from optimization trajectories. The paper also analyzes linear model combinations, showing that the optimal weights in such combinations align with the proposed ML estimators under certain conditions. For DNNs, the methodology involves evaluating the 'sum over training losses' (SOTL) obtained during SGD, correlating it with generalization error and ensemble weights in linear combinations of DNNs, and investigating subnetwork selection within a single DNN.",
    "experimental_setup": "The experimental evaluation covered linear models, infinite-width neural networks (NTK-GPs), and deep neural networks (DNNs) trained with SGD. For linear models, synthetic datasets were used for three model selection tasks: feature dimension selection (identifying informative features), prior variance selection (optimal regularization), and frequency (lengthscale) selection for Random Fourier Features (RFFs) on binarized MNIST. Validation involved comparing rankings from true ML, the proposed estimators (L, Lk, LS), and weights from optimized linear regressors. For NTK-GPs, fully-connected and convolutional architectures were tested on MNIST, visualizing the incremental change in the L(D) estimator. For DNNs, experiments were conducted on FashionMNIST and CIFAR-10 datasets using various MLP and CNN architectures. This included training linear combinations of DNNs with SGD (parallel and concurrent training schemes) to analyze the correlation between SOTL and test loss, and between individual test loss and ensemble weights. Additionally, subnetwork selection within a single MLP was explored by relating the final weights of penultimate layer nodes to their performance (SOTL of cross-entropy). All DNN experiments averaged results over multiple runs (10 seeds).",
    "limitations": "The primary theoretical guarantees for the proposed marginal likelihood estimators and the connection to training speed hold strictly for linear models and the infinite-width limit of neural networks. For finite-width, non-linear deep neural networks trained with SGD, the results are presented as empirical evidence and conjectures, as SGD lacks the exact posterior sampling interpretation of the algorithm. The LS estimator is specifically limited to models with Gaussian posterior predictive distributions. Furthermore, the theoretical result connecting linear model combination weights to marginal likelihood (Proposition 3.5) relies on strong independence assumptions on model errors which are only approximately satisfied in practice. The paper also acknowledges that, due to its theoretical nature and focus on i.i.d. assumptions, it does not immediately address issues like out-of-distribution inputs or inherent biases in training datasets.",
    "future_research_directions": "The paper suggests several promising avenues for future research. A key direction is to further formalize and explain the empirical generalization performance of deep neural networks, particularly the bias of SGD towards functions that generalize well, by extending the intuition developed in linear models and infinite-width settings to non-linear, finite-width DNNs. This includes investigating whether the analogue of the sum of training losses can reliably predict generalization error in SGD-trained models. Another direction is to explore if similar mechanisms (where more predictive components receive higher weights) occur within the internal layers of a neural network, beyond the final linear layer. More broadly, the work aims to pave the way for new practical model selection procedures that integrate closely with standard optimization schemes. Improved understanding of generalization could also lead to more efficient training methods (reducing environmental impact) and enhance the safety and robustness of deep learning systems."
}
