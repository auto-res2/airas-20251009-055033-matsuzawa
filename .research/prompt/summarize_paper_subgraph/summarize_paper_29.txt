
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Optimistic Meta-Gradients Sebastian Flennerhag DeepMind flennerhag@google.com Tom Zahavy DeepMind Brendan O’Donoghue DeepMind Hado van Hasselt DeepMind András György DeepMind Satinder Singh DeepMind Abstract We study the connection between gradient-based meta-learning and convex op- timisation. We observe that gradient descent with momentum is a special case of meta-gradients, and building on recent results in optimisation, we prove con- vergence rates for meta-learning in the single task setting. While a meta-learned update rule can yield faster convergence up to constant factor, it is not sufﬁcient for acceleration. Instead, some form of optimism is required. We show that op- timism in meta-learning can be captured through Bootstrapped Meta-Gradients [Flennerhag et al., 2022], providing deeper insight into its underlying mechanics. 1 Introduction 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training Steps 1e6 50 55 60 65 70 75Top-1 Test Accuracy (%) SGD Standard meta-learning Optimistic meta-learning Figure 1: ImageNet. We compare training a 50- layer ResNet using SGD against variants that tune an element-wise learning rate online using standard meta-learning or optimistic meta-learning. Shad- ing depicts 95% conﬁdence intervals over 3 seeds. In meta-learning, a learner is using a param- eterised algorithm to adapt to a given task. The parameters of the algorithm are then meta-learned by evaluating the learner’s result- ing performance [Schmidhuber, 1987, Hinton and Plaut, 1987, Bengio et al., 1991]. This paradigm has garnered wide empirical success [Hospedales et al., 2020]. For instance, it has been used to meta-learn how to explore in re- inforcement learning (RL) [Xu et al., 2018a, Alet et al., 2020], online hyper-parameter tun- ing of non-convex loss functions [Bengio, 2000, Maclaurin et al., 2015, Xu et al., 2018b, Zahavy et al., 2020], discovering black-box loss func- tions [Chen et al., 2016, Kirsch et al., 2019, Xu et al., 2020, Oh et al., 2020], black-box learning algorithms [Hochreiter et al., 2001, Wang et al., 2016], or entire training protocols [Real et al., 2020]. Yet, very little is known in terms of the theoretical properties of meta-learning. The reason for this is the complex interac- tion between the learner and the meta-learner. learner’s problemis to minimize the expected loss f of a stochastic objective by adapting its parametersx∈Rn. The learner has an update rule ϕat its disposal that generates new parameters xt = xt−1 + ϕ(xt−1,wt); we suppress data dependence to simplify notation. A simple example is when ϕrepresents gradient descent with wt = ηits step size, that is ϕ(xt−1,η) = −η∇f(xt−1) [Mahmood et al., 2012, van Erven and Koolen, 2016]; several works have explored meta-learning other aspects of a gradient-based update rule [Finn et al., 2017, Nichol et al., 2018, Flennerhag et al., 2019, Xu et al., 2018b, Zahavy et al., 2020, Flennerhag et al., arXiv:2301.03236v1  [cs.LG]  9 Jan 20232022, Kirsch et al., 2019, Oh et al., 2020]. More generally, ϕneed not be limited to the gradient of any function, for instance, it can represent some algorithm implemented within a Recurrent Neural Network [Schmidhuber, 1987, Hochreiter et al., 2001, Andrychowicz et al., 2016, Wang et al., 2016]. The meta-learner’s problemis to optimise the meta-parameters wt to yield effective updates. In a typical (gradient-based) meta-learning setting, it does so by treating xt as a function of w. Let ht, deﬁned by ht(w) = f(xt−1 + ϕ(xt−1,w)), denote the learner’s post-update performance as a function of w. The learner and the meta-learner co-evolve according to xt = xt−1 + ϕ(xt−1,wt), and wt+1 = wt −∇ht(wt) = wt −Dϕ(xt−1,wt)T∇f(xt), where Dϕ(x,w) denotes the Jacobian of ϕwith respect to w. The nested structure between these two updates makes it challenging to analyse meta-learning, in particular it depends heavily on the properties of the Jacobian. In practice, ϕis highly complex and so Dϕis almost always intractable. For instance, in Xu et al. [2018a], the meta-parameters deﬁne the data-distribution under which a stochastic gradient is computed. In Zahavy et al. [2020], the meta-parameters deﬁne auxiliary objectives that are meant to help with representation learning; in Vinyals et al. [2016] they learn an embedding space for nearest-neighbour predictions. For this reason, the only theoretical results we are aware of specialise to the multi-task setting and assume ϕrepresents adaptation by gradient descent. In this setting, at each iteration t, the learner must adapt to a new task ft. The learner adapts by taking a (or several) gradient step(s) on ft using either a meta-learned initialisation [Flennerhag et al., 2019, Finn et al., 2019, Fallah et al., 2020, Wang et al., 2022] or using a meta-learned regulariser [Khodak et al., 2019, Denevi et al., 2019]. Because the update rule has this form, it is possible to treat the meta-optimisation problem as an online learning problem and derive convergence guarantees. Acceleration in this setup is driven by the tasks similarity. That is, if all tasks are sufﬁciently similar, a meta-learned update can accelerate convergence [Khodak et al., 2019]. However, these results do not yield acceleration in the absence of a task distribution to the best of our knowledge. This paper provides an alternative view. We study the classical convex optimisation setting of approximating the minimiser minxf(x). We observe that setting the update rule equal to the gradient, i.e. ϕ: (x,w) ↦→w∇f(x), recovers gradient descent. Similarly, we show in Section 3 that ϕcan be chosen to recover gradient descent with momentum. This offers another view of meta-learning as a non-linear transformation of classical optimisation. A direct implication of this is that a task similarity is not necessary condition for improving the rate of convergence via meta-learning. While there is ample empirical evidence to that effect [Xu et al., 2018b, Zahavy et al., 2020, Flennerhag et al., 2022, Luketina et al., 2022], we are only aware of theoretical results in the special case of meta-learned step sizes [Mahmood et al., 2012, van Erven and Koolen, 2016]. In particular, we analyse meta-learning using recent techniques developed for convex optimisation [Cutkosky, 2019, Joulani et al., 2020, Wang et al., 2021]. Given a function f that is convex with Lipschitz smooth gradients, meta-learning improves the rate of convergence by a multiplicative factor λto O(λ/T), via the smoothness of the update rule. Importantly, these works show that to achieve accelerated convergence, O(1/T2), some form of optimism is required. This optimism essentially provides a prediction of the next gradient, and hence represents a model of the geometry. We consider optimism with meta-learning in the convex setting and prove accelerated rates of convergence, O(λ/T2). Again, meta-learning affects these bounds by a multiplicative factor. We further show that optimism in meta-learning can be expressed through the recently proposed Bootstrapped Meta- Gradient method [BMG; Flennerhag et al., 2022]. Our analysis provides a ﬁrst proof of convergence for BMG and highlights the underlying mechanics that enable faster learning with BMG. Our main contributions are as follows: 1. We show that meta-learning contains gradient descent with momentum (Heavy Ball [Polyak, 1964]; Section 3) and Nesterov Acceleration [Nesterov, 1983] as special cases (Section 6). 2. We show that gradient-based meta-learning can be understood as a non-linear transformation of an underlying optimisation method (Section 3). 3. We establish rates of convergence for meta-learning in the convex setting (Sections 5 and 6). 4. We show that optimism can be expressed through [Flennerhag et al., 2022]. Our analysis (Section 6) provides a ﬁrst proof of convergence for BMG. 2Algorithm 1: Meta-learning in practice. input :Weights {βt}T t=1 input :Update rule ϕ input :Initialisation (x0,w1) for t= 1,2,...,T : xt = xt−1 + ϕ(xt−1,wt) ht(·) = f(xt−1 + ρtϕ(xt−1,·)) wt+1 = wt −βt∇ht(wt) return xT Algorithm 2: Meta-learning in the convex setting. input :Weights {αt}T t=1,{βt}T t=1 input :Update rule ϕ input :Initialisation (¯x0,w1) for t= 1,2,...,T : xt = ϕ(¯xt−1,wt) ¯xt = (1 −αt/α1:t)¯xt−1 + (αt/α1:t)xt gt = Dϕ(¯xt−1,wt)T∇f(¯xt) wt+1=arg minw∈W ∑t s=1αs⟨gs,w⟩+ 1 2βt ∥w∥2 return ¯xT 2 Meta-learning meets convex optimisation Problem deﬁnition. This section deﬁnes the problem studied in this paper and introduces our notation. Let f : X→ R be a proper and convex function. The problem of interest is to approximate the global minimum minx∈Xf(x). We assume a global minimiser exists and is unique, deﬁned by x∗= arg min x∈X f(x). (1) We assume that X⊆ Rn is a closed, convex and non-empty set. f is differentiable and has Lipschitz smooth gradients with respect to a norm ∥·∥ , meaning that there exists L ∈(0,∞) such that ∥∇f(x) −∇f(y)∥∗≤L∥x−y∥for all x,y ∈X, where ∥·∥∗is the dual norm of ∥·∥. We consider the noiseless setting for simplicity; our results carry over to the stochastic setting by replacing the key online-to-batch bound used in our analysis by its stochastic counterpart [Joulani et al., 2020]. Algorithm. Algorithm 1 describes a typical meta-learning algorithm. Unfortunately, at this level of generality, little can be said about the its convergence properties. Instead, we consider a stylized variant of meta-learning, described in Algorithm 2. This model differs in three regards: (a) it relies on moving averages (b) we use a different online learning algorithm for the meta-update, and (c) we make stricter assumptions on the update rule. We describe each component in turn. Let [T] = {1,2,...,T }. We are given weights{αt}T t=1, each αt >0, and an initialisation(¯x0,w1) ∈ X×W . At each time t ∈ [T], an update rule ϕ : X×W → Xgenerates the update xt = ϕ(¯xt−1,wt), where W⊆ Rm is closed, convex, and non-empty. We discuss ϕmomentarily. The algorithm maintains the online average ¯xt = x1:t α1:t = (1 −ρt)¯xt−1 + ρtxt, (2) where x1:t = ∑t s=1 αsxs, α1:t = ∑t s=1 αs, and ρt = αt/α1:t. Our goal is to establish conditions under which {¯xt}T t=1 converges to the minimiser x∗. While this moving average is not always used in practical applications, it is required for accelerated rates in online-to-batch conversion [Wang and Abernethy, 2018, Cutkosky, 2019, Joulani et al., 2020]. Convergence depends on how each wt is chosen. In Algorithm 1, the meta-learner faces a sequence of losses ht : W →R deﬁned by the composition ht(w) = f((1 −ρt)¯xt−1 + ρtϕ(¯xt−1,w)). This makes meta-learning a form of online optimisation [McMahan, 2017]. The meta-updates in Algorithm 1 is an instance of online gradient descent, which we can model as Follow-The-Regularized- Leader (FTRL; reviewed in Section 4). Given some norm ∥·∥ , an initialization w0 and β >0, FTRL sets each wt according to wt+1 = arg min w∈W ( t∑ s=1 αs⟨∇hs(ws),w⟩+ 1 2β∥w∥2 ) . (3) If ∥·∥ is the Euclidean norm, the interior solution to Eq. 3 is given by wt+1 = wt −αtβ∇ht(wt), the meta-update in Algorithm 1. It is straightforward to extend Eq. 3 to account for meta-updates that use AdaGrad-like [Duchi et al., 2011] acceleration by altering the norms [Joulani et al., 2017]. 30 20 40 60 80 100 Iterations 10 28 10 16 10 4 Loss 0 20 40 60 80 100 Iterations 0 20 40 60 80 100 Iterations 0 20 40 60 80 100 Iterations 0 20 40 60 80 100 Iterations Momentum Meta-Momentum AdaGrad Meta-AdaGrad 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 10 22 10 12 10 2 Loss 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate Momentum Meta-Momentum AdaGrad Meta-AdaGrad Figure 2: Convex Quadratic. We generate convex quadratic loss functions with ill-conditioning and compare gradient descent with momentum and AdaGrad to meta-learning variants. Meta-Momentum uses ϕ: (x,w) ↦→w⊙∇f(x) while Meta-AdaGrad uses ϕ: (x,w) ↦→∇f(x)/√w, where division is element-wise. Top: loss per iteration for randomly sampled loss functions. Bottom: cumulative loss (regret) at the end of learning as a function of learning rate; details in Appendix B. Update rule. It is not possible to prove convergence outside of the convex setting, since ϕmay reach a local minimum where it cannot yield better updates, but the updates are not sufﬁcient to converge. Convexity means that each ht must be convex, which requires that ϕis afﬁne in w(but may vary non-linearly in x). We also assume that ϕis smooth with respect to ∥·∥, in the sense that it has bounded norm; for all x∈X and all w∈W we assume that there exists λ∈(0,∞) for which ∥Dϕ(x,w)T∇f(x)∥2 ∗≤λ∥∇f(x)∥2 ∗. These assumptions hold for any smooth update rule up to ﬁrst-order Taylor approximation error. 3 Meta-Gradients in the Convex Setting - An Overview In this section, we provide an informal discussion of our main results (full analysis; Sections 5 and 6). Meta-Gradients without Optimism. The main difference between classical optimisation and meta-learning is the introduction of the update rule ϕ. To see how this acts on optimisation, consider two special cases. If the update rule just return the gradient, ϕ = ∇f, Algorithm 2 is reduced to gradient descent (with averaging). The inductive bias is ﬁxed and does not change with past experience, and so acceleration is not possible—the rate of convergence is O(1/ √ T) [Wang et al., 2021]. The other extreme is an update rule that only depends on the meta-parameters, ϕ(x,w) = w. Here, the meta-learner has ultimate control and selects the next update without constraints. The only relevant inductive bias is contained in w. To see how this inductive bias is formed, suppose ∥·∥ = ∥·∥ 2 so that Eq. 3 yields wt+1 = wt −αtρtβ∇f(¯xt) (assuming an interior solution). Combining this with the moving average in Eq. 2, we may write the learner’s iterates as ¯xt = ¯xt−1 + ˜ρt(¯xt−1 −¯xt−2) −˜βt∇f(¯xt−1), where each ˜ρt = ρt 1−ρt−1 ρt−1 and ˜βt = αtρtβ; setting β = 1/(2L) and each αt = tyields ˜ρt = t−2 t+1 and ˜βt = t/(4(t+ 1)L). Hence, the canonical momentum algorithm, Polyak’s Heavy-Ball method [Polyak, 1964], is obtained as the special case of meta-learning under the update ruleϕ: (x,w) ↦→w. Because Heavy Ball carries momentum from past updates, it can encode a model of the learning dynamics that leads to faster convergence, on the order O(1/T). The implication of this is that the dynamics of meta-learning are fundamentally momentum-based and thus learns an update rule in the same cumulative manner. This manifests theoretically through its convergence guarantees. Theorem 1 (Informal). Set αt = 1 and β = 1 λL. If each xt is generated under Algorithm 2, then for any viable ϕ, f(¯xT) −f(x∗) ≤λLdiam(W) T . We refer the reader to Theorem 3 for a formal statement. Compared to Heavy Ball, meta-learning introduces a constant λthat captures the smoothness of the update rule. Hence, while meta-learning does not achieve better scaling inT through ϕ, it can improve upon classical optimisation by a constant factor if λ< 1. That meta-learning can improve upon momentum is borne out experimentally. In Figure 2, we consider the problem of minimizing a convex quadratic f : x ↦→⟨x,Qx⟩, where Q ∈Rn×n is PSD but ill-conditioned. We compare momentum to a meta-learned step-size, i.e. ϕ: (x,w) ↦→w⊙∇f(x), where ⊙is the Hadamard product. Across randomly sampled Qmatrices 4(details: Appendix B), we ﬁnd that introducing a non-linearityϕleads to a sizeable improvement in the rate of convergence. We also compare AdaGrad to a meta-learned version,ϕ: (x,w) ↦→∇f(x)/√w, where division is element-wise. While AdaGrad is a stronger baseline on account of being parameter- free, we ﬁnd that meta-learning the scale vector consistently leads to faster convergence. Meta-Gradients with Optimism. It is well known that minimizing a smooth convex function admits convergence rates of O(1/T2). Our analysis of standard meta-gradients does not achieve such acceleration. Previous work indicate that we should not expect to either; to achieve the theoretical lower-limit of O(1/T2), some form of optimism (reviewed in Section 4) is required. A typical form of optimism is to predict the next gradient. This is how Nesterov Acceleration operates [Nesterov, 1983] and is the reason for its O(1/T2) convergence guarantee. From our perspective, meta-learning is a non-linear transformation of the iterate x. Hence, we should expect optimism to play a similarly crucial role. Formally, optimism comes in the form of hint functions {˜gt}T t=1, each ˜gt ∈Rm, that are revealed to the meta-learner prior to selecting wt+1. These hints give rise to Optimistic Meta-Learning (OML) via meta-updates wt+1 = arg min w∈W ( αt+1˜gt+1 + t∑ s=1 αs⟨∇hs(ws),w⟩+ 1 2βt ∥w∥2 ) . (4) If the hints are accurate, meta-learning with optimism can achieve an accelerated rate of O(˜λ/T2), where ˜λis a constant that characterises the smoothness of ϕ, akin to λ. Again, we ﬁnd that meta- learning behaves as a non-linear transformation of classical optimism and its rate of convergence is governed by the geometry it induces. We summarise this result in the following result. Theorem 2 (Informal). Let each hint be given by ˜gt+1 = Dϕ(¯xt−1,wt)T∇f(¯xt). Assume that ϕis sufﬁciently smooth. Set αt = tand βt = t−1 2t˜λL, then f(¯xT) −f(x∗) ≤4˜λLdiam(W) T2−1 . For a formal statement, see Theorem 4. These predictions hold empirically in a non-convex setting. We train a 50-layer ResNet using either SGD with a ﬁxed learning rate, or an update rule that adapts a per-parameter learning rate online,ϕ: (x,w) ↦→w⊙∇f(x). We compare the standard meta-learning approach without optimism to optimistic meta-learning. Figure 1 shows that optimism is critical for meta-learning to achieve acceleration, as predicted by theory (experiment details in Appendix C). 4 Analysis preliminaries: Online Convex Optimisation In this section, we present analytical tools from the optimisation literature that we build upon. In a standard optimisation setting, there is no update rule ϕ; instead, the iterates xt are generated by a gradient-based algorithm, akin to Eq. 3. In particular, our setting reduces to standard optimisation if ϕis deﬁned by ϕ: (x,w) ↦→w, in which case xt = wt. A common approach to analysis is to treat the iterates x1,x2,... as generated by an online learning algorithm over online losses, obtain a regret guarantee for the sequence, and use online-to-batch conversion to obtain a rate of convergence. Online Optimisation. In online convex optimisation [Zinkevich, 2003], a learner is given a convex decision set Uand faces a sequence of convex loss functions {αtft}T t=1. At each time t ∈[T], it must make a prediction ut prior to observing αtft, after which it incurs a loss αtft(ut) and receives a signal—either αtft itself or a (sub-)gradient of αtft(ut). The learner’s goal is to minimiseregret, R(T) := ∑T t=1 αt(ft(ut) −ft(u)), against a comparator u∈U. An important property of a convex function f is f(u′) −f(u) ≤⟨∇f(u′),u′−u⟩. Hence, the regret is largest under linear losses:∑T t=1 αt(ft(ut) −ft(u)) ≤∑T t=1 αt⟨∇ft(ut),ut −u⟩. For this reason, it is sufﬁcient to consider regret under linear loss functions. An algorithm has sublinear regret if limT→∞R(T)/T = 0. FTRL & AO-FTRL. The meta-update in Eq. 3 is an instance of Follow-The-Regularised-Leader (FTRL) under linear losses. In Section 6, we show that BMG is an instance of the Adaptive-Optimistic FTRL (AO-FTRL), which is an extension due to [Rakhlin and Sridharan, 2013, Mohri and Yang, 2016, Joulani et al., 2020, Wang et al., 2021]. In AO-FTRL, we have a strongly convex regulariser ∥·∥2. FTRL and AO-FTRL sets the ﬁrst prediction u1 to minimise ∥·∥2. Given linear losses {gs}t−1 s=1 and learning rates {βt}T t=1, each βt > 0, the algorithm proceeds according to ut = arg min u∈U ( αt⟨˜gt,u⟩+ t−1∑ s=1 αs⟨gs,u⟩+ 1 2βt ∥u∥2 ) , (5) 5where each ˜gt is a “hint” that enables optimistic learning [Rakhlin and Sridharan, 2013, Mohri and Yang, 2016]; setting ˜gt = 0 recovers the original FTRL algorithm. The goal of a hint is to predict the next loss vector gt; if the predictions are accurate AO-FTRL can achieve lower regret than its non-optimistic counter-part. Since ∥·∥2 is strongly convex, FTRL is well deﬁned in the sense that the minimiser exists, is unique and ﬁnite [McMahan, 2017]. The regret of FTRL and AO-FTRL against any comparator u ∈U can be upper-bounded by R(T) = T∑ t=1 αt⟨gt,ut −u⟩≤ ∥u∥2 2βT + 1 2 T∑ t=1 α2 tβt∥gt −˜gt∥2 ∗. (6) Hence, hints that predict gt well can reduce the regret substantially. Without hints, FTRL can guarantee O( √ T) regret (for non strongly convex loss functions). However, Dekel et al. [2017] show that under linear losses, if hints are weakly positively correlated—deﬁned as ⟨gt,˜gt⟩≥ ϵ∥gt∥2 for some ϵ >0—then the regret guarantee improves to O(log T), even for non strongly-convex loss functions. We believe optimism provides an exciting opportunity for novel forms of meta-learning. Finally, we note that these regret bounds (and hence our analysis) can be extended to stochastic optimisation [Mohri and Yang, 2016, Joulani et al., 2017]. Online-to-batch conversion. The main idea behind online to batch conversion is that, forfconvex, Jensen’s inequality givesf(¯xT)−f(x∗) ≤∑T t=1 αt⟨∇f(xt),xt−x∗⟩/α1:T. Hence, one can provide a convergence rate by ﬁrst establishing the regret of the algorithm that generates xt, from which one obtains the convergence rate of the moving average of iterates. Applying this naively yields O(1/T) rate of convergence. In recent work, Cutkosky [2019] shows that one can upper-bound the sub-optimality gap by instead querying the gradient gradient at the average iterate, f(¯xT) −f(x∗) ≤∑T t=1 αt⟨∇f(¯xt),xt −x∗⟩/α1:T, which can yield faster rates of convergence. Recently, Joulani et al. [2020] tightened the analysis and proved that the sub-optimality gap can be bounded by f(¯xT) −f(x∗) ≤ 1 α1:T ( Rx(T) −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) , (7) were we deﬁne Rx(T) := ∑T t=1 αt⟨∇f(¯xt),xt −x∗⟩as the regret of the sequence {xt}T t=1 against the comparator x∗. With this machinery in place, we now turn to deriving our main results. 5 Analysis Our analytical goal is to apply the online-to-batch conversion bound in Eq. 7 to the iterates x1,x2,...,x T that Algorithm 2 generates. Our main challenge is that the update rule ϕprevents a straightforward application of this bound. Instead, we must upper bound the learner’s regret Rx by the meta-learner’s regret, which is deﬁned in terms of the iterates w1,w2,...,w T. To this end, we may decompose Rx as follows: Rx(T) = T∑ t=1 αt⟨∇f(¯xt),xt −x∗⟩= T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −x∗⟩ = T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩+ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩. The ﬁrst term in the ﬁnal expression can be understood as the regret under convex losses ℓt(·) = αt⟨∇f(¯xt),ϕ(¯xt−1,·)⟩. Since ϕ(¯xt−1,·) is afﬁne, ℓt is convex and can be upper bounded by its linearisation. The linearisation reads ⟨Dϕ(¯xt−1,wt)T∇f(¯xt),·⟩, which is identical the linear losses ⟨∇ht(wt),·⟩faced by the meta-learner in Eq. 3. Hence, we may upper bound Rx(T) by Rx(T) ≤ T∑ t=1 αt⟨Dϕ(¯xt−1,wt)T∇f(¯xt),wt −w∗⟩+ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩ = T∑ t=1 αt⟨∇ht(wt),wt −w∗⟩+ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩ = Rw(T) + T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩, (8) 6where the last identity follows by deﬁnition: Rw(T) := ∑T t=1 αt⟨∇ht(wt),wt −w∗⟩. For the last term in Eq. 8 to be negative, so that Rw(T) ≥Rx(T), we need the relative power of the comparator w∗to be greater than that of the comparator x∗. Intuitively, the comparator x∗is non-adaptive. It must make one choice x∗and suffer the average loss. In contrast, the comparator w∗becomes adaptive under the update rule; it can only choose one w∗, but on each round it plays ϕ(¯xt−1,w∗). If ϕ is sufﬁciently ﬂexible, this gives the comparator w∗more power than x∗, and hence it can force the meta-learner to suffer greater regret. When this is the case, we say that regret is preserved when moving from x∗ to w∗. Deﬁnition 1. Given f, {αt}T t=1, and {xt}T t=1, an update rule ϕ: X×W→X preserves regret if there exists a comparator w∈W that satisﬁes T∑ t=1 αt⟨ϕ(¯xt−1,w),∇f(¯xt)⟩≤ T∑ t=1 αt⟨x∗,∇f(¯xt)⟩. (9) If such wexists, let w∗denote the comparator with smallest norm ∥w∥. By inspecting Eq. 9, we see that if ϕ(¯xt−1,·) can be made to negatively align with the gradient ∇f(¯xt), the update rule preserves regret. Hence, any update rule that is gradient-like in its behaviour can be made to preserve regret. However, this must not hold on every step, only sufﬁciently often; nor does it imply that the update rule must explicitly invoke∇f; for instance, update rules that are afﬁne in wpreserve regret if the diameter of Wis sufﬁciently large, provided the update rule is not degenerate. Lemma 1. Given f, {αt}T t=1, and {xt}T t=1, if ϕpreserves regret, then Rx(T) = T∑ t=1 αt⟨∇f(¯xt),xt −x∗⟩≤ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩= Rw(T). Proof: Appendix D. With Lemma 1, we can provide a convergence guarantee for meta-gradients in the convex setting. The mechanics of the proof is to use online-to-batch conversion to upper bound f(¯xT)−f(x∗) ≤Rx(T)/α1:T and then appeal to Lemma 1 to obtainf(¯xT)−f(x∗) ≤Rw(T)/α1:T, from which point we can plug in the FTRL regret bound. Theorem 3. Let ϕpreserve regret and assume Algorithm 2 satisﬁes the assumptions in Section 2. Then f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 β + T∑ t=1 λβα2 t 2 ∥∇f(¯xt)∥2 ∗ −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . Moreover, ifx∗is a global minimiser of f, setting αt = 1 and β = 1 λL yields f(¯xT) −f(x∗) ≤λLdiam(W) T . Proof: Appendix D. 6 Meta-Learning meets Optimism The reason Theorem 3 fails to achieve acceleration is because the negative terms,−∥∇f(¯xt−1) − ∇f(¯xt)∥2 ∗, do not come into play. This is because the positive term in the bound involves the norm of the gradient, rather than the norm of the difference of two gradients. The former is typically a larger quantity and hence we cannot guarantee that they vanish. To obtain acceleration, we need some form of optimism. In this section, we consider an alteration to Algorithm 2 that uses AO-FTRL for the meta-updates. Given some sequence of hints {˜gt}T t=1, each ˜gt ∈Rm, each wt+1 is given by wt+1 = arg min w∈W ( αt+1˜gt+1 + t∑ s=1 αs⟨∇hs(ws),w⟩+ 1 2βt ∥w∥2 ) . (10) 7Algorithm 3: BMG in practice. input :Weights {βt}T t=1 input :Update rule ϕ input :Target oracle input :Initialisation (x0,w1) for t= 1,2,...,T : xt = xt−1 + ϕ(xt−1,wt) Query zt from target oracle dt(·) = ∥zt −xt + ϕ(xt,·)∥2 wt+1 = wt −βt∇dt(wt) return xT Algorithm 4: Convex optimistic meta-learning. input :Weights {αt}T t=1,{βt}T t=1 input :Update rule ϕ input :Hints {˜gt}T t=1 input :Initialisation (¯x0,w1) for t= 1,2,...,T : xt = ϕ(¯xt−1,wt) ¯xt = (1 −αt/α1:t)¯xt−1 + (αt/α1:t)xt gt = Dϕ(¯xt−1,wt)T∇f(¯xt) vt = αt+1˜gt+1 + ∑t s=1 αsgs wt+1 = arg minw∈W⟨vt,w⟩+ 1 2βt ∥w∥2 return ¯xT Otherwise, we proceed as in Algorithm 2; for a complete description, see Algorithm 4. The AO-FTRL updates do not correspond to a standard meta-update. However, we show momentarily that optimism can be instantiated via the BMG method, detailed in Algorithm 3. The proof for optimistic meta- gradients proceed largely as in Theorem 3, it only differs in that we apply the AO-FTRL regret bound. Theorem 4. Let ϕpreserve regret and assume Algorithm 4 satisfy the assumptions in Section 2. Then f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 βT + T∑ t=1 α2 tβt 2 ∥Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt∥2 ∗ −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . Moreover, assume each˜gt is such that ∥Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt∥2 ∗≤q∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ for some q >0. If each αt = tand βt = t−1 2tqL, then f(¯xt) −f(x∗) ≤4qLdiam(W) T2 −1 . Proof. The proof follows the same lines as that of Theorem 3. The only difference is that the regret of the {wt}T t=1 sequence can be upper bounded by ∥w∗∥2 βT + 1 2 ∑T t=1 α2 tβt∥∇ht(wt) −˜gt∥2 ∗instead of ∥w∗∥2 βT + 1 2 ∑T t=1 α2 tβt∥∇ht(wt)∥2 ∗, as per the AO-FTRL regret bound in Eq. 6. The ﬁnal part follows immediately by replacing the norms and plugging in the values for αand β. ■ From Theorem 4, it is clear that if ˜gt is a good predictor of Dϕ(¯xt−1,wt)T∇f(¯xt), then the positive term in the summation can be cancelled by the negative term. In a classical optimisation setting, Dϕ= In, and hence it is easy to see that simply choosing ˜gt to be the previous gradient is sufﬁcient to achieve the cancellation [Joulani et al., 2020]. Indeed, this choice gives us Nesterov’s Accelerated rate [Wang et al., 2021]. The upshot of this is that we can specialise Algorithm 4 to capture Nesterov’s Accelerated method by choosing ϕ: (x,w) ↦→w—as in the reduction to Heavy Ball—and setting the hints to ˜gt = ∇f(¯xt−1). Hence, while the standard meta-update without optimism contains Heavy Ball as a special case, the optimistic meta-update contains Nesterov Acceleration as a special case. In the meta-learning setting, Dϕis not an identity matrix, and hence the best targets for meta-learning are different. Naively, choosing ˜gt = Dϕ(¯xt−1,wt)T∇f(¯xt−1) would lead to a similar cancellation, but this is not allowed. At iteration t, we have not computed wt when ˜gt is chosen, and hence Dϕ(¯xt−1,wt) is not available. The nearest term that is accessible is Dϕ(¯xt−2,wt−1). Corollary 1. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T∇f(¯xt). Assume that ϕsatisﬁes Dϕ(x′,w)T∇f(x) −Dϕ(x′′,w′)T∇f(x′) 2 ∗≤˜λ∥∇f(x′) −∇f(x)∥2 ∗ for all x′′,x′,x ∈X and w,w′ ∈W , for some ˜λ >0. If each αt = t and βt = t−1 2t˜λL, then f(¯xT) −f(x∗) ≤4˜λLdiam(W) T2−1 . Proof: Appendix D. 86.1 Bootstrapped Meta-Gradients In this section, we present a simpliﬁed version of BMG for clarity, with Appendix E providing a fuller comparison. Essentially, BMG alters the meta-update in Algorithm 1; instead of directly minimising the loss f, it introduces a sequence of targets z1,z2,... and the meta-learner’s goal is select wso that the updated parameters minimise the distance these targets. Concretely, given an update xt = xt−1 + ϕ(xt−1,wt), targets are bootstrapped from xt, meaning that a vector yt is computed to produce the target zt = xt −yt. Assuming the distance to the target is measured under 1 2 ∥·∥ 2 2, the BMG meta-update takes the form wt+1 = wt −Dϕ(xt−1,wt)Tyt. Depending on how yt is computed, it can encode optimism. For instance, the authors rely on the update rule itself to compute a tangent yt = ϕ(xt,wt) −∇f(xt + ϕ(xt,wt)). This encodes optimism via ϕbecause it encourages the meta-learner to build up momentum (i.e. to accumulate past updates). We can contrast this with the types of updates produced by AO-FTRL in Eq. 10. If we have hints ˜gt+1 = Dϕ(¯xt−1,wt)T˜yt+1 for some ˜yt+1 ∈Rn and set ∥·∥ = ∥·∥ 2; assuming an interior solution, Eq. 10 yields wt+1 = wt −Dϕ(¯xt−1,wt)T(αt+1 ˜yt+1 + αt∇f(¯xt))   BMG update + αtDϕ(¯xt−2,wt−1)T˜yt   FTRL error correction . (11) Hence, BMG encodes very similar dynamics to those of AO-FTRL in Eq. 10. Under this choice of hints, the main qualitative difference is that AO-FTRL includes a correction term. The effect of this term is to “undo” previous hints to avoid feedback loops. Notably, BMG can suffer from divergence due to feedback if the gradient in yt is not carefully scaled [Flennerhag et al., 2022]. Our theoretical analysis suggests a simple correction method that may stabilize BMG in practice. More generally, targets in BMG are isomorphic to the hint function in AO-FTRL if the measure of distance in BMG is a Bregman divergence under a strongly convex function (Appendix E). An immediate implication of this is that the hints in Corollary 1 can be expressed as targets in BMG, and hence if BMG satisﬁes the assumptions involved, it converges at a rate O(˜λ/T2). More generally, Theorem 4 provides a sufﬁcient condition for any target bootstrap in BMG to achieve acceleration. Corollary 2. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T˜yt+1, for some ˜yt+1 ∈Rn. If each ˜yt+1 is a better predictor of the next gradient than ∇f(¯xt−1), in the sense that ∥Dϕ(¯xt−2,wt−1)T˜yt −Dϕ(¯xt−1,wt)T∇f(¯xt)∥∗≤˜λ∥∇f(¯xt) −∇f(¯xt−1)∥∗, then Algorithm 4 guarantees convergence at a rate O(˜λ/T2). 7 Conclusion This paper explores a connection between convex optimisation and meta-learning. We construct an algorithm for convex optimisation that aligns as closely as possible with how meta-learning is done in practice. Meta-learning introduces a transformation and we study the effect this transformation has on the rate of convergence. We ﬁnd that, while a meta-learned update rule cannot generate a better dependence on the horizon T, it can improve upon classical optimisation up to a constant factor. An implication of our analysis is that for meta-learning to achieve acceleration, it is important to introduce some form of optimism. From a classical optimisation point of view, such optimism arises naturally by providing the meta-learner with hints. If hints are predictive of the learning dynamics these can lead to signiﬁcant acceleration. We show that the recently proposed BMG method provides a natural avenue to incorporate optimism in practical application of meta-learning. Because targets in BMG and hints in optimistic online learning commute, our results provide ﬁrst rigorous proof of convergence for BMG, while providing a general condition under which optimism in BMG yields accelerated learning. 9References F. Alet, M. F. Schneider, T. Lozano-Perez, and L. P. Kaelbling. Meta-Learning Curiosity Algorithms. In International Conference on Learning Representations, 2020. M. Andrychowicz, M. Denil, S. Gómez, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning to Learn by Gradient Descent by Gradient Descent. In Advances in Neural Information Processing Systems, 2016. Y . Bengio. Gradient-Based Optimization of Hyperparameters.Neural computation, 12(8):1889–1900, 2000. Y . Bengio, S. Bengio, and J. Cloutier.Learning a Synaptic Learning Rule. Université de Montréal, Département d’informatique et de recherche opérationnelle, 1991. Y . Chen, M. W. Hoffman, S. G. Colmenarejo, M. Denil, T. P. Lillicrap, and N. de Freitas. Learning to learn for Global Optimization of Black Box Functions. In Advances in Neural Information Processing Systems, 2016. A. Cutkosky. Anytime Online-to-Batch, Optimism and Acceleration. In International Conference on Machine Learning, 2019. O. Dekel, A. Flajolet, N. Haghtalab, and P. Jaillet. Online learning with a hint. In Advances in Neural Information Processing Systems, 2017. G. Denevi, D. Stamos, C. Ciliberto, and M. Pontil. Online-Within-Online Meta-Learning. InAdvances in Neural Information Processing Systems, 2019. J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011. A. Fallah, A. Mokhtari, and A. Ozdaglar. On the Convergence Theory of Gradient-Based Model- Agnostic Meta-Learning Algorithms. In International Conference on Artiﬁcial Intelligence and Statistics, 2020. C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In International Conference on Machine Learning, 2017. C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online Meta-Learning. InInternational Conference on Machine Learning, 2019. S. Flennerhag, P. G. Moreno, N. D. Lawrence, and A. Damianou. Transferring Knowledge across Learning Processes. In International Conference on Learning Representations, 2019. S. Flennerhag, Y . Schroecker, T. Zahavy, H. van Hasselt, D. Silver, and S. Singh. Bootstrapped Meta-Learning. In International Conference on Learning Representations, 2022. G. E. Hinton and D. C. Plaut. Using Fast Weights to Deblur Old Memories. In Cognitive Science Society, 1987. S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning To Learn Using Gradient Descent. In International Conference on Artiﬁcial Neural Networks, 2001. T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-Learning in Neural Networks: A Survey. arXiv preprint arXiv:2004.05439, 2020. P. Joulani, A. György, and C. Szepesvári. A modular analysis of adaptive (non-) convex optimization: Optimism, composite objectives, and variational bounds. Journal of Machine Learning Research, 1:40, 2017. P. Joulani, A. Raj, A. Gyorgy, and C. Szepesvári. A Simpler Approach to Accelerated Optimization: Iterative Averaging Meets Optimism. In International Conference on Machine Learning, 2020. M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar. Adaptive Gradient-Based Meta-Learning Methods. In Advances in Neural Information Processing Systems, 2019. 10L. Kirsch, S. van Steenkiste, and J. Schmidhuber. Improving Generalization in Meta Reinforcement Learning Using Learned Objectives. arXiv preprint arXiv:1910.04098, 2019. J. Luketina, S. Flennerhag, Y . Schroecker, D. Abel, T. Zahavy, and S. Singh. Meta-gradients in non-stationary environments. In ICLR Workshop on Agent Learning in Open-Endedness, 2022. D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-Based Hyperparameter Optimization Through Reversible Learning. In International conference on machine learning, pages 2113–2122. PMLR, 2015. A. R. Mahmood, R. S. Sutton, T. Degris, and P. M. Pilarski. Tuning-Free Step-Size Adaptation. In ICASSP, 2012. H. B. McMahan. A survey of algorithms and analysis for adaptive online learning. The Journal of Machine Learning Research, 18(1):3117–3166, 2017. M. Mohri and S. Yang. Accelerating Online Convex Optimization via Adaptive Prediction. In International Conference on Artiﬁcial Intelligence and Statistics, 2016. Y . E. Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983. A. Nichol, J. Achiam, and J. Schulman. On First-Order Meta-Learning Algorithms. arXiv preprint ArXiv:1803.02999, 2018. J. Oh, M. Hessel, W. M. Czarnecki, Z. Xu, H. P. van Hasselt, S. Singh, and D. Silver. Discovering Reinforcement Learning Algorithms. In Advances in Neural Information Processing Systems , volume 33, 2020. B. T. Polyak. Some Methods of Speeding up the Convergence of Iteration Methods. USSR Computa- tional Mathematics and Mathematical Physics, 4(5):1–17, 1964. S. Rakhlin and K. Sridharan. Optimization, Learning, and Games with Predictable Sequences. In Advances in Neural Information Processing Systems, 2013. E. Real, C. Liang, D. R. So, and Q. V . Le. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch. In International Conference on Machine Learning, 2020. J. Schmidhuber. Evolutionary Principles in Self-Referential Learning . PhD thesis, Technische Universität München, 1987. T. van Erven and W. M. Koolen. MetaGrad: Multiple Learning Rates in Online Learning. InAdvances in Neural Information Processing Systems, 2016. O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching Networks for One Shot Learning. In Advances in Neural Information Processing Systems, 2016. H. Wang, Y . Wang, R. Sun, and B. Li. Global convergence of maml and theory-inspired neural architecture search for few-shot learning. In Computer Vision and Pattern Recognition, 2022. J.-K. Wang and J. Abernethy. Acceleration through Optimistic No-Regret Dynamics. arXiv preprint arXiv:1807.10455, 2018. J.-K. Wang, J. Abernethy, and K. Y . Levy. No-regret dynamics in the fenchel game: A uniﬁed framework for algorithmic convex optimization. arXiv preprint arXiv:2111.11309, 2021. J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to Reinforcement Learn. In Annual Meeting of the Cognitive Science Society, 2016. T. Xu, Q. Liu, L. Zhao, and J. Peng. Learning to Explore with Meta-Policy Gradient. In International Conference on Machine Learning, 2018a. Z. Xu, H. P. van Hasselt, and D. Silver. Meta-Gradient Reinforcement Learning. In Advances in Neural Information Processing Systems, 2018b. 11Z. Xu, H. P. van Hasselt, M. Hessel, J. Oh, S. Singh, and D. Silver. Meta-gradient reinforcement learning with an objective discovered online. Advances in Neural Information Processing Systems, 33:15254–15264, 2020. T. Zahavy, Z. Xu, V . Veeriah, M. Hessel, J. Oh, H. P. van Hasselt, D. Silver, and S. Singh. A Self-Tuning Actor-Critic Algorithm. In Advances in Neural Information Processing Systems , volume 33, 2020. M. Zinkevich. Online Convex Programming and Generalized Inﬁnitesimal Gradient Ascent. In International Conference on Machine Learning, 2003. 12Appendix A Notation Table 1: Notation Indices t Iteration index: t∈{1,...,T }. T Total number of iterations. [T] The set {1,2,...,T }. i Component index: xi is the ith component of x= (x1,...,x n). αa:b Sum of weights: αa:b = ∑b s=aαs xa:b Weighted sum: xa:b = ∑b s=aαsxs ¯xa:b Weighted average: ¯xa:b = xa:b/αa:b Parameters x∗∈X Minimiser of f. xt ∈X Parameter at time t ¯xt ∈X Moving average of {xs}t s=1 under weights {αs}t s=1. ρt ∈(0,∞) Moving average coefﬁcient αt/α1:t. wt ∈W Meta parameters w∗∈X w∈W that retains regret with smallest norm ∥w∥. αt ∈(0,∞) Weight coefﬁcients βt ∈(0,∞) Meta-learning rate Maps f : X→ R Objective function ∥·∥ : X→ R Norm on X. ∥·∥∗: X∗→R Dual norm of ∥·∥. ht : W→ R Online loss faced by the meta learner Rx(T) Regret of {xt}T t=1 against x∗: Rx(T) := ∑T t=1 αt⟨∇f(¯xt),xt−x∗⟩. Rw(T) Rw(T) := ∑T t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩. ϕ: Rn×Rm →Rn Generic update rule used in practice Dϕ(x,·) : Rm→Rn×m Jacobian of ϕw.r.t. its second argument, evaluated atx∈Rn. ϕ: X×W→X Update rule in convex setting Dϕ(x,·) : W→ Rn×m Jacobian of ϕw.r.t. its second argument, evaluated atx∈X. Bµ : Rn×Rn→[0,∞) Bregman divergence under µ: Rn →R. µ: Rn →R Convex distance generating function. 13Table 2: Hyper-parameter sweep on Convex Quadratics. All algorithms are tuned for learning rate and initialisation of w. Baselines are tuned for decay rate; meta-learned variant are tuned for the meta-learning rate. Learning rate [.1, .3, .7, .9, 3., 5.] winit scale [0., 0.3, 1., 3., 10., 30.] Decay rate / Meta-learning rate [0.001, 0.003, 0.01, .03, .1, .3, 1., 3., 10., 30.] B Convex Quadratic Experiments Loss function. We consider the problem of minimising a convex quadratic loss functions f : R2 →R of the form f(x) = xTQx, where Q is randomly sampled as follows. We sample a random orthogonal matrix U from the Haar distribution scipy.stats.ortho_group. We con- struct a diagonal matrix of eigenvalues, ranked smallest to largest, with λi = i2. Hence, the ﬁrst dimension has an eigenvalue 1 and the second dimension has eigenvalue 4. The matrix Qis given by UT diag(λ1,...,λ n)U. Protocol. Given that the solution is always (0,0), this experiment revolves around understanding how different algorithms deal with curvature. Given symmetry in the solution and ill-conditioning, we ﬁx the initialisation to x0 = (4 ,4) for all sampled Qs and all algorithms and train for 100 iterations. For each Qand each algorithm, we sweep over the learning rate, decay rate, and the initialization of w see Table 2. For each method, we then report the results for the combination of hyper parameters that performed the best. Results. We report the learning curves for the best hyper-parameter choice for 5 randomly sampled problems in the top row of Figure 2 (columns correspond to different Q). We also study the sensitivity of each algorithm to the learning rate in the bottom row Figure 2. For each learning rate, we report the cumulative loss during training. While baselines are relatively insensitive to hyper-parameter choice, meta-learned improve for certain choices, but are never worse than baselines. C Imagenet Experiments Protocol. We train a 50-layer ResNet following the Haiku example, available athttps://github. com/deepmind/dm-haiku/blob/main/examples/imagenet. We modify the default setting to run with SGD. We compare default SGD to variants that meta-learn an element-wise learning rate online, i.e. (x,w) ↦→w⊙∇f(x). For each variant, we sweep over the learning rate (for SGD) or meta-learning rate. We report results for the best hyper-parameter over three independent runs. Standard meta-learning. In the standard meta-learning setting, we apply the update rule once before differentiating w.r.t. the meta-parameters. That is, the meta-update takes the form wt+1 = wt −β∇ht(wt), where ht = f(xt + wt ⊙∇f(xt)). Because the update rule is linear in w, we can compute the meta-gradient analytically: ∇ht(wt) = ∇wf(x+ ϕ(x,w)) = Dϕ(x,w)T∇f(x′) = ∇f(x) ⊙∇f(x′), where x′ = x+ ϕ(x,w). Hence, we can compute the meta-updates in Algorithm 1 manually as wt+1 = max{wt−β∇f(xt) ⊙∇f(xt+1),0.}, where we introduce the max operator on an element- wise basis to avoid negative learning rates. Empirically, this was important to stabilize training. Optimistic meta-learning. For optimistic meta-learning, we proceed much in the same way, but include a gradient prediction ˜gt+1. For our prediction, we use the previous gradient, ∇f(xt+1), as our prediction. Following Eq. 11, this yields meta-updates of the form wt+1 = max { wt −β∇f(xt+1) ⊙(∇f(xt+1) + ∇f(xt)) −∇f(xt) ⊙∇f(xt),0. } . Results. We report Top-1 accuracy on the held-out test set as a function of training steps in Figure 1. Tuning the learning rate does not yield any statistically signiﬁcant improvements under standard meta-learning. However, with optimistic meta-learning, we obtain a signiﬁcant acceleration as well as improved ﬁnal performance, increasing the mean ﬁnal top-1 accuracy from 72% to 75%. 14Table 3: Hyper-parameter sweep on Imagenet. (Meta-)learning rate [0.001, 0.01, 0.02, 0.05, 0.1] D Proofs This section provides complete proofs. We restate the results for convenience. Lemma 1. Given f, {αt}T t=1, and {xt}T t=1, if ϕpreserves regret, then Rx(T) = T∑ t=1 αt⟨∇f(¯xt),xt −x∗⟩≤ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩= Rw(T). Proof. Starting from Rx in Eq. 8, if the update rule preserves regret, there exists w∗∈W for which Rx(T) = T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −x∗⟩ = T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩+ T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,w∗) −x∗⟩ ≤ T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩= Rw(T), since w∗is such that ∑T t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,w∗) −x∗⟩≤ 0. ■ Theorem 3. Let ϕpreserve regret and assume Algorithm 2 satisfy the assumptions in Section 2. Then f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 β + T∑ t=1 λβα2 t 2 ∥∇f(¯xt)∥2 ∗ −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . If x∗is a global minimiser of f, setting αt = 1 and β = 1 λL yields f(¯xT) −f(x∗) ≤λLdiam(W) T . Proof. Since ϕpreserves regret, by Lemma 1, the regret term Rx(T) in Eq. 7 is upper bounded by Rw(T). We therefore have f(¯xT) −f(x∗) ≤ 1 α1:T ( Rw(T) −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . (12) Next, we need to upper-bound Rw(T). Since, Rw(T) = ∑T t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) − ϕ(¯xt−1,w∗)⟩, the regret of {wt}T t=1 is deﬁned under loss functions ht : W →R given by ht = αt⟨∇f(¯xT),ϕ(¯xt−1,w))⟩. By assumption of convexity in ϕ, each ht is convex in w. Hence, the regret under {αtht}T t=1 can be upper bounded by the regret under the linear losses {αt⟨∇ht(wt),·⟩}T t=1. These linear losses correspond to the losses used in the meta-update in Eq. 3. Since the meta-update is an instance of FTRL, we may upper-bound Rw(T) by Eq. 6 with each 15˜gt = 0. Putting this together along with smoothness of ϕ, Rx(T) ≤Rw(T) = T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩ ≤ T∑ t=1 αt⟨∇ht(wt),wt −w∗⟩ ≤∥w∗∥2 β + β 2 T∑ t=1 α2 t∥∇ht(wt)∥2 ∗ = ∥w∗∥2 β + β 2 T∑ t=1 α2 t∥Dϕ(¯xt−1,wt)T∇f(¯xt)∥2 ∗ ≤∥w∗∥2 β + λβ 2 T∑ t=1 α2 t∥∇f(¯xt)∥2 ∗. (13) Putting Eq. 12 and Eq. 13 together gives the stated bound. Next, if x∗ is the global optimiser, ∇f(x∗) = 0 by ﬁrst-order condition. Setting β = 1/(Lλ) and αt = 1 means the ﬁrst two norm terms in the summation cancel. The ﬁnal norm term in the summation is negative and can be ignored. We are left with f(¯xT) −f(x∗) ≤λL∥w∗∥2 T ≤λLdiam(W) T . ■ Corollary 1. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T∇f(¯xt). Assume that ϕsatisﬁes Dϕ(x′,w)T∇f(x) −Dϕ(x′′,w′)T∇f(x′) 2 ∗≤˜λ∥∇f(x′) −∇f(x)∥2 ∗ for all x′′,x′,x ∈X and w,w′ ∈W , for some ˜λ >0. If each αt = t and βt = t−1 2t˜λL, then f(¯xT) −f(x∗) ≤ 4˜λLdiam(W) T2−1 . Proof. Plugging in the choice of ˜gt and using that Dϕ(¯xt−1,wt)T∇f(¯xt) −Dϕ(xt−2,wt−1)T∇f(¯xt−1) 2 ∗≤˜λ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗, the bound in Theorem 4 becomes f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 βT + 1 2 T∑ t=1 ( ˜λα2 tβt −α1:t−1 L ) ∥∇f(¯xt) −∇f(¯xt−1)∥2 ∗ ) , where we drop the negative terms ∥∇f(¯xt) −∇f(x∗)∥2 ∗. Setting αt = tyields α1:t−1 = (t−1)t 2 , while setting βt = t−1 2t˜λL means ˜λα2 tβt = (t−1)t 2L . Hence, ˜λα2 tβt −α1:t−1/Lcancels and we get f(¯xT) −f(x∗) ≤ ∥w∗∥2 βTα1:T = 4∥w∗∥2˜λL (T −1)(T + 1) ≤ 4˜λLdiam(W) (T −1)(T + 1) = 4˜λLdiam(W) T2 −1 . ■ Corollary 2. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T˜yt+1, for some ˜yt+1 ∈Rn. If each ˜yt+1 is a better predictor of the next gradient than ∇f(¯xt−1), in the sense that ∥Dϕ(¯xt−2,wt−1)T˜yt −Dϕ(¯xt−1,wt)T∇f(¯xt)∥∗≤˜λ∥∇f(¯xt) −∇f(¯xt−1)∥∗, then Algorithm 4 guarantees convergence at a rate O(˜λ/T2). Proof. The proof follows the same argument as Corollary 1. ■ 16Algorithm 5: BMG in practice (general version). input :Weights {ρt}T t=1,{βt}T t=1 input :Update rule ϕ input :Matching function Bµ input :Target oracle input :Initialisation (x0,w1) for t= 1,2,...,T : xt = xt−1 + ϕ(xt−1,wt) Query zt from target oracle dt : w↦→Bµ zt (xt−1 + ϕ(xt−1,w)) wt+1 = wt −βt∇dt(wt) return xT E BMG Errata: this was incorrectly referred to as Appendix F in our original submission. In this section, we provide a more comprehensive reduction of BMG to AO-FTRL. First, we provide a more general deﬁnition of BMG. Let µ : X →R be a convex distance generating function and deﬁne the Bregman Divergence Bµ : Rn×Rn →R by Bµ z(x) = µ(x) −µ(z) −⟨∇µ(z),x −z⟩. Given initial condition (x0,w1), the BMG updates proceed according to xt = xt−1 + ϕ(xt−1,wt) wt+1 = wt −βt∇dt(wt), (14) where dt : Rn →R is deﬁned by dt(w) = Bµ zt (xt−1 + ϕ(xt−1,wt)), where each zt ∈Rn is referred to as a target. See Algorithm 5 for an algorithmic summary. A bootstrapped target uses the meta-learner’s most recent update, xt, to compute the target, zt = xt + yt for some tangent vector yt ∈Rn. This tangent vector represents a form of optimism, and provides a signal to the meta-learner as to what would have been a more efﬁcient update. In particular, the author’s consider using the meta-learned update rule to construct yt; yt = ϕ(xt,wt) −∇f(xtϕ(xt,w −t)). Note that xt = xt−1 + ϕ(xt−1,wt), and hence this tangent vector is obtained by applying the update rule again, but now to xt. For this tangent to represent an improvement, it must be assumed that wt is a good parameterisation. Hence, bootstrapping represents a form of optimism. To see how BMG relates to Algorithm 4, and in particular, Eq. 10, expand Eq. 14 to get wt+1 = wt −βtDϕ(xt−1,wt)T (∇µ(xt) −∇µ(zt)) . (15) In contrast, AO-FTRL reduces to a slightly different type of update. Lemma 2. Consider Algorithm 4. Given online losses ht : W → R deﬁned by {⟨Dϕ(¯xt−1,wt)T∇f(¯xt),·⟩}T t=1 and hint functions {⟨˜gt,·,}⟩T t=1, with each ˜gt ∈Rm. If ∥·∥ = (1/2)∥·∥2, an interior solution to Eq. 10 is given by wt+1 = βt βt−1 wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) . 17Proof. By direct computation: wt+1 = arg min w∈W ( αt+1⟨˜gt+1,w⟩+ t∑ s=1 αs⟨Dϕ(¯xs−1,ws)T∇f(¯xs),w⟩+ 1 2βt ∥w∥2 2 ) = −βt ( αt+1˜gt+1 + t∑ s=1 αtDϕ(¯xs−1,ws)T∇f(¯xs)) ) = −βt ( αt+1˜gt+1 + αtDϕ(¯xt−1,wt)T∇f(¯xt) + (t−1∑ s=1 αtDϕ(¯xs−1,ws)T∇f(¯xs)) )) = −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) −βt ( αt˜gt + t−1∑ s=1 αtDϕ(¯xs−1,ws)T∇f(¯xs)) ) = βt βt−1 wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) . ■ AO-FTRL includes a decay rate βt/βt−1; this decay rate can be removed by instead using optimistic online mirror descent [Rakhlin and Sridharan, 2013, Joulani et al., 2017]—to simplify the exposition we consider only FTRL-based algorithms in this paper. An immediate implication of Lemma 2 is the error-corrected version of BMG. Corollary 3. Setting ˜gt+1 = Dϕ(¯xt−1,wt)T˜gt+1 for some ˜yt+1 ∈Rn yields an error-corrected version of the BMG meta-update in Eq. 14. Speciﬁcally, the meta-updates in Lemma 2 becomes wt+1 = βt βt−1 wt −βtDϕ(¯xt−1,wt)T(αt+1 ˜yt+1 + αt∇f(¯xt))   BML update + βtαtDϕ(¯xt−2,wt−1)T˜yt   FTRL error correction . Proof. Follows immediately by substituting for each ˜gt+1 in Lemma 2. ■ To illustrate this connection, Let µ = f. In this case, the BMG update reads wt+1 = wt − βtDϕ(xt−1,wt)T(∇f(zt) −∇f(xt)). The equivalent update in the convex optimisation setting (i.e. Algorithm 4) is obtained by setting ˜yt+1 = ∇f(zt), in which case Corollary 3 yields wt+1 = βt+1 βt wt −βtDϕ(¯xt−1,wt)T(αt+1∇f(zt) −αt∇f(¯xt)) + ξt, where ξt = βtαtDϕ(¯xt−2,wt−1)T∇f(¯xt−1) denotes the error correction term we pick up through AO-FTRL. Since Algorithm 5 does not average its iterates—while Algorithm 4 does—we see that these updates (ignoring ξt) are identical up to scalar coefﬁcients (that can be controlled for by scaling each βt and each ˜gt+1 accordingly). More generally, the mapping from targets in BMG and hints in AO-FTRL takes on a more complicated pattern. Our next results show that we can always map one into the other. To show this, we need to assume a certain recursion. It is important to notice however that at each iteration introduces an unconstrained variable and hence the assumption on the recursion is without loss of generality (as the free variable can override it). Theorem 5. Targets in Algorithm 5 and hints in algorithm 4 commute in the following sense. BMG →AO-FTRL. Let BMG targets {zt}T t=1 by given. A sequence of hints {˜g}T t=1 can be constructed recursively by αt+1˜gt+1 = Dϕ(¯xt−1,wt)T(∇µ(¯xt) −∇µ(zt) −αt∇f(¯xt)) + αt˜gt, t ∈[T], (16) so that interior updates for Algorithm 4 are given by wt+1 = βt βt−1 wt −βt(∇µ(zt) −∇µ(¯xt)) . 18AO-FTRL →BMG. Conversely, assume a sequence {˜yt}T t=1 are given, each ˜yt ∈Rn. If µstrictly convex, a sequence of BMG targets {zt}T t=1 can be constructed recursively by zt = ∇µ−1 (∇µ(xt) −(αt+1 ˜yt+1 + αt∇f(xt))) t∈[T], so that BMG updates in Eq. 14 are given by wt+1 = wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) , where each ˜gt+1 is the BMG-induced hint function, given by αt+1˜gt+1 = αt+1Dϕ(xt−1,wt)T˜yt+1 + αt˜gt. Proof. First, consider BMG →AO-FTRL. First note that ˜g1 is never used and can thus be chosen arbitrarily—here, we set ˜g1 = 0. For w2, Lemma 2 therefore gives the interior update w2 = β2 β1 w1 −β1(α2˜g2 + α1Dϕ(¯x0,w1)T∇f(¯x1)). Since the formulate for ˜g2 in Eq. 16 only depends on quantities with iteration index t= 0,1, we may set α2˜gt = Dϕ(¯x0,w1)T(∇µ(¯x1) −∇µ(zt) −αt∇f(¯x1)). This gives the update w2 = β2 β1 w1 −β1Dϕ(¯x0,w1)T(∇µ(¯x1) −∇µ(z1)). Now assume the recursion holds up to time t. As before, we may choose αt+1˜gt+1 according to the formula in Eq. 16 since all quantities on the right-hand side depend on quantities computed at iteration tor t−1. Subtituting this into Lemma 2, we have wt+1 = βt βt−1 wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) = βt βt−1 wt −βt ( Dϕ(¯xt−1,wt)T(∇µ(¯xt) −∇µ(zt) −αt∇f(¯xt)) + αt˜gt +αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) = βt βt−1 wt −βtDϕ(¯xt−1,wt)T(∇µ(¯xt) −∇µ(zt)). AO-FTRL →BMG. The proof in the other direction follows similarly. First, note that for µstrictly convex, ∇µ is invertible. Then, z1 = ∇µ−1(∇µ(x1) −(α2 ˜y2 + α1∇f(x1))). This target is permissible since x1 is already computed and {˜yt}T t=1 is given. Substituting this into the BMG meta-update in Eq. 14, we ﬁnd w2 = w1 −β1Dϕ(x0,w1)T(∇µ(x1) −∇µ(∇µ−1(∇µ(x1) −(α2 ˜y2 + α1∇f(x1))))) = w1 −β1Dϕ(x0,w1)T(α2 ˜y2 + α1∇f(x1)) = w1 −β1 ( α2˜g2 + α1(Dϕ(¯x0,w1)T∇f(¯x1) −˜g1) ) , where the last line uses that ˜g2 is deﬁned by α2˜g2 −α1˜g1 = Dϕ(¯x0,w1)T˜y2 and ˜g1 is arbitrary. Again, assume the recursion holds to time t. We then have wt+1 = wt −βtDϕ(xt−1,wt)T (∇µ(xt) −∇µ(zt)) = wt −βtDϕ(xt−1,wt)T(∇µ(xt) −∇µ(∇µ−1(∇µ(xt) −(αt+1 ˜yt+1 + αt∇f(xt))))) = wt −βtDϕ(xt−1,wt)T(αt+1 ˜yt+1 + αt∇f(xt)) = wt −βt(αt+1˜gt+1 + αt(Dϕ(xt−1,wt)T∇f(xt) −˜gt)). ■ 19
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper demonstrates that gradient descent with momentum (Heavy Ball) and Nesterov Acceleration are special cases of meta-gradients, showing that meta-learning can be understood as a non-linear transformation of classical optimization. It establishes convergence rates for meta-learning in the single-task convex setting, proving O(λ/T) convergence. Crucially, it shows that optimism is required for meta-learning to achieve accelerated convergence rates of O(λ/T^2). Finally, the paper reveals that optimism in meta-learning can be captured through Bootstrapped Meta-Gradients (BMG), providing the first proof of convergence for BMG and insight into its underlying mechanics.",
    "methodology": "The research employs a theoretical framework that connects meta-learning to convex optimization. It introduces a stylized variant of meta-learning (Algorithm 2) for rigorous analysis, which utilizes moving averages and an online learning algorithm for meta-updates, specifically Follow-The-Regularized-Leader (FTRL) and its adaptive-optimistic variant (AO-FTRL). The update rule (ϕ) is assumed to be affine in the meta-parameters (w) and smooth with bounded norm. Optimism is formally integrated into the meta-learning process through the use of hint functions in the AO-FTRL meta-updates (Optimistic Meta-Learning, OML). The analysis relies on online-to-batch conversion techniques and regret bounds from online convex optimization, proving how the update rule affects convergence rates and how optimism enables acceleration. The paper further establishes an isomorphism between targets in BMG and hint functions in AO-FTRL.",
    "experimental_setup": "The paper conducts experiments in two settings: (1) Convex Quadratics, where ill-conditioned convex quadratic loss functions (f(x) = xTQx with Q matrices having eigenvalues λi=i^2) are used. Momentum and AdaGrad are compared against their meta-learned variants (Meta-Momentum and Meta-AdaGrad, which meta-learn element-wise step sizes). Performance is evaluated based on loss per iteration and cumulative loss sensitivity to learning rates. (2) ImageNet, where a 50-layer ResNet is trained. Standard SGD is compared against meta-learning (tuning element-wise learning rates online) and optimistic meta-learning (using previous gradients as hints). Top-1 test accuracy over training steps is reported, with shading indicating 95% confidence intervals over 3 seeds. Hyper-parameter sweeps are conducted for learning rates, winit scale, and decay/meta-learning rates in both setups.",
    "limitations": "The theoretical results are primarily derived for the single-task convex optimization setting, which simplifies the complexities of general meta-learning. The analysis relies on a \"stylized variant\" of meta-learning and specific assumptions on the update rule ϕ (e.g., affine in w, smooth with bounded norm) that may not always hold in practical, complex scenarios. Computing the Jacobian Dϕ, which is central to the theoretical derivations, is often intractable in practical meta-learning applications. Standard meta-learning without optimism only achieves a slower O(1/T) convergence rate, underscoring the necessity of optimism for acceleration. Additionally, the paper notes that Bootstrapped Meta-Gradients (BMG) can suffer from divergence if the gradient used in its target computation (yt) is not carefully scaled.",
    "future_research_directions": "The paper identifies several future research avenues, including exploring novel forms of meta-learning that effectively incorporate optimism, as it is highlighted as an \"exciting opportunity.\" The theoretical analysis provides insights into stabilizing Bootstrapped Meta-Gradients (BMG), suggesting further work on implementing and evaluating simple correction methods in practice. Moreover, the paper notes that the presented regret bounds and analysis can be extended to stochastic optimization settings, indicating this as a promising direction for broader applicability of the theoretical framework."
}
