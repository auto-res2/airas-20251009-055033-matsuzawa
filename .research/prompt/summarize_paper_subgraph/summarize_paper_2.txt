
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log maxi Ni,t Nk,t  + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper proposes a novel online adaptation approach for object detection in continually changing test domains. Its key contributions include the introduction of architecture-agnostic and lightweight adaptor modules (requiring only 0.54% to 0.89% additional parameters) for efficient adaptation and prevention of catastrophic forgetting. It presents a practical and straightforward class-wise feature aligning method that operates at both image and object levels to resolve domain shifts. Furthermore, the method enhances efficiency by determining when the model requires adaptation through two novel criteria. This approach significantly outperforms baselines, achieving mAP improvements of up to 4.9%p and 7.9%p on COCO-corrupted and SHIFT benchmarks respectively, while maintaining high inference speeds (around 20 FPS or higher). It also demonstrates robust preservation of task-specific knowledge and wide applicability across diverse backbone architectures like CNNs and Transformers.",
    "methodology": "The proposed method focuses on three aspects: what, how, and when to update the object detector. For 'what to update', it introduces shallow, low-rank, architecture-agnostic adaptor modules in parallel to each backbone block (MLP for Transformers, 1x1 CNN for ResNet). Only these adaptors are updated, while the pre-trained backbone parameters remain frozen. For 'how to update', it employs an Exponentially Moving Average (EMA) feature alignment strategy. This involves aligning feature distributions at both image-level (Limg) and region-level class-wise (Lobj) by minimizing the KL divergence between training and test domain features. Mean and variance statistics are used, with test domain means estimated via EMA and test variances approximated from training variances. A class-frequency weighting scheme is applied for object-level alignment to address class imbalance. The total adaptation loss is a sum of Limg and Lobj. For 'when to update', two criteria are used to dynamically skip or resume adaptation: 1) Model updates if the ratio of the current image-level distribution gap (Limg) to a pre-computed in-domain distribution gap (Din KL) exceeds a threshold (τ1=1.1). 2) Model updates if Limg suddenly increases relative to its exponentially moving average (Lt ema) by a threshold (τ2=1.05). Adaptation proceeds if at least one criterion is met.",
    "experimental_setup": "The research used Faster-RCNN models with ResNet50 and Swin-Tiny backbones, both integrated with FPN. Experiments were conducted across three continually changing domain scenarios: COCO \n COCO-C (models trained on MS-COCO and evaluated sequentially on 15 types of corruptions from COCO-C, with additional evaluation on the original COCO validation set) and SHIFT-(Discrete / Continuous) (a synthetic driving dataset with 6 classes, featuring variations in weather and time-of-day, tested in sequential drastic shifts for Discrete and gradual transitions for Continuous). Publicly available models were used for COCO, while SHIFT models were trained using the Detectron2 framework. Test-time adaptation employed an SGD optimizer with a learning rate of 0.001, an EMA update rate (α) of 0.01, and thresholds τ1=1.1 and τ2=1.05 for adaptation criteria. A batch size of 4 was used for main experiments. Baselines included Direct-Test (no adaptation), ActMAD, NORM, DUA, and a Mean-Teacher model implemented based on TeST. Performance was evaluated using mAP, the number of forward and backward passes, and Frames Per Second (FPS).",
    "limitations": "The approximation of test feature variance (Σte \n Σtr) is made to reduce instability. The 'when to update' criteria rely on specific assumptions regarding the relationship between the image-level distribution gap (Limg) and both in-domain and historical gaps. Additionally, the class frequency weighting scheme (wk,t) for object-level alignment, while generally effective, showed a slight performance decrease in the SHIFT-Continuous setting during ablation studies, suggesting potential trade-offs in specific dynamic scenarios.",
    "future_research_directions": "Not mentioned"
}
