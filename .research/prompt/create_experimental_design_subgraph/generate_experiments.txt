
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 3 distinct Experiment objects.

# Instructions
- Generate 3 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 3 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "1. Existing forward-only TTA (NGFAT) still updates only BN γ,β and therefore cannot adapt models that use Group/Layer/Instance Norm or no normalisation at all (e.g. transformers, self-normalising nets).\n2. Forward-only natural-gradient steps are always executed although many test batches are easy – unnecessary parameter noise slows convergence and may accumulate drift in recurring scenarios.\n3. There is no mechanism to detect harmful updates on-device; collapsing on rare, high-entropy inputs remains possible and cannot be rolled back without labelled data.\n4. Current Fisher blocks ignore cross-sample curvature; mini-batches <8 (typical on MCU) give noisy statistics.",
    "Methods": "Method name: ZORRO – Zero-backward Online Risk-aware RObust adaptation.\nKey novelties:\na) Universal forward-Fisher: derive closed-form 2×2 (or 1×1) natural-gradient for *any* affine layer that is a linear map followed by element-wise normalisation f(x)=α(x−μ)/σ+β. This covers BN, GN, LN, IN and the RMS norm of ViT. Statistics (μ,σ) are read from the same forward pass; memory overhead 2×feature-dim.\n\nb) Cross-batch shrinkage: maintain a running Fisher F̂_t and apply James–Stein shrinkage with factor τ_t= n/(n+λ) where n is effective sample count; this drastically reduces noise for tiny batches (n≤4) without extra FLOPs.\n\nc) Accuracy-estimation gate: integrate AETTA-mini (dropout-free variant using Monte-Carlo weight masking) to obtain a label-free error estimate ĥ_t after each batch. Update is *committed* only if ĥ_t − ĥ_{t−1}>ϵ or entropy drop >δ; otherwise parameters are left unchanged (pure inference). This halves update frequency on slow streams and prevents drift.\n\nd) Rollback buffer: store last K=3 accepted parameter states + their ĥ. If two consecutive batches yield worse ĥ than all K checkpoints, revert to best state (cost O(K·P) copy, P tiny because we save only γ,β,α). Fully unsupervised.\n\ne) µC-friendly maths: all matrix inverses are 2×2; no sqrt/exp outside LUT; fits in CMSIS-NN.\n",
    "Experimental Setup": "Datasets & streams:\n• TinyImageNet-C (severity 3-5) with η_r∈{1,1/2,1/8}.\n• CIFAR-10-C and CIFAR-10.1 natural shift.\n• Google Speech Commands v2 with background noise (audio, shows non-vision generality).\n• Real on-device capture: 5-fps webcam stream on STM32H7 (640 KB SRAM).\n\nModels: ResNet-20-GN, MobileNet-V2-GN, ViT-Tiny with RMS Norm, and a 1-D CNN for speech.\n\nBaselines: Source, TENT, FATENT, NGFAT, RoTTA, AETTA-reset, BN adaptive.\n\nMetrics:\n1. Error after {1,2,4} batches.\n2. Area-under-error-curve under Realistic Online Protocol (ROP, η_r).\n3. Avg. wall-clock latency & energy (Arm-Cortex-M vs Jetson-Nano).\n4. False-update rate (updates skipped although target batch lowered error <0.1 %).\n5. Collapse-recovery success (% runs where rollback prevents >5 % error spike).\n",
    "Experimental Code": "# zorro.py  (core update-gate, 60 LOC)\n@torch.no_grad()\ndef zorro_step(model, batch, state):\n    out = model(batch)\n    H = entropy(out)\n    ## 1. Unsup. accuracy proxy via softmax disagreement\n    q = out.softmax(1)\n    p = (q * (1-q)).sum(1)             # variance proxy\n    acc_hat = 1 - p.mean()\n    ## 2. Decide whether to adapt\n    if (acc_hat < state.last_acc-1e-3) or (H.mean()>state.last_H*0.9):\n        for mod in state.affine:       # BN, GN, LN, RMSNorm\n            y = mod.output             # cached activation\n            g = state.dH_dz @ y.mean(0)  # ∂H/∂α like NGFAT\n            F = y.var(0)+1e-5\n            tau = state.n/(state.n+state.lambda_)\n            step = -tau * g / F\n            mod.weight += step\n        state.save_checkpoint(acc_hat)\n    state.update_stats(acc_hat, H)\n    return out",
    "Expected Result": "• Matches NGFAT accuracy on CIFAR-C but with 35 % fewer parameter updates.\n• On GN/LN models where BN-only methods fail, ZORRO reduces error from 38 % (source) to 24 % within 2 batches, beating TENT-GN (27 %).\n• MCU demo: 5 fps stream maintained at 3.7 mJ/frame vs 6.2 mJ for TENT.\n• Rollback prevents >90 % of catastrophic error spikes observed in NGFAT under recurring shift.\n• Audio CNN: word error 8.1 %→6.4 % after first noisy batch, showing modality-agnosticism.",
    "Expected Conclusion": "ZORRO generalises forward-only natural-gradient TTA to any normalisation layer, autonomously decides *when* to adapt via built-in label-free risk estimates, and adds a lightweight rollback against collapse. It retains single-pass speed, extends applicability to transformers and micro-controllers, and raises the bar for robust, energy-aware test-time learning in real-world streams."
}

# Experiment Strategy
Overall experimental strategy for demonstrating the effectiveness of ZORRO

1. Core validation axes
   a. Adaptation benefit – Does ZORRO lower on-line error faster and further than all competitors?
   b. Efficiency – Does it achieve that benefit with fewer parameter updates, lower latency, and lower energy?
   c. Robustness – Does it remain stable under severe or recurring shifts and avoid catastrophic drift?
   d. Generality – Does it work across normalisation types, model families, modalities, batch sizes, and hardware classes?
   e. Safety – Can the gate + rollback reject harmful updates better than prior art?

2. Comparative framework
   • Methods: (i) frozen source model, (ii) classic BN adaptation, (iii) forward-only SOTA (TENT, NGFAT, FATENT), (iv) risk-aware SOTA (RoTTA, AETTA-reset), (v) ZORRO ablations (−Fisher, −shrinkage, −gate, −rollback, −all) and full ZORRO.
   • Datasets/shifts: vision, audio, synthetic recurrences, real device streams. Each experiment will use an identical set of comparison methods so curves are directly overlayable.
   • Hardware profiles: A100 (heavy), Jetson-Nano (edge GPU), STM32H7 (MCU) – same software harness, differing batch-size budgets.

3. Multi-angle evidence collection
   • Quantitative accuracy: instantaneous error at fixed adaptation horizons {1,2,4,∞} batches; area-under-error-curve (AUEC) over the whole stream; Wilcoxon signed-rank test over 5 seeds.
   • Adaptation cost: (i) number of parameter update steps, (ii) wall-clock latency per frame, (iii) mean energy per frame (on-board power logger for Jetson / MCU).
   • Stability metrics: variance of error, maximum error spike, collapse rate, rollback recovery rate, false-update rate.
   • Qualitative analysis: histogram of gate decisions, t-SNE of feature drift, visual audio/vision examples where ZORRO reverted.
   • Computational analysis: FLOPs, memory, additional parameters; measured with Torch-Profiler and MCU on-device counters.

4. Experimental phases (common skeleton for every task)
   Phase-0: Baseline replication to calibrate code, fix seeds, and verify that results reproduce published numbers on clean data.
   Phase-1: Standard corruptions – run single-pass streams under ROP; log all core metrics.
   Phase-2: Recurring/easy-hard cycles – stress test the gate/rollback; identical shift schedule for all methods.
   Phase-3: Tiny-batch regime – constrain batch ≤4 to expose Fisher shrinkage; repeat Phase-1 metrics.
   Phase-4: Hardware deployment – run frozen inference & on-line adaptation in real-time on Jetson/MCU; capture energy & fps.
   Phase-5: Ablation sweep – repeat Phases 1–3 with each component disabled to pinpoint contribution.
   Phase-6: Cross-modality – apply to Speech Commands; same phases 1–3.

5. Success criteria (must hold in ≥4/5 seeds)
   • AUEC improvement ≥10% over best non-ZORRO competitor on every dataset-model pair.
   • Parameter-update count ≤70% of NGFAT average while maintaining equal or better final error.
   • Catastrophic collapse rate <1%; rollback recovers ≥80% of induced collapses.
   • Latency overhead ≤5% vs source inference on A100; energy per frame ≤65% of TENT on MCU.
   • No ablation variant may outperform full ZORRO on more than 10% of runs.

6. Statistical protocol
   • 5 random seeds × 5 shuffled test streams per condition.
   • Report median and 95% CI; conduct paired Wilcoxon tests with Holm-Bonferroni correction.
   • Release code, logs, and weights for full transparency.

7. Resource allocation
   • Distributed runner uses 6 A100 GPUs for parallel seeds, leaving 2 GPUs for hyper-parameter sweeps.
   • RAM headroom (2 TB) holds all datasets and checkpoints in a RAM-disk, eliminating I/O variance.
   • Edge hardware tests run nightly; results auto-synced to central dashboard.

This unified strategy ensures every experiment, regardless of dataset or model, produces directly comparable evidence on adaptation benefit, cost, robustness, generality, and safety, thereby establishing a comprehensive case for ZORRO’s effectiveness.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-1-main-performance",
            "run_variations": [
                "source-frozen",
                "bn-adapt",
                "TENT",
                "NGFAT",
                "ZORRO-full"
            ],
            "description": "Objective / hypothesis: Quantitatively verify that the proposed ZORRO method reduces online error faster and further than all strong forward-only TTA baselines on standard vision corruption benchmarks while using fewer updates and comparable compute.\n\nModels\n• ResNet-20-GN  (CIFAR sized)\n• MobileNet-V2-GN  (mobile vision)\n• ViT-Tiny-RMS  (transformer, no BN)\n\nDatasets\n• CIFAR-10-C  (15 corruption types × 5 severities)\n• CIFAR-10.1  (natural covariate shift)\n• TinyImageNet-C  (severity 3-5)\n\nDataset preprocessing\n• Images resized to native resolution, per-dataset mean/σ normalisation.\n• No data augmentation during test-time streams.\n\nData splitting / streaming protocol\n• Training: official training split only, no corruptions.\n• Validation: clean val split for early-stopping hyper-parameters (λ, ε, δ).\n• Test: Realistic Online Protocol (ROP) streams of length 10 k drawn from test-corruption pairs; η_r ∈ {1, 1/2, 1/8} controls arrival rate.\n• 5 independent shuffles × 5 random seeds = 25 runs per variation.\n\nEvaluation metrics\nPrimary: (i) error after {1,2,4} batches, (ii) Area-Under-Error-Curve (AUEC).\nSecondary: parameter-update count, wall-clock latency per frame (A100), memory Δ, energy/frame (collected with NVIDIA-SMI power log).\nStatistical test: paired Wilcoxon signed-rank with Holm–Bonferroni (α=0.05).\n\nComparisons\n• Baselines: frozen source, BN adaptation, TENT, NGFAT (official code, † tuned learning-rate), all run with identical batch sizes.\n• Proposed: ZORRO-full.\n† NGFAT uses author-reported η=0.001 for fair comparability.\n\nHyper-parameter analysis\n• Grid over learning-rate η ∈ {5e-4,1e-3,2e-3} (baselines) vs analytic NG for ZORRO.\n• Gate threshold ε ∈ {5e-3,1e-2}; shrinkage λ ∈ {4,8,16} tuned on validation streams.\n\nRobustness tests\n• Repeat full experiment with batch-size ≤4 (tiny-batch regime).\n• 10% additive Gaussian noise injected into activations to test stability.\n\nCompute / efficiency measurement\n• FLOPs via fvcore-flop-counter.\n• Torch profiler for time; nvidia-smi for power; report mean±95% CI.\n\nRepetitions & checkpointing\n• 5 seeds; metrics averaged, medians reported.\n• Early-stopping for hyper-param search by best validation AUEC, otherwise last checkpoint.\n\nExample code (fragment)\n```\nstream = CorruptionStream(dataset, bs=batch)\nfor batch in stream:\n    logits = model(batch.to(device))\n    if variant == 'ZORRO-full':\n         zorro_step(model, batch, state)\n    elif variant == 'NGFAT':\n         ngfat_step(model, batch)\n    ...  # other variants\n    meter.update(logits, labels=None)  # label-free metrics internally\n```\nExpected outcome: ZORRO produces ≥15% lower AUEC than the best competitor on every model–dataset pair while issuing ≤65% of NGFAT’s parameter updates and adding ≤5% latency on A100."
        },
        {
            "experiment_id": "exp-2-ablation-study",
            "run_variations": [
                "ZORRO-full",
                "no-fisher",
                "no-shrinkage",
                "no-gate",
                "no-rollback"
            ],
            "description": "Objective / hypothesis: Isolate the contribution of each novel component (a–e) of ZORRO and confirm that the full method is necessary for peak performance and stability.\n\nModels\n• MobileNet-V2-GN (vision, moderate size)\n• ViT-Tiny-RMS (transformer, no BN)\n\nDatasets\n• TinyImageNet-C  (all 75 corruption-severity pairs)\n• CIFAR-10-C  (subset: {fog, blur, jpeg} @ severity 3) – selected to shorten runtime.\n\nDataset preprocessing\nIdentical to exp-1.\n\nData splitting / streaming protocol\n• Same ROP streams; batch-size fixed to 4 to stress Fisher shrinkage.\n• Recurring shift schedule: easy (clean) 200 samples → hard (severity-5) 200 samples → repeat ×25.\n\nRepetitions\n• 5 random seeds; 3 independent stream realisations. Total 30 runs per variation.\n\nEvaluation metrics\nPrimary: AUEC, maximum error spike (↑ worst  batch – ↑ best  batch).\nSecondary: false-update rate, rollback recovery rate, parameter-update count.\n\nComparisons\n• Full method vs four single-component removals.\n\nHyper-parameter sensitivity\n• For each variant, sweep λ ∈ {2,4,8,16}, ε ∈ {0,5e-3,1e-2} via 3-fold cross-validation; plot heat-maps of AUEC.\n\nRobustness assessment\n• Adversarial perturbation: FGSM ε=2/255 applied to 10% of test batches.\n• Distribution shift: swap corruption type mid-stream (fog→jpeg) without warning; record collapse rate.\n\nCompute analysis\n• Record FLOPs, inferred extra memory per component.\n• Ablation “no-fisher” saves ~15% FLOPs, verify with profiler.\n\nSelection criteria\n• Best validation AUEC → pick λ, ε then report test.\n\nExample code (fragment)\n```\nfor variant in variants:\n    cfg = default_cfg()\n    if variant == 'no-fisher':\n        cfg.disable_fisher = True\n    elif variant == 'no-shrinkage':\n        cfg.shrinkage = None\n    ...\n    run_experiment(cfg)\n```\nExpected outcome: Every single ablation degrades AUEC by ≥8% vs full; ‘no-gate’ shows the largest error spikes, ‘no-rollback’ suffers 12× higher collapse rate, confirming the necessity of each module."
        },
        {
            "experiment_id": "exp-3-robustness-hardware-efficiency",
            "run_variations": [
                "source-frozen",
                "NGFAT",
                "RoTTA",
                "ZORRO-lite",
                "ZORRO-full"
            ],
            "description": "Objective / hypothesis: Demonstrate that ZORRO maintains robustness under recurring shifts and tiny micro-controller batches while offering superior energy & latency efficiency on resource-constrained hardware.\n\nModels\n• ResNet-20-GN (for STM32 MCU deployment)\n• 1-D CNN-Speech (5-layer, GN)\n\nDatasets / streams\n• TinyImageNet-C with easy↔hard cycles (as in exp-2) streamed at 5 FPS.\n• Google Speech Commands v2 with additive café/traffic noise (SNR 0–20 dB) streamed at real-time 16 kHz windows.\n• Live STM32H7 webcam stream (640×480 @5 FPS, indoor office lighting) captured via serial link.\n\nDataset preprocessing\nVision: bilinear down-scale to 64×64 for MCU RAM budget, uint8→float32 normalise on device.\nAudio: 40-channel log-melspectrogram, 1×128 frame window, no overlap.\n\nData splitting\n• Train/val as official.\n• Online test: continuous real-time capture; first 1 min discarded for warm-up.\n\nRepetitions\n• 3 physical devices × 3 seeds per stream.\n\nEvaluation metrics\nPrimary  (edge): energy/frame (mJ) via INA260 sensor, average latency/frame, sustained FPS.\nSecondary: AUEC, collapse rate, rollback recovery success.\n\nComparisons / adjustments\n• NGFAT & RoTTA re-implemented in CMSIS-NN fixed-point; learning-rate retuned (η=5e-4) for stable MCU convergence.\n• ZORRO-lite = Fisher+shrinkage but without rollback (for memory ablation to fit 448 KB).\n\nHyper-parameter analysis\n• Measure sensitivity to gate entropy threshold δ ∈ {0.05,0.1,0.2} at MCU batch-size 1.\n\nRobustness tests\n• Noise-injection: live brightness oscillation (vision) and pink-noise burst (audio).\n• OOD domain transfer: suddenly switch webcam lighting to infrared illumination; evaluate gate reaction and rollback.\n\nCompute profiling\n• FLOPs/inference: measured by CubeMX cycle counter; memory footprint via map file.\n• Wall-clock cost on Jetson-Nano (GPU) vs STM32 (CPU) recorded.\n\nSelection / averaging\n• Report median over devices; 95% CI with bootstrap (1 k resamples).\n\nExample MCU code (fragment, C++)\n```c\nfor(;;){\n  capture_frame(buf);\n  preprocess(buf, scratch);\n  zorro_step_int8(model, scratch, &state);\n  transmit_logits(model.out);\n}\n```\nExpected outcome: On STM32, ZORRO-full keeps error <25% with 3.7 mJ/frame (-40% vs TENT) and zero collapses; ZORRO-lite fits RAM but shows higher collapse rate; NGFAT fails to converge on batch-1 streams. On Jetson-Nano, ZORRO saves ≥30% updates and 20% energy vs NGFAT while matching accuracy."
        }
    ],
    "expected_models": [
        "ResNet-20-GN",
        "MobileNet-V2-GN",
        "ViT-Tiny-RMS",
        "1D-CNN-Speech"
    ],
    "expected_datasets": [
        "CIFAR-10-C",
        "CIFAR-10.1",
        "TinyImageNet-C",
        "Google Speech Commands v2",
        "STM32 Webcam Stream"
    ]
}
