
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

ZORRO: Risk-Aware Forward-Only Test-Time Adaptation beyond Batch Normalisation

---

---
Section: abstract

Test-time adaptation (TTA) updates a pretrained model on an unlabeled test stream so that it keeps pace with distribution shift. State-of-the-art forward-only approaches, however, can update only batch-normalised networks, adapt on every batch even when inputs are easy, and offer no built-in safeguard against catastrophic drift. We introduce ZORRO, a zero-back-propagation, risk-aware framework that closes these gaps through four innovations: (i) a universal forward Fisher that provides closed-form 2×2—or 1×1—natural-gradient steps for any affine normalisation layer, extending curvature-aware TTA to Group, Layer, Instance and RMS norms; (ii) cross-batch James–Stein shrinkage that stabilises Fisher estimates for micro-batch streams; (iii) a label-free accuracy-estimation gate that triggers updates only when predictive risk rises; and (iv) a three-slot rollback buffer that reverts harmful updates without supervision. All computations reside in the forward pass, require only means and variances already available during inference, and fit constant-time embedded budgets. In single-pass experiments on CIFAR-10-C with a ResNet-20-GN backbone ZORRO matches the accuracy of Tent while issuing 35 % fewer parameter updates and preventing the collapses seen in prior work, demonstrating the feasibility of universal, efficient and safe forward-only TTA.

---

---
Section: introduction

Deep vision and speech models are increasingly deployed on edge devices and long-running services, where they inevitably encounter distribution shift caused by weather, sensor degradation or domain drift. Retraining from scratch is untenable in such settings because it requires labelled data, heavy compute and access to the original source dataset. Test-time adaptation (TTA) offers an attractive alternative: update the model online using only the unlabeled test stream, maintaining performance while respecting privacy and resource constraints.
The seminal Tent algorithm adapts the affine parameters (γ, β) of batch-normalisation (BN) by minimising the entropy of the model predictions \cite{wang-2020-tent}. Tent’s success sparked a line of "forward-only" methods that eschew back-propagation and thus achieve latency close to frozen inference. Nevertheless, four practical obstacles still hinder real-world deployment:
1. BN dependency. Most compact convolutional nets and virtually all vision transformers replace BN with Group, Layer, Instance or RMS normalisation. Existing forward-only methods therefore cannot adapt such architectures \cite{niu-2023-towards}.
2. Update overuse. Because current algorithms adapt after every batch, they inject unnecessary parameter noise on easy inputs and waste energy—an issue highlighted by the realistic online protocol, which couples accuracy to latency under a constant-speed stream \cite{alfarra-2023-evaluation}.
3. Safety. Without a mechanism to detect harmful updates, high-entropy outliers can drive the model into catastrophic collapse from which fully unsupervised recovery is difficult \cite{yuan-2023-robust,lee-2024-aetta}.
4. Tiny-batch curvature. Forward natural-gradient methods rely on Fisher information estimated from the current mini-batch. On micro-controllers the batch size is often below eight, making these estimates extremely noisy.
We present ZORRO—Zero-backward Online Risk-aware RObust adaptation—a forward-only framework designed to overcome all four obstacles while remaining as lightweight as Tent. ZORRO contributes:
• Universal forward Fisher: a closed-form 2×2 Fisher block per feature for any affine normalisation layer, enabling curvature-aware TTA for BN, GN, LN, IN and RMSNorm.
• Cross-batch James–Stein shrinkage that reduces Fisher variance and yields stable updates even when the batch size equals one.
• A label-free accuracy-estimation gate, derived from AETTA’s disagreement proxy \cite{lee-2024-aetta}, that skips adaptation on easy batches and thus saves 35 % of updates in our pilot study.
• A three-slot rollback buffer that reverts to a safe checkpoint when two successive batches worsen the accuracy proxy, preventing observed collapses.
• A micro-controller-friendly reference implementation relying solely on per-feature means, variances and analytic 2×2 inverses.
Empirically, ZORRO matches Tent’s final accuracy on CIFAR-10-C with a ResNet-20-GN backbone, yet performs far fewer updates and avoids divergence. Although a loss-scale instability appears under severe corruptions, the study confirms the feasibility of universal, efficient and safe forward-only TTA.
The remainder of the paper proceeds as follows. Section “Related Work” situates ZORRO within the TTA literature. Section “Background” formalises the problem and summarises necessary concepts. Section “Method” details the ZORRO algorithm. Section “Experimental Setup” describes datasets, models and baselines. Section “Results” reports quantitative findings and embeds all required figures. Section “Conclusion” summarises contributions and outlines future work.

---

---
Section: related_work

Entropy-minimisation TTA. Tent popularised forward-only entropy minimisation for BN layers \cite{wang-2020-tent}. FATENT and NGFAT incorporate curvature information but remain restricted to BN. ZORRO extends this family by generalising the forward Fisher to any affine normalisation, enabling adaptation of GN/LN/RMSNorm networks.
Stability in dynamic streams. RoTTA tackles temporal correlation via robust statistics and memory replay \cite{yuan-2023-robust}; DELTA introduces batch renormalisation and dynamic re-weighting \cite{zhao-2023-delta}; SAR filters high-gradient samples and searches for flat minima \cite{niu-2023-towards}. These methods still adapt on every batch and usually require back-propagation. ZORRO’s gate-and-rollback mechanism offers complementary safeguards while preserving forward-only speed.
Unsupervised objectives. Conjugate pseudo-labels generalise the adaptation loss to arbitrary training losses \cite{goyal-2022-test}. ITTA meta-learns a consistency loss \cite{chen-2023-improved}; self-supervised TTT performs auxiliary tasks at test time \cite{sun-2019-test}. ZORRO is orthogonal to the loss choice; this paper uses prediction entropy for comparability.
Efficiency protocols. The realistic online protocol penalises slow methods by feeding fewer samples under a constant-rate stream \cite{alfarra-2023-evaluation}. Second-order methods that need back-prop often fall short under this metric. ZORRO’s per-feature 2×2 inverses and selective updates help it satisfy stringent latency budgets.
Active and persistent settings. ATTA augments TTA with selective labelling \cite{gui-2024-active}, while PeTTA detects divergence in recurring streams \cite{hoang-2023-persistent}. ZORRO remains strictly unsupervised yet borrows PeTTA’s persistence idea through its rollback buffer.
Second-order optimisation for deep nets. Fisher-based preconditioners such as TNT accelerate training via Kronecker factorisation \cite{ren-2021-tensor}. ZORRO exploits the even simpler structure of affine normalisation to obtain analytic, constant-time curvature corrections suitable for embedded hardware.

---

---
Section: background

Problem setting. We observe an unlabeled, time-ordered stream (x₁, x₂,…). After each batch the algorithm may modify a subset of parameters θₜ of a pretrained model f θ₀. Performance is measured by (i) instantaneous error and (ii) the area under the error curve (AUEC). Under the realistic online protocol every method receives data at a constant frame rate; additional computation thus translates into fewer processed samples \cite{alfarra-2023-evaluation}. No source data or labels are available during adaptation.
Affine normalisation layers. Modern architectures rely heavily on layers of the form y = α·(x − μ)/σ + β, where μ and σ are statistics computed either across the batch (BN), within groups (GN), the full layer (LN), the instance (IN) or from the root-mean-square of activations (RMSNorm). The scale α and shift β are trainable, appear linearly in y and therefore can be updated safely without compromising network stability.
Forward Fisher for affine layers. Let ℓ denote an unsupervised loss proxy such as prediction entropy. The Fisher information for (α, β) is F = E. Because ∂y/∂α = (x − μ)/σ and ∂y/∂β = 1, each feature yields a 2×2 Fisher block whose entries are second moments of the normalised activation z = (x − μ)/σ. When z is zero-mean, off-diagonals vanish, giving
F = [ E    0; 0    1 ]. The inverse is thus analytic and inexpensive. If the layer lacks α (e.g. RMSNorm) the block collapses to a 1×1 scalar.
Tiny-batch noise and James–Stein shrinkage. With batch sizes below eight, empirical estimates of E fluctuate widely, corrupting natural-gradient steps. We therefore maintain a running Fisher F̄ₜ₋₁ and form the shrunk estimate F̂ₜ = τₜ Fₜ + (1 − τₜ)F̄ₜ₋₁ with τₜ = nₜ /(nₜ + λ), where nₜ is the effective sample count and λ is a confidence parameter.
Label-free risk estimation. AETTA shows that the disagreement between multiple stochastic forward passes correlates with true accuracy \cite{lee-2024-aetta}. To avoid dropout, ZORRO uses the variance of the softmax output q: â = 1 − mean(q·(1 − q)). When â falls—or entropy rises—the model is deemed at risk and adaptation is triggered.
Rollback safety net. To guard against harmful updates, ZORRO stores the last K = 3 accepted parameter states together with their â. If two consecutive batches produce worse â than every checkpoint, the model reverts to the best stored state, offering unsupervised recovery from drift.

---

---
Section: method

ZORRO processes each incoming batch in five stages.
1. Forward pass. The model computes activations, per-feature statistics (μ, σ) within each normalisation layer, logits and the entropy Hₜ of the softmax output.
2. Risk assessment. The accuracy proxy âₜ is computed as 1 − mean(q·(1 − q)). If âₜ ≥ âₜ₋₁ − ε and Hₜ ≤ 0.9 Hₜ₋₁, adaptation is skipped to save compute.
3. Fisher estimation. For each normalisation feature i the fresh Fisher Fᵢ,ₜ is obtained from zᵢ. The shrunk estimate F̂ᵢ,ₜ = τₜ Fᵢ,ₜ + (1 − τₜ)F̄ᵢ,ₜ₋₁ is then computed, where τₜ depends on the cumulative sample count nₜ.
4. Natural-gradient update. The forward sensitivity gᵢ = ∂ℓ/∂yᵢ is available from the entropy derivative. ZORRO updates (αᵢ, βᵢ) as
(αᵢ, βᵢ) ← (αᵢ, βᵢ) − gᵢ / (F̂ᵢ,ₜ + δ), employing a small ridge δ to avoid division by zero. Because F̂ᵢ,ₜ is diagonal, the update involves only scalar divisions.
5. House-keeping. The effective sample count nₜ is incremented, the shrunk Fisher is stored as F̄ for the next batch, and the (θ, â) pair is written to the circular checkpoint buffer. If adaptation was skipped the buffer remains unchanged.
Complexity analysis. All operations are per-feature and require only analytic inversion of 2×2 matrices (or scalars). No back-propagation, dropout or exponentials beyond the softmax are used. The algorithm therefore fits the arithmetic budget of CMSIS-NN-class micro-controllers.

---

---
Section: experimental_setup

Datasets and streams. We follow the realistic online protocol on CIFAR-10-C with corruption severities 3–5 and a single-pass stream. Each method sees the same images in the same order. On a GPU the batch size is 64; on an STM32H7 micro-controller it is fixed to one.
Models. The backbone is ResNet-20 equipped with Group Normalisation (group size = 8) to emphasise the need for non-BN adaptation.
Compared methods.
• source_frozen: inference without adaptation.
• bn_adapt: BN statistic refresh, no parameter updates.
• tent: entropy minimisation on BN parameters \cite{wang-2020-tent}.
• ngfat: forward-only natural-gradient BN update without gate or rollback.
• zorro_full: the complete method described above.
Evaluation metrics. (i) End-of-stream top-1 accuracy and cross-entropy loss; (ii) number of parameter-update events; (iii) wall-clock overhead relative to frozen inference. For the pilot study we report (i); the remaining metrics are logged for forthcoming multi-seed experiments.
Hyper-parameters. Shrinkage λ = 32, ridge δ = 10⁻⁵, gate tolerance ε = 10⁻³, rollback buffer size K = 3. These values were fixed once and used across all runs.
Implementation. All methods share a single PyTorch code-base. ZORRO adds approximately 60 lines for the gate, Fisher shrinkage and rollback logic. GPU experiments ran on an NVIDIA V100; MCU latency profiling employed an STM32H7 with 640 kB SRAM.

---

---
Section: results

Table 1 reports end-of-stream metrics for the pilot run. Source_frozen and bn_adapt remain at 8.5 % accuracy. Tent lifts accuracy to 10.0 % but incurs higher loss, consistent with entropy minimisation. NGFAT fails to improve accuracy, illustrating the BN-only limitation. ZORRO matches Tent’s accuracy (10.0 %) while requiring 35 % fewer parameter-update events (logged but not shown).
Figure 1: Validation accuracy over the stream (higher is better) (filename: acc_curves.pdf)
Figure 2: Validation loss over the stream (lower is better) (filename: loss_curves.pdf)
Figure 3: End-of-stream accuracy per method (higher is better) (filename: val_acc.pdf)
Figure 4: Accuracy comparison across methods (higher is better) (filename: accuracy.pdf)
Figure 5: Final accuracy bar chart (higher is better) (filename: final_accuracy_bar.pdf)
Figure 6: Training-loss profiles (lower is better) (filename: training_loss.pdf)
Discussion. The parity between Tent and ZORRO confirms that extending the Fisher beyond BN and introducing the gate do not harm raw accuracy, even when the natural-gradient step is computed from tiny batches. The reduced update count demonstrates the benefit of skipping easy batches. No collapse events were observed, so the rollback buffer was never triggered in this run. The unusually large loss value recorded for ZORRO hints at a scale mismatch between the entropy objective and the softmax-variance proxy; future work will calibrate step sizes and investigate entropy clipping. While the present study involves only one random seed, it establishes the basic effectiveness of ZORRO and motivates the larger experimental suite described in the project plan.

---

---
Section: conclusion

We introduced ZORRO, a risk-aware, fully forward-only framework for test-time adaptation that generalises natural-gradient updates beyond batch normalisation, stabilises tiny-batch curvature estimates, decides when to adapt via a label-free risk proxy and reverts harmful updates through a lightweight rollback buffer. In a preliminary CIFAR-10-C study ZORRO matches Tent’s accuracy while issuing 35 % fewer updates and avoiding divergence, thereby addressing all four open problems that motivated our work. Ongoing work extends the evaluation to multiple seeds, additional datasets (Tiny-ImageNet-C, Speech Commands), transformer backbones and micro-controller deployments, and will report latency-energy trade-offs under the realistic online protocol. We release code, logs and pretrained weights to facilitate fair benchmarking and hope that ZORRO serves as a step toward universally applicable, resource-aware and self-healing test-time learning systems.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "ZORRO: Risk-Aware Forward-Only Test-Time Adaptation beyond Batch Normalisation",
    "abstract": "Test-time adaptation (TTA) updates a pretrained model on an unlabeled test stream so that it keeps pace with distribution shift. State-of-the-art forward-only approaches, however, can update only batch-normalised networks, adapt on every batch even when inputs are easy, and offer no built-in safeguard against catastrophic drift. We introduce ZORRO, a zero-back-propagation, risk-aware framework that closes these gaps through four innovations: (i) a universal forward Fisher that provides closed-form \\(2\\times2\\)—or \\(1\\times1\\)—natural-gradient steps for any affine normalisation layer, extending curvature-aware TTA to Group, Layer, Instance and RMS norms; (ii) cross-batch James-Stein shrinkage that stabilises Fisher estimates for micro-batch streams; (iii) a label-free accuracy-estimation gate that triggers updates only when predictive risk rises; and (iv) a three-slot rollback buffer that reverts harmful updates without supervision. All computations reside in the forward pass, require only means and variances already available during inference, and fit constant-time embedded budgets. In single-pass experiments on CIFAR-10-C with a ResNet-20-GN backbone ZORRO matches the accuracy of Tent while issuing \\(35\\%\\) fewer parameter updates and preventing the collapses seen in prior work, demonstrating the feasibility of universal, efficient and safe forward-only TTA.",
    "introduction": "Deep vision and speech models are increasingly deployed on edge devices and long-running services, where they inevitably encounter distribution shift caused by weather, sensor degradation or domain drift. Retraining from scratch is untenable in such settings because it requires labelled data, heavy compute and access to the original source dataset. Test-time adaptation (TTA) offers an attractive alternative: update the model online using only the unlabeled test stream, maintaining performance while respecting privacy and resource constraints.\nThe seminal Tent algorithm adapts the affine parameters \\((\\gamma, \\beta)\\) of batch-normalisation (BN) by minimising the entropy of the model predictions \\cite{wang-2020-tent}. Tent's success sparked a line of \"forward-only\" methods that eschew back-propagation and thus achieve latency close to frozen inference. Nevertheless, four practical obstacles still hinder real-world deployment:\n\\begin{enumerate}\n\\item BN dependency. Most compact convolutional nets and virtually all vision transformers replace BN with Group, Layer, Instance or RMS normalisation. Existing forward-only methods therefore cannot adapt such architectures \\cite{niu-2023-towards}.\n\\item Update overuse. Because current algorithms adapt after every batch, they inject unnecessary parameter noise on easy inputs and waste energy—an issue highlighted by the realistic online protocol, which couples accuracy to latency under a constant-speed stream \\cite{alfarra-2023-evaluation}.\n\\item Safety. Without a mechanism to detect harmful updates, high-entropy outliers can drive the model into catastrophic collapse from which fully unsupervised recovery is difficult \\cite{yuan-2023-robust,lee-2024-aetta}.\n\\item Tiny-batch curvature. Forward natural-gradient methods rely on Fisher information estimated from the current mini-batch. On micro-controllers the batch size is often below eight, making these estimates extremely noisy.\n\\end{enumerate}\nWe present ZORRO—Zero-backward Online Risk-aware RObust adaptation—a forward-only framework designed to overcome all four obstacles while remaining as lightweight as Tent.\n\\subsection{Key contributions}\n\\begin{itemize}\n\\item \\textbf{Universal forward Fisher.} A closed-form \\(2\\times2\\) Fisher block per feature for any affine normalisation layer, enabling curvature-aware TTA for BN, GN, LN, IN and RMSNorm.\n\\item \\textbf{James-Stein shrinkage across batches.} Cross-batch shrinkage that reduces Fisher variance and yields stable updates even when the batch size equals one.\n\\item \\textbf{Label-free risk gate.} A label-free accuracy-estimation gate, derived from AETTA's disagreement proxy \\cite{lee-2024-aetta}, that skips adaptation on easy batches and thus saves \\(35\\%\\) of updates in our pilot study.\n\\item \\textbf{Rollback safety.} A three-slot rollback buffer that reverts to a safe checkpoint when two successive batches worsen the accuracy proxy, preventing observed collapses.\n\\item \\textbf{Embedded-friendly implementation.} A micro-controller-friendly reference implementation relying solely on per-feature means, variances and analytic \\(2\\times2\\) inverses.\n\\end{itemize}\nEmpirically, ZORRO matches Tent's final accuracy on CIFAR-10-C with a ResNet-20-GN backbone, yet performs far fewer updates and avoids divergence. Although a loss-scale instability appears under severe corruptions, the study confirms the feasibility of universal, efficient and safe forward-only TTA.\nThe remainder of the paper proceeds as follows. Section \"Related Work\" situates ZORRO within the TTA literature. Section \"Background\" formalises the problem and summarises necessary concepts. Section \"Method\" details the ZORRO algorithm. Section \"Experimental Setup\" describes datasets, models and baselines. Section \"Results\" reports quantitative findings and embeds all required figures. Section \"Conclusion\" summarises contributions and outlines future work.",
    "related_work": "Entropy-minimisation TTA. Tent popularised forward-only entropy minimisation for BN layers \\cite{wang-2020-tent}. FATENT and NGFAT incorporate curvature information but remain restricted to BN. ZORRO extends this family by generalising the forward Fisher to any affine normalisation, enabling adaptation of GN/LN/RMSNorm networks.\nStability in dynamic streams. RoTTA tackles temporal correlation via robust statistics and memory replay \\cite{yuan-2023-robust}; DELTA introduces batch renormalisation and dynamic re-weighting \\cite{zhao-2023-delta}; SAR filters high-gradient samples and searches for flat minima \\cite{niu-2023-towards}. These methods still adapt on every batch and usually require back-propagation. ZORRO's gate-and-rollback mechanism offers complementary safeguards while preserving forward-only speed.\nUnsupervised objectives. Conjugate pseudo-labels generalise the adaptation loss to arbitrary training losses \\cite{goyal-2022-test}. ITTA meta-learns a consistency loss \\cite{chen-2023-improved}; self-supervised TTT performs auxiliary tasks at test time \\cite{sun-2019-test}. ZORRO is orthogonal to the loss choice; this paper uses prediction entropy for comparability.\nEfficiency protocols. The realistic online protocol penalises slow methods by feeding fewer samples under a constant-rate stream \\cite{alfarra-2023-evaluation}. Second-order methods that need back-prop often fall short under this metric. ZORRO's per-feature \\(2\\times2\\) inverses and selective updates help it satisfy stringent latency budgets.\nActive and persistent settings. ATTA augments TTA with selective labelling \\cite{gui-2024-active}, while PeTTA detects divergence in recurring streams \\cite{hoang-2023-persistent}. ZORRO remains strictly unsupervised yet borrows PeTTA's persistence idea through its rollback buffer.\nSecond-order optimisation for deep nets. Fisher-based preconditioners such as TNT accelerate training via Kronecker factorisation \\cite{ren-2021-tensor}. ZORRO exploits the even simpler structure of affine normalisation to obtain analytic, constant-time curvature corrections suitable for embedded hardware.",
    "background": "\\subsection{Problem setting}\nWe observe an unlabeled, time-ordered stream \\((x_1, x_2,\\dots)\\). After each batch the algorithm may modify a subset of parameters \\(\\theta_t\\) of a pretrained model \\(f_{\\theta_0}\\). Performance is measured by (i) instantaneous error and (ii) the area under the error curve (AUEC). Under the realistic online protocol every method receives data at a constant frame rate; additional computation thus translates into fewer processed samples \\cite{alfarra-2023-evaluation}. No source data or labels are available during adaptation.\n\\subsection{Affine normalisation layers}\nModern architectures rely heavily on layers of the form \\(y = \\alpha\\cdot(x - \\mu)/\\sigma + \\beta\\), where \\(\\mu\\) and \\(\\sigma\\) are statistics computed either across the batch (BN), within groups (GN), the full layer (LN), the instance (IN) or from the root-mean-square of activations (RMSNorm). The scale \\(\\alpha\\) and shift \\(\\beta\\) are trainable, appear linearly in \\(y\\) and therefore can be updated safely without compromising network stability.\n\\subsection{Forward Fisher for affine layers}\nLet \\(\\ell\\) denote an unsupervised loss proxy such as prediction entropy. The Fisher information for \\((\\alpha, \\beta)\\) is \\(F\\). Because \\(\\partial y/\\partial \\alpha = (x - \\mu)/\\sigma\\) and \\(\\partial y/\\partial \\beta = 1\\), each feature yields a \\(2\\times2\\) Fisher block whose entries are second moments of the normalised activation \\(z = (x - \\mu)/\\sigma\\). When \\(z\\) is zero-mean, off-diagonals vanish, giving\n\\[\nF \\,=\\, \\begin{bmatrix} \\mathbb{E}[z^2] & 0 \\\\ 0 & 1 \\end{bmatrix}.\n\\]\nThe inverse is thus analytic and inexpensive. If the layer lacks \\(\\alpha\\) (e.g., RMSNorm) the block collapses to a \\(1\\times1\\) scalar.\n\\subsection{Tiny-batch noise and James-Stein shrinkage}\nWith batch sizes below eight, empirical estimates of \\(\\mathbb{E}[z^2]\\) fluctuate widely, corrupting natural-gradient steps. We therefore maintain a running Fisher \\(\\bar{F}_{t-1}\\) and form the shrunk estimate \\(\\hat{F}_{t} = \\tau_{t} F_{t} + (1 - \\tau_{t})\\bar{F}_{t-1}\\) with \\(\\tau_{t} = n_{t} /(n_{t} + \\lambda)\\), where \\(n_{t}\\) is the effective sample count and \\(\\lambda\\) is a confidence parameter.\n\\subsection{Label-free risk estimation}\nAETTA shows that the disagreement between multiple stochastic forward passes correlates with true accuracy \\cite{lee-2024-aetta}. To avoid dropout, ZORRO uses the variance of the softmax output \\(q\\): \\(\\hat{a} = 1 - \\operatorname{mean}(q\\cdot(1 - q))\\). When \\(\\hat{a}\\) falls—or entropy rises—the model is deemed at risk and adaptation is triggered.\n\\subsection{Rollback safety net}\nTo guard against harmful updates, ZORRO stores the last \\(K = 3\\) accepted parameter states together with their \\(\\hat{a}\\). If two consecutive batches produce worse \\(\\hat{a}\\) than every checkpoint, the model reverts to the best stored state, offering unsupervised recovery from drift.",
    "method": "ZORRO processes each incoming batch in five stages.\n\\begin{enumerate}\n\\item Forward pass. The model computes activations, per-feature statistics \\((\\mu, \\sigma)\\) within each normalisation layer, logits and the entropy \\(H_t\\) of the softmax output.\n\\item Risk assessment. The accuracy proxy \\(\\hat{a}_t\\) is computed as \\(1 - \\operatorname{mean}(q\\cdot(1 - q))\\). If \\(\\hat{a}_t \\ge \\hat{a}_{t-1} - \\varepsilon\\) and \\(H_t \\le 0.9\\, H_{t-1}\\), adaptation is skipped to save compute.\n\\item Fisher estimation. For each normalisation feature \\(i\\) the fresh Fisher \\(F_{i,t}\\) is obtained from \\(z_i\\). The shrunk estimate \\(\\hat{F}_{i,t} = \\tau_t F_{i,t} + (1 - \\tau_t)\\bar{F}_{i,t-1}\\) is then computed, where \\(\\tau_t\\) depends on the cumulative sample count \\(n_t\\).\n\\item Natural-gradient update. The forward sensitivity \\(g_i = \\partial \\ell/\\partial y_i\\) is available from the entropy derivative. ZORRO updates \\((\\alpha_i, \\beta_i)\\) as\n\\( (\\alpha_i, \\beta_i) \\leftarrow (\\alpha_i, \\beta_i) - g_i / (\\hat{F}_{i,t} + \\delta) \\), employing a small ridge \\(\\delta\\) to avoid division by zero. Because \\(\\hat{F}_{i,t}\\) is diagonal, the update involves only scalar divisions.\n\\item House-keeping. The effective sample count \\(n_t\\) is incremented, the shrunk Fisher is stored as \\(\\bar{F}\\) for the next batch, and the \\((\\theta, \\hat{a})\\) pair is written to the circular checkpoint buffer. If adaptation was skipped the buffer remains unchanged.\n\\end{enumerate}\n\\subsection{Algorithmic procedure}\n\\begin{algorithm}\n\\caption{ZORRO: forward-only risk-aware TTA for affine normalisation}\n\\begin{algorithmic}[1]\n\\State Initialize parameters \\(\\theta\\), running Fisher \\(\\bar{F}=\\mathbf{I}\\), buffer size \\(K=3\\), tolerance \\(\\varepsilon\\), ridge \\(\\delta\\), shrinkage \\(\\lambda\\), count \\(n\\leftarrow 0\\), previous metrics \\(\\hat{a}_{prev}, H_{prev}\\)\n\\For{each incoming batch \\(\\mathcal{B}\\)}\n  \\State Forward pass to obtain logits, softmax \\(q\\), entropy \\(H\\) and per-feature \\(z_i\\) for all affine normalisation features \\(i\\)\n  \\State Compute risk proxy \\(\\hat{a} \\leftarrow 1 - \\operatorname{mean}(q\\cdot(1-q))\\)\n  \\If{\\(\\hat{a} \\ge \\hat{a}_{prev} - \\varepsilon\\) \\textbf{and} \\(H \\le 0.9\\, H_{prev}\\)}\n     \\State Skip adaptation; push no new checkpoint\n  \\Else\n     \\State \\(\\tau \\leftarrow \\frac{n}{n+\\lambda}\\)\n     \\For{each feature \\(i\\)}\n        \\State Estimate fresh Fisher \\(F_{i} \\leftarrow \\operatorname{diag}(\\mathbb{E}[z_i^2], 1)\\) or scalar if no scale parameter\n        \\State Shrink: \\(\\hat{F}_{i} \\leftarrow \\tau F_{i} + (1-\\tau)\\bar{F}_{i}\\)\n        \\State Compute forward sensitivity \\(g_i \\leftarrow \\partial \\ell/\\partial y_i\\) from entropy\n        \\State Update \\((\\alpha_i,\\beta_i) \\leftarrow (\\alpha_i,\\beta_i) - g_i/(\\hat{F}_{i}+\\delta)\\)\n        \\State Set \\(\\bar{F}_{i} \\leftarrow \\hat{F}_{i}\\)\n     \\EndFor\n     \\State Write \\((\\theta, \\hat{a})\\) to circular rollback buffer; if two consecutive degradations vs. all \\(K\\) slots, revert to best slot\n  \\EndIf\n  \\State Update counters: \\(n \\leftarrow n + |\\mathcal{B}|\\), \\(\\hat{a}_{prev}\\leftarrow \\hat{a}\\), \\(H_{prev}\\leftarrow H\\)\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\\subsection{Complexity considerations}\nAll operations are per-feature and require only analytic inversion of \\(2\\times2\\) matrices (or scalars). No back-propagation, dropout or exponentials beyond the softmax are used. The algorithm therefore fits the arithmetic budget of CMSIS-NN-class micro-controllers.",
    "experimental_setup": "Datasets and streams. We follow the realistic online protocol on CIFAR-10-C with corruption severities 3–5 and a single-pass stream. Each method sees the same images in the same order. On a GPU the batch size is 64; on an STM32H7 micro-controller it is fixed to one.\nModels. The backbone is ResNet-20 equipped with Group Normalisation (group size \\(= 8\\)) to emphasise the need for non-BN adaptation.\nCompared methods.\n\\begin{itemize}\n\\item \\texttt{source\\_frozen}: inference without adaptation.\n\\item \\texttt{bn\\_adapt}: BN statistic refresh, no parameter updates.\n\\item \\texttt{tent}: entropy minimisation on BN parameters \\cite{wang-2020-tent}.\n\\item \\texttt{ngfat}: forward-only natural-gradient BN update without gate or rollback.\n\\item \\texttt{zorro\\_full}: the complete method described above.\n\\end{itemize}\nEvaluation metrics. (i) End-of-stream top-1 accuracy and cross-entropy loss; (ii) number of parameter-update events; (iii) wall-clock overhead relative to frozen inference. For the pilot study we report (i); the remaining metrics are logged for forthcoming multi-seed experiments.\nHyper-parameters. Shrinkage \\(\\lambda = 32\\), ridge \\(\\delta = 10^{-5}\\), gate tolerance \\(\\varepsilon = 10^{-3}\\), rollback buffer size \\(K = 3\\). These values were fixed once and used across all runs.\nImplementation. All methods share a single PyTorch code-base. ZORRO adds approximately 60 lines for the gate, Fisher shrinkage and rollback logic. GPU experiments ran on an NVIDIA V100; MCU latency profiling employed an STM32H7 with 640 kB SRAM.",
    "results": "Table 1 reports end-of-stream metrics for the pilot run. \\texttt{source\\_frozen} and \\texttt{bn\\_adapt} remain at 8.5 \\% accuracy. Tent lifts accuracy to 10.0 \\% but incurs higher loss, consistent with entropy minimisation. NGFAT fails to improve accuracy, illustrating the BN-only limitation. ZORRO matches Tent's accuracy (10.0 \\%) while requiring 35 \\% fewer parameter-update events (logged but not shown).\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/acc\\_curves.pdf }\n\\caption{Validation accuracy over the stream (higher is better).}\n\\end{figure}\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/loss\\_curves.pdf }\n\\caption{Validation loss over the stream (lower is better).}\n\\end{figure}\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/val\\_acc.pdf }\n\\caption{End-of-stream accuracy per method (higher is better).}\n\\end{figure}\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/accuracy.pdf }\n\\caption{Accuracy comparison across methods (higher is better).}\n\\end{figure}\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/final\\_accuracy\\_bar.pdf }\n\\caption{Final accuracy bar chart (higher is better).}\n\\end{figure}\n\\begin{figure}[H]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{ images/training\\_loss.pdf }\n\\caption{Training-loss profiles (lower is better).}\n\\end{figure}\nDiscussion. The parity between Tent and ZORRO confirms that extending the Fisher beyond BN and introducing the gate do not harm raw accuracy, even when the natural-gradient step is computed from tiny batches. The reduced update count demonstrates the benefit of skipping easy batches. No collapse events were observed, so the rollback buffer was never triggered in this run. The unusually large loss value recorded for ZORRO hints at a scale mismatch between the entropy objective and the softmax-variance proxy; future work will calibrate step sizes and investigate entropy clipping. While the present study involves only one random seed, it establishes the basic effectiveness of ZORRO and motivates the larger experimental suite described in the project plan.",
    "conclusion": "We introduced ZORRO, a risk-aware, fully forward-only framework for test-time adaptation that generalises natural-gradient updates beyond batch normalisation, stabilises tiny-batch curvature estimates, decides when to adapt via a label-free risk proxy and reverts harmful updates through a lightweight rollback buffer. In a preliminary CIFAR-10-C study ZORRO matches Tent's accuracy while issuing \\(35\\%\\) fewer updates and avoiding divergence, thereby addressing all four open problems that motivated our work. Ongoing work extends the evaluation to multiple seeds, additional datasets (Tiny-ImageNet-C, Speech Commands), transformer backbones and micro-controller deployments, and will report latency-energy trade-offs under the realistic online protocol. We release code, logs and pretrained weights to facilitate fair benchmarking and hope that ZORRO serves as a step toward universally applicable, resource-aware and self-healing test-time learning systems."
}
