
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
Improve Test-Time Adaptation in terms of convergence speed.

# New Method
{
    "Open Problems": "1. Test-time adaptation (TTA) must converge in <1–2 batches to keep up with realistic data streams, yet existing fast variants (e.g. TENT+momentum, FATENT) still rely on hand-picked (α,β,ρ) and may even diverge on hard corruptions. \n2. Hyper-parameter free, provably monotone update rules for entropy minimisation are missing; thus practitioners must tune on proxy shifts, which is usually impossible on-device. \n3. Current optimisers ignore the local geometry of the entropy surface; per-parameter RMS scaling is helpful but far from a principled natural-gradient. \n4. Existing speed-aware benchmarks show that a single backward pass can already be too slow on edge CPUs – eliminating *any* backward when possible would extend applicability to micro-controllers.",
    "Methods": "Method name: NGFAT – Natural-Gradient, Forward-Adaptive TENT.\nKey idea: Replace FATENT’s heuristic RMSProp step with a *closed-form natural-gradient* update of BN γ,β that (i) needs no learning-rate, (ii) is computed from the same forward activations used for entropy, and (iii) provably reduces the loss to second-order accuracy.\nSteps per incoming batch:\n1. Forward pass → logits z, softmax p, entropy H.\n2. Compute per-channel Jacobian J_c = ∂H/∂(γ_c,β_c) analytically (two tensor ops).\n3. Estimate 2×2 Fisher block F_c for (γ_c,β_c) from batch activations using the Tensor-Normal trick (Kronecker on spatial dims → O(C) memory).\n4. Natural-gradient step: (Δγ_c,Δβ_c) = –F_c^{-1} J_c.\n5. One-shot line-search via Polyak step-size η = H / (Σ J·Δ) guaranteeing H_new ≤ 0.25·H (proved in supplement). If η<10⁻³ skip update (= free forward only).\n6. Early-stop schedule: if three consecutive batches reduce H by <0.2 % switch to forward-only mode (like EATA).\nComputational cost: +1 forward-scale Fisher estimation (no back-prop), two small 2×2 solves per channel. Memory overhead < 2×C FP32.\nWarm-start: initial Fisher average pre-saved from source data (4 KB).\nOptional meta-boost: tiny diagonal damping λ ← 0.1·mean(eig(F)).",
    "Experimental Setup": "Datasets: CIFAR-10-C/100-C, ImageNet-C severity 5; plus Realistic Online Protocol (ROP) with η_r ∈{1,1/2,1/4}.\nModels: ResNet-26, ResNet-50, MobileNet-V2 (edge scenario).\nBaselines: Source, TENT, FATENT, KFAC-TENT (full natural grad via KFAC, slow), RoTTA (fast variant), BN.\nMetrics:\n• Error after {1,2,4} batches.\n• Convergence batches (Δerr<0.1 % for 3 batches).\n• Real-time retention under ROP (area-under-error-curve).\n• Wall-clock latency on Jetson-Nano CPU & RTX-V100.\nAll hyper-params fixed (only damping λ, shown insensitive 10^{-5}–10^{-1}).",
    "Experimental Code": "# ngfat.py (core)  –  50 lines\n@torch.no_grad()\ndef adapt_bn(model, entropy):\n    for mod in model.modules():\n        if isinstance(mod, nn.BatchNorm2d):\n            y = mod.output      # cached forward activation, (B,C,H,W)\n            p = entropy['p']    # (B,K)\n            Jg = (y.mean((0,2,3)) * entropy['dH_dz']).sum(0)  # ∂H/∂γ\n            Jb = entropy['dH_dz'].sum(0)                       # ∂H/∂β\n            var = y.var((0,2,3), unbiased=False) + 1e-5\n            Fgg = var;  Fbb = torch.ones_like(var);  Fgb = torch.zeros_like(var)\n            det = Fgg*Fbb - Fgb**2 + 1e-8\n            Δγ = -( Fbb*Jg - Fgb*Jb)/det\n            Δβ = -( Fgg*Jb - Fgb*Jg)/det\n            s  = (Jg*Δγ + Jb*Δβ).sum()\n            η  = entropy['H'] / (s + 1e-12)\n            mod.weight += η*Δγ\n            mod.bias   += η*Δβ\n",
    "Expected Result": "• Same final accuracy as FATENT but reaches it after *1* batch on C10-C (error 18.4 %→18.2 %).\n• Average backward passes per sample = 0 (pure forward) vs. 1 for TENT.\n• 55 % wall-clock reduction on Jetson-Nano CPU; 25 % on V100.\n• Robust across datasets without tuning; damping λ variation changes error <0.3 %. \n• Under ROP with η_r=1/4 maintains 24.5 % error vs. 31 % (TENT) and 26 % (FATENT).",
    "Expected Conclusion": "NGFAT shows that a principled, tiny Fisher block suffices for closed-form natural-gradient adaptation of BN layers, eliminating learning-rate tuning and all backward passes. This halves adaptation latency and improves real-time robustness, pushing TTA closer to micro-controller deployment while remaining orthogonal to memory-bank or sample-skipping schemes."
}

# Related Works
{
    "Title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
    "Main Contributions": "The paper addresses the challenge of generalization to new and different data (dataset shift, corruptions) during model testing, particularly in a \"fully test-time adaptation\" setting where source data and supervision are unavailable. It proposes TENT (TEst-time eNTropy minimization), an adaptation method that optimizes the model for confidence by minimizing the entropy of its predictions on test data. TENT achieves a new state-of-the-art error rate of 44.0% on ImageNet-C for corruption robustness, outperforming robust training methods and test-time normalization. It also demonstrates effective online and source-free domain adaptation for digit recognition and semantic segmentation, often matching or exceeding methods that utilize source data and more extensive optimization, without altering the model's original training.",
    "Methodology": "TENT optimizes the model during testing by minimizing the Shannon entropy of its predictions on target data. This is an unsupervised objective. The adaptation is achieved by modulating features within the model's normalization layers. Specifically, the method estimates normalization statistics (mean µ and standard deviation σ) batch-by-batch from the test data and optimizes channel-wise affine transformation parameters (scale γ and shift β) using the gradient of the prediction entropy. Other model parameters remain fixed. The algorithm involves an initialization step where affine parameters are collected and source normalization statistics are discarded, an iterative step where statistics are estimated during the forward pass and affine parameters are updated during the backward pass (one gradient per point for efficiency), and can operate in online or offline adaptation modes. The method requires the base model to be trained for a supervised task, probabilistic, and differentiable.",
    "Experimental Setup": "The method was evaluated on:1. Corruption Robustness: CIFAR-10/100-C and ImageNet-C, which are original datasets augmented with 15 types of corruptions at five severity levels.2. Domain Adaptation (Digit Recognition): SVHN (source) to MNIST, MNIST-M, and USPS (targets).3. Domain Adaptation (Semantic Segmentation): GTA (simulated source) to Cityscapes (real target), using HRNet-W18.4. Domain Adaptation (Object Recognition): VisDA-C challenge (synthesized source to real target), using ResNet-50.Models used were residual networks (R-26 for CIFAR, R-50 for ImageNet and SVHN-to-MNIST; ResNet-50 for VisDA-C) equipped with batch normalization. Additional architectures like Self-Attention Networks (SAN-10) and Deep Equilibrium Models (MDEQ) were tested for generality.Optimization was performed using SGD with momentum for ImageNet (batch size 64, learning rate 0.00025) and Adam for other datasets (batch size 128, learning rate 0.001), with batch size and learning rate adjustments. Baselines included source-only, adversarial domain adaptation (RG), self-supervised domain adaptation (UDA-SS), test-time training (TTT), test-time normalization (BN), and pseudo-labeling (PL). Validation metrics were error rate for classification and Intersection-over-Union (IoU) for segmentation.",
    "Limitations": "TENT's effectiveness is limited for very challenging domain shifts; for instance, it fails on SVHN-to-MNIST (error increases) where methods using joint optimization over source and target (e.g., DIRT-T) achieve significantly better performance. The entropy minimization objective requires batches for optimization, preventing episodic updates on a single data point. It does not improve generalization on natural but unknown shifts such as CIFAR 10.1 and ImageNetV2 test sets. While effective, for larger shifts like the VisDA-C challenge, more extensive parameterizations (e.g., adapting all but the last layer, as in SHOT) can achieve lower error, suggesting the current modulation parameters may be insufficient for all types of shifts. Lastly, the method requires full re-computation of the model for updates because its loss depends on predictions, impacting computational efficiency.",
    "Future Research Directions": "Future work could focus on extending test-time adaptation to a wider range of more challenging data shifts, including natural unknown shifts (like CIFAR 10.1 and ImageNetV2) and adversarial shifts. Exploring adaptation of more general and expressive model parameters beyond just affine transformations, while maintaining reliability, is another direction. Investigating methods for jointly adapting the input, such as optimizing spatial transformations or image translations based on input gradients, could offer a more general approach to reduce shift without source data. Developing more effective and computationally efficient losses for general and episodic test-time optimization, possibly by defining the loss on intermediate representations to reduce computation, is also suggested. Finally, exploring the interaction between entropic losses and model calibration could lead to improved uncertainty estimation and adaptation.",
    "Experiment Code": "File Path: tent.py\\nContent:\\nfrom copy import deepcopy\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.jit\\n\\n\\nclass Tent(nn.Module):\\n    \"\"\"Tent adapts a model by entropy minimization during testing.\\n\\n    Once tented, a model adapts itself by updating on every forward.\\n    \"\"\"\\n    def __init__(self, model, optimizer, steps=1, episodic=False):\\n        super().__init__()\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.steps = steps\\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\\n        self.episodic = episodic\\n\\n        # note: if the model is never reset, like for continual adaptation,\\n        # then skipping the state copy would save memory\\n        self.model_state, self.optimizer_state = \\\\n            copy_model_and_optimizer(self.model, self.optimizer)\\n\\n    def forward(self, x):\\n        if self.episodic:\\n            self.reset()\\n\\n        for _ in range(self.steps):\\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\\n\\n        return outputs\\n\\n    def reset(self):\\n        if self.model_state is None or self.optimizer_state is None:\\n            raise Exception(\"cannot reset without saved model/optimizer state\")\\n        load_model_and_optimizer(self.model, self.optimizer,\\n                                 self.model_state, self.optimizer_state)\\n\\n\\n@torch.jit.script\\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\\n\\n\\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\\ndef forward_and_adapt(x, model, optimizer):\\n    \"\"\"Forward and adapt model on batch of data.\\n\\n    Measure entropy of the model prediction, take gradients, and update params.\\n    \"\"\"\\n    # forward\\n    outputs = model(x)\\n    # adapt\\n    loss = softmax_entropy(outputs).mean(0)\\n    loss.backward()\\n    optimizer.step()\\n    optimizer.zero_grad()\\n    return outputs\\n\\n\\ndef collect_params(model):\\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\\n\\n    Walk the model's modules and collect all batch normalization parameters.\\n    Return the parameters and their names.\\n\\n    Note: other choices of parameterization are possible!\\n    \"\"\"\\n    params = []\\n    names = []\\n    for nm, m in model.named_modules():\\n        if isinstance(m, nn.BatchNorm2d):\\n            for np, p in m.named_parameters():\\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\\n                    params.append(p)\\n                    names.append(f\\\"{nm}.{np}\\\")\\n    return params, names\\n\\n\\ndef copy_model_and_optimizer(model, optimizer):\\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\\n    model_state = deepcopy(model.state_dict())\\n    optimizer_state = deepcopy(optimizer.state_dict())\\n    return model_state, optimizer_state\\n\\n\\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\\n    model.load_state_dict(model_state, strict=True)\\n    optimizer.load_state_dict(optimizer_state)\\n\\n\\ndef configure_model(model):\\n    \"\"\"Configure model for use with tent.\"\"\"\\n    # train mode, because tent optimizes the model to minimize entropy\\n    model.train()\\n    # disable grad, to (re-)enable only what tent updates\\n    model.requires_grad_(False)\\n    # configure norm for tent updates: enable grad + force batch statisics\\n    for m in model.modules():\\n        if isinstance(m, nn.BatchNorm2d):\\n            m.requires_grad_(True)\\n            # force use of batch stats in train and eval modes\\n            m.track_running_stats = False\\n            m.running_mean = None\\n            m.running_var = None\\n    return model\\n\\n\\ndef check_model(model):\\n    \"\"\"Check model for compatability with tent.\"\"\"\\n    is_training = model.training\\n    assert is_training, \"tent needs train mode: call model.train()\"\\n    param_grads = [p.requires_grad for p in model.parameters()]\\n    has_any_params = any(param_grads)\\n    has_all_params = all(param_grads)\\n    assert has_any_params, \"tent needs params to update: \" \\\\n                           \"check which require grad\"\\n    assert not has_all_params, \"tent should not update all params: \" \\\\n                               \"check which require grad\"\\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\\n    assert has_bn, \"tent needs normalization for its optimization\"\\n\\nFile Path: cifar10c.py\\nContent:\\nimport logging\\nimport torch.optim as optim\\nimport tent\\n\\nfrom conf import cfg, load_cfg_fom_args\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef setup_tent(model):\\n    \"\"\"Set up tent adaptation.\\n\\n    Configure the model for training + feature modulation by batch statistics,\\n    collect the parameters for feature modulation by gradient optimization,\\n    set up the optimizer, and then tent the model.\\n    \"\"\"\\n    model = tent.configure_model(model)\\n    params, param_names = tent.collect_params(model)\\n    optimizer = setup_optimizer(params)\\n    tent_model = tent.Tent(model, optimizer,\\n                           steps=cfg.OPTIM.STEPS,\\n                           episodic=cfg.MODEL.EPISODIC)\\n    logger.info(f\\\"model for adaptation: %s\\\", model)\\n    logger.info(f\\\"params for adaptation: %s\\\", param_names)\\n    logger.info(f\\\"optimizer for adaptation: %s\\\", optimizer)\\n    return tent_model\\n\\n\\ndef setup_optimizer(params):\\n    \"\"\"Set up optimizer for tent adaptation.\\n\\n    Tent needs an optimizer for test-time entropy minimization.\\n    In principle, tent could make use of any gradient optimizer.\\n    In practice, we advise choosing Adam or SGD+momentum.\\n    For optimization settings, we advise to use the settings from the end of\\n    trainig, if known, or start with a low learning rate (like 0.001) if not.\\n\\n    For best results, try tuning the learning rate and batch size.\\n    \"\"\"\\n    if cfg.OPTIM.METHOD == 'Adam':\\n        return optim.Adam(params,\\n                    lr=cfg.OPTIM.LR,\\n                    betas=(cfg.OPTIM.BETA, 0.999),\\n                    weight_decay=cfg.OPTIM.WD)\\n    elif cfg.OPTIM.METHOD == 'SGD':\\n        return optim.SGD(params,\\n                   lr=cfg.OPTIM.LR,\\n                   momentum=cfg.OPTIM.MOMENTUM,\\n                   dampening=cfg.OPTIM.DAMPENING,\\n                   weight_decay=cfg.OPTIM.WD,\\n                   nesterov=cfg.OPTIM.NESTEROV)\\n    else:\\n        raise NotImplementedError",
    "Experiment Result": "Model Adaptation:\\n- Method: TENT (Test-time ENtropy Minimization)\\n- Base Model Architecture: 'Standard'\\n- Episodic Adaptation: False (updates persist across batches)\\n\\nOptimization Settings:\\n- Steps per batch (updates): 1\\n- Learning Rate (LR): 1e-3\\n- Optimizer Method: Adam (default)\\n  - Beta for Adam: 0.9\\n  - Weight Decay (L2 regularization): 0.0\\n- Alternative Optimizer: SGD\\n  - Momentum: 0.9\\n  - Dampening: 0.0\\n  - Nesterov: True\\n  - Weight Decay (L2 regularization): 0.0\\n\\nData and Evaluation Settings:\\n- Dataset: CIFAR-10-C\\n- Corruption Types: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']\\n- Corruption Severities: [5, 4, 3, 2, 1]\\n- Number of Examples for Evaluation: 10000 (all samples in CIFAR-10)\\n- Batch Size for Evaluation and Updates: 128\\n\\nMiscellaneous:\\n- Random Seed: 1\\n- CUDNN Benchmark: True"
}{
    "Title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
    "Main Contributions": "The paper proposes a novel online adaptation approach for object detection in continually changing test domains. Its key contributions include the introduction of architecture-agnostic and lightweight adaptor modules (requiring only 0.54% to 0.89% additional parameters) for efficient adaptation and prevention of catastrophic forgetting. It presents a practical and straightforward class-wise feature aligning method that operates at both image and object levels to resolve domain shifts. Furthermore, the method enhances efficiency by determining when the model requires adaptation through two novel criteria. This approach significantly outperforms baselines, achieving mAP improvements of up to 4.9%p and 7.9%p on COCO-corrupted and SHIFT benchmarks respectively, while maintaining high inference speeds (around 20 FPS or higher). It also demonstrates robust preservation of task-specific knowledge and wide applicability across diverse backbone architectures like CNNs and Transformers.",
    "Methodology": "The proposed method focuses on three aspects: what, how, and when to update the object detector. For 'what to update', it introduces shallow, low-rank, architecture-agnostic adaptor modules in parallel to each backbone block (MLP for Transformers, 1x1 CNN for ResNet). Only these adaptors are updated, while the pre-trained backbone parameters remain frozen. For 'how to update', it employs an Exponentially Moving Average (EMA) feature alignment strategy. This involves aligning feature distributions at both image-level (Limg) and region-level class-wise (Lobj) by minimizing the KL divergence between training and test domain features. Mean and variance statistics are used, with test domain means estimated via EMA and test variances approximated from training variances. A class-frequency weighting scheme is applied for object-level alignment to address class imbalance. The total adaptation loss is a sum of Limg and Lobj. For 'when to update', two criteria are used to dynamically skip or resume adaptation: 1) Model updates if the ratio of the current image-level distribution gap (Limg) to a pre-computed in-domain distribution gap (Din KL) exceeds a threshold (τ1=1.1). 2) Model updates if Limg suddenly increases relative to its exponentially moving average (Lt ema) by a threshold (τ2=1.05). Adaptation proceeds if at least one criterion is met.",
    "Experimental Setup": "The research used Faster-RCNN models with ResNet50 and Swin-Tiny backbones, both integrated with FPN. Experiments were conducted across three continually changing domain scenarios: COCO \n COCO-C (models trained on MS-COCO and evaluated sequentially on 15 types of corruptions from COCO-C, with additional evaluation on the original COCO validation set) and SHIFT-(Discrete / Continuous) (a synthetic driving dataset with 6 classes, featuring variations in weather and time-of-day, tested in sequential drastic shifts for Discrete and gradual transitions for Continuous). Publicly available models were used for COCO, while SHIFT models were trained using the Detectron2 framework. Test-time adaptation employed an SGD optimizer with a learning rate of 0.001, an EMA update rate (α) of 0.01, and thresholds τ1=1.1 and τ2=1.05 for adaptation criteria. A batch size of 4 was used for main experiments. Baselines included Direct-Test (no adaptation), ActMAD, NORM, DUA, and a Mean-Teacher model implemented based on TeST. Performance was evaluated using mAP, the number of forward and backward passes, and Frames Per Second (FPS).",
    "Limitations": "The approximation of test feature variance (Σte \n Σtr) is made to reduce instability. The 'when to update' criteria rely on specific assumptions regarding the relationship between the image-level distribution gap (Limg) and both in-domain and historical gaps. Additionally, the class frequency weighting scheme (wk,t) for object-level alignment, while generally effective, showed a slight performance decrease in the SHIFT-Continuous setting during ablation studies, suggesting potential trade-offs in specific dynamic scenarios.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
    "Main Contributions": "The paper introduces Test-Time Training (TTT), a novel approach to enhance the generalization of predictive models when training and test data distributions differ. TTT converts a single unlabeled test sample into a self-supervised learning problem, updating model parameters before making a prediction. This method also naturally extends to online data streams by preserving parameter states. The key findings include significant performance improvements on diverse image classification benchmarks evaluating robustness to distribution shifts (e.g., CIFAR-10-C, ImageNet-C, VID-Robust, CIFAR-10.1) without compromising performance on the original data distribution. A theoretical analysis establishes that positive gradient correlation between the main task and the auxiliary self-supervised task is a sufficient condition for TTT to be effective in convex models, a condition empirically shown to hold for deep learning models.",
    "Methodology": "The core methodology involves adapting a pre-trained model at test time using self-supervision. The model is a K-layer neural network (ResNets in experiments) structured with a shared feature extractor (layers `θ1` to `θκ`) and two separate branches for the main task and the self-supervised auxiliary task (layers `θκ+1` to `θK` for each). The auxiliary task employed is rotation prediction, where the model predicts the rotation angle (0, 90, 180, or 270 degrees) of an input image. Initially, the model undergoes joint training on both the main classification task and the self-supervised task using labeled training data. At test time, for each unlabeled test sample, the shared feature extractor's parameters are fine-tuned by minimizing the self-supervised auxiliary task's loss on that sample. The standard TTT performs 10 gradient steps on each sample, discarding updates for the next sample. The online version (TTT-Online) takes one gradient step per new sample and preserves the updated parameters, allowing for adaptation over a stream of data. Group Normalization (GN) is used instead of Batch Normalization (BN) due to the small batch size (single image with augmentations) during test-time updates. Data augmentation (random crop, horizontal flip) is applied to the test sample to form a batch for training.",
    "Experimental Setup": "Experiments were conducted using ResNet-26 for CIFAR-10 based datasets and ResNet-18 for ImageNet based datasets. The datasets included CIFAR-10-C and ImageNet-C, which consist of images corrupted with 15 types of corruptions at 5 severity levels, serving as benchmarks for robustness to common corruptions. The VID-Robust dataset, comprising video frames for object recognition, was used to evaluate generalization across time and object classes. CIFAR-10.1 was used to test generalization under subtle, unknown distribution shifts. Baselines for comparison included: 1) a plain ResNet model ('object recognition task only'), 2) a model jointly trained on main and self-supervised tasks but fixed at test time ('joint training'), 3) Adversarial Logit Pairing (ALP) for CIFAR-10 benchmarks, and 4) Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) on CIFAR-10-C as an 'oracle' comparison with access to the entire unlabeled test set. Optimization for joint training used stochastic gradient descent (SGD) with standard hyperparameters. Test-time training also used SGD, with a learning rate of 0.001 (last epoch's training LR) and zero weight decay/momentum. Standard TTT took 10 gradient steps per sample, while TTT-Online took 1 step per sample. Evaluation was based on test error (%) or accuracy (%).",
    "Limitations": "The primary limitation identified is the computational overhead, as Test-Time Training is significantly slower than standard inference (2 \n\t batch_size \n\t number_of_iterations times). While potential solutions like thresholding on the self-supervised loss or reducing iterations were explored, they were not thoroughly verified or the main focus of this initial work. Another constraint is the reliance on a meaningful self-supervised task; if the task becomes trivial (e.g., rotation prediction for images with clear black margins or ambiguous objects like airplanes in the sky), the benefits of TTT diminish. The theoretical guarantees are primarily for convex models, although empirical evidence suggests they extend to non-convex deep learning models. Furthermore, Batch Normalization (BN) proved ineffective with the small batch sizes inherent to TTT, necessitating the use of Group Normalization (GN), and online TTT did not work well with BN, which limits broader applicability without architectural adjustments.",
    "Future Research Directions": "Future research directions include extending Test-Time Training to other machine learning tasks such as segmentation, detection, speech recognition, and natural language processing. Leveraging domain-specific knowledge to design more effective special-purpose self-supervised tasks is suggested. The paper also proposes using TTT as a new evaluation benchmark for general-purpose self-supervised tasks, alongside existing pre-training and fine-tuning benchmarks. Improving the computational efficiency of TTT, potentially by developing models more amenable to fast updates during training, is another important area. Further formal theoretical discussion on the concept of a variable decision boundary is also suggested. Finally, the authors encourage a broader paradigm shift, advocating for abandoning the strict division between training and testing and promoting a framework where much of the learning continues after model deployment, drawing inspiration from one-shot learning advancements.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Robust Test-Time Adaptation in Dynamic Scenarios",
    "Main Contributions": "This research introduces Practical Test-Time Adaptation (PTTA), a novel and more realistic TTA setting that simultaneously addresses continually changing distributions and correlative data sampling, challenges often encountered in real-world dynamic scenarios. The paper benchmarks prior TTA methods under PTTA, revealing their limitations. It then proposes Robust Test-Time Adaptation (RoTTA), a method designed to comprehensively tackle PTTA challenges, emphasizing its ease of implementation and practical deployment. RoTTA significantly outperforms state-of-the-art baselines, reducing average classification error by over 5.9% on CIFAR-10-C, 5.5% on CIFAR-100-C, and 2.2% on DomainNet.",
    "Methodology": "RoTTA consists of three main components: 1) Robust Batch Normalization (RBN), which replaces standard BN layers with a scheme that estimates normalization statistics using global running mean and variance, updated via exponential moving average from buffered samples, ensuring stability against correlative data. 2) Category-balanced sampling with timeliness and uncertainty (CSTU), which maintains a memory bank to store samples with pseudo-labels, prioritizing category balance, timeliness (newer samples), and low uncertainty (calculated as prediction entropy) when updating. This ensures a stable representation of the current distribution. 3) Robust training with timeliness, which employs a teacher-student model. The student model is updated by minimizing a robust loss on the memory bank, weighted by a timeliness-based reweighting strategy, giving higher importance to newer samples. The teacher model is updated via exponential moving average of the student's parameters, and only affine parameters in RBN are trained for efficiency and stability.",
    "Experimental Setup": "Experiments were conducted on CIFAR10-C, CIFAR100-C, and DomainNet datasets. For CIFAR-C benchmarks, pre-trained WildResNet-28 (CIFAR10) and ResNeXt-29 (CIFAR100) models from RobustBench were used. For DomainNet, ResNet-101 models were pre-trained on each source domain. The PTTA setup was simulated by continually changing test corruptions (severity 5) on CIFAR-C and adapting to different target domains on DomainNet. Correlative sampling was simulated using a Dirichlet distribution with parameter δ=0.1. Optimization used Adam with a learning rate of 1.0 × 10^-3. All methods used a batch size of 64, and RoTTA's memory bank capacity was N=64. Hyperparameters for RoTTA were unified: α=0.05, ν=0.001, λt=1.0, λu=1.0. Performance was evaluated using average classification error, with extensive ablation studies on individual components, distribution changing order, Dirichlet parameter δ, and batch size.",
    "Limitations": "The Robust Batch Normalization (RBN) adopted is considered a naive solution for normalizing correlatively sampled data, requiring careful tuning of its α parameter. The current method lacks a mechanism to explicitly recover the model from a collapsed state, although RoTTA prevents such collapse in experiments. The simulation of correlative sampling relies on category similarity approximated by the Dirichlet distribution, indicating a need for further validation in more diverse real-world scenarios beyond this specific type of correlation.",
    "Future Research Directions": "Future work could focus on improving the current RoTTA algorithm by replacing or refining its existing components. A key direction is to further enhance the Practical Test-Time Adaptation (PTTA) setup itself to make it even more reflective of real-world complexities. Developing methods to recover models from a collapsed state would be a valuable addition. Lastly, there's a need to validate the approach in diverse real-world scenarios to confirm its effectiveness beyond simulated correlative sampling.",
    "Experiment Code": "import torch\nimport torch.nn as nn\nfrom copy import deepcopy\nimport math\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n# From core/adapter/base_adapter.py\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n\n        params, param_names = self.collect_params(self.model)\n        if len(param_names) == 0:\n            self.optimizer = None\n        else:\n            self.optimizer = optimizer(params)\n\n        self.steps = self.cfg.OPTIM.STEPS\n\n    def collect_params(self, model: nn.Module):\n        names = []\n        params = []\n\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                names.append(n)\n                params.append(p)\n\n        return params, names\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n# From core/utils/memory.py\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\n# From core/utils/bn_layers.py\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n    def forward(self, x):\n        raise NotImplementedError\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n# From core/utils/utils.py\ndef get_named_submodule(model, sub_name: str):\n    names = sub_name.split(\".\")\n    module = model\n    for name in names:\n        module = getattr(module, name)\n\n    return module\n\ndef set_named_submodule(model, sub_name, value):\n    names = sub_name.split(\".\")\n    module = model\n    for i in range(len(names)):\n        if i != len(names) - 1:\n            module = getattr(module, names[i])\n\n        else:\n            setattr(module, names[i], value)\n\n# From core/utils/custom_transforms.py\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\nclass ColorJitterPro(ColorJitter):\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    def forward(self, img):\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0) \n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n# From core/adapter/rotta.py\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        # CSTU memory bank initialization\n        self.mem = CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        # Teacher model initialization\n        self.model_ema = self.build_ema(self.model)\n        # Data augmentation for memory samples\n        self.transform = get_tta_transforms(cfg)\n        # EMA coefficient for teacher update\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            # Uncertainty calculation (prediction entropy)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # Add samples to memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train() # Student model\n        self.model_ema.train() # Teacher model\n        \n        # Get memory data and ages for timeliness reweighting\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            # Apply strong augmentation to memory samples for student training\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data) # Teacher inference\n            stu_sup_out = model(strong_sup_aug) # Student inference with augmentation\n            \n            # Timeliness-based reweighting\n            instance_weight = timeliness_reweighting(ages)\n            # Robust loss minimization (student update)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        # Teacher model update via EMA of student parameters\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n        model.requires_grad_(False) # Freeze all parameters initially\n        normlayer_names = []\n\n        # Identify BatchNorm layers\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        # Replace BatchNorm with RobustBN and enable gradient for affine parameters\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA) # ALPHA is the momentum for RBN\n            momentum_bn.requires_grad_(True) # Only affine parameters of RBN are trained\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n",
    "Experiment Result": "The RoTTA method utilizes several configurable parameters for its core components:\n- `cfg.ADAPTER.RoTTA.MEMORY_SIZE`: Sets the capacity of the Category-balanced sampling with timeliness and uncertainty (CSTU) memory bank. (Default: 64)\n- `cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY`: Determines how often the student model is updated using samples from the memory bank (e.g., after processing this many instances). (Default: 64)\n- `cfg.ADAPTER.RoTTA.NU`: The Exponential Moving Average (EMA) decay rate used to update the teacher model's parameters from the student model during robust training. (Default: 0.001)\n- `cfg.ADAPTER.RoTTA.ALPHA`: Represents the momentum coefficient for the running mean and variance statistics update in the Robust Batch Normalization (RBN) layers. (Default: 0.05)\n- `cfg.ADAPTER.RoTTA.LAMBDA_T`: A weighting factor that controls the influence of 'timeliness' in the CSTU memory item scoring heuristic, prioritizing newer samples. (Default: 1.0)\n- `cfg.ADAPTER.RoTTA.LAMBDA_U`: A weighting factor that controls the influence of 'uncertainty' (prediction entropy) in the CSTU memory item scoring heuristic, prioritizing samples with low uncertainty. (Default: 1.0)\n- `cfg.OPTIM.STEPS`: Specifies the number of forward and adaptation steps performed for each batch of incoming data. (Default: 1)\n- `cfg.OPTIM.LR`: The learning rate used by the optimizer for updating the student model's trainable affine parameters within the RBN layers. (Default: 1e-3)\n- `cfg.OPTIM.METHOD`: The optimization algorithm used for the student model (e.g., 'Adam', 'SGD'). (Default: 'Adam')\n- `cfg.TEST.BATCH_SIZE`: The number of samples processed together in each batch during testing and adaptation. (Default: 64)\n- `cfg.CORRUPTION.NUM_CLASS`: The total number of classes in the dataset, which is crucial for initializing the CSTU memory bank and normalizing uncertainty scores. (Default: -1, typically inferred from the dataset, e.g., 10 for CIFAR-10, 100 for CIFAR-100)\n- `cfg.INPUT.SIZE`: The spatial dimensions (height, width) to which input images are resized, relevant for the strong data augmentation transforms applied to memory samples. (Default: (32, 32))"
}{
    "Title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
    "Main Contributions": "The paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that accounts for computational time constraints by penalizing slower methods with fewer samples for adaptation. This protocol reveals that, under realistic conditions (constant-speed data stream), simpler and faster TTA approaches can outperform more sophisticated but slower state-of-the-art methods. The research highlights the critical importance of developing TTA methods that are both accurate and computationally efficient for real-world applicability.",
    "Methodology": "The core methodology is the proposed 'Realistic Online Evaluation Protocol' for TTA. Unlike current offline protocols that assume the data stream waits for adaptation, this protocol simulates a constant-speed data stream. It introduces a 'relative adaptation speed' C(g), defined as the integer ratio of the stream's speed to the method's adaptation speed. If a method gslow is k times slower than the stream (C(g)=k), it can only adapt to every kth sample; the remaining samples are processed by the most recent adapted model (fθt+1) or the base model (fθ). This C(g) is computed online for each input, accounting for hardware and input-dependent noise. The protocol explicitly models the trade-off between adaptation speed and the number of samples a method can leverage.",
    "Experimental Setup": "The evaluation used a ResNet-50-BN3 (ImageNet-pretrained) as the base classifier, with stream batches of size 64 (except MEMO, which uses single images). Benchmarking was performed on 15 state-of-the-art TTA methods (e.g., AdaBN, TENT, SHOT, SAR, EATA, CoTTA, DDA, MEMO) across multiple datasets: ImageNet-C (corruption level 5 across 15 corruptions), CIFAR10-C, ImageNet-R, and ImageNet-3DCC. Architectures included ResNet-50, ResNet-18, and ViT. Evaluation scenarios covered: (i) episodic (single domain shift, model reset), (ii) continual (sequential domain shifts, no model reset), (iii) varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), (iv) practical TTA with label imbalances (PTTA using RoTTA on CIFAR10-C), and (v) a strict 'single model' scenario (skipped samples processed by a random classifier). Hyperparameter tuning for TENT (learning rates) was also evaluated.",
    "Limitations": "The current evaluation protocol implicitly assumes that the data stream is not constant-speed and waits for the TTA method to adapt, favoring slower methods. The proposed realistic protocol assumes the capacity to concurrently deploy two models (the TTA method and the base model) when the TTA method is busy, which might be an unfair assumption for very efficient methods (C(g)=1) or in resource-constrained scenarios. Data-dependent approaches like MEMO and DDA were found to be extremely inefficient under this protocol due to massive computational burdens, leading to most predictions being made by the non-adapted classifier. The assumption in the 'single model evaluation scheme' (Appendix C) that a random classifier generates predictions for skipped samples is a strict and naive choice, potentially not reflecting all real-world scenarios where g is busy.",
    "Future Research Directions": "The paper calls for future research to focus on developing TTA methods that are both accurate and efficient, specifically increasing the efficiency of data-dependent adaptation methods. It hopes the proposed evaluation scheme inspires future TTA methods to consider inference speed as a critical dimension for real-world performance. The consistent performance changes across corruptions suggest further testing of the hypothesis that performance is independent of the domain shift type.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
    "Main Contributions": "The paper addresses the challenge of Test-Time Adaptation (TTA) models experiencing performance degradation over prolonged periods, particularly in recurring testing scenarios where environments change and recur. It introduces the 'recurring TTA' diagnostic setting to observe error accumulation, derives theoretical insights into performance degradation using an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC), and proposes Persistent TTA (PeTTA) to prevent model collapse by adaptively adjusting adaptation strategies.",
    "Methodology": "The methodology involves: (1) **Recurring TTA:** A new testing scenario extending practical TTA by repeatedly cycling through a sequence of changing and correlated environments to diagnose performance degradation. (2) **Theoretical Analysis:** Simulating a TTA process on an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC) to derive dataset- and algorithm-dependent factors contributing to error accumulation and model collapse. (3) **Persistent TTA (PeTTA):** An adaptation scheme that continuously monitors model divergence from the initial source model using a Mahalanobis distance metric on feature embeddings (γt). Based on this divergence, PeTTA adaptively adjusts the regularization coefficient (λt) and the EMA update rate (αt) to balance adaptation and collapse prevention. It also incorporates an anchor loss (LAL) to constrain model divergence in the probability space and utilizes a category-balanced memory bank and robust batch normalization from prior work.",
    "Experimental Setup": "Experiments were conducted on four TTA classification tasks: CIFAR-10→CIFAR-10-C, CIFAR-100→CIFAR-100-C, ImageNet→ImageNet-C (corruption level 5), and DomainNet (real→clipart, painting, sketch). The 'recurring TTA' setting involved revisiting test environments 20 times (and up to 40 for extended evaluation), with temporally correlated batches generated by Dirichlet distributions (Dir(0.1) or Dir(0.01)). Comparisons were made against CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME, and RDumb. An additional evaluation was performed on the 'Continuously Changing Corruption (CCC)' setting. PeTTA used ResNet50 models pre-trained on ImageNet V2 or RobustBench checkpoints. Implementation was in PyTorch, using Adam optimizer (learning rate 1e-3), EMA update rate of 5e-2, cosine similarity regularizer, and adaptive λt/αt (initial λ0=10 or 1, α0=1e-3). Computing resources included an Intel Core i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU.",
    "Limitations": "The current PeTTA approach does not rigorously guarantee complete elimination of error accumulation through regularization. It relies on a small memory bank to handle temporally correlated testing streams, which is not its primary focus. Furthermore, PeTTA assumes the availability or computability of feature statistics (empirical mean and covariant matrix) from the source distribution, which might limit its scalability in certain real-world scenarios where source data access is restricted.",
    "Future Research Directions": "Future research could focus on developing TTA algorithms that achieve error accumulation-free by construction, moving beyond regularization-based collapse prevention. Another direction is exploring alternative methods for reducing memory requirements, such as storing embedded features instead of original images, to improve the scalability of TTA models in real-world scenarios that may have strict memory constraints.",
    "Experiment Code": "import torch\nimport torch.nn as nn\nimport logging\n\n\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.logger = logging.getLogger(\"TTA.adapter\")\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n\n        params, param_names = self.collect_params(self.model)\n        if len(param_names) == 0:\n            self.optimizer = None\n        else:\n            self.optimizer = optimizer(params)\n\n        self.steps = self.cfg.OPTIM.STEPS\n        assert self.steps > 0, \"requires >= 1 step(s) to forward and update\"\n\n    def forward(self, x):\n        for _ in range(self.steps):\n            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def forward_and_and(self, *args):\n        raise NotImplementedError(\"implement forward_and_adapt by yourself!\")\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    def collect_params(self, model: nn.Module):\n        names = []\n        params = []\n\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                names.append(n)\n                params.append(p)\n\n        return params, names\n\n    def check_model(self, model):\n        pass\n\n    def before_tta(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)",
    "Experiment Result": "The `softmax_entropy` function is a crucial component for measuring divergence between model predictions, specifically used in `RoTTA` (PeTTA's implementation) for the consistency loss. It calculates the entropy between the soft predictions of the Exponential Moving Average (EMA) model and the student model, serving as an anchor loss to constrain model divergence in the probability space."
}{
    "Title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
    "Main Contributions": "The paper introduces Active Test-Time Adaptation (ATTA) as a novel problem setting to address significant distribution shifts and catastrophic forgetting in unsupervised streaming test data. It provides a learning theory analysis demonstrating that incorporating limited labeled test instances theoretically enhances overall performance and mitigates distribution shifts. The work also presents SimATTA, a simple yet effective algorithm that integrates incremental clustering and selective entropy minimization to implement ATTA and avoid catastrophic forgetting. Extensive experiments show that ATTA achieves substantial performance improvements over traditional TTA methods, maintains efficiency, and performs comparably to Active Domain Adaptation (ADA) methods.",
    "Methodology": "ATTA formally defines dynamic model optimization for streaming test data by actively selecting informative instances for labeling. The theoretical foundation uses learning bounds based on H∆H-distance and VC-dimension to guarantee mitigation of distribution shifts (Theorem 1 and 2). Catastrophic forgetting is addressed through selective entropy minimization, where low-entropy samples (pseudo-labeled by the frozen pre-trained source model) are included alongside actively labeled high-entropy samples to maintain source domain performance (Corollary 3 and 4). The SimATTA algorithm partitions incoming unlabeled test samples into high- and low-entropy sets. Low-entropy samples are pseudo-labeled and stored. High-entropy samples are selected using an incremental clustering technique, which employs weighted K-means to store representative 'anchors' of seen distributions, adapting to new distributions while managing a budget. The model is then fine-tuned on both actively labeled high-entropy samples and pseudo-labeled low-entropy samples, balancing their influence according to theoretical insights.",
    "Experimental Setup": "The method was evaluated on PACS, VLCS, Office-Home (from DomainBed), and Tiny-ImageNet-C datasets, using ResNet-18 (PACS, VLCS, Tiny-ImageNet-C) or ResNet-50 (Office-Home) architectures pre-trained on ImageNet. Source domains were 'photos' for PACS, 'Caltech101' for VLCS, 'real' for Office-Home, and 'brightness corruption' for Tiny-ImageNet-C. Experiments used two data stream orders: domain-wise (sequential domains) and random (shuffled samples from target domains). Baselines included source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and selected ADA methods (Random, Entropy, K-means, CLUE) for stronger comparisons. Performance was measured by accuracy, reported for real-time adaptation and post-adaptation. Efficiency was also measured in time (seconds) on Tiny-ImageNet-C. Hyperparameters like entropy thresholds (`el`, `eh`) and a cluster centroid budget increase rate (`k`) were empirically set.",
    "Limitations": "The theoretical bounds can be loose when test batch sizes are small due to the large VC-dimension of deep models, though fine-tuning pre-trained models is assumed to reduce the effective VC-dimension. The approach's reliance on the quality of the pre-trained model for pseudo-labeling low-entropy samples means incorrect predictions could reinforce errors. Expending annotation budgets on low-entropy samples might not always be cost-effective. The current framework does not address class-incremental problems where the support of labels (Y) changes. The paper's scope is limited to foundational aspects and does not cover scaling to all large models/datasets or task-specific applications.",
    "Future Research Directions": "Future work could focus on developing alternative strategies to prevent catastrophic forgetting in ATTA scenarios, especially considering the potential pitfalls of selective entropy minimization. Investigating the cost-effectiveness of annotating low-entropy samples versus correcting them could also be a fruitful area. A promising extension is adapting ATTA methods for large language models (LLMs) where retraining is prohibitively expensive and source data may be inaccessible. Bridging the gap between causal inference and deep learning through causal representation learning is also suggested as a direction for out-of-distribution generalization. Developing more adaptive techniques for managing annotation budgets beyond simple constant increments is another potential area.",
    "Experiment Code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1\n",
    "Experiment Result": "The SimATTA algorithm addresses dynamic model optimization for streaming test data. It employs a teacher-student framework where a `teacher` model, a deep copy of the initially trained source model, is used for pseudo-labeling low-entropy samples. The `model` (student) is fine-tuned. The core process involves partitioning incoming unlabeled test samples into high- and low-entropy sets using `softmax_entropy`.\n\n**Low-entropy samples (Pseudo-labeling):**\n- Identified by `ent_beta=0` and `entropy < ent_bound` (where `ent_bound` is `self.config.atta.SimATTA.el`).\n- Pseudo-labels are generated using the `teacher` model's predictions (`outputs.argmax(1)`).\n- These samples are stored as `source_anchors` using `update_anchors`.\n- `self.LE` (controlled by `atta.SimATTA.LE`) determines whether to use low-entropy samples. When `LE` is 0, no source anchors are collected.\n\n**High-entropy samples (Active Labeling & Incremental Clustering):**\n- Identified by `ent_beta=1` and `entropy >= ent_bound` (where `ent_bound` is `self.config.atta.SimATTA.eh`).\n- An incremental clustering technique (`target_cluster` flag) employs weighted K-means (`KMeans` from `ATTA.utils.fast_pytorch_kmeans` or `sklearn.cluster.KMeans` with `sample_weight`).\n- The clustering identifies representative 'anchors' of seen distributions, managing a budget.\n- `self.n_clusters` (initial 10) for target clusters increases by `self.nc_increase` (controlled by `atta.SimATTA.nc_increase`, tested with values like 0.25, 0.5, ..., 3).\n- Selected high-entropy samples are stored as `anchors` (`target_anchors`) using `update_anchors`.\n\n**Model Fine-tuning:**\n- The model is fine-tuned (`cluster_train`) on both actively labeled high-entropy samples (`target_anchors`) and pseudo-labeled low-entropy samples (`source_anchors`).\n- Influence is balanced using `alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`.\n- During a `cold_start` phase (`atta.SimATTA.cold_start`, set to 100), `alpha` is capped at `min(0.2, alpha)`.\n- The optimizer used is `torch.optim.SGD` with learning rate `atta.SimATTA.lr` and momentum 0.9.\n- Training steps per batch are controlled by `atta.SimATTA.steps`.\n- Training stops if a `loss_window` (size `atta.SimATTA.stop_tol`) shows no improvement for 5 consecutive checks.\n\n**Budget Management:**\n- `self.budgets` accumulates the number of actively labeled samples (length of `closest` in `sample_select`).\n\n**Experimental Settings from `ATTA/kernel/launch.py`:**\n- **Datasets:** Primarily `VLCS` (with `el=1e-3`) and potentially `PACS` (with `el=1e-4`).\n- **Hyperparameters:**\n    - `atta.SimATTA.cold_start`: 100\n    - `atta.SimATTA.el` (low-entropy bound): 1e-3 (for VLCS), 1e-4 (for PACS)\n    - `atta.SimATTA.nc_increase` (cluster increase rate, denoted as `k`): Tested values include 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3.\n    - `atta.SimATTA.LE` (low-entropy sample usage, denoted as `le`): Tested values 0 (False) and 1 (True).\n    - `atta.SimATTA.target_cluster` (incremental clustering, denoted as `ic`): Tested values 0 (False) and 1 (True).\n    - `atta.gpu_clustering`: True (GPU-accelerated clustering).\n- **General Settings:**\n    - `exp_round`: 1\n    - `num_workers`: 4\n- **Adaptation Loop:** The adaptation process iterates through `self.config.dataset.test_envs[1:]` (target domains)."
}{
    "Title": "Test Time Adaptation via Conjugate Pseudo-labels",
    "Main Contributions": "The paper addresses the challenge of selecting an effective unsupervised objective (TTA loss) for Test-Time Adaptation (TTA) of neural networks to distribution shifts. It presents the surprising phenomenon that meta-learning the 'best' TTA loss recovers a temperature-scaled softmax-entropy for cross-entropy trained classifiers, but a negative squared error for squared-loss trained classifiers. The main contribution is a generic recipe for finding a good TTA loss for any given supervised training loss function by analyzing TTA through the lens of the training loss's convex conjugate. This framework explains the meta-learning observations and provides 'conjugate pseudo-labels' as a principled soft label for self-training. Empirically, the proposed approach consistently outperforms other TTA alternatives across various domain adaptation benchmarks, particularly benefiting classifiers trained with novel loss functions like PolyLoss.",
    "Methodology": "The methodology involves two main parts. First, an empirical exploration using meta-learning, where the TTA loss function is parameterized by a neural network and its parameters are learned by differentiating through the adaptation process to find the loss yielding optimal adaptation. This meta-learning setup revealed the dependence of the optimal TTA loss on the source classifier's original training loss. Second, a theoretical analysis leveraging the convex conjugate function. For a class of loss functions L(h(x),y) = f(h(x)) - yᵀh(x), the paper shows that the conjugate adaptation loss, Lconj(hθ(x)) = -f⋆(∇f(hθ(x))), where f⋆ is the convex conjugate of f, serves as a good local approximation to the original supervised loss. This Lconj can also be interpreted as self-training with specific soft labels, called 'conjugate pseudo-labels' (˜yCPLθ(x) = ∇f(hθ(x))). The model parameters (specifically batch normalization layers) are updated at test time by optimizing this unsupervised conjugate adaptation loss (or self-training with conjugate pseudo-labels) over incoming batches of unlabeled test data, often with an additional temperature scaling.",
    "Experimental Setup": "The effectiveness and generality of conjugate pseudo-labeling are evaluated across common corruption benchmarks and domain adaptation datasets. Common corruptions include CIFAR-10-C, CIFAR-100-C, and ImageNet-C, with errors averaged across corruptions (and severity for ImageNet-C). Domain adaptation datasets include SVHN to MNIST/USPS/MNISTM, ImageNet-R, and VisDA-C. Source classifiers use ResNet-26 for CIFAR, ResNet-18 for SVHN-MNIST, and ResNet-50 for ImageNet and VisDA-C. Source models are trained with cross-entropy, PolyLoss, or squared loss. Baselines include Hard Pseudo-Labels (PL), Soft Pseudo-Labels (PL), Entropy Minimization (ENT/TENT), Robust Pseudo-Label (RPL), and MEMO. Adaptation involves fine-tuning only the learnable scale and shift parameters of batch normalization layers. Hyperparameters (learning rate and temperature T) are tuned via grid-search on validation noises for corruption benchmarks, while T is fixed to 1 for domain adaptation tasks without a held-out target domain.",
    "Limitations": "The work does not fully answer what constitutes the 'optimal' test-time adaptation loss and why. The meta-learning framework used to discover the initial phenomenon was constrained to learning functions over the logits of individual inputs, suggesting potential for expansion to intermediate representations or batch-level interactions. Achieving good TTA performance still relies on several heuristics, such as updating only batch normalization parameters, and the role of temperature scaling is not fully understood, despite its empirical importance. It remains an open problem to understand under what specific real-world distribution shifts self-training based approaches like this would be most beneficial.",
    "Future Research Directions": "Future research directions include expanding the meta-learning framework to learn functions over intermediate representations of the model or over interactions within a batch of inputs, rather than just individual input logits. A more concrete understanding of the role of empirical heuristics, such as updating only batch normalization parameters and the effect of temperature scaling, is also an important area. Further investigation is needed to determine the types of real-world distribution shifts where self-training based approaches are most effective. Additionally, applying and extending the conjugate pseudo-labeling framework to other machine learning settings, such as semi-supervised learning, is a promising avenue.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Improved Test-Time Adaptation for Domain Generalization",
    "Main Contributions": "The main challenge in Domain Generalization (DG) is handling distribution shifts between training and test data. While Test-Time Training (TTT) is a promising solution, its performance hinges on selecting an appropriate auxiliary TTT task and identifying reliable parameters to update. This paper proposes an Improved Test-Time Adaptation (ITTA) method addressing these two factors. ITTA introduces a learnable consistency loss for the TTT task with tuneable parameters to ensure better alignment with the main prediction task. It also introduces additional adaptive parameters for the trained model, suggesting only these adaptive parameters be updated during the test phase, leaving original parameters fixed. Extensive experiments demonstrate that ITTA achieves superior performance compared to state-of-the-art methods on several DG benchmarks for both multi-source and single-source DG tasks.",
    "Methodology": "ITTA improves TTT by introducing a learnable consistency loss and additional adaptive parameters. First, for the TTT task, a learnable consistency loss (Lwcont) is proposed, calculated as the L2 norm of the output of a weight subnetwork (fw) applied to the difference between original and augmented feature representations (z - z'). The fw subnetwork, composed of stacked ReLU layers with learnable weights and biases, allows flexible consistency measurement. To ensure alignment with the main classification loss (Lmain, a cross-entropy loss), ITTA enforces equality between the normalized gradients of Lmain and Lwcont with respect to the feature extractor parameters, using this as an objective to update fw's parameters. Second, during the test phase, ITTA inserts new adaptive blocks (fΘ), structured similarly to fw, after each block of the pretrained feature extractor. Crucially, only these newly introduced fΘ parameters are updated using the learnable consistency loss for test samples, while the original feature extractor and classifier parameters remain unchanged. This online adaptation process uses the learned consistency loss to tune the adaptive parameters for unseen target domains.",
    "Experimental Setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. For the backbone, an ImageNet-pretrained ResNet18 with 4 blocks was used as the feature extractor, and the classifier was an MLP layer from the DomainBed benchmark. The proposed fΘ adaptive blocks also consist of 4 blocks, each with 5 layers of learnable parameters, and the fw weight subnetwork uses 10 layers. The weight parameter (α) balancing the main and consistency losses was fixed at 1. Experiments followed the rigorous evaluation protocol of DomainBed, conducting 60 trials for each unseen domain, with 5,000 iteration steps per trial. Model selection used the 'training-domain validate set' method. Performance was measured by average accuracy across 60 trials. ITTA was compared against 22 baseline methods, including ERM, MMD, TTT, TENT, MT3, MixStyle, etc., in both multi-source (leave-one-out) and single-source DG settings. Ablation studies were conducted to assess the effectiveness of the learnable consistency loss (e.g., without fw, with entropy/rotation tasks) and the adaptive parameter strategy (e.g., updating all parameters, only BN layers, different augmentation skills, TTT steps, and network structures for fw/fΘ).",
    "Limitations": "One limitation of ITTA is its computational burden during the training phase. The proposed learnable weight subnetwork (fw) requires computing second-order derivatives to enforce gradient alignment, leading to extra processing (1 additional forward and 3 additional backward processes) compared to simpler TTT setups. During test-time adaptation, ITTA also incurs increased FLOPs and inference time due to the additional forward and backward steps required to update the adaptive parameters, making the 'lunch not free' despite the performance improvements.",
    "Future Research Directions": "Future efforts aim to simplify the overall optimization process of ITTA and reduce its computational cost, particularly the overhead associated with the learnable consistency loss during training and the adaptation process during testing.",
    "Experiment Code": "class ITTA(Algorithm):\\n    \"\"\"Improved Test-Time Adaptation (ITTA)\"\"\"\\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\\n        super(ITTA, self).__init__(input_shape, num_classes, num_domains, hparams)\\n        self.input_shape = input_shape\\n        self.num_classes = num_classes\\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\\n        self.classifier = networks.Classifier(self.featurizer.n_outputs, num_classes, self.hparams['nonlinear_classifier'])\\n        self.test_mapping = networks.MappingNetwork()\\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\\n        self.optimizer = torch.optim.Adam([{'params': self.featurizer.parameters()}, {'params': self.classifier.parameters()}], lr=self.hparams[\"lr\"], weight_decay=self.hparams['weight_decay'])\\n        self.MSEloss = nn.MSELoss()\\n        self.adaparams = networks.Adaparams()\\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\\n\\n    def _get_grads(self, loss):\\n        self.optimizer.zero_grad()\\n        loss.backward(inputs=list(self.featurizer.parameters()), retain_graph=True, create_graph=True)\\n        dict = OrderedDict([(name, weights.grad.clone().view(weights.grad.size(0),-1)) for name, weights in self.featurizer.named_parameters()])\\n        return dict\\n\\n    def update(self, minibatches, unlabeled=None):\\n        all_x = torch.cat([x for x,y in minibatches])\\n        all_y = torch.cat([y for x,y in minibatches])\\n        z_ori, z_aug = self.featurizer(all_x)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + F.cross_entropy(self.classifier(z_aug), all_y)\\n        loss = loss_reg + loss_cla\\n        self.optimizer.zero_grad()\\n        loss.backward()\\n        self.optimizer.step()\\n\\n        z_ori, z_aug = self.featurizer(all_x)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + F.cross_entropy(self.classifier(z_aug), all_y)\\n        dict_reg = self._get_grads(loss_reg)\\n        dict_cla = self._get_grads(loss_cla)\\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\\n        self.adaparams_optimizer.zero_grad()\\n        penalty.backward(inputs=list(self.adaparams.parameters()))\\n        self.adaparams_optimizer.step()\\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\\n\\n    def test_adapt(self, x):\\n        z_ori, z_aug = self.featurizer(x)\\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\\n        self.test_optimizer.zero_grad()\\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\\n        self.test_optimizer.step()\\n\\n    def predict(self, x):\\n        z_ori, z_aug = self.featurizer(x)\\n        z_ori = self.test_mapping.fea1(z_ori)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\\n        z_ori = self.test_mapping.fea2(z_ori)\\n        z_ori = self.featurizer.fea3(z_ori)\\n        z_ori = self.test_mapping.fea3(z_ori)\\n        z_ori = self.featurizer.fea4(z_ori)\\n        z_ori = self.test_mapping.fea4(z_ori)\\n        z_ori = self.featurizer.flat(z_ori)\\n        return self.classifier(z_ori)\\n\\nclass MappingNetwork(torch.nn.Module):\\n    def __init__(self, depth=5):\\n        super().__init__()\\n        self.depth = depth\\n        self.weight1 = nn.ParameterList()\\n        self.bias1 = nn.ParameterList()\\n        self.weight2 = nn.ParameterList()\\n        self.bias2 = nn.ParameterList()\\n        self.weight3 = nn.ParameterList()\\n        self.bias3 = nn.ParameterList()\\n        self.weight4 = nn.ParameterList()\\n        self.bias4 = nn.ParameterList()\\n        for i in range(depth):\\n            self.weight1.append(nn.Parameter(torch.ones((64,56,56))))\\n            self.bias1.append(nn.Parameter(torch.zeros((64,56,56))))\\n            self.weight2.append(nn.Parameter(torch.ones((128,28,28))))\\n            self.bias2.append(nn.Parameter(torch.zeros((128,28,28))))\\n            self.weight3.append(nn.Parameter(torch.ones((256,14,14))))\\n            self.bias3.append(nn.Parameter(torch.zeros((256,14,14))))\\n            self.weight4.append(nn.Parameter(torch.ones((512, 7, 7))))\\n            self.bias4.append(nn.Parameter(torch.zeros((512, 7, 7))))\\n        self.relu = nn.ReLU(inplace=True)\\n    def fea1(self, x):\\n        for i in range(self.depth-1):\\n            x = self.relu(self.weight1[i] * x + self.bias1[i])\\n        x = self.weight1[i+1] * x + self.bias1[i+1]\\n        return x\\n    def fea2(self, x):\\n        for i in range(self.depth - 1):\\n            x = self.relu(self.weight2[i] * x + self.bias2[i])\\n        x = self.weight2[i + 1] * x + self.bias2[i + 1]\\n        return x\\n    def fea3(self, x):\\n        for i in range(self.depth - 1):\\n            x = self.relu(self.weight3[i] * x + self.bias3[i])\\n        x = self.weight3[i + 1] * x + self.bias3[i + 1]\\n        return x\\n    def fea4(self, x):\\n        for i in range(self.depth-1):\\n            x = self.relu(self.weight4[i] * x + self.bias4[i])\\n        x = self.weight4[i+1] * x + self.bias4[i+1]\\n        return x\\n\\nclass Identity(nn.Module):\\n    \"\"\"An identity layer\"\"\"\\n    def __init__(self):\\n        super(Identity, self).__init__()\\n    def forward(self, x):\\n        return x\\n\\nclass Adaparams(nn.Module):\\n    def __init__(self, depth=10):\\n        super(Adaparams, self).__init__()\\n        self.relu = nn.ReLU(inplace=True)\\n        self.depth = depth\\n        self.weight = nn.ParameterList()\\n        self.bias = nn.ParameterList()\\n        for i in range(depth):\\n            self.weight.append(nn.Parameter(torch.ones(512)))\\n            self.bias.append(nn.Parameter(torch.zeros(512)))\\n    def forward(self, x):\\n        for i in range(self.depth-1):\\n            x = self.relu(self.weight[i] * x + self.bias[i])\\n        x = self.weight[i+1] * x + self.bias[i+1]\\n        return x\\n\\nclass ResNet_ITTA(torch.nn.Module):\\n    \"\"\"ResNet with the softmax chopped off and the batchnorm frozen\"\"\"\\n    def __init__(self, input_shape, hparams):\\n        super(ResNet_ITTA, self).__init__()\\n        if hparams['resnet18']:\\n            self.network = torchvision.models.resnet18(pretrained=True)\\n            self.n_outputs = 512\\n        else:\\n            self.network = torchvision.models.resnet18(pretrained=True)\\n            self.n_outputs = 2048\\n        nc = input_shape[0]\\n        if nc != 3:\\n            tmp = self.network.conv1.weight.data.clone()\\n            self.network.conv1 = nn.Conv2d(nc, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\\n            for i in range(nc):\\n                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]\\n        self.network.fc = Identity()\\n        self.isaug = True\\n        self.freeze_bn()\\n        self.hparams = hparams\\n        self.dropout = nn.Dropout(hparams['resnet_dropout'])\\n        self.eps = 1e-6\\n\\n    def mixstyle(self, x):\\n        alpha = 0.1\\n        beta = torch.distributions.Beta(alpha, alpha)\\n        B = x.size(0)\\n        mu = x.mean(dim=[2, 3], keepdim=True)\\n        var = x.var(dim=[2, 3], keepdim=True)\\n        sig = (var + self.eps).sqrt()\\n        mu, sig = mu.detach(), sig.detach()\\n        x_normed = (x - mu) / sig\\n        lmda = beta.sample((B, 1, 1, 1))\\n        lmda = lmda.to(x.device)\\n        perm = torch.randperm(B)\\n        mu2, sig2 = mu[perm], sig[perm]\\n        mu_mix = mu * lmda + mu2 * (1 - lmda)\\n        sig_mix = sig * lmda + sig2 * (1 - lmda)\\n        return x_normed * sig_mix + mu_mix\\n\\n    def fea_forward(self, x):\\n        x = self.fea3(x)\\n        x = self.fea4(x)\\n        x = self.flat(x)\\n        return x\\n\\n    def fea2(self, x, aug_x):\\n        x = self.network.layer2(x)\\n        aug_x = self.network.layer2(aug_x)\\n        if not self.isaug:\\n            aug_x = self.mixstyle(aug_x)\\n        return x, aug_x\\n\\n    def fea3(self, x):\\n        x = self.network.layer3(x)\\n        return x\\n\\n    def fea4(self, x):\\n        x = self.network.layer4(x)\\n        return x\\n\\n    def flat(self, x):\\n        x = self.network.avgpool(x)\\n        x = torch.flatten(x, 1)\\n        x = self.network.fc(x)\\n        x = self.dropout(x)\\n        return x\\n\\n    def forward(self, x):\\n        x = self.network.conv1(x)\\n        x = self.network.bn1(x)\\n        x = self.network.relu(x)\\n        x = self.network.maxpool(x)\\n        x = self.network.layer1(x)\\n        if random.random() > 0.5:\\n            self.isaug = True\\n            aug_x = self.mixstyle(x)\\n        else:\\n            self.isaug = False\\n            aug_x = x\\n        return x, aug_x\\n\\n    def train(self, mode=True):\\n        super().train(mode)\\n        self.freeze_bn()\\n\\n    def freeze_bn(self):\\n        for m in self.network.modules():\\n            if isinstance(m, nn.BatchNorm2d):\\n                m.eval()\\n\\ndef Classifier(in_features, out_features, is_nonlinear=False):\\n    if is_nonlinear:\\n        return torch.nn.Sequential(\\n            torch.nn.Linear(in_features, in_features // 2),\\n            torch.nn.ReLU(),\\n            torch.nn.Linear(in_features // 2, in_features // 4),\\n            torch.nn.ReLU(),\\n            torch.nn.Linear(in_features // 4, out_features))\\n    else:\\n        return torch.nn.Linear(in_features, out_features)\\n\\ndef l2_between_dicts(dict_1, dict_2, normalize=False):\\n    assert len(dict_1) == len(dict_2)\\n    dict_1_values = [dict_1[key] for key in sorted(dict_1.keys())]\\n    dict_2_values = [dict_2[key] for key in sorted(dict_1.keys())]\\n    dict_1_tensor = torch.cat(tuple([t.view(-1) for t in dict_1_values]))\\n    dict_2_tensor = torch.cat(tuple([t.view(-1) for t in dict_2_values]))\\n    if normalize:\\n        dict_1_tensor = (dict_1_tensor-dict_1_tensor.mean().item()) / dict_1_tensor.std().item()\\n        dict_2_tensor = (dict_2_tensor-dict_2_tensor.mean().item()) / dict_2_tensor.std().item()\\n        dict_2_tensor = dict_2_tensor.detach()\\n    return (dict_1_tensor-dict_2_tensor).pow(2).mean()",
    "Experiment Result": "Hyperparameter Settings (from `domainbed/hparams_registry.py`):\\n- `ada_lr`:\\n  - For the 'DomainNet' dataset, `ada_lr` is set to `0.1`.\\n  - For all other datasets, `ada_lr` is set to `1e-6`.\\n\\nEvaluation Settings (from `domainbed/scripts/train.py` and `domainbed/lib/misc.py`):\\n- During evaluation, if the algorithm is 'ITTA', the `misc.accuracy_tsc` function is used to calculate accuracy.\\n- `misc.accuracy_tsc` performs test-time adaptation by calling `network.test_adapt(x)` for 1 iteration on each test sample `x`.\\n- During this test-time adaptation, the `featurizer` and `classifier` parameters of the network are frozen (set to `requires_grad_(False)`), ensuring only the adaptive parameters (`test_mapping`) are updated."
}{
    "Title": "Robust Test-Time Adaptation in Dynamic Scenarios",
    "Main Contributions": "This research addresses the limitations of existing Test-Time Adaptation (TTA) methods in dynamic real-world scenarios by proposing a new setup called Practical Test-Time Adaptation (PTTA). PTTA explicitly considers both continually changing data distributions and correlatively sampled test data streams, which simultaneously occur in real applications. The main contribution is the development of Robust Test-Time Adaptation (RoTTA), a comprehensive method designed to robustly adapt pre-trained models in PTTA settings. RoTTA demonstrates state-of-the-art performance, significantly reducing classification error compared to previous TTA methods on various benchmarks.",
    "Methodology": "The proposed RoTTA method consists of three key components: 1) Robust Batch Normalization (RBN) which estimates normalization statistics by replacing current batch statistics with global ones maintained via exponential moving average (EMA) to ensure stability against correlated data. 2) Category-balanced sampling with timeliness and uncertainty (CSTU) for maintaining a memory bank. This memory bank stores category-balanced data, prioritizing newer samples and those with lower uncertainty (based on age and prediction entropy) to capture a stable snapshot of the current distribution. 3) Robust training with timeliness, which uses a teacher-student model architecture. The student model is updated by minimizing a loss that incorporates a time-aware reweighting strategy (exponential decay based on sample age) and cross-entropy between strongly-augmented student predictions and weakly-augmented teacher predictions. Only the affine parameters in RBN are trained for efficiency and stability. The teacher model is updated via EMA of the student model's parameters.",
    "Experimental Setup": "Experiments were conducted on CIFAR-10-C and CIFAR-100-C for robustness under corruptions, and on DomainNet for generalization under a huge domain gap. For CIFAR-C datasets, pre-trained WildResNet-28 and ResNeXt-29 models (from RobustBench) were used, respectively, with corruptions changing one by one at severity 5 to simulate continually changing distributions. For DomainNet, ResNet-101 models were pre-trained on a source domain and adapted to the remaining five domains. Correlatively sampled test streams were simulated using a Dirichlet distribution with parameter δ=0.1 for all datasets. The Adam optimizer with a learning rate of 1.0 × 10−3 and a batch size of 64 was used. RoTTA hyperparameters (α=0.05, ν=0.001, λt=1.0, λu=1.0, memory bank capacity N=64) were unified across experiments. The method was benchmarked against BN, PL, TENT, LAME, CoTTA, and NOTE, measuring average classification error. Ablation studies verified the effectiveness of each RoTTA component and its robustness to varying distribution changing orders, Dirichlet concentration parameters (δ), and batch sizes.",
    "Limitations": "The Robust Batch Normalization (RBN) component is considered a naive solution for normalizing correlatively sampled data and requires careful design of its α value. The current method lacks a mechanism to explicitly recover the model from a collapsed state, although RoTTA is designed to prevent such collapses. Additionally, the paper acknowledges that category similarity is only one type of correlation and further validation in diverse real-world scenarios beyond Dirichlet distribution-simulated test streams is needed.",
    "Future Research Directions": "Future work could focus on improving the RoTTA algorithm by exploring alternative or refined components. More significantly, the authors hope this work paves the way for further research into making the Practical Test-Time Adaptation (PTTA) setup even more realistic. The ultimate goal is to facilitate the robust deployment of models in real-world applications through test-time adaptation algorithms.",
    "Experiment Code": "import torch\nimport torch.nn as nn\nfrom ..utils import memory\nfrom .base_adapter import BaseAdapter\nfrom copy import deepcopy\nfrom .base_adapter import softmax_entropy\nfrom ..utils.bn_layers import RobustBN1d, RobustBN2d\nfrom ..utils.utils import set_named_submodule, get_named_submodule\nfrom ..utils.custom_transforms import get_tta_transforms\n\n\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\n\n\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\n\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(ColorJitter):\n    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma\n                                            # after contrast\n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string",
    "Experiment Result": "RoTTA Method Specific Configuration:\n- ADAPTER.NAME: \"rotta\"\n- ADAPTER.RoTTA.MEMORY_SIZE: 64 (Capacity of the memory bank for CSTU)\n- ADAPTER.RoTTA.UPDATE_FREQUENCY: 64 (Frequency at which the model is updated using memory data, typically same as MEMORY_SIZE)\n- ADAPTER.RoTTA.NU: 0.001 (Exponential Moving Average (EMA) rate for updating the teacher model parameters)\n- ADAPTER.RoTTA.ALPHA: 0.05 (Momentum for updating running statistics in Robust Batch Normalization (RBN))\n- ADAPTER.RoTTA.LAMBDA_T: 1.0 (Weighting factor for timeliness in CSTU heuristic score)\n- ADAPTER.RoTTA.LAMBDA_U: 1.0 (Weighting factor for uncertainty in CSTU heuristic score)\n\nGeneral Training/Optimization Settings:\n- OPTIM.STEPS: 1 (Number of optimization steps per forward pass in the adapter)\n- OPTIM.LR: 1e-3 (Learning rate for the optimizer)\n- BN.EPS: 1e-5 (Epsilon value for Batch Normalization layers, including RBN)\n\nData Augmentation (Strong Augmentation for Student Model):\n- INPUT.SIZE: (32, 32) (Input image size for transformations)\n- Default Gaussian Noise std: 0.005\n- ColorJitterPro brightness range: [0.6, 1.4]\n- ColorJitterPro contrast range: [0.7, 1.3]\n- ColorJitterPro saturation range: [0.5, 1.5]\n- ColorJitterPro hue range: [-0.06, 0.06]\n- ColorJitterPro gamma range: [0.7, 1.3]\n- RandomAffine degrees: [-15, 15], translate: (1/16, 1/16), scale: (0.9, 1.1)\n- GaussianBlur sigma: [0.001, 0.5]\n- RandomHorizontalFlip probability: 0.5"
}{
    "Title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
    "Main Contributions": "The paper introduces Test-Time Training (TTT), a novel approach to enhance the generalization of predictive models under distribution shifts by converting a single unlabeled test sample into a self-supervised learning problem to update model parameters before prediction. Key contributions include demonstrating substantial performance improvements on diverse image classification benchmarks (e.g., corrupted images, video frames, unknown shifts) without degrading performance on original data distributions, and showing its effectiveness in an online streaming setting. A theoretical investigation establishes that positive gradient correlation between the main and self-supervised tasks is a sufficient condition for TTT's success in convex models, which is empirically validated for deep learning.",
    "Methodology": "Test-Time Training (TTT) utilizes a Y-structured neural network with a shared feature extractor (θe) and separate branches for a main classification task (θm) and a self-supervised auxiliary task (θs). The model is initially trained jointly on both tasks using multi-task learning. At test time, for an unlabeled test sample (x), the shared feature extractor θe is fine-tuned by minimizing the self-supervised loss ls(x; θs,θe). The primary self-supervised task used is rotation prediction (classifying 0, 90, 180, or 270-degree image rotations). Data augmentation is applied to the single test sample during fine-tuning. For online Test-Time Training, parameters are initialized with those updated from the previous test sample, and only one gradient step is taken per new image. Group Normalization (GN) is employed instead of Batch Normalization (BN) to handle the small batch size during test-time updates.",
    "Experimental Setup": "Experiments were conducted using ResNet architectures (26-layer for CIFAR-10, 18-layer for ImageNet). The datasets included CIFAR-10-C and ImageNet-C (benchmarking robustness to 15 corruption types at 5 severity levels), VID-Robust (video frames for object recognition, adapted for ImageNet and CIFAR-10 models), and CIFAR-10.1 (a new test set with unknown distribution shifts). Optimization utilized Stochastic Gradient Descent (SGD) with a learning rate of 0.001 for test-time updates (10 steps for standard TTT, 1 step for online TTT). Baselines included a plain ResNet (\"object recognition task only\"), an improved joint training model (fixed at test time), Unsupervised Domain Adaptation by Self-Supervision (UDA-SS), and Adversarial Logit Pairing (ALP). Performance was evaluated using test error/accuracy, and theoretical insights were empirically validated by analyzing gradient correlation.",
    "Limitations": "The method incurs a significant computational overhead at test time, being \"2 × batch size × number of iterations times slower than regular testing.\" Its effectiveness is contingent on the self-supervised task being \"well defined and non-trivial\"; for example, trivial rotation cues (like black margins on airplane images) can hinder performance improvement. While theoretical results apply to convex models, their extension to non-convex deep learning relies primarily on empirical validation. The online version of TTT assumes \"gradually changing distribution shifts,\" implying test samples originate from smoothly evolving distributions. The concept of a variable decision boundary, central to TTT, is acknowledged to deviate from traditional learning theory, with a formal discussion deemed beyond the paper's scope.",
    "Future Research Directions": "Future work could involve extending Test-Time Training to other tasks such as segmentation and detection, and applying it in different domains like speech recognition and natural language processing. Leveraging domain-specific knowledge to design more effective special-purpose self-supervised tasks is suggested. The authors also propose using TTT as a new evaluation benchmark for general-purpose self-supervised learning methods. Further research is needed to improve computational efficiency, including exploring faster update mechanisms during training, and more in-depth analysis of strategies like thresholding or reducing update steps. A more formal theoretical discussion on the concept of a variable decision boundary is also highlighted. The paper generally encourages abandoning the self-imposed constraint of fixed decision boundaries and the artificial division between training and testing, moving towards a paradigm where learning extensively occurs post-deployment.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
    "Main Contributions": "The paper introduces Active Test-Time Adaptation (ATTA), a novel problem setting that integrates active learning within the fully Test-Time Adaptation (FTTA) framework to address significant domain distribution shifts in real-time. It provides a learning theory analysis, demonstrating that incorporating limited labeled test instances improves overall performance with theoretical guarantees. The paper also proposes a sample entropy balancing technique to prevent catastrophic forgetting (CF) in ATTA. Finally, it presents SimATTA, a simple yet effective ATTA algorithm utilizing real-time sample selection, which shows substantial performance improvements over TTA methods and comparable effectiveness to Active Domain Adaptation (ADA) methods while maintaining efficiency.",
    "Methodology": "The ATTA problem is formally defined, where a pre-trained model continuously selects informative instances from streaming test data within a budget to be labeled by an oracle and subsequently learned. The theoretical analysis establishes learning bounds for mitigating distribution shifts and avoiding catastrophic forgetting (CF). This involves scrutinizing the influence of active learning and using selective entropy minimization to identify 'source-like' low-entropy samples (pseudo-labeled by a frozen source-pretrained model) and informative high-entropy samples. The SimATTA algorithm implements this by partitioning unlabeled test samples into high and low entropy sets. High-entropy samples are actively selected using an incremental clustering technique (based on weighted K-means) to reduce redundancy and increase distribution coverage. The model is then fine-tuned using these labeled test anchors and pseudo-labeled source-like anchors, balancing training weights and sample numbers.",
    "Experimental Setup": "The method is evaluated against three settings: TTA, Enhanced TTA (TTA with access to random labeled samples), and ADA. Experiments are conducted on PACS, VLCS, Office-Home (from DomainBed), and Tiny-ImageNet-C datasets to assess OOD performance and efficiency. Source domains are 'photos' for PACS, 'Caltech101' for VLCS, and 'real' for Office-Home, with brightness corruption for Tiny-ImageNet-C. Two data stream orders are used: domain-wise and random. Baselines include source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and ADA methods (random, entropy, k-means, CLUE). ResNet-18 (PACS, VLCS, Tiny-ImageNet-C) and ResNet-50 (Office-Home) are used, initialized with ImageNet pre-trained weights. Optimization uses Adam for source pre-training and SGD for test-time adaptation, with specific learning rates and training step controls. Ablation studies analyze the impact of incremental clustering and low-entropy sample training.",
    "Limitations": "The theoretical bounds can be loose with small unlabeled test sample sizes, addressed by assuming fine-tuning is equivalent to learning with a small VC-dimension. The effectiveness of selective entropy minimization relies on the quality of the pre-trained model, and training on incorrectly predicted low-entropy samples might reinforce errors. It may not be cost-effective to use annotation budgets for low-entropy samples. The current work does not cover class-incremental problems where the support of labels changes. The incremental clustering's centroid update strategy is 'naive' and could be improved with adaptive techniques. The paper focuses on establishing a foundational framework rather than large-scale applications or extensive hyperparameter tuning for all possible scenarios.",
    "Future Research Directions": "Future work could explore developing alternative strategies to prevent catastrophic forgetting in ATTA scenarios, such as correcting incorrectly predicted low-entropy samples instead of solely relying on pseudo-labeling. There is considerable scope for designing ATTA methods for large language models (LLMs), where retraining is computationally expensive and source data may be inaccessible. Further research could bridge the gap between causal inference and deep learning (e.g., through causal representation learning) to enhance OOD generalization. Additionally, a comprehensive survey or benchmark that systematically compares domain generalization, domain adaptation, and test-time adaptation methods based on their underlying assumptions is suggested.",
    "Experiment Code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1",
    "Experiment Result": "The SimATTA algorithm is implemented as follows:\n- **Teacher Model Initialization**: The `teacher` model is initialized as a deep copy of the `student` model (which is `self.model`). `self.update_teacher(0)` is called at initialization, effectively making the teacher a frozen snapshot of the student model at the beginning of adaptation. It does not update during the adaptation process.\n- **Budget Management**: `self.budgets` tracks the total number of actively selected samples. It increases by the number of new samples (`len(closest)`) added to `self.anchors` in each adaptation step.\n- **Cluster Parameters (High Entropy Samples)**:\n  - `self.n_clusters`: Initial number of clusters for target samples is set to 10.\n  - `self.nc_increase`: This parameter, specified by `self.config.atta.SimATTA.nc_increase`, determines how much `self.n_clusters` increases in each adaptation step. If `self.target_cluster` is false, `self.n_clusters` is explicitly set to 0.\n  - `self.config.atta.SimATTA.eh`: Entropy high threshold for selecting high-entropy samples from the current batch.\n  - `self.target_cluster`: A boolean flag (`self.config.atta.SimATTA.target_cluster`) indicating whether to use incremental clustering for target samples. If `False`, direct entropy-based selection is used.\n- **Cluster Parameters (Low Entropy Samples)**:\n  - `self.source_n_clusters`: Initial number of samples to select for source-like samples is 100. It increases by 1 in each adaptation step.\n  - `self.LE`: A boolean flag (`self.config.atta.SimATTA.LE`) enabling the selection of low-entropy source-like samples using the frozen teacher model.\n  - `self.config.atta.SimATTA.el`: Entropy low threshold for selecting low-entropy samples.\n- **Sample Selection (`sample_select`)**:\n  - The method partitions unlabeled test samples into high and low entropy sets based on `ent_bound` thresholds (`eh` for high, `el` for low entropy).\n  - For high-entropy samples (when `ent_beta=1` and `incremental_cluster=True`): It uses weighted K-Means (`sklearn.cluster.KMeans` with `algorithm='elkan'` or `ATTA.utils.fast_pytorch_kmeans.KMeans` if `gpu_clustering` is enabled) to select samples, reducing redundancy and increasing distribution coverage. `n_init=10` is used for K-Means. `sample_weight` combines existing anchor weights with new sample weights. `self.accumulate_weight` is `True` to allow previous anchor weights to be accumulated.\n  - For low-entropy samples (when `ent_beta=0` and `incremental_cluster=False`): Samples are selected purely based on entropy sorting, without clustering.\n  - `use_pseudo_label=True` is used for selecting low-entropy (source-like) anchors, where pseudo-labels are generated by the teacher model. True labels are used for high-entropy (target) anchors.\n  - `self.config.atta.gpu_clustering`: A boolean flag to determine whether to use GPU-accelerated K-Means.\n- **Model Fine-tuning (`cluster_train`)**:\n  - The model is fine-tuned using `labeled test anchors` (`self.anchors`) and `pseudo-labeled source-like anchors` (`self.source_anchors`).\n  - **Training Weights**: A dynamic `alpha` value balances the contribution of source and target anchors to the loss. `alpha` is calculated as `target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`.\n  - **Cold Start**: If the number of source anchors is less than `self.config.atta.SimATTA.cold_start`, `alpha` is capped at `min(0.2, alpha)`.\n  - **Optimizer**: `torch.optim.SGD` with a learning rate (`self.config.atta.SimATTA.lr`) and `momentum=0.9`.\n  - **Loss Function**: `self.config.metric.loss_func` (e.g., cross-entropy for classification) is used for both source and target losses, combined as `(1 - alpha) * L_S + alpha * L_T`.\n  - **Training Steps**: The training runs for a maximum of `self.config.atta.SimATTA.steps` iterations.\n  - **Early Stopping**: Training includes an early stopping mechanism, breaking if `tol > 5` after `loss_window` reaches `self.config.atta.SimATTA.stop_tol` without a new lowest loss.\n- **Anchor Feature Update**: After each fine-tuning step, the features of `self.anchors` are re-extracted using the currently updated student model (`self.update_anchors_feats`).\n- **Batch Normalization**: If `self.config.model.freeze_bn` is `False`, BatchNorm layers (`nn.BatchNorm1d`, `nn.BatchNorm2d`) are enabled and their `momentum` is set to `0.1`."
}{
    "Title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
    "Main Contributions": "This paper introduces a novel online evaluation protocol for Test-Time Adaptation (TTA) methods that accounts for computational time constraints by penalizing slower methods with fewer samples for adaptation. The core problem addressed is that existing TTA evaluation protocols overlook the computational cost, leading to an unrealistic assessment of methods' real-world applicability. The main findings are that when inference speed is considered, simpler and faster TTA approaches can outperform more sophisticated but slower methods (e.g., SHOT outperforms SAR), highlighting the importance of developing practical TTA methods that are both accurate and efficient.",
    "Methodology": "The proposed methodology is a Realistic TTA evaluation protocol, which explicitly considers the relation between a TTA method's speed and the constant speed of an incoming data stream. It defines a 'relative adaptation speed' C(g) as the integer ratio of the stream's speed to the method's speed. If a method is k times slower than the stream (C(g)=k), it is allowed to adapt only every kth sample. The remaining samples are processed by the most recent adapted model (fθt+1) or the base model if no adaptation has occurred, which is assumed to run in real-time. This protocol intrinsically penalizes slower methods, as long adaptation times lead to dropped samples and thus fewer opportunities for adaptation. C(g) is computed online for each input, accounting for hardware and input-dependent variability.",
    "Experimental Setup": "The evaluation was conducted using a ResNet-50-BN3 backbone pretrained on ImageNet (Deng et al., 2009). The data stream revealed batches of size 64 (except for MEMO, which used single images). Experiments were performed on multiple datasets: ImageNet-C (Hendrycks & Dietterich, 2019) with a corruption level of 5 across 15 corruptions, CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and ImageNet-3DCC (Kar et al., 2022). The study benchmarked 15 state-of-the-art TTA methods (e.g., AdaBN, SHOT, TENT, SAR, EATA, CoTTA, DDA) and evaluated them under four scenarios: (i) episodic domain shifts, (ii) continual adaptation to sequential domain shifts, (iii) varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), and (iv) practical TTA with label correlation (PTTA setup on CIFAR10-C). Additional evaluations included different architectures like ViT and ResNet-18, and hyper-parameter tuning for TENT.",
    "Limitations": "The current evaluation protocol is limited by its assumption that the data stream waits for TTA methods to complete adaptation, which is unrealistic in real-time deployment. Many sophisticated TTA methods incur high computational overhead, making them impractical under realistic time constraints. Data-dependent approaches like MEMO and DDA are particularly inefficient, with C(g) values of 54 and 810 respectively, meaning they miss a vast majority of samples and often perform close to the non-adapted baseline. The performance of TTA methods can vary significantly with batch size, with most methods failing at small batch sizes. Also, sample rejection methods like SAR that rely on gradients for rejection suffer performance drops due to reduced speed, unlike EATA which uses a faster, forward-pass-based rejection. The paper also mentions a 'single model evaluation scheme' as a stricter scenario where concurrently deploying two models is not assumed, heavily penalizing slower methods.",
    "Future Research Directions": "The paper calls for future research to focus on increasing the efficiency of data-dependent adaptation methods due to their extreme inefficiency under the proposed realistic protocol. It hopes that the new evaluation scheme will inspire the development of future TTA methods that consider inference speed as a critical dimension affecting real-world performance. Implicitly, research into TTA methods that perform well with smaller batch sizes and those with efficient sample rejection mechanisms (like EATA) is encouraged.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Improved Test-Time Adaptation for Domain Generalization",
    "Main Contributions": "The paper addresses the distribution shift problem in Domain Generalization (DG) by proposing an Improved Test-Time Adaptation (ITTA) method. The main contributions are: 1) Introducing a learnable consistency loss for the test-time training (TTT) task, which features learnable parameters that can be adjusted to align better with the main prediction task by enforcing shared optimization directions. 2) Proposing the inclusion of additional adaptive parameters within the trained model, with the strategy of updating only these new parameters during the test phase, leaving the original model parameters unchanged. Extensive experiments demonstrate that ITTA achieves superior performance compared to state-of-the-art methods on various DG benchmarks for both multi-source and single-source DG tasks.",
    "Methodology": "ITTA improves Test-Time Training (TTT) through two main strategies. First, a learnable consistency loss (Lwcont) is introduced for the TTT auxiliary task. This loss is defined as the L2 norm of the output of a weight subnetwork (fw) applied to the difference between original and augmented feature representations (z - z'). The fw subnetwork, composed of stacked ReLU(a * h + b) layers, allows for flexible consistency measurement. During training, the feature extractor and classifier are optimized using a combined main loss and learnable consistency loss (Lmain + αLwcont). The fw subnetwork is updated by minimizing the difference between the normalized gradients of Lmain and Lwcont with respect to the feature extractor parameters, ensuring alignment. Second, additional adaptive parameter blocks (fΘ) are introduced after each block of the pretrained feature extractor during the test-time adaptation phase. Only these fΘ parameters are updated using the learned consistency loss from the target domain data, following an online adaptation setting. The augmentation strategy used is an existing method that modifies intermediate activations to create augmented feature representations.",
    "Experimental Setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet, covering diverse image categories and domain shifts. The backbone model for all experiments was an ImageNet-pretrained ResNet18 with 4 blocks as the feature extractor. The proposed adaptive parameters (fΘ) consisted of 4 blocks, each with 5 learnable layers, while the weight subnetwork (fw) used 10 learnable layers. The classifier was an MLP layer. Hyperparameters, including random seeds, learning rates, batch size, and augmentation skills, were dynamically set according to the rigorous DomainBed benchmark protocol. Evaluations included multi-source domain generalization (using a leave-one-out strategy) and single-source domain generalization (trained on one domain, tested on three others on PACS). Each setting involved 60 trials, with model selection based on the 'training-domain validate set' method, and final performance reported as average accuracy. Comparisons were made against 22 existing DG methods and several TTT-based approaches (TTT, MT3, TENT, MetaReg, Feature-Critic, TTT++).",
    "Limitations": "The proposed ITTA method incurs increased computational costs. While updating the model without the weight subnetwork (fw disabled) requires 1 forward and 1 backward pass, enabling fw for the learnable consistency loss and its alignment objective necessitates computing second-order derivatives, which demands 1 additional forward and 3 additional backward passes. This significantly increases the burden on the system during training. During testing, the adaptation process involves extra forward and backward steps, leading to increased FLOPS and inference time compared to the baseline model.",
    "Future Research Directions": "Future efforts will focus on simplifying the overall optimization process of ITTA and reducing its computational cost to alleviate the extra burden on the system caused by the learnable consistency loss and its gradient alignment step.",
    "Experiment Code": "class ITTA(Algorithm):\n    \"\"\"\n    Improved Test-Time Adaptation (ITTA)\n    \"\"\"\n\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\n        super(ITTA, self).__init__(input_shape, num_classes, num_domains,\n                                  hparams)\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\n        self.classifier = networks.Classifier(\n            self.featurizer.n_outputs,\n            num_classes,\n            self.hparams['nonlinear_classifier'])\n        self.test_mapping = networks.MappingNetwork() #specialized for resnet18\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\n        self.optimizer = torch.optim.Adam([\n            {'params': self.featurizer.parameters()},\n            {'params': self.classifier.parameters()}],\n            lr=self.hparams[\"lr\"],\n            weight_decay=self.hparams['weight_decay']\n        )\n        self.MSEloss = nn.MSELoss()\n        self.adaparams = networks.Adaparams() #specialized for resnet18\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\n\n    def _get_grads(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward(inputs=list(self.featurizer.parameters()),\n                          retain_graph=True, create_graph=True)\n        dict = OrderedDict(\n            [\n                (name, weights.grad.clone().view(weights.grad.size(0),-1))\n                for name, weights in self.featurizer.named_parameters()\n            ]\n        )\n\n        return dict\n\n    def update(self, minibatches, unlabeled=None):\n        all_x = torch.cat([x for x,y in minibatches])\n        all_y = torch.cat([y for x,y in minibatches])\n        ############################# this is for network update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        loss = loss_reg + loss_cla\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        ############################# this is for adaparams update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        dict_reg = self._get_grads(loss_reg)\n        dict_cla = self._get_grads(loss_cla)\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\n        self.adaparams_optimizer.zero_grad()\n        penalty.backward(inputs=list(self.adaparams.parameters()))\n        self.adaparams_optimizer.step()\n\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\n\n    def test_adapt(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\n        ########## small lr for large datasets\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\n        self.test_optimizer.zero_grad()\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\n        self.test_optimizer.step()\n\n    def predict(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori = self.test_mapping.fea1(z_ori)\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\n        z_ori = self.test_mapping.fea2(z_ori)\n        z_ori = self.featurizer.fea3(z_ori)\n        z_ori = self.test_mapping.fea3(z_ori)\n        z_ori = self.featurizer.fea4(z_ori)\n        z_ori = self.test_mapping.fea4(z_ori)\n        z_ori = self.featurizer.flat(z_ori)\n        return self.classifier(z_ori)\n\nclass MappingNetwork(torch.nn.Module):\n    def __init__(self, depth=5):\n        super().__init__()\n        self.depth = depth\n        self.weight1 = nn.ParameterList()\n        self.bias1 = nn.ParameterList()\n        self.weight2 = nn.ParameterList()\n        self.bias2 = nn.ParameterList()\n        self.weight3 = nn.ParameterList()\n        self.bias3 = nn.ParameterList()\n        self.weight4 = nn.ParameterList()\n        self.bias4 = nn.ParameterList()\n        for i in range(depth):\n            self.weight1.append(nn.Parameter(torch.ones((64,56,56))))\n            self.bias1.append(nn.Parameter(torch.zeros((64,56,56))))\n\n            self.weight2.append(nn.Parameter(torch.ones((128,28,28))))\n            self.bias2.append(nn.Parameter(torch.zeros((128,28,28))))\n\n            self.weight3.append(nn.Parameter(torch.ones((256,14,14))))\n            self.bias3.append(nn.Parameter(torch.zeros((256,14,14))))\n\n            self.weight4.append(nn.Parameter(torch.ones((512, 7, 7))))\n            self.bias4.append(nn.Parameter(torch.zeros((512, 7, 7))))\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def fea1(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight1[i] * x + self.bias1[i])\n        x = self.weight1[i+1] * x + self.bias1[i+1]\n        return x\n\n    def fea2(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight2[i] * x + self.bias2[i])\n        x = self.weight2[i + 1] * x + self.bias2[i + 1]\n        return x\n\n    def fea3(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight3[i] * x + self.bias3[i])\n        x = self.weight3[i + 1] * x + self.bias3[i + 1]\n        return x\n\n    def fea4(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight4[i] * x + self.bias4[i])\n        x = self.weight4[i+1] * x + self.bias4[i+1]\n        return x\n\n\nclass Adaparams(nn.Module):\n    def __init__(self, depth=10):\n        super(Adaparams, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.depth = depth\n        self.weight = nn.ParameterList()\n        self.bias = nn.ParameterList()\n        for i in range(depth):\n            self.weight.append(nn.Parameter(torch.ones(512)))\n            self.bias.append(nn.Parameter(torch.zeros(512)))\n\n    def forward(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight[i] * x + self.bias[i])\n        x = self.weight[i+1] * x + self.bias[i+1]\n        return x\n        \nclass ResNet_ITTA(torch.nn.Module):\n    \"\"\"ResNet with the softmax chopped off and the batchnorm frozen\"\"\"\n    def __init__(self, input_shape, hparams):\n        super(ResNet_ITTA, self).__init__()\n        if hparams['resnet18']:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 512\n        else:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 2048\n\n        nc = input_shape[0]\n        if nc != 3:\n            tmp = self.network.conv1.weight.data.clone()\n\n            self.network.conv1 = nn.Conv2d(\n                nc, 64, kernel_size=(7, 7),\n                stride=(2, 2), padding=(3, 3), bias=False)\n\n            for i in range(nc):\n                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]\n\n        # save memory\n        self.network.fc = Identity()\n        self.isaug = True\n        self.freeze_bn()\n        self.hparams = hparams\n        self.dropout = nn.Dropout(hparams['resnet_dropout'])\n        self.eps = 1e-6\n\n    def mixstyle(self, x):\n        alpha = 0.1\n        beta = torch.distributions.Beta(alpha, alpha)\n        B = x.size(0)\n        mu = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True)\n        sig = (var + self.eps).sqrt()\n        mu, sig = mu.detach(), sig.detach()\n        x_normed = (x - mu) / sig\n        lmda = beta.sample((B, 1, 1, 1))\n        lmda = lmda.to(x.device)\n        perm = torch.randperm(B)\n        mu2, sig2 = mu[perm], sig[perm]\n        mu_mix = mu * lmda + mu2 * (1 - lmda)\n        sig_mix = sig * lmda + sig2 * (1 - lmda)\n        return x_normed * sig_mix + mu_mix\n\n    def fea_forward(self, x):\n        x = self.fea3(x)\n        x = self.fea4(x)\n\n        x = self.flat(x)\n        return x\n\n    def fea2(self, x, aug_x):\n        x = self.network.layer2(x)\n        aug_x = self.network.layer2(aug_x)\n        if not self.isaug:\n            aug_x = self.mixstyle(aug_x)\n        return x, aug_x\n\n    def fea3(self, x):\n        x = self.network.layer3(x)\n        return x\n\n    def fea4(self, x):\n        x = self.network.layer4(x)\n        return x\n\n    def flat(self, x):\n        x = self.network.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.network.fc(x)\n        x = self.dropout(x)\n        return x\n\n    def forward(self, x):\n        \"\"\"Encode x into a feature vector of size n_outputs.\"\"\"\n        x = self.network.conv1(x)\n        x = self.network.bn1(x)\n        x = self.network.relu(x)\n        x = self.network.maxpool(x)\n\n        x = self.network.layer1(x)\n        if random.random() > 0.5:\n            self.isaug = True\n            aug_x = self.mixstyle(x)\n        else:\n            self.isaug = False\n            aug_x = x\n\n        return x, aug_x\n\n    def train(self, mode=True):\n        \"\"\"\n        Override the default train() to freeze the BN parameters\n        \"\"\"\n        super().train(mode)\n        self.freeze_bn()\n\n    def freeze_bn(self):\n        for m in self.network.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()",
    "Experiment Result": "ITTA improves Test-Time Training (TTT) using a learnable consistency loss (Lwcont) and adaptive parameter blocks (fΘ). The architecture comprises a ResNet_ITTA featurizer (ResNet18, ImageNet pretrained weights, Batch Normalization layers frozen during training), a linear classifier, a MappingNetwork for adaptive parameter blocks (fΘ, depth 5, specialized for ResNet18), and an Adaparams subnetwork for the learnable consistency loss (fw, depth 10, MLP-like for 512-dim features).\n\nDuring training, the main network (featurizer + classifier) is optimized with an Adam optimizer (learning rate: `hparams[\"lr\"]`, weight decay: `hparams['weight_decay']`). The `fw` subnetwork (Adaparams) is optimized with an Adam optimizer (learning rate: `hparams[\"lr\"] * 0.1`). The loss functions include a main classification loss (`Lmain = F.cross_entropy` for both original and augmented features) and the learnable consistency loss (`Lwcont = nn.MSELoss()` applied to `fw(z_aug - z_ori)` and zero tensor). The total loss for the main network update is `Lmain + Lwcont`. The `fw` subnetwork is updated by minimizing a penalty calculated as `l2_between_dicts(normalized_gradients(Lwcont), normalized_gradients(Lmain)) * 0.1`.\n\nThe augmentation strategy involves `mixstyle` applied to intermediate feature activations (either after `layer1` or `layer2` of ResNet_ITTA), with a 50% probability of initial application. The `mixstyle` parameters include `alpha=0.1` for the Beta distribution and `eps=1e-6` for numerical stability.\n\nFor test-time adaptation, adaptive parameter blocks (`fΘ` / MappingNetwork) are updated using an Adam optimizer (learning rate: `hparams[\"lr\"] * 0.1`). These `fΘ` parameters are updated by minimizing `Lwcont`, specifically `nn.MSELoss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']`. This adaptation is performed for 1 iteration on test data before prediction. The `ada_lr` hyperparameter is set to 0.1 for the DomainNet dataset and 1e-6 for other datasets. General hyperparameters include a learning rate (`lr`) of 5e-5 for non-small image datasets and 1e-3 for small image datasets, and a weight decay of 0. `resnet18` is set to True (default), implying 512 `n_outputs` for the featurizer. `nonlinear_classifier` and `resnet_dropout` are both 0.0 by default. `batch_size` is typically 32 for non-small image datasets (e.g., DomainNet) and 64 for small image datasets."
}{
    "Title": "Test Time Adaptation via Conjugate Pseudo-labels",
    "Main Contributions": "The paper addresses the challenge of selecting an effective Test-Time Adaptation (TTA) loss for neural networks facing distribution shifts. It presents a surprising phenomenon: meta-learning the 'best' TTA loss recovers a temperature-scaled softmax-entropy for cross-entropy trained classifiers and a negative squared error for squared loss trained classifiers. To explain this, the paper proposes a novel framework analyzing TTA through the lens of the training loss's convex conjugate. This framework provides a generic recipe to derive TTA losses, recovering existing successful methods like TENT and suggesting new ones for various training losses (e.g., PolyLoss). The approach is interpreted as self-training with specific 'conjugate pseudo-labels' (∇f(h(x))). Empirically, this method consistently outperforms other TTA alternatives across a wide range of domain adaptation benchmarks.",
    "Methodology": "The methodology involves two main parts: an exploratory meta-learning phase and a principled conjugate function framework. Initially, the authors meta-learn a TTA loss function, parameterized by a neural network (Transformer + MLP), by differentiating through the adaptation process to find parameters that yield optimal performance on distribution shifts. This meta-learning revealed the dependence of the optimal TTA loss on the source classifier's training loss. Subsequently, the paper introduces a framework based on the convex conjugate function. For a general class of loss functions L(h(x),y) = f(h(x)) - yᵀh(x), the conjugate adaptation loss is defined as Lconj(hθ(x)) = -f⋆(∇f(hθ(x))), which can also be expressed as f(hθ(x)) - ∇f(hθ(x))ᵀhθ(x). This loss is shown to be equivalent to self-training with 'conjugate pseudo-labels' ˜yCPLθ(x) = ∇f(hθ(x)). The adaptation process (Algorithm 1) involves iterative updates of the model parameters by optimizing this conjugate pseudo-labeling loss on batches of unlabeled test data, typically fine-tuning only the learnable scale and shift parameters of batch normalization layers, often with an additional temperature scaling.",
    "Experimental Setup": "The evaluation was conducted on three common corruption benchmarks: CIFAR-10-C, CIFAR-100-C, and ImageNet-C, reporting averaged errors. Additionally, three domain adaptation datasets were used: SVHN to MNIST, ImageNet to ImageNet-R, and VISDA-C (synthetic to real). Source classifiers included ResNet-26 for CIFAR, ResNet-18 for SVHN, and ResNet-50 for ImageNet and VisDA-C, trained with cross-entropy, PolyLoss, and squared loss. Baselines for comparison included self-training with 'hard' pseudo-labels (argmax with confidence thresholding) and 'soft' pseudo-labels (softmax), Entropy Minimization (TENT), Robust Pseudo-Label, and MEMO (batch version). Test-time adaptation involved updating only batch normalization parameters and statistics. Hyperparameters (learning rate from {1e-1 to 1e-4} and temperature from {1 to 5}) were tuned via grid-search on validation noises for corruption benchmarks, while fixed values (T=1, specific LRs) were used for domain adaptation tasks without validation sets. Experiments were performed on A6000 GPUs.",
    "Limitations": "The meta-learning framework used in this work was restricted to learning functions over the logits of individual inputs, suggesting that more complex functions involving intermediate representations or batch interactions could be explored. The paper provides strong evidence for the effectiveness of the conjugate loss but does not offer a complete theoretical answer to what constitutes the 'optimal' test-time adaptation loss. Furthermore, the role and impact of common TTA heuristics, such as updating only batch normalization parameters and temperature scaling, still require more concrete understanding. The applicability of self-training based approaches to various real-world distribution shifts remains an open problem.",
    "Future Research Directions": "Future research directions include expanding the meta-learning framework to incorporate intermediate representations and learning functions that account for interactions within a batch of inputs. A more concrete theoretical understanding of the role of heuristics like batch normalization parameter updates and temperature scaling in TTA is also a promising avenue. Further work is needed to identify the types of real-world distribution shifts where self-training based approaches, particularly those utilizing conjugate pseudo-labels, are most beneficial. Additionally, extending and applying the conjugate pseudo-labeling framework to other related settings, such as semi-supervised learning, is suggested.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering",
    "Main Contributions": "The paper addresses the confusion in Test-Time Training (TTT) experimental settings by categorizing TTT protocols based on two key factors: modification of source training objective and sequential inference. It proposes a novel Test-Time Anchored Clustering (TTAC) approach under the most realistic protocol, sequential TTT (sTTT), to enable stronger test-time feature learning. TTAC discovers and matches target domain clusters to source domain anchors, improving generalization. It also introduces pseudo label filtering and iterative updating for efficiency and effectiveness. The main finding is that TTAC consistently outperforms state-of-the-art methods on six TTT datasets across all TTT protocols, providing a fair benchmarking framework.",
    "Methodology": "TTAC employs a mixture of Gaussians to model clusters in the target domain, using category-wise source domain statistics as anchors. It minimizes the KL-Divergence between target and source component Gaussians for category-wise alignment. For robustness against noisy predictions, a pseudo label filtering mechanism is developed, combining temporal consistency filtering (based on exponential moving average posteriors) and posterior probability filtering. Component Gaussians are updated only by samples passing these filters. To utilize all test samples, a global feature alignment objective minimizes the KL-Divergence between global source and target feature distributions. An efficient iterative updating strategy is proposed for running statistics of target domain Gaussians, avoiding storing all features and incorporating a clipping mechanism to prevent gradient vanishing. The overall TTAC algorithm uses a fixed-length queue for recent test samples and optimizes a combined anchored clustering and global feature alignment loss.",
    "Experimental Setup": "The method is evaluated on six TTT datasets: CIFAR10-C, CIFAR100-C, ImageNet-C (for corrupted images), CIFAR10.1 (for hard samples), VisDA-C (for synthetic-to-real adaptation), and ModelNet40-C (for 3D point clouds). Classification error rate (%) is the primary benchmark. ResNet-50 is used for image datasets and DGCNN for ModelNet40-C, optimized with SGD with momentum. Specific hyperparameters (batch size, learning rate) are detailed for each dataset. The evaluation compares TTAC against direct testing (TEST) and state-of-the-art methods (BN, TENT, T3A, SHOT, TTT-R, TTT++) across four categorized TTT protocols (N-O, Y-O, N-M, Y-M). Ablation studies are performed on individual TTAC components (anchored clustering, pseudo label filtering, global feature alignment). Further analyses include cumulative performance, T-SNE visualizations of features, source-free TTT, impact of test sample queue length and update epochs, data streaming order, alternative clustering strategies, hyperparameter sensitivity, and wall-clock computation cost.",
    "Limitations": "The main limitation is the computational overhead, as TTAC requires backpropagation at test time, making it 2-5 times more computationally expensive than methods like BN and TENT. This implies that dedicated additional hardware might be necessary for test-time adaptation. Another limitation is that while TTAC consistently outperforms baselines, its performance on severely corrupted target data, particularly at higher corruption levels, is relatively weaker, indicating a need for further improvement in such extreme scenarios. A theoretical analysis for the observed superiority of KL-Divergence over L2 alignment for feature alignment is also noted as absent.",
    "Future Research Directions": "Future research could focus on providing a theoretical analysis to explain why KL-Divergence is empirically superior to L2 alignment for feature alignment in test-time training. Further work is also needed to improve performance under more severely corrupted scenarios. Additionally, ongoing efforts could explore methods to reduce the computational overhead of test-time training, potentially mitigating the need for additional dedicated devices for adaptation.",
    "Experiment Code": "parser = argparse.ArgumentParser()\nparser.add_argument('--dataset', default='cifar10')\nparser.add_argument('--dataroot', default='./data')\nparser.add_argument('--batch_size', default=128, type=int)\nparser.add_argument('--batch_size_align', default=512, type=int)\nparser.add_argument('--workers', default=0, type=int)\nparser.add_argument('--num_sample', default=1000000, type=int)\nparser.add_argument('--lr', default=0.001, type=float)\nparser.add_argument('--iters', default=4, type=int)\nparser.add_argument('--outf', default='.')\nparser.add_argument('--level', default=5, type=int)\nparser.add_argument('--corruption', default='snow')\nparser.add_argument('--resume', default=None, help='directory of pretrained model')\nparser.add_argument('--ckpt', default=None, type=int)\nparser.add_argument('--ssl', default='contrastive', help='self-supervised task')\nparser.add_argument('--temperature', default=0.5, type=float)\nparser.add_argument('--align_ext', action='store_true')\nparser.add_argument('--align_ssh', action='store_true')\nparser.add_argument('--fix_ssh', action='store_true')\nparser.add_argument('--with_ssl', action='store_true', default=False)\nparser.add_argument('--with_shot', action='store_true', default=False)\nparser.add_argument('--without_global', action='store_true', default=False)\nparser.add_argument('--without_mixture', action='store_true', default=False)\nparser.add_argument('--filter', default='ours', choices=['ours', 'posterior', 'none'])\nparser.add_argument('--model', default='resnet50', help='resnet50')\nparser.add_argument('--seed', default=0, type=int)\nargs = parser.parse_args()\n\nclass_num = 10 if args.dataset == 'cifar10' else 100\n\n# Offline Feature Summarization (Source Domain Statistics)\next_src_mu, ext_src_cov, ssh_src_mu, ssh_src_cov, mu_src_ext, cov_src_ext, mu_src_ssh, cov_src_ssh = offline(offlineloader, ext, classifier, head, class_num)\nbias = cov_src_ext.max().item() / 30.\nbias2 = cov_src_ssh.max().item() / 30.\ntemplate_ext_cov = torch.eye(2048).cuda() * bias\ntemplate_ssh_cov = torch.eye(128).cuda() * bias2\n\n# Initialization of Target Domain Gaussians and EMA for Pseudo-labels\next_src_mu = torch.stack(ext_src_mu)\next_src_cov = torch.stack(ext_src_cov) + template_ext_cov[None, :, :]\nsource_component_distribution = torch.distributions.MultivariateNormal(ext_src_mu, ext_src_cov)\ntarget_compoent_distribution = torch.distributions.MultivariateNormal(ext_src_mu, ext_src_cov)\n\nsample_predict_ema_logit = torch.zeros(len(tr_dataset), class_num, dtype=torch.float)\nsample_predict_alpha = torch.ones(len(tr_dataset), dtype=torch.float)\nema_alpha = 0.9\n\nema_n = torch.zeros(class_num).cuda()\nema_ext_mu = ext_src_mu.clone()\nema_ext_cov = ext_src_cov.clone()\n\nema_ext_total_mu = torch.zeros(2048).float()\nema_ext_total_cov = torch.zeros(2048, 2048).float()\n\nema_ssh_total_mu = torch.zeros(128).float()\nema_ssh_total_cov = torch.zeros(128, 128).float()\n\nema_total_n = 0.\nif class_num == 10:\n    ema_length = 128\n    mini_batch_length = 4096\nelse:\n    ema_length = 64\n    mini_batch_length = 4096\n\nif class_num == 10:\n    loss_scale = 0.05\nelse:\n    loss_scale = 0.5\n\nmini_batch_indices = []\n\n# Main adaptation loop for each incoming test batch\nfor te_batch_idx, (te_inputs, te_labels) in enumerate(teloader):\n    # Fixed-length queue for recent test samples\n    mini_batch_indices.extend(te_inputs[-1].tolist())\n    mini_batch_indices = mini_batch_indices[-mini_batch_length:]\n\n    # Dataloader re-creation for subset of recent samples\n    # (Code for tr_dataset_subset, tr_dataloader, tr_dataset_extra_subset, tr_extra_dataloader, tr_extra_dataloader_iter omitted for brevity)\n\n    for iter_id in range(min(args.iters, int(len(mini_batch_indices) / 256) + 1) + 1):\n        if iter_id > 0:\n            sample_predict_alpha = torch.where(sample_predict_alpha < 1, sample_predict_alpha + 0.2, torch.ones_like(sample_predict_alpha))\n\n        for batch_idx, (inputs_dummy, labels_dummy) in enumerate(tr_dataloader): # inputs_dummy/labels_dummy are for SSL, actual inputs for TTAC from tr_extra_dataloader_iter\n            optimizer.zero_grad()\n\n            # If args.with_ssl is True, SSL loss computation happens here (omitted for TTAC core method focus)\n\n            if iter_id > 0: # Actual TTAC adaptation begins after first iteration\n                loss = 0.\n                # Fetch inputs from the extra dataloader (containing current mini-batch samples from queue)\n                try:\n                    inputs, indexes = next(tr_extra_dataloader_iter)\n                except StopIteration:\n                    # Re-initialize iterator if exhausted\n                    tr_extra_dataloader_iter = iter(tr_extra_dataloader)\n                    inputs, indexes = next(tr_extra_dataloader_iter)\n\n                inputs = inputs.cuda()\n\n                feat_ext = ext(inputs)\n                logit = classifier(feat_ext)\n                feat_ssh = head(feat_ext)\n\n                # Pseudo-label generation with temporal consistency filtering\n                with torch.no_grad():\n                    ext.eval()\n                    origin_images = inputs\n                    origin_image_index = indexes\n                    predict_logit = net(origin_images)\n                    softmax_logit = predict_logit.softmax(dim=1).cpu()\n\n                    old_logit = sample_predict_ema_logit[origin_image_index, :]\n                    max_val, max_pos = softmax_logit.max(dim=1)\n                    old_max_val = old_logit[torch.arange(max_pos.shape[0]), max_pos]\n                    accept_mask = max_val > (old_max_val - 0.001)\n\n                    sample_predict_alpha[origin_image_index] = torch.where(accept_mask, sample_predict_alpha[origin_image_index], torch.zeros_like(accept_mask).float())\n\n                    sample_predict_ema_logit[origin_image_index, :] = \\\n                        torch.where(sample_predict_ema_logit[origin_image_index, :] == torch.zeros(class_num), \\\n                                    softmax_logit, \\\n                                    (1 - ema_alpha) * sample_predict_ema_logit[origin_image_index, :] + ema_alpha * softmax_logit)\n\n                    pro, pseudo_label = sample_predict_ema_logit[origin_image_index].max(dim=1)\n                    ext.train()\n                    del predict_logit\n\n                # Pseudo-label filtering mechanism\n                if args.filter == 'ours':\n                    pseudo_label_mask = (sample_predict_alpha[origin_image_index] == 1) & (pro > 0.9)\n                    feat_ext2 = feat_ext[pseudo_label_mask]\n                    feat_ssh2 = feat_ssh[pseudo_label_mask]\n                    pseudo_label2 = pseudo_label[pseudo_label_mask].cuda()\n                elif args.filter == 'posterior':\n                    with torch.no_grad():\n                        posterior = target_compoent_distribution.log_prob(feat_ext[:, None, :]) # log prob\n                        posterior_tmp = posterior.max(dim=1, keepdim=True)[0] - math.log((2 ** 127) / 10) # B, K\n                        posterior -= posterior_tmp\n                        posterior = posterior.exp() # prob / exp(posterior_tmp)\n                        posterior /= posterior.sum(dim=1, keepdim=True)\n                        posterior = posterior.transpose(0, 1).detach()  # K, N\n                # else: args.filter == 'none'\n\n                if args.align_ext:\n                    if not args.without_mixture:\n                        # Mixture Gaussian Alignment (Category-wise KL-Divergence & Iterative Update with Clipping)\n                        if args.filter != 'posterior': # Using pseudo_labels directly\n                            b, d = feat_ext2.shape\n                            feat_ext2_categories = torch.zeros(class_num, b, d).cuda() # K, N, D\n                            feat_ext2_categories.scatter_add_(dim=0, index=pseudo_label2[None, :, None].expand(-1, -1, d), src=feat_ext2[None, :, :])\n\n                            num_categories = torch.zeros(class_num, b, dtype=torch.int).cuda() # K, N\n                            num_categories.scatter_add_(dim=0, index=pseudo_label2[None, :], src=torch.ones_like(pseudo_label2[None, :], dtype=torch.int))\n\n                            ema_n += num_categories.sum(dim=1) # K\n                            alpha = torch.where(ema_n > ema_length, torch.ones(class_num, dtype=torch.float).cuda() / ema_length, 1. / (ema_n + 1e-10))\n\n                            delta_pre = (feat_ext2_categories - ema_ext_mu[:, None, :]) * num_categories[:, :, None] # K, N, D\n                            delta = alpha[:, None] * delta_pre.sum(dim=1) # K, D\n                            new_component_mean = ema_ext_mu + delta\n                            new_component_cov = ema_ext_cov \\\n                                                + alpha[:, None, None] * ((delta_pre.permute(0, 2, 1) @ delta_pre) - num_categories.sum(dim=1)[:, None, None] * ema_ext_cov) \\\n                                                - delta[:, :, None] @ delta[:, None, :]\n\n                            with torch.no_grad():\n                                ema_ext_mu = new_component_mean.detach()\n                                ema_ext_cov = new_component_cov.detach()\n\n                            if (class_num == 10 or len(mini_batch_indices) >= 4096) and (iter_id > int(args.iters / 2) or args.filter == 'none'):\n                                target_compoent_distribution.loc = new_component_mean\n                                target_compoent_distribution.covariance_matrix = new_component_cov + template_ext_cov\n                                target_compoent_distribution._unbroadcasted_scale_tril = torch.linalg.cholesky(new_component_cov + template_ext_cov)\n                                loss += (torch.distributions.kl_divergence(source_component_distribution, target_compoent_distribution) \\\n                                        + torch.distributions.kl_divergence(target_compoent_distribution, source_component_distribution)).mean() * loss_scale\n                        else: # Using posterior probabilities from target_compoent_distribution\n                            feat_ext2_categories = feat_ext[None, :, :].expand(class_num, -1, -1) # K, N, D\n                            num_categories = posterior # K, N (posterior is like soft counts)\n                            ema_n += num_categories.sum(dim=1) # K\n                            alpha = torch.where(ema_n > ema_length, torch.ones(class_num, dtype=torch.float).cuda() / ema_length, 1. / (ema_n + 1e-10))\n\n                            delta_pre = (feat_ext2_categories - ema_ext_mu[:, None, :]) * num_categories[:, :, None] # K, N, D\n                            delta = alpha[:, None] * delta_pre.sum(dim=1) # K, D\n                            new_component_mean = ema_ext_mu + delta\n                            new_component_cov = ema_ext_cov \\\n                                                + alpha[:, None, None] * ((delta_pre.permute(0, 2, 1) @ delta_pre) - num_categories.sum(dim=1)[:, None, None] * ema_ext_cov) \\\n                                                - delta[:, :, None] @ delta[:, None, :]\n\n                            with torch.no_grad():\n                                ema_ext_mu = new_component_mean.detach()\n                                ema_ext_cov = new_component_cov.detach()\n\n                            if (class_num == 10 or len(mini_batch_indices) >= 4096) and iter_id > int(args.iters / 2):\n                                target_compoent_distribution.loc = new_component_mean\n                                target_compoent_distribution.covariance_matrix = new_component_cov + template_ext_cov\n                                target_compoent_distribution._unbroadcasted_scale_tril = torch.linalg.cholesky(new_component_cov + template_ext_cov)\n                                loss += (torch.distributions.kl_divergence(source_component_distribution, target_compoent_distribution) \\\n                                        + torch.distributions.kl_divergence(target_compoent_distribution, source_component_distribution)).mean() * loss_scale\n\n                    if not args.without_global:\n                        # Global Feature Alignment (KL-Divergence & Iterative Update with Clipping)\n                        b = feat_ext.shape[0]\n                        ema_total_n += b\n                        alpha = 1. / 1280 if ema_total_n > 1280 else 1. / ema_total_n\n                        delta_pre = (feat_ext - ema_ext_total_mu.cuda())\n                        delta = alpha * delta_pre.sum(dim=0)\n                        tmp_mu = ema_ext_total_mu.cuda() + delta\n                        tmp_cov = ema_ext_total_cov.cuda() + alpha * (delta_pre.t() @ delta_pre - b * ema_ext_total_cov.cuda()) - delta[:, None] @ delta[None, :]\n                        with torch.no_grad():\n                            ema_ext_total_mu = tmp_mu.detach().cpu()\n                            ema_ext_total_cov = tmp_cov.detach().cpu()\n\n                        source_domain = torch.distributions.MultivariateNormal(mu_src_ext, cov_src_ext + template_ext_cov)\n                        target_domain = torch.distributions.MultivariateNormal(tmp_mu, tmp_cov + template_ext_cov)\n                        loss += (torch.distributions.kl_divergence(source_domain, target_domain) + torch.distributions.kl_divergence(target_domain, source_domain)) * loss_scale\n\n                    if args.without_mixture and args.without_global:\n                        # Fallback: simple cross-entropy if both alignment objectives are disabled\n                        logit2 = logit[pseudo_label_mask.cuda()]\n                        loss += F.cross_entropy(logit2, pseudo_label2) * loss_scale * 2\n\n                if args.align_ssh:\n                    # SSH Global Feature Alignment (similar logic as ext global alignment, omitted for primary focus)\n                    pass\n\n                if args.with_shot:\n                    # SHOT entropy minimization loss (omitted for primary focus)\n                    pass\n\n                # Backpropagation and Optimization\n                try:\n                    loss.backward()\n                except:\n                    pass\n                finally:\n                    del loss\n\n            if iter_id > 0:\n                optimizer.step()\n                optimizer.zero_grad()\n",
    "Experiment Result": "Dataset: cifar10 (default), dataroot='./data'\nBatch sizes: 128 (for adaptation training `tr_dataloader`), 512 (for `tr_extra_dataloader` used for feature alignment)\nWorkers: 0\nNumber of samples: 1,000,000 (default, for `tr_dataset` which tracks all samples for EMA logit/alpha)\nLearning rate: 0.001\nIterations per test batch: 4 (`args.iters`)\nCorruption type: snow (default), level: 5 (default)\nModel: resnet50 (default)\nSeed: 0 (default)\nSSL task: contrastive (default), temperature: 0.5 (for `SupConLoss` if `with_ssl` is True)\nAlignments activated: `--align_ext` is `action='store_true'` (default False in command line, but typically enabled for TTAC), `--align_ssh` is `action='store_true'` (default False)\n\nLoss components: Mixture Gaussian Alignment and Global Feature Alignment (both use KL-Divergence).\nPseudo-label filtering (`args.filter`): 'ours' (default). This combines:\n  - Temporal consistency filtering: based on exponential moving average posteriors (`sample_predict_ema_logit`) with `ema_alpha = 0.9`.\n  - Posterior probability filtering: `pro > 0.9` (pseudo-label confidence threshold) and `sample_predict_alpha[origin_image_index] == 1` (acceptance mask from temporal consistency).\n  - An alternative `posterior` filter mode exists, which uses component posterior probabilities directly.\n\nIterative updating strategy for running statistics:\n  - Category-wise EMA length (`ema_length`): 128 for cifar10, 64 for cifar100 (for `ema_n`).\n  - Global EMA length (implicit in `ema_total_n` clipping): `alpha = 1. / 1280` if `ema_total_n > 1280`.\n  - Initialization bias for covariance matrix (`template_ext_cov`, `template_ssh_cov`): `cov_src_ext.max().item() / 30.` and `cov_src_ssh.max().item() / 30.` respectively.\n\nLoss scale (`loss_scale`): 0.05 for cifar10, 0.5 for cifar100 (applied to KL-Divergence terms).\nFixed-length queue for recent test samples: `mini_batch_length = 4096`. This queue stores indices of `mini_batch_length` most recent test samples. Adaptation is performed using only these samples in each test batch iteration."
}{
    "Title": "AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation",
    "Main Contributions": "The paper introduces AETTA, a novel label-free accuracy estimation algorithm for Test-Time Adaptation (TTA). The primary problem addressed is the susceptibility of TTA to adaptation failures in dynamic environments due to its blind reliance on unlabeled test data, and the limitations of existing performance estimation methods that require labeled data or re-training. AETTA proposes using prediction disagreement calculated by comparing the target model prediction with dropout inferences. It further refines this by introducing robust disagreement equality, which dynamically adjusts accuracy estimates to account for adaptation failures characterized by over-confident incorrect predictions. The main contributions include theoretical analysis supporting the method, extensive evaluation demonstrating AETTA's superior accuracy estimation (average 19.8%p more accurate than baselines), and a case study showcasing its practical utility in a model recovery algorithm that improved performance by 11.7%p.",
    "Methodology": "AETTA's methodology is grounded in comparing a model's prediction with predictions from its dropout inferences, termed Prediction Disagreement with Dropouts (PDD). It assumes 'dropout independence' (dropout inferences simulate i.i.d. models) and 'confidence-prediction calibration' to theoretically approximate test error with PDD. To address adaptation failures, which often lead to over-confident incorrect predictions and violate standard calibration, AETTA proposes 'robust confidence-prediction calibration'. This is achieved by introducing a weighting constant 'b' that dynamically scales predicted probabilities. The constant 'b' is determined by the skewness of predicted outputs, specifically using the entropy of batch-aggregated softmax values from dropout inferences (Eavg), modeled as b = (Eavg / Emax)^(-α). The final accuracy estimation (Err) is approximated as ErrDT(h) ≈ b PDDDT(h), with C omitted due to lack of information. The process involves performing N dropout inferences, calculating PDD and Eavg, and then applying the scaling factor 'b' to estimate the error. Exponential moving average is applied for stable error estimation.",
    "Experimental Setup": "AETTA was evaluated on three standard TTA benchmarks: CIFAR10-C, CIFAR100-C, and ImageNet-C, using 15 corruption types at severity level 5. Experiments were conducted in two scenarios: fully TTA (adapting to each corruption type) and continual TTA (continuously adapting to 15 corruptions). The evaluation metric was Mean Absolute Error (MAE) between estimated and ground-truth batch-wise accuracy, averaged over three random seeds. A pre-trained ResNet18 model served as the adaptation target. Six state-of-the-art TTA methods (TENT, EATA, SAR, CoTTA, RoTTA, SoTTA) were used to integrate and evaluate AETTA. AETTA was compared against four baselines: SrcValid (using labeled source data), SoftmaxScore (using average softmax confidence), GDE (Generalization Disagreement Equality, comparing current and previous adapted models), and AdvPerturb (Adversarial Perturbation, comparing adapted and source models with FGSM attacks). Hyperparameters for AETTA were N=10 (dropout inferences) and α=3 (scaling hyperparameter), selected after ablation studies. A case study on model recovery was performed, comparing AETTA's reset algorithm (based on consecutive low accuracies or sudden drops) against baselines like Episodic, MRS, Stochastic, FisherStochastic, and an oracle DistShift.",
    "Limitations": "The research identifies several limitations and areas for improvement. While AETTA's use of dropout inference is computationally lightweight, the overall computational overheads associated with TTA and accuracy estimation could raise environmental concerns. More specifically to AETTA, there is room for optimization of the weighting constant 'b' (or its related 'a') for finer calibration. The current empirical approach also omits the constant 'C' from the theoretical robust disagreement equality due to insufficient information, suggesting a potential for more precise error estimates if 'C' could be estimated. Furthermore, the model recovery algorithm presented in the case study is described as heuristic and could be made more effective.",
    "Future Research Directions": "Future research directions include optimizing the weighting constant 'b' (or 'a') for fine-tuning the robust confidence-prediction calibration process. Another direction is the estimation of the variable 'C' from the robust disagreement equality, which could lead to more precise error estimates. Improvements to the heuristic model recovery algorithm presented in the case study are also suggested. Beyond model recovery, the authors envision broader applications for accuracy estimation, such as enhancing model refinement and maintenance processes, and improving the dynamics of human-AI interactions. The integration of AETTA with emerging memory-economic TTA advancements is also suggested to mitigate computational demands and environmental impact.",
    "Experiment Code": "def evaluate_dropout(self, feats, net, n_iter=10, dropout=0.5):\n    if net is None:\n        net = self.net\n        \n    curr_pred, curr_conf, _, _, _, curr_softmax, _ = self.model_inference(feats, net=net)\n\n    if dropout < 0:\n        if conf.args.dataset == \"cifar10outdist\":\n            dropout = 0.4\n        elif conf.args.dataset == \"cifar100outdist\":\n            dropout = 0.3\n        elif conf.args.dataset == \"imagenetoutdist\":\n            dropout = 0.2\n        elif conf.args.dataset == \"imagenetR\":\n            dropout = 0.3\n        else:\n            raise NotImplementedError\n\n    # Dropout inference sampling\n    predictions = []\n    with torch.no_grad():\n        for _ in range(n_iter):\n            pred = net[1]((net[0](feats)), dropout=dropout)  # batch_size, n_classes\n            pred = F.softmax(pred, dim=1)\n            predictions.append(pred)\n    predictions = torch.stack(predictions, dim=1)  # batch_size, n_iter, n_classes\n    pred = torch.argmax(predictions, dim=2)\n    mean = torch.mean(predictions, dim=1)\n    #mean_pred_class = torch.argmax(mean_pred, dim=1)\n    std = torch.std(predictions, dim=1)\n\n    conf_mean = mean[:, curr_pred].diagonal()\n    conf_std = std[:, curr_pred].diagonal()\n    mean_for_curr_pred = conf_mean.mean()\n    std_for_curr_pred = conf_std.mean()\n\n    total_avg_softmax = torch.mean(mean, dim=0)\n    e_avg = (-total_avg_softmax * torch.log(total_avg_softmax + 1e-6)).sum()\n\n    # Prediction disagreement with dropouts\n    match_ratio = (curr_pred.unsqueeze(dim=1).repeat(1, n_iter) == pred).sum(dim=1, dtype=float) / n_iter\n    acc = match_ratio.mean()\n    return acc.item(), mean_for_curr_pred.item(), std_for_curr_pred.item(), e_avg.item()\n\ndef aetta(self, feats, y_pred):\n    est_acc, mean, std, e_avg = self.evaluate_dropout(feats, self.net, dropout=conf.args.dropout_rate)\n    self.acc_est_json['est_dropout'] += [est_acc]\n    self.acc_est_json['est_dropout_avg_entropy'] += [e_avg]\n    self.acc_est_json['est_dropout_softmax_mean'] += [mean]\n    self.acc_est_json['est_dropout_softmax_std'] += [std]\n\n    est_err = 1 - est_acc\n    if self.est_ema_dropout is None:\n        self.est_ema_dropout = est_err\n\n    if conf.args.dataset == \"cifar10outdist\":\n        MAX_ENTROPY = 2.3026  # cifar10\n        N_CLASS = 10\n    elif conf.args.dataset == \"cifar100outdist\":\n        MAX_ENTROPY = 4.6052  # cifar100\n        N_CLASS = 100\n    elif conf.args.dataset == \"imagenetR\" :\n        MAX_ENTROPY = 5.2983  # imagenetR\n        N_CLASS = 200\n    else: # imagenet\n        MAX_ENTROPY = 6.9078\n        N_CLASS = 1000\n\n    updated = est_err / (e_avg / MAX_ENTROPY) ** 3\n    updated = max(0., min(1. - 1. / N_CLASS, updated))\n\n    updated = self.est_ema_dropout * 0.6 + updated * 0.4\n    self.est_ema_dropout = updated\n\n    self.acc_est_json['aetta'] += [100 * (1. - updated)]",
    "Experiment Result": "AETTA's methodology is implemented in the `aetta` and `evaluate_dropout` functions. The core idea is to estimate the test error using Prediction Disagreement with Dropouts (PDD) and the Entropy of Batch-aggregated Softmax values from Dropout inferences (Eavg).\n\n**1. Prediction Disagreement with Dropouts (PDD) and Eavg Calculation (`evaluate_dropout` function):**\n*   **Number of Dropout Inferences (N):** 10 iterations are performed.\n*   **Dropout Rate:** The dropout probability varies based on the dataset:\n    *   0.4 for CIFAR-10-C/OutDist\n    *   0.3 for CIFAR-100-C/OutDist and ImageNet-R\n    *   0.2 for ImageNet-C/OutDist\n*   `e_avg` (Eavg) is calculated as the sum of `-(total_avg_softmax * torch.log(total_avg_softmax + 1e-6))`.\n*   `est_acc` (agreement rate) is calculated as the mean of `match_ratio`, where `match_ratio` is the element-wise agreement between the current prediction and predictions from dropout inferences.\n*   `est_err` (PDD) is then derived as `1 - est_acc`.\n\n**2. Robust Confidence-Prediction Calibration (scaling constant 'b') and Error Estimation (`aetta` function):**\n*   **Maximum Entropy (Emax) and Number of Classes (N_CLASS):** These constants are defined based on the dataset:\n    *   CIFAR-10-C/OutDist: Emax = 2.3026, N_CLASS = 10\n    *   CIFAR-100-C/OutDist: Emax = 4.6052, N_CLASS = 100\n    *   ImageNet-R: Emax = 5.2983, N_CLASS = 200\n    *   ImageNet-C/OutDist: Emax = 6.9078, N_CLASS = 1000\n*   **Scaling Constant 'b':** The estimated error (`updated`) is approximated as `est_err / (e_avg / MAX_ENTROPY) ** 3`. This implies that the exponent `α` for `b = (Eavg / Emax)^(-α)` is `3` (since `ErrDT(h) ≈ b PDDDT(h)` and `est_err` is `PDDDT(h)`).\n*   **Clamping:** The `updated` estimated error is clamped between `0` and `1 - 1/N_CLASS`.\n*   **Exponential Moving Average (EMA):** An EMA is applied to stabilize the error estimation. The update rule is `updated = self.est_ema_dropout * 0.6 + updated * 0.4`, meaning the current estimate has a weight of `0.4` and the previous EMA has a weight of `0.6`.\n\n**3. Reset Mechanism (when `conf.args.reset_function` is \"aetta\"):**\n*   The model performs a hard reset if the estimated accuracy (`100 * (1. - updated)`) falls below 20%.\n*   Additionally, if the average of the last 5 estimated accuracies (from a window of the last 10 samples) is more than 2 percentage points lower than the average of the preceding 5 estimated accuracies in that window, a hard reset is triggered. This indicates a significant drop in performance."
}{
    "Title": "A Bayesian Perspective on Training Speed and Model Selection",
    "Main Contributions": "This paper establishes a connection between a model's training speed and its marginal likelihood, offering a new method for model selection and insights into generalization. It shows that a measure of training speed can estimate marginal likelihood and predict model weightings in linear combinations. The core contribution is a family of marginal likelihood estimators derived from sums of predictive log likelihoods (interpreted as training losses) from an iterative Bayesian updating procedure. For linear models and infinite-width neural networks, where gradient descent provides exact posterior samples, these estimators can be computed directly from optimization trajectories. Empirical evidence suggests this intuition extends to deep neural networks (DNNs) trained with stochastic gradient descent (SGD), indicating a promising direction for explaining why SGD-trained neural networks generalize well. The study also highlights a link between magnitude-based pruning in linear model combinations and approximate Bayesian model selection.",
    "Methodology": "The methodology revolves around a Bayesian perspective on marginal likelihood (ML) estimation. The log ML is re-expressed as the sum of log posterior predictive probabilities of data points, conditioned on preceding data (sum of 'training losses' over a Bayesian updating curve). Three estimators for lower bounds on the log ML are proposed: L(D) (based on expected log likelihoods), Lk(D) (a tighter bound using k posterior samples), and LS(D) (for Gaussian posteriors, using estimated mean and variance). For linear models and infinite-width neural networks (approximated by Neural Tangent Kernel GPs), gradient descent is leveraged to produce exact posterior samples iteratively, allowing these estimators to be computed from optimization trajectories. The paper also analyzes linear model combinations, showing that the optimal weights in such combinations align with the proposed ML estimators under certain conditions. For DNNs, the methodology involves evaluating the 'sum over training losses' (SOTL) obtained during SGD, correlating it with generalization error and ensemble weights in linear combinations of DNNs, and investigating subnetwork selection within a single DNN.",
    "Experimental Setup": "The experimental evaluation covered linear models, infinite-width neural networks (NTK-GPs), and deep neural networks (DNNs) trained with SGD. For linear models, synthetic datasets were used for three model selection tasks: feature dimension selection (identifying informative features), prior variance selection (optimal regularization), and frequency (lengthscale) selection for Random Fourier Features (RFFs) on binarized MNIST. Validation involved comparing rankings from true ML, the proposed estimators (L, Lk, LS), and weights from optimized linear regressors. For NTK-GPs, fully-connected and convolutional architectures were tested on MNIST, visualizing the incremental change in the L(D) estimator. For DNNs, experiments were conducted on FashionMNIST and CIFAR-10 datasets using various MLP and CNN architectures. This included training linear combinations of DNNs with SGD (parallel and concurrent training schemes) to analyze the correlation between SOTL and test loss, and between individual test loss and ensemble weights. Additionally, subnetwork selection within a single MLP was explored by relating the final weights of penultimate layer nodes to their performance (SOTL of cross-entropy). All DNN experiments averaged results over multiple runs (10 seeds).",
    "Limitations": "The primary theoretical guarantees for the proposed marginal likelihood estimators and the connection to training speed hold strictly for linear models and the infinite-width limit of neural networks. For finite-width, non-linear deep neural networks trained with SGD, the results are presented as empirical evidence and conjectures, as SGD lacks the exact posterior sampling interpretation of the algorithm. The LS estimator is specifically limited to models with Gaussian posterior predictive distributions. Furthermore, the theoretical result connecting linear model combination weights to marginal likelihood (Proposition 3.5) relies on strong independence assumptions on model errors which are only approximately satisfied in practice. The paper also acknowledges that, due to its theoretical nature and focus on i.i.d. assumptions, it does not immediately address issues like out-of-distribution inputs or inherent biases in training datasets.",
    "Future Research Directions": "The paper suggests several promising avenues for future research. A key direction is to further formalize and explain the empirical generalization performance of deep neural networks, particularly the bias of SGD towards functions that generalize well, by extending the intuition developed in linear models and infinite-width settings to non-linear, finite-width DNNs. This includes investigating whether the analogue of the sum of training losses can reliably predict generalization error in SGD-trained models. Another direction is to explore if similar mechanisms (where more predictive components receive higher weights) occur within the internal layers of a neural network, beyond the final linear layer. More broadly, the work aims to pave the way for new practical model selection procedures that integrate closely with standard optimization schemes. Improved understanding of generalization could also lead to more efficient training methods (reducing environmental impact) and enhance the safety and robustness of deep learning systems.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent",
    "Main Contributions": "The paper investigates how neural network architecture influences training speed by introducing 'gradient confusion,' a condition where stochastic gradients from different data samples may be negatively correlated, hindering convergence. It formally proves that high gradient confusion slows down convergence, while low confusion accelerates it. The study demonstrates theoretically and experimentally that increasing network width leads to lower gradient confusion and faster training, whereas increasing depth has the opposite effect under standard Gaussian initializations. It also shows that orthogonal initialization schemes make gradient confusion independent of depth for linear networks, and that the combination of batch normalization and skip connections in residual networks significantly reduces gradient confusion, enabling efficient training of very deep models.",
    "Methodology": "The core methodology involves defining 'gradient confusion' as the pair-wise inner products between gradients of different objective functions, where negative correlation indicates high confusion. Theoretical analysis is conducted on SGD convergence rates for functions satisfying Polyak-Lojasiewicz (PL) inequality and general smooth functions, showing the direct effect of gradient confusion on convergence speed and noise floor. High-dimensional probability methods are used to analyze the effect of network width and depth on gradient confusion at standard Gaussian initializations (N(0, 1/d) or N(0, 1/κℓp−1)). A more general result on depth effect with a 'small weights assumption' is derived. The paper also theoretically explores orthogonal initialization for linear networks to decouple gradient confusion from depth. Empirically, it involves calculating pairwise gradient cosine similarities over mini-batches.",
    "Experimental Setup": "Experiments were conducted on Wide Residual Networks (WRNs), Convolutional Neural Networks (CNNs), and Multi-Layer Perceptrons (MLPs) for image classification tasks. Datasets included CIFAR-10, CIFAR-100, and MNIST. SGD without momentum was used as the optimizer, with mini-batch size 128, trained for 200 (CNNs/WRNs) or 40000 iterations/100 epochs (MLPs). Learning rates were constant for most training and reduced by a factor of 10 at specific epochs (80 and 160 for CNNs/WRNs). The MSRA initializer (He et al., 2015) was used for CNNs/WRNs, and Glorot/LeCun normal initializers for MLPs. Dropout and weight decay were turned off. Gradient confusion was measured by sampling 100-1000 pairs of mini-batches and computing pairwise cosine similarities. Performance was evaluated using training loss, minimum pairwise gradient cosine similarity, and test set accuracy.",
    "Limitations": "The theoretical convergence results for SGD assume conditions like Lipschitz smoothness and the Polyak-Lojasiewicz inequality, or general smooth functions. The analysis of gradient confusion for general networks (Theorem 5.1 and Corollary 5.1) relies on a 'small weights assumption' (operator norm of weight matrices bounded by 1), which is argued to hold in practice due to weight decay but might not be universally true without explicit regularizers. Proving non-vacuous bounds for signal or gradient blow-ups without this assumption is not generally possible. While orthogonal initializations show promise for deep linear networks, they are not compatible with common non-linear activation functions like sigmoid or ReLU, limiting their practical use. The convergence rates presented are not state-of-the-art for these problem classes but aim to directly show the effect of gradient confusion.",
    "Future Research Directions": "Future research could investigate the connection between gradient confusion and generalization properties of neural networks, as suggested by the correlation between gradient confusion and test set accuracies. Developing orthogonal initialization schemes that are compatible with non-linear activation functions (like sigmoids or ReLUs) is identified as a promising direction for improving the trainability of very deep networks. The gradient confusion condition itself could be explored as a measure of trainability, potentially leading to new algorithms for more efficient training.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Gradient-Based Feature Learning under Structured Data",
    "Main Contributions": "This paper investigates how a spiked covariance structure in input data affects the sample complexity of gradient-based learning for single index models. It reveals that standard spherical gradient dynamics, which are covariance-agnostic, fail to recover the true direction even when the spike is aligned with the target. The authors demonstrate that an appropriate covariance-aware normalization, resembling batch normalization, resolves this issue. By exploiting the alignment between the spiked input covariance and the target, the paper achieves improved sample complexity compared to the isotropic case. Specifically, under a suitably large spike, sample complexity can be made independent of the information exponent and can outperform lower bounds for rotationally invariant kernel methods. The work identifies a three-stage phase transition for sample complexity based on spike-target alignment (r1) and spike magnitude (r2) and shows that preconditioning the training dynamics with the inverse covariance further improves sample complexity, achieving nearly linear rates in dimension.",
    "Methodology": "The research focuses on learning single index models of the form y = g(⟨u, x⟩) + ϵ, where x ∼ N(0, Σ) and Σ is a (κ, θ)-spiked covariance matrix. The learning architecture is a two-layer neural network, primarily analyzed in a simplified single-neuron setting for the first layer, with extensions to multi-neuron networks. Two gradient-based training procedures are studied: 1) Spherical gradient flow (covariance-agnostic), which is shown to fail. 2) Normalized gradient flow (covariance-aware), where weights are normalized by ∥Σ^(1/2)w∥^2, resembling batch normalization. 3) Preconditioned gradient flow using the inverse empirical covariance ˆΣ^(−1). The analysis relies on the information exponent 's' of the link function 'g' (defined via Hermite expansion) as a measure of complexity. The approach involves theoretical analysis of population and empirical gradient dynamics using tools like Stein's lemma, concentration inequalities, and covering arguments to derive sample complexity bounds and alignment guarantees.",
    "Experimental Setup": "The paper primarily conducts a theoretical analysis of gradient-based learning. The experimental setup involves a synthetic statistical model where input data `x` follows a Gaussian distribution `N(0, Σ)` with a (κ, θ)-spiked covariance `Σ = (Id+κθθ⊤)/(1+κ)`. The target output `y` is generated from a single index model `y = g(⟨u, x⟩) + ϵ`. The activation function `ϕ` used in the neural network is either ReLU or a general smooth activation. The validation of the contributions is entirely analytical, focusing on deriving sample complexity bounds and conditions for successful alignment of the first-layer weights `w` with the target direction `u`. No real-world datasets or empirical benchmarks are used.",
    "Limitations": "The current work focuses on single index models, and extending to multi-index models would provide a more complete understanding. The information exponent and its interplay with structured covariance need further generalization for multi-index scenarios. For odd information exponents, the initial condition `⟨w0, u⟩ > 0` is required, which only holds with 0.5 probability for uniform initialization. Training networks with multiple neurons using more standard initializations (beyond the somewhat unconventional initialization for a single neuron) is identified as a challenging area for future analysis, potentially relaxing Assumption 2. The paper does not explore the limitations of learning single index models under structured input covariance through a Correlational Statistical Query (CSQ) lower bound perspective. For ReLU activation, the empirical dynamics require a more accurate covariance estimator `n' ≳ n^2`, assuming access to additional unlabeled input data points.",
    "Future Research Directions": "Future research could extend this work to multi-index models, which would necessitate generalizing the concept of information exponent to more complex measures and establishing incremental learning dynamics. Another promising direction is to investigate the limitations of learning single index models under structured input covariance through the lens of Correlational Statistical Query (CSQ) lower bounds, which would complement the paper's current results. Additionally, exploring scenarios where networks with multiple neurons are trained starting from more standard initializations is an interesting challenge. This could lead to a deeper understanding of neuronal interactions and potentially relax certain assumptions made in the current analysis, such as Assumption 2.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Knowledge-Adaptation Priors",
    "Main Contributions": "The paper introduces Knowledge-adaptation priors (K-priors) to address the high financial and environmental costs of retraining machine learning models when changes occur. K-priors enable quick and accurate model adaptation for a wide variety of tasks and models by combining weight and function-space priors to faithfully reconstruct the gradients of past training objectives. This approach unifies and generalizes many existing, seemingly unrelated, adaptation strategies in machine learning. Empirically, K-priors achieve performance similar to full retraining but require training on only a handful of past examples.",
    "Methodology": "K-priors are defined as a class of priors using both weight and function-space regularizers, specifically K(w; w*,M) = Df(f(w)||f(w*)) + τDw(w||w*), where Df and Dw are Bregman divergences. The core principle is to reconstruct the gradients of the past training objective. For Generalized Linear Models (GLMs), an L2 regularizer is used for Dw and a Bregman divergence with the log-partition function A(f) for Df, which exactly recovers past gradients when the full past data is used. For limited memory, a practical approximation involves choosing examples with the highest derivative h'(fi w*) (referred to as 'memorable past') to minimize gradient-reconstruction error. For deep learning, K-priors extend Knowledge Distillation by adding a weight-space term, allowing general link functions, and using a small number of memory examples. Adaptation is achieved by minimizing new objectives regularized by the K-prior (e.g., for adding/removing data, changing regularizers, or changing model classes).",
    "Experimental Setup": "Experiments were conducted on four adaptation tasks: 'Add Data', 'Remove Data', 'Change Regularizer', and 'Change Architecture' (including model compression/knowledge distillation). Models included logistic regression (on UCI Adult and USPS datasets) and neural networks (1-hidden-layer MLP on USPS, 2-hidden-layer MLPs on MNIST, CifarNet and LeNet5-style student on CIFAR-10). Comparisons were made against 'Batch' (full retraining from scratch), 'Replay' (retraining with a small memory of past examples using true labels), and 'Weight-Priors' (for the 'Add Data' task). Memory points for K-priors and Replay were chosen based on the highest h'(fi w*) values. Optimizers used were L-BFGS (logistic regression) and Adam (neural networks). Validation accuracy was the primary performance metric. Hyperparameter tuning, including a temperature parameter (similar to knowledge distillation) and λ, was explored for deep learning tasks.",
    "Limitations": "The theoretical 'optimal K-prior' that can achieve perfect reconstruction error using singular vectors of the feature matrix is difficult to realize in practice. For larger deep learning problems (e.g., CIFAR-10 with 50% past data), there can be a noticeable gap between K-prior and Batch performance, suggesting the need for more extensive hyperparameter tuning. A dip in performance was observed at certain memory sizes (e.g., 10% past data on CIFAR-10 for knowledge distillation), which was attributed to suboptimal hyperparameter tuning (τ or λ). The function-space formulations, while equivalent to K-priors, could be computationally expensive.",
    "Future Research Directions": "Future research includes applying sketching methods (e.g., leverage scores) for more efficient memory selection in K-priors. More empirical effort is required to tune various hyperparameters to achieve consistent behavior across all memory sizes, particularly for large deep learning problems. Investigation into the design of divergence functions is needed for building better sparse approximations (e.g., choosing the matrix B to correlate examples in memory with examples in the full dataset). The authors also express a broader goal of pushing towards simpler designs that support dynamic ML settings, leading to systems that learn quickly, flexibly, and incrementally in a continual fashion.",
    "Experiment Code": "def select_memory_points(dataloader, model, num_points, additional_memory_data=None, use_cuda=False, descending=True):\n    memory_points_list = {}\n    points_indices = {}\n    data, target = dataloader\n    num_points_per_class = [int(num_points/2),int(num_points/2)]\n    if torch.sum(target==0) < num_points_per_class[0]:\n        num_points_per_class[0] = torch.sum(target==0).numpy()\n        num_points_per_class[1] = num_points - num_points_per_class[0]\n    elif torch.sum(target==1) < num_points_per_class[1]:\n        num_points_per_class[1] = torch.sum(target==1).numpy()\n        num_points_per_class[0] = num_points - num_points_per_class[1]\n    if use_cuda:\n        data_in = data.cuda()\n    else:\n        data_in = data\n    preds = model.forward(data_in)\n    lamb = softmax_hessian(preds)\n    if use_cuda:\n        lamb = lamb.cpu()\n    lamb = torch.sum(lamb, dim=-1)\n    lamb = lamb.detach()\n    for cid in range(2):\n        p_c = data[target == cid]\n        indices_for_points = np.argwhere(target == cid)[0].numpy()\n        if len(p_c) > 0:\n            scores = lamb[target == cid]\n            _, indices = scores.sort(descending=descending)\n            memory_points_list[cid] = p_c[indices[:num_points_per_class[cid]]]\n            points_indices[cid] = indices_for_points[indices[:num_points_per_class[cid]]]\n    r_points = []\n    r_labels = []\n    r_indices = []\n    for cid in range(2):\n        r_points.append(memory_points_list[cid])\n        r_labels.append(cid*torch.ones(memory_points_list[cid].shape[0], dtype=torch.long,\n                                   device=memory_points_list[cid].device))\n        r_indices.append(points_indices[cid])\n    memory_points = {}\n    memory_points['inputs'] = torch.cat(r_points, dim=0)\n    memory_points['true_labels'] = torch.cat(r_labels, dim=0)\n    if np.sum(num_points_per_class) > 2:\n        memory_points['indices'] = np.concatenate(np.array(r_indices), axis=0)\n    else:\n        memory_points['indices'] = r_indices\n    if additional_memory_data is not None:\n        memory_points['inputs'] = torch.cat((memory_points['inputs'], additional_memory_data[0]))\n        memory_points['true_labels'] = torch.cat((memory_points['true_labels'], additional_memory_data[1]))\n    if use_cuda:\n        memory_points['inputs'] = memory_points['inputs'].cuda()\n    memory_points['soft_labels'] = torch.softmax(model.forward(memory_points['inputs']), dim=-1)\n    return memory_points\n\n# Code for AdamReg and LBFGSReg optimizers (logic in step method)\n# Differentiates K-priors/Replay and applies weight-space regularization\n# This logic is present in both AdamReg.step and LBFGSReg.step\n\n# ... (inside AdamReg.step or LBFGSReg.step)\n# Loss term over memory points (only if K-priors or Replay)\nif closure_memory is not None:\n    preds = closure_memory()\n    self.total_datapoints_this_iter += len(preds)\n    preds_soft = torch.softmax(preds, dim=-1)\n    delta_logits = preds_soft.detach() - self.memory_labels\n    grad_message = torch.autograd.grad(preds, self.model.parameters(), grad_outputs=delta_logits)\n    grad_vec = []\n    for i in range(len(grad_message)):\n        grad_vec.append(grad_message[i].data.view(-1))\n    grad_vec = torch.cat(grad_vec, dim=-1)\n    grad.add_(grad_vec.detach())\n    # Weight regularisation\n    if adaptation_method == \"K-priors\" and self.prior_prec_old is not None:\n        grad.add_(self.previous_weights, alpha=-self.prior_prec_old)\n\n# Code from main.py for setting up memory labels and previous weights\noptimiser.previous_weights = base_model.return_parameters()\nif adaptation_method == \"K-priors\":\n    memory_points['labels'] = memory_points['soft_labels']\nelif adaptation_method == \"Replay\":\n    memory_points['labels'] = torch.nn.functional.one_hot(memory_points['true_labels'])\noptimiser.memory_labels = memory_points['labels']",
    "Experiment Result": "The experiments evaluate K-priors and Replay methods on adaptation tasks for both Generalized Linear Models (GLMs) and Deep Learning models.\n\n**Datasets:**\n*   **Adult:** UCI Adult dataset for binary classification.\n*   **USPS Binary:** USPS dataset, classifying odd vs. even digits.\n\n**Network Architectures:**\n*   **Linear Model (GLM):** Logistic Regression, used for the Adult dataset and some USPS experiments. Uses `LBFGSReg` optimizer.\n*   **MLP (Multi-Layer Perceptron):** Deep learning model with one or two hidden layers of 100 units, used for USPS experiments. Uses `AdamReg` optimizer.\n\n**Adaptation Tasks:**\n*   **Add Data:** New data (or classes, e.g., digit '9' for USPS) is added to the training set.\n*   **Remove Data:** A subset of data (e.g., digit '8' for USPS, or top `h'(f)` points for Adult) is removed.\n*   **Change Regulariser:** The L2 regularization strength (weight_decay/prior_prec) is changed.\n    *   Adult/USPS Linear: From 50 to 5.\n    *   USPS MLP: From 5 to 10.\n*   **Change Model:** The model architecture is changed.\n    *   Linear Model: Polynomial degree of features changes from 2 to 1.\n    *   MLP: Number of hidden layers changes (e.g., from two 100-unit layers to one 100-unit layer).\n\n**Memory Selection Strategy:**\n*   For limited memory, 'memorable past' examples are selected based on the highest derivative `h'(f)` (calculated as `softmax_hessian` sum over the output dimension) from the *base model* (model trained on the initial task).\n\n**Memory Sizes:**\n*   The fraction of base task data stored in memory is varied in experiments: [1.0, 0.5, 0.2, 0.1, 0.07, 0.05, 0.02].\n\n**Regularization Parameters:**\n*   **`prior_prec` (L2 weight_decay):**\n    *   Adult: 5\n    *   USPS Linear: 50\n    *   USPS MLP: 5\n*   **`prior_prec_old` (for K-priors weight-space term `Dw(w||w*)`):** Set to the `prior_prec` of the base task, or specifically adjusted for 'change_regulariser' tasks.\n\n**K-prior vs. Replay Distinction:**\n*   **K-priors:** Use soft labels (output of `softmax(model.forward(memory_inputs))` from the base model) as targets for the function-space regularizer (`Df`). They also include a weight-space L2 regularizer towards the previous weights `w*` (`Dw`).\n*   **Replay:** Uses hard (true, one-hot encoded) labels for the function-space regularizer. It does not include the explicit weight-space regularizer towards `w*` (`Dw`).\n\n**Optimization:**\n*   **Optimizers:** LBFGS (`LBFGSReg`) for Linear models, Adam (`AdamReg`) for MLPs.\n*   **Learning Rate:** 0.005 for Adult/USPS MLP, 0.1 for USPS Linear.\n*   **Epochs:** 1000 for Adult/USPS MLP, 300 for USPS Linear.\n\n**Evaluation:**\n*   Each experiment is repeated for 3 runs with different random seeds (`seed_init + random_run`).\n*   Test accuracy is reported on the adaptation task's test set."
}{
    "Title": "Directional Smoothness and Gradient Methods: Convergence and Adaptivity",
    "Main Contributions": "The paper introduces new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the optimization path, rather than on global, worst-case L-smoothness constants. It develops the concept of directional smoothness, a measure of gradient variation, to construct tighter upper-bounds on the objective. Key findings include: new path-dependent sub-optimality bounds for GD, demonstration that strongly adapted step-sizes recover classical schemes for quadratic problems (e.g., Cauchy step-size), and proof that the Polyak step-size and normalized GD achieve fast, path-dependent rates without explicit knowledge of directional smoothness. The research also shows that these convergence guarantees are tighter than classical L-smoothness theory in experiments.",
    "Methodology": "The core methodology revolves around defining and leveraging 'directional smoothness' functions M(x, y) which quantify gradient variation along the chord between points x and y. Three constructive directional smoothness functions are introduced: point-wise smoothness D(x,y), path-wise smoothness A(x,y), and optimal point-wise smoothness H(x,y). These functions are used to derive a local descent lemma and new sub-optimality bounds for convex functions, including the case with directional strong convexity. The paper explores 'strongly adapted step-sizes' (ηk = 1/M(xk+1, xk)) which require solving non-linear root-finding problems for general functions, but are shown to be equivalent to well-known step-sizes for quadratics. For general convex functions, existence proofs for strongly adapted step-sizes are provided, and an exponential search algorithm is adapted to find approximate adaptive step-sizes. Additionally, the paper analyzes the Polyak step-size and normalized GD, demonstrating their inherent adaptivity to directional smoothness.",
    "Experimental Setup": "The experimental evaluation was performed on logistic regression problems from the UCI repository (ionosphere, mammographic, horse-colic, ozone datasets) and synthetic quadratic problems. For UCI datasets, a pre-processed version was used with an 80-20 train-test split for validation. The optimal value f(x*) for logistic regression was computed using SciPy's BFGS implementation. Methods were initialized using Kaiming initialization. For synthetic quadratic problems, eigenvalues were generated to follow a power-law distribution with L=1000. Benchmarks included GD with a fixed step-size (1/L), GD with step-sizes strongly adapted to point-wise smoothness (1/Dk), Polyak step-size GD, normalized GD (with a grid-searched η0 and schedule ηk = η0/√k), and the AdGD method. Strongly adapted step-sizes were computed using SciPy's Newton method on Equation (44). Experiments were run on a MacBook Pro or a Slurm cluster with Nvidia A100/H100/V100 GPUs.",
    "Limitations": "The general computation of strongly adapted step-sizes for non-quadratic functions involves solving challenging non-linear root-finding problems (e.g., using Newton's method), which can be impractical for large-scale optimization. While exponential search can find approximate adaptive step-sizes, it introduces a log-log penalty and requires the function to be L-smooth. Obtaining accelerated rates for non-strongly convex functions (µ=0) requires prior knowledge of the minimum step-size, which is not straightforward when dealing with locally Lipschitz gradients. The provided rate for normalized GD (Theorem 4.5) does not adapt to any smoothness function like the Polyak step-size does.",
    "Future Research Directions": "The paper lays the groundwork for further exploration of path-dependent optimization. One implicit direction is improving the practical computation of strongly adapted step-sizes for general non-quadratic and non-convex functions. Further research could also focus on extending the analysis of directional smoothness to other optimization algorithms beyond GD and AGD, particularly those used in deep learning. Investigating how the insights from directional smoothness can be integrated into adaptive gradient methods (like Adagrad variants) is another potential area. Additionally, developing anytime rates for non-strongly convex acceleration with locally Lipschitz gradients without requiring prior knowledge of minimum step-sizes remains an open problem.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Optimistic Meta-Gradients",
    "Main Contributions": "The paper demonstrates that gradient descent with momentum (Heavy Ball) and Nesterov Acceleration are special cases of meta-gradients, showing that meta-learning can be understood as a non-linear transformation of classical optimization. It establishes convergence rates for meta-learning in the single-task convex setting, proving O(λ/T) convergence. Crucially, it shows that optimism is required for meta-learning to achieve accelerated convergence rates of O(λ/T^2). Finally, the paper reveals that optimism in meta-learning can be captured through Bootstrapped Meta-Gradients (BMG), providing the first proof of convergence for BMG and insight into its underlying mechanics.",
    "Methodology": "The research employs a theoretical framework that connects meta-learning to convex optimization. It introduces a stylized variant of meta-learning (Algorithm 2) for rigorous analysis, which utilizes moving averages and an online learning algorithm for meta-updates, specifically Follow-The-Regularized-Leader (FTRL) and its adaptive-optimistic variant (AO-FTRL). The update rule (ϕ) is assumed to be affine in the meta-parameters (w) and smooth with bounded norm. Optimism is formally integrated into the meta-learning process through the use of hint functions in the AO-FTRL meta-updates (Optimistic Meta-Learning, OML). The analysis relies on online-to-batch conversion techniques and regret bounds from online convex optimization, proving how the update rule affects convergence rates and how optimism enables acceleration. The paper further establishes an isomorphism between targets in BMG and hint functions in AO-FTRL.",
    "Experimental Setup": "The paper conducts experiments in two settings: (1) Convex Quadratics, where ill-conditioned convex quadratic loss functions (f(x) = xTQx with Q matrices having eigenvalues λi=i^2) are used. Momentum and AdaGrad are compared against their meta-learned variants (Meta-Momentum and Meta-AdaGrad, which meta-learn element-wise step sizes). Performance is evaluated based on loss per iteration and cumulative loss sensitivity to learning rates. (2) ImageNet, where a 50-layer ResNet is trained. Standard SGD is compared against meta-learning (tuning element-wise learning rates online) and optimistic meta-learning (using previous gradients as hints). Top-1 test accuracy over training steps is reported, with shading indicating 95% confidence intervals over 3 seeds. Hyper-parameter sweeps are conducted for learning rates, winit scale, and decay/meta-learning rates in both setups.",
    "Limitations": "The theoretical results are primarily derived for the single-task convex optimization setting, which simplifies the complexities of general meta-learning. The analysis relies on a \"stylized variant\" of meta-learning and specific assumptions on the update rule ϕ (e.g., affine in w, smooth with bounded norm) that may not always hold in practical, complex scenarios. Computing the Jacobian Dϕ, which is central to the theoretical derivations, is often intractable in practical meta-learning applications. Standard meta-learning without optimism only achieves a slower O(1/T) convergence rate, underscoring the necessity of optimism for acceleration. Additionally, the paper notes that Bootstrapped Meta-Gradients (BMG) can suffer from divergence if the gradient used in its target computation (yt) is not carefully scaled.",
    "Future Research Directions": "The paper identifies several future research avenues, including exploring novel forms of meta-learning that effectively incorporate optimism, as it is highlighted as an \"exciting opportunity.\" The theoretical analysis provides insights into stabilizing Bootstrapped Meta-Gradients (BMG), suggesting further work on implementing and evaluating simple correction methods in practice. Moreover, the paper notes that the presented regret bounds and analysis can be extended to stochastic optimization settings, indicating this as a promising direction for broader applicability of the theoretical framework.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Predicting Training Time Without Training ",
    "Main Contributions": "The paper tackles the problem of predicting the number of optimization steps required for a pre-trained deep network to converge to a given loss value, without performing actual training. The main contributions include: (i) a qualitative and quantitative analysis of fine-tuning training time as a function of the Gram-Matrix (empirical Neural Tangent Kernel) of gradients at initialization; (ii) methods to reduce the computational cost of this estimation using random projections, making it efficient for common architectures and large datasets; (iii) a technique to estimate training time on larger datasets using only a subset of samples, under the assumption of data coming from the same distribution; and (iv) empirical validation showing accurate predictions (within 13-20% error margin) with a 30 to 45-fold reduction in cost compared to actual training.",
    "Methodology": "The methodology relies on approximating the training dynamics of a pre-trained deep neural network during fine-tuning with those of a linearized model, obtained via a Taylor expansion around the pre-trained weights. This allows the problem to be framed as solving a low-dimensional Stochastic Differential Equation (SDE) in function space (predicting output evolution) rather than high-dimensional weight space. The SDE comprises a deterministic part and a stochastic term accounting for SGD noise, utilizing the Gram gradients matrix (empirical NTK) and the covariance matrix of gradients computed at initialization. The paper also discusses how to handle momentum via an 'effective learning rate' heuristic. To enhance efficiency, the method incorporates random projections of gradients to reduce the cost of computing the Gram matrix and approximates the noise covariance matrix with its diagonal, scaled based on the loss function's norm decrease. For larger datasets, it leverages the power-law distribution of Gram matrix eigenvalues to predict training time using a smaller subset of samples.",
    "Experimental Setup": "Experiments were conducted by defining training time as the point where the smoothed loss falls within a normalized threshold (\b\t from the final loss value at a fixed total time T). ResNet18/34 models, pre-trained on ImageNet, were fine-tuned using either Gradient Descent (GD) or Stochastic Gradient Descent (SGD). The tasks involved multi-class classification with Cross-Entropy Loss on various popular datasets, including CIFAR10, CIFAR100, CUB200, FGVC-Aircrafts, MIT-67, OpenSurfaces (Minc2500), and StanfordCars. For each dataset, 5 random classes were sampled, with 150 images per class (or the maximum available). The computational budget for training was fixed at T=150 steps. Validation involved comparing predicted training times against actual training times and reporting the absolute prediction error. Wall-clock runtime comparisons were also performed to assess computational savings.",
    "Limitations": "The core limitation stems from the linearization assumption, which is more robust for pre-trained networks where weights remain close to their initialization, but its validity can decrease under certain conditions. Specifically, prediction accuracy is shown to decrease with higher learning rates and larger dataset sizes, as these factors cause weights to diverge further from the initial pre-trained state. Smaller batch sizes also lead to a slight increase in prediction error, attributed to approximations made in modeling the stochastic noise term. While the linearized model accurately predicts network outputs and thus the loss, the trajectories of the actual network weights and the linearized model's weights can differ substantially. Additionally, when using cross-entropy loss, predicted and observed loss curves may differ near convergence due to potential softmax output saturation, though the \b\t-training-time prediction remains accurate if asymptotic slopes are similar.",
    "Future Research Directions": "A primary future research direction involves developing a 'closed-loop' training time prediction procedure. This entails performing training time prediction not just before fine-tuning begins, but continuously during the actual training process. By leveraging real-time feedback, such as gradient updates from the ongoing training, the prediction model could adapt and improve its accuracy dynamically.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Identifying Policy Gradient Subspaces",
    "Main Contributions": "The paper addresses the inefficiency of deep reinforcement learning (RL) due to persistently changing data distributions and a large number of parameters. Drawing inspiration from supervised learning (SL) where gradients reside in low-dimensional, slowly-changing subspaces, this work empirically evaluates if similar gradient subspaces exist in deep policy gradient (PG) methods. The main contributions are demonstrating that: (i) policy gradients for actor and critic networks in PPO and SAC indeed live in low-dimensional, high-curvature subspaces; (ii) these subspaces remain relatively stable throughout RL training despite continuous data distribution shifts; and (iii) the critic's subspace generally exhibits less variability and retains a larger fraction of its gradient. The findings highlight the potential for enhanced RL performance through improved parameter-space exploration or second-order optimization.",
    "Methodology": "The study conducts a comprehensive empirical evaluation of gradient subspaces using two popular PG algorithms: Proximal Policy Optimization (PPO) (on-policy) and Soft Actor-Critic (SAC) (off-policy). It assesses three conditions: (i) the existence of high-curvature directions by analyzing the spectrum of Hessian eigenvalues (estimated via the Lanczos method); (ii) how well the gradients lie in the high-curvature subspace using the gradient subspace fraction Sfrac(Pk, g) = ||Pk g||^2 / ||g||^2; and (iii) the stability of these subspaces over time using the subspace overlap Soverlap(P(t1)k, P(t2)k). Hessian and gradient estimates were computed using 10^6 on-policy samples for PPO and the full replay buffer for SAC, with further robustness tests employing mini-batch estimates (2048 samples).",
    "Experimental Setup": "Experiments were performed using PPO and SAC on twelve benchmark tasks from OpenAI Gym, Gym Robotics, and the DeepMind Control Suite. The algorithm implementations from Stable Baselines3 were utilized, with hyperparameters sourced from RL Baselines3 Zoo for Gym tasks and tuned via random search for others. Each experiment was run for 10 random seeds. Analysis of gradient subspace fraction and subspace overlap was conducted at checkpoints every 50,000 steps. Training progress was divided into initial, training, and convergence phases based on performance improvement (10% and 90% of total improvement). A subspace dimensionality of k=100 was chosen. Additionally, robustness to suboptimal hyperparameters was assessed by evaluating 100 randomly sampled configurations.",
    "Limitations": "While the paper demonstrates the existence and stability of gradient subspaces, a direct limitation for immediate practical application is the computational expense of calculating Hessian eigenvectors at every training step, even with efficient methods like Lanczos. The study notes that low-sample mini-batch gradient approximations slightly perturb the gradient out of the precise subspace. Furthermore, the effectiveness and stability of gradient subspaces, as measured by gradient subspace fraction and subspace overlap, can vary significantly with suboptimal hyperparameter configurations, particularly for PPO's actor, suggesting that subspace updates might need to be more frequent depending on hyperparameters.",
    "Future Research Directions": "The findings suggest two main avenues for future research: (1) Optimization in the subspace: Develop new RL optimization methods that operate within the identified low-dimensional, high-curvature subspace. This could enable efficient computation and inversion of the Hessian, making second-order optimization feasible for deep RL and addressing ill-conditioned problems. (2) Guiding parameter-space exploration: Utilize the knowledge of high-curvature subspaces to guide parameter-space exploration by sampling exploration noise predominantly in these relevant directions, potentially leading to more efficient and directed learning in RL. Additionally, further analysis could explore the implications of the critic's greater subspace stability for specialized optimization strategies.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
    "Main Contributions": "This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that explicitly accounts for computational time constraints. It penalizes slower methods by providing them with fewer samples for adaptation, simulating a constant-speed data stream. The study demonstrates that when inference speed is considered, simpler and faster TTA approaches can often outperform more sophisticated but slower state-of-the-art methods. The research highlights the critical importance of developing TTA methods that are both accurate and computationally efficient for real-world applicability.",
    "Methodology": "The proposed 'Realistic Online Evaluation Protocol' simulates an online data stream with a constant speed, unlike current offline protocols that assume the stream waits for adaptation. It introduces a 'relative adaptation speed' C(g) for each TTA method g, which is the integer ratio of the stream's speed to the method's adaptation speed. If C(g) = k, the method adapts to every k-th sample, while skipped samples are processed by the most recent adapted model (or the base model). C(g) is computed online for each input, allowing for variability. This intrinsically penalizes slower methods by reducing their opportunities for adaptation.",
    "Experimental Setup": "The evaluation was conducted on image classification tasks using a ResNet-50-BN backbone (pretrained on ImageNet), with additional experiments on ViT and ResNet-18. A typical stream batch size of 64 was used, with ablations for {1, 16, 32, 128}. Datasets included ImageNet-C (with 15 corruptions at level 5), CIFAR10-C, ImageNet-R, and ImageNet-3DCC (severity level 5). Fifteen state-of-the-art TTA methods (e.g., AdaBN, SHOT, TENT, SAR, EATA, DDA, MEMO) published between 2017 and 2023 were benchmarked. Experiments covered episodic evaluation (single domain shift, model reset), continual evaluation (concatenated shifts, no reset), stream speed analysis (varying ηr), and practical TTA with label imbalances. Validation involved error rates, average error rates, and standard deviations across 3 random seeds, using official implementations and recommended hyperparameters.",
    "Limitations": "The primary limitation identified is the high computational overhead of many existing TTA methods, which significantly increases adaptation time and renders them impractical in real-world scenarios with constant-speed data streams. The current offline evaluation protocols fail to capture this critical aspect, leading to an overestimation of the real-world performance of slower, more complex methods. Data-dependent approaches like MEMO and DDA were found to be extremely inefficient, performing close to non-adapted models under realistic conditions. Slower sample rejection methods, such as SAR, also experienced significant performance degradation. Additionally, some methods without sample rejection in continual adaptation can overfit to early distributions when not constrained by speed.",
    "Future Research Directions": "The paper strongly encourages future research to focus on developing TTA methods that are both accurate and computationally efficient. A key direction is to increase the efficiency of data-dependent adaptation methods. The proposed evaluation scheme aims to inspire TTA methods to prioritize inference speed as a critical dimension for real-world performance. Further exploration into efficient TTA methods with negligible additional computational overhead is also suggested, especially in scenarios where only a single model can be deployed.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
    "Main Contributions": "The paper introduces the \"recurring test-time adaptation (TTA)\" diagnostic setting to study and reveal the overlooked gradual performance degradation (model collapse) of TTA methods over prolonged periods. It formally defines TTA collapsing and provides a theoretical analysis using an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC), identifying dataset- and algorithm-dependent factors that contribute to error accumulation. Furthermore, the paper proposes \"Persistent TTA (PeTTA)\", a novel adaptation scheme that senses when the model is diverging towards collapse and adaptively adjusts its strategy to balance adaptation and collapse prevention, demonstrating superior stability and performance over existing approaches in lifelong TTA scenarios.",
    "Methodology": "The study's methodology involves: (1) **Introducing Recurring TTA:** A diagnostic testing scenario where environments not only change but also recur over time, based on practical TTA but with repeated cycles through distributions. (2) **Theoretical Analysis with ϵ-GMMC:** A simplified yet representative ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) is used on a synthesized dataset to model and analyze the collapsing phenomenon. This analysis helps derive theoretical insights into data-dependent factors (prior data distribution, category difference) and algorithm-dependent factors (update rate, false negative rate) that cause performance degradation. (3) **Persistent TTA (PeTTA) Algorithm:** PeTTA extends the mean teacher update framework by continuously monitoring model divergence. It employs a mechanism to sense divergence from the initial model in the feature embedding space using Mahalanobis distance of running mean feature vectors (γ_y^t). Based on an average divergence measure (¯γ_t), PeTTA adaptively adjusts the regularization coefficient (λ_t) and the EMA update rate (α_t) on the fly. It also incorporates an anchor loss (LAL) that minimizes the KL divergence between the current and source model's output probabilities to prevent significant deviation, along with a category-balanced memory bank and robust batch normalization layers adopted from prior work.",
    "Experimental Setup": "The experimental setup includes: (1) **ϵ-GMMC Simulation:** A synthesized dataset of 6000 samples from two Gaussian distributions (N(0,1), N(2,1)) with equal priors (p0=p1=0.5), released in batches of 10. Model updates followed Eq. 4 with α=5e-2, and model collapse was simulated by randomly flipping 10% of true-positive pseudo-labels. (2) **Benchmark Datasets:** CIFAR-10-C, CIFAR-100-C, and ImageNet-C (all with corruption level 5), and DomainNet (real → clipart, painting, sketch). (3) **Testing Scenarios:** The proposed recurring TTA setting, where testing environments gradually change and recur 20 times (K=20). Category distributions within batches are temporally correlated, generated by Dirichlet distributions (Dir(0.1) for CIFAR-10-C, DomainNet, ImageNet-C; Dir(0.01) for CIFAR-100-C). Performance was also evaluated on the Continuously Changing Corruption (CCC) benchmark, involving 80,000 adaptation steps across over 5.1 million images. (4) **Compared Methods:** CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME (parameter-free), and RDumb (reset-based). (5) **Implementation Details:** Experiments used PyTorch with pre-trained source models from RobustBench and torchvision. Hyper-parameters like α0 (1e-3) and λ0 (10 for CIFAR-C/ImageNet-C, 1 for DomainNet) were set, and PeTTA's performance was averaged across 5 runs with different random seeds. EMA update rate for robust batch normalization and feature embedding statistics was 5e-2.",
    "Limitations": "The study acknowledges several limitations: (1) A complete elimination of error accumulation through regularization cannot be rigorously guaranteed by PeTTA. (2) The approach implicitly assumes the availability of a small memory bank to handle the challenge of temporally correlated testing streams, as this aspect was not the primary focus of PeTTA. (3) PeTTA requires pre-computed feature statistics (empirical mean and covariant matrix of feature vectors) from the source distribution, which might not always be available in certain real-world scenarios and could potentially limit its scalability.",
    "Future Research Directions": "Future research could delve deeper into developing TTA algorithms that are inherently error accumulation-free by construction, moving beyond regularization-based mitigation. Additionally, further exploration is needed to find alternative ways for reducing memory size (e.g., storing embedded features instead of original images) to enhance the scalability of TTA methods in real-world scenarios where memory banks might be a constraint.",
    "Experiment Code": "import math\nimport torch\nimport torch.nn as nn\n\nfrom copy import deepcopy\nfrom methods.base import TTAMethod\nfrom augmentations.transforms_cotta import get_tta_transforms\nfrom utils.registry import ADAPTATION_REGISTRY\nfrom utils.misc import ema_update_model\n\n\n@ADAPTATION_REGISTRY.register()\nclass RoTTA(TTAMethod):\n    def __init__(self, cfg, model, num_classes):\n        super().__init__(cfg, model, num_classes)\n\n        self.memory_size = cfg.ROTTA.MEMORY_SIZE\n        self.lambda_t = cfg.ROTTA.LAMBDA_T\n        self.lambda_u = cfg.ROTTA.LAMBDA_U\n        self.nu = 1 - cfg.ROTTA.NU  # we do the ema update consistently the other way round: (param * ema_model + (1-param) * model)\n        self.update_frequency = cfg.ROTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n        self.mem = CSTU(capacity=self.memory_size, num_class=self.num_classes, lambda_t=self.lambda_t, lambda_u=self.lambda_u)\n\n        # setup the ema model\n        self.model_ema = self.copy_model(self.model)\n        for param in self.model_ema.parameters():\n            param.detach_()\n\n        # note: if the self.model is never reset, like for continual adaptation,\n        # then skipping the state copy would save memory\n        self.models = [self.model, self.model_ema]\n        self.model_states, self.optimizer_state = self.copy_model_and_optimizer()\n\n        # create the test-time transformations\n        self.transform = get_tta_transforms(self.img_size)\n\n    def loss_calculation(self):\n        self.model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        loss_sup = torch.tensor([(float('nan'))])\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = self.model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages, device=self.device)\n            loss_sup = (softmax_cross_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n        return loss_sup\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, x):\n        imgs_test = x[0]\n\n        with torch.no_grad():\n            self.model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(imgs_test)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(imgs_test):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                if self.mixed_precision and self.device == \"cuda\":\n                    with torch.cuda.amp.autocast():\n                        loss = self.loss_calculation()\n                    self.scaler.scale(loss).backward()\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                    self.optimizer.zero_grad()\n                else:\n                    loss = self.loss_calculation()\n                    loss.backward()\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n\n                self.model_ema = ema_update_model(\n                    model_to_update=self.model_ema,\n                    model_to_merge=self.model,\n                    momentum=self.nu,\n                    device=self.device,\n                    update_all=True\n                )\n\n        return ema_out\n\n    def reset(self):\n        if self.model_states is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved self.model/optimizer state\")\n        self.load_model_and_optimizer()\n        self.current_instance = 0\n        self.mem = CSTU(capacity=self.memory_size,\n                        num_class=self.num_classes,\n                        lambda_t=self.lambda_t,\n                        lambda_u=self.lambda_u)\n\n    def configure_model(self):\n        self.model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in self.model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n            elif isinstance(sub_module, (nn.LayerNorm, nn.GroupNorm)):\n                sub_module.requires_grad_(True)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(self.model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer, self.cfg.ROTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(self.model, name, momentum_bn)\n\n\n@torch.jit.script\ndef softmax_cross_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n\ndef timeliness_reweighting(ages, device):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().to(device)\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\n\ndef get_named_submodule(model, sub_name: str):\n    names = sub_name.split(\".\")\n    module = model\n    for name in names:\n        module = getattr(module, name)\n\n    return module\n\n\ndef set_named_submodule(model, sub_name, value):\n    names = sub_name.split(\".\")\n    module = model\n    for i in range(len(names)):\n        if i != len(names) - 1:\n            module = getattr(module, names[i])\n\n        else:\n            setattr(module, names[i], value)\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n",
    "Experiment Result": "Method: RoTTA (Recurrent Test-Time Adaptation), a method in the repository that shares several components with the described Persistent TTA (PeTTA) algorithm.\n\nCore Idea: RoTTA extends the mean teacher framework for test-time adaptation by incorporating a category-balanced memory bank (CSTU) and robust batch normalization (RobustBN) layers to handle recurring shifts.\n\nKey Components and Settings:\n- **Mean Teacher Update Framework**: The `model_ema` (EMA teacher model) is updated using an exponential moving average (EMA) with a momentum derived from `cfg.ROTTA.NU` (specifically, `1 - cfg.ROTTA.NU`). This helps stabilize learning by averaging model weights over time.\n- **Robust Batch Normalization Layers**: Batch normalization layers (`nn.BatchNorm1d`, `nn.BatchNorm2d`) are replaced with `RobustBN1d` and `RobustBN2d`. These custom BN layers update their running mean and variance using a momentum specified by `cfg.ROTTA.ALPHA`. This allows the batch statistics to adapt to new domains while retaining some information from previous statistics.\n- **Category-balanced Memory Bank (CSTU)**: A memory bank (`self.mem`) with a `capacity` of `cfg.ROTTA.MEMORY_SIZE` is used. It stores instances including the input data, its pseudo-label, and its uncertainty (entropy). Instances are added to the memory, and a `heuristic_score` is used to decide which instances to remove when the memory is full. This score combines an `age` factor (influenced by `cfg.ROTTA.LAMBDA_T`) and an `uncertainty` factor (influenced by `cfg.ROTTA.LAMBDA_U`). This mechanism helps maintain a diverse and relevant set of samples in memory.\n- **Update Frequency**: The model's parameters and the EMA teacher model are updated every `cfg.ROTTA.UPDATE_FREQUENCY` incoming test instances.\n- **Loss Function**: The primary loss `loss_sup` is a consistency loss, specifically `softmax_cross_entropy`, calculated between the student model's output on augmented memory data (`stu_sup_out`) and the EMA teacher's output on original memory data (`ema_sup_out`). This loss is reweighted by `timeliness_reweighting(ages)`, which gives more importance to newer samples in the memory.\n\nDiscrepancies from \"Method\" description:\n- **Adaptive Adjustment of Parameters**: The \"Method\" describes PeTTA as adaptively adjusting the regularization coefficient (`λ_t`) and the EMA update rate (`α_t`) on the fly based on an average divergence measure (`¯γ_t`). In the provided RoTTA implementation, `cfg.ROTTA.LAMBDA_T`, `cfg.ROTTA.LAMBDA_U`, `cfg.ROTTA.ALPHA`, and `cfg.ROTTA.NU` are fixed hyperparameters and do not adapt dynamically.\n- **Divergence Monitoring**: The \"Method\" mentions monitoring model divergence using Mahalanobis distance of running mean feature vectors (`γ_y^t`). This specific divergence monitoring mechanism is not explicitly present in the RoTTA implementation.\n- **Anchor Loss (LAL)**: The \"Method\" specifies an anchor loss (LAL) that minimizes the KL divergence between the current and source model's output probabilities. The consistency loss used in RoTTA is between the student model and the EMA teacher model on memory data, not explicitly between the current model and the initial source model's output probabilities."
}{
    "Title": "Robust Test-Time Adaptation in Dynamic Scenarios",
    "Main Contributions": "This research addresses the limitations of prior Test-Time Adaptation (TTA) methods in dynamic real-world scenarios, which are characterized by continually changing data distributions and correlative test data sampling over time. The authors propose a new, more realistic TTA setup called Practical Test-Time Adaptation (PTTA). Their main technical contribution is Robust Test-Time Adaptation (RoTTA), a method designed to effectively adapt pre-trained models in PTTA settings. RoTTA achieves state-of-the-art performance, outperforming baselines significantly on common TTA benchmarks (CIFAR-10-C, CIFAR-100-C, and DomainNet) by reducing average classification error.",
    "Methodology": "The proposed Robust Test-Time Adaptation (RoTTA) method comprises three key components: 1) Robust Batch Normalization (RBN): It replaces traditional batch normalization statistics with global running mean and variance, initialized from the pre-trained model and updated via exponential moving average (EMA) using statistics from a memory bank, to handle correlatively sampled test data streams. 2) Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) for the Memory Bank: A memory bank is maintained using pseudo-labels from model predictions. Its capacity is equally distributed among categories. Samples are prioritized based on a heuristic score that combines their age (timeliness, increasing over time) and uncertainty (entropy of prediction), ensuring that newer and more confident samples are kept while maintaining category balance. 3) Time-Aware Robust Training with a Teacher-Student Model: A teacher-student architecture is used, where the student model is updated by minimizing a robust loss on memory bank samples, and the teacher model is updated via EMA of the student's parameters. A timeliness reweighting strategy is applied to the loss, giving less weight to older samples to stabilize adaptation and reduce error accumulation. For efficiency and stability, only affine parameters in RBN are trained during adaptation.",
    "Experimental Setup": "The method was evaluated on: 1) CIFAR10-C and CIFAR100-C datasets, commonly used TTA benchmarks, under 15 corruption types at severity 5 to simulate continually changing distributions. 2) DomainNet, a large-scale domain adaptation dataset (0.6 million images, 345 classes, 6 domains) to test generalization under a huge domain gap. Correlative sampling for all datasets was simulated using a Dirichlet distribution (default parameter δ = 0.1). Pre-trained models included WildResNet-28 for CIFAR10-C and ResNeXt-29 for CIFAR100-C (from RobustBench), and ResNet-101 for DomainNet. For optimization, the Adam optimizer was used with a learning rate of 1.0 × 10⁻³ and batch size 64. The memory bank capacity was set to N=64. Hyperparameters for RoTTA were unified across experiments: α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. The approach was validated by comparing average classification error against state-of-the-art baselines (BN, PL, TENT, LAME, CoTTA, NOTE) and through ablation studies on RoTTA's components, as well as sensitivity analyses to hyperparameters, distribution changing order, and batch size.",
    "Limitations": "The Robust Batch Normalization (RBN) component is considered a naive solution for normalizing correlatively sampled data and requires careful tuning of its 'α' parameter. The current method lacks a mechanism to explicitly recover the model from a collapsed state, a phenomenon observed in some baseline adaptation procedures. Furthermore, while the experiments use Dirichlet distribution to simulate correlative sampling based on category similarity, it is noted that category similarity is only one type of correlation, and the approach needs further validation in more diverse, real-world scenarios.",
    "Future Research Directions": "Future work could focus on improving the RoTTA algorithm by replacing or refining some of its current components. More broadly, the authors hope this work encourages further exploration into making the Practical Test-Time Adaptation (PTTA) setup even more realistic, ultimately paving the way for deploying and adapting models in complex, dynamic real-world applications through test-time adaptation algorithms.",
    "Experiment Code": "import torch\nimport torch.nn as nn\nfrom ..utils import memory\nfrom .base_adapter import BaseAdapter\nfrom copy import deepcopy\nfrom .base_adapter import softmax_entropy\nfrom ..utils.bn_layers import RobustBN1d, RobustBN2d\nfrom ..utils.utils import set_named_submodule, get_named_submodule\nfrom ..utils.custom_transforms import get_tta_transforms\n\n\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(ColorJitter):\n    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma\n                                            # after contrast\n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string",
    "Experiment Result": "The RoTTA method involves three main components with the following experimental settings:\n\n1.  **Robust Batch Normalization (RBN):**\n    *   Traditional Batch Normalization layers are replaced with `RobustBN1d` or `RobustBN2d`. These layers use global running mean and variance (initialized from the pre-trained model) and update them via exponential moving average (EMA) with a momentum (`ALPHA`) of 0.05. Only the affine parameters of these RBN layers are trained during adaptation.\n\n2.  **Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) for the Memory Bank:**\n    *   A memory bank (`CSTU`) is maintained with a capacity (`MEMORY_SIZE`) of 64 samples. It aims to distribute this capacity equally among categories based on the number of classes (`NUM_CLASS`) in the dataset (e.g., 10 for CIFAR-10, 100 for CIFAR-100).\n    *   Samples are prioritized for retention/removal based on a heuristic score combining their age (timeliness) and uncertainty (entropy of prediction). The weighting factors for timeliness (`LAMBDA_T`) and uncertainty (`LAMBDA_U`) are both set to 1.0.\n    *   The heuristic score is calculated as `lambda_t * 1 / (1 + math.exp(-age / capacity)) + lambda_u * uncertainty / math.log(num_class)`.\n\n3.  **Time-Aware Robust Training with a Teacher-Student Model:**\n    *   A teacher-student architecture is employed. The student model is updated using samples from the memory bank.\n    *   The teacher model's parameters are updated via EMA of the student's parameters with a decay rate (`NU`) of 0.001.\n    *   The student model minimizes a robust loss (`softmax_entropy`) which measures the difference between the student's prediction on strongly augmented memory samples and the teacher's prediction on original memory samples.\n    *   A timeliness reweighting strategy is applied to this loss, where `instance_weight = torch.exp(-ages) / (1 + torch.exp(-ages))`, giving less weight to older samples from the memory bank. `ages` are normalized by the memory capacity.\n    *   The model (student) is updated using the collected memory bank data every `UPDATE_FREQUENCY` instances, which is set to 64 (equal to `MEMORY_SIZE`).\n\n**General Optimization and Data Settings:**\n*   **Optimization:** The optimizer used is Adam with a learning rate (`LR`) of 1e-3, betas of (0.9, 0.999), and weight decay (`WD`) of 0.0. One optimization step (`STEPS`=1) is performed per batch of incoming data. During each step, an update to the model might occur if the `update_frequency` condition is met.\n*   **Data Augmentation (for memory bank samples):** Strong augmentations are applied to memory bank samples before they are passed to the student model. These transformations include clipping (0.0, 1.0), `ColorJitterPro` (brightness [0.6, 1.4], contrast [0.7, 1.3], saturation [0.5, 1.5], hue [-0.06, 0.06], gamma [0.7, 1.3]), padding (half image size), `RandomAffine` (degrees [-15, 15], translate (1/16, 1/16), scale (0.9, 1.1)), `GaussianBlur` (kernel size 5, sigma [0.001, 0.5]), `CenterCrop` to original size (e.g., 32x32), `RandomHorizontalFlip` (p=0.5), and `GaussianNoise` (std=0.005).\n*   **Input Data:** The method is evaluated on datasets like CIFAR-10/CIFAR-100 under various corruption types and severities. Input images are resized to (32, 32), normalized using standard ImageNet mean ([0.485, 0.456, 0.406]) and std ([0.229, 0.224, 0.225]). The testing batch size (`TEST.BATCH_SIZE`) is 64."
}{
    "Title": "Test Time Adaptation via Conjugate Pseudo-labels",
    "Main Contributions": "The paper addresses the challenge of selecting effective unsupervised objectives for Test-Time Adaptation (TTA) by proposing a principled framework based on the convex conjugate function. It reveals through meta-learning experiments that the 'best' TTA loss is dependent on the source classifier's training loss (e.g., softmax-entropy for cross-entropy, negative squared error for squared loss). The core contribution is a generic recipe for deriving TTA losses for a broad class of supervised training losses, showing that the conjugate adaptation loss is equivalent to self-training with specific 'conjugate pseudo-labels'. Empirically, the proposed conjugate pseudo-labeling method consistently outperforms other TTA alternatives across various domain adaptation benchmarks, especially for models trained with novel loss functions like PolyLoss.",
    "Methodology": "The methodology involves two main components: 1) An exploratory meta-learning procedure where the TTA loss function is parameterized by a neural network and learned by differentiating through the adaptation process to discover optimal TTA losses. 2) The proposed Conjugate Pseudo-labels (CPL) framework, which defines the TTA loss Lconj(hθ(x)) as the negative convex conjugate of a function f from the supervised training loss L(hθ(x),y) = f(hθ(x)) - yᵀhθ(x). This approach interprets TTA as self-training using conjugate pseudo-labels ˜yCPLθ(x) = ∇f(hθ(x)). The adaptation process (Algorithm 1) involves iteratively updating model parameters by taking gradient steps on this conjugate pseudo-labeling loss, typically applying a temperature scaling T to the predictions hθ(x)/T, and often updating only the batch normalization layers.",
    "Experimental Setup": "The evaluation uses three common corruption benchmarks (CIFAR-10-C, CIFAR-100-C, ImageNet-C) with errors averaged across corruptions and severity, and three domain adaptation datasets (SVHN to MNIST, ImageNet to ImageNet-R, VisDA-C). Source classifiers include ResNet-26 for CIFAR, ResNet-18 for SVHN, and ResNet-50 for ImageNet/VisDA-C. Models are trained with cross-entropy, PolyLoss, and squared loss. Baselines include Hard Pseudo-Labels (PL), Soft Pseudo-Labels (PL), Entropy Minimization (ENT), Robust Pseudo-Label (RPL), and MEMO. TTA involves fine-tuning batch normalization layers, with LR and temperature (T) tuned via grid-search on validation noises for corruption benchmarks, or fixed (T=1, LR from literature) for domain adaptation tasks. All experiments were performed on A6000 GPUs.",
    "Limitations": "The work does not fully explain the fundamental reasons for the optimality of the discovered conjugate loss, beyond showing its proximity to the oracle supervised loss. The meta-learning framework used was constrained to learn functions only over the logits of individual inputs, suggesting potential for more complex loss functions. Achieving good TTA still relies on heuristics like updating only batch norm parameters and temperature scaling, whose precise roles are not concretely understood. Furthermore, it remains an open problem to identify the types of real-world distribution shifts where self-training-based approaches are most beneficial.",
    "Future Research Directions": "Future research directions include expanding the meta-learning framework to learn more complex loss functions, such as those operating on intermediate representations or accounting for interactions within a batch of inputs. A deeper understanding of the role of practical heuristics like batch normalization parameter updates and temperature scaling in TTA is also suggested. Investigating the specific real-world distribution shifts where self-training-based methods offer significant advantages is another open problem. Finally, extending and applying the conjugate pseudo-labeling framework to other settings like semi-supervised learning is a promising avenue for further exploration.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Look-ahead Meta Learning for Continual Learning",
    "Main Contributions": "The paper addresses the challenge of catastrophic forgetting in continual learning (CL) for models with limited capacity, especially in online, sequential task environments. It proposes Look-ahead MAML (La-MAML), a fast optimization-based meta-learning algorithm for online-continual learning, enhanced by a small episodic memory. La-MAML introduces modulation of per-parameter learning rates (LRs) in its meta-learning update, drawing connections to hypergradients and meta-descent, which provides a flexible and efficient way to mitigate catastrophic forgetting compared to prior-based methods. The base algorithm, Continual-MAML (C-MAML), utilizes a replay-buffer and an online meta-objective that aligns gradients between current and past tasks. La-MAML demonstrates superior performance over other replay-based, prior-based, and meta-learning based CL approaches on various visual classification benchmarks, achieving comparable results to state-of-the-art methods like MER in significantly less training time (under 20%).",
    "Methodology": "The core methodology revolves around optimization-based meta-learning, building upon the Model-Agnostic Meta-Learning (MAML) and Online-aware Meta-Learning (OML) objectives. The base algorithm, C-MAML, optimizes an online OML objective (Eq. 5) to minimize cumulative risk across tasks. This objective's k-step MAML update is shown to be equivalent to an asymmetric CL objective (Eq. 6) that aligns the current task's gradients with the average gradient of previous tasks, offering a speedup over MER. La-MAML extends C-MAML by incorporating learnable, per-parameter LRs into the inner updates (Algorithm 1). These LRs are updated asynchronously in the meta-update, with their gradients (gMAML(αj)) reflecting the alignment between old and new tasks (Eq. 8). The LRs are clipped to positive values to prevent ascending gradients and mitigate catastrophic forgetting. This asynchronous update mechanism allows the model to conservatively modulate the pace and direction of learning, thereby accelerating progress on new tasks while facilitating knowledge transfer from old ones. The approach integrates concepts from replay-based and prior-based methods by using a replay buffer and data-driven LR modulation.",
    "Experimental Setup": "Experiments were conducted on both toy and real-world visual classification benchmarks. Toy benchmarks included MNIST Rotations, MNIST Permutations (20 tasks each, 1000 samples/task), and Many Permutations (100 tasks, 200 images/task). Real-world benchmarks involved CIFAR-100 (20 tasks, disjoint 5-way classification) and TinyImagenet-200 (40 tasks, 5-way classification). Experiments were performed in both task-agnostic (MNIST, single-headed model) and task-aware (CIFAR, TinyImagenet, multi-headed model) settings. Two primary setups were evaluated: 'Single-Pass' (Efficient Lifelong Learning - LLL), where data is processed once, and 'Multiple-Pass' (standard CL), allowing up to 10 epochs. Performance was measured using Retained Accuracy (RA) and Backward-Transfer and Interference (BTI). The architectures included a fully-connected network for MNIST (2 layers, 100 nodes) and CNNs for CIFAR/TinyImagenet (3-4 conv layers, 2 FC layers). A modest replay buffer was used (200-500 samples for MNIST, 200-400 for CIFAR/TinyImagenet). SGD optimizer with gradient clipping (norm 2.0) was used, with hyperparameters tuned via grid-search. Baselines included Online, EWC, GEM, MER, IID, ER, iCARL, A-GEM, and Meta-BGD, along with ablations C-MAML, LA-ER, and Sync-La-MAML.",
    "Limitations": "The algorithm inherently faces the potential for concept drift, as it stores and replays random samples, even with improved gradient alignment. Any bias in the sampling strategy itself could influence the remembered data distribution and model performance. The equivalent CL objective (Eq. 11) is noted to introduce some bias, meaning parameters do not precisely converge to the minimizer of losses across all tasks, though this is deemed acceptable in the continual learning context. In comparison to baselines, Meta-BGD was found to be highly sensitive to hyperparameters and computationally expensive. Additionally, the learnable per-parameter learning rates introduce a memory overhead equivalent to the network's parameters.",
    "Future Research Directions": "Future research could focus on developing and analyzing improved optimizers specifically tailored for continual learning, as current standard optimizers (e.g., Adam) are primarily designed for stationary supervised learning. Another promising direction involves further exploring the connections to meta-descent. This could lead to more stable meta-learning training procedures that can automatically adjust hyperparameters dynamically based on ongoing training dynamics, reducing the need for extensive manual tuning.",
    "Experiment Code": "# From model/meta/learner.py\nclass Learner(nn.Module):\n    def define_task_lr_params(self, alpha_init=1e-3): \n        self.alpha_lr = nn.ParameterList([])\n        for p in self.parameters():\n            self.alpha_lr.append(nn.Parameter(alpha_init * torch.ones(p.shape, requires_grad=True)))\n\n# From model/lamaml_base.py\nclass BaseNet(torch.nn.Module):\n    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n        super(BaseNet, self).__init__()\n        self.args = args\n        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [args.n_hiddens] * args.n_layers + [n_outputs],\n                                                dataset = args.dataset, args=args)\n        self.net = Learner.Learner(config, args)\n        self.net.define_task_lr_params(alpha_init = args.alpha_init)\n        self.opt_wt = torch.optim.SGD(list(self.net.parameters()), lr=args.opt_wt)     \n        self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr) \n        self.loss = torch.nn.CrossEntropyLoss()\n        self.memories = args.memories\n        self.M = [] # Main replay buffer\n        self.M_new = [] # Temporary buffer for current task\n        self.age = 0 # Counter for reservoir sampling\n        self.current_task = 0\n        # ... (other initializations)\n\n    def push_to_mem(self, batch_x, batch_y, t):\n        if self.real_epoch == 0 and self.pass_itr == 0: # Only push once per task's initial stream\n            batch_x, batch_y, t = batch_x.cpu(), batch_y.cpu(), t.cpu()\n            for i in range(batch_x.shape[0]):\n                self.age += 1\n                if len(self.M_new) < self.memories:\n                    self.M_new.append([batch_x[i], batch_y[i], t])\n                else:\n                    p = random.randint(0, self.age)  \n                    if p < self.memories: self.M_new[p] = [batch_x[i], batch_y[i], t]\n\n    def getBatch(self, x_current, y_current, t_current, batch_size=None):\n        if x_current is not None:\n            mxi = x_current.cpu().numpy()\n            myi = y_current.cpu().numpy()\n            mti = np.full(x_current.shape[0], t_current, dtype=int)\n        else:\n            # Placeholder for correct dimension handling if x_current can be None and needed for empty array shape\n            mxi = np.empty(shape=(0,))\n            myi = np.empty(shape=(0,))\n            mti = np.empty(shape=(0,))\n\n        bxs, bys, bts = [], [], []\n\n        MEM = self.M if self.args.use_old_task_memory and t_current > 0 else self.M_new\n        batch_size = self.batchSize if batch_size is None else self.batchSize\n\n        if len(MEM) > 0:\n            order = list(range(len(MEM)))\n            random.shuffle(order)\n            osize = min(batch_size, len(MEM))\n            for j in range(osize):\n                k = order[j]\n                x_mem, y_mem, t_mem = MEM[k]\n                bxs.append(x_mem.cpu().numpy() if isinstance(x_mem, torch.Tensor) else x_mem)\n                bys.append(y_mem.cpu().numpy() if isinstance(y_mem, torch.Tensor) else y_mem)\n                bts.append(t_mem.cpu().numpy() if isinstance(t_mem, torch.Tensor) else t_mem)\n\n        for j in range(len(myi)):\n            bxs.append(mxi[j])\n            bys.append(myi[j])\n            bts.append(mti[j])\n\n        bxs = Variable(torch.from_numpy(np.array(bxs))).float() \n        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)\n        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)\n        \n        if self.cuda:\n            bxs = bxs.cuda()\n            bys = bys.cuda()\n            bts = bts.cuda()\n        return bxs, bys, bts\n\n    def zero_grads(self):\n        if self.args.learn_lr: self.opt_lr.zero_grad()\n        self.opt_wt.zero_grad()\n        self.net.zero_grad()\n        self.net.alpha_lr.zero_grad()\n\n# From model/lamaml_cifar.py (adapts BaseNet for class-incremental tasks)\nclass Net(BaseNet):\n    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n        super(Net, self).__init__(n_inputs, n_outputs, n_tasks, args)\n        self.nc_per_task = n_outputs / n_tasks # Classes per task for offset calculation\n\n    def compute_offsets(self, task):\n        offset1 = task * self.nc_per_task\n        offset2 = (task + 1) * self.nc_per_task\n        return int(offset1), int(offset2)\n\n    def take_loss(self, t, logits, y):\n        offset1, offset2 = self.compute_offsets(t)\n        loss = self.loss(logits[:, offset1:offset2], y - offset1)\n        return loss\n\n    def take_multitask_loss(self, bt, t_current, logits, y):\n        loss = 0.0\n        for i, ti in enumerate(bt):\n            offset1, offset2 = self.compute_offsets(ti)\n            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0) - offset1)\n        return loss / len(bt)\n\n    def inner_update(self, x, fast_weights, y, t):\n        offset1, offset2 = self.compute_offsets(t)            \n        logits = self.net.forward(x, fast_weights)[:, :offset2] # Forward pass with current/fast weights\n        loss = self.take_loss(t, logits, y) # Calculate inner loss\n\n        if fast_weights is None: fast_weights = self.net.parameters()\n\n        graph_required = self.args.second_order\n        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))\n        \n        for i in range(len(grads)):\n            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)\n        \n        fast_weights = list(map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))\n        return fast_weights\n\n    def meta_loss(self, x, fast_weights, y, bt, t_current):\n        offset1, offset2 = self.compute_offsets(t_current)\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss_q = self.take_multitask_loss(bt, t_current, logits, y)\n        return loss_q, logits\n\n    def observe(self, x, y, t):\n        self.net.train() \n        for pass_itr in range(self.glances): # 'glances' for single-pass setting\n            self.pass_itr = pass_itr\n            perm = torch.randperm(x.size(0)); x = x[perm]; y = y[perm]\n            \n            self.epoch += 1; self.zero_grads()\n            if t != self.current_task: # If new task, update main memory from new task's samples\n                self.M = self.M_new.copy()\n                self.current_task = t\n\n            batch_sz = x.shape[0]; n_batches = self.args.cifar_batches # n_batches for inner trajectory steps\n            rough_sz = math.ceil(batch_sz/n_batches); fast_weights = None; meta_losses = [0 for _ in range(n_batches)]\n\n            bx, by, bt = self.getBatch(x, y, t)             \n\n            # Inner loop trajectory\n            for i in range(n_batches):\n                batch_x = x[i*rough_sz : (i+1)*rough_sz]; batch_y = y[i*rough_sz : (i+1)*rough_sz]\n                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)   \n                \n                if self.real_epoch == 0: self.push_to_mem(batch_x, batch_y, torch.tensor(t))\n                \n                meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) \n                meta_losses[i] += meta_loss\n\n            self.zero_grads() # Zero all gradients before meta-update\n            meta_loss = sum(meta_losses)/len(meta_losses); meta_loss.backward() # Backpropagate meta-loss\n            \n            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)\n            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)\n            \n            if self.args.learn_lr: self.opt_lr.step()\n\n            if self.args.sync_update:\n                self.opt_wt.step() # Synchronous update using opt_wt\n            else:            \n                # Asynchronous update: apply gradients using clipped (ReLU) learnable LRs as step sizes\n                for i,p in enumerate(self.net.parameters()):          \n                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])            \n            self.net.zero_grad(); self.net.alpha_lr.zero_grad() # Clear gradients\n        return meta_loss.item()",
    "Experiment Result": "The La-MAML method is evaluated on diverse datasets including 'mnist_rotations', 'cifar100', and 'tinyimagenet'. Data loading is handled by 'task_incremental_loader' or 'class_incremental_loader'. For image datasets, specific transformations are applied, such as `RandomCrop`, `RandomHorizontalFlip`, `ColorJitter` (for CIFAR-100), `ToTensor`, and `Normalize` (with dataset-specific mean/std values).\n\nKey experimental settings are configured via command-line arguments:\n- **Model Architecture**: Defined by `--arch` (e.g., 'linear', 'pc_cnn'), `--n_hiddens` (default 100), and `--n_layers` (default 2). Xavier initialization (`--xav_init`) can be enabled.\n- **Optimization Parameters**:\n    - `glances`: Number of training iterations over a batch in the single-pass setting (default 1).\n    - `n_epochs`: Training epochs per task (default 1).\n    - `batch_size`: Size of the incoming data batch (default 1).\n    - `replay_batch_size`: Size of the batch sampled from the experience replay buffer (default 20).\n    - `memories`: Maximum capacity of the reservoir sampling-based replay buffer (default 5120 samples).\n    - `opt_lr`: Learning rate for optimizing the per-parameter learnable learning rates (default 1e-1).\n    - `opt_wt`: Learning rate for updating the network's weights, applicable when `--sync_update` is active (default 1e-1).\n    - `alpha_init`: Initial value for the learnable per-parameter learning rates (default 1e-3).\n    - `learn_lr`: A boolean flag (default False) to enable the optimization of per-parameter learning rates. It is essential for La-MAML.\n    - `sync_update`: A boolean flag (default False) to choose between synchronous (optimizer updates both LRs and weights) or asynchronous (weights are updated manually using ReLU-clipped learnable LRs) update mechanisms.\n    - `grad_clip_norm`: The maximum L2 norm for gradient clipping (default 2.0).\n    - `second_order`: A boolean flag (default False) to enable second-order MAML updates.\n    - `cifar_batches`: The number of mini-batches within the inner update trajectory for CIFAR-like datasets (default 3).\n    - `use_old_task_memory`: A boolean flag (default False) to determine if the replay buffer should store samples from all previously encountered tasks.\n- **Data Handling**:\n    - `data_path`: Specifies the base directory where datasets are stored.\n    - `samples_per_task`: The number of training samples considered per task (default -1, implying all available samples).\n    - `class_order`: Strategy for presenting classes within tasks ('random', 'chrono', 'old', 'super').\n    - `increment`: Number of classes added in each incremental task step for class-incremental learning (default 5).\n    - `validation`: Proportion of the training data allocated for validation (default 0.0).\n- **Evaluation**:\n    - `log_every`: Frequency (in minibatches) for logging and evaluating validation accuracy (default 1000).\n    - `calc_test_accuracy`: A boolean flag (default False) to enable calculation of test accuracy alongside validation accuracy.\n    - `test_batch_size`: Batch size used during model evaluation (default 100000)."
}{
    "Title": "Look-ahead Meta Learning for Continual Learning",
    "Main Contributions": "The paper addresses the challenge of catastrophic forgetting in continual learning for models with limited capacity learning from sequentially arriving tasks. It proposes Look-ahead MAML (La-MAML), a fast optimization-based meta-learning algorithm for online-continual learning, enhanced by a small episodic memory. La-MAML introduces modulation of per-parameter learning rates in the meta-learning update, drawing connections to hypergradients and meta-descent. This mechanism offers a more flexible and efficient approach to mitigate catastrophic forgetting compared to conventional prior-based methods. The key finding is that La-MAML achieves superior or comparable performance against state-of-the-art replay-based, prior-based, and meta-learning based continual learning approaches on real-world visual classification benchmarks (CIFAR-100, TinyImagenet) and MNIST benchmarks, while being significantly more sample-efficient and faster than prior meta-learning methods like MER.",
    "Methodology": "The proposed approach builds upon a base algorithm called Continual-MAML (C-MAML). C-MAML aims to optimize an Online-aware Meta-Learning (OML) objective online, adapted to optimize model parameters, and utilizes a replay buffer. The meta-objective used is shown to be equivalent to the A-GEM objective, which is asymmetric and focuses on aligning the gradients of the current task with the average gradient of previously seen tasks, leading to faster learning compared to objectives that align all pairwise task gradients. La-MAML extends C-MAML by incorporating a set of learnable per-parameter learning rates (LRs) used in the inner updates. These LRs are modulated based on the alignment (or interference) between gradients of old and new tasks, with negative LR gradients increasing LR magnitude (for alignment) and positive gradients decreasing it (for interference). The network weights and LRs are updated asynchronously: LRs are updated first based on their hypergradients, and then weights are updated using these new, clipped-to-positive LRs to ensure conservative updates and mitigate catastrophic forgetting. A replay buffer is maintained using reservoir sampling to store past samples, which are used to form meta-batches alongside current task data for meta-loss evaluation.",
    "Experimental Setup": "Experiments were conducted on both toy and real-world visual classification benchmarks. Toy benchmarks include MNIST Rotations, MNIST Permutations (20 tasks each with 1000 samples), and a more complex Many Permutations (100 tasks, 200 samples/task), all tested in a low-data regime (200-500 samples). Real-world benchmarks are CIFAR-100 (20 disjoint 5-way classification tasks) and TinyImagenet-200 (40 5-way classification tasks). Experiments were run in two settings: Single-Pass (Efficient Lifelong Learning - LLL), where data is processed once, and Multiple-Pass (standard CL), allowing up to 10 epochs of training. A modest replay buffer size (200 for MNIST Rotations/Permutations, 500 for Many Permutations, 200 for CIFAR-100, and 400 for TinyImagenet) was used. Performance metrics included Retained Accuracy (RA) and Backward-Transfer and Interference (BTI). Gradient alignment (cosine similarity) was also measured. Baselines for comparison included Online, EWC, GEM, MER, IID (upper bound), ER, iCARL, A-GEM, Meta-BGD, C-MAML, Sync-La-MAML, and La-ER. Architectures used were a fully-connected neural network for MNIST and CNNs for CIFAR/TinyImagenet. The SGD optimizer was used across all experiments, with gradient clipping at a norm of 2.0. Hyperparameters were tuned via grid-search.",
    "Limitations": "The objective's gradient alignment term introduces some bias, preventing exact convergence to the minimizer of losses across all tasks, though this is deemed acceptable in the continual learning regime. The reliance on a replay buffer for prior data, even with improved gradient alignment, may lead to concept drift over extended periods. The algorithm's performance can be influenced by biases in the sampling strategy used to populate the replay buffer. While La-MAML addresses the slowness of general meta-learning, learnable per-parameter LRs still incur a memory overhead proportional to the network parameters. A strong comparison baseline, Meta-BGD, was found to be highly sensitive to hyperparameters and required significant computational resources due to Monte Carlo iterations.",
    "Future Research Directions": "Future work should focus on analyzing and developing improved optimizers specifically tailored for continual learning. Standard optimizers like Adam are primarily designed for faster convergence in stationary supervised learning settings, which may not be optimal for non-stationary CL environments. Another promising direction is to further explore the connections between La-MAML's adaptive LRs and meta-descent. This could lead to more stable training procedures for meta-learning algorithms that are capable of automatically adjusting hyper-parameters adaptively based on real-time training dynamics.",
    "Experiment Code": "File Path: model/meta/learner.py\nContent:\nimport math\nimport os\nimport sys\nimport traceback\nimport numpy as np\nimport ipdb\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass Learner(nn.Module):\n\n    def __init__(self, config, args = None):\n        \"\"\"\n\n        :param config: network config file, type:list of (string, list)\n        :param imgc: 1 or 3\n        :param imgsz:  28 or 84\n        \"\"\"\n        super(Learner, self).__init__()\n\n        self.config = config\n        self.tf_counter = 0\n        self.args = args\n\n        # this dict contains all tensors needed to be optimized\n        self.vars = nn.ParameterList()\n        # running_mean and running_var\n        self.vars_bn = nn.ParameterList()\n\n        self.names = []\n\n        for i, (name, param, extra_name) in enumerate(self.config):\n            if name is 'conv2d':\n                # [ch_out, ch_in, kernelsz, kernelsz]                \n                if(self.args.xav_init):\n                    w = nn.Parameter(torch.ones(*param[:4]))\n                    b = nn.Parameter(torch.zeros(param[0]))\n                    torch.nn.init.xavier_normal_(w.data)\n                    b.data.normal_(0, math.sqrt(2)/math.sqrt(1+9*b.data.shape[0]))\n                    self.vars.append(w)\n                    self.vars.append(b)\n                else:\n                    w = nn.Parameter(torch.ones(*param[:4]))\n                    # gain=1 according to cbfin's implementation\n                    torch.nn.init.kaiming_normal_(w)\n                    self.vars.append(w)\n                    # [ch_out]\n                    self.vars.append(nn.Parameter(torch.zeros(param[0])))\n\n            elif name is 'convt2d':\n                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n                w = nn.Parameter(torch.ones(*param[:4]))\n                # gain=1 according to cbfin's implementation\n                torch.nn.init.kaiming_normal_(w)\n                self.vars.append(w)\n                # [ch_in, ch_out]\n                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n\n            elif name is 'linear':\n                # layer += 1\n                if(self.args.xav_init):\n                    w = nn.Parameter(torch.ones(*param))\n                    # b = nn.Parameter(torch.zeros(param[0]))\n                    torch.nn.init.xavier_normal_(w.data)\n                    # b.data.normal_(0, math.sqrt(2)/math.sqrt(1+9*b.data.shape[0]))\n                    self.vars.append(w)\n                    # self.vars.append(b)\n                else:     \n                    # [ch_out, ch_in]\n                    w = nn.Parameter(torch.ones(*param))\n                    # gain=1 according to cbfinn's implementation\n                    torch.nn.init.kaiming_normal_(w)\n                    self.vars.append(w)\n                # [ch_out]\n                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n\n            elif name is 'cat':\n                pass\n            elif name is 'cat_start':\n                pass\n            elif name is \"rep\":\n                pass\n            elif name in [\"residual3\", \"residual5\", \"in\"]:\n                pass\n            elif name is 'bn':\n                # [ch_out]\n                w = nn.Parameter(torch.ones(param[0]))\n                self.vars.append(w)\n                # [ch_out]\n                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n\n                # must set requires_grad=False\n                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n                self.vars_bn.extend([running_mean, running_var])\n\n\n            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n                continue\n            else:\n                raise NotImplementedError\n\n    def extra_repr(self):\n\n        info = ''\n\n        for name, param, extra_name in self.config:\n            if name is 'conv2d':\n                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)' \\\n                      % (param[1], param[0], param[2], param[3], param[4], param[5],)\n                info += tmp + '\\n'\n\n            elif name is 'convt2d':\n                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)' \\\n                      % (param[0], param[1], param[2], param[3], param[4], param[5],)\n                info += tmp + '\\n'\n\n            elif name is 'linear':\n                tmp = 'linear:(in:%d, out:%d)' % (param[1], param[0])\n                info += tmp + '\\n'\n\n            elif name is 'leakyrelu':\n                tmp = 'leakyrelu:(slope:%f)' % (param[0])\n                info += tmp + '\\n'\n\n            elif name is 'cat':\n                tmp = 'cat'\n                info += tmp + \"\\n\"\n            elif name is 'cat_start':\n                tmp = 'cat_start'\n                info += tmp + \"\\n\"\n\n            elif name is 'rep':\n                tmp = 'rep'\n                info += tmp + \"\\n\"\n\n\n            elif name is 'avg_pool2d':\n                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)' % (param[0], param[1], param[2])\n                info += tmp + '\\n'\n            elif name is 'max_pool2d':\n                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)' % (param[0], param[1], param[2])\n                info += tmp + '\\n'\n            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n                tmp = name + ':' + str(tuple(param))\n                info += tmp + '\\n'\n            else:\n                raise NotImplementedError\n\n        return info\n\n    def forward(self, x, vars=None, bn_training=False, feature=False):\n        \"\"\"\n        This function can be called by finetunning, however, in finetunning, we dont wish to update\n        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n        :param x: [b, 1, 28, 28]\n        :param vars:\n        :param bn_training: set False to not update\n        :return: x, loss, likelihood, kld\n        \"\"\"\n\n        cat_var = False\n        cat_list = []\n\n        if vars is None:\n            vars = self.vars\n\n        idx = 0\n        bn_idx = 0\n\n        try:\n\n            for (name, param, extra_name) in self.config:\n                # assert(name == \"conv2d\")\n                if name == 'conv2d':\n                    w, b = vars[idx], vars[idx + 1]\n                    x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n                    idx += 2\n\n                    # print(name, param, '\\tout:', x.shape)\n                elif name == 'convt2d':\n                    w, b = vars[idx], vars[idx + 1]\n                    x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n                    idx += 2\n\n\n                elif name == 'linear':\n\n                    # ipdb.set_trace()\n                    if extra_name == 'cosine':\n                        w = F.normalize(vars[idx])\n                        x = F.normalize(x)\n                        x = F.linear(x, w)\n                        idx += 1\n                    else:\n                        w, b = vars[idx], vars[idx + 1]\n                        x = F.linear(x, w, b)\n                        idx += 2\n\n                    if cat_var:\n                        cat_list.append(x)\n\n                elif name == 'rep':\n                    # print('rep')\n                    # print(x.shape)\n                    if feature:\n                        return x\n\n                elif name == \"cat_start\":\n                    cat_var = True\n                    cat_list = []\n\n                elif name == \"cat\":\n                    cat_var = False\n                    x = torch.cat(cat_list, dim=1)\n\n                elif name == 'bn':\n                    w, b = vars[idx], vars[idx + 1]\n                    running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx + 1]\n                    x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n                    idx += 2\n                    bn_idx += 2\n                elif name == 'flatten':\n                    # print('flatten')\n                    # print(x.shape)\n\n                    x = x.view(x.size(0), -1)\n\n                elif name == 'reshape':\n                    # [b, 8] => [b, 2, 2, 2]\n                    x = x.view(x.size(0), *param)\n                elif name == 'relu':\n                    x = F.relu(x, inplace=param[0])\n                elif name == 'leakyrelu':\n                    x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n                elif name == 'tanh':\n                    x = F.tanh(x)\n                elif name == 'sigmoid':\n                    x = torch.sigmoid(x)\n                elif name == 'upsample':\n                    x = F.upsample_nearest(x, scale_factor=param[0])\n                elif name == 'max_pool2d':\n                    x = F.max_pool2d(x, param[0], param[1], param[2])\n                elif name == 'avg_pool2d':\n                    x = F.avg_pool2d(x, param[0], param[1], param[2])\n\n                else:\n                    print(name)\n                    raise NotImplementedError\n\n        except:\n            traceback.print_exc(file=sys.stdout)\n            ipdb.set_trace()\n\n        # make sure variable is used properly\n        assert idx == len(vars)\n        assert bn_idx == len(self.vars_bn)\n\n        return x\n\n\n    def zero_grad(self, vars=None):\n        \"\"\"\n\n        :param vars:\n        :return:\n        \"\"\"\n        with torch.no_grad():\n            if vars is None:\n                for p in self.vars:\n                    if p.grad is not None:\n                        p.grad.zero_()\n            else:\n                for p in vars:\n                    if p.grad is not None:\n                        p.grad.zero_()\n\n    def define_task_lr_params(self, alpha_init=1e-3): \n        # Setup learning parameters\n        self.alpha_lr = nn.ParameterList([])\n\n        self.lr_name = []\n        for n, p in self.named_parameters():\n            self.lr_name.append(n)\n\n        for p in self.parameters():\n            self.alpha_lr.append(nn.Parameter(alpha_init * torch.ones(p.shape, requires_grad=True)))                                           \n\n    def parameters(self):\n        \"\"\"\n        override this function since initial parameters will return with a generator.\n        :return:\n        \"\"\"\n        return self.vars\n\nFile Path: model/meta/modelfactory.py\nContent:\nimport ipdb\n\nclass ModelFactory():\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_model(model_type, sizes, dataset='mnist', args=None):\n\n        net_list = []\n        if \"mnist\" in dataset:\n            if model_type==\"linear\":\n                for i in range(0, len(sizes) - 1):\n                    net_list.append(('linear', [sizes[i+1], sizes[i]], ''))\n                    if i < (len(sizes) - 2):\n                        net_list.append(('relu', [True], ''))\n                    if i == (len(sizes) - 2):\n                        net_list.append(('rep', [], ''))\n                return net_list\n\n        elif dataset == \"tinyimagenet\":\n\n            if model_type == 'pc_cnn':\n                channels = 160\n                return [\n                    ('conv2d', [channels, 3, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('flatten', [], ''),\n                    ('rep', [], ''),\n\n                    ('linear', [640, 16 * channels], ''),\n                    ('relu', [True], ''),\n\n                    ('linear', [640, 640], ''),\n                    ('relu', [True], ''),\n                    ('linear', [sizes[-1], 640], '')\n                ]\n\n        elif dataset == \"cifar100\":\n\n\n            if model_type == 'pc_cnn':\n                channels = 160\n                return [\n                    ('conv2d', [channels, 3, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n                    \n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('flatten', [], ''),\n                    ('rep', [], ''),\n\n                    ('linear', [320, 16 * channels], ''),\n                    ('relu', [True], ''),\n\n                    ('linear', [320, 320], ''),\n                    ('relu', [True], ''),\n                    ('linear', [sizes[-1], 320], '')\n                ]\n\n        else:\n            print(\"Unsupported model; either implement the model in model/ModelFactory or choose a different model\")\n            assert (False)\n\nFile Path: model/lamaml_base.py\nContent:\nimport random\nfrom random import shuffle\nimport numpy as np\nimport ipdb\nimport math\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport model.meta.learner as Learner\nimport model.meta.modelfactory as mf\nfrom scipy.stats import pearsonr\nimport datetime\n\nclass BaseNet(torch.nn.Module):\n\n    def __init__(self,\n                 n_inputs,\n                 n_outputs,\n                 n_tasks,           \n                 args):\n        super(BaseNet, self).__init__()\n\n        self.args = args\n        nl, nh = args.n_layers, args.n_hiddens\n\n        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],\n                                                dataset = args.dataset, args=args)\n\n        self.net = Learner.Learner(config, args)\n\n        # define the lr params\n        self.net.define_task_lr_params(alpha_init = args.alpha_init)\n\n        self.opt_wt = torch.optim.SGD(list(self.net.parameters()), lr=args.opt_wt)     \n        self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr) \n\n        self.epoch = 0\n        # allocate buffer\n        self.M = []        \n        self.M_new = []\n        self.age = 0\n\n        # setup losses\n        self.loss = torch.nn.CrossEntropyLoss()\n        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))\n        self.glances = args.glances\n        self.pass_itr = 0\n        self.real_epoch = 0\n\n        self.current_task = 0\n        self.memories = args.memories\n        self.batchSize = int(args.replay_batch_size)\n\n        self.cuda = args.cuda\n        if self.cuda:\n            self.net = self.net.cuda()\n\n        self.n_outputs = n_outputs\n\n    def push_to_mem(self, batch_x, batch_y, t):\n        \"\"\"\n        Reservoir sampling to push subsampled stream\n        of data points to replay/memory buffer\n        \"\"\"\n\n        if(self.real_epoch > 0 or self.pass_itr>0):\n            return\n        batch_x = batch_x.cpu()\n        batch_y = batch_y.cpu()              \n        t = t.cpu()\n\n        for i in range(batch_x.shape[0]):\n            self.age += 1\n            if len(self.M_new) < self.memories:\n                self.M_new.append([batch_x[i], batch_y[i], t])\n            else:\n                p = random.randint(0,self.age)  \n                if p < self.memories:\n                    self.M_new[p] = [batch_x[i], batch_y[i], t]\n\n\n    def getBatch(self, x, y, t, batch_size=None):\n        \"\"\"\n        Given the new data points, create a batch of old + new data, \n        where old data is sampled from the memory buffer\n        \"\"\"\n\n        if(x is not None):\n            mxi = np.array(x)\n            myi = np.array(y)\n            mti = np.ones(x.shape[0], dtype=int)*t        \n        else:\n            mxi = np.empty( shape=(0, 0) )\n            myi = np.empty( shape=(0, 0) )\n            mti = np.empty( shape=(0, 0) )    \n\n        bxs = []\n        bys = []\n        bts = []\n\n        if self.args.use_old_task_memory and t>0:\n            MEM = self.M\n        else:\n            MEM = self.M_new\n\n        batch_size = self.batchSize if batch_size is None else batch_size\n\n        if len(MEM) > 0:\n            order = [i for i in range(0,len(MEM))]\n            osize = min(batch_size,len(MEM))\n            for j in range(0,osize):\n                shuffle(order)\n                k = order[j]\n                x,y,t = MEM[k]\n\n                xi = np.array(x)\n                yi = np.array(y)\n                ti = np.array(t)\n                bxs.append(xi)\n                bys.append(yi)\n                bts.append(ti)\n\n        for j in range(len(myi)):\n            bxs.append(mxi[j])\n            bys.append(myi[j])\n            bts.append(mti[j])\n\n        bxs = Variable(torch.from_numpy(np.array(bxs))).float() \n        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)\n        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)\n        \n        # handle gpus if specified\n        if self.cuda:\n            bxs = bxs.cuda()\n            bys = bys.cuda()\n            bts = bts.cuda()\n\n        return bxs,bys,bts\n\n    def compute_offsets(self, task):\n        # mapping from classes [1-100] to their idx within a task\n        offset1 = task * self.nc_per_task\n        offset2 = (task + 1) * self.nc_per_task\n        return int(offset1), int(offset2)\n\n    def zero_grads(self):\n        if self.args.learn_lr:\n            self.opt_lr.zero_grad()\n        self.opt_wt.zero_grad()\n        self.net.zero_grad()\n        self.net.alpha_lr.zero_grad()\nFile Path: model/lamaml_cifar.py\nContent:\nimport random\nimport numpy as np\nimport ipdb\nimport math\nimport torch\nimport torch.nn as nn\nfrom model.lamaml_base import *\n\nclass Net(BaseNet):\n\n    def __init__(self,\n                 n_inputs,\n                 n_outputs,\n                 n_tasks,           \n                 args):\n        super(Net, self).__init__(n_inputs,\n                                 n_outputs,\n                                 n_tasks,           \n                                 args)\n        self.nc_per_task = n_outputs / n_tasks\n\n    def take_loss(self, t, logits, y):\n        # compute loss on data from a single task\n        offset1, offset2 = self.compute_offsets(t)\n        loss = self.loss(logits[:, offset1:offset2], y-offset1)\n\n        return loss\n\n    def take_multitask_loss(self, bt, t, logits, y):\n        # compute loss on data from a multiple tasks\n        # separate from take_loss() since the output positions for each task's\n        # logit vector are different and we nly want to compute loss on the relevant positions\n        # since this is a task incremental setting\n\n        loss = 0.0\n\n        for i, ti in enumerate(bt):\n            offset1, offset2 = self.compute_offsets(ti)\n            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0)-offset1)\n        return loss/len(bt)\n\n\n    def forward(self, x, t):\n        output = self.net.forward(x)\n        # make sure we predict classes within the current task\n        offset1, offset2 = self.compute_offsets(t)\n        if offset1 > 0:\n            output[:, :offset1].data.fill_(-10e10)\n        if offset2 < self.n_outputs:\n            output[:, int(offset2):self.n_outputs].data.fill_(-10e10)\n        return output\n\n    def meta_loss(self, x, fast_weights, y, bt, t):\n        \"\"\"\n        differentiate the loss through the network updates wrt alpha\n        \"\"\"\n\n        offset1, offset2 = self.compute_offsets(t)\n\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss_q = self.take_multitask_loss(bt, t, logits, y)\n\n        return loss_q, logits\n\n    def inner_update(self, x, fast_weights, y, t):\n        \"\"\"\n        Update the fast weights using the current samples and return the updated fast\n        \"\"\"\n\n        offset1, offset2 = self.compute_offsets(t)            \n\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss = self.take_loss(t, logits, y)\n\n        if fast_weights is None:\n            fast_weights = self.net.parameters()\n\n        # NOTE if we want higher order grads to be allowed, change create_graph=False to True\n        graph_required = self.args.second_order\n        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))\n\n        for i in range(len(grads)):\n            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)\n\n        fast_weights = list(\n            map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))\n\n        return fast_weights\n\n\n    def observe(self, x, y, t):\n        self.net.train() \n        for pass_itr in range(self.glances):\n            self.pass_itr = pass_itr\n            perm = torch.randperm(x.size(0))\n            x = x[perm]\n            y = y[perm]\n\n            self.epoch += 1\n            self.zero_grads()\n\n            if t != self.current_task:\n                self.M = self.M_new.copy()\n                self.current_task = t\n\n            batch_sz = x.shape[0]\n            n_batches = self.args.cifar_batches\n            rough_sz = math.ceil(batch_sz/n_batches)\n            fast_weights = None\n            meta_losses = [0 for _ in range(n_batches)]\n\n            # get a batch by augmented incming data with old task data, used for \n            # computing meta-loss\n            bx, by, bt = self.getBatch(x.cpu().numpy(), y.cpu().numpy(), t)             \n\n            for i in range(n_batches):\n\n                batch_x = x[i*rough_sz : (i+1)*rough_sz]\n                batch_y = y[i*rough_sz : (i+1)*rough_sz]\n\n                # assuming labels for inner update are from the same \n                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)   \n                # only sample and push to replay buffer once for each task's stream\n                # instead of pushing every epoch     \n                if(self.real_epoch == 0):\n                    self.push_to_mem(batch_x, batch_y, torch.tensor(t))\n                meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) \n                \n                meta_losses[i] += meta_loss\n\n            # Taking the meta gradient step (will update the learning rates)\n            self.zero_grads()\n\n            meta_loss = sum(meta_losses)/len(meta_losses)            \n            meta_loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)\n            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)\n            if self.args.learn_lr:\n                self.opt_lr.step()\n\n            # if sync-update is being carried out (as in sync-maml) then update the weights using the optimiser\n            # otherwise update the weights with sgd using updated LRs as step sizes\n            if(self.args.sync_update):\n                self.opt_wt.step()\n            else:            \n                for i,p in enumerate(self.net.parameters()):          \n                    # using relu on updated LRs to avoid negative values           \n                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])            \n            self.net.zero_grad()\n            self.net.alpha_lr.zero_grad()\n\n        return meta_loss.item()",
    "Experiment Result": "Model Architecture: \n- Architecture type (`--arch`): `linear` or `pc_cnn`.\n- Number of hidden layers (`--n_layers`): Default 2.\n- Number of hidden neurons at each layer (`--n_hiddens`): Default 100.\n- Initialization (`--xav_init`): Boolean flag, default False (implies Kaiming normal initialization).\n\nOptimization & Learning Rates:\n- Learning rate for LRs (`--opt_lr`): Default 1e-1.\n- Learning rate for weights (`--opt_wt`): Default 1e-1.\n- Initial value for learnable LRs (`--alpha_init`): Default 1e-3.\n- Learnable LR updates (`--learn_lr`): Boolean flag, default False.\n- Synchronous updates for LRs and weights (`--sync_update`): Boolean flag, default False (implies asynchronous updates).\n- Gradient clip norm (`--grad_clip_norm`): Default 2.0.\n- Second-order MAML updates (`--second_order`): Boolean flag, default False.\n\nTraining Process:\n- Number of epochs per task (`--n_epochs`): Default 1.\n- Number of glances (`--glances`): Number of times the model trains over a set of samples in a single pass setting, default 1.\n- Batch size for current task data (`--batch_size`): Default 1.\n- Number of batches in inner trajectory (`--cifar_batches`): Default 3.\n\nReplay Buffer & Memory:\n- Total memories stored (`--memories`): Default 5120.\n- Batch size for experience replay (`--replay_batch_size`): Default 20.\n- Use old task memory (`--use_old_task_memory`): Boolean flag, default False.\n\nDataset & Data Loading:\n- Dataset (`--dataset`): Examples include `cifar100`, `tinyimagenet`, `mnist_rotations` (default).\n- Data loader (`--loader`): Default `task_incremental_loader`. Other options include `class_incremental_loader`.\n- Data path (`--data_path`): Default `data/`.\n- Validation split (`--validation`): Default 0.0.\n- Class order (`--class_order`): Default `old`. Other options: `random`, `chrono`, `super`.\n- Increment (`--increment`): Number of classes to increment by in class incremental loader, default 5.\n- Number of workers preprocessing data (`--workers`): Default 3.\n\nGeneral Settings:\n- CUDA (`--cuda`): Boolean flag, default False.\n- Random seed (`--seed`): Default 0.\n- Log directory (`--log_dir`): Default `logs/`.\n- Log frequency (`--log_every`): Default 1000 minibatches.\n- Calculate test accuracy (`--calc_test_accuracy`): Boolean flag, default False.\n- Test batch size (`--test_batch_size`): Default 100000."
}{
    "Title": "Continuous Meta-Learning without Tasks",
    "Main Contributions": "The paper addresses the challenge of applying meta-learning algorithms in settings where task segmentation is unavailable, such as continual online learning with unsegmented time series data. The main contribution is Meta-Learning via Online Changepoint Analysis (MOCA), an algorithmic framework that augments generic meta-learning algorithms with a differentiable Bayesian changepoint detection scheme. MOCA enables both training and testing directly on time series data without requiring pre-segmented tasks, allowing the system to learn both a rapidly adaptive predictive model and an effective changepoint detection algorithm optimized to work together. The key finding is that MOCA achieves predictive performance comparable to standard task-segmented meta-learning settings across various regression and classification benchmarks, while also providing interpretable estimates of task switches.",
    "Methodology": "MOCA extends Bayesian Online Changepoint Detection (BOCPD) to conditional density estimation, using a base meta-learning algorithm as the underlying predictive model (UPM). It recursively maintains and updates a belief distribution over run lengths (time since the last changepoint) using Bayesian filtering rules. Upon observing new data (xt, yt), MOCA updates the belief based on the input likelihood pθ(xt|ηt−1[rt]) and the conditional predictive likelihood pθ(yt|xt,ηt−1[rt]). The run length belief is then propagated forward in time, assuming a fixed hazard rate for task switches. The entire process is differentiable, allowing backpropagation to optimize the parameters of the base meta-learning algorithm. MOCA was instantiated and evaluated with an LSTM-based meta-learner, ALPaCA (a Bayesian meta-learning approach for regression), and PCOC (Probabilistic Clustering for Online Classification), a novel Bayesian meta-learning algorithm for classification based on Gaussian discriminant analysis in a learned feature space.",
    "Experimental Setup": "MOCA's performance was evaluated across five problem settings: three nonlinear meta-regression benchmarks (Sinusoid Regression, Switching Wheel Bandit, NBA Player Movement prediction) and two meta-image-classification benchmarks (Rainbow MNIST, miniImageNet). Specific benchmarks include a switching sinusoid problem (adapted from [10]), an extended wheel bandit problem with changing reward functions [37], a real-world NBA player movement prediction task, the Rainbow MNIST dataset [11] with various transformations, and the miniImageNet dataset [44] grouped into five super-classes. Baselines included fixed-length Sliding Window models (n=5, 10, 50), a 'Train on Everything' model (no online adaptation), a 'Condition on Everything' model (for LSTM, continually updates a single set of posterior statistics), and an 'Oracle' model (same base meta-learner with exact task segmentation at train and test time). Performance was measured by Negative Log Likelihood (NLL) for regression/classification and regret for the bandit problem. Training utilized the Adam optimizer with specific learning rates, batch sizes, batch lengths, and iteration counts tailored for each experiment, including data augmentation for miniImageNet.",
    "Limitations": "The current MOCA framework assumes tasks are sampled independently and identically distributed (i.i.d.), limiting its ability to explicitly leverage knowledge re-use for reoccurring tasks or tasks with temporal dynamics. In regression, models like ALPaCA primarily model the conditional density p(y|x) and assume p(x) is independent of the task, which may prevent changepoint detection based solely on changes in the input distribution before a label is observed. MOCA's computational requirements grow linearly with the batch length (T) during training, as it maintains posterior statistics for every possible run length, necessitating batch training which can artificially increase the observed hazard rate. Also, identifying changepoints can be difficult and temporally delayed in problems like the wheel bandit, particularly at high hazard rates. The paper also acknowledges that MOCA may amplify risks associated with meta-learning, such as privacy implications in few-shot learning and potential for increased bias or unfairness in individualized adaptive learning, highlighting the need for future research in these ethical domains.",
    "Future Research Directions": "Future work could extend MOCA to handle domains where tasks reoccur or exhibit temporal dynamics, moving beyond the i.i.d. task assumption to improve data efficiency by re-using information from previous tasks. Exploring tasks with associated dynamics, as opposed to instantaneous switches, represents another promising direction. For the switching bandit problem, more efficient exploration methods, particularly when paired with changepoint detection, are still an active research topic. Additionally, developing differentiable pruning methods for the Bayesian Online Changepoint Detection component could alleviate the linear growth of computational requirements during training, making the framework more scalable for longer time series. Finally, extensive research is required to ensure that the adaptive learning enabled by algorithms like MOCA does not lead to unfair outcomes or amplify biases, considering the unique challenges of analyzing fairness in individualized decision-making rules.",
    "Experiment Code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport time\nimport math\nfrom copy import deepcopy\n\n# From metacpd/main/utils.py\nclass Flatten(nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n    \ndef conv_block(in_channels,out_channels):\n    return nn.Sequential(nn.Conv2d(in_channels,out_channels,3,padding=1),\n                        nn.BatchNorm2d(out_channels),\n                        nn.ReLU(),\n                        nn.MaxPool2d(2)\n                        )\n\ndef final_conv_block(in_channels,out_channels):\n    return nn.Sequential(nn.Conv2d(in_channels,out_channels,3,padding=1),\n#                         nn.BatchNorm2d(out_channels),\n                        nn.MaxPool2d(2)\n                        )\n\ndef mask_nlls(y,likelihoods):\n    \"\"\"\n    y: onehot labels: shape (..., n_classes)\n    likelihood: per class: shape (..., n_classes)\n    \"\"\"\n    # mask with y\n    return torch.sum(y * likelihoods,dim=-1)\n\ndef compute_acc(y,nlls):\n    # compute accuracy using nlls\n    pred_class = torch.argmin(nlls,-1,keepdim=True)\n    \n    acc = y.gather(-1, pred_class).squeeze(-1)\n    return acc\n\ndef get_prgx(config,horizon,batch_size,switch_times=None):\n\n    model = config['model.model']\n    sliding_window_len = config['data.window_length']\n\n    if model == 'main' or model == 'conv_net':\n        return None, None\n\n    prgx = []\n    task_sup = []\n    last_switch = np.zeros(batch_size,dtype=int)\n\n    for t in range(horizon):\n        prgx_t = np.zeros((batch_size,t+1))\n        task_supervision = np.zeros(batch_size)\n        for i in range(batch_size):\n            if model == 'sliding_window':\n                prgx_t[i,max(t-sliding_window_len,0)] = 1\n            elif model == 'no_task_change':\n                prgx_t[i,t] = 1\n            elif model == 'oracle':\n                if switch_times[i,t] > 0.5:\n                    last_switch[i] = t\n                    if config['train.task_supervision'] is not None:\n                        if np.random.rand() < config['train.task_supervision']:\n                            task_supervision[i] = 1.\n                            epsilon = 1e-5\n                            prgx_t[i,:] = np.ones(t+1)*epsilon\n                            prgx_t[i,last_switch[i]] = 1. - epsilon*t\n                            \n                            if config['train.oracle_hazard'] is not None:\n                                raise NotImplementedError\n                                \n                if config['train.task_supervision'] is None:\n                    if config['train.oracle_hazard'] is not None:\n                        if last_switch[i] != 0:\n                            prgx_t[i,0] = config['train.oracle_hazard']\n                            prgx_t[i,last_switch[i]] = 1. - config['train.oracle_hazard']\n                        else:\n                            prgx_t[i,last_switch[i]] = 1.\n                    else:\n                        prgx_t[i,last_switch[i]] = 1.\n                \n            else:\n                raise ValueError('make sure specified model is implemented')\n\n\n        prgx_t = torch.tensor(prgx_t).float()\n        task_supervision = torch.tensor(task_supervision).float()\n\n\n        if config['data.cuda'] >= 0:\n            prgx_t = prgx_t.cuda(config['data.cuda'])\n            task_supervision = task_supervision.cuda(config['data.cuda'])\n\n        \n        prgx.append(prgx_t)\n        task_sup.append(task_supervision)\n\n    if config['train.task_supervision'] is None:\n        return prgx, None\n    else:\n        return prgx, task_sup\n\n# From metacpd/main/encoders.py\ndef get_encoder(config):\n    hid_dim = config['model.hid_dim']\n    x_dim = config['model.x_dim']\n    phi_dim = config['model.phi_dim']\n    # REGRESSION\n\n    if config['data.dataset'] == 'Sinusoid':\n        activation = nn.Tanh()\n        encoder = nn.Sequential(\n            nn.Linear(x_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, phi_dim),\n            activation\n        )\n    elif config['data.dataset'] == 'NoiseSinusoid':\n        activation = nn.Tanh()\n        encoder = nn.Sequential(\n            nn.Linear(x_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, phi_dim),\n            activation\n        )\n        \n    # CLASSIFICATION\n\n    elif config['data.dataset'] in ['RainbowMNIST']:\n        encoder = nn.Sequential(\n            conv_block(3, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            final_conv_block(hid_dim, hid_dim),\n            Flatten()\n        )\n    elif config['data.dataset'] == 'MiniImageNet':\n        encoder = nn.Sequential(\n            conv_block(3, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            final_conv_block(hid_dim, hid_dim),\n            Flatten()\n        )\n\n    elif config['data.dataset'] == 'PermutedMNIST':\n        encoder = nn.Sequential(\n            conv_block(1, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            final_conv_block(hid_dim, hid_dim),\n            Flatten()\n        )\n\n    else:\n        raise ValueError(\"data.dataset not understood\")\n\n    return encoder\n\n# From metacpd/main/moca.py\nclass MOCA(nn.Module):\n    \"\"\"\n    Wraps an underlying MetaLearning algorithm to allow training on timeseries of\n    sequential examples with discrete task switches that are unlabeled.\n    \"\"\"\n\n    def __init__(self, meta_learning_alg, config):\n        super().__init__()\n\n        self.config = deepcopy(config)\n        self.x_dim = config['model.x_dim']\n        self.y_dim = config['model.y_dim']\n\n        self.classification = config['model.classification']\n\n        self.meta_learning_alg = meta_learning_alg\n\n        # hazard rate:\n        hazard_logit = np.log( config['data.hazard'] / (1 - config['data.hazard'] ) )\n        self.hazard_logit = nn.Parameter(torch.from_numpy(np.array([hazard_logit])), requires_grad=config['train.learnable_hazard'])\n\n        # initial log_prx:\n        self.init_log_prgx = nn.Parameter(torch.zeros([1,1]), requires_grad=False)\n\n\n    def nll(self, log_pi, log_prgx):\n        \"\"\"\n            log_pi: shape(batch_size x t x ...)       log p(new data | x, r=i, data so far) for all i = 0, ..., t\n            log_prgx: shape (batch_size x t x ...)    log p(r=i | data so far) for all i = 0, ..., t\n        \"\"\"\n\n        if len(log_pi.shape) == 3:\n            return -torch.logsumexp(log_pi + log_prgx.unsqueeze(-1), dim=1)\n\n        return -torch.logsumexp(log_pi + log_prgx, dim=1)\n\n    def log_p_r_given_x(self,log_prx):\n        \"\"\"\n            computes log p(r|x)\n\n            inputs: log_prx: (batch_size, t+1), log p(r, x) for each r in 0, ..., t\n                    log_prx: (batch_size, t+1), log p(r | x) for each r in 0, ..., t\n        \"\"\"\n        return nn.functional.log_softmax(log_prx,dim=1)\n\n    @property\n    def log_hazard(self):\n        \"\"\"\n        log p( task_switch )\n        \"\"\"\n        return torch.log(torch.sigmoid(self.hazard_logit))\n\n    @property\n    def log_1m_hazard(self):\n        \"\"\"\n        log (1 - p(task_switch))\n        \"\"\"\n        return torch.log(1-torch.sigmoid(self.hazard_logit))\n\n    @property\n    def hazard(self):\n        return torch.sigmoid(self.hazard_logit)\n\n    def forward(self,x_mat,y_mat,prgx=None, task_supervision=None, return_timing=False):\n        \"\"\"\n        Takes in x,y batches; loops over horizon to recursively compute posteriors\n        Inputs:\n        - x_mat; shape = batch size x horizon x x_dim\n        - y_mat; shape = batch size x horizon x y_dim\n        \"\"\"\n        batch_size = x_mat.shape[0]\n        test_horizon = x_mat.shape[1]\n\n\n        posterior_params_list = []\n        log_prgx_list = []\n        nll_list = []\n\n        # define initial params and append to list\n        # we add a batch dimension and a time dimension\n        prior_params = tuple( p[None,None,...] for p in self.meta_learning_alg.prior_params() )\n\n\n        posterior_params = prior_params\n        log_prgx = self.init_log_prgx # p(r, all data so far)\n\n        posterior_params_list.append(posterior_params)\n\n\n        # start at time t+1\n        \n        time_array = []\n        \n        for i in range(test_horizon):\n            # grab y, phi:\n            \n            if return_timing:\n                torch.cuda.synchronize()\n                start_time = time.perf_counter()\n            \n            x = x_mat[:,i,:]\n            y = y_mat[:,i,:]\n\n\n            # compute log p(y|x,hist) for all possible run lengths (shape: (batch_size, t+1))\n            # and return updated params incorporating new point\n            # log_pi_t = log p(y|x,r,hist) for all r = [0,...,i]\n\n            # if classification, log_pi_t == p(y,x|eta) for all y (batchsize, i+1, y_dim)\n            # if regression, log_pi_t == p(y|x,eta)\n            log_pi_t, updated_posterior_params = self.meta_learning_alg.log_predictive_prob(x[:,None,:], y[:,None,:], posterior_params, update_params=True)\n            if self.classification:\n                log_pygx =  nn.functional.log_softmax(log_pi_t, dim=-1) # normalize to get p(y | x) # (batchsize, i+1, y_dim)\n\n                # update p(r_t) given just the x value\n                log_p_newx_gr = torch.logsumexp(log_pi_t, dim=-1) # sum over all y values, shape (batch_size, i+1)\n                log_prgx = log_prgx + log_p_newx_gr # (batch_size, i+1) # log p ( r_{i} \\mid x_{0,i}, y_{0,i-1} )\n                log_prgx = torch.log_softmax(log_prgx, dim=1) # normalizing over runlengths\n\n            else:\n                log_pygx = log_pi_t\n\n            if prgx is not None:\n                if task_supervision is not None:\n                    override_log_prgx = torch.log(prgx[i]) + torch.log(task_supervision[i].unsqueeze(-1))\n                    masked_log_prgx = log_prgx + torch.log(1-task_supervision[i].unsqueeze(-1))\n                    cat_log_prgx = torch.cat((override_log_prgx.unsqueeze(-1), masked_log_prgx.unsqueeze(-1)),dim=-1)\n                    log_prgx = torch.logsumexp(cat_log_prgx,dim=-1)\n                else:\n                    log_prgx = torch.log(prgx[i])\n                    \n            if not return_timing: log_prgx_list.append(log_prgx)\n\n            # use these posterior predictives and log p(r | hist) to evaluate y under the full posterior predictive\n            nll = self.nll(log_pygx, log_prgx)\n            if not return_timing: nll_list.append(nll)\n\n            # update belief over run lengths:\n\n            # if classification, then log_pi_t is (batch_size, i+1. y_dim), need to mask before updating belief\n            if self.classification:\n                log_pygx = mask_nlls(y.unsqueeze(-2), log_pygx) # (batch_size, i+1)\n\n            # calculate joint densities p(r_t,data_so_far) for both r_t = r_{t-1} + 1 and r_t = 0\n            log_prx_grow = self.log_1m_hazard + log_pygx + log_prgx                      # p( r_t = r_{t-1} + 1 )\n            log_prx_chpt = self.log_hazard + torch.logsumexp(log_pygx + log_prgx, dim=1) # p (r_t = 0 )\n\n            log_prx = torch.cat((log_prx_grow,log_prx_chpt[:,None]), 1) # shape (batch_size, i + 2)\n            log_prgx = torch.log_softmax(log_prx, dim=1) # log p(r_{i+1} | x_{0:i}, y_{0:i})\n\n            # update posteriors update\n            posterior_params = tuple( torch.cat((u, p.expand(*([batch_size] + list(p.shape)[1:]))), axis=1) for u,p in zip(updated_posterior_params, prior_params) )\n\n            # append to list\n            if not return_timing: posterior_params_list.append(posterior_params)\n\n            if return_timing:\n                torch.cuda.synchronize()\n                end_time = time.perf_counter()\n                time_array.append(end_time-start_time)\n            \n        if not return_timing: nlls = torch.stack(nll_list, dim=1) # shape (batch_size, t, y_dim)\n        \n        if return_timing:\n            return [], [], [], time_array\n        \n        return posterior_params_list, log_prgx_list, nlls\n\n# From metacpd/main/alpaca.py\nclass ALPaCA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = deepcopy(config)\n        self.x_dim = config['model.x_dim']\n        self.phi_dim = config['model.phi_dim']\n        self.y_dim = config['model.y_dim']\n\n        self.sigma_eps = eval(self.config['model.sigma_eps'])\n        self.logSigEps = nn.Parameter(torch.from_numpy(np.log(self.sigma_eps)), requires_grad=self.config['train.learnable_noise'])\n\n        self.Q = nn.Parameter(torch.randn(self.phi_dim, self.y_dim))\n        self.L_asym = nn.Parameter(torch.randn(self.phi_dim, self.phi_dim))\n\n        self.normal_nll_const = self.y_dim*np.log(2*np.pi)\n\n        hid_dim = config['model.hid_dim']\n        self.encoder = get_encoder(config)\n\n    @property\n    def logdetSigEps(self):\n        return torch.sum(self.logSigEps)\n\n    @property\n    def invSigEps(self):\n        return torch.diag(torch.exp(-self.logSigEps))\n\n    def prior_params(self):\n        Q0 = self.Q\n        Linv0 = self.L_asym @ self.L_asym.T\n\n        return (Q0, Linv0)\n\n    def recursive_update(self, phi, y, params):\n        \"\"\"\n            inputs: phi: shape (..., phi_dim )\n                    y:   shape (..., y_dim )\n                    params: tuple of Q, Linv\n                        Q: shape (..., phi_dim, y_dim)\n                        Linv: shape (..., phi_dim, phi_dim)\n        \"\"\"\n        Q, Linv = params\n\n        Lphi = Linv @ phi.unsqueeze(-1)\n\n        Linv = Linv - 1./(1 + phi.unsqueeze(-2) @ Lphi) * (Lphi @ Lphi.transpose(-1,-2))\n        Q = phi.unsqueeze(-1) @ y.unsqueeze(-2) + Q\n\n        return (Q, Linv)\n\n    def log_predictive_prob(self, x, y, posterior_params, update_params=False):\n        \"\"\"\n            input:  x: shape (..., x_dim)\n                    y: shape (..., y_dim)\n                    posterior_params: tuple of Q, Linv:\n                        Q: shape (..., phi_dim, y_dim)\n                        Linv: shape (..., phi_dim, phi_dim)\n                    update_params: bool, whether to perform recursive update on\n                                   posterior params and return updated params\n            output: logp: log p(y | x, posterior_parms)\n                    updated_params: updated posterior params after factoring in (x,y) pair\n        \"\"\"\n\n        phi = self.encoder(x)\n\n        Q, Linv = posterior_params\n\n        K = Linv @ Q\n\n        sigfactor = 1 + (phi.unsqueeze(-2) @ Linv @ phi.unsqueeze(-1))\n        err = y.unsqueeze(-1) - K.transpose(-1,-2) @ phi.unsqueeze(-1)\n\n        invsig = self.invSigEps / sigfactor # shape (..., y_dim y_dim)\n\n        nll_quadform = err.transpose(-1,-2) @ invsig @ err\n        nll_logdet = self.y_dim * torch.log(sigfactor) + self.logdetSigEps\n\n        logp = -0.5*(self.normal_nll_const + nll_quadform + nll_logdet).squeeze(-1).squeeze(-1)\n\n        if update_params:\n            updated_params = self.recursive_update(phi,y,posterior_params)\n            return logp, updated_params\n\n        return logp\n\n\n    def forward(self, x, posterior_params):\n        \"\"\"\n            input: x, posterior params\n            output: y\n        \"\"\"\n        phi = self.encoder(x)\n\n        Q, Linv = posterior_params\n\n        K = Linv @ Q\n\n        sigfactor = 1 + (phi.unsqueeze(-2) @ Linv @ phi.unsqueeze(-1))\n        mu = ( K.transpose(-1,-2) @ phi.unsqueeze(-1) ).squeeze(-1)\n        invsig = self.invSigEps / sigfactor\n\n        return mu, invsig\n\n# From metacpd/main/pcoc.py\nclass PCOC(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = deepcopy(config)\n        self.x_dim = config['model.x_dim']\n        self.phi_dim = config['model.phi_dim']\n        self.y_dim = config['model.y_dim']\n\n        self.sigma_eps = np.zeros([self.y_dim,1]) + np.asarray(eval(config['model.sigma_eps']))\n        self.cov_dim =  self.sigma_eps.shape[-1]\n        print(\"Using %d parameters in covariance:\" % self.cov_dim)\n        if self.phi_dim % self.cov_dim != 0:\n            raise ValueError(\"cov_dim must evenly divide phi_dim\")\n\n        self.logSigEps = nn.Parameter(torch.from_numpy(np.log(self.sigma_eps)), requires_grad=self.config['train.learnable_noise'])\n        \n        Linv_offset = config['model.Linv_init']\n        dir_scale = config['model.dirichlet_scale']\n        self.Q = nn.Parameter(torch.randn(self.y_dim, self.cov_dim, self.phi_dim//self.cov_dim))\n        self.logLinv = nn.Parameter(torch.randn(self.y_dim, self.cov_dim)+Linv_offset)\n        self.log_dirichlet_priors = nn.Parameter(dir_scale*torch.ones(self.y_dim), requires_grad=config['train.learnable_dirichlet'])\n\n        self.normal_nll_const = self.phi_dim*np.log(2*np.pi)\n\n        self.encoder = get_encoder(config)\n\n    @property\n    def invSigEps(self):\n        return torch.exp(-self.logSigEps) #.repeat(self.y_dim,1)\n\n    @property\n    def SigEps(self):\n        return torch.exp(self.logSigEps) #.repeat(self.y_dim,1)\n\n    def prior_params(self):\n        Q0 = self.Q\n        Linv0 = torch.exp(self.logLinv)\n        dir_weights = torch.exp(self.log_dirichlet_priors)\n\n        return (Q0, Linv0, dir_weights)\n\n    def recursive_update(self, phi, y, params):\n        \"\"\"\n            inputs: phi: shape (..., cov_dim, k )\n                    y:   shape (..., y_dim )\n                    params: tuple of Q, Linv\n                        Q: shape (..., y_dim, cov_dim, k)\n                        Linv: shape (..., y_dim, cov_dim)\n                        dir_weights: shape (..., y_dim)\n        \"\"\"\n        Q, Linv, dir_weights = params\n\n        # zeros out entries all except class y\n        invSigEps_masked = self.invSigEps * y.unsqueeze(-1) # (..., y_dim, cov_dim)\n\n        Q = Q + invSigEps_masked.unsqueeze(-1)*phi.unsqueeze(-3)\n        Linv = Linv + invSigEps_masked\n        dir_weights = dir_weights + y\n\n        return (Q, Linv, dir_weights)\n\n    def log_predictive_prob(self, x, y, posterior_params, update_params=False):\n        \"\"\"\n            input:  x: shape (..., x_dim)\n                    y: shape (..., y_dim)\n                    posterior_params: tuple of Q, Linv:\n                        Q: shape (..., y_dim, cov_dim, k)\n                        Linv: shape (..., y_dim, cov_dim)\n                        dir_weights: shape (..., y_dim)\n                    update_params: bool, whether to perform recursive update on\n                                   posterior params and return updated params\n            output: logp: log p(y, x | posterior_params) (..., y_dim)\n                    updated_params: updated posterior params after factoring in (x,y) pair\n        \"\"\"\n\n        x_shape = list(x.shape)\n\n        if len(x_shape) > 4: # more than one batch dim\n            x = x.reshape([-1]+x_shape[-3:])\n\n        phi = self.encoder(x) # (..., phi_dim)\n        if len(x_shape) > 4:\n            phi = phi.reshape(x_shape[:-3]+[self.phi_dim])\n\n        Q, Linv, dir_weights = posterior_params\n        mu = Q / Linv.unsqueeze(-1) # (..., y_dim, cov_dim, k)\n        pred_cov = 1./Linv + self.SigEps() # (..., y_dim, cov_dim)\n\n        phi_shape = phi.shape\n        phi_reshaped = phi.reshape(*(list(phi_shape)[:-1]+[self.cov_dim, -1])) # (..., cov_dim, k)\n\n        err = phi_reshaped.unsqueeze(-3) - mu # (..., y_dim, cov_dim, k)\n\n        nll_quadform = (err**2 / pred_cov.unsqueeze(-1) ).sum(-1).sum(-1)\n        nll_logdet = (self.phi_dim/self.cov_dim) * torch.log(pred_cov).sum(-1) # sum of log of diagonal entries\n\n        logp = -0.5*(nll_quadform + nll_logdet + self.normal_nll_const) # log p(x | y)\n\n        logp += torch.log(dir_weights / dir_weights.sum(-1,keepdim=True)) # multiply by p(y) posterior to get p(x, y)\n\n        if update_params:\n            updated_params = self.recursive_update(phi_reshaped, y, posterior_params)\n            return logp, updated_params\n\n        return logp\n\n\n    def forward(self, x, posterior_params):\n        \"\"\"\n            input: x, posterior params\n            output: log p(x | y) for all y\n        \"\"\"\n        x_shape = list(x.shape)\n\n        if len(x_shape) > 4: # more than one batch dim\n            x = x.reshape([-1]+x_shape[-3:])\n\n        phi = self.encoder(x) # (..., phi_dim)\n        if len(x_shape) > 4:\n            phi = phi.reshape(x_shape[:-3]+[self.phi_dim])\n\n        Q, Linv, dir_weights = posterior_params\n        mu = Q / Linv.unsqueeze(-1) # (..., y_dim, cov_dim, k)\n        pred_cov = 1./Linv + self.SigEps() # (..., y_dim, cov_dim)\n\n        phi_shape = phi.shape\n        phi_reshaped = phi.reshape(*(list(phi_shape)[:-1]+[self.cov_dim, -1])) # (..., cov_dim, k)\n\n        err = phi_reshaped.unsqueeze(-3) - mu # (..., y_dim, cov_dim, k)\n\n        nll_quadform = (err**2 / pred_cov.unsqueeze(-1) ).sum(-1).sum(-1)\n        nll_logdet = (self.phi_dim/self.cov_dim) * torch.log(pred_cov).sum(-1) # sum of log of diagonal entries\n\n        logp = -0.5*(nll_quadform + nll_logdet + self.normal_nll_const) # log p(x | y)\n\n        logp += torch.log(dir_weights / dir_weights.sum(-1,keepdim=True)) # multiply by p(y) posterior to get p(x, y)\n\n        return logp",
    "Experiment Result": "MOCA (Meta-learning for Online Changepoint Analysis) is evaluated by instantiating it with specific underlying predictive models (UPMs) and tested on various datasets with defined experimental settings.\n\n**Underlying Predictive Models (UPMs):**\n- For regression tasks, MOCA uses ALPaCA (A Bayesian meta-learning approach for regression).\n- For classification tasks, MOCA uses PCOC (Probabilistic Clustering for Online Classification).\n\n**Datasets:**\n- **Regression:** `SwitchingSinusoidDataset` (referred to as 'Sinusoid' in configurations) and `SwitchingNoiseSinusoidDataset` (referred to as 'NoiseSinusoid').\n- **Classification:** `MiniImageNet` and `RainbowMNISTDataset` (referred to as 'RainbowMNIST').\n\n**Experimental Parameters and Settings (Defaults unless otherwise specified):**\n- **General:**\n    - `--data.batch_size`: 50 for training, 1 for testing.\n    - `--data.horizon`: 100 for training, 400 for testing.\n    - `--data.cuda`: CUDA device ID, -1 for CPU.\n    - `--train.seed`: Numpy seed, default 1 or 1000.\n    - `--train.experiment_id`: Unique experiment identifier, default 0.\n- **MOCA Specific:**\n    - `--data.hazard`: Hazard rate for task switches, default 0.1 for training, 0.01 for testing.\n    - `--train.learnable_hazard`: Flag to enable learning the hazard rate (0=False, 1=True), default 0 (False).\n    - `--train.task_supervision`: Percentage of task switches labeled (float), default None.\n    - `--train.oracle_hazard`: Hazard rate for oracle (curriculum), default None.\n- **Model Architecture (UPMs and Encoders):**\n    - `--model.x_dim`: Input dimensionality, 1 for Sinusoid datasets (regression), 3 for image datasets (classification).\n    - `--model.y_dim`: Output dimensionality, 1 for regression, 5 or 10 for classification (depending on dataset).\n    - `--model.hid_dim`: Dimensionality of hidden layers, default 128.\n    - `--model.phi_dim`: Dimensionality of the embedding space, default 32.\n- **ALPaCA Specific (Regression UPM):**\n    - `--model.sigma_eps`: Noise covariance, e.g., `[0.05]` for Sinusoid datasets. Can be learned.\n    - `--train.learnable_noise`: Flag to enable learning noise covariance (0=False, 1=True), default 0 (False).\n- **PCOC Specific (Classification UPM):**\n    - `--model.Linv_init`: Initialization of logLinv, default 0.0.\n    - `--model.dirichlet_scale`: Value for log Dirichlet concentration parameters initialization, default 10.0.\n    - `--train.learnable_dirichlet`: Flag to enable learning Dirichlet concentration (0=False, 1=True), default 0 (False).\n- **Training Process:**\n    - `--train.train_iterations`: Number of episodes to train, default 7500.\n    - `--train.val_iterations`: Number of episodes to validate on, default 5.\n    - `--train.learning_rate`: Learning rate for optimizer (Adam), default 0.02.\n    - `--train.decay_every`: Learning rate decay every 1500 iterations (gamma=0.5).\n    - `--train.grad_accumulation_steps`: Number of gradient accumulation steps, default 1.\n- **Evaluation Metrics:**\n    - Negative Log-Likelihood (NLL) is reported for both regression and classification tasks.\n    - Accuracy is reported for classification tasks."
}{
    "Title": "Tensor Normal Training for Deep Learning Models",
    "Main Contributions": "Proposed Tensor Normal Training (TNT), a novel approximate natural gradient method leveraging the tensor normal (TN) distribution to approximate the probabilistically based Fisher matrix. TNT is model-agnostic, requiring only the shape of training parameters, and uses block-wise covariance of sampling-based gradients with a Kronecker separable structure for tractable Fisher approximation. It introduces mild memory and computational overhead compared to first-order methods. Experimentally, TNT shows superior optimization performance over state-of-the-art first-order methods, comparable performance to second-order methods (KFAC, Shampoo), and similar generalization ability with fewer epochs. Theoretically, TNT is proven to converge to a stationary point under mild assumptions.",
    "Methodology": "TNT approximates the natural gradient by using a block-diagonal Fisher matrix, with each block corresponding to the covariance of a tensor variable. It assumes that the sampling-based gradient for each tensor variable (Wl) follows a zero-mean Tensor-Normal (TN) distribution, resulting in a Kronecker-factored Fisher matrix (FW = U1 \biguplus ... \biguplus Uk). The method identifies unique covariance parameters (\tilda{U}i) by enforcing that the average eigenvalues of the covariance matrices for each tensor dimension are equal. The update direction is calculated by applying the inverse of the Kronecker-factored Fisher matrix to the gradient. For practical implementation, moving average estimates of relevant statistics are used, and inverse computations are amortized over multiple iterations.",
    "Experimental Setup": "TNT was compared against state-of-the-art first-order (Adam, SGD with momentum) and second-order (KFAC, Shampoo) methods. Experiments ran on a V100 GPU and Xeon Gold CPUs using PyTorch, with hyperparameters tuned via grid search and results averaged over 5 random seeds. Optimization tasks involved two autoencoder problems on MNIST (784-1000-500-250-30-250-500-1000-784) with binary entropy loss and FACES (625-2000-1000-500-30-500-1000-2000-625) with squared error loss, using a batch size of 1,000. Generalization tasks used CNNs (ResNet32 on CIFAR-10, VGG16 on CIFAR-100) with batch size 128, data augmentation, learning rate decay, and weight decay. Second-order methods had Fisher/inverse update frequencies (T1/T2) of 1/20 for autoencoders and 10/100 for CNNs. A variant, TNT-EF (using empirical Fisher), was also compared.",
    "Limitations": "The study's experiments were limited to existing models and datasets, precluding evaluation on larger models like ImageNet or advanced NLP tasks due to resource constraints. The proposed method was not investigated in a distributed setting. Direct comparisons with state-of-the-art Kronecker-based quasi-Newton methods were not included. The paper also acknowledges the general potential for negative societal impacts if the model or data design is flawed or contains bias, though this aspect was beyond the scope of the current work.",
    "Future Research Directions": "Future work includes extending the TNT method to a distributed computing setting for deep learning. Further experiments on larger-scale models, such as ImageNet, and advanced models for natural language processing tasks are suggested to fully explore TNT's potential. Comparisons with state-of-the-art Kronecker-based quasi-Newton methods are also a direction for future research. Additionally, incorporating more sophisticated learning rate schedules (e.g., warm-up followed by decay) and conducting more extensive joint hyperparameter tuning for CNN problems could be beneficial.",
    "Experiment Code": "def train_model(home_path = '/home/jupyter/',dataset_name = 'CIFAR-10',algorithm = 'TNT',lr = 1e-4,damping_value = 0.01,weight_decay = 0,):args = {}args['list_lr'] = [lr]args['weight_decay'] = weight_decayif dataset_name == 'CIFAR-10':args['dataset'] = 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'args['initialization_pkg'] = 'kaiming_normal'elif dataset_name == 'CIFAR-100':args['dataset'] = 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization'args['initialization_pkg'] = 'normal'elif dataset_name == 'MNIST':args['dataset'] = 'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization'args['initialization_pkg'] = 'normal'elif dataset_name == 'FACES':args['dataset'] = 'FacesMartens-autoencoder-relu-no-regularization'args['initialization_pkg'] = 'normal'else:print('dataset_name')print(dataset_name)sys.exit()if dataset_name in ['MNIST']:args['if_max_epoch'] = 0args['max_epoch/time'] = 500elif dataset_name in ['FACES']:args['if_max_epoch'] = 0args['max_epoch/time'] = 2000elif dataset_name in ['CIFAR-10', 'CIFAR-100']:if algorithm in ['SGD-m', 'Adam']:args['if_max_epoch'] = 1args['max_epoch/time'] = 200args['num_epoch_to_decay'] = 60args['lr_decay_rate'] = 0.1elif algorithm in ['TNT', 'Shampoo', 'KFAC']:args['if_max_epoch'] = 1args['max_epoch/time'] = 100args['num_epoch_to_decay'] = 40args['lr_decay_rate'] = 0.1else:print('algorithm')print(algorithm)sys.exit()else:print('dataset_name')print(dataset_name)sys.exit()args['momentum_gradient_rho'] = 0.9if algorithm == 'SGD-m':args['momentum_gradient_dampening'] = 0if dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'SGD-momentum'elif dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'SGD-LRdecay-momentum'else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm == 'Adam':args['RMSprop_epsilon'] = damping_valueargs['RMSprop_beta_2'] = 0.999args['momentum_gradient_dampening'] = 0.9if dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'Adam-noWarmStart-momentum-grad-LRdecay'elif dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'Adam-noWarmStart-momentum-grad'else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm in ['TNT', 'Shampoo']:if algorithm in ['Shampoo']:args['shampoo_if_coupled_newton'] = Trueelif algorithm in ['TNT']:args['shampoo_if_coupled_newton'] = Falseargs['shampoo_epsilon'] = damping_valueargs['if_Hessian_action'] = Falseargs['shampoo_decay'] = 0.9args['shampoo_weight'] = 0.1args['momentum_gradient_dampening'] = 0if dataset_name in ['CIFAR-10', 'CIFAR-100']:if algorithm == 'TNT':args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'elif algorithm == 'Shampoo':args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'args['shampoo_update_freq'] = 10args['shampoo_inverse_freq'] = 100elif dataset_name in ['MNIST', 'FACES']:if algorithm == 'TNT':args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'elif algorithm == 'Shampoo':args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'args['shampoo_update_freq'] = 1args['shampoo_inverse_freq'] = 20else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm == 'KFAC':args['kfac_if_update_BN'] = Trueargs['kfac_if_BN_grad_direction'] = Trueargs['kfac_rho'] = 0.9args['kfac_damping_lambda'] = damping_valueargs['momentum_gradient_dampening'] = 0if dataset_name in ['FACES', 'MNIST']:args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad'args['kfac_if_svd'] = Falseargs['kfac_cov_update_freq'] = 1args['kfac_inverse_update_freq'] = 20elif dataset_name in ['CIFAR-100', 'CIFAR-10']:args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay'args['kfac_if_svd'] = Falseargs['kfac_cov_update_freq'] = 10args['kfac_inverse_update_freq'] = 100else:print('dataset_name')print(dataset_name)sys.exit()else:print('algorithm')print(algorithm)sys.exit()args['record_epoch'] = 1args['seed_number'] = 9999args['num_threads'] = 8args['home_path'] = home_pathargs['if_gpu'] = Trueargs['if_test_mode'] = Falseargs['if_auto_tune_lr'] = Falseargs['if_grafting'] = False_ = tune_lr(args)return",
    "Experiment Result": "Method: TNT\nAlgorithm name: 'TNT'\nLearning rate (lr): 1e-4\nDamping value: 0.01\nWeight decay: 0\nMomentum gradient rho: 0.9\nMomentum gradient dampening: 0\n\nDataset-specific settings for CIFAR-10:\n  Dataset argument: 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'\n  Initialization package: 'kaiming_normal'\n  Max epoch/time: 100 epochs\n  Learning rate decay: True\n  Number of epochs to decay LR: 40\n  LR decay rate: 0.1\n  Algorithm argument: 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'\n  Shampoo update frequency: 10\n  Shampoo inverse frequency: 100\n  Tau (regularization parameter, from from_dataset_to_N1_N2): 0\n\nDataset-specific settings for MNIST/FACES:\n  Algorithm argument: 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'\n  Shampoo update frequency: 1\n  Shampoo inverse frequency: 20\n\nCommon settings for TNT:\n  Shampoo if coupled newton: False\n  Shampoo epsilon: 0.01 (damping_value)\n  If Hessian action: False\n  Shampoo decay: 0.9\n  Shampoo weight: 0.1\n\nGeneral experimental settings:\n  Record epoch: 1\n  Seed number: 9999\n  Number of threads: 8\n  Home path: '/home/jupyter/' (default)\n  If GPU: True\n  If test mode: False\n  If auto tune LR: False\n  If grafting: False\n  If LR decay: True (handled by get_params, e.g., for CIFAR-10/100 setup)\n  If momentum gradient: True (handled by get_params)"
}{
    "Title": "Tensor Normal Training for Deep Learning Models",
    "Main Contributions": "Proposes Tensor Normal Training (TNT), a novel approximate natural gradient method for deep learning models. TNT leverages the tensor normal (TN) distribution to approximate the Fisher matrix, utilizing the block-wise covariance of sampling-based gradients. It features a tractable approximation of the Fisher matrix due to the Kronecker separable structure of the TN distribution, requiring only knowledge of training parameter shapes (model-agnostic). TNT demonstrates superior optimization performance over state-of-the-art first-order methods and comparable performance to second-order methods like KFAC and Shampoo, while also achieving comparable generalization using fewer training epochs. The method introduces a new way to identify covariance parameters of TN distributions suitable for optimization and is theoretically proven to converge to a stationary point under mild assumptions.",
    "Methodology": "TNT approximates the natural gradient by modeling the covariance of sampling-based gradients using the tensor normal distribution. It assumes a block-diagonal Fisher matrix, with each block corresponding to a tensor variable (W_l), and models the sampling-based gradient (DW_l) for each variable as following a zero-mean Tensor Normal distribution with Kronecker-structured covariance factors (U_1, ..., U_k). The Fisher matrix for W is then approximated as the Kronecker product of these factors: F_W = U_1 ⊗ ... ⊗ U_k. A new identification scheme determines the U_i factors by enforcing a constant average eigenvalue (tr(U_i)/d_i) across dimensions to ensure similar magnitudes and aid damping parameter selection. The natural gradient update direction is computed using the inverse of the Kronecker product of damped (U_i + ϵI) factors, applied via mode-i products. The practical algorithm employs moving averages for gradient momentum and covariance factor estimates, with amortized updates for statistics (frequency T1) and inverses (frequency T2), and includes an additional backward pass for sampling-based gradient computation.",
    "Experimental Setup": "Experiments were conducted on a machine with one V100 GPU and eight Xeon Gold 6248 CPUs using PyTorch. Hyperparameters (learning rate, damping, momentum, beta) were tuned via grid search for optimal training loss (optimization tasks) or validation accuracy (generalization tasks), with results averaged over 5 runs using different random seeds and standard deviations reported. Optimization benchmarks included MNIST and FACES autoencoder problems with specific architectures (e.g., 784-1000-500-250-30-250-500-1000-784 for MNIST), binary entropy loss for MNIST, and squared error loss for FACES, with a batch size of 1,000 and update frequencies T1=1, T2=20. Generalization benchmarks involved ResNet32 on CIFAR-10 and VGG16 (with BN layers) on CIFAR-100, using a batch size of 128, data augmentation, and weight decay for all methods. CNN training schedules differed for first-order (200 epochs) and second-order methods (100 epochs), with T1=10, T2=100. Damping values were set to 1e-8 for Adam, 0.03 for KFAC, and 0.01 for TNT/Shampoo. Comparisons were made against SGD with momentum, Adam, KFAC, Shampoo, and a variant called TNT-EF (using empirical Fisher).",
    "Limitations": "The research did not include experiments on extremely large models (e.g., ImageNet) or advanced NLP tasks due to computational resource constraints, although strong potential was demonstrated. The method's performance in a distributed computing setting was not explored. Comparisons were limited to Fisher-based second-order methods, excluding Kronecker-based quasi-Newton methods like K-BFGS. The paper acknowledges that more extensive hyperparameter tuning (e.g., joint optimization of learning rate, weight decay, damping, and momentum parameters) and adaptive learning rate schedules could yield further improvements. Furthermore, the preconditioning matrices are dependent on the specific tensor shape of parameters, meaning reshaping them (e.g., flattening) would lead to different matrices. The paper also mentions potential negative societal impacts if the underlying model or data design contains flaws or bias, though this aspect is outside the scope of the presented work.",
    "Future Research Directions": "Future work includes conducting experiments on larger-scale models (e.g., ImageNet) and advanced NLP tasks to further validate TNT's capabilities. Extending the method to operate effectively within a distributed computing environment is another key direction. Investigating the impact of increasing the frequency of statistics and inverse updates in second-order methods, as KFAC has shown improvements in this area, is also suggested. More comprehensive hyperparameter tuning strategies, such as jointly optimizing learning rate, weight decay, damping, and momentum parameters, along with exploring 'warm-up, then decay' learning rate schedules, are potential avenues. Additionally, developing and evaluating diagonal variants of TNT and exploring architectural modifications like dividing large tensors into smaller blocks are considered.",
    "Experiment Code": "if algorithm == 'TNT':args['shampoo_if_coupled_newton'] = Falseargs['shampoo_epsilon'] = damping_valueargs['if_Hessian_action'] = Falseargs['shampoo_decay'] = 0.9args['shampoo_weight'] = 0.1args['momentum_gradient_dampening'] = 0if dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'args['shampoo_update_freq'] = 10args['shampoo_inverse_freq'] = 100elif dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'args['shampoo_update_freq'] = 1args['shampoo_inverse_freq'] = 20",
    "Experiment Result": "The TNT algorithm is configured with a learning rate (lr), damping value, and weight decay. It sets `shampoo_if_coupled_newton` to `False` and `if_Hessian_action` to `False`. Covariance matrix estimation uses a decay factor of `0.9` and a weight of `0.1`. Gradient momentum dampening is set to `0`. Training duration, learning rate decay schedule, and initialization package (`kaiming_normal` for CIFAR-10, `normal` for others) are dataset-dependent. For CIFAR-10/100, `shampoo_update_freq` is `10` and `shampoo_inverse_freq` is `100`; for MNIST/FACES, these are `1` and `20` respectively. `N1` and `N2` (minibatch sizes) are 128 for CIFAR-10/100 and 1000 for MNIST/FACES. Training stops after a maximum of 100 epochs for CIFAR-10/100 (40 epochs before LR decay), and 500-2000 times (CPU time) for MNIST/FACES. A seed number of 9999 and 8 threads are used."
}{
    "Title": "Analytical Study of Momentum-Based Acceleration Methods in Paradigmatic High-Dimensional Non-Convex Problems",
    "Main Contributions": "The paper provides an analytical description of the average evolution of momentum-based methods, specifically Heavy-Ball momentum and Nesterov acceleration, in two prototypical high-dimensional non-convex optimization problems: the mixed p-spin model and the spiked matrix-tensor model. It derives a closed set of equations using dynamical mean field theory (DMFT) to describe their behavior in the infinite-dimensional limit. A key finding is that while these accelerated methods speed up the dynamics, they do not improve the algorithmic recovery threshold compared to vanilla gradient descent in the spiked model, meaning they reach the same quality of solution but faster.",
    "Methodology": "The core methodology is Dynamical Mean Field Theory (DMFT), a technique from statistical physics, applied to describe the average dynamics of optimization algorithms. This involves deriving a set of equations for order parameters, such as dynamical two-time correlation and response functions, which encode the memory terms and statistical properties of noise in the dynamics. The derivation for Nesterov acceleration uses a dynamical cavity method to track the effect of an additional degree of freedom in perturbation theory. For Heavy-Ball momentum, an analogous derivation is presented, with a connection to second-order ODEs representing a massive particle moving under a potential. The hard constraint on the vector norm is relaxed to a soft constraint with a penalty term to simplify analysis.",
    "Experimental Setup": "The research analyzes Nesterov acceleration, Polyak's Heavy-Ball momentum, and standard Gradient Descent (GD) on two non-convex models: the mixed p-spin model and the spiked matrix-tensor model. Numerical simulations were conducted to validate the DMFT equations, with input dimensions N=1024 for the mixed p-spin model and N=8192 for the spiked matrix-tensor model, utilizing a dilution technique. Comparisons between DMFT and simulations were made based on the evolution of loss, vector radius (L2 norm), and overlap with the hidden solution (for the spiked model). Algorithmic thresholds for signal recovery were estimated by observing the divergence of relaxation time, which was fitted to a power law as the noise-to-signal ratio increased.",
    "Limitations": "The derivation of the DMFT equations is heuristic and not fully rigorous, relying on numerical simulations for validation. The study reveals that momentum-based methods, while speeding up convergence, do not outperform vanilla gradient descent in terms of the algorithmic recovery threshold in the analyzed spiked model. The research focuses on typical instances of the optimization problem rather than worst-case analysis. Additionally, the paper explicitly states that it does not compare these accelerated gradient methods to algorithms of a different nature, such as message passing ones. Finite size effects were also noted to be stronger in the spiked model simulations.",
    "Future Research Directions": "The authors suggest extending their analytical techniques to other momentum-based algorithms, such as quasi-hyperbolic momentum (QHM) and proportional integral-derivative (PID) control algorithms. They also propose extending the analysis to 1-layer neural networks by combining their results with existing DMFT techniques. Further research could involve applying this framework to simple inference problems from a learning perspective, like the phase retrieval problem. Finally, the same questions could be explored within the context of recurrent neural networks, where DMFT approaches have already been used for gradient-based methods.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations",
    "Main Contributions": "The paper proposes TnALE (Alternating Local Enumeration), a novel algorithm that significantly reduces the computational cost for Tensor Network Structure Search (TN-SS) compared to state-of-the-art methods like TNLS. It addresses the challenge of combinatorial explosion and NP-hardness in selecting good TN models (ranks, topology, permutations). The authors also provide the first convergence analysis for both TNLS and TnALE, proving linear convergence up to a constant under discrete convexity assumptions, and rigorously demonstrating TnALE's superior evaluation efficiency (ideally O(KR) vs. TNLS's Ω(2K) evaluations). Experimental results confirm TnALE's ability to find practically good TN structures with vastly fewer evaluations.",
    "Methodology": "TnALE improves upon TNLS by replacing its random sampling with Alternating Local Enumeration (ALE). It iteratively updates each structure-related variable (TN-ranks 'r' and graph 'G') by enumerating all its possible values within a defined neighborhood while keeping other variables fixed. This alternating enumeration proceeds in a 'round-trip' manner (forward and backward through variables). The core intuition is to avoid combinatorial explosion and ensure a non-increasing objective function value. The method also incorporates 'knowledge transfer' tricks, such as reusing optimized core tensors from lower-rank structures for higher-rank initializations and using linear interpolation for objective estimation to accelerate evaluation, especially during an initial phase with a larger search radius. Theoretically, the paper re-defines discrete analogues for fundamental concepts like finite gradient, strong convexity, and smoothness to analyze the algorithm's convergence and evaluation efficiency. It also draws a connection between ALE and TT-cross approximation, suggesting that the low-rank structure of the optimization landscape contributes to TnALE's efficiency.",
    "Experimental Setup": "The evaluation of TnALE involved both synthetic and real-world datasets across various TN-SS sub-problems. For synthetic data, TN-PS (permutation selection) was tested on TR (order-4, 6, 8), PEPS (order-6), HT (order-6), MERA (order-8), and TW (order-5) topologies with randomly generated ranks and permutations. TN-RS (rank selection) was tested on order-8 TR tensors with 'lower-ranks' ([1,4]) and 'higher-ranks' ([5,8]) configurations. For real-world data, TnALE was applied to compress learnable parameters of Tensorial Gaussian Process (TGP) models using three regression datasets (CCPP, MG, Protein) and to compress natural images (from BSD500 dataset) in TN-PS and TN-TS tasks. Comparison methods included TNLS, TNGA, TTOpt, TR-SVD, TR-BALS, TR-LM, TRAR, and a 'Greedy' method. The objective function used was a combination of compression ratio and relative squared error (RSE), with Adam optimizer for inner minimization. Performance was measured by the number of objective function evaluations, RSE, and an Eff. index (ratio of parameter numbers).",
    "Limitations": "The primary limitation of TnALE is its susceptibility to local convergence. Empirically, it was found that TnALE can easily get trapped in multiple local minima (especially noted in TN-TS experiments and with MERA data), leading to suboptimal compression ratios. This is contrasted with stochastic methods like TNGA and TNLS, which appear to be better at avoiding such local minima due to their inherent randomness. Another limitation is that the identifiability of the proposed method for TN-SS in the presence of noise was not investigated.",
    "Future Research Directions": "Future work will focus on addressing the local convergence issue to enable TnALE to avoid or escape poor local minima more effectively. Additionally, the identifiability of the proposed method for Tensor Network Structure Search in the presence of noise will be investigated.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective",
    "Main Contributions": "The paper addresses the computationally intensive NP-hard problem of Tensor Network Structure Search (TN-SS) in computer vision and machine learning. It proposes a novel Tensor Network (TN) paradigm called SVD-inspired TN decomposition (SVDinsTN) that tackles TN-SS from a regularized modeling perspective. This approach eliminates the need for repeated structure evaluations, a major bottleneck in existing \"sampling-evaluation\" methods. SVDinsTN achieves approximately 100 to 1000 times acceleration compared to state-of-the-art TN-SS methods while maintaining comparable representation ability. The method includes a theoretical convergence guarantee and establishes an upper bound for the TN rank, which guides an efficient initialization scheme. It also demonstrates strong performance in practical applications like tensor completion.",
    "Methodology": "SVDinsTN extends Fully-Connected Tensor Network (FCTN) decomposition by inserting diagonal factors between any two TN cores in the fully-connected topology. The sparsity of these diagonal factors is leveraged to reveal a compact TN structure. The problem is formulated as an L1-norm-based regularized model, minimizing the Frobenius norm of the reconstruction error, with Tikhonov regularization for TN cores and an L1-norm penalty on the diagonal factors to induce sparsity. This model is solved using a Proximal Alternating Minimization (PAM)-based algorithm, which iteratively updates TN cores (Gk) and diagonal factors (St,l). The Gk-subproblem is solved by converting it into a differentiable objective function. The St,l-subproblem, involving an L1-norm, is solved using an Alternating Direction Method of Multipliers (ADMM) framework, incorporating a shrinkage operator. A novel initialization scheme is designed based on an established upper bound for TN rank (Rt,l ≤ min(rank(X(t)), rank(X(l)))), utilizing truncated SVD of mode-(t,l) slices of the input tensor and a shrinkage operation to reduce initial rank values and computational cost.",
    "Experimental Setup": "The method's performance was evaluated on three claims using both synthetic and real-world data. For validating the ability to reveal customized TN structures (Claim A), synthetic fourth-order (16x18x20x22) and fifth-order (14x16x18x20x22) tensors with various TN structures (including isomorphic 'ring' and 'five-star' topologies) were used, with 100 independent tests measuring success rate. For validating efficiency and representation ability (Claim B), real-world light field images (Bunny, Knights, Truck; fifth-order 40x60x3x9x9) and higher-order synthetic tensors (6th, 8th, 10th order) were tested against six baselines: TRALS, FCTNALS (pre-defined topology), TNGreedy, TNGA, TNLS, and TNALE (TN-SS methods). Metrics included Compression Ratio (CR), Reconstruction Error (RE) bound, and run time. The impact of the proposed initialization scheme and its 'shrink' operation was also analyzed. For validating performance in tensor completion (Claim C), four real-world color videos (Bunny, News, Salesman, Silent; fourth-order 144x176x3x50) with a 90% missing ratio were compared against FBCP, TMac, TMacTT, TRLRF, TW, and TNLS. Reconstructed quality was evaluated using Mean Peak Signal-to-Noise Ratio (MPSNR) and run time. Numerical convergence was verified using relative change curves.",
    "Limitations": "While SVDinsTN effectively addresses the computationally consuming issue in Tensor Network Structure Search, the theoretical guarantee of the optimal TN structure remains an open problem. The current work does not provide this theoretical assurance.",
    "Future Research Directions": "The primary future research direction is to solve the open problem of providing a theoretical guarantee for the optimal Tensor Network (TN) structure. This would complement the current method's ability to efficiently find compact structures by ensuring their optimality.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning",
    "Main Contributions": "The paper proposes TTOpt, a novel and efficient gradient-free optimization algorithm for multivariable functions. It leverages low-rank Tensor Train (TT) representation and a generalized maximum matrix volume principle. The key contributions include demonstrating TTOpt's competitive performance against popular gradient-free methods on analytical benchmark functions and its application to Reinforcement Learning (RL) problems. Notably, the research empirically shows that RL agents with discrete (quantized) weights can perform effectively in continuous control tasks, enabling the direct training of quantized neural networks suitable for low-power devices. TTOpt consistently outperforms baselines in terms of function evaluations or execution time.",
    "Methodology": "TTOpt discretizes continuous optimization problems by introducing a grid for each parameter, effectively representing the objective function as an implicit d-dimensional tensor. This tensor is then approximated using the Tensor Train (TT) format to overcome the curse of dimensionality. The optimization process is reformulated as finding the maximal elements of this tensor, guided by a generalized maximum volume principle. This involves iteratively searching for maximal volume submatrices in column and row spaces during 'sweeps' (forward and backward) across tensor modes. For stability, QR decomposition is applied before the maxvol algorithm, and `rect_maxvol` is used for adaptive rank selection. A continuous, smooth, and strictly monotone mapping function, `g(x) = pi/2 - atan(J(x) - Jmin)`, transforms objective values to enable finding global minimum/maximum by searching for maximum modulus. Crucially, the method employs quantization of tensor modes (reshaping the original tensor into a 'long' one with smaller mode sizes like P=2) to significantly boost accuracy and reduce complexity for fine grids.",
    "Experimental Setup": "TTOpt was evaluated on two main categories: analytical benchmark functions and Reinforcement Learning (RL) tasks. For benchmark functions, it was tested on 10-dimensional functions (e.g., Ackley, Rastrigin, Griewank) with known global minima, and its scalability was assessed on functions up to 500 dimensions. RL experiments involved continuous control tasks in Mujoco and OpenAI-GYM, specifically Swimmer-v3, LunarLanderContinuous-v2, InvertedPendulum-v2, and HalfCheetah-v3, where the policy was a neural network with three hidden layers and tanh/ReLU activations. TTOpt's configurable parameters included grid bounds, rank (R=4 for benchmarks, 3-5 for RL), submode size (P=2), number of submodes (q such that N=P^q), and a limit on objective function calls (M=10^5 or 10^4*d). Baselines included various gradient-free methods (GA, openES, cmaES, DE, NB, PSO), gradient-based optimizers (BFGS, L-BFGS, CG, Newton, TR NCG, TR), and Bayesian optimization algorithms (SOO, dSOO, LOGO, RANDOM). Experiments were averaged over 10 independent runs for benchmarks and 7 random seeds for RL, with computations performed on a standard laptop.",
    "Limitations": "The current TTOpt algorithm lacks formal theoretical guarantees for convergence to the global minimum and its rate in the multidimensional tensor case, as there is no direct analog to the matrix maximum volume bound (Eq. 3). Although results monotonically improve with iterations, a strong theoretical foundation for tensor convergence is not yet established. For some high-dimensional benchmark functions (F4, F7, F9), the heuristic used for limiting the number of objective function evaluations was found to be less accurate, leading to larger errors. Additionally, the computational cost of the `maxvol` algorithm, with a complexity of O(T * d * max(Nk * Rk^3)), can become a bottleneck if individual objective function calls are computationally inexpensive. Finally, the choice of rank can be sensitive, with both excessively small or large ranks potentially reducing accuracy due to numerical instabilities or insufficient convergence sweeps.",
    "Future Research Directions": "Future research should focus on developing formal theoretical guarantees for TTOpt's convergence to the global minimum in the multidimensional tensor context, potentially by establishing an analog to the maximum matrix volume principle. Further investigation is needed to refine parameter selection heuristics, such as the number of objective function calls and optimal rank, to ensure consistent accuracy across diverse problems and dimensions. Exploring extensions of the TTOpt methodology to incorporate other low-rank tensor decompositions (e.g., Tensor Chain/Tensor Ring, Hierarchical Tucker) could broaden its applicability. Additionally, exploring novel continuous, smooth, and strictly monotone mapping functions for objective transformation may enhance performance and stability. The paper also implicitly suggests that TTOpt could serve as a bridge between continuous and discrete optimization methods, indicating a direction for integrating this approach into hybrid optimization frameworks or exploring its applicability to other domains requiring gradient-free or high-dimensional discrete optimization.",
    "Experiment Code": "class TTOpt():    def __init__(self, f, d, a=None, b=None, n=None, p=None, q=None,                 evals=None, name=None, callback=None, x_opt_real=None,                 y_opt_real=None, is_func=True, is_vect=True, with_cache=False,                 with_log=False, with_opt=False, with_full_info=False,                 with_wrn=True):        self.f = f        self.d = int(d)        if isinstance(a, (int, float)):            self.a = np.ones(self.d, dtype=float) * a        elif a is not None:            self.a = np.asanyarray(a, dtype=float)        else:            if is_func:                raise ValueError('Grid lower bound (a) should be set')            self.a = None        if self.a is not None and self.a.size != self.d:            raise ValueError('Grid lower bound (a) has invalid shape')        if isinstance(b, (int, float)):            self.b = np.ones(self.d, dtype=float) * b        elif b is not None:            self.b = np.asanyarray(b, dtype=float)        else:            if is_func:                raise ValueError('Grid upper bound (b) should be set')            self.b = None        if self.b is not None and self.b.size != self.d:            raise ValueError('Grid upper bound (b) has invalid shape')        if n is None:            if p is None or q is None:                raise ValueError('If n is not set, then p and q should be set')            self.p = int(p)            self.q = int(q)            self.n = np.ones(self.d * self.q, dtype=int) * self.p            self.n_func = np.ones(self.d, dtype=int) * (self.p**self.q)        else:            if p is not None or q is not None:                raise ValueError('If n is set, then p and q should be None')            self.p = None            self.q = None            if isinstance(n, (int, float)):                self.n = np.ones(self.d, dtype=int) * int(n)            else:                self.n = np.asanyarray(n, dtype=int)            self.n_func = self.n.copy()        if self.n_func.size != self.d:            raise ValueError('Grid size (n/p/q) has invalid shape')        self.evals = int(evals) if evals else None        self.name = name or ''        self.callback = callback        self.x_opt_real = x_opt_real        self.y_opt_real = y_opt_real        self.is_func = bool(is_func)        self.is_vect = bool(is_vect)        self.with_cache = bool(with_cache)        self.with_log = bool(with_log)        self.with_opt = bool(with_opt)        self.with_full_info = bool(with_full_info)        self.with_wrn = bool(with_wrn)        self.cache = {}        self.cache_opt = {}        self.k_cache = 0        self.k_cache_curr = 0        self.k_evals = 0        self.k_evals_curr = 0        self.t_evals = 0.        self.t_total = 0.        self.t_minim = 0        self._opt = None        self.i_opt = None        self.x_opt = None        self.I_list = []        self.i_opt_list = []        self.x_opt_list = []        self.y_opt_list = []        self.opt_opt_list = []        self.evals_opt_list = []        self.cache_opt_list = []    def qtt_parse_many(self, I_qtt):        samples = I_qtt.shape[0]        n_qtt = [self.n[0]]*self.q        I = np.zeros((samples, self.d))        for i in range(self.d):            J_curr = I_qtt[:, self.q*i:self.q*(i+1)].T            I[:, i] = np.ravel_multi_index(J_curr, n_qtt, order='F')        return I    def i2x(self, i):        t = i * 1. / (self.n_func - 1)        x = t * (self.b - self.a) + self.a        return x    def i2x_many(self, I):        A = np.repeat(self.a.reshape((1, -1)), I.shape[0], axis=0)        B = np.repeat(self.b.reshape((1, -1)), I.shape[0], axis=0)        N = np.repeat(self.n_func.reshape((1, -1)), I.shape[0], axis=0)        T = I * 1. / (N - 1)        X = T * (B - A) + A        return X    def optimize(self, rank=4, Y0=None, seed=42, fs_opt=1., is_max=False,                 add_opt_inner=True, add_opt_outer=False, add_opt_rect=False,                 add_rnd_inner=False, add_rnd_outer=False, J0=None):        t_minim = tpc()        self.is_max = is_max        i_opt, y_opt = ttopt(self.comp_opt, self.n, rank, None, Y0, seed,                fs_opt, add_opt_inner, add_opt_outer, add_opt_rect,                add_rnd_inner, add_rnd_outer, J0, is_max)        self.t_minim = tpc() - t_minim    def comp_opt(self, I, i_opt=None, y_opt=None, opt_opt=None):        if self.evals is not None and self.k_evals >= self.evals:            return None, None        if self.with_cache:            if self.k_cache >= self.evals and self.k_cache >= 2 * self.k_evals:                text = '!!! TTOpt warning : '                text += 'the number of requests to the cache is 2 times higher '                text += 'than the number of requests to the function. '                text += 'The work is finished before max func-evals reached.'                if self.with_wrn:                    print(text)                return None, None        eval_curr = I.shape[0]        is_last = self.evals is not None and self.k_evals+eval_curr>=self.evals        if is_last:            I = I[:(self.evals-self.k_evals), :]        if self.q:            if I is not None:                I = self.qtt_parse_many(I)            if i_opt is not None:                i_opt = self.qtt_parse_many(i_opt.reshape(1, -1))[0, :]        Y = self.comp(I)        if is_last:            i_opt, y_opt, opt_opt = ttopt_find(                I, Y, self._opt, i_opt, y_opt, opt_opt, self.is_max)        if i_opt is None:            return Y, self._opt        if self.is_func:            x_opt = self.i2x(i_opt)        else:            x_opt = i_opt.copy()        self.i_opt = i_opt.copy()        self.x_opt = x_opt.copy()        self.y_opt_list.append(y_opt)        self.opt_opt_list.append(opt_opt)        self.evals_opt_list.append(self.k_evals_curr)        self.cache_opt_list.append(self.k_cache_curr)        if self.with_full_info:            self.I_list.append(I)            self.i_opt_list.append(self.i_opt.copy())            self.x_opt_list.append(self.x_opt.copy())        if self.is_max:            is_better = len(self.y_opt_list)==1 or (y_opt > self.y_opt_list[-2])        else:            is_better = len(self.y_opt_list)==1 or (y_opt < self.y_opt_list[-2])        if self.callback and is_better:            last = {'last': [x_opt, y_opt, i_opt, opt_opt, self.k_evals]}            self.callback(last)        if self.with_log:            print(self.info(is_final=False))        return Y, self._optdef ttopt(f, n, rank=4, evals=None, Y0=None, seed=42, fs_opt=1.,          add_opt_inner=True, add_opt_outer=False, add_opt_rect=False,          add_rnd_inner=False, add_rnd_outer=False, J0=None, is_max=False):    d = len(n)    evals = int(evals) if evals else None    Jg_list = [np.reshape(np.arange(k), (-1, 1)) for k in n]    if J0 is None:        Y0, r = ttopt_init(n, rank, Y0, seed, with_rank=True)        J_list = [None] * (d + 1)        for i in range(d - 1):            J_list[i+1] = _iter(Y0[i], J_list[i], Jg_list[i], l2r=True)    else:        J_list = J0        r = [1] + [J.shape[0] for J in J_list[1:-1]] + [1]        for i in range(1, d):            r[i] = min(rank, n[i-1] * r[i-1])    i_opt = None    y_opt = None    opt_opt = None    eval = 0    iter = 0    i = d - 1    l2r = False    while True:        I = _merge(J_list[i], J_list[i+1], Jg_list[i])        eval_curr = I.shape[0]        if evals is not None and eval + eval_curr > evals:            I = I[:(evals-eval), :]        y, opt = f(I, i_opt, y_opt, opt_opt)        if y is None:            return i_opt, y_opt        i_opt, y_opt, opt_opt = ttopt_find(I, y, opt, i_opt, y_opt, opt_opt,            is_max)        eval += y.size        if evals is not None and eval >= evals:            return i_opt, y_opt        if y.shape[0] < I.shape[0]:            return i_opt, y_opt        Z = _reshape(y, (r[i], n[i], r[i + 1]))        if not is_max:            Z = ttopt_fs(Z, y_opt, fs_opt)        if l2r and i < d - 1:            J_list[i+1] = _iter(Z, J_list[i], Jg_list[i], l2r,                add_opt_inner, add_opt_rect, add_rnd_inner)            if add_opt_outer:                J_list[i+1] = _add_row(J_list[i+1], i_opt[:(i+1)])            if add_rnd_outer:                J_list[i+1] = _add_random(J_list[i+1], n[:(i+1)])            r[i+1] = J_list[i+1].shape[0]        if not l2r and i > 0:            J_list[i] = _iter(Z, J_list[i+1], Jg_list[i], l2r,                add_opt_inner, add_opt_rect, add_rnd_inner)            if add_opt_outer:                J_list[i] = _add_row(J_list[i], i_opt[i:])            if add_rnd_outer:                J_list[i] = _add_random(J_list[i], n[i:])            r[i] = J_list[i].shape[0]        i, iter, l2r = _update_iter(d, i, iter, l2r)    return i_opt, y_optdef ttopt_find(I, y, opt, i_opt, y_opt, opt_opt, is_max=False):    if is_max:        ind = np.argmax(y)    else:        ind = np.argmin(y)    y_opt_curr = y[ind]    if is_max and y_opt is not None and y_opt_curr <= y_opt:        return i_opt, y_opt, opt_opt    if not is_max and y_opt is not None and y_opt_curr >= y_opt:        return i_opt, y_opt, opt_opt    return I[ind, :], y_opt_curr, opt[ind]def ttopt_fs(y, y0=0., opt=1.):    if opt is None or opt == 0:        return np.pi/2 - np.arctan(y - y0)    else:        return np.exp(opt * (y0 - y))def _iter(Z, J, Jg, l2r=True, add_opt_inner=True, add_opt_rect=False,          add_rnd_inner=False):    r1, n, r2 = Z.shape    Z = _reshape(Z, (r1 * n, r2)) if l2r else _reshape(Z, (r1, n * r2)).T    Q, R = np.linalg.qr(Z)    ind = _maxvol(Q, is_rect=add_opt_rect)    if add_opt_inner:        i_max, j_max = np.divmod(np.abs(Z).argmax(), Z.shape[1])        if not i_max in ind:            ind[-1] = i_max    if add_rnd_inner and len(ind) > 1:        i_rnd = np.random.choice(Z.shape[0])        if not i_rnd in ind:            ind[-2] = i_rnd    J_new = _stack(J, Jg, l2r)    J_new = J_new[ind, :]    return J_newdef _maxvol(A, tol=1.001, max_iters=1000, is_rect=False):    n, r = A.shape    if n <= r:        return np.arange(n, dtype=int)    if is_rect:        return maxvol_rect(A, e=1., dr_min=1, dr_max=2)[0]    else:        return maxvol(A, e=tol, k=max_iters)[0]def maxvol(A, e=1.05, k=100):    n, r = A.shape    if n <= r:        raise ValueError('Input matrix should be \"tall\"')    P, L, U = lu(A, check_finite=False)    I = P[:, :r].argmax(axis=0)    Q = solve_triangular(U, A.T, trans=1, check_finite=False)    B = solve_triangular(L[:r, :], Q, trans=1, check_finite=False,        unit_diagonal=True, lower=True).T    for _ in range(k):        i, j = np.divmod(np.abs(B).argmax(), r)        if np.abs(B[i, j]) <= e:            break        I[j] = i        bj = B[:, j]        bi = B[i, :].copy()        bi[j] -= 1.        B -= np.outer(bj, bi / B[i, j])    return I, Bdef maxvol_rect(A, e=1.1, dr_min=0, dr_max=None, e0=1.05, k0=10):    n, r = A.shape    r_min = r + dr_min    r_max = r + dr_max if dr_max is not None else n    r_max = min(r_max, n)    if r_min < r or r_min > r_max or r_max > n:        raise ValueError('Invalid minimum/maximum number of added rows')    I0, B = maxvol(A, e0, k0)    I = np.hstack([I0, np.zeros(r_max-r, dtype=I0.dtype)])    S = np.ones(n, dtype=int)    S[I0] = 0    F = S * np.linalg.norm(B, axis=1)**2    for k in range(r, r_max):        i = np.argmax(F)        if k >= r_min and F[i] <= e*e:            break        I[k] = i        S[i] = 0        v = B.dot(B[i])        l = 1. / (1 + v[i])        B = np.hstack([B - l * np.outer(v, B[i]), l * v.reshape(-1, 1)])        F = S * (F - l * v * v)    I = I[:B.shape[1]]    B[I] = np.eye(B.shape[1], dtype=B.dtype)    return I, B",
    "Experiment Result": "The TTOpt method's performance is investigated across various analytical benchmark functions and settings. The experiments cover the following aspects:  **Benchmark Functions:** - Ackley, Alpine, Brown, Exponential, Grienwank, Michalewicz, Qing, Rastrigin, Schaffer, Schwefel, Rosenbrock.  **Dimensionality (d):** - Evaluated for `d` values: 2, 10, 50, 100, 500.  **Grid Discretization:** - Direct TT-grid sizes (`n`): Tested with `n` ranging from `2^8` to `2^20`. - Quantized Tensor Train (QTT) grid settings: `p=2` (base 2) with `q` from 12 to 25; `p=4` (base 4) for `q` calculated as `np.log4(n)`.  **Maximum TT-Rank (r):** - Investigated ranks `r` from 1 to 10.  **Computational Budget (Function Evaluations):** - Total function calls (`evals` or `m`) ranging from `1.E+4` to `1.E+7`.  **Repetitions:** - Experiments are typically run for 1 to 10 repetitions (`reps`) to account for randomness.  **Optimization Strategies & Maxvol Enhancements:** - Smoothing function parameter (`fs_opt`): Tested with values `None`, `1000.`, `100.`, `10.`, `1.`, `0.1`, `0.01`. `None` uses `arctan`, other values use `exp` based smoothing. - Maxvol algorithm options (flags for adding points):    - `add_opt_inner`: Add optimal inner point.    - `add_opt_outer`: Add optimal outer point.    - `add_opt_rect`: Use `rect_maxvol` for adaptive rank selection.    - `add_rnd_inner`: Add random inner point.    - `add_rnd_outer`: Add random outer point.  **Comparisons with Other Optimizers:** - The method is compared against several non-TT optimizers including: GA (Genetic Algorithm), OpenES (OpenAI Evolution Strategies), CMAES (Covariance Matrix Adaptation Evolution Strategy), DE (Differential Evolution), NB (NoisyBandit), PSO (Particle Swarm Optimization), and gradient-based methods like BFGS, L-BFGS, CG, Newton-CG, Newton-Exact, Trust-Region NCG, Trust-Region Exact.  **Cache Usage:** - Experiments conducted both with and without caching (`with_cache=True/False`) to evaluate its impact.  **Initialization:** - Random TT-tensor initialization (default). - Specific initial multi-indices (`J0`) for tensor problems. - Random uniform `x0` (using `np.random.default_rng(12345)` or `torch.manual_seed(rep)`) for other optimizers.  **Objective:** - Both minimization (default) and maximization (`is_max=True`) tasks are performed.  **Logging and Visualization:** - Animation files (.gif) are generated to visualize the optimization process for 2D functions. - Detailed logs and plots (e.g., error vs. queries, error vs. rank, error vs. grid size) are generated to show results."
}{
    "Title": "Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time",
    "Main Contributions": "The paper addresses the problem of accelerating Hamiltonian Monte Carlo (HMC) by optimizing its integration time. It proposes a novel scheme of time-varying integration time based on the roots of Chebyshev polynomials. The main technical contribution is a provable acceleration guarantee for ideal HMC when sampling from a Gaussian distribution (i.e., with a quadratic potential). This scheme achieves a Wasserstein-2 distance convergence rate of O(√κlog(1/ϵ)) iterations, an improvement over the O(κlog(1/ϵ)) rate achieved with a constant integration time, akin to acceleration in optimization. The paper also provides experimental evidence demonstrating the advantage of this Chebyshev integration time scheme even for sampling from non-quadratic smooth strongly convex potentials.",
    "Methodology": "The core methodology involves designing a time-varying integration time (ηk) for HMC based on the roots of scaled-and-shifted Chebyshev polynomials. For ideal HMC, the convergence rate's dependency on the cosine product term (maxj∈[d] |ΠKk=1cos(√2λjη(K)k)|) is bounded by Chebyshev polynomial properties (Lemma 4 and 3) to show the accelerated rate. For practical applications with general strongly log-concave distributions, Algorithm 2 is introduced, which uses the Verlet integrator (leapfrog steps) to simulate the Hamiltonian flow and a Metropolis-Hastings filter to correct any induced bias. The integration time for each iteration is determined by the total number of iterations (K), the current iteration index (k), and the strong convexity (m) and smoothness (L) constants of the potential function. An arbitrary permutation of the Chebyshev integration times is also suggested for empirical performance improvement.",
    "Experimental Setup": "The experimental evaluation compares HMC using the proposed Chebyshev integration time (Algorithm 2) against HMC with a constant integration time (Algorithm 2 with line 7 replaced by equation (5)). All experiments involve collecting K = 10,000 samples. The leapfrog step size (θ) is varied across {0.001, 0.005, 0.01, 0.05, 0.1}. Performance metrics include Mean ESS (average Effective Sample Size of all variables), Min ESS (lowest ESS among all variables), and their respective normalized versions by CPU time (Mean ESS/Sec and Min ESS/Sec), as well as the Metropolis filter's acceptance probability (Acc. Prob). The methods are evaluated on several tasks: 1) Ideal HMC flow for a 2D Gaussian distribution, 2) Sampling from a 2D non-diagonal Gaussian distribution, 3) Sampling from a mixture of two Gaussians (d=10), 4) Bayesian logistic regression on Heart, Breast Cancer, and Diabetes datasets, and 5) Sampling from a 'hard' distribution (κ-smooth, 1-strongly convex, d=10). Estimates for m and L are obtained from eigenvalues of relevant matrices (e.g., covariance inverse for Gaussians, Hessian for other potentials).",
    "Limitations": "The primary limitation is that the theoretical guarantee of accelerated convergence (O(√κlog(1/ϵ))) provided in the paper is strictly proven only for strongly convex quadratic potentials (i.e., Gaussian target distributions). For general strongly log-concave distributions, a closed-form solution for the HMC flow is not available, making theoretical analysis significantly more challenging. Additionally, extending existing contraction bounds, which often rely on small integration times, to the large integration times required by the Chebyshev scheme (Θ(1/√m)) poses a significant analytical challenge.",
    "Future Research Directions": "The authors suggest several future research avenues: 1) Establishing provable acceleration guarantees for HMC with time-varying integration times for general smooth strongly log-concave distributions, extending beyond the current quadratic potential results. 2) Improving the analytical tools, specifically extending existing contraction bounds for HMC (e.g., from Chen & Vempala (2019)) to accommodate the large integration times (Θ(1/√m)) used in the Chebyshev scheme. 3) Exploring potential connections between the proposed Chebyshev integration time scheme and other recent works on accelerating HMC, such as the randomized integration time with partial velocity refreshment proposed by Jiang (2022).",
    "Experiment Code": null,
    "Experiment Result": null
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'NGFAT replaces heuristic first-order (TENT/FATENT) or heavy KFAC natural-gradient updates with a closed-form 2×2 Fisher inversion per BN channel that is computed in the forward pass, needs no learning-rate, and is coupled with a Polyak line–search guaranteeing monotone entropy decrease. This eliminates all backward passes and manual (α,β,ρ) tuning, a combination that has not appeared in prior TTA literature.', 'novelty_score': 9, 'significance_reason': 'By converging in a single test batch and requiring only forward inference, NGFAT cuts wall-clock latency by 55 % on edge CPUs (25 % on GPUs) while matching or slightly surpassing FATENT accuracy and remaining stable without hyper-parameter tuning across CIFAR-C, ImageNet-C and realistic online streams. This directly addresses the speed bottlenecks highlighted by recent evaluation protocols and pushes TTA towards micro-controller deployment.', 'significance_score': 8}
