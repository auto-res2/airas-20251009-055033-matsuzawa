{
  "research_topic": "Improve Test-Time Adaptation in terms of convergence speed.",
  "queries": [
    "fast test-time adaptation",
    "test-time adaptation convergence",
    "unsupervised TTA acceleration",
    "gradient-based adaptation speed",
    "meta-learning test-time adaptation speed",
    "TENT convergence acceleration"
  ],
  "research_study_list": [
    {
      "title": "Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection"
    },
    {
      "title": "Tent: Fully Test-Time Adaptation by Entropy Minimization",
      "abstract": "A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.",
      "full_text": "Published as a conference paper at ICLR 2021 TENT : F ULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION Dequan Wang1∗, Evan Shelhamer2∗†, Shaoteng Liu1, Bruno Olshausen1, Trevor Darrell1 dqwang@cs.berkeley.edu, shelhamer@google.com UC Berkeley1 Adobe Research2 ABSTRACT A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent 1): we optimize the model for conﬁdence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise afﬁne transformations to update online on each batch. Tent reduces generalization error for image classiﬁcation on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adapta- tion on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training. 1 I NTRODUCTION Deep networks can achieve high accuracy on training and testing data from the same distribution, as evidenced by tremendous benchmark progress (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016). However, generalization to new and different data is limited (Hendrycks & Dietterich, 2019; Recht et al., 2019; Geirhos et al., 2018). Accuracy suffers when the training (source) data differ from the testing (target) data, a condition known as dataset shift(Quionero-Candela et al., 2009). Models can be sensitive to shifts during testing that were not known during training, whether natural variations or corruptions, such as unexpected weather or sensor degradation. Nevertheless, it can be necessary to deploy a model on different data distributions, so adaptation is needed. During testing, the model must adapt given only its parameters and the target data. Thisfully test-time adaptation setting cannot rely on source data or supervision. Neither is practical when the model ﬁrst encounters new testing data, before it can be collected and annotated, as inference must go on. Real-world usage motivates fully test-time adaptation by data, computation, and task needs: 1. Availability. A model might be distributed without source data for bandwidth, privacy, or proﬁt. 2. Efﬁciency. It might not be computationally practical to (re-)process source data during testing. 3. Accuracy. A model might be too inaccurate without adaptation to serve its purpose. To adapt during testing we minimize the entropy of model predictions. We call this objective the test entropy and name our method tent after it. We choose entropy for its connections to error and shift. Entropy is related to error, as more conﬁdent predictions are all-in-all more correct (Figure 1). Entropy is related to shifts due to corruption, as more corruption results in more entropy, with a strong rank correlation to the loss for image classiﬁcation as the level of corruption increases (Figure 2). To minimize entropy, tent normalizes and transforms inference on target data by estimating statistics and optimizing afﬁne parameters batch-by-batch. This choice of low-dimensional, channel-wise feature modulation is efﬁcient to adapt during testing, even for online updates. Tent does not restrict or alter model training: it is independent of the source data given the model parameters. If the model can be run, it can be adapted. Most importantly, tent effectively reduces not just entropy but error. ∗Equal contribution. †Work done at Adobe Research; the author is now at DeepMind. 1Please see the project page at https://github.com/DequanWang/tent for the code and more. 1 arXiv:2006.10726v3  [cs.LG]  18 Mar 2021Published as a conference paper at ICLR 2021 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Entropy 0 20 40 60 80Error (%) Figure 1: Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing. 0.2 0.3 0.4 0.5 0.6 Entropy 0.2 0.4 0.6 0.8 1.0 1.2Loss = 0.61 original noise blur digital weather  level level Figure 2: More corruption causes more loss and entropy on CIFAR-100-C. Entropy can estimate the degree of shift without training data or labels. Our results evaluate generalization to corruptions for image classiﬁcation, to domain shift for digit recognition, and to simulation-to-real shift for semantic segmentation. For context with more data and optimization, we evaluate methods for robust training, domain adaptation, and self-supervised learning given the labeled source data. Tent can achieve less error given only the target data, and it improves on the state-of-the-art for the ImageNet-C benchmark. Analysis experiments support our entropy objective, check sensitivity to the amount of data and the choice of parameters for adaptation, and back the generality of tent across architectures. Our contributions • We highlight the setting of fully test-time adaptation with only target data and no source data. To emphasize practical adaptation during inference we benchmark with ofﬂine and online updates. • We examine entropy as an adaptation objective and propose tent: a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. • For robustness to corruptions, tent reaches 44.0% error on ImageNet-C, better than the state-of- the-art for robust training (50.2%) and the strong baseline of test-time normalization (49.9%). • For domain adaptation, tent is capable of online and source-free adaptation for digit classiﬁcation and semantic segmentation, and can even rival methods that use source data and more optimization. 2 S ETTING : F ULLY TEST-TIME ADAPTATION Adaptation addresses generalization from source to target. A model fθ(x) with parameters θtrained on source data and labels xs,ys may not generalize when tested on shifted target data xt. Table 1 summarizes adaptation settings, their required data, and types of losses. Our fully test-time adaptation setting uniquely requires only the model fθ and unlabeled target data xt for adaptation during inference. Existing adaptation settings extend training given more data and supervision. Transfer learning by ﬁne-tuning (Donahue et al., 2014; Yosinski et al., 2014) needs target labels to (re-)train with a supervised loss L(xt,yt). Without target labels, our setting denies this supervised training. Domain adaptation (DA) (Quionero-Candela et al., 2009; Saenko et al., 2010; Ganin & Lempitsky, 2015; Tzeng et al., 2015) needs both the source and target data to train with a cross-domain loss L(xs,xt). Test-time training (TTT) (Sun et al., 2019b) adapts during testing but ﬁrst alters training to jointly optimize its supervised loss L(xs,ys) and self-supervised loss L(xs). Without source, our setting denies joint training across domains (DA) or losses (TTT). Existing settings have their purposes, but do not cover all practical cases when source, target, or supervision are not simultaneously available. Unexpected target data during testing requires test-time adaptation. TTT and our setting adapt the model by optimizing an unsupervised loss during testing L(xt). During training, TTT jointly optimizes this same loss on source data L(xs) with a supervised loss L(xs,ys), to ensure the parameters θare shared across losses for compatibility with adaptation by L(xt). Fully test-time adaptation is independent of the training data and training loss given the parameters θ. By not changing training, our setting has the potential to require less data and computation for adaptation. 2Published as a conference paper at ICLR 2021 Table 1: Adaptation settings differ by their data and therefore losses during training and testing. Of the source s and target t data xand labels y, our fully test-time setting only needs the target data xt. setting source data target data train loss test loss ﬁne-tuning - xt,yt L(xt,yt) - domain adaptation xs, ys xt L(xs,ys) + L(xs,xt) - test-time training xs, ys xt L(xs,ys) + L(xs) L(xt) fully test-time adaptation - xt - L(xt)     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t     = f (     ; θ)  Loss (   ,      ) θ (a) training <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"1psCF/1OyjuZ4TwBu21voNG0XaI=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8eK1hbaWDbbSbt0swm7GyGE/gQvHhTEq3/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gQcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj66nffkKleSzvTZagH9Gh5CFn1FjpLnvU/WrNrbszkGXiFaQGBZr96ldvELM0QmmYoFp3PTcxfk6V4UzgpNJLNSaUjekQu5ZKGqH289mpE3JilQEJY2VLGjJTf0/kNNI6iwLbGVEz0oveVPzP66YmvPRzLpPUoGTzRWEqiInJ9G8y4AqZEZkllClubyVsRBVlxqZTsSF4iy8vk/ZZ3Tuve97tea1xVeRRhiM4hlPw4AIacANNaAGDITzDK7w5wnlx3p2PeWvJKWYO4Q+czx8GrY4b</latexit> y s <latexit sha1_base64=\"XlB1POiMMxMFBsxKqM8k7anLbzY=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8mj61Zpbd2cgy8QrSA0KtPrVr94gZmnEFTJJjel6boJ+TjUKJvmk0ksNTygb0yHvWqpoxI2fzw6ekBOrDEgYa1sKyUz9PZHTyJgsCmxnRHFkFr2p+J/XTTG88HOhkhS5YvNFYSoJxmT6PRkIzRnKzBLKtLC3EjaimjK0GVVsCN7iy8ukc1b3GnXPu2nUmpdFHmU4gmM4BQ/OoQnX0II2MIjgGV7hzdHOi/PufMxbS04xcwh/4Hz+ANhzkOg=</latexit> ˆy s <latexit sha1_base64=\"YDIW6Mi/jc4gnv843zXedRTilJU=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49mF654lbdGcgy8XJSgRyNXvmr249ZGnGFTFJjOp6boJ9RjYJJPil1U8MTykZ0wDuWKhpx42ezUyfkxCp9EsbalkIyU39PZDQyZhwFtjOiODSL3lT8z+ukGF74mVBJilyx+aIwlQRjMv2b9IXmDOXYEsq0sLcSNqSaMrTplGwI3uLLy6R1VvVqVc+7qVXql3keRTiCYzgFD86hDtfQgCYwGMAzvMKbI50X5935mLcWnHzmEP7A+fwBBSaOGg==</latexit> x s (b) fully test-time adaptation θ    = f (    ; θ+Δ)  Entropy (    ) <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t <latexit sha1_base64=\"m/ZzdjACtPk7VVv8qMUMxHkn5f0=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrS20sWy2m3bpZhN2J2IJ/QlePCiIV/+QN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgorq2vrG8XN0tb2zu5eef/g3sSpZrzJYhnrdkANl0LxJgqUvJ1oTqNA8lYwupr6rUeujYjVHY4T7kd0oEQoGEUr3T49YK9ccavuDGSZeDmpQI5Gr/zV7ccsjbhCJqkxHc9N0M+oRsEkn5S6qeEJZSM64B1LFY248bPZqRNyYpU+CWNtSyGZqb8nMhoZM44C2xlRHJpFbyr+53VSDC/8TKgkRa7YfFGYSoIxmf5N+kJzhnJsCWVa2FsJG1JNGdp0SjYEb/HlZdI6q3q1qufd1Cr1yzyPIhzBMZyCB+dQh2toQBMYDOAZXuHNkc6L8+58zFsLTj5zCH/gfP4ABquOGw==</latexit> x t <latexit sha1_base64=\"uzCfvi+otYu2ihjbD7PaMp0JG7Y=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoMeiF48VrFXaWDbbTbt0swm7EyGE/govHhTEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O6WV1bX1jfJmZWt7Z3evun9wZ+JUM95msYz1fUANl0LxNgqU/D7RnEaB5J1gfDX1O09cGxGrW8wS7kd0qEQoGEUrPfRGFPNs8oj9as2tuzOQZeIVpAYFWv3qV28QszTiCpmkxnQ9N0E/pxoFk3xS6aWGJ5SN6ZB3LVU04sbPZwdPyIlVBiSMtS2FZKb+nshpZEwWBbYzojgyi95U/M/rphhe+LlQSYpcsfmiMJUEYzL9ngyE5gxlZgllWthbCRtRTRnajCo2BG/x5WXSOat7jbrn3TRqzcsijzIcwTGcggfn0IRraEEbGETwDK/w5mjnxXl3PuatJaeYOYQ/cD5/ANn4kOk=</latexit> ˆy t Figure 3: Method overview. Tent does not alter training (a), but minimizes the entropy of predictions during testing (b) over a constrained modulation ∆, given the parameters θand target data xt. 3 M ETHOD : T EST ENTROPY MINIMIZATION VIA FEATURE MODULATION We optimize the model during testing to minimize the entropy of its predictions by modulating its features. We call our method tent for test entropy. Tent requires a compatible model, an objective to minimize (Section 3.1), and parameters to optimize over (Section 3.2) to fully deﬁne the algorithm (Section Section 3.3). Figure 3 outlines our method for fully test-time adaptation. The model to be adapted must be trained for the supervised task, probabilistic, and differentiable. No supervision is provided during testing, so the model must already be trained. Measuring the entropy of predictions requires a distribution over predictions, so the model must be probabilistic. Gradients are required for fast iterative optimization, so the model must be differentiable. Typical deep networks for supervised learning satisfy these model requirements. 3.1 E NTROPY OBJECTIVE Our test-time objective L(xt) is to minimize the entropy H(ˆy) of model predictions ˆy= fθ(xt). In particular, we measure the Shannon entropy (Shannon, 1948), H(ˆy) = −∑ cp(ˆyc) logp(ˆyc) for the probability ˆyc of class c. Note that optimizing a single prediction has a trivial solution: assign all probability to the most probable class. We prevent this by jointly optimizing batched predictions over parameters that are shared across the batch. Entropy is an unsupervised objective because it only depends on predictions and not annotations. However, as a measure of the predictions it is directly related to the supervised task and model. In contrast, proxy tasks for self-supervised learning are not directly related to the supervised task. Proxy tasks derive a self-supervised label y′from the input xt without the task label y. Examples of these proxies include rotation prediction (Gidaris et al., 2018), context prediction (Doersch et al., 2015), and cross-channel auto-encoding (Zhang et al., 2017). Too much progress on a proxy task could interfere with performance on the supervised task, and self-supervised adaptation methods have to limit or mix updates accordingly (Sun et al., 2019b;a). As such, care is needed to choose a proxy compatible with the domain and task, to design the architecture for the proxy model, and to balance optimization between the task and proxy objectives. Our entropy objective does not need such efforts. 3.2 M ODULATION PARAMETERS The model parameters θare a natural choice for test-time optimization, and these are the choice of prior work for train-time entropy minimization (Grandvalet & Bengio, 2005; Dhillon et al., 2020; Carlucci et al., 2017). However, θis the only representation of the training/source data in our setting, and altering θcould cause the model to diverge from its training. Furthermore, f can be nonlinear and θcan be high dimensional, making optimization too sensitive and inefﬁcient for test-time usage. 3Published as a conference paper at ICLR 2021 IN OUT+ <latexit sha1_base64=\"FGMSn1olAms3UkJ+mUM6lRBkJrw=\">AAAB6HicbVDLSgNBEOyNryS+oh69DAZBEMKuKHoMevGYgHlgsoTZSW8yZvbBzKwYlnyBFw+K5OoP+C/e/BqdJB40saChqOqmu8uLBVfatj+tzNLyyupaNpdf39jc2i7s7NZVlEiGNRaJSDY9qlDwEGuaa4HNWCINPIENb3A18Rv3KBWPwhs9jNENaC/kPmdUG6l63CkU7ZI9BVkkzg8plnPx+Pb94avSKXy0uxFLAgw1E1SplmPH2k2p1JwJHOXbicKYsgHtYcvQkAao3HR66IgcGqVL/EiaCjWZqr8nUhooNQw80xlQ3Vfz3kT8z2sl2r9wUx7GicaQzRb5iSA6IpOvSZdLZFoMDaFMcnMrYX0qKdMmm7wJwZl/eZHUT0rOaemsatK4hBmysA8HcAQOnEMZrqECNWCA8AjP8GLdWU/WqzWetWasn5k9+APr7RuTUJCF</latexit> \u0000 <latexit sha1_base64=\"8eHH7cr25vA7s0zJYYCDPQNSaT0=\">AAAB7XicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhMwDwgWcLsZDYZM7OzzMwKYcnRuxcPinj1F/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk3LRBFaJZJL1QiwppxFtGqY4bQRK4pFwGk96N+M/foDVZrJ6M4MYuoL3I1YyAg2Vqq1ulgI3M4X3KI7AVok3owUSoejyvfj0ajczn+2OpIkgkaGcKx103Nj46dYGUY4HeZaiaYxJn3cpU1LIyyo9tPJtUN0YpUOCqWyFRk0UX9PpFhoPRCB7RTY9PS8Nxb/85qJCS/9lEVxYmhEpovChCMj0fh11GGKEsMHlmCimL0VkR5WmBgbUM6G4M2/vEhqZ0XvvHhVsWlcwxRZOIBjOAUPLqAEt1CGKhC4hyd4gVdHOs/Om/M+bc04s5l9+APn4wd3ypLI</latexit> ⇥ <latexit sha1_base64=\"r9CoIRh1LwyAxszWUWZZpZEIYvU=\">AAAB7XicbVA9TwJBEJ3DL8Av1NLmIjGxIndGoyXRxhIT+YhwIXvLHqzs7V5254yE8B9sLDDG1tL/Yuev0QUsFHzJJC/vzWRmXpgIbtDzPp3M0vLK6lo2l1/f2NzaLuzs1oxKNWVVqoTSjZAYJrhkVeQoWCPRjMShYPWwfznx6/dMG67kDQ4SFsSkK3nEKUEr1VrIY2bahaJX8qZwF4n/Q4rlXDK+fX/4qrQLH62OomnMJFJBjGn6XoLBkGjkVLBRvpUalhDaJ13WtFQSuyQYTq8duYdW6biR0rYkulP198SQxMYM4tB2xgR7Zt6biP95zRSj82DIZZIik3S2KEqFi8qdvO52uGYUxcASQjW3t7q0RzShaAPK2xD8+ZcXSe245J+UTq9tGhcwQxb24QCOwIczKMMVVKAKFO7gEcbw7CjnyXlxXmetGednZg/+wHn7Btf2kwo=</latexit> \u0000 <latexit sha1_base64=\"icKTvSnYuWAwxCN4MXaVcPxJrUE=\">AAAB7HicbVBNS8NAEN34WetX1aMiwSJ4KokI6q3oxWMLpi20oWy2k3bpZhN2J0IJPXr24kERr/6G/g5v/gb/hNuPg7Y+GHi8N8PMvCARXKPjfFlLyyura+u5jfzm1vbObmFvv6bjVDHwWCxi1QioBsEleMhRQCNRQKNAQD3o3479+gMozWN5j4ME/Ih2JQ85o2gkrxUA0nah6JScCexF4s5IsXw0qn4/Ho8q7cJnqxOzNAKJTFCtm66ToJ9RhZwJGOZbqYaEsj7tQtNQSSPQfjY5dmifGqVjh7EyJdGeqL8nMhppPYgC0xlR7Ol5byz+5zVTDK/8jMskRZBsuihMhY2xPf7c7nAFDMXAEMoUN7farEcVZWjyyZsQ3PmXF0ntvORelK6rJo0bMkWOHJITckZccknK5I5UiEcY4eSJvJBXS1rP1pv1Pm1dsmYzB+QPrI8ftLWSVw==</latexit> \u0000 <latexit sha1_base64=\"6pSYsGji0D9Bm0vY9by0e43+pZo=\">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgxbArAfUW9OIxAfOAZAmzk95kzOzsMjMrhJAv8OJBEa9+kjf/xkmyB00saCiquunuChLBtXHdbye3tr6xuZXfLuzs7u0fFA+PmjpOFcMGi0Ws2gHVKLjEhuFGYDtRSKNAYCsY3c381hMqzWP5YMYJ+hEdSB5yRo2V6he9Ysktu3OQVeJlpAQZar3iV7cfszRCaZigWnc8NzH+hCrDmcBpoZtqTCgb0QF2LJU0Qu1P5odOyZlV+iSMlS1pyFz9PTGhkdbjKLCdETVDvezNxP+8TmrCa3/CZZIalGyxKEwFMTGZfU36XCEzYmwJZYrbWwkbUkWZsdkUbAje8surpHlZ9irlm3qlVL3N4sjDCZzCOXhwBVW4hxo0gAHCM7zCm/PovDjvzseiNedkM8fwB87nD3htjL0=</latexit> ÷ <latexit sha1_base64=\"KLNiQjydwC+UjsLtIanox9T+rq8=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoN6KXjxWsB/QhrLZbNqlu5uwuymU0L/gxYMiXv1D3vw3btoctPXBwOO9GWbmBQln2rjut1Pa2Nza3invVvb2Dw6PqscnHR2nitA2iXmsegHWlDNJ24YZTnuJolgEnHaDyX3ud6dUaRbLJzNLqC/wSLKIEWxyaRCy6bBac+vuAmideAWpQYHWsPo1CGOSCioN4Vjrvucmxs+wMoxwOq8MUk0TTCZ4RPuWSiyo9rPFrXN0YZUQRbGyJQ1aqL8nMiy0nonAdgpsxnrVy8X/vH5qohs/YzJJDZVkuShKOTIxyh9HIVOUGD6zBBPF7K2IjLHCxNh4KjYEb/XlddK5qnuN+u1jo9a8K+IowxmcwyV4cA1NeIAWtIHAGJ7hFd4c4bw4787HsrXkFDOn8AfO5w8aWY5N</latexit> µ <latexit sha1_base64=\"lbHwl5bkUbenc+Yo+u8yNzpxsy0=\">AAAB6nicbVDLSgNBEOyNrxhfUY+KDAbBU9gVQb0FvXhM0DwgWcLsZDYZMjO7zMwKYcnRoxcPinj1I/Id3vwGf8LJ46CJBQ1FVTfdXUHMmTau++VklpZXVtey67mNza3tnfzuXk1HiSK0SiIeqUaANeVM0qphhtNGrCgWAaf1oH8z9usPVGkWyXsziKkvcFeykBFsrHTXEkk7X3CL7gRokXgzUigdjirfj0ejcjv/2epEJBFUGsKx1k3PjY2fYmUY4XSYayWaxpj0cZc2LZVYUO2nk1OH6MQqHRRGypY0aKL+nkix0HogAtspsOnpeW8s/uc1ExNe+imTcWKoJNNFYcKRidD4b9RhihLDB5Zgopi9FZEeVpgYm07OhuDNv7xIamdF77x4VbFpXMMUWTiAYzgFDy6gBLdQhioQ6MITvMCrw51n5815n7ZmnNnMPvyB8/EDTj2RiQ==</latexit> \u0000 <latexit sha1_base64=\"xnrzB72KzfqBMQ17s1zlsxQWR+k=\">AAAB7XicbZDLSgMxFIbP1Fsdb1WXboJFcFVmRFAXYtGNywr2Au1QMmmmjU0yQ5IRytB3cONCETcufBT3bsS3Mb0stPWHwMf/n0POOWHCmTae9+3kFhaXllfyq+7a+sbmVmF7p6bjVBFaJTGPVSPEmnImadUww2kjURSLkNN62L8a5fV7qjSL5a0ZJDQQuCtZxAg21qq1NOsK3C4UvZI3FpoHfwrFiw/3PHn7civtwmerE5NUUGkIx1o3fS8xQYaVYYTTodtKNU0w6eMubVqUWFAdZONph+jAOh0Uxco+adDY/d2RYaH1QIS2UmDT07PZyPwva6YmOg0yJpPUUEkmH0UpRyZGo9VRhylKDB9YwEQxOysiPawwMfZArj2CP7vyPNSOSv5x6ezGK5YvYaI87ME+HIIPJ1CGa6hAFQjcwQM8wbMTO4/Oi/M6Kc05055d+CPn/Qf/xpJs</latexit> <latexit sha1_base64=\"9MzbukliF0G5U4WyINCTJmMNjA8=\">AAACNnicdVBNS8NAFNz4bf2KevSyWAQFLUlR9CiK4EWoYFuhiWWz3dSlu0nYfVFL6K/y4u/w1osHRbz6E9y0PWjVgYVhZh773gSJ4Bocp29NTE5Nz8zOzRcWFpeWV+zVtZqOU0VZlcYiVtcB0UzwiFWBg2DXiWJEBoLVg85p7tfvmNI8jq6gmzBfknbEQ04JGKlpX3gyxZ5gIRCl4nvsSQK3QZCd9RoPTfB3sad5W5Kb8j+h7Xx+D5vszk3Zb9pFp+QMgH8Td0SKaIRK0372WjFNJYuACqJ1w3US8DOigFPBegUv1SwhtEParGFoRCTTfjY4u4e3jNLCYazMiwAP1O8TGZFad2VgkvnCetzLxb+8RgrhkZ/xKEmBRXT4UZgKDDHOO8QtrhgF0TWEUMXNrpjeEkUomKYLpgR3/OTfpFYuuQcl53K/eHwyqmMObaBNtI1cdIiO0TmqoCqi6BH10St6s56sF+vd+hhGJ6zRzDr6AevzC4nRq7w=</latexit> µ  E [ x t ] , \u0000 2  E [( µ \u0000 x t ) 2 ] <latexit sha1_base64=\"5uCFLjsyhVlotMr43Rw1BdZFk0s=\">AAACYXicbZFLS+RAFIUrGZ/tK+Ms3RQ2gqC0iSgzy2bcuHTAVqHTNDfVN21hVRKqbmamCf0nZzcbN/4RKzH4vlBw+O659TiVFEpaCsP/nv9lYXFpeWW1s7a+sbkVfN2+snlpBA5ErnJzk4BFJTMckCSFN4VB0InC6+TurO5f/0ZjZZ5d0qzAkYZpJlMpgBwaB3/jKWgNPFaYEhiT/+EtOeBxAYYkqFgD3UqqzudHL6wxHfI4QXo73YBPh59RbRkH3bAXNsU/iqgVXdbWxTj4F09yUWrMSCiwdhiFBY2qekuhcN6JS4sFiDuY4tDJDDTaUdUkNOd7jkx4mhu3MuINfT1RgbZ2phPnrO9r3/dq+FlvWFL6Y1TJrCgJM/F0UFoqTjmv4+YTaVCQmjkBwkh3Vy5uwYAg9ykdF0L0/skfxdVxLzrthb9Ouv2fbRwrbIftsn0Wse+sz87ZBRswwe69BW/D2/Qe/FU/8LefrL7Xznxjb8rfeQSpH7dZ</latexit> \u0000  \u0000 + @ H / @\u0000 , \u0000  \u0000 + @ H / @\u0000 normalization transformation Figure 4: Tent modulates features during testing by estimating normalization statistics µ,σ and optimizing transformation parameters γ,β. Normalization and transformation apply channel-wise scales and shifts to the features. The statistics and parameters are updated on target data without use of source data. In practice, adapting γ,β is efﬁcient because they make up <1% of model parameters. For stability and efﬁciency, we instead only update feature modulations that are linear (scales and shifts), and low-dimensional (channel-wise). Figure 4 shows the two steps of our modulations: normalization by statistics and transformation by parameters. Normalization centers and standardizes the input xinto ¯x= (x−µ)/σby its mean µand standard deviation σ. Transformation turns ¯xinto the output x′= γ¯x+ βby afﬁne parameters for scale γand shift β. Note that the statistics µ,σ are estimated from the data while the parameters γ,β are optimized by the loss. For implementation, we simply repurpose the normalization layers of the source model. We update their normalization statistics and afﬁne parameters for all layers and channels during testing. 3.3 A LGORITHM Initialization The optimizer collects the afﬁne transformation parameters {γl,k,βl,k}for each normalization layer land channel kin the source model. The remaining parameters θ\\{γl,k,βl,k} are ﬁxed. The normalization statistics {µl,k,σl,k}from the source data are discarded. Iteration Each step updates the normalization statistics and transformation parameters on a batch of data. The normalization statistics are estimated for each layer in turn, during the forward pass. The transformation parameters γ,β are updated by the gradient of the prediction entropy ∇H(ˆy), during the backward pass. Note that the transformation update follows the prediction for the current batch, and so it only affects the next batch (unless forward is repeated). This needs just one gradient per point of additional computation, so we use this scheme by default for efﬁciency. Termination For online adaptation, no termination is necessary, and iteration continues as long as there is test data. For ofﬂine adaptation, the model is ﬁrst updated and then inference is repeated. Adaptation may of course continue by updating for multiple epochs. 4 E XPERIMENTS We evaluate tent for corruption robustness on CIFAR-10/CIFAR-100 and ImageNet, and for domain adaptation on digit adaptation from SVHN to MNIST/MNIST-M/USPS. Our implementation is in PyTorch (Paszke et al., 2019) with the pycls library (Radosavovic et al., 2019). Datasets We run on image classiﬁcation datasets for corruption and domain adaptation conditions. For large-scale experiments we choose ImageNet (Russakovsky et al., 2015), with 1,000 classes, a training set of 1.2 million, and a validation set of 50,000. For experiments at an accessible scale we choose CIFAR-10/CIFAR-100 (Krizhevsky, 2009), with 10/100 classes, a training set of 50,000, and a test set of 10,000. For domain adaptation we choose SVHN (Netzer et al., 2011) as source and MNIST (LeCun et al., 1998)/MNIST-M (Ganin & Lempitsky, 2015)/USPS (Hull, 1994) as targets, with ten classes for the digits 0–9. SVHN has color images of house numbers from street views with a training set of 73,257 and test set of 26,032. MNIST/MNIST-M/USPS have handwritten digits with a training sets of 60,000/60,000/7,291 and test sets of 10,000/10,000/2,007. Models For corruption we use residual networks (He et al., 2016) with 26 layers (R-26) on CIFAR- 10/100 and 50 layers (R-50) on ImageNet. For domain adaptation we use the R-26 architecture. For fair comparison, all methods in each experimental condition share the same architecture. Our networks are equipped with batch normalization (Ioffe & Szegedy, 2015). For the source model without adaptation, the normalization statistics are estimated during training on the source data. For all test-time adaptation methods, we estimate these statistics during testing on the target data, as done in concurrent work on adaptation by normalization (Schneider et al., 2020; Nado et al., 2020). 4Published as a conference paper at ICLR 2021 Table 2: Corruption benchmark on CIFAR-10-C and CIFAR-100-C for the highest severity. Tent has least error, with less optimization than domain adaptation (RG, UDA-SS) and test-time training (TTT), and improves on test-time norm (BN). Method Source Target Error (%) C10-C C100-C Source train 40.8 67.2 RG train train 18.3 38.9 UDA-SS train train 16.7 47.0 TTT train test 17.5 45.0 BN test 17.3 42.6 PL test 15.7 41.2 Tent (ours) test 14.3 37.3 originalgaussshot impulsedefocus glassmotionzoomsnowfrostfog bright contrastelasticpixeljpeg 0 25 50 75Error (%) source 59.5% norm 49.9% tent 44.0% ANT 50.2% Figure 5: Corruption benchmark on ImageNet-C: error for each type averaged over severity levels. Tent improves on the prior state-of-the-art, adver- sarial noise training (Rusak et al., 2020), by fully test-time adaptation without altering training. Optimization We optimize the modulation parameters γ,β following the training hyperparameters for the source model with few changes. On ImageNet we optimize by SGD with momentum; on other datasets we optimize by Adam (Kingma & Ba, 2015). We lower the batch size (BS) to reduce memory usage for inference, then lower the learning rate (LR) by the same factor to compensate (Goyal et al., 2017). On ImageNet, we set BS = 64 and LR = 0.00025, and on other datasets we set BS = 128 and LR = 0.001.We control for ordering by shufﬂing and sharing the order across methods. Baselines We compare to domain adaptation, self-supervision, normalization, and pseudo-labeling: • source applies the trained classiﬁer to the test data without adaptation, • adversarial domain adaptation (RG) reverses the gradients of a domain classiﬁer on source and target to optimize for a domain-invariant representation (Ganin & Lempitsky, 2015), • self-supervised domain adaptation (UDA-SS) jointly trains self-supervised rotation and position tasks on source and target to optimize for a shared representation (Sun et al., 2019a), • test-time training (TTT) jointly trains for supervised and self-supervised tasks on source, then keeps training the self-supervised task on target during testing (Sun et al., 2019b), • test-time normalization (BN) updates batch normalization statistics (Ioffe & Szegedy, 2015) on the target data during testing (Schneider et al., 2020; Nado et al., 2020), • pseudo-labeling (PL) tunes a conﬁdence threshold, assigns predictions over the threshold as labels, and then optimizes the model to these pseudo-labels before testing (Lee, 2013). Only test-time normalization (BN), pseudo-labeling (PL), and tent (ours) are fully test-time adaptation methods. See Section 2 for an explanation and contrast with domain adaptation and test-time training. 4.1 R OBUSTNESS TO CORRUPTIONS To benchmark robustness to corruption, we make use of common image corruptions (see Appendix A for examples). The CIFAR-10/100 and ImageNet datasets are turned into the CIFAR-10/100-C and ImageNet-C corruption benchmarks by duplicating their test/validation sets and applying 15 types of corruptions at ﬁve severity levels (Hendrycks & Dietterich, 2019). Tent improves more with less data and computation.Table 2 reports errors averaged over corrup- tion types at the severest level of corruption. On CIFAR-10/100-C we compare all methods, including those that require joint training across domains or losses, given the convenient sizes of these datasets. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent improves on the fully test-time adaptation baselines (BN, PL) but also the domain adaptation (RG, UDA-SS) and test-time training (TTT) methods that need several epochs of optimization on source and target. Tent consistently improves across corruption types.Figure 5 plots the error for each corruption type averaged over corruption levels on ImageNet-C. We compare the most efﬁcient methods—source, normalization, and tent—given the large scale of the source data (>1 million images) needed by other methods and the 75 target combinations of corruption types and levels. Tent and BN adapt online to rival the efﬁciency of inference without adaptation. Tent reaches the least error for most corruption types without increasing the error on the original data. 5Published as a conference paper at ICLR 2021 Table 3: Digit domain adaptation from SVHN to MNIST/MNIST-M/USPS. Source-free adaptation is not only feasible, but more efﬁcient. Tent always improves on normalization (BN), and in 2/3 cases achieves less error than domain adaptation (RG, UDA-SS) without joint training on source & target. Method Source Target Epochs Error (%) Source + Target MNIST MNIST-M USPS Source train - 18.2 39.7 19.3 RG train train 10 + 10 15.0 33.4 18.9 UDA-SS train train 10 + 10 11.1 22.2 18.4 BN test 0 + 1 15.7 39.7 18.0 Tent (ours) test 0 + 1 10.0 37.0 16.3 Tent (ours) test 0 + 10 8.2 36.8 14.4 Tent reaches a new state-of-the-art without altering training.The state-of-the-art methods for robustness extend training with adversarial noise (ANT) (Rusak et al., 2020) for 50.2% error or mixtures of data augmentations (AugMix) (Hendrycks et al., 2020) for 51.7% error. Combined with stylization from external images (SIN) (Geirhos et al., 2019), ANT+SIN reaches 47.4%. Tent reaches a new state-of-the-art of 44.0% by online adaptation and 42.3% by ofﬂine adaptation. It improves on ANT for all types except noise, on which ANT is trained. This requires just one gradient per test point, without more optimization on the training set (ANT, AugMix) or use of external images (SIN). Among fully test-time adaptation methods, tent reduces the error beyond test-time normalization for 18% relative improvement. In concurrent work, Schneider et al. (2020) report 49.3% error for test-time normalization, for which tent still gives 14% relative improvement. 4.2 S OURCE -FREE DOMAIN ADAPTATION We benchmark digit adaptation (Ganin & Lempitsky, 2015; Tzeng et al., 2015; 2017; Shu et al., 2018) for shifts from SVHN to MNIST/MNIST-M/USPS. Recall that unsupervised domain adaptation makes use the labeled source data and unlabeled target data, while our fully test-time adaptation setting denies use of source data. Adaptation is ofﬂine for fair comparison with ofﬂine baselines. Tent adapts to target without source.Table 3 reports the target errors for domain adaptation and fully test-time adaptation methods. Test-time normalization (BN) marginally improves, while adversarial domain adaptation (RG) and self-supervised domain adaptation (UDA-SS) improve more by joint training on source and target. Tent always has lower error than the source model and BN, and it achieves the lowest error in 2/3 cases, even in just one epoch and without use of source data. While encouraging for fully test-time adaptation, unsupervised domain adaptation remains necessary for the highest accuracy and harder shifts. For SVHN-to-MNIST, DIRT-T (Shu et al., 2018) achieves a remarkable 0.6% error 2. For MNIST-to-SVHN, a difﬁcult shift with source-only error of 71.3%, DIRT-T reaches45.5% and UDA-SS reaches 38.7%. Tent fails on this shift and increases error to 79.8%. In this case success presently requires joint optimization over source and target. Tent needs less computation, but still improves with more.Tent adapts efﬁciently on target data alone with just one gradient per point. RG & UDA-SS also use the source data (SVHN train), which is ∼7×the size of the target data (MNIST test), and optimize for 10 epochs. Tent adapts with ∼80× less computation. With more updates, tent reaches 8.2% error in 10 epochs and 6.5% in 100 epochs. With online updates, tent reaches 12.5% error in one epoch and 8.4% error in 10 epochs. Tent scales to semantic segmentation.To show scalability to large models and inputs, we evaluate semantic segmentation (pixel-wise classiﬁcation) on a domain shift from a simulated source to a real target. The source is GTA (Richter et al., 2017), a video game in an urban environment, and the target is Cityscapes (Cordts et al., 2016), an urban autonomous driving dataset. The model is HRNet-W18, a fully convolutional network (Shelhamer et al., 2017) with high-resolution architecture (Wang et al., 2020). The target intersection-over-union scores (higher is better) are source 28.8%, BN 31.4%, and tent 35.8% with ofﬂine optimization by Adam. For adaptation to a single image, tent reaches 36.4% in 10 iterations with episodic optimization. See the appendix for a qualitative example (Appendix B). 2We exclude DIRT-T from our experiments because of incomparable differences in architecture and model selection. DIRT-T tunes with labeled target data, but we do not. Please refer to Shu et al. (2018) for more detail. 6Published as a conference paper at ICLR 2021 Figure 6: Tent reduces the entropy and loss. We plot changes in entropy∆Hand loss ∆Lfor all of CIFAR-100-C. Change in entropy rank-correlates with change in loss: note the dark diagonal and the rank correlation coefﬁcient of 0.22. (a) Source (b) BN  (c) Tent (d) Oracle  Figure 7: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts fea- tures away from the reference, but BN reduces the shifts. Tent instead shifts features more, and closer to an oracle that optimizes on target labels. Tent scales to the VisDA-C challenge.To show adaptation on a more difﬁcult benchmark, we evaluate on the VisDA-C challenge (Peng et al., 2017). The task is object recognition for 12 classes where the source data is synthesized by rendering 3D models and the target data is collected from real scenes. The validation error for our source model (ResNet-50, pretrained on ImageNet) is 56.1%, while tent reaches 45.6%, and improves to 39.6% by updating all layers except for the ﬁnal classiﬁer as done by Liang et al. (2020). Although ofﬂine source-free adaptation by model adaptation (Li et al., 2020) or SHOT (Liang et al., 2020) can reach lower error with more computation and tuning, tent can adapt online during testing. 4.3 A NALYSIS Tent reduces entropy and error.Figure 6 veriﬁes tent does indeed reduce the entropy and the task loss (softmax cross-entropy). We plot changes in entropy and loss on CIFAR-100-C for all 75 corruption type/level combinations. Both axes are normalized by the maximum entropy of a prediction (log 100) and clipped to ±1. Most points have lower entropy and error after adaptation. Tent needs feature modulation.We ablate the normalization and transformation steps of feature modulation. Not updating normalization increases errors, and can fail to improve over BN and PL. Not updating transformation parameters reduces the method to test-time normalization. Updating only the last layer of the model can improve but then degrades with further optimization. Updating the full model parameters θnever improves over the unadapted source model. Tent generalizes across target data.Adaptation could be limited to the points used for updates. We check that adaptation generalizes across points by adapting on target train and not target test. Test errors drop: CIFAR-100-C error goes from 37.3% to 34.2% and SVHN-to-MNIST error goes from 8.2% to 6.5%. (Train is larger than test; when subsampling to the same size errors differ by <0.1%.) Therefore the adapted modulation is not point speciﬁc but general. Tent modulation differs from normalization.Modulation normalizes and transforms features. We examine the combined effect. Figure 7 contrasts adapted features on corrupted data against reference features on uncorrupted data. We plot features from the source model, normalization, tent, and an oracle that optimizes on the target labels. Normalization makes features more like the reference, but tent does not. Instead, tent makes features more like the oracle. This suggests a different and task-speciﬁc effect. See the appendix for visualizations of more layers (Appendix C). 7Published as a conference paper at ICLR 2021 Tent adapts alternative architectures.Tent is architecture agnostic in principle. To gauge its generality in practice, we evaluate new architectures based on self-attention (SAN) (Zhao et al., 2020) and equilibrium solving (MDEQ) (Bai et al., 2020) for corruption robustness on CIFAR-100-C. Table 4 shows that tent reduces error with the same settings as convolutional residual networks. Table 4: Tent adapts alternative architectures on CIFAR-100-C without tuning. Results are error (%). SAN-10 (pair) SAN-10 (patch) MDEQ (large) Source BN Tent Source BN Tent Source BN Tent 55.3 39.7 36.7 48.0 31.8 29.2 53.3 44.9 41.7 5 R ELATED WORK We relate tent to existing adaptation, entropy minimization, and feature modulation methods. Train-Time AdaptationDomain adaptation jointly optimizes on source and target by cross-domain losses L(xs,xt) to mitigate shift. These losses optimize feature alignment (Gretton et al., 2009; Sun et al., 2017), adversarial invariance (Ganin & Lempitsky, 2015; Tzeng et al., 2017), or shared proxy tasks (Sun et al., 2019a). Transduction (Gammerman et al., 1998; Joachims, 1999; Zhou et al., 2004) jointly optimizes on train and test to better ﬁt speciﬁc test instances. While effective in their settings, neither applies when joint use of source/train and target/test is denied. Tent adapts on target alone. Recent “source-free” methods (Li et al., 2020; Kundu et al., 2020; Liang et al., 2020) also adapt without source data. Li et al. (2020); Kundu et al. (2020) rely on generative modeling and optimize multiple models with multiple losses. Kundu et al. (2020); Liang et al. (2020) also alter training. Tent does not need generative modeling, nor does it alter training, and so it can deployed more generally to adapt online with much more computational efﬁciency. SHOT (Liang et al., 2020) adapts by informa- tion maximization (entropy minimization and diversity regularization), but differs in its other losses and its parameterization. These source-free methods optimize ofﬂine with multiple losses for multiple epochs, which requires more tuning and computation than tent, but may achieve more accuracy with more computation. Tent optimizes online with just one loss and an efﬁcient parameterization of modulation to emphasize fully test-time adaptation during inference. We encourage examination of each of these works on the frontier of adaptation without source data. Chidlovskii et al. (2016) are the ﬁrst to motivate adaptation without source data for legal, commercial, or technical concerns. They adapt predictions by applying denoising auto-encoders while we adapt models by entropy minimization. We share their motivations, but the methods and experiments differ. Test-Time AdaptationTent adapts by test-time optimization and normalization to update the model. Test-time adaptation of predictions, through which harder and uncertain cases are adjusted based on easier and certain cases (Jain & Learned-Miller, 2011), provides inspiration for certainty-based model adaptation schemes like our own. Test-time training (TTT) (Sun et al., 2019b) also optimizes during testing, but differs in its loss and must alter training. TTT relies on a proxy task, such as recognizing rotations of an image, and so its loss depends on the choice of proxy. (Indeed, its authors caution that the proxy must be “both well-deﬁned and non-trivial in the new domain”). TTT alters training to optimize this proxy loss on source before adapting to target. Tent adapts without proxy tasks and without altering training. Normalizing feature statistics is common for domain adaptation (Gretton et al., 2009; Sun et al., 2017). For batch normalization Li et al. (2017); Carlucci et al. (2017) separate source and target statistics during training. Schneider et al. (2020); Nado et al. (2020) estimate target statistics during testing to improve generalization. Tent builds on test-time normalization to further reduce generalization error. Entropy MinimizationEntropy minimization is a key regularizer for domain adaptation (Carlucci et al., 2017; Shu et al., 2018; Saito et al., 2019; Roy et al., 2019), semi-supervised learning (Grandvalet & Bengio, 2005; Lee, 2013; Berthelot et al., 2019), and few-shot learning (Dhillon et al., 2020). Regularizing entropy penalizes decisions at high densities in the data distribution to improve accuracy for distinct classes (Grandvalet & Bengio, 2005). These methods regularize entropy during training in concert with other supervised and unsupervised losses on additional data. Tent is the ﬁrst to minimize 8Published as a conference paper at ICLR 2021 entropy during testing, for adaptation to dataset shifts, without other losses or data. Entropic losses are common; our contribution is to exhibit entropy as the sole lossfor fully test-time adaptation. Feature ModulationModulation makes a model vary with its input. We optimize modulations that are simpler than the full model for stable and efﬁcient adaptation. We modulate channel-wise afﬁne transformations, for their effectiveness in tandem with normalization (Ioffe & Szegedy, 2015; Wu & He, 2018), and for their ﬂexibility in conditioning for different tasks (Perez et al., 2018). These normalization and conditioning methods optimize the modulation during training by a supervised loss, but keep it ﬁxed during testing. We optimize the modulation during testing by an unsupervised loss, so that it can adapt to different target data. 6 D ISCUSSION Tent reduces generalization error on shifted data by test-time entropy minimization. In minimizing entropy, the model adapts itself to feedback from its own predictions. This is truly self-supervised self-improvement. Self-supervision of this sort is totally deﬁned by the supervised task, unlike proxy tasks designed to extract more supervision from the data, and yet it remarkably still reduces error. Nevertheless, errors due to corruption and other shifts remain, and therefore more adaptation is needed. Next steps should pursue test-time adaptation on more and harder types of shift, over more general parameters, and by more effective and efﬁcient losses. Shifts Tent reduces error for a variety of shifts including image corruptions, simple changes in appearance for digits, and simulation-to-real discrepancies. These shifts are popular as standardized benchmarks, but other real-world shifts exist. For instance, the CIFAR 10.1 and ImageNetV2 test sets (Recht et al., 2018; 2019), made by reproducing the dataset collection procedures, entail natural but unknown shifts. Although error is higher on both sets, indicating the presence of shift, tent does not improve generalization. Adversarial shifts (Szegedy et al., 2014) also threaten real-world usage, and attackers keep adapting to defenses. While adversarial training (Madry et al., 2018) makes a difference, test-time adaptation could help counter such test-time attacks. Parameters Tent modulates the model by normalization and transformation, but much of the model stays ﬁxed. Test-time adaptation could update more of the model, but the issue is to identify parameters that are both expressive and reliable, and this may interact with the choice of loss. TTT adapts multiple layers of features shared by supervised and self-supervised models and SHOT adapts all but the last layer(s) of the model. These choices depend on the model architecture, the loss, and tuning. For tent modulation is reliable, but the larger shift on VisDA is better addressed by the SHOT parameterization. Jointly adapting the input could be a more general alternative. If a model can adapt itself on target, then perhaps its input gradients might optimize spatial transformations or image translations to reduce shift without source data. Losses Tent minimizes entropy. For more adaptation, is there an effective loss for general but episodic test-time optimization? Entropy is general across tasks but limited in scope. It needs batches for optimization, and cannot update episodically on one point at a time. TTT can do so, but only with the right proxy task. For less computation, is there an efﬁcient loss for more local optimization? Tent and TTT both require full (re-)computation of the model for updates because they depend on its predictions. If the loss were instead deﬁned on the representation, then updates would require less forward and backward computation. Returning to entropy speciﬁcally, this loss may interact with calibration (Guo et al., 2017), as better uncertainty estimation could drive better adaptation. We hope that the fully test-time adaptation setting can promote new methods for equipping a model to adapt itself, just as tent yields a new model with every update. ACKNOWLEDGMENTS We thank Eric Tzeng for discussions on domain adaptation, Bill Freeman for comments on the experiments, Yu Sun for consultations on test-time training, and Kelsey Allen for feedback on the exposition. We thank the anonymous reviewers of ICLR 2021 for their feedback, which certainly improved the latest adaptation of the paper. 9Published as a conference paper at ICLR 2021 REFERENCES Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint arXiv:2006.08656, 2020. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial: Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5077–5085. IEEE, 2017. Boris Chidlovskii, Stephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In SIGKDD, pp. 451–460, 2016. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classiﬁcation. In ICLR, 2020. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014. A Gammerman, V V ovk, and V Vapnik. Learning by transduction. InUAI, 1998. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2015. Robert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. Generalisation in humans and deep neural networks. In NeurIPS, 2018. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, 2005. A. Gretton, AJ. Smola, J. Huang, M. Schmittfull, KM. Borgwardt, and B. Schölkopf. Covariate shift and local learning by distribution matching. In Dataset Shift in Machine Learning, pp. 131–160. MIT Press, Cambridge, MA, USA, 2009. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML, 2017. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, June 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019. Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. In ICLR, 2020. Jonathan J. Hull. A database for handwritten text recognition research. TPAMI, 1994. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 10Published as a conference paper at ICLR 2021 Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR, 2011. Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. NeurIPS, 25, 2012. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In CVPR, pp. 4544–4553, 2020. Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In ICML Workshop on challenges in representation learning, 2013. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In CVPR, June 2020. Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In ICLRW, 2017. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In ICML, 2020. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. VisDA: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. MIT Press, Cambridge, MA, USA, 2009. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Dollár. On network design spaces for visual recognition. In ICCV, 2019. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classiﬁers generalize to ImageNet? In ICML, 2019. Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, 2017. Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and consensus loss. In CVPR, 2019. 11Published as a conference paper at ICLR 2021 Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Julian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In ECCV, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pp. 213–226. Springer, 2010. Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised domain adaptation via minimax entropy. In ICCV, 2019. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improv- ing robustness against common corruptions by covariate shift adaptation. arXiv preprint arXiv:2006.16971, 2020. C.E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948. Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. PAMI, 2017. Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. In ICLR, 2018. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. In Domain Adaptation in Computer Vision Applications, pp. 153–171. Springer, 2017. Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self- supervision. arXiv preprint arXiv:1909.11825, 2019a. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for out-of-distribution generalization. arXiv preprint arXiv:1909.13231, 2019b. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. 2014. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. PAMI, 2020. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross- channel prediction. In CVPR, 2017. Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global consistency. NeurIPS, 2004. 12Published as a conference paper at ICLR 2021 APPENDIX This supplement summarizes the image corruptions used in our experiments, highlights a qualitative example of instance-wise adaptation for semantic segmentation, and visualizes feature shifts across more layers. A R OBUSTNESS TO CORRUPTIONS In Section 4.1 we evaluate methods on a common image corruptions benchmark. Table 2 reports errors on the most severe level of corruption, level 5, and Figure 5 reports errors for each corruption type averaged across each of the levels 1–5. We summarize these corruptions types by example in Figure 8. Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure 8: Examples of each corruption type in the image corruptions benchmark. While synthetic, this set of corruptions aims to represent natural factors of variation like noise, blur, weather, and digital imaging effects. This ﬁgure is reproduced from Hendrycks & Dietterich (2019). B S OURCE -FREE ADAPTATION FOR SEMANTIC SEGMENTATION Figure 9 shows a qualitative result on source-free adaptation for semantic segmentation (pixel-wise classiﬁcation) with simulation-to-real (sim-to-real) shift. For this sim-to-real condition, the source data is simulated while the target data is real. Our source data is GTA Richter et al. (2017), a visually-sophisticated video game set in an urban environment, and our target data is Cityscapes Cordts et al. (2016), an urban autonomous driving dataset. The supervised model is HRnet-W18, a fully convolutional network Shelhamer et al. (2017) in the high-resolution network family Wang et al. (2020). For this qualitative example, we run tent on a single image for multiple iterations, because an image is in effect a batch of pixels. This demonstrates adaptation to a target instance, without any further access to the target domain through usage of multiple images from the target distribution. 13Published as a conference paper at ICLR 2021 image label source-only tent, iteration 1 tent, iteration 5 tent, iteration 10 Figure 9: Adaptation for semantic segmentation with simulation-to-real shift from GTA Richter et al. (2017) to Cityscapes Cordts et al. (2016). Tent only uses the target data, and optimizes over a single image as a dataset of pixel-wise predictions. This episodic optimization in effect ﬁts a custom model to each image of the target domain. In only 10 iterations our method suppresses noise (see the completion of the street segment, in purple) and recovers missing classes (see the motorcycle and rider, center). 14Published as a conference paper at ICLR 2021 C F EATURE SHIFTS ACROSS LAYERS AND METHODS (a) Source (b) BN (c) Tent (d) Oracle Layer 2 Layer 5 Layer 8 Layer 11 Layer 14 Layer 18 Layer 20 Layer 23 Layer 26 Figure 10: Adapted features on CIFAR-100-C with Gaussian noise (front) and reference features without corruption (back). Corruption shifts the source features from the reference. BN shifts the features back to be more like the reference. Tent shifts features to be less like the reference, and more like an oracle that optimizes on target labels. 15",
      "meta_data": {
        "arxiv_id": "2006.10726v3",
        "authors": [
          "Dequan Wang",
          "Evan Shelhamer",
          "Shaoteng Liu",
          "Bruno Olshausen",
          "Trevor Darrell"
        ],
        "published_date": "2020-06-18T17:55:28Z",
        "pdf_url": "https://arxiv.org/pdf/2006.10726v3.pdf",
        "github_url": "https://github.com/DequanWang/tent"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of generalization to new and different data (dataset shift, corruptions) during model testing, particularly in a \"fully test-time adaptation\" setting where source data and supervision are unavailable. It proposes TENT (TEst-time eNTropy minimization), an adaptation method that optimizes the model for confidence by minimizing the entropy of its predictions on test data. TENT achieves a new state-of-the-art error rate of 44.0% on ImageNet-C for corruption robustness, outperforming robust training methods and test-time normalization. It also demonstrates effective online and source-free domain adaptation for digit recognition and semantic segmentation, often matching or exceeding methods that utilize source data and more extensive optimization, without altering the model's original training.",
        "methodology": "TENT optimizes the model during testing by minimizing the Shannon entropy of its predictions on target data. This is an unsupervised objective. The adaptation is achieved by modulating features within the model's normalization layers. Specifically, the method estimates normalization statistics (mean µ and standard deviation σ) batch-by-batch from the test data and optimizes channel-wise affine transformation parameters (scale γ and shift β) using the gradient of the prediction entropy. Other model parameters remain fixed. The algorithm involves an initialization step where affine parameters are collected and source normalization statistics are discarded, an iterative step where statistics are estimated during the forward pass and affine parameters are updated during the backward pass (one gradient per point for efficiency), and can operate in online or offline adaptation modes. The method requires the base model to be trained for a supervised task, probabilistic, and differentiable.",
        "experimental_setup": "The method was evaluated on:1. Corruption Robustness: CIFAR-10/100-C and ImageNet-C, which are original datasets augmented with 15 types of corruptions at five severity levels.2. Domain Adaptation (Digit Recognition): SVHN (source) to MNIST, MNIST-M, and USPS (targets).3. Domain Adaptation (Semantic Segmentation): GTA (simulated source) to Cityscapes (real target), using HRNet-W18.4. Domain Adaptation (Object Recognition): VisDA-C challenge (synthesized source to real target), using ResNet-50.Models used were residual networks (R-26 for CIFAR, R-50 for ImageNet and SVHN-to-MNIST; ResNet-50 for VisDA-C) equipped with batch normalization. Additional architectures like Self-Attention Networks (SAN-10) and Deep Equilibrium Models (MDEQ) were tested for generality.Optimization was performed using SGD with momentum for ImageNet (batch size 64, learning rate 0.00025) and Adam for other datasets (batch size 128, learning rate 0.001), with batch size and learning rate adjustments. Baselines included source-only, adversarial domain adaptation (RG), self-supervised domain adaptation (UDA-SS), test-time training (TTT), test-time normalization (BN), and pseudo-labeling (PL). Validation metrics were error rate for classification and Intersection-over-Union (IoU) for segmentation.",
        "limitations": "TENT's effectiveness is limited for very challenging domain shifts; for instance, it fails on SVHN-to-MNIST (error increases) where methods using joint optimization over source and target (e.g., DIRT-T) achieve significantly better performance. The entropy minimization objective requires batches for optimization, preventing episodic updates on a single data point. It does not improve generalization on natural but unknown shifts such as CIFAR 10.1 and ImageNetV2 test sets. While effective, for larger shifts like the VisDA-C challenge, more extensive parameterizations (e.g., adapting all but the last layer, as in SHOT) can achieve lower error, suggesting the current modulation parameters may be insufficient for all types of shifts. Lastly, the method requires full re-computation of the model for updates because its loss depends on predictions, impacting computational efficiency.",
        "future_research_directions": "Future work could focus on extending test-time adaptation to a wider range of more challenging data shifts, including natural unknown shifts (like CIFAR 10.1 and ImageNetV2) and adversarial shifts. Exploring adaptation of more general and expressive model parameters beyond just affine transformations, while maintaining reliability, is another direction. Investigating methods for jointly adapting the input, such as optimizing spatial transformations or image translations based on input gradients, could offer a more general approach to reduce shift without source data. Developing more effective and computationally efficient losses for general and episodic test-time optimization, possibly by defining the loss on intermediate representations to reduce computation, is also suggested. Finally, exploring the interaction between entropic losses and model calibration could lead to improved uncertainty estimation and adaptation.",
        "experimental_code": "File Path: tent.py\\nContent:\\nfrom copy import deepcopy\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.jit\\n\\n\\nclass Tent(nn.Module):\\n    \"\"\"Tent adapts a model by entropy minimization during testing.\\n\\n    Once tented, a model adapts itself by updating on every forward.\\n    \"\"\"\\n    def __init__(self, model, optimizer, steps=1, episodic=False):\\n        super().__init__()\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.steps = steps\\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\\n        self.episodic = episodic\\n\\n        # note: if the model is never reset, like for continual adaptation,\\n        # then skipping the state copy would save memory\\n        self.model_state, self.optimizer_state = \\\\n            copy_model_and_optimizer(self.model, self.optimizer)\\n\\n    def forward(self, x):\\n        if self.episodic:\\n            self.reset()\\n\\n        for _ in range(self.steps):\\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\\n\\n        return outputs\\n\\n    def reset(self):\\n        if self.model_state is None or self.optimizer_state is None:\\n            raise Exception(\"cannot reset without saved model/optimizer state\")\\n        load_model_and_optimizer(self.model, self.optimizer,\\n                                 self.model_state, self.optimizer_state)\\n\\n\\n@torch.jit.script\\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\\n\\n\\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\\ndef forward_and_adapt(x, model, optimizer):\\n    \"\"\"Forward and adapt model on batch of data.\\n\\n    Measure entropy of the model prediction, take gradients, and update params.\\n    \"\"\"\\n    # forward\\n    outputs = model(x)\\n    # adapt\\n    loss = softmax_entropy(outputs).mean(0)\\n    loss.backward()\\n    optimizer.step()\\n    optimizer.zero_grad()\\n    return outputs\\n\\n\\ndef collect_params(model):\\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\\n\\n    Walk the model's modules and collect all batch normalization parameters.\\n    Return the parameters and their names.\\n\\n    Note: other choices of parameterization are possible!\\n    \"\"\"\\n    params = []\\n    names = []\\n    for nm, m in model.named_modules():\\n        if isinstance(m, nn.BatchNorm2d):\\n            for np, p in m.named_parameters():\\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\\n                    params.append(p)\\n                    names.append(f\\\"{nm}.{np}\\\")\\n    return params, names\\n\\n\\ndef copy_model_and_optimizer(model, optimizer):\\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\\n    model_state = deepcopy(model.state_dict())\\n    optimizer_state = deepcopy(optimizer.state_dict())\\n    return model_state, optimizer_state\\n\\n\\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\\n    model.load_state_dict(model_state, strict=True)\\n    optimizer.load_state_dict(optimizer_state)\\n\\n\\ndef configure_model(model):\\n    \"\"\"Configure model for use with tent.\"\"\"\\n    # train mode, because tent optimizes the model to minimize entropy\\n    model.train()\\n    # disable grad, to (re-)enable only what tent updates\\n    model.requires_grad_(False)\\n    # configure norm for tent updates: enable grad + force batch statisics\\n    for m in model.modules():\\n        if isinstance(m, nn.BatchNorm2d):\\n            m.requires_grad_(True)\\n            # force use of batch stats in train and eval modes\\n            m.track_running_stats = False\\n            m.running_mean = None\\n            m.running_var = None\\n    return model\\n\\n\\ndef check_model(model):\\n    \"\"\"Check model for compatability with tent.\"\"\"\\n    is_training = model.training\\n    assert is_training, \"tent needs train mode: call model.train()\"\\n    param_grads = [p.requires_grad for p in model.parameters()]\\n    has_any_params = any(param_grads)\\n    has_all_params = all(param_grads)\\n    assert has_any_params, \"tent needs params to update: \" \\\\n                           \"check which require grad\"\\n    assert not has_all_params, \"tent should not update all params: \" \\\\n                               \"check which require grad\"\\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\\n    assert has_bn, \"tent needs normalization for its optimization\"\\n\\nFile Path: cifar10c.py\\nContent:\\nimport logging\\nimport torch.optim as optim\\nimport tent\\n\\nfrom conf import cfg, load_cfg_fom_args\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef setup_tent(model):\\n    \"\"\"Set up tent adaptation.\\n\\n    Configure the model for training + feature modulation by batch statistics,\\n    collect the parameters for feature modulation by gradient optimization,\\n    set up the optimizer, and then tent the model.\\n    \"\"\"\\n    model = tent.configure_model(model)\\n    params, param_names = tent.collect_params(model)\\n    optimizer = setup_optimizer(params)\\n    tent_model = tent.Tent(model, optimizer,\\n                           steps=cfg.OPTIM.STEPS,\\n                           episodic=cfg.MODEL.EPISODIC)\\n    logger.info(f\\\"model for adaptation: %s\\\", model)\\n    logger.info(f\\\"params for adaptation: %s\\\", param_names)\\n    logger.info(f\\\"optimizer for adaptation: %s\\\", optimizer)\\n    return tent_model\\n\\n\\ndef setup_optimizer(params):\\n    \"\"\"Set up optimizer for tent adaptation.\\n\\n    Tent needs an optimizer for test-time entropy minimization.\\n    In principle, tent could make use of any gradient optimizer.\\n    In practice, we advise choosing Adam or SGD+momentum.\\n    For optimization settings, we advise to use the settings from the end of\\n    trainig, if known, or start with a low learning rate (like 0.001) if not.\\n\\n    For best results, try tuning the learning rate and batch size.\\n    \"\"\"\\n    if cfg.OPTIM.METHOD == 'Adam':\\n        return optim.Adam(params,\\n                    lr=cfg.OPTIM.LR,\\n                    betas=(cfg.OPTIM.BETA, 0.999),\\n                    weight_decay=cfg.OPTIM.WD)\\n    elif cfg.OPTIM.METHOD == 'SGD':\\n        return optim.SGD(params,\\n                   lr=cfg.OPTIM.LR,\\n                   momentum=cfg.OPTIM.MOMENTUM,\\n                   dampening=cfg.OPTIM.DAMPENING,\\n                   weight_decay=cfg.OPTIM.WD,\\n                   nesterov=cfg.OPTIM.NESTEROV)\\n    else:\\n        raise NotImplementedError",
        "experimental_info": "Model Adaptation:\\n- Method: TENT (Test-time ENtropy Minimization)\\n- Base Model Architecture: 'Standard'\\n- Episodic Adaptation: False (updates persist across batches)\\n\\nOptimization Settings:\\n- Steps per batch (updates): 1\\n- Learning Rate (LR): 1e-3\\n- Optimizer Method: Adam (default)\\n  - Beta for Adam: 0.9\\n  - Weight Decay (L2 regularization): 0.0\\n- Alternative Optimizer: SGD\\n  - Momentum: 0.9\\n  - Dampening: 0.0\\n  - Nesterov: True\\n  - Weight Decay (L2 regularization): 0.0\\n\\nData and Evaluation Settings:\\n- Dataset: CIFAR-10-C\\n- Corruption Types: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']\\n- Corruption Severities: [5, 4, 3, 2, 1]\\n- Number of Examples for Evaluation: 10000 (all samples in CIFAR-10)\\n- Batch Size for Evaluation and Updates: 128\\n\\nMiscellaneous:\\n- Random Seed: 1\\n- CUDNN Benchmark: True"
      }
    },
    {
      "title": "What How and When Should Object Detectors Update in Continually Changing Test Domains?",
      "abstract": "It is a well-known fact that the performance of deep learning models\ndeteriorates when they encounter a distribution shift at test time. Test-time\nadaptation (TTA) algorithms have been proposed to adapt the model online while\ninferring test data. However, existing research predominantly focuses on\nclassification tasks through the optimization of batch normalization layers or\nclassification heads, but this approach limits its applicability to various\nmodel architectures like Transformers and makes it challenging to apply to\nother tasks, such as object detection. In this paper, we propose a novel online\nadaption approach for object detection in continually changing test domains,\nconsidering which part of the model to update, how to update it, and when to\nperform the update. By introducing architecture-agnostic and lightweight\nadaptor modules and only updating these while leaving the pre-trained backbone\nunchanged, we can rapidly adapt to new test domains in an efficient way and\nprevent catastrophic forgetting. Furthermore, we present a practical and\nstraightforward class-wise feature aligning method for object detection to\nresolve domain shifts. Additionally, we enhance efficiency by determining when\nthe model is sufficiently adapted or when additional adaptation is needed due\nto changes in the test distribution. Our approach surpasses baselines on widely\nused benchmarks, achieving improvements of up to 4.9\\%p and 7.9\\%p in mAP for\nCOCO $\\rightarrow$ COCO-corrupted and SHIFT, respectively, while maintaining\nabout 20 FPS or higher.",
      "full_text": "What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Jayeon Yoo1 Dongkwan Lee1 Inseop Chung1 Donghyun Kim2∗ Nojun Kwak1∗ 1Seoul National University 2Korea University 1{jayeon.yoo, biancco, jis3613, nojunk}@snu.ac.kr 2d kim@korea.ac.kr Abstract It is a well-known fact that the performance of deep learning models deteriorates when they encounter a dis- tribution shift at test time. Test-time adaptation (TTA) al- gorithms have been proposed to adapt the model online while inferring test data. However, existing research pre- dominantly focuses on classification tasks through the op- timization of batch normalization layers or classification heads, but this approach limits its applicability to various model architectures like Transformers and makes it chal- lenging to apply to other tasks, such as object detection. In this paper, we propose a novel online adaption approach for object detection in continually changing test domains, considering which part of the model to update, how to up- date it, and when to perform the update. By introducing architecture-agnostic and lightweight adaptor modules and only updating these while leaving the pre-trained backbone unchanged, we can rapidly adapt to new test domains in an efficient way and prevent catastrophic forgetting. Fur- thermore, we present a practical and straightforward class- wise feature aligning method for object detection to resolve domain shifts. Additionally, we enhance efficiency by deter- mining when the model is sufficiently adapted or when ad- ditional adaptation is needed due to changes in the test dis- tribution. Our approach surpasses baselines on widely used benchmarks, achieving improvements of up to 4.9%p and 7.9%p in mAP for COCO → COCO-corrupted and SHIFT, respectively, while maintaining about 20 FPS or higher. 1. Introduction Although deep learning models have demonstrated remark- able success in numerous vision-related tasks, they remain susceptible to domain shifts where the test data distribu- tion differs from that of the training data [3, 25, 40]. In real-world applications, domain shifts frequently occur at test-time due to natural variations, corruptions, changes in weather conditions (e.g., fog, rain) , camera sensor differ- Figure 1. We propose an online adaptation method for object detection in continually changing test domains. Object detectors trained with clean images suffer from performance degradation due to various corruption, such as camera sensor degradation or environmental changes (Direct-Test). Updating full parameters for online adaptation require a large number of test samples and vul- nerable to drastic domain changes (Full-Finetuning), while using only our lightweight adaptor is robust and quickly adapts within a few time steps (Ours). We can further improve efficiency by skip- ping unnecessary adaptation steps (Ours-Skip). ences (e.g., pixelate, defocus blur) , and various other fac- tors. Test-Time Adaptation (TTA) [3, 25, 30, 40, 43, 47] has been proposed to solve the domain shifts in test-time by adapting models to a specific target (test) distribution in an online manner. Furthermore, it is essential to take into account continuously changing test distributions, as the test distribution has the potential to undergo changes and devel- opments as time progresses (i.e., Continual Test-time Adap- tation (CTA)). For instance, autonomous driving systems may experience transitions from clear and sunny conditions to rainy or from daytime to nighttime, which causes contin- ually changing domain shifts [39]. While it is an important research topic, continual test-time adaptation for object de- tection has not been well explored. Recently, several TTA methods [6, 29, 36] tailored for 1 arXiv:2312.08875v1  [cs.CV]  12 Dec 2023object detection have been proposed. ActMAD [29] aligns all the output feature maps ( RC×H×W ) after Batch Nor- malization (BN) layers [14] to adapt the test domain to be similar to that of the training domain. However, this ap- proach requires significant memory during adaptation and does not explicitly consider the objects present in the image. TeST [36] and STFAR [6] adapt to a test domain by utiliz- ing weak and strong augmented test samples with a teacher- student network [37], but they significantly increase infer- ence costs since they require additional forward passes and update steps. Also, these methods update all network pa- rameters, making them highly inefficient in online adapta- tion and vulnerable to losing task-specific knowledge when the test domain experiences continual or drastic changes. In this paper, we aim to develop an efficient continual test-time adaptation (CTA) method for object detection. We investigate the following three key aspects to improve ef- ficiency; what to update: while previous TTA methods for object detection [6, 29, 36] use full fine-tuning, updating all parameters at test time, they are inefficient and prone to losing task-specific knowledge in relatively complex object detection tasks. Updating BN layers, as done in many TTA methods for classification [17, 30, 43, 47], is not as effective for object detection, given its smaller batch size compared to classification and the limitation in applying various back- bones, such as Transformer [26, 41].how to update: several previous TTA methods for object detection [6, 36] adapt the model by using teacher-student networks, resulting in a significant decrease in inference speed, which is detri- mental during test time. While another existing method [29] aligns feature distributions for adaptation, it does not con- sider each object individually, focusing only on image fea- tures, making it less effective for object detection. when to update: most TTA or CTA methods update models using all incoming test samples. However, it is inefficient to update continuously the model if it is already sufficiently adapted when the change of the test domain is not significant. To this end, (1) we propose an efficient continual test- time adaptation method for object detectors to adapt to continually changing test domains through the use of lightweight adaptors which require only 0.54%∼0.89% ad- ditional parameters compared to the full model. It exhibits efficiency in parameters, memory usage, and adaptation time, along with robustness to continuous domain shifts without catastrophic forgetting. Additionally, it demon- strates wide applicability to various backbone types com- pared to BN-based TTA methods [17, 22, 30, 43, 47, 48]. (2) To enhance the adaptation effectiveness in the object detec- tion task, we align the feature distribution of the test domain with that of the training domain at both the image-level and object-level using only the mean and variance of features. For estimating the mean of the test domain features, we employ Exponentially Moving Average (EMA) as we can leverage only the current incoming test samples, not the en- tire test domain data. Due to the unavailability of training data access, we utilize only the mean and variance of the features from a few training samples. (3) We also introduce two novel criteria that do not require additional resources to determine when the model needs adaptation to enhance efficiency in a continually changing test domain environ- ment. As illustrated in Fig. 1, our approach Ours, employ- ing adaptors, tends to adapt much faster to domain changes compared to full parameter updates. This enables efficient TTA by using only a few test samples to update the adaptor and skipping the rest of the updates as shown in Ours-Skip. Our main contributions are summarized as follows: • We introduce an architecture-agnostic lightweight adap- tor, constituting only a maximum of 0.89% of the total model parameters, into the backbone of the object de- tector to adapt the model in a continually changing test domain. This approach ensures efficiency in parameters, memory usage, and adaptation speed, demonstrating the robust preservation of task-specific knowledge owing to its inherent structural characteristics. • We propose a straightforward and effective adaptation loss for CTA in object detection tasks. This is achieved by aligning the distribution of training and test domain fea- tures at both the image and object levels, utilizing only the mean and variance of a few training samples and EMA- updated mean features of the test domain. • We also propose two criteria to determine when the model requires adaptation, enabling dynamic skipping or resum- ing adaptation as needed. This enhancement significantly boosts inference speed by up to about 2 times while main- taining adaptation performance. • Our adaptation method proves effective for diverse types of domain shifts, including weather changes and sensor variations, regardless of whether the domain shift is dras- tic or continuous. In particular, our approach consistently improves the mAP by up to 7.9% in COCO →COCO-C and SHIFT-Discrete/Continuous with higher than 20 FPS. 2. Related Work Test-time adaptation. Recently, there has been a surge of interest in research that adapts models online using unla- beled test samples while simultaneously inferring the test sample to address the domain shift problem, where the test data distribution differs from that of the training data. There are two lines for online adaptation to the test do- main, Test-time Training (TTT) and Test-time Adaptation (TTA). TTT [1, 2, 25, 40] involves modifying the model architecture during training to train it with self-supervised loss, allowing adaptation to the test domain in the test time by applying this self-supervised loss to the unlabeled test samples. On the other hand, TTA aims to adapt the trained model directly to the test domain without specifically tai- 2lored model architectures or losses during training time. NORM [35] and DUA [28] address the domain shifts by adjusting the statistics of batch normalization (BN) layers using the current test samples, without updating other pa- rameters, inspired by [21]. Following this, [22, 30, 43, 48] and [17] update the affine parameters of BN layers using unsupervised loss, entropy minimization loss to enhance the confidence of test data predictions, and feature distribution alignments loss, respectively. Several studies [15, 16] up- date the classifier head using the pseudo-prototypes from the test domain. However, these methods limit their appli- cability to architectures without BN layers or to object de- tection tasks that involve multiple objects in a single im- age. Others [29, 38, 47] update full parameters for online adaptation to the test domain in an online manner, but this approach is inefficient and susceptible to the noisy signal from the unsupervised loss. While existing TTA methods are oriented towards classification tasks, we aim to propose an effective and efficient method for online adaptation in the object detection task. Continual test-time adaptation. Recent studies [31, 44] point out that existing TTA methods have primarily focused on adapting to test domains following an i.i.d assumption and may not perform well when the test data distribution deviates from this assumption. [44] introduces a Contin- ual TTA (CTA) method designed for scenarios where the test domain continuously changes over time. This poses challenges in preventing the model from over-adapting to a particular domain shift and preserving the knowledge of the pre-trained model to avoid catastrophic forgetting. In the field of CTA, the self-training strategy adopting an Exponentially Moving Average (EMA) teacher-student structure is attracting interest as an effective algorithm en- abling robust representation to be learned through self- knowledge distillation. In many studies, the EMA teacher- student structure and catastrophic restoration of source model weights have been proposed as a solution to achieve the goal of CTA [4, 44, 45]. Approaches using source re- play [32], and anti-forgetting regularization [30] have also achieved good performances in robust continuous adapta- tion. Furthermore, there is growing attention on methods that mitigate the computational and memory challenges as- sociated with CTA, such as [12], which relies on updates to batch normalization statistics. Test-time adaptive object detection. Research on TTA for Object Detection (TTAOD) is progressively emerging [6, 29, 36, 42]. Most existing TTAOD methods [6, 36, 42] exploit a teacher-student network to adapt to the test do- main, following the self-training approach commonly em- ployed in Unsupervised Domain Adaptation for object de- tection [7, 18, 19, 34]. However, it is inefficient for TTA due to data augmentation requirements and additional for- ward and backward steps, resulting in slower inference speeds and higher memory usage. Another approach, Act- MAD [29], aligns the distributions of output feature maps after all BN layers along the height, width, and channel axes to adapt to the test domain. However, this location-aware feature alignment is limited to datasets with fixed location priors, such as driving datasets, and is less effective for nat- ural images like COCO. Additionally, CTA for Object De- tection (CTAOD)have not been thoroughly explored. There- fore, there is a need for an effective CTAOD method con- sidering memory and time efficiency. 3. Method To enable the efficient and effective Continual Test-time Adaptation of Object Detectors (CTAOD), we introduce an approach that specifies which part of the model should be updated, describes how to update those using unlabeled test data, and determines whether we perform model updates or not to improve efficiency. 3.1. Preliminary Assume that we have an object detector h ◦ gΘ, here h and g are the RoI head and the backbone, respectively with their parameters being Θ. The training dataset is denoted as Dtrain = {(xi, yi)}N i=1, where xi ∼ Ptrain(x) and yi = ( bboxi, ci), containing information on the bounding box (bbox) and class label ci ∈ C. Consider deploying the detector to the test environments where the test data at pe- riod T is denoted as xT j ∼ PT test(x), PT test ̸= Ptrain and PT test deviates from the i.i.d. assumption. In addition, the domain of PT test continually changes according to T (i.e., PT test ̸= PT−1 test ). Our goal is to adapt the detector h ◦ g to PT test using only test data xT j while making predictions. 3.2. What to update: Adaptation via an adaptor Previous methods [6, 29, 36, 42] adapt the model to the test domain by updating all parameters Θ, leading to in- efficiency at test time and a high risk of losing task knowl- edge from the training data. In contrast, we adapt the model by introducing an adaptor with an extremely small set of parameters and updating only this module while freezing Θ. We introduce a shallow adaptor in parallel for each block, inspired by [5, 13], where transformer-based mod- els are fine-tuned for downstream tasks through parameter- efficient adaptors, as shown in Fig. 2. Each adaptor consists of down-projection layers Wdown ∈ Rd×d r , up-projection layers Wup ∈ R d r ×d and ReLUs, where d denotes the in- put channel dimension and r is the channel reduction ratio set to 32 for all adaptors. We use MLP layers for the Trans- former block (Fig. 2a) and 1×1 convolutional layers for the ResNet block (Fig. 2b) to introduce architecture-agnostic adaptors. The up-projection layer is initialized to 0 values so that the adaptor does not modify the output of the block, 3(a) A block of Transformer  (b) A block of ResNet Figure 2. We attach an adaptor, which is a shallow and low-rank MLP or CNN, to every N block in parallel. We update only these adaptors while other parameters are frozen. Our approach can be applied to diverse architectures including CNNs and Transformers. but as the adaptor is gradually updated, it adjusts the output of the block to adapt to the test domain. Even as the adaptor is updated in the test domain, the original backbone param- eter Θ remains frozen and fully preserved. This structural preservation, as evident in Ours in Fig. 1, enables robust and efficient adaptation to domain changes by maintaining relatively complex task knowledge in object detection and updating very few parameters. 3.3. How to update: EMA feature alignment To adapt the object detector to the test domain, we align the feature distribution of the test domain with that of the training data, inspired by [17, 29, 38]. In contrast to these methods that solely align image feature distribution, we ad- ditionally align object-level features in a class-wise manner, considering class frequency, to enhance its effectiveness for object detection. As the training data is not accessible dur- ing test time, we pre-compute the first and second-order statistics, denoted as µtr = E[Ftr] and Σtr = Var[Ftr], where the operators E and Var represent the mean and vari- ance respectively. The features Ftr = {gΘ(xtr)} are com- puted using only 2,000 training samples, a small subset of the training data. Since a sufficient amount of test domain data is not available at once, and only the current incoming test data, whose features are denoted as Ft te, is accessible at time step t, we estimate the mean of test data features using an exponentially moving average (EMA) as follows: µt te = (1 − α) · µt−1 te + α · E[Ft te], s.t. µ0 te = µtr. (1) Considering the typically small batch size in object detec- tion compared to classification, we approximate the vari- ance of the test features as Σte ≃ Σtr to reduce instability. Image-level feature alignment. We estimate the training and test feature distributions as normal distributions and minimize the KL divergence between them as follows: Limg = DKL(N(µtr, Σtr), N(µt te, Σtr)). (2) Region-level class-wise feature alignment. In object de- tection, we deal with multiple objects within a single image, making it challenging to apply the class-wise feature align- ment proposed in [38], a TTA method for classification. To handle region-level features that correspond to an object, we use ground truth bounding boxes for the training data and utilize the class predictions of RoI pooled features, ft te, for unlabeled test data. In object detection, domain shifts often result in lower recall rates, as a significant number of proposals are predicted as background [20]. To mitigate this issue, we filter out features with background scores exceed- ing a specific threshold. Subsequently, we assign them to the foreground class with the highest probability, as follows: Fk,t te = {ft te|argmax c pfg = k, pbg < 0.5}, where hcls(ft te) = [pfg , pbg] = [p0, ..., pC−1, pbg]. (3) We estimate the class-wise feature distribution of the test domain by exploiting Fk,t te and Eq.1. Furthermore, we in- troduce a weighting scheme for aligning features of less frequently appearing classes, taking into account the severe class imbalance where specific instance ( e.g., person) may appear multiple times within a single image, as follows: Nk,t = Nk,t−1 + ||Fk,t te ||, s.t. Nk,0 = 0 wk,t = log \u0012maxi Ni,t Nk,t \u0013 + 0.01 Lobj = X k wk,t · DKL(N(µk tr, Σk tr), N(µk,t te , Σk tr)). (4) Here, the class-wise mean µk and variance Σk of the train- ing and test data are obtained in the same way as the image- level features. We can effectively adapt the object detector by updating the model to align the feature distribution at both the image and object levels as L = Limg + Lobj. 3.4. When to update: Adaptation on demand As shown in Fig. 1, Ours, which only updates the adaptor proposed in Sec. 3.2, efficiently adapts to changes in the test domain, even with a small subset of early test samples. We leverage its rapid adaptation characteristics to reduce com- putational costs by skipping model updates ( i.e., skipping backward passes) when the model has already sufficiently adapted to the current test domain and resuming model up- dates when confronted with a new test domain. Therefore, we introduce two criteria to determine when to update the model or not as follows: (Criterion 1) When the distribution gap exceeds the in- domain distribution gap. Recall that Limg (Eq. 2) mea- sures the distribution gap between the test and train distri- butions. We assume a model is well-adapted to the current test domain when Limg is closer to the in-domain distri- bution gap. We measure the in-domain distribution gap by 4(a) The ratio of Limg to Din KL (b) The ratio of Limg to Lt ema Figure 3. The test domain undergoes a shift every 4,000 time steps, and each metric reaches its peak at the same intervals. sampling two disjoint subsets, xi and xj, of training fea- tures Ftr from Sec. 3.3 as follows: Din KL = DKL(N(µi tr, Σi tr), N(µj tr, Σj tr)), (5) where µi tr, Σi tr are obtained from xi ∼ Ptrain(x) and µj tr, Σj tr from xj ∼ Ptrain(x). In other words, if Limg is noticeably higher than the in-domain distribution gapDin KL, we consider a model encountering a test domain whose dis- tribution differs from Ptrain(x) and needs to be updated. Based on this, we introduce a new index Limg Din KL . Fig. 3a plots the trend of this index during the model adaptation to a con- tinually changing test domain. It shows that the index has a large value in the early stages of a domain change, decreases rapidly, and then maintains a value close to 1. This index exhibits a similar trend regardless of the backbone type and dataset, as included in the appendix. Therefore, we establish the criterion that model updates are necessary when this in- dex exceeds a certain threshold, τ1, as Limg Din KL > τ1. (Criterion 2 ) When the distribution gap suddenly in- creases. Additionally, we can determine when the test dis- tribution changes and model updates are necessary by ob- serving the trend of the distribution gap ( i.e., Limg). The convergence of Limg indicates that a model is well-adapted to the current test domain. To put it differently, Limg will exhibit a sudden increase when the model encounters a new test domain. We introduce an additional index, denoted as Limg Ltema , representing the ratio of the currentLimg to its expo- nentially moving averageLt ema at time t. We calculate it us- ing the following formula:Lt ema = 0.99·Lt−1 ema+0.01·Limg. Fig. 3b illustrates the trend of the ratio of Limg over the timesteps. It tends to reach a value of 1 as the loss stabilizes at a specific level. Nevertheless, when the model encounters shifts in the test distribution, the ratio experiences a sharp increase, indicating the necessity of a model update when it exceeds a specific threshold, τ2, as Limg Ltema > τ2. If at least one of the two criteria is satisfied, we conclude that the model requires adaptation and proceed to update it. 4. Experiments Sec. 4.1 presents the two object detection benchmark datasets with test distributions that change continuously, ei- ther in a drastic or gradual manner, and our implementation detail is in 4.2. Sec. 4.4 compares our method with other TTA baselines described in Secs. 4.3.. We present detailed ablation studies of our method analyzing the effectiveness and efficiency of our method in terms of what, how, and when to update the models for CTAOD in Sec. 4.5. 4.1. Datasets We experiment with the following three scenarios. COCO → COCO-C simulates continuous and drastic real- istic test domain changes over a long sequence. MS-COCO [23] collects 80 classes of common objects in their natural context with 118k training images and 5k validation images. COCO-C is created by employing 15 types of realistic cor- ruptions [27], such as image distortion and various weather conditions, to simulate test domain changes. In the experi- ments, the model is only trained on the COCO train set and sequentially evaluated on each corruption in the COCO-C validation set during test-time for reproducing continually changing test domains. Finally, the model is evaluated on the original COCO validation set to assess how well it pre- serves knowledge of the original domain (denoted as Org.). SHIFT-(Discrete / Continuous) [39] is a synthetic driving image dataset with 6 classes under different conditions us- ing five weather attributes (clear, cloudy, overcast, fog, rain) and three time-of-day attributes ( daytime, dawn, night ). In SHIFT-Discrete, there are image sets for each attribute, and the model is sequentially evaluated on these attributes, cloudy → overcast → foggy → rainy → dawn → night → clear which contains 2.4k, 1.6k, 2.7k, 3.2k, 1.2k, 1.4k, and 2.8k validation images, respectively. This simulates scenar- ios where the domain undergoes drastic changes. InSHIFT- Continuous, the model is evaluated on four sequences, each consisting of 4k frames, continuously transitioning from clear to foggy (or rainy) and back to clear. 4.2. Implementation Detail We experiment with Faster-RCNN [33] models using ResNet50 [10] and Swin-Tiny [26] as a backbone with FPN [24]. For the COCO → COCO-C adaptation, we em- ploy the publicity available models trained on COCO re- leased in [46] and [26] for ResNet5- and Swin-Tiny-based Faster-RCNN, respectively. For SHIFT experiments, mod- els are trained on the training domain using the detectron2 framework following [33] and [26]. For test-time adapta- tion, we always set the learning to 0.001 for the SGD opti- mizer, and α of Eq. 1 to 0.01, while τ1 and τ2 are set to 1.1 5Table 1. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on COCO→ COCO- C. Our model consistently outperforms baselines on the two different backbones. Furthermore, Ours-Skip with ResNet notably reduces backward passes by as much as 90.5%, leading to a significantly improved frames per second (FPS) rate by up to 109.9%. Noise Blur Weather Digital # step Backbone Method Gau Sht Imp Def Gls Mtn Zm Snw Frs Fog Brt Cnt Els Px Jpg Org. Avg. For. Back. FPS Swin-T [26] Direct-Test 9.7 11.4 10.0 13.4 7.5 12.1 5.2 20.7 24.8 36.1 36.0 12.9 19.1 4.9 15.8 43.0 17.7 80K 0 21.5 ActMAD 10.7 12.0 9.4 12.3 5.7 9.5 4.5 15.3 17.5 27.6 28.2 1.1 16.7 2.6 8.7 36.3 13.9 80K 80K 8.3 Mean-Teacher 10.0 12.1 11.2 12.8 8.1 12.1 4.9 19.6 23.7 34.9 34.0 8.0 18.9 6.1 17.6 41.0 17.2 160K 80K 6.9 Ours 13.6 16.6 16.1 14.0 13.6 14.2 8.3 23.7 27.2 37.4 36.4 27.2 27.2 22.2 22.3 42.3 22.6 80K 80K 9.5 Ours-Skip 13.3 15.3 15.1 14.0 12.8 13.9 6.5 22.0 25.4 35.5 34.9 26.5 25.9 23.4 20.2 41.2 21.6 80K 9.7K 17.7 ResNet50 [10] Direct-Test 9.1 11.0 9.8 12.6 4.5 8.8 4.6 19.1 23.1 38.4 38.0 21.4 15.6 5.3 11.9 44.2 17.3 80K 0 25.8 NORM 9.9 11.9 11.0 12.6 5.2 9.1 5.1 19.4 23.5 38.2 37.6 22.4 17.2 5.7 10.3 43.4 17.5 80K 0 25.8 DUA 9.8 11.7 10.8 12.8 5.2 8.9 5.1 19.3 23.7 38.4 37.8 22.3 17.2 5.4 10.1 44.1 17.1 80K 0 25.8 ActMAD 9.1 9.6 7.0 11.0 3.2 6.1 3.3 12.8 14.0 27.7 27.8 3.9 12.9 2.3 7.2 34.3 10.5 80K 80K 9.6 Mean-Teacher 9.6 12.5 12.0 4.0 2.9 4.8 3.1 16.2 23.5 35.1 34.0 21.8 16.6 8.2 12.7 40.3 14.5 160K 80K 8.1 Ours 12.7 17.8 17.5 12.4 11.5 11.3 6.6 22.8 26.9 38.6 38.5 28.0 25.1 21.2 22.2 41.8 22.2 80K 80K 10.1 Ours-Skip 14.4 17.1 16.0 13.9 11.7 12.2 6.3 22.1 25.5 37.7 37.1 25.5 24.1 23.1 21.1 42.8 21.9 80K 7.6K 21.2 and 1.05, respectively. We use the same hyper-parameters across all backbones and datasets. All experiments are con- ducted with a batch size of 4. 4.3. Baselines Direct-Test evaluates the model trained in the training do- main without adaptation to the test domain. ActMAD [29] is a TTA method aligning the distribution of output features across all BN layers. To apply ActMAD to the Swin Trans- former-based model, we align the output features of the LN layers. We implement Mean-Teacher using a teacher- student network framework to reproduce as close as possi- ble to TeST [36], as its implementation is not publicly avail- able. We follow the FixMatch [37] augmentation method and report results after tuning all hyper-parameters in our scenario. NORM [35] and DUA [28], TTA methods ini- tially designed for classification, are directly applicable to detection tasks by either mixing a certain amount of current batch statistics or updating batch statistics via EMA. How- ever, these are only compatible with architectures contain- ing BN layers. Additional details are provided in Appendix. 4.4. Main Results We compare the performance of each method using mAP and efficiency metrics, including the number of forward and backward passes, as well as FPS during test-time adapta- tion. Results of COCO and SHIFT are in Tab. 1 and 2, re- spectively. COCO → COCO-C. Tab. 1 demonstrates the effective adaptation performance of Ours in the challenging COCO benchmark with 80 classes due to object-level class-wise feature alignment. ActMAD also aligns feature distribution for TTA, but is not effective since it only aligns whole fea- ture maps without considering specific classes in the im- age. NORM and DUA, applicable only to ResNet [10], show minimal performance improvement by adaptation as they are not specifically tailored for object detection and only modify batch statistics across the entire feature map. Ad- ditionally, ActMAD and Mean-Teacher, updating full pa- rameters, gradually lose task knowledge in the continually changing test distributions, resulting in much lower perfor- mance on Org. , the domain identical to the training data, than that of Direct-Test. In contrast, Ours effectively pre- vents catastrophic forgetting by freezing the original param- eters of the models and updating only the adaptor, obtain- ing performance on par with Direct-Test on the Org. do- main and consistently high performance across corrupted domains, with an average mAP improvement of 4.9%p compared to that of Direct-Test. Furthermore, leveraging the rapid adaptation ability of the adaptor,Ours-Skip, which skips unnecessary adaptation, allows using only a maxi- mum of about 12% of the total samples for adaptation with- out significant performance loss. This leads to a substantial improvement in inference speed, more than doubling com- pared to other TTA methods, reaching over 17.7 FPS. SHIFT-Discrete. Ours is also effective in SHIFT, which simulates continuous changes in weather and time in driv- ing scenarios according to the left section of Tab. 2. Espe- cially, Ours shows significant improvements in mAP by 7- 9%p, particularly for the foggy and dawn attributes where Direct-Test obtains lower performance due to severe do- main shift. In contrast, with ActMAD, catastrophic forget- ting takes place when adapting to the cloudy and overcast weather. This is due to the updating of the full parame- ters, despite that Direct-Test already shows proficient per- formance in these conditions. As a result, the performance in the later domains is worse than that of the Direct-Test. DUA, which updates batch statistics using EMA, shows a gradual decrease in performance as the domain contin- uously changes, resulting in much lower performance in the original clear domain ( i.e., clear ). On the other hand, NORM, which utilizes the statistics of the current batch samples, exhibits no catastrophic forgetting and relatively good adaptation, as SHIFT is a relatively easier task com- pared to COCO due to having only 6 classes. Compared to NORM, Ours shows better adaptation performance, and is 6Table 2. Comparison of mAP, the number of backward and forward passes, and FPS between baselines and our model on SHIFT-Discrete and SHIFT-Continuous. Baselines perform effectively in a particular setting but lack generalizability across various settings. Our method consistently achieves results that are either better or on par with the best model in all settings, demonstrating its strong stability. Ours-Skip also effectively reduces the number of backward passes without compromising mAP performance, resulting in a higher FPS. SHIFT-Discrete SHIFT-Continuous mAP # step mAP # Avg. step Backbone Method cloudy overc. fog rain dawn night clear Avg. For. Back. FPS clear↔fog clear ↔rain For. Back. FPS Swin-T [26] Direct-Test 50.0 38.9 23.1 45.1 26.9 39.5 45.9 38.5 15.3K 0 27.5 18.1 21.1 4K 0 28.3 ActMAD 49.8 38.4 21.4 43.1 19.0 32.0 44.8 35.5 15.3K 15.3K 9.3 15.6 16.3 4K 4K 9.8 Mean-Teacher 50.0 39.2 25.7 45.4 26.0 37.5 42.2 38.0 15.3K 15.3K 7.8 20.4 24.3 8K 4K 6.5 Ours 50.3 39.2 32.2 46.7 30.4 39.9 44.3 40.4 15.3K 15.3K 11.2 23.9 22.6 4K 4K 11.6 Ours-Skip 50.3 39.7 29.1 47.1 30.2 41.5 45.9 40.6 15.3K 6.1K 20.0 25.1 23.8 4K 0.83K 19.2 ResNet50 [10] Direct-Test 49.4 37.9 19.7 43.1 20.1 35.3 45.6 35.9 15.3K 0 30.1 12.1 15.4 4K 0 30.0 NORM 49.7 38.6 22.9 44.7 25.1 37.4 45.5 37.7 15.3K 0 30.1 16.9 19.4 4K 0 30.0 DUA 45.2 31.5 27.7 31.9 15.2 18.6 21.1 27.3 15.3K 0 30.1 22.5 22.4 4K 0 30.0 ActMAD 49.2 37.7 18.0 40.6 16.0 32.9 44.3 34.1 15.3K 15.3K 11.3 12.7 16.3 4K 4K 11.2 Mean-Teacher 49.6 38.4 26.8 43.4 26.6 33.1 41.6 37.1 15.3K 15.3K 9.9 16.0 20.8 8K 4K 9.8 Ours 49.7 38.7 27.4 46.3 27.4 37.6 43.8 38.7 15.3K 15.3K 12.9 20.9 21.9 4K 4K 13.9 Ours-Skip 49.7 38.8 26.9 46.2 27.6 38.8 45.0 39.0 15.3K 8.9K 21.5 20.0 22.5 4K 0.75K 21.3 Table 3. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to which part of the backbone is updated. SD / SC de- notes SHIFT-Discrete/Continuous, respectively. mAP # Params Cache Backbone Trainable Params SD SC Num Ratio Avg. Max Swin-T Full-params 38.4 20.6 27.7M 100% 0.86 11.0 LayerNorm 38.5 20.0 0.03M 0.1% 0.65 7.49 adaptor (Ours) 40.4 23.2 0.15M 0.5% 0.65 6.96 ResNet50 Full-params 37.6 20.4 23.7M 100% 1.65 9.29 BatchNorm 37.9 20.2 0.05M 0.2% 1.47 9.11 adaptor (Ours) 38.7 21.7 0.21M 0.9% 1.48 5.41 also applicable to BN-layer-free Swin Transformers. SHIFT-Continuous. In scenarios where the test domain gradually changes across the entire sequence, Ours also demonstrates effectiveness, improving mAP by up to 7%p, as shown in the right section of Tab. 2. WhileDUA performs well in the clear to foggy transition, it is prone to catas- trophic forgetting in situations where the sequence becomes longer, and the test domain changes more diversely, as seen in the left section. Our strategy for determining when model adaptation is necessary is particularly effective in SHIFT. It improves FPS by about 9, reaching about 20 FPS, while en- hancing mAP. This is likely due to avoiding overfitting that can occur when adapting to all repetitive frames in SHIFT, which consists of continuous frames, leading to improve- ments in both inference speed and adaptation performance. 4.5. Additional Analyses We aim to demonstrate the effectiveness and detailed anal- ysis of our proposed model in terms of 1) which parts of, 2) how, and 3) when the model should be updated. Which part to update? Tab. 3 shows how updating dif- ferent parts of the backbone model affects the performance and the memory usage during continual test-time adapta- Table 4. Ablation on each component of our loss. SHIFT-D / C denotes SHIFT-Discrete / Continuous, respectively. The left and right value in each cell corresponds to the mAP for the Swin-T and ResNet50 backbone, respectively. Limg Lobj COCO SHIFT-D. SHIFT-C. - - 17.7/ 17.3 38.5/ 35.9 19.6/ 13.8 ✔ - 16.7/ 18.1 36.6/ 37.0 19.1/ 16.0 ✔ no class weight 17.8/ 18.9 39.7/ 38.0 25.1/ 23.4 ✔ class weight wk,t 22.6/ 22.2 40.4/ 38.7 23.2/ 21.7 tion. We compare (1) updating full parameters, (2) affine parameters of the normalization layer, and (3) our proposed adaptor for each backbone on the SHIFT dataset. Although our adaptor has fewer parameters, about 0.9% or less of the full parameters, it demonstrates the best adaptation perfor- mance. Updating only the affine parameters of the normal- ization layer, while having fewer parameters, seems less ef- fective for adaptation in object detection compared to clas- sification [30, 43]. Additionally, our adaptor requires only about 60% of the memory compared to updating the full parameters, making it memory-efficient. Ablation study on each component in our loss. Tab. 4 presents the effects of image-level feature alignment,Limg, object-level feature class-wise alignment Lobj, and class frequency weighting wk,t proposed to address class im- balance. Aligning only the image-level feature distribu- tion with Limg (first row) leads to modest adaptation in the ResNet50 backbone, while performance in the Swin- T backbone is even lower than without adaptation. No- tably, aligning object-level features with Lobj leads to a substantial improvement, with the mAP increasing by approximately 10%p compared to the no-adaptation sce- nario. Introducing class-specific frequency-based weighting wk,t, despite a slight performance decrease in the SHIFT- Continuous setting, proves highly effective, particularly in scenarios with significant class imbalance, such as COCO 7(a) Swin Transformer backbone  (b) ResNet50 backbone Figure 4. Comparison of mAP and FPS fromOurs-Skip with vary- ing values of τ1 (♦) and τ2 (▲) against Evenly-Skip (×), adapting every N-th instances, on COCO→COCO-C using both (a) Swin- T and (b) ResNet50. The upward and rightward movement indi- cates a better strategy with higher mAP and faster inference speed, showing that Ours-Skip is consistently better than Evenly-Skip. (a) Accumulated number of backward steps (b) Number of backward steps and mAP of Direct-Test in each domain Figure 5. Analysis of the adaptation of Ours-Skip. with 80 classes, where it enhances the mAP by around 5%p. Trade-off between adaptation performance and effi- ciency according to different skipping strategies. Fig. 4 presents mAP and FPS depending on the values ofτ1 and τ2 in the Sec. 3.4 on COCO → COCO-C, which are used for two criteria to determine when the adaptation is needed. We also show the simple baselineEvenly-Skip, which adapts ev- ery N-th step and skips the rest. In Fig. 4, the blue lines (▲) show the results when τ1 is changing from 1.0 to infinity, where only criterion 2 is used, while τ2 is fixed at 1.05. As τ1 decreases, more adaptation is required, leading to slower FPS but higher mAP. The green lines (♦) show the results of changing τ2, where ‘τ2 = inf’ denotes using only criterion 1, without criterion 2. For all main experiments, we set τ1 and τ2 as 1.1 and 1.05, respectively, considering the balance between mAP and FPS. Additionally, our skipping strategy consistently outperforms Evenly-Skip, achieving higher val- ues in both mAP and FPS. This indicates that our criterion for deciding when to bypass model updates provides an ef- fective balance between accuracy and speed. When do models actually update? We analyze when the model actually skips adaptation and only performs infer- ence or actively utilizes test samples for model adaptation based on the two criteria we propose. This analysis is con- ducted in COCO to COCO-C with 15 corruption domains and 1 original domain. Fig. 5a plots the number of back- ward passes, i.e., the number of batches of test samples used for adaptation, with different values of τ1 for the two backbones. The horizontal and vertical axes represent se- quentially incoming test domains and the cumulative back- ward numbers, respectively. A steep slope in a region in- dicates frequent adaptation, while a gentle slope indicates skipping adaptation, performing only inference. Notably, even without explicit information about when the test do- main changes, the model actively performs adaptation, es- pecially right after the test domain changes. This trend is consistent regardless of changes in τ value or backbone type. Furthermore, it is evident that the number of backward passes is primarily determined by the value ofτ1 rather than the type of backbone, suggesting that a consistent τ1 value can be used irrespective of the backbone. Fig. 5b visually represents the adaptation tendencies by dividing backward steps for each domain in the case of Swin-T backbone with τ1 = 1.1. More clearly, it shows that adaptation occurs ac- tively around the points where each domain changes, and af- terward, adaptation happens intermittently or almost not at all. The light pink bars represent the performance ofDirect- Test, showing that domains with initially high model per- formance tend to have less adaptation, while domains with lower performance initially need more adaptation. In other words, the amount of skipping adaptation is proportional to the amount of the domain shift. Interestingly, the second do- main, ’Shot Noise’, shows almost no adaptation despite the lower performance of the Direct-Test. We conjecture that the preceding domain, ’Gaussian Noise’, shares a similar nature of noise, leading the model to decide that additional adaptation steps may not be necessary. As a result, our skip- ping strategy enables the model to efficiently adapt, consid- ering both the original domain the model is trained on and the previous domain the model has been adapted to. 5. Conclusion We introduce an efficient Continual Test-time Adaptation (CTA) method for object detection in the continually chang- ing domain. Our approach involves 1) lightweight adap- tors, 2) class-wise object-level feature alignment, and 3) skipping unnecessary adaptation. These contributions col- lectively yield a highly efficient and effective adaptation method, showcasing robustness to diverse domain shifts, and achieving notable improvements in mAP performance across various CTA scenarios without serious slowdown in the inference speed. 8What, How, and When Should Object Detectors Update in Continually Changing Test Domains? Supplementary Material 6. Additional Details for Baselines We provide additional implementation details for each base- line model. Our framework incorporates all baseline models using the official code except Mean-Teacher. The results of the experiments are reported based on the optimal hyperpa- rameters that yield the best results in our scenario. ActMAD [29] As ActMAD exclusively conducts experi- ments on the KITTI dataset, where all images have a con- stant height and width (e.g., 370 x 1224), ensuring consis- tent feature map sizes for all samples. ActMAD can easily align them along the spatial axis. However, in the general setting of object detection tasks, such as the COCO bench- mark set, where image sizes and width-to-height ratios vary, aligning feature maps along the spatial axis becomes chal- lenging due to different sizes. To adapt ActMAD to our COCO → COCO-C scenario, we perform center cropping on the feature maps to match the size of training domain fea- ture maps and the current test sample feature maps. We em- ploy a learning rate of 1e-5 for COCO and 1e-4 for SHIFT, respectively. Mean-Teacher As the official code of TeST [36] is not available, we implement the EMA-updated Teacher and Student models following TeST [36], to conduct experi- ments in our scenarios. TeST involves three forward steps for a batch: forwarding weakly augmented samples through the student network, strong augmented samples through the teacher network, and original samples through the teacher network for outputs. However, for a fair comparison, we perform two forward steps, forwarding the original sample through the teacher network and strong augmented sam- ples through the student network, to make predictions be- fore adaptation for every samples. We utilize a learning rate of 1e-5 and set the EMA update rate for the teacher network to 0.999. NORM [35] We set the hyperparameter N that controls the trade-off between training statistics and estimated tar- get statistics as 128. DUA [28] We set the momentum decay as 0.94, minimum momentum constant as 1e-4, and the initial momentum de- cay as 1e-3. 7. The effect of Bottleneck Reduction Ratio in the Adaptor Table 5 shows the results for COCO → COCO-C, SHIFT- Discrete, and SHIFT-Continuous based on the dimension reduction ratio ( r) discussed in Section 3.2, representing Table 5. Comparison of adaptation performance (mAP), the num- ber of trainable parameters (# Params), and memory usage (Cache) according to r of Sec. 3.2, the bottleneck reduction ratio in the adaptor. We set r as 32 for all our experiments in the main paper. SD / SC denotes SHIFT-Discrete / Continuous, respectively. mAP # Params Cache Backbone r COCO SD SC Num Ratio Avg. Max Swin-T 1 22.6 40.0 21.3 4.33M 15.7% 0.75 7.51 2 22.6 40.3 23.2 2.17M 7.85% 0.73 7.27 4 22.6 40.4 23.2 1.09M 3.95% 0.70 7.06 8 22.6 40.4 23.2 0.55M 2.00% 0.69 7.00 16 22.6 40.4 23.2 0.28M 1.02% 0.67 6.98 32 22.6 40.4 23.2 0.15M 0.54% 0.65 6.96 64 22.6 40.4 23.2 0.08M 0.29% 0.65 6.95 ResNet50 1 22.5 38.7 20.8 6.31M 26.7% 1.55 5.89 2 22.4 38.7 20.9 3.16M 13.4% 1.51 5.64 4 22.3 38.6 21.3 1.59M 6.71% 1.49 5.52 8 22.3 38.6 21.4 0.80M 3.39% 1.48 5.46 16 22.2 38.6 21.4 0.41M 1.73% 1.48 5.43 32 22.2 38.7 21.4 0.21M 0.89% 1.48 5.41 64 22.1 38.7 21.3 0.11M 0.48% 1.48 5.40 the ratio of bottleneck size compared to the input size in the adaptor. The adaptation performance remains consistent across different r values. However, in the case of r = 1 in SHIFT experiments, mAP decreases, potentially due to catastrophic forgetting resulting from a large number of adaptable parameters. Since increasing the value of r sig- nificantly reduces the number of learnable parameters and memory usage, we set r to 32 in all other experiments. 8. Results on the KITTI Dataset We conduct additional experiments on the KITTI [8] dataset, the commonly used object detection dataset consist- ing of driving scenes with 8 classes (car, van, truck, person, person sitting, cyclist, tram, misc). To simulate the continu- ally changing domains, we use the following scenario ( Fog → Rain → Snow → Clear) as done in [29]. We use the physics-based rendered dataset [9] forfog and rain and sim- ulate snow using the corruption library from [11]. We use the same split of [29], which divides the 7,441 training sam- ples into 3,740 training and 3,741 test samples. We train the Faster-RCNN using 3,741 training samples representing the Clear attribute with Swin-Transformer and ResNet50 back- bones, and evaluate it sequentially on Fog, Rain, Snow, and Clear test samples. We conduct all experiments with a batch size of 16 on 1 RTX A6000 GPU. Table 6 shows the mAP@50, the num- 1Table 6. Comparison of mAP, the number of backward and forward passes, FPS, and memory usage between baselines and our models on the continually changing KITTI datasets ( Fog → Rain → Snow → Clear). Our models improve mAP@50 by 15.1 and 11.3 for Swin-T and ResNet50 backbone, respectively, compared to Direct-Test while maintaining comparable FPS. All experiments are conducted with a batch size of 16. mAP@50 # For. Steps # Backward Steps FPS Cache Backbone Method Fog Rain Snow Clear Avg. All Fog Rain Snow Clear All Avg. Avg. Max Swin-T Direct-Test 46.9 69.5 28.7 89.6 58.7 936 0 0 0 0 0 24.7 0.4 5.5 ActMAD 53.3 78.1 41.2 90.7 65.8 936 234 234 234 234 936 16.8 0.8 21.9 Mean-Teacher 54.5 80.2 43.2 92.4 67.6 936 234 234 234 234 936 10.0 1.0 22.6 Ours 56.7 82.1 64.6 91.8 73.8 936 234 234 234 234 936 17.1 0.4 11.8 Ours-Skip 57.4 81.5 64.3 91.3 73.6 936 234 65 224 36 559 22.9 0.4 11.8 ResNet50 Direct-Test 33.4 63.5 29.8 88.6 53.8 936 0 0 0 0 0 27.7 0.8 4.3 NORM 38.4 66.4 35.9 87.3 57.0 936 0 0 0 0 0 27.7 0.8 4.3 DUA 34.8 67.7 30.9 89.0 55.6 936 0 0 0 0 0 27.7 0.8 4.3 ActMAD 40.4 66.5 42.7 84.5 58.5 936 234 234 234 234 936 18.5 1.6 22.6 Mean-Teacher 39.6 71.3 43.5 88.2 60.6 936 234 234 234 234 936 11.1 1.8 31.1 Ours 45.6 71.4 52.5 88.3 64.5 936 234 234 234 234 936 18.8 0.8 9.4 Ours-Skip 45.8 71.3 50.9 88.4 64.1 936 234 111 98 45 488 24.5 0.8 9.4 (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 6. Results of COCO images corrupted by Shot-Noise. In the analysis of Sec. 4.5, we conjecture that Ours largely skips adaptation in Shot-Noise domain, despite the low mAP of Direct-Test, because the model has already adapted to a similar domain, Gaussian-Noise. In (c), at the first step before adaptation to the Shot-Noise, our model already predicts ’Oven’ and ’Refrigerator’ which Direct-Test fails to detect. This results in a much faster adaptation, and Ours successfully detects various objects, including rare ones such as ’Fire Hydrants’, in the remaining images of the Shot-Noise domain. ber of forward and backward steps, FPS, and memory usage (Cache). Ours improves the mAP@50 by 15.1 and 10.7 for Swin-T and ResNet50 backbones, respectively, compared to Direct-Test. Compared to ActMAD and Mean-Teacher, our model not only improves the adaptation performance but also reduces memory usage, as we update only an ex- tremely small number of parameters of the adaptor. Further- more, using our skipping criteria of Sec. 3.4 with τ = 1.1 and β = 1.05, we can improve FPS by more than 5.8 with- out sacrificing mAP@50, resulting in much faster inference 2(a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 7. Results for COCO images corrupted by Pixelate. In the Pixelate domain, where the model has already experienced various corruptions in a long sequence, Ours initially incorrectly detects objects. In (c), it misidentifies a bed as a couch in the first step. However, it rapidly adapts to the Pixelate domain and effectively detects various objects. Notably, even in cases whereDirect-Testcorrectly identifies objects but with low confidence, Ours detects them with much higher confidence. (a) GT bounding boxes. (b) Prediction results of Direct-Test. (c) Prediction results of Ours. Figure 8. Results for SHIFT-Discrete with continually changing attributes, foggy → rainy → dawn → night. speed compared to other TTA baselines. 39. Qualitative Results Fig. 6 and 7 and Fig. 8 show the qualitative results of Ours and Direct-Test which predict the samples without adapta- tion for COCO → COCO-C and SHIFT, respectively. 9.1. COCO → COCO-C Fig. 6 and 7 compare the prediction results for COCO im- ages corrupted. When the model encounters test images with various corruptions sequentially ( Gaussian-Noise → Shot-Noise → Impulse-Noise → Defocus-Blur → Glass- Blur → Motion-Blur → Zoom-Blur → Snow → Frost → Fog → Brightness → Contrast → Elastic-Transform → Pixelate → JPEG-Compression → Original), Fig. 6 and 7 shows the results when the test images are corrupted by Shot-Noise and Pixelate, respectively. Compared to Direct- Test, our model adapts to the current domain within a few steps, such as 100 iterations, and detects various objects very well in the remaining incoming images. 9.2. SHIFT-Discrete Fig. 8 shows the qualitative results for SHIFT-Discrete. In the SHIFT-Discrete scenario, the model encounters environ- ments sequentially, transitioning from cloudy → overcast → foggy → rainy → dawn → night → clear. Figure. 8 se- lectively shows the foggy → rainy → dawn → night se- quence, where the domain gap from the original clear envi- ronments is relatively large. Compared to Direct-Test, Ours detects various objects such as ’cars’ and ’pedestrians’ re- gardless of distribution changes. References [1] Alexander Bartler, Florian Bender, Felix Wiewel, and Bin Yang. Ttaps: Test-time adaption by aligning prototypes using self-supervision. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2022. 2 [2] Alexander Bartler, Andre B ¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics , pages 3080–3090. PMLR, 2022. 2 [3] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 8344–8353, 2022. 1 [4] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3582–3591, 2023. 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition.Advances in Neural Information Processing Systems, 35:16664–16678, 2022. 3 [6] Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Im- proving object detection robustness at test-time by self- training with feature alignment regularization.arXiv preprint arXiv:2303.17937, 2023. 1, 2, 3 [7] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un- biased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4091–4101, 2021. 3 [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The Inter- national Journal of Robotics Research , 32(11):1231–1237, 2013. 1 [9] Shirsendu Sukanta Halder, Jean-Franc ¸ois Lalonde, and Raoul de Charette. Physics-based rendering for improving robustness to rain. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 10203–10212, 2019. 1 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6, 7 [11] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 1 [12] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. Mecta: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2022. 3 [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [14] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International conference on machine learn- ing, pages 448–456. pmlr, 2015. 2 [15] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for modelagnostic domain generaliza- tion. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 3 [16] Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test- time adaptation via self-training with nearest neighbor infor- mation. In International Conference on Learning Represen- tations (ICLR), 2023. 3 [17] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Sha- ban, Byron Boots, and Jaegul Choo. Cafa: Class-aware fea- ture alignment for test-time adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2023. 2, 3, 4 [18] Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 480– 490, 2019. 3 [19] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object 4detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6092–6101, 2019. 3 [20] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper- vised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 8474–8481, 2021. 4 [21] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical do- main adaptation, 2017. 3 [22] Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization in test- time adaptation, 2023. 2, 3 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117–2125, 2017. 5 [25] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems , 34: 21808–21820, 2021. 1, 2 [26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 2, 5, 6, 7 [27] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win- ter is coming. arXiv preprint arXiv:1907.07484, 2019. 5 [28] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsuper- vised domain adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14765–14775, 2022. 3, 6, 1 [29] Muhammad Jehanzeb Mirza, Pol Jan ´e Soneira, Wei Lin, Ma- teusz Kozinski, Horst Possegger, and Horst Bischof. Act- mad: Activation matching to align distributions for test-time- training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 24152– 24161, 2023. 1, 2, 3, 4, 6 [30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Interna- tional conference on machine learning, pages 16888–16905. PMLR, 2022. 1, 2, 3, 7 [31] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. arXiv preprint arXiv:2302.12400, 2023. 3 [32] Mario obler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 7704–7714, 2023. 3 [33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. 2016. 5 [34] Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and Erik Learned-Miller. Automatic adaptation of object detectors to new domains using self-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3 [35] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing sys- tems, 33:11539–11551, 2020. 3, 6, 1 [36] Samarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In Proceedings of the IEEE/CVF Winter Conference on Ap- plications of Computer Vision, pages 2759–2769, 2023. 1, 2, 3, 6 [37] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596– 608, 2020. 2, 6 [38] Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test- time training: Sequential inference and adaptation by an- chored clustering. Advances in Neural Information Process- ing Systems, 35:17543–17555, 2022. 3, 4 [39] Tao Sun, Mattia Segu, Janis Postels, Yuxuan Wang, Luc Van Gool, Bernt Schiele, Federico Tombari, and Fisher Yu. Shift: a synthetic driving dataset for continuous multi-task domain adaptation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 21371–21382, 2022. 1, 5 [40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229– 9248. PMLR, 2020. 1, 2 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [42] Vibashan VS, Poojan Oza, and Vishal M Patel. Towards on- line domain adaptive object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 478–488, 2023. 3 [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:2006.10726, 2020. 1, 2, 3, 7 5[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022. 3 [45] Zehao Xiao, Xiantong Zhen, Shengcai Liao, and Cees GM Snoek. Energy-based test sample adaptation for domain gen- eralization. arXiv preprint arXiv:2302.11215, 2023. 3 [46] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End- to-end semi-supervised object detection with soft teacher. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5 [47] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems , 35: 38629–38642, 2022. 1, 2, 3 [48] Bowen Zhao, Chen Chen, and Shu-Tao Xia1. Delta: Degradation-free fully test-time adaptation. In International Conference on Learning Representations (ICLR), 2023. 2, 3 6",
      "meta_data": {
        "arxiv_id": "2312.08875v1",
        "authors": [
          "Jayeon Yoo",
          "Dongkwan Lee",
          "Inseop Chung",
          "Donghyun Kim",
          "Nojun Kwak"
        ],
        "published_date": "2023-12-12T07:13:08Z",
        "pdf_url": "https://arxiv.org/pdf/2312.08875v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a novel online adaptation approach for object detection in continually changing test domains. Its key contributions include the introduction of architecture-agnostic and lightweight adaptor modules (requiring only 0.54% to 0.89% additional parameters) for efficient adaptation and prevention of catastrophic forgetting. It presents a practical and straightforward class-wise feature aligning method that operates at both image and object levels to resolve domain shifts. Furthermore, the method enhances efficiency by determining when the model requires adaptation through two novel criteria. This approach significantly outperforms baselines, achieving mAP improvements of up to 4.9%p and 7.9%p on COCO-corrupted and SHIFT benchmarks respectively, while maintaining high inference speeds (around 20 FPS or higher). It also demonstrates robust preservation of task-specific knowledge and wide applicability across diverse backbone architectures like CNNs and Transformers.",
        "methodology": "The proposed method focuses on three aspects: what, how, and when to update the object detector. For 'what to update', it introduces shallow, low-rank, architecture-agnostic adaptor modules in parallel to each backbone block (MLP for Transformers, 1x1 CNN for ResNet). Only these adaptors are updated, while the pre-trained backbone parameters remain frozen. For 'how to update', it employs an Exponentially Moving Average (EMA) feature alignment strategy. This involves aligning feature distributions at both image-level (Limg) and region-level class-wise (Lobj) by minimizing the KL divergence between training and test domain features. Mean and variance statistics are used, with test domain means estimated via EMA and test variances approximated from training variances. A class-frequency weighting scheme is applied for object-level alignment to address class imbalance. The total adaptation loss is a sum of Limg and Lobj. For 'when to update', two criteria are used to dynamically skip or resume adaptation: 1) Model updates if the ratio of the current image-level distribution gap (Limg) to a pre-computed in-domain distribution gap (Din KL) exceeds a threshold (τ1=1.1). 2) Model updates if Limg suddenly increases relative to its exponentially moving average (Lt ema) by a threshold (τ2=1.05). Adaptation proceeds if at least one criterion is met.",
        "experimental_setup": "The research used Faster-RCNN models with ResNet50 and Swin-Tiny backbones, both integrated with FPN. Experiments were conducted across three continually changing domain scenarios: COCO \n COCO-C (models trained on MS-COCO and evaluated sequentially on 15 types of corruptions from COCO-C, with additional evaluation on the original COCO validation set) and SHIFT-(Discrete / Continuous) (a synthetic driving dataset with 6 classes, featuring variations in weather and time-of-day, tested in sequential drastic shifts for Discrete and gradual transitions for Continuous). Publicly available models were used for COCO, while SHIFT models were trained using the Detectron2 framework. Test-time adaptation employed an SGD optimizer with a learning rate of 0.001, an EMA update rate (α) of 0.01, and thresholds τ1=1.1 and τ2=1.05 for adaptation criteria. A batch size of 4 was used for main experiments. Baselines included Direct-Test (no adaptation), ActMAD, NORM, DUA, and a Mean-Teacher model implemented based on TeST. Performance was evaluated using mAP, the number of forward and backward passes, and Frames Per Second (FPS).",
        "limitations": "The approximation of test feature variance (Σte \n Σtr) is made to reduce instability. The 'when to update' criteria rely on specific assumptions regarding the relationship between the image-level distribution gap (Limg) and both in-domain and historical gaps. Additionally, the class frequency weighting scheme (wk,t) for object-level alignment, while generally effective, showed a slight performance decrease in the SHIFT-Continuous setting during ablation studies, suggesting potential trade-offs in specific dynamic scenarios.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
      "abstract": "Fully test-time adaptation aims at adapting a pre-trained model to the test\nstream during real-time inference, which is urgently required when the test\ndistribution differs from the training distribution. Several efforts have been\ndevoted to improving adaptation performance. However, we find that two\nunfavorable defects are concealed in the prevalent adaptation methodologies\nlike test-time batch normalization (BN) and self-learning. First, we reveal\nthat the normalization statistics in test-time BN are completely affected by\nthe currently received test samples, resulting in inaccurate estimates. Second,\nwe show that during test-time adaptation, the parameter update is biased\ntowards some dominant classes. In addition to the extensively studied test\nstream with independent and class-balanced samples, we further observe that the\ndefects can be exacerbated in more complicated test environments, such as\n(time) dependent or class-imbalanced data. We observe that previous approaches\nwork well in certain scenarios while show performance degradation in others due\nto their faults. In this paper, we provide a plug-in solution called DELTA for\nDegradation-freE fuLly Test-time Adaptation, which consists of two components:\n(i) Test-time Batch Renormalization (TBR), introduced to improve the estimated\nnormalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to\naddress the class bias within optimization. We investigate various test-time\nadaptation methods on three commonly used datasets with four scenarios, and a\nnewly introduced real-world dataset. DELTA can help them deal with all\nscenarios simultaneously, leading to SOTA performance.",
      "meta_data": {
        "arxiv_id": "2301.13018v1",
        "authors": [
          "Bowen Zhao",
          "Chen Chen",
          "Shu-Tao Xia"
        ],
        "published_date": "2023-01-30T15:54:00Z",
        "pdf_url": "https://arxiv.org/pdf/2301.13018v1.pdf"
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Test-Time Training (TTT), a novel approach to enhance the generalization of predictive models when training and test data distributions differ. TTT converts a single unlabeled test sample into a self-supervised learning problem, updating model parameters before making a prediction. This method also naturally extends to online data streams by preserving parameter states. The key findings include significant performance improvements on diverse image classification benchmarks evaluating robustness to distribution shifts (e.g., CIFAR-10-C, ImageNet-C, VID-Robust, CIFAR-10.1) without compromising performance on the original data distribution. A theoretical analysis establishes that positive gradient correlation between the main task and the auxiliary self-supervised task is a sufficient condition for TTT to be effective in convex models, a condition empirically shown to hold for deep learning models.",
        "methodology": "The core methodology involves adapting a pre-trained model at test time using self-supervision. The model is a K-layer neural network (ResNets in experiments) structured with a shared feature extractor (layers `θ1` to `θκ`) and two separate branches for the main task and the self-supervised auxiliary task (layers `θκ+1` to `θK` for each). The auxiliary task employed is rotation prediction, where the model predicts the rotation angle (0, 90, 180, or 270 degrees) of an input image. Initially, the model undergoes joint training on both the main classification task and the self-supervised task using labeled training data. At test time, for each unlabeled test sample, the shared feature extractor's parameters are fine-tuned by minimizing the self-supervised auxiliary task's loss on that sample. The standard TTT performs 10 gradient steps on each sample, discarding updates for the next sample. The online version (TTT-Online) takes one gradient step per new sample and preserves the updated parameters, allowing for adaptation over a stream of data. Group Normalization (GN) is used instead of Batch Normalization (BN) due to the small batch size (single image with augmentations) during test-time updates. Data augmentation (random crop, horizontal flip) is applied to the test sample to form a batch for training.",
        "experimental_setup": "Experiments were conducted using ResNet-26 for CIFAR-10 based datasets and ResNet-18 for ImageNet based datasets. The datasets included CIFAR-10-C and ImageNet-C, which consist of images corrupted with 15 types of corruptions at 5 severity levels, serving as benchmarks for robustness to common corruptions. The VID-Robust dataset, comprising video frames for object recognition, was used to evaluate generalization across time and object classes. CIFAR-10.1 was used to test generalization under subtle, unknown distribution shifts. Baselines for comparison included: 1) a plain ResNet model ('object recognition task only'), 2) a model jointly trained on main and self-supervised tasks but fixed at test time ('joint training'), 3) Adversarial Logit Pairing (ALP) for CIFAR-10 benchmarks, and 4) Unsupervised Domain Adaptation by Self-Supervision (UDA-SS) on CIFAR-10-C as an 'oracle' comparison with access to the entire unlabeled test set. Optimization for joint training used stochastic gradient descent (SGD) with standard hyperparameters. Test-time training also used SGD, with a learning rate of 0.001 (last epoch's training LR) and zero weight decay/momentum. Standard TTT took 10 gradient steps per sample, while TTT-Online took 1 step per sample. Evaluation was based on test error (%) or accuracy (%).",
        "limitations": "The primary limitation identified is the computational overhead, as Test-Time Training is significantly slower than standard inference (2 \n\t batch_size \n\t number_of_iterations times). While potential solutions like thresholding on the self-supervised loss or reducing iterations were explored, they were not thoroughly verified or the main focus of this initial work. Another constraint is the reliance on a meaningful self-supervised task; if the task becomes trivial (e.g., rotation prediction for images with clear black margins or ambiguous objects like airplanes in the sky), the benefits of TTT diminish. The theoretical guarantees are primarily for convex models, although empirical evidence suggests they extend to non-convex deep learning models. Furthermore, Batch Normalization (BN) proved ineffective with the small batch sizes inherent to TTT, necessitating the use of Group Normalization (GN), and online TTT did not work well with BN, which limits broader applicability without architectural adjustments.",
        "future_research_directions": "Future research directions include extending Test-Time Training to other machine learning tasks such as segmentation, detection, speech recognition, and natural language processing. Leveraging domain-specific knowledge to design more effective special-purpose self-supervised tasks is suggested. The paper also proposes using TTT as a new evaluation benchmark for general-purpose self-supervised tasks, alongside existing pre-training and fine-tuning benchmarks. Improving the computational efficiency of TTT, potentially by developing models more amenable to fast updates during training, is another important area. Further formal theoretical discussion on the concept of a variable decision boundary is also suggested. Finally, the authors encourage a broader paradigm shift, advocating for abandoning the strict division between training and testing and promoting a framework where much of the learning continues after model deployment, drawing inspiration from one-shot learning advancements."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "This research introduces Practical Test-Time Adaptation (PTTA), a novel and more realistic TTA setting that simultaneously addresses continually changing distributions and correlative data sampling, challenges often encountered in real-world dynamic scenarios. The paper benchmarks prior TTA methods under PTTA, revealing their limitations. It then proposes Robust Test-Time Adaptation (RoTTA), a method designed to comprehensively tackle PTTA challenges, emphasizing its ease of implementation and practical deployment. RoTTA significantly outperforms state-of-the-art baselines, reducing average classification error by over 5.9% on CIFAR-10-C, 5.5% on CIFAR-100-C, and 2.2% on DomainNet.",
        "methodology": "RoTTA consists of three main components: 1) Robust Batch Normalization (RBN), which replaces standard BN layers with a scheme that estimates normalization statistics using global running mean and variance, updated via exponential moving average from buffered samples, ensuring stability against correlative data. 2) Category-balanced sampling with timeliness and uncertainty (CSTU), which maintains a memory bank to store samples with pseudo-labels, prioritizing category balance, timeliness (newer samples), and low uncertainty (calculated as prediction entropy) when updating. This ensures a stable representation of the current distribution. 3) Robust training with timeliness, which employs a teacher-student model. The student model is updated by minimizing a robust loss on the memory bank, weighted by a timeliness-based reweighting strategy, giving higher importance to newer samples. The teacher model is updated via exponential moving average of the student's parameters, and only affine parameters in RBN are trained for efficiency and stability.",
        "experimental_setup": "Experiments were conducted on CIFAR10-C, CIFAR100-C, and DomainNet datasets. For CIFAR-C benchmarks, pre-trained WildResNet-28 (CIFAR10) and ResNeXt-29 (CIFAR100) models from RobustBench were used. For DomainNet, ResNet-101 models were pre-trained on each source domain. The PTTA setup was simulated by continually changing test corruptions (severity 5) on CIFAR-C and adapting to different target domains on DomainNet. Correlative sampling was simulated using a Dirichlet distribution with parameter δ=0.1. Optimization used Adam with a learning rate of 1.0 × 10^-3. All methods used a batch size of 64, and RoTTA's memory bank capacity was N=64. Hyperparameters for RoTTA were unified: α=0.05, ν=0.001, λt=1.0, λu=1.0. Performance was evaluated using average classification error, with extensive ablation studies on individual components, distribution changing order, Dirichlet parameter δ, and batch size.",
        "limitations": "The Robust Batch Normalization (RBN) adopted is considered a naive solution for normalizing correlatively sampled data, requiring careful tuning of its α parameter. The current method lacks a mechanism to explicitly recover the model from a collapsed state, although RoTTA prevents such collapse in experiments. The simulation of correlative sampling relies on category similarity approximated by the Dirichlet distribution, indicating a need for further validation in more diverse real-world scenarios beyond this specific type of correlation.",
        "future_research_directions": "Future work could focus on improving the current RoTTA algorithm by replacing or refining its existing components. A key direction is to further enhance the Practical Test-Time Adaptation (PTTA) setup itself to make it even more reflective of real-world complexities. Developing methods to recover models from a collapsed state would be a valuable addition. Lastly, there's a need to validate the approach in diverse real-world scenarios to confirm its effectiveness beyond simulated correlative sampling.",
        "experimental_code": "import torch\nimport torch.nn as nn\nfrom copy import deepcopy\nimport math\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n# From core/adapter/base_adapter.py\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n\n        params, param_names = self.collect_params(self.model)\n        if len(param_names) == 0:\n            self.optimizer = None\n        else:\n            self.optimizer = optimizer(params)\n\n        self.steps = self.cfg.OPTIM.STEPS\n\n    def collect_params(self, model: nn.Module):\n        names = []\n        params = []\n\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                names.append(n)\n                params.append(p)\n\n        return params, names\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n# From core/utils/memory.py\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\n# From core/utils/bn_layers.py\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n    def forward(self, x):\n        raise NotImplementedError\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n# From core/utils/utils.py\ndef get_named_submodule(model, sub_name: str):\n    names = sub_name.split(\".\")\n    module = model\n    for name in names:\n        module = getattr(module, name)\n\n    return module\n\ndef set_named_submodule(model, sub_name, value):\n    names = sub_name.split(\".\")\n    module = model\n    for i in range(len(names)):\n        if i != len(names) - 1:\n            module = getattr(module, names[i])\n\n        else:\n            setattr(module, names[i], value)\n\n# From core/utils/custom_transforms.py\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\nclass ColorJitterPro(ColorJitter):\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    def forward(self, img):\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0) \n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n# From core/adapter/rotta.py\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        # CSTU memory bank initialization\n        self.mem = CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        # Teacher model initialization\n        self.model_ema = self.build_ema(self.model)\n        # Data augmentation for memory samples\n        self.transform = get_tta_transforms(cfg)\n        # EMA coefficient for teacher update\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            # Uncertainty calculation (prediction entropy)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # Add samples to memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train() # Student model\n        self.model_ema.train() # Teacher model\n        \n        # Get memory data and ages for timeliness reweighting\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            # Apply strong augmentation to memory samples for student training\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data) # Teacher inference\n            stu_sup_out = model(strong_sup_aug) # Student inference with augmentation\n            \n            # Timeliness-based reweighting\n            instance_weight = timeliness_reweighting(ages)\n            # Robust loss minimization (student update)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        # Teacher model update via EMA of student parameters\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n        model.requires_grad_(False) # Freeze all parameters initially\n        normlayer_names = []\n\n        # Identify BatchNorm layers\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        # Replace BatchNorm with RobustBN and enable gradient for affine parameters\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA) # ALPHA is the momentum for RBN\n            momentum_bn.requires_grad_(True) # Only affine parameters of RBN are trained\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n",
        "experimental_info": "The RoTTA method utilizes several configurable parameters for its core components:\n- `cfg.ADAPTER.RoTTA.MEMORY_SIZE`: Sets the capacity of the Category-balanced sampling with timeliness and uncertainty (CSTU) memory bank. (Default: 64)\n- `cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY`: Determines how often the student model is updated using samples from the memory bank (e.g., after processing this many instances). (Default: 64)\n- `cfg.ADAPTER.RoTTA.NU`: The Exponential Moving Average (EMA) decay rate used to update the teacher model's parameters from the student model during robust training. (Default: 0.001)\n- `cfg.ADAPTER.RoTTA.ALPHA`: Represents the momentum coefficient for the running mean and variance statistics update in the Robust Batch Normalization (RBN) layers. (Default: 0.05)\n- `cfg.ADAPTER.RoTTA.LAMBDA_T`: A weighting factor that controls the influence of 'timeliness' in the CSTU memory item scoring heuristic, prioritizing newer samples. (Default: 1.0)\n- `cfg.ADAPTER.RoTTA.LAMBDA_U`: A weighting factor that controls the influence of 'uncertainty' (prediction entropy) in the CSTU memory item scoring heuristic, prioritizing samples with low uncertainty. (Default: 1.0)\n- `cfg.OPTIM.STEPS`: Specifies the number of forward and adaptation steps performed for each batch of incoming data. (Default: 1)\n- `cfg.OPTIM.LR`: The learning rate used by the optimizer for updating the student model's trainable affine parameters within the RBN layers. (Default: 1e-3)\n- `cfg.OPTIM.METHOD`: The optimization algorithm used for the student model (e.g., 'Adam', 'SGD'). (Default: 'Adam')\n- `cfg.TEST.BATCH_SIZE`: The number of samples processed together in each batch during testing and adaptation. (Default: 64)\n- `cfg.CORRUPTION.NUM_CLASS`: The total number of classes in the dataset, which is crucial for initializing the CSTU memory bank and normalizing uncertainty scores. (Default: -1, typically inferred from the dataset, e.g., 10 for CIFAR-10, 100 for CIFAR-100)\n- `cfg.INPUT.SIZE`: The spatial dimensions (height, width) to which input images are resized, relevant for the strong data augmentation transforms applied to memory samples. (Default: (32, 32))"
      }
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that accounts for computational time constraints by penalizing slower methods with fewer samples for adaptation. This protocol reveals that, under realistic conditions (constant-speed data stream), simpler and faster TTA approaches can outperform more sophisticated but slower state-of-the-art methods. The research highlights the critical importance of developing TTA methods that are both accurate and computationally efficient for real-world applicability.",
        "methodology": "The core methodology is the proposed 'Realistic Online Evaluation Protocol' for TTA. Unlike current offline protocols that assume the data stream waits for adaptation, this protocol simulates a constant-speed data stream. It introduces a 'relative adaptation speed' C(g), defined as the integer ratio of the stream's speed to the method's adaptation speed. If a method gslow is k times slower than the stream (C(g)=k), it can only adapt to every kth sample; the remaining samples are processed by the most recent adapted model (fθt+1) or the base model (fθ). This C(g) is computed online for each input, accounting for hardware and input-dependent noise. The protocol explicitly models the trade-off between adaptation speed and the number of samples a method can leverage.",
        "experimental_setup": "The evaluation used a ResNet-50-BN3 (ImageNet-pretrained) as the base classifier, with stream batches of size 64 (except MEMO, which uses single images). Benchmarking was performed on 15 state-of-the-art TTA methods (e.g., AdaBN, TENT, SHOT, SAR, EATA, CoTTA, DDA, MEMO) across multiple datasets: ImageNet-C (corruption level 5 across 15 corruptions), CIFAR10-C, ImageNet-R, and ImageNet-3DCC. Architectures included ResNet-50, ResNet-18, and ViT. Evaluation scenarios covered: (i) episodic (single domain shift, model reset), (ii) continual (sequential domain shifts, no model reset), (iii) varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), (iv) practical TTA with label imbalances (PTTA using RoTTA on CIFAR10-C), and (v) a strict 'single model' scenario (skipped samples processed by a random classifier). Hyperparameter tuning for TENT (learning rates) was also evaluated.",
        "limitations": "The current evaluation protocol implicitly assumes that the data stream is not constant-speed and waits for the TTA method to adapt, favoring slower methods. The proposed realistic protocol assumes the capacity to concurrently deploy two models (the TTA method and the base model) when the TTA method is busy, which might be an unfair assumption for very efficient methods (C(g)=1) or in resource-constrained scenarios. Data-dependent approaches like MEMO and DDA were found to be extremely inefficient under this protocol due to massive computational burdens, leading to most predictions being made by the non-adapted classifier. The assumption in the 'single model evaluation scheme' (Appendix C) that a random classifier generates predictions for skipped samples is a strict and naive choice, potentially not reflecting all real-world scenarios where g is busy.",
        "future_research_directions": "The paper calls for future research to focus on developing TTA methods that are both accurate and efficient, specifically increasing the efficiency of data-dependent adaptation methods. It hopes the proposed evaluation scheme inspires future TTA methods to consider inference speed as a critical dimension for real-world performance. The consistent performance changes across corruptions suggest further testing of the hypothesis that performance is independent of the domain shift type."
      }
    },
    {
      "title": "Leveraging Proxy of Training Data for Test-Time Adaptation"
    },
    {
      "title": "TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?"
    },
    {
      "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
      "abstract": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning\nmodel to environments that change continuously. Yet, it is unclear whether TTA\nmethods can maintain their adaptability over prolonged periods. To answer this\nquestion, we introduce a diagnostic setting - recurring TTA where environments\nnot only change but also recur over time, creating an extensive data stream.\nThis setting allows us to examine the error accumulation of TTA models, in the\nmost basic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-perturbed Gaussian Mixture Model Classifier, deriving\ntheoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose persistent TTA (PeTTA), which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks. Our project page is available at https://hthieu166.github.io/petta.",
      "full_text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Trung-Hieu Hoang1 Duc Minh Vo2 Minh N. Do1,3 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp Abstract Current test-time adaptation (TTA) approaches aim to adapt a machine learn- ing model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where envi- ronments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet repre- sentative ϵ-perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. 1 Introduction Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models’ adaptability when confronted with a new testing environment that has been investigated [ 30, 53, 14]. Among common domain generalization methods [ 58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the ever- changing and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accom- modate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2311.18193v4  [cs.CV]  2 Nov 2024Testing Error Time Day 1 Illumination Condition Day 2 Day 3 0 50 100 150 200 250 300 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Test-time adaptation step Testing Error No TTA RoTTA PeTTA (ours) Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: • First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. • Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an ϵ-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. • Third, we introduce persistent TTA (PeTTA)- a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. For more context on related work, readers are directed to visit our discussions in Appdx. A. 2 Background Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier ft : X → Ywith parameter θt ∈ Θ (parameter space) gradually changing over time (t ∈ T) that maps an input image x ∈ Xto a category (label) y ∈ Y. Let the capital letters (Xt, Yt) ∈ X × Ydenote a pair of random variables with the joint distribution Pt(x, y) ∈ Pd, t∈ T. Here, Pd belongs to collection of D sets of testing scenarios (domains) {Pd}D d=1. The covariate shift [46] is assumed: Pt(x) and Pt′(x) 2could be different but Pt(y|x) = Pt′(y|x) holds ∀t ̸= t′. At t = 0, θ0 is initialized by a supervised model trained on P0 ∈ P0 (source dataset). The model then explores an online stream of testing data. For each t >0, it receives Xt (typically in form of a batch of Nt testing samples) for adapting itself ft−1 → ft before making the final prediction ft (Xt). TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model ft are updated indirectly through a student model with parameters θ′ t [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] ˆYt for each Xt: ˆYt = ft−1(Xt). (1) With a classification loss LCLS (e.g., cross-entropy [16]), and a model parameters regularizer R, the student model is first updated with a generic optimization operatorOptim, followed by an exponential moving average (EMA) update of the teacher model parameter θt−1: θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011i + λR(θ′), (2) θt = (1 − α)θt−1 + αθ′ t, (3) with α ∈ (0, 1) - the update rate of EMA, andλ ∈ R+ - the weighting coefficient of the regularization term, are the two hyper-parameters. Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt’s can be partitioned by td’s in which {Pt}td t=td−1 ⊂ Pd. Here, each partition of consecutive steps follows the same underlying distribution which will change continually through D domains [59] (P1 → P2 ··· → PD). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution Pt(y) = 0, ∀y ̸∈ Yt ⊂ Yeven though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (|Yt| ≪ |Y|) situation can slowly degenerate the model [7]. 3 Recurring TTA and Theoretical Analysis This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). 3.1 Recurring TTA and Model Collapse Recurring TTA.To study the gradual performance degradation (or model collapse), we propose anew testing scenario based on practical TTA [61]. Conducting a single pass through D distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions K times to compare the incremental error versus the previous visits. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. Appdx. D extends our justifications on constructing recurring TTA. Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Here, upon collapsing, a model tends to ignore almost categories in ˜Y. As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to θ0. 3.2 Simulation of Failure and Theoretical Analysis Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). 3Pseudo-label Predictor ˆYt = argmax y∈Y Pr(Xt|y;θt−1) Xt Mean-teacher Update θ′ t = Optim θ′∈Θ EPt h LCLS \u0010ˆYt, Xt;θ′\u0011i θt = (1−α)θt−1 +αθ′ t ϵt ··· θt−1 θt ··· Figure 2: ϵ-perturbed binary Gaussian Mix- ture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher up- date (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of ϵt to simulate an undesirable TTA testing stream. Simulated Testing Stream. Observing a testing stream with (Xt, Yt) ∈ X × Y= R × {0, 1} and the underlying joint distribution Pt(x, y) = py,t · N(x; µy, σ2 y). The main task is predicting Xt was sampled from cluster 0 or 1 (negative or positive). Conveniently, let py,t ∆ = Pt(y) = Pr(Yt = y) and ˆpy,t ∆ = Pr( ˆYt = y) be the marginal distribution of the true label Yt and pseudo label ˆYt. GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [ 61, 15]). Thus, it simplifies ft into a maximum likelihood estimation ft(x) = argmaxy∈Y Pr(x|y; θt) with Pr(x|y; θt) = N(x; ˆµy,t, ˆσ2 y,t). The goal is estimating a set of parameters θt = {ˆµy,t, ˆσ2 y,t}y∈Y. A perfect classifier θ0 = {µy, σ2 y}y∈Y is initialized at t = 0. For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding θ′ t, Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating EPt. The mean teacher update (Eq. 3) for GMMC is: ˆµy,t = ( (1 − α)ˆµy,t−1 + αEPt h Xt|ˆYt i if ˆYt = y ˆµy,t−1 otherwise . (4) The update of ˆσ2 y,t is similar. ˆYt = ft−1(Xt) can be interpreted as a pseudo label (Eq. 1). ϵ-GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor . Instead of directly modeling the dynamic changes of py,t (which can be complicated depending on the dataset), we study an ϵ−pertubed GMMC (ϵ−GMMC), where py,t is assumed to be static (defined below) and the pseudo- label predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ϵt = Pr{Yt = 1|ˆYt = 0} (5) be the false negative rate (FNR) of the model at step t. Without loss of generality, we study the increasing type II collapse of ϵ-GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of ϵt is maintained (Fig. 2). Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Lemma 1 states the negative correlation between ˆp1,t and ϵt. Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of p1. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Lemma 2 states the resulting ϵ−GMMC after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when ˆµ0,t moves toward µ1. We next investigate the factors and conditions for this undesirable convergence. 4Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . From Thm. 1, we observe that the distance d0→1 t ’s converges (also indicating the convergence to the distribution in Lemma 2) if d0→1 t < d0→1 t−1 . The model collapse happens when this condition holds for a sufficiently long period. Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Corollary 1 introduces a condition ϵ-GMMC collapse. Here, ϵt’s are non-decreasing, lim t→τ ϵt = p1. Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution (p0), the nature difference between two categories (|µ0 − µ1|); and (ii) algorithm-dependent factors: the update rate (α), the FNR at each step (ϵt). ϵ-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). 3.3 Connection to Existing Solutions Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies. Regularization Term for θt. Knowing that f0 is always well-behaved, an attempt is restricting the divergence of θt from θ0, e.g. using R(θt) ∆ = ∥θ0 − θt∥2 2 regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. Memory Bank for Harmonizing Pt(x). Upon receiving Xt, samples in this batch are selectively updated to a memory bank M (which already contains a subset of some instances ofXt′, t′ < tin the previous steps). By keeping a balanced number of samples from each category, distribution PM t (y) of samples in M is expected to have less zero entries than Pt(y), making the optimization step over PM t more desirable. From Thm. 1, M moderates the extreme value of the category distribution (p0 term) which typically appears on batches with low intra-batch category diversity. 4 Persistent Test-time Adaptation (PeTTA) Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while ϵt (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to d0→1 t−1 term) can provide hints to fine-tune the adaptation process. Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing ϵt’s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of λ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, α (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting λ and α on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing λ) and preserving the model from previous steps (decreasing α) when there is a significant deviation from θ0. In [40, 61, 59], λ and α were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when ϵt increases, making constant λ, αinsufficient to prevent collapse. Sensing the Divergence of θt. We first equip PeTTA with a mechanism for measuring its divergence from θ0. Since ft(x) = argmax y∈Y Pr(y|x; θt), we can decompose Pr(y|x; θt) = [h (ϕθt(x))]y, with ϕθt(·) is a θt-parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) h(·). The operator [·]y extracts the yth component of a vector. 5Since h(·) remains unchanged, instead of comparing the divergence in the parameter space (Θ) or between the output probability Pr(y|x; θt) and Pr(y|x; θ0), we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let z = ϕθt(x), we keep track of a collection of the running mean of feature vector z: {ˆµy t }y∈Y in which ˆµy t is EMA updated with vector z if ft(x) = y. The divergence of θt at step t, evaluated on class y is defined as: γy t = 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 , (6) where µy 0 and Σy 0 are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (P0). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). Here, we implicitly expect the independence of each entry in z and TTA approaches learn to align feature vectors of new domains back to the source domain (P0). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of γy t ∈ [0, 1] is close to 0 when θt = θ0 and increases exponentially as ˆµy t diverging from µy 0. Adaptive Regularization and Model Update. With α0, λ0 are initial values, utilizing γy t derived in Eq. 6, a pair of (λt, αt) is adaptively chosen at each step: ¯γt = 1 | ˆYt| X y∈ ˆYt γy t , ˆYt = n ˆY (i) t |i = 1, ··· , Nt o ; λt = ¯γt · λ0, α t = (1 − ¯γt) · α0, (7) ˆYt is a set of unique pseudo labels in a testing batch ( ˆY (i) t is the ith realization of ˆYt). Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space (Θ) is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss LAL can nail down the similarity between ft and f0 in the probability space [32, 12]: LAL(Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ), (8) which is equivalent to minimizing the KL divergence DKL (Pr(y|Xt; θ0)∥Pr(y|Xt; θ)). Persistent TTA.Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. ForLCLS, either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With R(θ), cosine similarity or L2 distance are both valid metrics for measuring the distance between θ and θ0 in the parameter space. Fisher regularizer coefficient [ 40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with λt, αt (Eq. 7) and LAL (Eq. 8): θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′), θt = (1 − αt)θt−1 + αtθ′ t. 5 Experimental Results 5.1 ϵ−MMC Simulation Result Simulation Setup. A total of 6000 samples from two Gaussian distributions: N(µ0 = 0, σ2 0 = 1) and N(µ1 = 2, σ2 1 = 1) with p0 = p1 = 1 2 are synthesized and gradually released in a batch of B = 10 samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). ϵ−GMMC update follows Eq. 4 with α = 5e−2. To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and theϵt (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, 60 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) 0 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) −4 −2 0 2 4x−4 −2 0 2 40 0.2 0.4 0.6 0.8 1 x Probability density N(µ0, σ0) N(µ1, σ1) N(ˆµ0, ˆσ0) N(ˆµ1, ˆσ1) 0 100 200 300 400 500 600 0.8 1.2 1.6 2.0 Testing step (t) |ˆµ0,t −µ1| Numerical Simulation Theoretical Result 0 100 200 300 400 500 600 0.1 0.2 0.3 0.4 0.5 Testing step (t) ϵt Prediction Frequency GMMCϵ-GMMC ϵ-GMMC GMMC (a) (b) (c) Figure 3: Simulation result on ϵ-perturbed Gaussian Mixture Model Classifier ( ϵ-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of ϵ-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward µ1 (|EPt [ˆµ0,t] − µ1|) and false- negative rate (ϵt) in simulation coincides with the result in Thm. 1 (with ϵt following Corollary 1). ϵ-GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from ˆµ0,t (initialized at µ0) towards µ1 converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when ϵt is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. 5.2 Setup - Benchmark Datasets Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 → CIFAR10-C, CIFAR100→ CIFAR100-C, and ImageNet → ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real → clipart, painting, sketch. Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [ 59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME [ 7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every 1, 000 images): RDumb [45] are also included. Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and Dir(0.01) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits K = 20 (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to 5e−2; α0 = 1e−3 and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [ 12] for LCLS and λ0 = 10 while the regular cross-entropy loss [ 13] and λ0 = 1 (severe domain shift requires prioritizing 7Table 1: Average classification error of the task CIFAR-10→ CIFAR-10-C in recurring TTA. The lowest error is in bold,(∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 43.5 43.5 LAME [7] 31.1 31.1 CoTTA [59]82.2 85.6 87.2 87.8 88.2 88.5 88.7 88.7 88.9 88.9 88.9 89.2 89.2 89.2 89.1 89.2 89.2 89.1 89.3 89.388.3EATA [40]81.6 87.0 88.7 88.7 88.9 88.7 88.6 89.0 89.3 89.6 89.5 89.6 89.7 89.7 89.3 89.6 89.6 89.8 89.9 89.488.8RMT [12]77.5 76.9 76.5 75.8 75.5 75.5 75.4 75.4 75.5 75.3 75.5 75.6 75.5 75.5 75.7 75.6 75.7 75.6 75.7 75.875.8MECTA [22]72.2 82.0 85.2 86.3 87.0 87.3 87.3 87.5 88.1 88.8 88.9 88.9 88.6 89.1 88.7 88.8 88.5 88.6 88.3 88.886.9RoTTA [61]24.6 25.5 29.6 33.6 38.2 42.8 46.2 50.6 52.2 54.1 56.5 57.5 59.4 60.2 61.7 63.0 64.8 66.1 68.2 70.351.3RDumb [45]31.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9ROID [37]72.7 72.6 73.1 72.4 72.7 72.8 72.7 72.7 72.9 72.8 72.9 72.9 72.8 72.5 73.0 72.8 72.5 72.5 72.7 72.772.7TRIBE [52]15.3 16.6 16.6 16.3 16.7 17.0 17.3 17.4 17.4 18.0 17.9 18.0 17.9 18.6 18.2 18.8 18.0 18.2 18.4 18.017.5PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter λ0 in PeTTA. 5.3 Result - Benchmark Datasets Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR-10→CIFAR-10-C, and ImageNet → ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). 0 10 20 30 40 16 18 20 22 24 Recurring TTA Visit Classification Error PeTTA (ours) TRIBE [52] Figure 4: Classification error of TRIBE [ 52] and PeTTA (ours) of the task CIFAR-10→CIFAR10-C task in recurring TTA with 40 visits. In average, PeTTA outperforms almost every baseline approaches and persists across 20 vis- its over the three datasets. The only exception is at the case of TRIBE [ 52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recur- ring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains sta- ble afterward. Fig. 5b-bottom exhibits the con- fusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffling the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. 8Table 2: Average classification error of the task ImageNet → ImageNet-C in recurring TTA scenario. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 82.0 82.0 LAME [7] 80.9 80.9 CoTTA [59]98.6 99.1 99.4 99.4 99.5 99.5 99.5 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.7 99.799.5EATA [40]60.4 59.3 65.4 72.6 79.1 84.2 88.7 92.7 95.2 96.9 97.7 98.1 98.4 98.6 98.7 98.8 98.8 98.9 98.9 99.089.0RMT [12]72.3 71.0 69.9 69.1 68.8 68.5 68.4 68.3 70.0 70.2 70.1 70.2 72.8 76.8 75.6 75.1 75.1 75.2 74.8 74.771.8MECTA [22]77.2 82.8 86.1 87.9 88.9 89.4 89.8 89.9 90.0 90.4 90.6 90.7 90.7 90.8 90.8 90.9 90.8 90.8 90.7 90.889.0RoTTA [61]68.3 62.1 61.8 64.5 68.4 75.4 82.7 95.1 95.8 96.6 97.1 97.9 98.3 98.7 99.0 99.1 99.3 99.4 99.5 99.687.9RDumb [45]72.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8ROID [37]62.7 62.3 62.3 62.3 62.5 62.3 62.4 62.4 62.3 62.6 62.5 62.3 62.5 62.4 62.5 62.4 62.4 62.5 62.4 62.562.4TRIBE [52]63.664.0 64.9 67.8 69.6 71.7 73.5 75.5 77.4 79.8 85.0 96.5 99.4 99.8 99.9 99.8 99.8 99.9 99.9 99.984.4PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. CCC [45] Adaptation Step− − − − − − − − − − − − − − − − − − − − − − − − − → Method6700 13400 20100 26800 33500 40200 46900 53600 60200 66800 73400 80000Avg Source 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.83 RoTTA [61]0.70 0.85 0.92 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.95 RDumb [45]0.78 0.74 0.75 0.77 0.75 0.72 0.75 0.77 0.75 0.74 0.75 0.75 0.75 PeTTA(ours) 0.67 0.63 0.62 0.65 0.65 0.64 0.64 0.68 0.63 0.63 0.65 0.65 0.64 0.46 0.44 0.4 0.43 0.46 0.47 0.44 0.43 0.48 0.4 0.43 0.43 0.41 airplane bird cat dog frog ship auto deer horse truck 0.13 0.34 0.44 0.32 0.14 0.44 0.51 0.46 0.46 0.34 airplane bird catdog frog ship auto deer horse truck Inter-category cosine similarity (source model)Misclassification rate of collapsed RoTTA 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits RoTTA [61] PeTTA (ours) PeTTA (ours) - 20th visit RoTTA [61] - 20th visit Predicted label (a) (b)(c) True labelTrue label Prediction Frequency Figure 5: Recurring TTA (20 visits) on CIFAR-10 →CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [ 61] in an recurring TTA setting (Fig. 5a-left). Similar to ϵ-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the 9Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) R(θ), LAL; LAL only; fixed regularization coefficient λ; adaptive coef- ficient λt, update rate αt; using anchor loss LAL. Method CF-10-CCF-100-CDN IN-C Baseline w/oR(θ),LAL 42.6 63.0 77.9 93.4 R(θ)fixedλ= 0.1λ0 43.3 65.0 80.0 92.5R(θ)fixedλ=λ0 42.0 64.6 66.6 92.9 LALonly 25.4 56.5 47.5 68.1 PeTTA -λt 27.1 55.0 59.7 92.7PeTTA -λt +αt 23.9 41.4 44.5 75.7PeTTA -λt +LAL 26.2 36.3 43.2 62.0 PeTTA -λt +αt +LAL 22.8 35.1 42.9 60.5 Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of reg- ularizers R(θ): L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. Method CF-10-CCF-100-CDN IN-CR(θ) Fisher L2 ✗ 23.0 35.6 43.1 70.8✓ 22.7 36.0 43.9 70.0 Cosine ✗ 22.8 35.1 42.9 60.5✓ 22.6 35.9 43.3 63.8 CF: CIFAR, DN: DomainNet, IN: ImageNet top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to |µ0 − µ1| (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. 5.4 Ablation Study Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (R(θ)) with a fixed choice of λ, αnot only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (LAL) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptiveλt scheme alone (row 5) or in conjunction with either αt or anchor loss LAL (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting αt adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without LAL is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regu- larization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. 6 Discussions and Conclusion On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as astraightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study theperformance degradation problem of TTA. The proposed recurring TTAsetting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of ϵ−GMMC paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. 10Acknowledgements This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. References [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution gener- alization. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample se- lection for online continual learning. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf. [4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127–131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/. [5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957. [6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Con- siderations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334–8343, 2022. doi: 10.1109/CVPR52688.2022.00816. [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022. [9] Thomas M. Cover and Joy A. Thomas.Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022. doi: 10.1109/ TPAMI.2021.3057446. [12] Mario Döbler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704–7714, June 2022. [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html. [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189– 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10 . [15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 11[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026–1034, 2015. [19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. [21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320–8329, 2021. doi: 10.1109/ICCV48922.2021.00823. [22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh. [23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203–211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z . [24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 2427–2440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf. [25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and compu- tation. Mathematics Operations Research, 12:262–268, 1987. URL https://api.semanticscholar. org/CorpusID:656323. [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.Pro- ceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114. [28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013. [29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643–28652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706. [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg. [32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. doi: 10.1109/TPAMI.2017.2773081. [33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020. 12[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. [35] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. https: //github.com/pytorch/vision, 2016. [36] Robert A Marsden, Mario Döbler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. [37] Robert A Marsden, Mario Döbler, and Bin Yang. Universal test-time adaptation through weight ensem- bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555–2565, 2024. [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge. [40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. [41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. [42] K. R. Parthasarathy. Introduction to Probability and Measure , volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9 . [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406–1415, 2019. [45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc. [46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html. [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. [49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543–546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388 . [50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Di- mensional Data. Annals of Data Science , 6(4):737–763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4 . 13[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126– 15135, 2024. [53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229–9248. PMLR, 13–18 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html. [54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018. [55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 1195–1204, 2017. ISBN 9781510860964. [56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z . [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. [58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627–4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track. [59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, June 2022. [60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models – a case study in healthcare information systems. International Journal of Information Management Data Insights , 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143. [61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922– 15932, 2023. [62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. 14Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices Table of Contents A Related Work 16 B Proof of Lemmas and Theorems 16 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Proof of Lemma 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Proof of Theorem 1 and Corollary 1. . . . . . . . . . . . . . . . . . . . . . . . 18 C Further Justifications on Gaussian Mixture Model Classifier 19 D Further Justifications on the Recurring Testing Scenario 20 D.1 Recurring TTA Follows the Design of a Practical TTA Stream . . . . . . . . . . 20 D.2 Recurring TTA as a Diagnostic Tool . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Recurring TTA with Random Orders . . . . . . . . . . . . . . . . . . . . . . . 20 E Further Justifications on Persistent TTA (PeTTA) 21 E.1 Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Anchor Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3 The Use of the Memory Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset . 23 E.5 Novelty of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Additional Experimental Results of PeTTA 24 F.1 Performance of PeTTA Versus Compared Methods . . . . . . . . . . . . . . . . 24 F.2 An Inspection of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Does Model Reset Help? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.4 PeTTA with 40 Recurring Visits . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.5 The Sensitivity of Hyper-parameter Choices in PeTTA . . . . . . . . . . . . . . 27 F.6 More Details on the Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . 27 F.7 More Confusion Matrices in Recurring TTA Setting . . . . . . . . . . . . . . . 29 G Experimental Details 29 G.1 Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.2 Experiments on CCC Testing Stream . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 Test-time Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4 The Use of Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 15A Related Work Towards Robust and Practical TTA. While forming the basis, early single-target TTA ap- proaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing envi- ronments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuanet al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. Temporal Performance Degradation.By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI “aging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [ 12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [ 41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [ 7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. B Proof of Lemmas and Theorems In this section, we prove the theoretical results regarding the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Preliminary. Following the same set of notations introduced in the main text, recall that we denoted py,t ∆ = Pr{Yt = y}, ˆpy,t ∆ = Pr{ˆYt = y} (marginal distribution of the true label Yt and pseudo label ˆYt receiving label y, respectively) and ϵt = Pr{Yt = 1|ˆYt = 0} (the false negative rate (FNR) of 16ϵ−GMMC). At testing step t, we obtain the following relations: EPt h Xt|ˆYt = 0 i = (1 − ϵt)µ0 + ϵtµ1, (9) EPt h Xt|ˆYt = 1 i = µ1, (10) VarPt \u0010 Xt|ˆYt = 0 \u0011 = (1 − ϵt)σ2 0 + ϵtσ2 1 + ϵt(1 − ϵt)(µ0 − µ1)2, (11) VarPt \u0010 Xt|ˆYt = 1 \u0011 = σ2 1. (12) In addition, under Assumption 1, the marginal distribution Pt(x) (also referred as data distribution in our setup) is: Pt(x) = N(x; p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2) ∀t ∈ T. (13) B.1 Proof of Lemma 1 Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Proof. Under Assumption 1, we have EPt [Xt] = p0µ0 + (1 − p0)µ1. Also note that: EPt [Xt] = EPt h EPt h Xt|ˆYt ii = EPt h Xt|ˆYt = 0 i ˆp0,t + EPt h Xt|ˆYt = 1 i ˆp1,t (14) = [(1 − ϵt)µ0 + ϵtµ1] ˆp0,t + µ1(1 − ˆp0,t) = [(1 − ϵt)ˆp0,t] µ0 + [1 − ˆp0,t(1 − ϵt)] µ1 = p0µ0 + (1 − p0)µ1, where the second equality follows Eqs. 9-10. Therefore: ˆp0,t = p0 1 − ϵt . (15) Eq. 15 shows positive correlation between ˆp0,t and ϵt. Given lim t→τ ϵt = p1, taking the limit introduces: lim t→τ ˆp0,t = lim t→τ p0 1 − ϵt = p0 1 − p1 = 1. Similarly, having lim t→τ ˆp0,t = 1, the false negative rate ϵt when t → τ is: lim t→τ ϵt = 1 − p0 = p1. Since ˆp0,t + ˆp1,t = 1, lim t→τ ˆp1,t = 0, equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model ϵt also raises correspondingly. B.2 Proof of Lemma 2. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Proof. From Eqs. 9-10, under the increasing type II collapse of ϵ−GMMC setting, the perturbation does not affect the approximation of µ1. Meanwhile, when ϵt increases, one can expect that ˆµ0,t 17moves further away from µ0 toward µ1. Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: EPt h ˆµ0,t|ˆYt = 1 i = EPt−1 [ˆµ0,t−1] , EPt h ˆµ0,t|ˆYt = 0 i = (1 − α)EPt−1 h ˆµ0,t−1|ˆYt = 0 i + αEPt h Xt|ˆYt = 0 i = (1 − α)EPt−1 [ˆµ0,t−1] + α \u0010 EPt h Xi|ˆYt = 0 i\u0011 , EPt h ˆµ1,t|ˆYt = 1 i = (1 − α)EPt−1 h ˆµ1,t−1|ˆYt = 1 i + αEPt h Xt|ˆYt = 1 i = (1 − α)EPt−1 [ˆµ1,t−1] + α \u0010 EPt h Xi|ˆYt = 1 i\u0011 , EPt h ˆµ1,t|ˆYt = 0 i = EPt−1 [ˆµ1,t−1] . By defining uy,t = EPt [ˆµy,t], we obtain the following recurrence relation between u0,t and u0,t−1: u0,t = EPt h ˆµ0,t|ˆYt = 0 i ˆp0,t + EPt h ˆµ0,t|ˆYt = 1 i ˆp1,t = \u0010 (1 − α)u0,t−1 + αEPt h Xt|ˆYt = 0 i\u0011 ˆp0,t + u0,t−1 ˆp1,t = [(1 − α)ˆp0,t + ˆp1,t] u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,t [(1 − ϵt)µ0 + ϵtµ1] . (16) Given lim t→τ ˆp0,t = 1, it follows that lim t→τ ϵ0,t = p1 by Lemma 1. From this point: u0,t = (1 − α)u0,t−1 + α (p0µ0 + p1µ1) ∀t > τ. Taking the limit t → ∞: lim t→∞ u0,t = lim t→∞ (1 − α)u0,t−1 + α (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + α tX i=1 (1 − α)i−1 (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + (1 − (1 − α)t)(p0µ0 + p1µ1) = p0µ0 + p1µ1. The second equation is obtained by solving the recurrence relation. When lim t→τ ˆp0,t = 1, {ˆµy,t}y∈{0,1} becomes a deterministic values. Hence, giving uy,t = EPt [ˆµy,t] = ˆµ0,t(∀t > τ) and lim t→∞ ˆµ0,t = lim t→∞ u0,t = p0µ0 + p1µ1. (17) Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for σ2 0,t: lim t→∞ ˆσ2 0,t = p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2. (18) By Lévy’s continuity theorem (p. 302, [ 42]), from Eqs. 17-18, when t → ∞, the estimated distribution of the first cluster N(x; ˆµ0,tˆσ2 0,t) converges to the whole data distribution Pt(x) (Eq. 13) when collapsing. B.3 Proof of Theorem 1 and Corollary 1. Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . 18Proof. Substituting Eq. 15 into ˆp0,t of Eq. 16 gives: u0,t = \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0 1 − ϵt [(1 − ϵt)µ0 + ϵtµ1] . Hence, we have the distance from u0,t toward µ1: |u0,t − µ1| = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0µ0 + αp0ϵtµ1 1 − ϵt − µ1 \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 + αp0ϵtµ1 1 − ϵt − αp0µ1 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 − αp0µ1(1 − ϵt) 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0(µ0 − µ1) \f\f\f\f ≤ \u0012 1 − αp0 1 − ϵt \u0013 |u0,t−1 − µ1| + αp0|µ0 − µ1|. The last inequality holds due to the triangle inequality. Equivalently, |u0,t − µ1| − |u0,t−1 − µ1| ≤α · p0 · \u0012 |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt \u0013 . Let d0→1 t = |EPt [ˆµ0,t] − µ1|, we conclude that: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Proof. Initialized at µ0, ϵ-GMMC is collapsing when ˆµ0,t converges to the mid-point p0µ0 + p1µ1 (Lemma 2), i.e., moving closer to µ1. From Thm. 1, the distance towards µ1 d0→1 t < d0→1 t−1 if |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt < 0 ⇔ |µ0 − µ1| < |u0,t−1 − µ1| 1 − ϵt ⇔ ϵt > 1 − |u0,t−1 − µ1| |µ0 − µ1| . When there exists this sequence{ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) it follows that d0→1 t < d0→1 t−1 and ϵt > ϵt−1 is guaranteed ∀t ∈ [τ − ∆τ , τ]. Hence, lim t→τ ϵt = p1 (model collapsed, by Lemma 1). C Further Justifications on Gaussian Mixture Model Classifier One may notice that in ϵ-GMMC (Sec. 4.2), the classifier is defined ft(x) = argmaxy∈Y Pr(x|y; θt) (maximum likelihood estimation) while in general, ft(x) = argmaxy∈Y Pr(y|x; θt) (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., Pr(y; θt) = Pr(y′; θt), ∀y, y′ ∈ C) is enforced in ϵ-GMMC, the two definitions are equivalent. Proof. Having: argmaxy∈Y Pr(y|x; θt) = argmaxy∈Y Pr(x|y; θt) Pr(y; θt)P y′∈Y Pr(x|y′; θt) Pr(y′; θt) = argmaxy∈Y Pr(x|y; θt). We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. 19D Further Justifications on the Recurring Testing Scenario D.1 Recurring TTA Follows the Design of a Practical TTA Stream Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). D.2 Recurring TTA as a Diagnostic Tool Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [ 15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA’s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment(the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. D.3 Recurring TTA with Random Orders Recall that in Sec. 3.1,recurring TTAis constructed by repeatingthe same sequence of D distributions K times. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C:motion → snow → fog → shot → defocus → contrast → zoom → brightness → frost → elastic → glass → gaussian → pixelated → jpeg → impulse). Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across K recurrences are now entirely different. We redo the experiment of the second setting three times (with different random seeds = 0, 1, 2). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. 201 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (a) CIFAR-10 → CIFAR-10-C task. 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (b) CIFAR-100 → CIFAR-100-C task. 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (c) ImageNet → ImageNet-C task. Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model-×- as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. E Further Justifications on Persistent TTA (PeTTA) E.1 Pseudo Code We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a 21Algorithm 1 Persistent TTA (PeTTA) Input: Classification model ft and its deep feature extractor ϕθt, both parameterized by θt ∈ Θ. Testing stream {Xt}T t=0, initial model parameter (θ0), initial update rate (α0), regularization term coefficient (λ0), empirical mean ({µy 0}y∈Y) and covariant matrix ({Σy 0}y∈Y) of feature vectors in the training set, ˆµy t EMA update rate (ν). 1 ˆµy 0 ← µy 0, ∀y ∈ Y; // Initialization 2 for t ∈ [1, ··· , T] do 3 ˆYt ← ft−1(Xt) ; // Obtaining pseudo-labels for all samples in Xt 4 // Persistent TTA (PeTTA) 5 ˆYt ← n ˆY (i) t |i = 1, ··· , Nt o ; // Set of (unique) pseudo-labels in Xt 6 ¯γt ← 0 ; 7 for y ∈ ˆYt do 8 γy t ← 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 ; // Divergence sensing term on category y 9 ¯γt ← ¯γt + γy t | ˆYt| ; // Average divergence sensing term for step t 10 ˆµy t ← (1 − ν)ˆµy t−1 + νϕθt−1 (Xt|ˆYt = y) ; // EMA update of ˆµy t for samples with ˆYt = y 11 end 12 λt ← ¯γt · λ0 ; // Computing adaptive regularization term coefficient 13 αt ← (1 − ¯γt) · α0 ; // Computing adaptive update rate 14 // Regular Mean-teacher Update 15 θ′ t ← Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′) ; // Student model update 16 θt ← (1 − αt)θt−1 + αtθ′ t. ; // Teacher model update 17 // Final prediction 18 yeild ft(Xt) ; // Returning the final inference with updated model ft 19 end shorthand notation, ϕθt−1 (Xt|ˆYt = y) denotes the empirical mean of all feature vectors of X(i) t (extracted by ϕθt−1 \u0010 X(i) t \u0011 ) if ˆY (i) t = y, i= 1, ··· , Nt in the current testing batch. E.2 Anchor Loss KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss LAL is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by θ0 and θ. Proof. Having: DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) = X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ0) Pr(y|Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ) | {z } LAL(Xt;θ) −H(Pr(y|Xt; θ0))| {z } constant . Hence, argmin θ∈Θ LAL(Xt; θ) = argmin θ∈Θ DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) . 22Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. LAL fits this purpose, constraining the KL divergence between two models at each step. Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss ( LAL) and the Regularizer term ( R(θ)) is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. E.3 The Use of the Memory Bank The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods.Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [ 61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [ 22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset Two Ways of Computing µy 0 and Σy 0 in Practice. One may notice that in PeTTA, computing γy t requires the pre-computed empirical mean (µy 0) and covariance (Σy 0) of the source dataset . This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: 1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA. 2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. 23We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption . Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample 25%, 50%, 75%, and 100% of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean (µy 0) and covariant matrix (Σy 0). TTA Task 25% 50% 75% 100% CIFAR-10→CIFAR-10-C 22.96 22.99 23.03 22.75 CIFAR-100→CIFAR-100-C 35.01 35.11 35.09 35.15 DomainNet:real→clip→paint→sketch 43.18 43.12 43.15 42.89 ImageNet→ImageNet-C 61.37 59.68 61.05 60.46 The default choice of PeTTA is using 100% samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. E.5 Novelty of PeTTA PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [ 32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. F Additional Experimental Results of PeTTA F.1 Performance of PeTTA Versus Compared Methods Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR-100→CIFAR-100-C, and real → clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with (*) in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs 24Table 7: Average classification error of the task CIFAR-100 → CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 46.5 46.5 LAME [7] 40.5 40.5 CoTTA [59]53.4 58.4 63.4 67.6 71.4 74.9 78.2 81.1 84.0 86.7 88.8 90.7 92.3 93.5 94.7 95.6 96.3 97.0 97.3 97.683.1EATA [40]88.5 95.0 96.8 97.3 97.4 97.2 97.2 97.3 97.4 97.5 97.5 97.5 97.6 97.7 97.7 97.7 97.8 97.8 97.7 97.796.9RMT [12]50.5 48.6 47.9 47.4 47.3 47.1 46.9 46.9 46.6 46.8 46.7 46.5 46.5 46.6 46.5 46.5 46.5 46.5 46.5 46.547.1MECTA [22]44.8 44.3 44.6 43.1 44.8 44.2 44.4 43.8 43.8 43.9 44.6 43.8 44.4 44.6 43.9 44.2 43.8 44.4 44.9 44.244.2RoTTA [61]35.5 35.2 38.5 41.9 45.3 49.2 52.0 55.2 58.1 61.5 64.6 67.5 70.7 73.2 75.4 77.1 79.2 81.5 82.8 84.561.4RDumb [45]36.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6ROID [37]76.4 76.4 76.2 76.2 76.3 76.1 75.9 76.1 76.3 76.3 76.6 76.3 76.8 76.7 76.6 76.3 76.2 76.0 75.9 76.076.3TRIBE [52]33.8 33.335.334.935.335.137.1 37.2 37.2 39.1 39.2 41.1 41.0 43.1 45.1 45.1 45.0 44.9 44.9 44.939.6PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 8: Average classification error of the task real → clipart → painting → sketch on DomainNet dataset in recurring TTA scenario. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 45.3 45.3 LAME [7] 45.6 45.6 CoTTA [59]96.2 97.1 97.4 97.8 98.1 98.2 98.4 98.4 98.4 98.5 98.6 98.6 98.6 98.6 98.6 98.7 98.7 98.7 98.7 98.798.3RMT [12]76.2 77.1 77.3 77.3 77.2 77.1 76.8 76.9 76.5 76.4 76.4 76.3 76.4 76.2 76.2 76.1 76.4 76.1 76.0 75.876.5MECTA [22]94.6 98.4 98.6 98.8 99.1 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.098.7RoTTA [61]44.3 43.8 44.7 46.7 48.7 50.8 52.7 55.0 57.1 59.7 62.7 65.1 68.0 70.3 72.7 75.2 77.2 79.6 82.6 85.362.1RDumb [45]44.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 stays within ±0.1% for small datasets (CIFAR-10-C, CIFAR-100-C) and±0.5% for larger datasets (ImageNet-C, DomainNet). Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Dataset1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg CF-10-C24.3 23.0 22.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8±0.4±0.3±0.4±0.3±0.3±0.3±0.4±0.2±0.3±0.4±0.4±0.2±0.1±0.3±0.5±0.2±0.2±0.3±0.4±0.5 ±0.1 CF-100-C35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1±0.4±0.4±0.2±0.2±0.1±0.1±0.2±0.2±0.1±0.2±0.1±0.2±0.2±0.1±0.1±0.1±0.1±0.1±0.2±0.2 ±0.1 DN43.8 42.6 42.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9±0.1±0.1±0.2±0.2±0.3±0.3±0.3±0.4±0.4±0.4±0.4±0.4±0.4±0.3±0.3±0.2±0.4±0.3±0.3±0.3 ±0.3 IN-C65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5±0.6±0.5±0.5±0.5±1.4±1.1±1.0±0.5±0.8±0.9±0.4±0.8±0.9±0.8±0.9±0.8±1.0±0.6±0.6±0.7 ±0.5 F.2 An Inspection of PeTTA In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR-10→ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters ( ¯γt, λt, and αt), adaptation losses (LCLS, LAL) and regularization term (R(θ)) are provided. Here, we observe the values of adaptive parameters λt and αt continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanismstabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). F.3 Does Model Reset Help? Experiment Setup. We use the term “model reset” to represent the action of “reverting the current TTA model to the source model” . This straightforward approach is named RDumb [ 45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [ 61] stream. The model (including model 25parameters, the optimizer state, and the memory bank) is reset after adapting itself to T images.1 For each dataset, three values of this hyper-parameter T are selected: • T = 1, 000: This is the value selected by the RDumb’s authors [ 45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables. • T = 10, 000 (CIFAR-10/100-C), T = 5, 000 (ImageNet-C) and T = 24, 237 (Domain- Net).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each Pi’s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead. • T = 150, 000 (CIFAR-10/100-C), T = 75, 000 (ImageNet-C) and T = 72, 712 (Domain- Net). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visitingP1 → ··· → PD. Here, D = 15 - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and D = 3 for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of T. The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-10→ CIFAR-10-C task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100031.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9T= 1000025.8 25.9 26.5 26.1 26.4 25.4 25.8 25.8 26.1 26.2 26.1 26.1 26.1 26.1 26.1 25.9 25.5 25.5 25.7 26.226.0T= 15000024.8 25.3 24.3 24.1 25.3 25.4 25.4 24.5 25.0 24.9 25.0 24.8 25.0 24.5 24.9 24.1 24.0 24.7 24.9 24.424.8 PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100036.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6T= 1000043.5 43.6 43.7 43.7 43.4 43.5 43.6 43.4 43.5 43.6 43.8 43.5 43.5 43.6 43.4 43.6 43.5 43.8 43.7 43.643.6T= 15000035.435.4 35.4 35.3 35.4 35.4 35.5 35.6 35.4 35.4 35.535.3 35.235.435.135.835.135.6 35.3 35.835.4 PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100044.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3T= 2423744.1 44.3 43.9 44.2 44.1 44.3 44.2 44.4 44.1 44.1 44.0 44.3 44.1 44.0 44.0 44.2 44.1 44.1 44.1 44.444.1T= 7271244.3 44.3 44.0 44.3 44.1 44.3 44.2 44.4 44.2 44.1 44.0 44.1 44.2 44.1 44.1 44.1 44.1 44.0 44.0 44.344.2 PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. 1A slight abuse of notation. T here is the number of images between two consecutive resets, following the notation on Sec. 3 of [45], not the sample indices in our notations. 2A subset of 5, 000 samples from ImageNet-C are selected following RobustBench [10] for a consistent evaluation with other benchmarks. 26Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100072.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8T= 500070.2 70.8 71.6 72.1 72.4 72.6 72.9 73.1 73.2 73.6 73.7 73.9 74.0 74.0 74.3 74.1 74.1 73.8 73.5 71.973.0T= 7500067.0 67.1 67.2 67.5 67.5 67.6 67.8 67.6 67.6 67.6 67.5 67.7 67.6 67.9 68.1 67.9 67.4 67.5 67.7 67.567.6 PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of T, the better performance on one dataset does not guarantee the same performance on other datasets. For example, T = 1, 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of T negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. F.4 PeTTA with 40 Recurring Visits To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. F.5 The Sensitivity of Hyper-parameter Choices in PeTTA Table 15: Sensitivity of PeTTA with different choices ofλ0. Dataset λ0 = 1e0 λ0 = 5e0 λ0 = 1e1 λ0 = 5e1 λ0 = 1e2 CIFAR-10-C 22.9 22.7 22.8 23.2 24.1 CIFAR-100-C 35.7 35.3 35.1 35.6 36.1 ImageNet-C 61.2 61.0 60.5 61.3 62.4 There are two hyper-parameters in PeTTA: α0 and λ0. The initial learning rate of α0 = 1e−3 is used for all experiments. We do not tune this hyper-parameter, and the choice of α0 is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). Since λ0 is more specific to PeTTA, we included a sensitive analysis with different choices of λ0 on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of λ0 is not extremely sensitive, and while the best value is1e1 on most datasets, other choices such as 5e0 or 5e1 also produce roughly similar performance. Selecting λ0 is intuitive, the larger value of λ0 stronger prevents the model from collapsing but also limits its adaptability as a trade-off. In action, λ0 is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyper- parameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. F.6 More Details on the Ablation Study We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 → CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of ¯γt (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of ¯γt increases. Unable to adjust αt or constraint the probability space via LAL limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of ¯γt is all observed. Therefore, incorporating all components in PeTTA is necessary. 27Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10→ CIFAR10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 23.5 24.0 27.4 29.9 33.4 35.6 38.0 40.7 43.1 45.0 46.0 48.6 50.0 49.7 50.8 51.5 52.3 53.3 54.3 55.542.6 R(θ)fixedλ= 0.1λ0 23.5 24.0 27.2 29.8 33.4 35.3 37.9 40.5 43.3 45.3 46.8 49.3 50.9 51.0 52.1 53.2 54.0 54.8 56.0 57.643.3R(θ)fixedλ=λ0 23.5 23.6 26.2 28.4 31.6 33.5 36.4 38.7 41.1 43.1 44.8 47.6 49.3 49.5 50.9 52.1 53.1 54.2 55.6 57.042.0 PeTTA-λt 24.9 25.3 26.0 26.4 27.2 26.5 27.2 27.1 27.4 27.7 27.8 28.0 27.5 28.0 27.7 27.4 27.0 27.6 27.8 27.827.1PeTTA-λt+αt 25.5 24.5 23.7 23.1 23.222.423.3 23.2 23.7 24.1 23.9 24.5 24.3 24.0 23.8 23.9 23.8 24.1 24.6 24.723.9PeTTA-λt+LAL 23.323.9 24.6 25.3 26.2 25.9 26.4 26.6 26.9 26.6 26.7 26.7 26.7 26.8 26.8 27.2 26.9 26.9 26.8 27.026.2 PeTTAαt+LAL 24.323.0 22.6 22.4 22.422.522.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8 Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 → CIFAR100-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 40.2 46.3 51.2 54.4 57.3 59.4 61.3 62.6 63.9 65.1 66.3 67.1 68.1 68.9 69.6 70.3 71.1 71.6 72.4 72.963.0 R(θ)fixedλ= 0.1λ0 40.5 46.1 51.5 55.1 58.2 60.5 62.6 64.2 65.7 67.3 68.6 69.5 70.6 71.6 72.5 73.4 74.2 74.9 75.8 76.565.0R(θ)fixedλ=λ0 41.8 47.6 52.6 56.1 58.9 60.7 62.5 63.9 65.0 66.2 67.1 68.3 69.5 70.3 71.4 72.4 73.4 74.1 75.0 75.664.6 PeTTA-λt 39.4 43.4 46.6 49.1 51.0 52.6 53.8 54.7 55.7 56.5 57.1 57.7 58.3 58.8 59.3 59.9 60.6 61.0 61.6 62.155.0PeTTA-λt+αt 39.4 40.1 40.8 40.7 41.2 41.5 41.4 41.6 41.5 41.5 41.7 41.6 41.8 41.7 41.8 42.0 41.9 41.9 42.0 41.841.4PeTTA-λt+LAL 36.2 35.6 35.7 36.1 36.2 36.4 36.4 36.5 36.2 36.2 36.6 36.5 36.5 36.6 36.5 36.6 36.5 36.5 36.3 36.536.3 PeTTAλt+αt+LAL 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1 Table 18: Average classification error of multiple variations of PeTTA. Experiments onreal → clipart, painting, sketch task from DomainNet [44] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 52.3 69.0 68.6 68.6 69.4 70.5 71.8 73.4 75.6 77.6 78.8 81.0 82.8 84.3 85.9 87.4 88.5 89.9 90.8 92.177.9 R(θ)fixedλ= 0.1λ0 52.5 70.0 69.8 70.0 71.1 72.5 74.6 76.1 77.8 80.4 81.9 83.5 85.2 87.2 89.1 90.2 91.5 93.2 94.1 94.980.0R(θ)fixedλ=λ0 54.6 69.8 63.7 56.0 61.7 76.4 70.4 62.5 58.2 76.0 73.6 66.8 58.6 62.3 80.8 75.5 67.0 59.9 59.3 78.366.6 PeTTA-λt 49.2 64.5 62.4 60.9 59.6 58.6 57.7 57.8 57.6 57.7 58.0 58.5 59.0 59.5 59.8 61.1 62.0 62.6 63.6 64.959.7PeTTA-λt+αt 43.942.5 42.3 42.3 42.6 42.843.1 43.7 43.9 44.3 44.6 45.1 45.4 45.7 45.7 46.1 46.1 46.2 46.5 46.444.5PeTTA-λt+LAL 43.6 42.542.6 42.6 42.9 43.0 43.3 43.4 43.1 43.243.143.3 43.3 43.2 43.2 43.9 43.7 43.0 43.2 43.543.2 PeTTAλt+αt+LAL 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9 Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet→ ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 66.9 61.9 72.7 93.6 97.4 97.8 98.0 98.2 98.3 98.3 98.4 98.4 98.5 98.5 98.6 98.6 98.6 98.6 98.7 98.793.4 R(θ)fixedλ= 0.1λ0 65.5 70.9 79.1 85.2 90.3 92.6 95.8 95.8 95.4 97.3 96.9 97.7 97.9 98.2 98.0 98.7 98.6 98.4 98.4 98.792.5R(θ)fixedλ=λ0 66.5 62.1 73.0 93.5 97.0 97.2 97.5 97.5 97.6 97.5 97.7 97.7 97.7 97.8 97.9 97.9 98.0 98.0 98.0 97.992.9 PeTTA-λt 65.9 62.1 76.3 96.7 97.0 96.9 96.9 96.9 97.0 97.1 97.0 97.2 97.0 97.1 97.1 97.0 97.0 97.0 97.0 97.092.7PeTTA-λt+αt 64.870.5 74.6 75.8 75.5 75.8 76.1 76.2 76.2 76.5 76.7 77.0 76.9 77.4 77.1 77.3 77.2 77.4 77.6 77.475.7PeTTA-λt+LAL 64.8 61.160.0 59.8 60.4 60.4 61.2 61.2 61.8 61.9 62.1 62.2 62.1 62.9 62.1 62.8 62.7 62.1 62.8 66.662.0 PeTTA(ours)(∗) 65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5 Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-10 → CIFAR-10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 25.6 24.8 23.8 23.1 23.2 22.7 23.0 22.7 22.7 22.7 22.8 22.7 22.8 22.7 22.522.3 22.2 22.4 22.7 22.823.0L2+Fisher25.2 23.7 22.5 21.8 22.3 21.5 22.3 22.1 22.5 22.8 22.6 22.622.622.8 22.6 22.9 22.6 22.9 23.0 23.322.7 Cosine 24.3 23.022.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8Cosine+Fisher25.1 23.822.2 21.6 22.0 21.4 22.0 21.8 22.1 22.3 22.5 22.4 22.6 22.6 22.422.7 22.6 22.8 22.8 23.322.6 Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-100 → CIFAR-100-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 36.9 35.5 35.5 35.5 35.7 35.6 35.6 35.5 35.5 35.4 35.6 35.5 35.7 35.7 35.7 35.7 35.8 35.5 35.4 35.535.6L2+Fisher36.8 35.4 35.4 35.8 35.9 36.0 35.9 35.9 35.9 35.8 36.1 36.1 36.1 36.1 36.1 36.1 36.2 36.0 36.0 35.936.0 Cosine 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1Cosine+Fisher36.7 35.2 35.5 35.6 35.9 35.9 36.1 36.0 36.0 35.9 36.0 36.0 36.0 36.1 36.0 36.0 35.9 35.9 35.9 36.035.9 28Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on real → clipart, painting, sketch task from DomainNet [44] dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 43.8 42.7 42.5 42.4 42.8 42.9 43.0 43.1 43.1 43.2 43.4 43.3 43.2 43.3 43.2 43.2 43.4 43.0 43.1 43.143.1L2+Fisher43.9 42.8 42.7 43.0 43.2 43.4 43.6 43.8 43.9 44.1 44.0 44.2 44.2 44.2 44.4 44.4 44.5 44.5 44.5 44.543.9 Cosine 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9Cosine+Fisher43.7 42.542.5 42.6 42.9 43.2 43.2 43.5 43.4 43.5 43.4 43.5 43.4 43.6 43.5 43.5 43.4 43.5 43.3 43.443.3 Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet → ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 70.8 72.2 71.5 69.8 72.3 69.3 70.3 70.5 70.0 70.8 70.2 72.1 71.4 70.8 70.9 70.9 69.7 71.0 71.1 70.470.8L2+Fisher70.5 70.0 69.5 69.4 69.6 69.9 69.2 69.3 72.2 70.4 71.0 70.5 71.7 71.5 71.3 68.4 68.6 68.8 68.7 68.770.0 Cosine 65.361.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5Cosine+Fisher65.1 61.760.9 61.2 61.9 62.6 62.8 63.2 64.2 63.4 64.3 64.4 63.9 64.3 65.8 65.5 64.9 65.0 65.2 65.263.8 F.7 More Confusion Matrices in Recurring TTA Setting For the task CIFAR-10→ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. G Experimental Details G.1 Computing Resources A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. G.2 Experiments on CCC Testing Stream In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [ 45] setting. Here we use the baseline accuracy 20%, transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with T = 1000 is used for RDumb [45]. G.3 Test-time Adaptation Methods Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and Do- mainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal 1e−3, and β = (0.9, 0.999) is selected as a universal choice for all experiments. More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. 3https://github.com/oripress/CCC 4https://github.com/DianCh/AdaContrast 29G.4 The Use of Existing Assets Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR- 100-C and ImageNet-C [ 19] are publicly available online, released under Apache-2.0 license. 7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 5https://github.com/BIT-DA/RoTTA 6https://github.com/mariodoebler/test-time-adaptation 7https://github.com/hendrycks/robustness 8https://ai.bu.edu/M3SDA/ 300 10000 20000 30000 40000 Test-time adaptation step (t) 0.40 0.60 0.80 1.00 ¯γt 0 10000 20000 30000 40000 Test-time adaptation step (t) 4.00 6.00 8.00 10.00λt 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00αt 1e 3 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 1.00 2.00 3.00 4.00LCLS 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 2.50 5.00 7.50 10.00LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.50 1.00 1.50 2.00 R(θ) 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00Testing error Figure 7: An inspection of PeTTA on the task CIFAR-10 → CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters (¯γt, λt, αt), (rows 4-5) the value of the loss functions (LCLS, LAL) and (row 6) the value of the regularization term (R(θ)) and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of λt, αt through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. 31Figure 8: Testing error of PeTTA with 40 recurring TTA visits. Total Visits CF-10-C CF-100-C IN-C 20 visits 22.8 35.1 60.5 40 visits 22.9 35.1 61.0 Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the 20 th visit milestone (Fig. 8’s horizontal dashed line). 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.30 0.40 0.50 0.60 0.70 0.80Testing Error PeTTA - λt Baseline w/o R(θ) PeTTA - λt + αt R(θ) fixed λ= 0.1λ0 PeTTA - λt + LAL R(θ) fixed λ= λ0 PeTTA - λt  + αt  + LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.40 0.60 0.80 1.00 ¯γt PeTTA - λt PeTTA - λt + αt PeTTA - λt + LAL PeTTA - λt  + αt  + LAL Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 → CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) R(θ), or fixed regularization coefficient ( λ = λ0/0.1λ0) degrades through time (the top 3 lines). The degradation of PeTTA -λt is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA -λt + αt + LAL achieves the best performance. (bottom): changes of ¯γt in multiple variations of PeTTA. When limiting the degree of freedom in adjusting αt or lacking of supervision from LAL (e.g., PeTTA -λt + αt, PeTTA -λt + LAL, and especially PeTTA -λt), the value of γt, unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. 320: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.79 0.01 0.04 0.03 0.02 0.01 0.01 0.02 0.05 0.02 0.02 0.82 0.01 0.01 0 0.01 0.01 0.01 0.01 0.09 0.06 0 0.68 0.07 0.04 0.03 0.06 0.03 0.01 0.01 0.02 0.01 0.04 0.66 0.04 0.08 0.07 0.05 0.01 0.02 0.03 0 0.04 0.06 0.68 0.02 0.06 0.09 0.01 0.01 0.03 0 0.05 0.15 0.03 0.61 0.03 0.07 0.01 0.01 0.02 0.01 0.03 0.07 0.02 0.02 0.8 0.02 0 0.01 0.01 0 0.02 0.03 0.03 0.02 0.01 0.87 0 0.01 0.09 0.02 0.02 0.02 0.01 0 0.02 0.01 0.77 0.04 0.03 0.03 0.01 0.01 0 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.03 0.03 0.01 0 0.03 0.02 0.07 0.03 0.02 0.76 0 0.01 0 0 0.03 0.01 0.02 0.16 0.07 0 0.63 0.08 0.06 0.02 0.08 0.04 0.01 0.01 0.02 0 0.04 0.7 0.04 0.04 0.09 0.05 0.01 0.02 0.03 0 0.03 0.05 0.73 0.01 0.06 0.08 0.01 0.01 0.01 0 0.03 0.23 0.04 0.53 0.06 0.08 0.01 0.01 0.02 0 0.02 0.1 0.02 0.01 0.81 0.01 0 0.01 0.01 0 0.01 0.05 0.03 0.01 0.01 0.87 0 0.01 0.08 0.01 0.01 0.02 0.01 0 0.02 0.01 0.8 0.04 0.03 0.02 0.01 0.02 0 0 0.02 0.01 0.02 0.87 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.7 0.01 0.03 0.04 0.02 0 0.03 0.03 0.09 0.06 0.01 0.72 0 0.01 0 0 0.04 0 0.01 0.2 0.07 0 0.56 0.1 0.08 0.02 0.09 0.04 0.01 0.02 0.01 0 0.03 0.7 0.05 0.02 0.13 0.04 0 0.02 0.04 0 0.03 0.07 0.69 0 0.08 0.07 0.01 0.01 0.01 0 0.04 0.26 0.05 0.42 0.13 0.07 0 0.01 0.01 0 0.02 0.11 0.03 0 0.8 0.01 0 0.01 0.01 0 0.02 0.06 0.05 0.01 0.04 0.8 0 0.01 0.07 0.01 0.01 0.03 0.01 0 0.03 0.01 0.78 0.05 0.02 0.01 0.01 0.02 0.01 0 0.04 0.01 0.02 0.86 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0.01 0.03 0.06 0.03 0 0.05 0.04 0.09 0.08 0.01 0.66 0 0.02 0.01 0 0.04 0 0.02 0.25 0.07 0 0.48 0.13 0.1 0.02 0.13 0.03 0.01 0.02 0.01 0 0.02 0.68 0.05 0.02 0.17 0.03 0 0.02 0.03 0 0.02 0.07 0.67 0 0.12 0.07 0.01 0.01 0.01 0 0.02 0.29 0.07 0.39 0.14 0.06 0 0.01 0.01 0 0.01 0.11 0.04 0 0.8 0.01 0 0.01 0.01 0 0.02 0.08 0.06 0.01 0.06 0.75 0 0.01 0.05 0.01 0.01 0.04 0.02 0 0.05 0.01 0.74 0.07 0.01 0.01 0 0.03 0.01 0 0.05 0.01 0.02 0.86 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0.03 0.07 0.04 0 0.07 0.04 0.1 0.1 0.01 0.61 0 0.01 0.01 0 0.07 0 0.02 0.26 0.08 0 0.42 0.13 0.13 0.02 0.15 0.03 0.01 0.02 0.02 0 0.01 0.62 0.06 0.02 0.21 0.03 0 0.02 0.03 0 0.02 0.06 0.66 0 0.16 0.06 0.01 0.01 0.01 0 0.02 0.3 0.08 0.34 0.17 0.06 0 0.02 0.01 0 0.01 0.12 0.07 0 0.76 0.01 0 0.02 0.01 0 0.02 0.1 0.08 0.01 0.08 0.69 0 0.02 0.05 0.01 0.01 0.05 0.02 0 0.09 0.01 0.68 0.09 0.01 0.01 0 0.03 0.02 0 0.09 0.01 0.02 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.51 0 0.02 0.07 0.04 0 0.09 0.03 0.1 0.14 0.01 0.56 0 0.01 0.02 0 0.09 0 0.02 0.29 0.08 0 0.35 0.15 0.16 0.02 0.18 0.03 0.01 0.03 0.02 0 0.01 0.57 0.07 0.02 0.27 0.02 0 0.03 0.04 0 0.01 0.08 0.62 0 0.18 0.05 0.01 0.01 0.01 0 0.01 0.29 0.09 0.3 0.21 0.05 0 0.02 0.01 0 0.01 0.12 0.09 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.12 0.01 0.1 0.6 0 0.03 0.06 0.01 0 0.04 0.02 0 0.09 0 0.66 0.11 0.01 0.01 0 0.02 0.03 0 0.11 0 0.02 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.48 0 0.02 0.08 0.04 0 0.11 0.03 0.11 0.13 0.01 0.54 0 0.01 0.02 0 0.11 0 0.02 0.28 0.09 0 0.3 0.16 0.16 0.02 0.21 0.02 0.01 0.03 0.02 0 0.01 0.51 0.08 0.01 0.33 0.01 0.01 0.02 0.03 0 0.01 0.05 0.65 0 0.21 0.03 0.01 0.01 0.02 0 0.01 0.27 0.11 0.25 0.28 0.03 0 0.02 0.01 0 0.01 0.12 0.1 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.13 0.01 0.13 0.56 0 0.03 0.06 0 0 0.06 0.03 0 0.13 0 0.6 0.11 0.02 0.01 0 0.03 0.04 0 0.15 0 0.02 0.73 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.07 0.06 0 0.13 0.01 0.09 0.15 0.01 0.48 0 0.01 0.04 0 0.16 0 0.01 0.28 0.09 0 0.27 0.15 0.19 0.01 0.23 0.01 0.01 0.03 0.02 0 0.01 0.44 0.12 0.01 0.37 0.01 0.01 0.02 0.04 0 0.01 0.05 0.63 0 0.23 0.02 0.01 0.01 0.02 0 0.01 0.25 0.13 0.22 0.33 0.02 0 0.01 0.01 0 0 0.11 0.15 0 0.71 0 0 0.01 0.02 0 0.01 0.09 0.22 0 0.15 0.47 0 0.02 0.08 0 0 0.06 0.05 0 0.15 0 0.55 0.1 0.02 0.01 0 0.04 0.05 0 0.16 0 0.02 0.7 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.06 0.06 0 0.13 0.01 0.1 0.16 0.02 0.47 0 0.01 0.04 0 0.13 0 0.03 0.29 0.1 0 0.24 0.12 0.22 0.01 0.24 0.01 0.01 0.03 0.03 0 0 0.4 0.12 0.01 0.39 0 0.01 0.02 0.05 0 0.01 0.06 0.61 0 0.23 0.02 0.01 0.01 0.03 0 0.01 0.22 0.15 0.2 0.35 0.02 0 0.02 0.01 0 0 0.11 0.15 0 0.7 0 0.01 0.01 0.03 0 0.01 0.08 0.25 0 0.15 0.44 0.01 0.03 0.09 0 0 0.04 0.07 0 0.14 0 0.55 0.1 0.02 0.01 0 0.03 0.05 0 0.16 0 0.02 0.7 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.05 0.07 0 0.14 0.01 0.1 0.16 0.04 0.43 0 0.02 0.07 0 0.14 0 0.03 0.27 0.11 0 0.22 0.11 0.23 0.01 0.26 0.01 0.02 0.03 0.04 0 0 0.33 0.16 0.01 0.43 0 0.01 0.02 0.05 0 0 0.03 0.66 0 0.23 0.01 0.01 0.01 0.04 0 0.01 0.22 0.15 0.18 0.37 0.01 0.01 0.02 0.01 0 0 0.1 0.19 0 0.68 0 0.01 0.01 0.03 0 0.01 0.08 0.28 0.01 0.16 0.41 0.01 0.03 0.11 0 0 0.04 0.05 0 0.14 0 0.56 0.09 0.04 0.01 0 0.02 0.08 0 0.18 0 0.02 0.65 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.04 0.07 0 0.14 0 0.1 0.16 0.04 0.42 0 0.01 0.07 0 0.15 0 0.05 0.26 0.11 0 0.21 0.1 0.26 0.01 0.26 0 0.02 0.03 0.05 0 0 0.31 0.18 0.01 0.42 0 0.01 0.02 0.06 0 0 0.04 0.65 0 0.21 0.01 0.01 0.02 0.04 0 0.01 0.17 0.21 0.15 0.39 0.01 0.01 0.02 0.01 0 0 0.1 0.24 0 0.64 0 0.01 0.01 0.04 0 0.01 0.09 0.28 0 0.16 0.39 0 0.03 0.14 0 0 0.03 0.07 0 0.14 0 0.52 0.09 0.05 0.01 0 0.03 0.1 0 0.18 0 0.03 0.61 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.49 0 0.01 0.03 0.06 0 0.14 0 0.11 0.17 0.07 0.4 0 0.01 0.07 0 0.12 0 0.07 0.27 0.13 0 0.19 0.08 0.27 0.01 0.25 0 0.02 0.03 0.07 0 0 0.27 0.19 0 0.43 0 0.02 0.03 0.07 0 0 0.02 0.64 0 0.23 0.01 0.01 0.01 0.06 0 0.01 0.19 0.18 0.13 0.39 0.01 0.01 0.02 0.02 0 0 0.09 0.22 0 0.65 0 0.01 0.01 0.05 0 0 0.07 0.32 0 0.15 0.36 0.01 0.04 0.17 0 0 0.03 0.07 0 0.12 0 0.53 0.08 0.06 0.01 0 0.01 0.13 0 0.17 0 0.03 0.59 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.5 0 0 0.02 0.08 0 0.13 0 0.1 0.15 0.09 0.37 0 0.01 0.11 0 0.11 0 0.08 0.24 0.15 0 0.18 0.07 0.31 0.01 0.24 0 0.03 0.02 0.09 0 0 0.24 0.17 0 0.44 0 0.02 0.03 0.08 0 0 0.02 0.66 0 0.19 0.01 0.02 0.02 0.08 0 0.01 0.15 0.23 0.11 0.38 0.01 0.01 0.02 0.02 0 0 0.08 0.31 0 0.55 0 0.02 0.01 0.05 0 0 0.05 0.37 0 0.14 0.34 0.01 0.04 0.2 0 0 0.03 0.06 0 0.12 0 0.52 0.08 0.08 0.01 0 0.01 0.11 0 0.15 0 0.04 0.59 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.54 0 0 0.02 0.06 0 0.11 0 0.12 0.15 0.13 0.35 0 0.01 0.1 0 0.09 0 0.12 0.21 0.16 0 0.18 0.07 0.29 0.01 0.24 0 0.03 0.02 0.11 0 0 0.22 0.19 0 0.42 0 0.03 0.03 0.08 0 0 0.03 0.65 0 0.2 0.01 0.02 0.01 0.09 0 0.01 0.12 0.29 0.08 0.37 0 0.02 0.02 0.02 0 0 0.09 0.29 0 0.56 0 0.02 0.01 0.06 0 0 0.05 0.39 0 0.13 0.32 0.01 0.04 0.23 0 0 0.02 0.07 0 0.1 0 0.51 0.07 0.12 0.01 0 0.01 0.11 0 0.13 0 0.05 0.57 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0 0.02 0.08 0 0.1 0 0.12 0.12 0.18 0.32 0 0 0.11 0 0.08 0 0.13 0.19 0.18 0 0.15 0.05 0.34 0 0.2 0 0.04 0.02 0.12 0 0 0.19 0.27 0 0.36 0 0.04 0.02 0.09 0 0 0.02 0.69 0 0.15 0.01 0.02 0.02 0.11 0 0 0.1 0.33 0.07 0.33 0 0.03 0.01 0.03 0 0 0.09 0.35 0 0.5 0 0.02 0.01 0.08 0 0 0.04 0.43 0 0.1 0.29 0.01 0.04 0.26 0 0 0.02 0.08 0 0.08 0 0.51 0.06 0.15 0.01 0 0.01 0.12 0 0.1 0 0.07 0.55 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.58 0 0 0.01 0.07 0 0.09 0 0.13 0.11 0.16 0.32 0 0 0.11 0 0.07 0 0.16 0.18 0.18 0 0.15 0.05 0.36 0 0.19 0 0.04 0.02 0.14 0 0 0.18 0.26 0 0.35 0 0.05 0.02 0.1 0 0 0.01 0.69 0 0.15 0.01 0.03 0.01 0.11 0 0 0.1 0.36 0.05 0.32 0 0.04 0.01 0.03 0 0 0.08 0.38 0 0.46 0 0.03 0.01 0.09 0 0 0.04 0.43 0 0.09 0.29 0.02 0.04 0.29 0 0 0.02 0.09 0 0.08 0 0.47 0.06 0.18 0.01 0 0.01 0.11 0 0.08 0 0.1 0.5 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.6 0 0 0.01 0.08 0 0.08 0 0.13 0.1 0.2 0.28 0 0 0.1 0 0.06 0 0.19 0.17 0.2 0 0.14 0.05 0.36 0 0.18 0 0.05 0.02 0.17 0 0 0.16 0.28 0 0.29 0 0.08 0.02 0.1 0 0 0.01 0.71 0 0.11 0.01 0.04 0.02 0.13 0 0 0.1 0.4 0.04 0.27 0 0.05 0.01 0.04 0 0 0.09 0.4 0 0.41 0 0.04 0.01 0.1 0 0 0.04 0.45 0 0.07 0.27 0.03 0.04 0.34 0 0 0.01 0.08 0 0.05 0 0.47 0.05 0.22 0.01 0 0.01 0.13 0 0.06 0 0.12 0.44 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0 0 0.01 0.09 0 0.08 0 0.13 0.08 0.24 0.26 0 0 0.1 0 0.05 0 0.19 0.15 0.2 0 0.13 0.04 0.41 0 0.16 0 0.05 0.02 0.16 0 0 0.14 0.3 0 0.29 0 0.09 0.02 0.11 0 0 0.01 0.7 0 0.1 0.01 0.05 0.02 0.14 0 0 0.09 0.42 0.02 0.27 0 0.05 0.01 0.03 0 0 0.09 0.44 0 0.39 0 0.04 0.01 0.12 0 0 0.04 0.43 0 0.06 0.28 0.03 0.04 0.35 0 0 0.01 0.07 0 0.06 0 0.46 0.04 0.26 0.01 0 0.01 0.13 0 0.06 0 0.13 0.41 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.67 0 0 0 0.1 0 0.05 0 0.11 0.06 0.3 0.21 0 0 0.1 0 0.03 0 0.21 0.13 0.26 0 0.11 0.04 0.4 0 0.11 0 0.06 0.02 0.2 0 0 0.13 0.32 0 0.21 0 0.12 0.02 0.13 0 0 0.01 0.72 0 0.07 0.01 0.05 0.01 0.2 0 0 0.09 0.42 0.01 0.19 0 0.08 0.01 0.04 0 0 0.08 0.49 0 0.3 0 0.07 0.01 0.16 0 0 0.03 0.45 0 0.04 0.24 0.05 0.03 0.42 0 0 0.01 0.06 0 0.04 0 0.43 0.04 0.33 0.01 0 0 0.11 0 0.03 0 0.15 0.36 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. 330: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.03 0.01 0.02 0.02 0.05 0.02 0.02 0.84 0.01 0.02 0 0.01 0.02 0.01 0.02 0.06 0.04 0 0.69 0.07 0.05 0.05 0.05 0.02 0.01 0.01 0.04 0.01 0.05 0.62 0.05 0.1 0.06 0.04 0.01 0.02 0.03 0 0.06 0.07 0.68 0.05 0.04 0.05 0.01 0.01 0.01 0 0.04 0.14 0.03 0.7 0.03 0.04 0.01 0.01 0.01 0.01 0.04 0.06 0.03 0.03 0.78 0.01 0.01 0.01 0.03 0 0.03 0.04 0.04 0.04 0.01 0.79 0.01 0.01 0.08 0.02 0.02 0.02 0.01 0.01 0.02 0.01 0.8 0.03 0.03 0.05 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.02 0.01 0.03 0.01 0.06 0.02 0.01 0.87 0.01 0.01 0 0.01 0.01 0 0.02 0.05 0.04 0 0.7 0.09 0.05 0.03 0.06 0.02 0.01 0.01 0.03 0.01 0.06 0.64 0.05 0.08 0.06 0.04 0.01 0.02 0.02 0 0.05 0.06 0.74 0.03 0.05 0.04 0.01 0.01 0.01 0 0.05 0.15 0.04 0.66 0.04 0.04 0.01 0.01 0.02 0.01 0.04 0.06 0.02 0.02 0.78 0.01 0.01 0.03 0.02 0 0.03 0.05 0.05 0.03 0.01 0.81 0 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0.01 0.83 0.03 0.02 0.05 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.03 0.02 0 0.03 0.01 0.06 0.02 0.02 0.87 0.01 0.02 0 0 0.01 0 0.02 0.05 0.05 0 0.7 0.07 0.05 0.03 0.06 0.02 0.01 0.01 0.02 0.01 0.05 0.68 0.05 0.07 0.07 0.03 0.01 0.02 0.02 0 0.05 0.06 0.77 0.02 0.04 0.03 0 0 0.01 0 0.07 0.15 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.03 0.02 0.83 0.01 0 0.01 0.01 0 0.03 0.04 0.04 0.02 0.01 0.82 0 0.01 0.06 0.02 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.01 0.02 0.01 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.05 0.04 0.02 0 0.02 0.01 0.07 0.03 0.01 0.87 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.03 0.06 0.01 0.01 0.01 0.01 0.01 0.05 0.71 0.05 0.06 0.06 0.03 0.01 0.01 0.02 0 0.04 0.05 0.78 0.02 0.04 0.03 0.01 0 0.01 0 0.06 0.17 0.04 0.64 0.04 0.03 0.01 0.01 0.01 0 0.03 0.06 0.03 0.01 0.85 0.01 0 0.01 0.01 0 0.04 0.04 0.05 0.02 0.01 0.81 0.01 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.02 0.01 0 0.02 0.01 0.04 0.83 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.02 0.04 0.04 0.02 0 0.02 0.01 0.08 0.02 0.02 0.86 0.01 0.02 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.07 0.05 0.03 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.69 0.05 0.06 0.07 0.02 0.01 0.01 0.02 0 0.05 0.07 0.76 0.02 0.04 0.03 0.01 0 0.01 0 0.07 0.17 0.04 0.64 0.03 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.84 0.01 0.01 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.81 0.01 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.05 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.05 0.7 0.06 0.06 0.07 0.02 0.01 0.01 0.01 0 0.04 0.06 0.79 0.02 0.04 0.02 0.01 0 0.01 0 0.06 0.17 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0.01 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.87 0.02 0.02 0.05 0.02 0.02 0 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.04 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.74 0.06 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.68 0.05 0.06 0.08 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.04 0.02 0 0 0.01 0 0.07 0.18 0.05 0.61 0.04 0.03 0.01 0.01 0 0 0.03 0.06 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0 0.01 0.06 0.02 0.02 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.03 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.04 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.05 0.73 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.06 0.18 0.04 0.63 0.04 0.02 0.01 0.01 0 0 0.03 0.08 0.02 0.01 0.83 0 0 0 0.01 0 0.04 0.05 0.07 0.02 0.01 0.79 0 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.04 0.01 0.03 0.01 0 0.03 0.01 0.04 0.81 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.71 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.04 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.62 0.04 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.05 0.05 0.08 0.02 0.02 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.05 0.7 0.06 0.05 0.08 0.02 0.02 0.02 0.02 0 0.05 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.03 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.04 0.05 0.08 0.02 0.01 0.78 0 0 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.02 0.06 0.05 0.02 0 0.04 0.01 0.07 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.74 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.04 0.02 0.01 0 0 0 0.06 0.19 0.05 0.61 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.05 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.04 0.02 0.03 0.01 0 0.02 0 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.04 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.04 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.05 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.06 0.19 0.05 0.61 0.04 0.03 0.01 0 0 0 0.03 0.09 0.02 0.01 0.83 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.78 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.01 0.05 0.01 0.03 0.01 0 0.02 0 0.04 0.83 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.05 0.04 0.02 0 0.03 0.01 0.09 0.02 0.01 0.86 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.02 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.77 0 0 0.03 0.02 0.02 0.02 0.01 0 0.04 0 0.83 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.02 0 0 0.02 0 0.02 0.05 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0.01 0.05 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.57 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.02 0 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.05 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.04 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.2 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.85 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.78 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.02 0.01 0.09 0.02 0.01 0.86 0.01 0.02 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.02 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.06 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.03 0.03 0.01 0 0.04 0 0.8 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.04 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.74 0.05 0.05 0.06 0.01 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0 0 0.01 0 0.07 0.2 0.05 0.59 0.06 0.02 0.01 0.01 0 0 0.04 0.08 0.02 0.01 0.84 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.05 0.01 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.03 0 0.03 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.04 0.04 0 0.73 0.07 0.06 0.02 0.06 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.01 0 0.06 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.59 0.04 0.02 0.01 0 0 0 0.04 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.76 0.01 0.01 0.05 0.02 0.02 0.03 0.01 0 0.02 0 0.85 0.01 0.02 0.05 0.02 0.03 0.01 0 0.03 0.01 0.04 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.58 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0.01 0.01 0.01 0 0.06 0.05 0.08 0.02 0.02 0.75 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. 34NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 353. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 36(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: This study does not involve any private datasets. All datasets used in our exper- iments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titledExperiment Setup summarizing the experimental details before presenting the results. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 37Answer: [Yes] Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 38• The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgment, this study poses no risks for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? 39Answer: [Yes] Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This study does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 40Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 41",
      "meta_data": {
        "arxiv_id": "2311.18193v4",
        "authors": [
          "Trung-Hieu Hoang",
          "Duc Minh Vo",
          "Minh N. Do"
        ],
        "published_date": "2023-11-30T02:24:44Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18193v4.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of Test-Time Adaptation (TTA) models experiencing performance degradation over prolonged periods, particularly in recurring testing scenarios where environments change and recur. It introduces the 'recurring TTA' diagnostic setting to observe error accumulation, derives theoretical insights into performance degradation using an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC), and proposes Persistent TTA (PeTTA) to prevent model collapse by adaptively adjusting adaptation strategies.",
        "methodology": "The methodology involves: (1) **Recurring TTA:** A new testing scenario extending practical TTA by repeatedly cycling through a sequence of changing and correlated environments to diagnose performance degradation. (2) **Theoretical Analysis:** Simulating a TTA process on an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC) to derive dataset- and algorithm-dependent factors contributing to error accumulation and model collapse. (3) **Persistent TTA (PeTTA):** An adaptation scheme that continuously monitors model divergence from the initial source model using a Mahalanobis distance metric on feature embeddings (γt). Based on this divergence, PeTTA adaptively adjusts the regularization coefficient (λt) and the EMA update rate (αt) to balance adaptation and collapse prevention. It also incorporates an anchor loss (LAL) to constrain model divergence in the probability space and utilizes a category-balanced memory bank and robust batch normalization from prior work.",
        "experimental_setup": "Experiments were conducted on four TTA classification tasks: CIFAR-10→CIFAR-10-C, CIFAR-100→CIFAR-100-C, ImageNet→ImageNet-C (corruption level 5), and DomainNet (real→clipart, painting, sketch). The 'recurring TTA' setting involved revisiting test environments 20 times (and up to 40 for extended evaluation), with temporally correlated batches generated by Dirichlet distributions (Dir(0.1) or Dir(0.01)). Comparisons were made against CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME, and RDumb. An additional evaluation was performed on the 'Continuously Changing Corruption (CCC)' setting. PeTTA used ResNet50 models pre-trained on ImageNet V2 or RobustBench checkpoints. Implementation was in PyTorch, using Adam optimizer (learning rate 1e-3), EMA update rate of 5e-2, cosine similarity regularizer, and adaptive λt/αt (initial λ0=10 or 1, α0=1e-3). Computing resources included an Intel Core i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU.",
        "limitations": "The current PeTTA approach does not rigorously guarantee complete elimination of error accumulation through regularization. It relies on a small memory bank to handle temporally correlated testing streams, which is not its primary focus. Furthermore, PeTTA assumes the availability or computability of feature statistics (empirical mean and covariant matrix) from the source distribution, which might limit its scalability in certain real-world scenarios where source data access is restricted.",
        "future_research_directions": "Future research could focus on developing TTA algorithms that achieve error accumulation-free by construction, moving beyond regularization-based collapse prevention. Another direction is exploring alternative methods for reducing memory requirements, such as storing embedded features instead of original images, to improve the scalability of TTA models in real-world scenarios that may have strict memory constraints.",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport logging\n\n\nclass BaseAdapter(nn.Module):\n    def __init__(self, cfg, model, optimizer):\n        super().__init__()\n        self.logger = logging.getLogger(\"TTA.adapter\")\n        self.cfg = cfg\n        self.model = self.configure_model(model)\n\n        params, param_names = self.collect_params(self.model)\n        if len(param_names) == 0:\n            self.optimizer = None\n        else:\n            self.optimizer = optimizer(params)\n\n        self.steps = self.cfg.OPTIM.STEPS\n        assert self.steps > 0, \"requires >= 1 step(s) to forward and update\"\n\n    def forward(self, x):\n        for _ in range(self.steps):\n            outputs = self.forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def forward_and_and(self, *args):\n        raise NotImplementedError(\"implement forward_and_adapt by yourself!\")\n\n    def configure_model(self, model):\n        raise NotImplementedError(\"implement configure_model by yourself!\")\n\n    def collect_params(self, model: nn.Module):\n        names = []\n        params = []\n\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                names.append(n)\n                params.append(p)\n\n        return params, names\n\n    def check_model(self, model):\n        pass\n\n    def before_tta(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def build_ema(model):\n        ema_model = deepcopy(model)\n        for param in ema_model.parameters():\n            param.detach_()\n        return ema_model\n\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)",
        "experimental_info": "The `softmax_entropy` function is a crucial component for measuring divergence between model predictions, specifically used in `RoTTA` (PeTTA's implementation) for the consistency loss. It calculates the entropy between the soft predictions of the Exponential Moving Average (EMA) model and the student model, serving as an anchor loss to constrain model divergence in the probability space."
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf",
        "github_url": "https://github.com/divelab/ATTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Active Test-Time Adaptation (ATTA) as a novel problem setting to address significant distribution shifts and catastrophic forgetting in unsupervised streaming test data. It provides a learning theory analysis demonstrating that incorporating limited labeled test instances theoretically enhances overall performance and mitigates distribution shifts. The work also presents SimATTA, a simple yet effective algorithm that integrates incremental clustering and selective entropy minimization to implement ATTA and avoid catastrophic forgetting. Extensive experiments show that ATTA achieves substantial performance improvements over traditional TTA methods, maintains efficiency, and performs comparably to Active Domain Adaptation (ADA) methods.",
        "methodology": "ATTA formally defines dynamic model optimization for streaming test data by actively selecting informative instances for labeling. The theoretical foundation uses learning bounds based on H∆H-distance and VC-dimension to guarantee mitigation of distribution shifts (Theorem 1 and 2). Catastrophic forgetting is addressed through selective entropy minimization, where low-entropy samples (pseudo-labeled by the frozen pre-trained source model) are included alongside actively labeled high-entropy samples to maintain source domain performance (Corollary 3 and 4). The SimATTA algorithm partitions incoming unlabeled test samples into high- and low-entropy sets. Low-entropy samples are pseudo-labeled and stored. High-entropy samples are selected using an incremental clustering technique, which employs weighted K-means to store representative 'anchors' of seen distributions, adapting to new distributions while managing a budget. The model is then fine-tuned on both actively labeled high-entropy samples and pseudo-labeled low-entropy samples, balancing their influence according to theoretical insights.",
        "experimental_setup": "The method was evaluated on PACS, VLCS, Office-Home (from DomainBed), and Tiny-ImageNet-C datasets, using ResNet-18 (PACS, VLCS, Tiny-ImageNet-C) or ResNet-50 (Office-Home) architectures pre-trained on ImageNet. Source domains were 'photos' for PACS, 'Caltech101' for VLCS, 'real' for Office-Home, and 'brightness corruption' for Tiny-ImageNet-C. Experiments used two data stream orders: domain-wise (sequential domains) and random (shuffled samples from target domains). Baselines included source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and selected ADA methods (Random, Entropy, K-means, CLUE) for stronger comparisons. Performance was measured by accuracy, reported for real-time adaptation and post-adaptation. Efficiency was also measured in time (seconds) on Tiny-ImageNet-C. Hyperparameters like entropy thresholds (`el`, `eh`) and a cluster centroid budget increase rate (`k`) were empirically set.",
        "limitations": "The theoretical bounds can be loose when test batch sizes are small due to the large VC-dimension of deep models, though fine-tuning pre-trained models is assumed to reduce the effective VC-dimension. The approach's reliance on the quality of the pre-trained model for pseudo-labeling low-entropy samples means incorrect predictions could reinforce errors. Expending annotation budgets on low-entropy samples might not always be cost-effective. The current framework does not address class-incremental problems where the support of labels (Y) changes. The paper's scope is limited to foundational aspects and does not cover scaling to all large models/datasets or task-specific applications.",
        "future_research_directions": "Future work could focus on developing alternative strategies to prevent catastrophic forgetting in ATTA scenarios, especially considering the potential pitfalls of selective entropy minimization. Investigating the cost-effectiveness of annotating low-entropy samples versus correcting them could also be a fruitful area. A promising extension is adapting ATTA methods for large language models (LLMs) where retraining is prohibitively expensive and source data may be inaccessible. Bridging the gap between causal inference and deep learning through causal representation learning is also suggested as a direction for out-of-distribution generalization. Developing more adaptive techniques for managing annotation budgets beyond simple constant increments is another potential area.",
        "experimental_code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1\n",
        "experimental_info": "The SimATTA algorithm addresses dynamic model optimization for streaming test data. It employs a teacher-student framework where a `teacher` model, a deep copy of the initially trained source model, is used for pseudo-labeling low-entropy samples. The `model` (student) is fine-tuned. The core process involves partitioning incoming unlabeled test samples into high- and low-entropy sets using `softmax_entropy`.\n\n**Low-entropy samples (Pseudo-labeling):**\n- Identified by `ent_beta=0` and `entropy < ent_bound` (where `ent_bound` is `self.config.atta.SimATTA.el`).\n- Pseudo-labels are generated using the `teacher` model's predictions (`outputs.argmax(1)`).\n- These samples are stored as `source_anchors` using `update_anchors`.\n- `self.LE` (controlled by `atta.SimATTA.LE`) determines whether to use low-entropy samples. When `LE` is 0, no source anchors are collected.\n\n**High-entropy samples (Active Labeling & Incremental Clustering):**\n- Identified by `ent_beta=1` and `entropy >= ent_bound` (where `ent_bound` is `self.config.atta.SimATTA.eh`).\n- An incremental clustering technique (`target_cluster` flag) employs weighted K-means (`KMeans` from `ATTA.utils.fast_pytorch_kmeans` or `sklearn.cluster.KMeans` with `sample_weight`).\n- The clustering identifies representative 'anchors' of seen distributions, managing a budget.\n- `self.n_clusters` (initial 10) for target clusters increases by `self.nc_increase` (controlled by `atta.SimATTA.nc_increase`, tested with values like 0.25, 0.5, ..., 3).\n- Selected high-entropy samples are stored as `anchors` (`target_anchors`) using `update_anchors`.\n\n**Model Fine-tuning:**\n- The model is fine-tuned (`cluster_train`) on both actively labeled high-entropy samples (`target_anchors`) and pseudo-labeled low-entropy samples (`source_anchors`).\n- Influence is balanced using `alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`.\n- During a `cold_start` phase (`atta.SimATTA.cold_start`, set to 100), `alpha` is capped at `min(0.2, alpha)`.\n- The optimizer used is `torch.optim.SGD` with learning rate `atta.SimATTA.lr` and momentum 0.9.\n- Training steps per batch are controlled by `atta.SimATTA.steps`.\n- Training stops if a `loss_window` (size `atta.SimATTA.stop_tol`) shows no improvement for 5 consecutive checks.\n\n**Budget Management:**\n- `self.budgets` accumulates the number of actively labeled samples (length of `closest` in `sample_select`).\n\n**Experimental Settings from `ATTA/kernel/launch.py`:**\n- **Datasets:** Primarily `VLCS` (with `el=1e-3`) and potentially `PACS` (with `el=1e-4`).\n- **Hyperparameters:**\n    - `atta.SimATTA.cold_start`: 100\n    - `atta.SimATTA.el` (low-entropy bound): 1e-3 (for VLCS), 1e-4 (for PACS)\n    - `atta.SimATTA.nc_increase` (cluster increase rate, denoted as `k`): Tested values include 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3.\n    - `atta.SimATTA.LE` (low-entropy sample usage, denoted as `le`): Tested values 0 (False) and 1 (True).\n    - `atta.SimATTA.target_cluster` (incremental clustering, denoted as `ic`): Tested values 0 (False) and 1 (True).\n    - `atta.gpu_clustering`: True (GPU-accelerated clustering).\n- **General Settings:**\n    - `exp_round`: 1\n    - `num_workers`: 4\n- **Adaptation Loop:** The adaptation process iterates through `self.config.dataset.test_envs[1:]` (target domains)."
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of selecting an effective unsupervised objective (TTA loss) for Test-Time Adaptation (TTA) of neural networks to distribution shifts. It presents the surprising phenomenon that meta-learning the 'best' TTA loss recovers a temperature-scaled softmax-entropy for cross-entropy trained classifiers, but a negative squared error for squared-loss trained classifiers. The main contribution is a generic recipe for finding a good TTA loss for any given supervised training loss function by analyzing TTA through the lens of the training loss's convex conjugate. This framework explains the meta-learning observations and provides 'conjugate pseudo-labels' as a principled soft label for self-training. Empirically, the proposed approach consistently outperforms other TTA alternatives across various domain adaptation benchmarks, particularly benefiting classifiers trained with novel loss functions like PolyLoss.",
        "methodology": "The methodology involves two main parts. First, an empirical exploration using meta-learning, where the TTA loss function is parameterized by a neural network and its parameters are learned by differentiating through the adaptation process to find the loss yielding optimal adaptation. This meta-learning setup revealed the dependence of the optimal TTA loss on the source classifier's original training loss. Second, a theoretical analysis leveraging the convex conjugate function. For a class of loss functions L(h(x),y) = f(h(x)) - yᵀh(x), the paper shows that the conjugate adaptation loss, Lconj(hθ(x)) = -f⋆(∇f(hθ(x))), where f⋆ is the convex conjugate of f, serves as a good local approximation to the original supervised loss. This Lconj can also be interpreted as self-training with specific soft labels, called 'conjugate pseudo-labels' (˜yCPLθ(x) = ∇f(hθ(x))). The model parameters (specifically batch normalization layers) are updated at test time by optimizing this unsupervised conjugate adaptation loss (or self-training with conjugate pseudo-labels) over incoming batches of unlabeled test data, often with an additional temperature scaling.",
        "experimental_setup": "The effectiveness and generality of conjugate pseudo-labeling are evaluated across common corruption benchmarks and domain adaptation datasets. Common corruptions include CIFAR-10-C, CIFAR-100-C, and ImageNet-C, with errors averaged across corruptions (and severity for ImageNet-C). Domain adaptation datasets include SVHN to MNIST/USPS/MNISTM, ImageNet-R, and VisDA-C. Source classifiers use ResNet-26 for CIFAR, ResNet-18 for SVHN-MNIST, and ResNet-50 for ImageNet and VisDA-C. Source models are trained with cross-entropy, PolyLoss, or squared loss. Baselines include Hard Pseudo-Labels (PL), Soft Pseudo-Labels (PL), Entropy Minimization (ENT/TENT), Robust Pseudo-Label (RPL), and MEMO. Adaptation involves fine-tuning only the learnable scale and shift parameters of batch normalization layers. Hyperparameters (learning rate and temperature T) are tuned via grid-search on validation noises for corruption benchmarks, while T is fixed to 1 for domain adaptation tasks without a held-out target domain.",
        "limitations": "The work does not fully answer what constitutes the 'optimal' test-time adaptation loss and why. The meta-learning framework used to discover the initial phenomenon was constrained to learning functions over the logits of individual inputs, suggesting potential for expansion to intermediate representations or batch-level interactions. Achieving good TTA performance still relies on several heuristics, such as updating only batch normalization parameters, and the role of temperature scaling is not fully understood, despite its empirical importance. It remains an open problem to understand under what specific real-world distribution shifts self-training based approaches like this would be most beneficial.",
        "future_research_directions": "Future research directions include expanding the meta-learning framework to learn functions over intermediate representations of the model or over interactions within a batch of inputs, rather than just individual input logits. A more concrete understanding of the role of empirical heuristics, such as updating only batch normalization parameters and the effect of temperature scaling, is also an important area. Further investigation is needed to determine the types of real-world distribution shifts where self-training based approaches are most effective. Additionally, applying and extending the conjugate pseudo-labeling framework to other machine learning settings, such as semi-supervised learning, is a promising avenue."
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf",
        "github_url": "https://github.com/liangchen527/ITTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The main challenge in Domain Generalization (DG) is handling distribution shifts between training and test data. While Test-Time Training (TTT) is a promising solution, its performance hinges on selecting an appropriate auxiliary TTT task and identifying reliable parameters to update. This paper proposes an Improved Test-Time Adaptation (ITTA) method addressing these two factors. ITTA introduces a learnable consistency loss for the TTT task with tuneable parameters to ensure better alignment with the main prediction task. It also introduces additional adaptive parameters for the trained model, suggesting only these adaptive parameters be updated during the test phase, leaving original parameters fixed. Extensive experiments demonstrate that ITTA achieves superior performance compared to state-of-the-art methods on several DG benchmarks for both multi-source and single-source DG tasks.",
        "methodology": "ITTA improves TTT by introducing a learnable consistency loss and additional adaptive parameters. First, for the TTT task, a learnable consistency loss (Lwcont) is proposed, calculated as the L2 norm of the output of a weight subnetwork (fw) applied to the difference between original and augmented feature representations (z - z'). The fw subnetwork, composed of stacked ReLU layers with learnable weights and biases, allows flexible consistency measurement. To ensure alignment with the main classification loss (Lmain, a cross-entropy loss), ITTA enforces equality between the normalized gradients of Lmain and Lwcont with respect to the feature extractor parameters, using this as an objective to update fw's parameters. Second, during the test phase, ITTA inserts new adaptive blocks (fΘ), structured similarly to fw, after each block of the pretrained feature extractor. Crucially, only these newly introduced fΘ parameters are updated using the learnable consistency loss for test samples, while the original feature extractor and classifier parameters remain unchanged. This online adaptation process uses the learned consistency loss to tune the adaptive parameters for unseen target domains.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet. For the backbone, an ImageNet-pretrained ResNet18 with 4 blocks was used as the feature extractor, and the classifier was an MLP layer from the DomainBed benchmark. The proposed fΘ adaptive blocks also consist of 4 blocks, each with 5 layers of learnable parameters, and the fw weight subnetwork uses 10 layers. The weight parameter (α) balancing the main and consistency losses was fixed at 1. Experiments followed the rigorous evaluation protocol of DomainBed, conducting 60 trials for each unseen domain, with 5,000 iteration steps per trial. Model selection used the 'training-domain validate set' method. Performance was measured by average accuracy across 60 trials. ITTA was compared against 22 baseline methods, including ERM, MMD, TTT, TENT, MT3, MixStyle, etc., in both multi-source (leave-one-out) and single-source DG settings. Ablation studies were conducted to assess the effectiveness of the learnable consistency loss (e.g., without fw, with entropy/rotation tasks) and the adaptive parameter strategy (e.g., updating all parameters, only BN layers, different augmentation skills, TTT steps, and network structures for fw/fΘ).",
        "limitations": "One limitation of ITTA is its computational burden during the training phase. The proposed learnable weight subnetwork (fw) requires computing second-order derivatives to enforce gradient alignment, leading to extra processing (1 additional forward and 3 additional backward processes) compared to simpler TTT setups. During test-time adaptation, ITTA also incurs increased FLOPs and inference time due to the additional forward and backward steps required to update the adaptive parameters, making the 'lunch not free' despite the performance improvements.",
        "future_research_directions": "Future efforts aim to simplify the overall optimization process of ITTA and reduce its computational cost, particularly the overhead associated with the learnable consistency loss during training and the adaptation process during testing.",
        "experimental_code": "class ITTA(Algorithm):\\n    \"\"\"Improved Test-Time Adaptation (ITTA)\"\"\"\\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\\n        super(ITTA, self).__init__(input_shape, num_classes, num_domains, hparams)\\n        self.input_shape = input_shape\\n        self.num_classes = num_classes\\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\\n        self.classifier = networks.Classifier(self.featurizer.n_outputs, num_classes, self.hparams['nonlinear_classifier'])\\n        self.test_mapping = networks.MappingNetwork()\\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\\n        self.optimizer = torch.optim.Adam([{'params': self.featurizer.parameters()}, {'params': self.classifier.parameters()}], lr=self.hparams[\"lr\"], weight_decay=self.hparams['weight_decay'])\\n        self.MSEloss = nn.MSELoss()\\n        self.adaparams = networks.Adaparams()\\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\\n\\n    def _get_grads(self, loss):\\n        self.optimizer.zero_grad()\\n        loss.backward(inputs=list(self.featurizer.parameters()), retain_graph=True, create_graph=True)\\n        dict = OrderedDict([(name, weights.grad.clone().view(weights.grad.size(0),-1)) for name, weights in self.featurizer.named_parameters()])\\n        return dict\\n\\n    def update(self, minibatches, unlabeled=None):\\n        all_x = torch.cat([x for x,y in minibatches])\\n        all_y = torch.cat([y for x,y in minibatches])\\n        z_ori, z_aug = self.featurizer(all_x)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + F.cross_entropy(self.classifier(z_aug), all_y)\\n        loss = loss_reg + loss_cla\\n        self.optimizer.zero_grad()\\n        loss.backward()\\n        self.optimizer.step()\\n\\n        z_ori, z_aug = self.featurizer(all_x)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + F.cross_entropy(self.classifier(z_aug), all_y)\\n        dict_reg = self._get_grads(loss_reg)\\n        dict_cla = self._get_grads(loss_cla)\\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\\n        self.adaparams_optimizer.zero_grad()\\n        penalty.backward(inputs=list(self.adaparams.parameters()))\\n        self.adaparams_optimizer.step()\\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\\n\\n    def test_adapt(self, x):\\n        z_ori, z_aug = self.featurizer(x)\\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\\n        self.test_optimizer.zero_grad()\\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\\n        self.test_optimizer.step()\\n\\n    def predict(self, x):\\n        z_ori, z_aug = self.featurizer(x)\\n        z_ori = self.test_mapping.fea1(z_ori)\\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\\n        z_ori = self.test_mapping.fea2(z_ori)\\n        z_ori = self.featurizer.fea3(z_ori)\\n        z_ori = self.test_mapping.fea3(z_ori)\\n        z_ori = self.featurizer.fea4(z_ori)\\n        z_ori = self.test_mapping.fea4(z_ori)\\n        z_ori = self.featurizer.flat(z_ori)\\n        return self.classifier(z_ori)\\n\\nclass MappingNetwork(torch.nn.Module):\\n    def __init__(self, depth=5):\\n        super().__init__()\\n        self.depth = depth\\n        self.weight1 = nn.ParameterList()\\n        self.bias1 = nn.ParameterList()\\n        self.weight2 = nn.ParameterList()\\n        self.bias2 = nn.ParameterList()\\n        self.weight3 = nn.ParameterList()\\n        self.bias3 = nn.ParameterList()\\n        self.weight4 = nn.ParameterList()\\n        self.bias4 = nn.ParameterList()\\n        for i in range(depth):\\n            self.weight1.append(nn.Parameter(torch.ones((64,56,56))))\\n            self.bias1.append(nn.Parameter(torch.zeros((64,56,56))))\\n            self.weight2.append(nn.Parameter(torch.ones((128,28,28))))\\n            self.bias2.append(nn.Parameter(torch.zeros((128,28,28))))\\n            self.weight3.append(nn.Parameter(torch.ones((256,14,14))))\\n            self.bias3.append(nn.Parameter(torch.zeros((256,14,14))))\\n            self.weight4.append(nn.Parameter(torch.ones((512, 7, 7))))\\n            self.bias4.append(nn.Parameter(torch.zeros((512, 7, 7))))\\n        self.relu = nn.ReLU(inplace=True)\\n    def fea1(self, x):\\n        for i in range(self.depth-1):\\n            x = self.relu(self.weight1[i] * x + self.bias1[i])\\n        x = self.weight1[i+1] * x + self.bias1[i+1]\\n        return x\\n    def fea2(self, x):\\n        for i in range(self.depth - 1):\\n            x = self.relu(self.weight2[i] * x + self.bias2[i])\\n        x = self.weight2[i + 1] * x + self.bias2[i + 1]\\n        return x\\n    def fea3(self, x):\\n        for i in range(self.depth - 1):\\n            x = self.relu(self.weight3[i] * x + self.bias3[i])\\n        x = self.weight3[i + 1] * x + self.bias3[i + 1]\\n        return x\\n    def fea4(self, x):\\n        for i in range(self.depth-1):\\n            x = self.relu(self.weight4[i] * x + self.bias4[i])\\n        x = self.weight4[i+1] * x + self.bias4[i+1]\\n        return x\\n\\nclass Identity(nn.Module):\\n    \"\"\"An identity layer\"\"\"\\n    def __init__(self):\\n        super(Identity, self).__init__()\\n    def forward(self, x):\\n        return x\\n\\nclass Adaparams(nn.Module):\\n    def __init__(self, depth=10):\\n        super(Adaparams, self).__init__()\\n        self.relu = nn.ReLU(inplace=True)\\n        self.depth = depth\\n        self.weight = nn.ParameterList()\\n        self.bias = nn.ParameterList()\\n        for i in range(depth):\\n            self.weight.append(nn.Parameter(torch.ones(512)))\\n            self.bias.append(nn.Parameter(torch.zeros(512)))\\n    def forward(self, x):\\n        for i in range(self.depth-1):\\n            x = self.relu(self.weight[i] * x + self.bias[i])\\n        x = self.weight[i+1] * x + self.bias[i+1]\\n        return x\\n\\nclass ResNet_ITTA(torch.nn.Module):\\n    \"\"\"ResNet with the softmax chopped off and the batchnorm frozen\"\"\"\\n    def __init__(self, input_shape, hparams):\\n        super(ResNet_ITTA, self).__init__()\\n        if hparams['resnet18']:\\n            self.network = torchvision.models.resnet18(pretrained=True)\\n            self.n_outputs = 512\\n        else:\\n            self.network = torchvision.models.resnet18(pretrained=True)\\n            self.n_outputs = 2048\\n        nc = input_shape[0]\\n        if nc != 3:\\n            tmp = self.network.conv1.weight.data.clone()\\n            self.network.conv1 = nn.Conv2d(nc, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\\n            for i in range(nc):\\n                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]\\n        self.network.fc = Identity()\\n        self.isaug = True\\n        self.freeze_bn()\\n        self.hparams = hparams\\n        self.dropout = nn.Dropout(hparams['resnet_dropout'])\\n        self.eps = 1e-6\\n\\n    def mixstyle(self, x):\\n        alpha = 0.1\\n        beta = torch.distributions.Beta(alpha, alpha)\\n        B = x.size(0)\\n        mu = x.mean(dim=[2, 3], keepdim=True)\\n        var = x.var(dim=[2, 3], keepdim=True)\\n        sig = (var + self.eps).sqrt()\\n        mu, sig = mu.detach(), sig.detach()\\n        x_normed = (x - mu) / sig\\n        lmda = beta.sample((B, 1, 1, 1))\\n        lmda = lmda.to(x.device)\\n        perm = torch.randperm(B)\\n        mu2, sig2 = mu[perm], sig[perm]\\n        mu_mix = mu * lmda + mu2 * (1 - lmda)\\n        sig_mix = sig * lmda + sig2 * (1 - lmda)\\n        return x_normed * sig_mix + mu_mix\\n\\n    def fea_forward(self, x):\\n        x = self.fea3(x)\\n        x = self.fea4(x)\\n        x = self.flat(x)\\n        return x\\n\\n    def fea2(self, x, aug_x):\\n        x = self.network.layer2(x)\\n        aug_x = self.network.layer2(aug_x)\\n        if not self.isaug:\\n            aug_x = self.mixstyle(aug_x)\\n        return x, aug_x\\n\\n    def fea3(self, x):\\n        x = self.network.layer3(x)\\n        return x\\n\\n    def fea4(self, x):\\n        x = self.network.layer4(x)\\n        return x\\n\\n    def flat(self, x):\\n        x = self.network.avgpool(x)\\n        x = torch.flatten(x, 1)\\n        x = self.network.fc(x)\\n        x = self.dropout(x)\\n        return x\\n\\n    def forward(self, x):\\n        x = self.network.conv1(x)\\n        x = self.network.bn1(x)\\n        x = self.network.relu(x)\\n        x = self.network.maxpool(x)\\n        x = self.network.layer1(x)\\n        if random.random() > 0.5:\\n            self.isaug = True\\n            aug_x = self.mixstyle(x)\\n        else:\\n            self.isaug = False\\n            aug_x = x\\n        return x, aug_x\\n\\n    def train(self, mode=True):\\n        super().train(mode)\\n        self.freeze_bn()\\n\\n    def freeze_bn(self):\\n        for m in self.network.modules():\\n            if isinstance(m, nn.BatchNorm2d):\\n                m.eval()\\n\\ndef Classifier(in_features, out_features, is_nonlinear=False):\\n    if is_nonlinear:\\n        return torch.nn.Sequential(\\n            torch.nn.Linear(in_features, in_features // 2),\\n            torch.nn.ReLU(),\\n            torch.nn.Linear(in_features // 2, in_features // 4),\\n            torch.nn.ReLU(),\\n            torch.nn.Linear(in_features // 4, out_features))\\n    else:\\n        return torch.nn.Linear(in_features, out_features)\\n\\ndef l2_between_dicts(dict_1, dict_2, normalize=False):\\n    assert len(dict_1) == len(dict_2)\\n    dict_1_values = [dict_1[key] for key in sorted(dict_1.keys())]\\n    dict_2_values = [dict_2[key] for key in sorted(dict_1.keys())]\\n    dict_1_tensor = torch.cat(tuple([t.view(-1) for t in dict_1_values]))\\n    dict_2_tensor = torch.cat(tuple([t.view(-1) for t in dict_2_values]))\\n    if normalize:\\n        dict_1_tensor = (dict_1_tensor-dict_1_tensor.mean().item()) / dict_1_tensor.std().item()\\n        dict_2_tensor = (dict_2_tensor-dict_2_tensor.mean().item()) / dict_2_tensor.std().item()\\n        dict_2_tensor = dict_2_tensor.detach()\\n    return (dict_1_tensor-dict_2_tensor).pow(2).mean()",
        "experimental_info": "Hyperparameter Settings (from `domainbed/hparams_registry.py`):\\n- `ada_lr`:\\n  - For the 'DomainNet' dataset, `ada_lr` is set to `0.1`.\\n  - For all other datasets, `ada_lr` is set to `1e-6`.\\n\\nEvaluation Settings (from `domainbed/scripts/train.py` and `domainbed/lib/misc.py`):\\n- During evaluation, if the algorithm is 'ITTA', the `misc.accuracy_tsc` function is used to calculate accuracy.\\n- `misc.accuracy_tsc` performs test-time adaptation by calling `network.test_adapt(x)` for 1 iteration on each test sample `x`.\\n- During this test-time adaptation, the `featurizer` and `classifier` parameters of the network are frozen (set to `requires_grad_(False)`), ensuring only the adaptive parameters (`test_mapping`) are updated."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the limitations of existing Test-Time Adaptation (TTA) methods in dynamic real-world scenarios by proposing a new setup called Practical Test-Time Adaptation (PTTA). PTTA explicitly considers both continually changing data distributions and correlatively sampled test data streams, which simultaneously occur in real applications. The main contribution is the development of Robust Test-Time Adaptation (RoTTA), a comprehensive method designed to robustly adapt pre-trained models in PTTA settings. RoTTA demonstrates state-of-the-art performance, significantly reducing classification error compared to previous TTA methods on various benchmarks.",
        "methodology": "The proposed RoTTA method consists of three key components: 1) Robust Batch Normalization (RBN) which estimates normalization statistics by replacing current batch statistics with global ones maintained via exponential moving average (EMA) to ensure stability against correlated data. 2) Category-balanced sampling with timeliness and uncertainty (CSTU) for maintaining a memory bank. This memory bank stores category-balanced data, prioritizing newer samples and those with lower uncertainty (based on age and prediction entropy) to capture a stable snapshot of the current distribution. 3) Robust training with timeliness, which uses a teacher-student model architecture. The student model is updated by minimizing a loss that incorporates a time-aware reweighting strategy (exponential decay based on sample age) and cross-entropy between strongly-augmented student predictions and weakly-augmented teacher predictions. Only the affine parameters in RBN are trained for efficiency and stability. The teacher model is updated via EMA of the student model's parameters.",
        "experimental_setup": "Experiments were conducted on CIFAR-10-C and CIFAR-100-C for robustness under corruptions, and on DomainNet for generalization under a huge domain gap. For CIFAR-C datasets, pre-trained WildResNet-28 and ResNeXt-29 models (from RobustBench) were used, respectively, with corruptions changing one by one at severity 5 to simulate continually changing distributions. For DomainNet, ResNet-101 models were pre-trained on a source domain and adapted to the remaining five domains. Correlatively sampled test streams were simulated using a Dirichlet distribution with parameter δ=0.1 for all datasets. The Adam optimizer with a learning rate of 1.0 × 10−3 and a batch size of 64 was used. RoTTA hyperparameters (α=0.05, ν=0.001, λt=1.0, λu=1.0, memory bank capacity N=64) were unified across experiments. The method was benchmarked against BN, PL, TENT, LAME, CoTTA, and NOTE, measuring average classification error. Ablation studies verified the effectiveness of each RoTTA component and its robustness to varying distribution changing orders, Dirichlet concentration parameters (δ), and batch sizes.",
        "limitations": "The Robust Batch Normalization (RBN) component is considered a naive solution for normalizing correlatively sampled data and requires careful design of its α value. The current method lacks a mechanism to explicitly recover the model from a collapsed state, although RoTTA is designed to prevent such collapses. Additionally, the paper acknowledges that category similarity is only one type of correlation and further validation in diverse real-world scenarios beyond Dirichlet distribution-simulated test streams is needed.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by exploring alternative or refined components. More significantly, the authors hope this work paves the way for further research into making the Practical Test-Time Adaptation (PTTA) setup even more realistic. The ultimate goal is to facilitate the robust deployment of models in real-world applications through test-time adaptation algorithms.",
        "experimental_code": "import torch\nimport torch.nn as nn\nfrom ..utils import memory\nfrom .base_adapter import BaseAdapter\nfrom copy import deepcopy\nfrom .base_adapter import softmax_entropy\nfrom ..utils.bn_layers import RobustBN1d, RobustBN2d\nfrom ..utils.utils import set_named_submodule, get_named_submodule\nfrom ..utils.custom_transforms import get_tta_transforms\n\n\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\n\n\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n\n\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(ColorJitter):\n    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma\n                                            # after contrast\n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string",
        "experimental_info": "RoTTA Method Specific Configuration:\n- ADAPTER.NAME: \"rotta\"\n- ADAPTER.RoTTA.MEMORY_SIZE: 64 (Capacity of the memory bank for CSTU)\n- ADAPTER.RoTTA.UPDATE_FREQUENCY: 64 (Frequency at which the model is updated using memory data, typically same as MEMORY_SIZE)\n- ADAPTER.RoTTA.NU: 0.001 (Exponential Moving Average (EMA) rate for updating the teacher model parameters)\n- ADAPTER.RoTTA.ALPHA: 0.05 (Momentum for updating running statistics in Robust Batch Normalization (RBN))\n- ADAPTER.RoTTA.LAMBDA_T: 1.0 (Weighting factor for timeliness in CSTU heuristic score)\n- ADAPTER.RoTTA.LAMBDA_U: 1.0 (Weighting factor for uncertainty in CSTU heuristic score)\n\nGeneral Training/Optimization Settings:\n- OPTIM.STEPS: 1 (Number of optimization steps per forward pass in the adapter)\n- OPTIM.LR: 1e-3 (Learning rate for the optimizer)\n- BN.EPS: 1e-5 (Epsilon value for Batch Normalization layers, including RBN)\n\nData Augmentation (Strong Augmentation for Student Model):\n- INPUT.SIZE: (32, 32) (Input image size for transformations)\n- Default Gaussian Noise std: 0.005\n- ColorJitterPro brightness range: [0.6, 1.4]\n- ColorJitterPro contrast range: [0.7, 1.3]\n- ColorJitterPro saturation range: [0.5, 1.5]\n- ColorJitterPro hue range: [-0.06, 0.06]\n- ColorJitterPro gamma range: [0.7, 1.3]\n- RandomAffine degrees: [-15, 15], translate: (1/16, 1/16), scale: (0.9, 1.1)\n- GaussianBlur sigma: [0.001, 0.5]\n- RandomHorizontalFlip probability: 0.5"
      }
    },
    {
      "title": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts",
      "abstract": "In this paper, we propose Test-Time Training, a general approach for\nimproving the performance of predictive models when training and test data come\nfrom different distributions. We turn a single unlabeled test sample into a\nself-supervised learning problem, on which we update the model parameters\nbefore making a prediction. This also extends naturally to data in an online\nstream. Our simple approach leads to improvements on diverse image\nclassification benchmarks aimed at evaluating robustness to distribution\nshifts.",
      "full_text": "Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Yu Sun1 Xiaolong Wang1 2 Zhuang Liu1 John Miller1 Alexei A. Efros1 Moritz Hardt1 Abstract In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a sin- gle unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on di- verse image classiﬁcation benchmarks aimed at evaluating robustness to distribution shifts. 1. Introduction Supervised learning remains notoriously weak at generaliza- tion under distribution shifts. Unless training and test data are drawn from the same distribution, even seemingly minor differences turn out to defeat state-of-the-art models (Recht et al., 2018). Adversarial robustness and domain adapta- tion are but a few existing paradigms that try to anticipate differences between the training and test distribution with either topological structure or data from the test distribution available during training. We explore a new take on gener- alization that does not anticipate the distribution shifts, but instead learns from them at test time. We start from a simple observation. The unlabeled test sample xpresented at test time gives us a hint about the distribution from which it was drawn. We propose to take advantage of this hint on the test distribution by allowing the model parameters θto depend on the test sample x, but not its unknown label y. The concept of a variable decision boundary θ(x) is powerful in theory since it breaks away from the limitation of ﬁxed model capacity (see additional discussion in Section A1), but the design of a feedback mechanism from xto θ(x) raises new challenges in practice that we only begin to address here. 1University of California, Berkeley 2University of California, San Diego. Correspondence to: Yu Sun <yusun@berkeley.edu>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Our proposed test-time training method creates a self- supervised learning problem based on this single test sample x, updating θat test time before making a prediction. Self- supervised learning uses an auxiliary task that automatically creates labels from unlabeled inputs. In our experiments, we use the task of rotating each input image by a multiple of 90 degrees and predicting its angle (Gidaris et al., 2018). This approach can also be easily modiﬁed to work outside the standard supervised learning setting. If several test samples arrive in a batch, we can use the entire batch for test-time training. If samples arrive in an online stream, we obtain further improvements by keeping the state of the parameters. After all, prediction is rarely a single event. The online version can be the natural mode of deployment under the additional assumption that test samples are produced by the same or smoothly changing distribution shifts. We experimentally validate our method in the context of object recognition on several standard benchmarks. These include images with diverse types of corruption at various levels (Hendrycks & Dietterich, 2019), video frames of moving objects (Shankar et al., 2019), and a new test set of unknown shifts collected by (Recht et al., 2018). Our algorithm makes substantial improvements under distribu- tion shifts, while maintaining the same performance on the original distribution. In our experiments, we compare with a strong baseline (labeled joint training) that uses both supervised and self- supervised learning at training-time, but keeps the model ﬁxed at test time. Recent work shows that training-time self- supervision improves robustness (Hendrycks et al., 2019a); our joint training baseline corresponds to an improved imple- mentation of this work. A comprehensive review of related work follows in Section 5. We complement the empirical results with theoretical inves- tigations in Section 4, and establish an intuitive sufﬁcient condition on a convex model of when Test-Time Training helps; this condition, roughly speaking, is to have correlated gradients between the loss functions of the two tasks. Project website: https://test-time-training.github.io/. arXiv:1909.13231v3  [cs.LG]  1 Jul 2020Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 2. Method This section describes the algorithmic details of our method. To set up notation, consider a standard K-layer neural net- work with parameters θk for layer k. The stacked parameter vector θ = ( θ1,...,θ K) speciﬁes the entire model for a classiﬁcation task with loss function lm(x,y; θ) on the test sample (x,y). We call this the main task, as indicated by the subscript of the loss function. We assume to have training data (x1,y1),..., (xn,yn) drawn i.i.d. from a distribution P. Standard empirical risk minimization solves the optimization problem: min θ 1 n n∑ i=1 lm(xi,yi; θ). (1) Our method requires a self-supervised auxiliary task with loss function ls(x). In this paper, we choose the rotation prediction task (Gidaris et al., 2018), which has been demon- strated to be simple and effective at feature learning for convolutional neural networks. The task simply rotates x in the image plane by one of 0, 90, 180 and 270 degrees and have the model predict the angle of rotation as a four- way classiﬁcation problem. Other self-supervised tasks in Section 5 might also be used for our method. The auxiliary task shares some of the model parameters θe = ( θ1,...,θ κ) up to a certain κ ∈ {1,...,K }. We designate those κlayers as a shared feature extractor. The auxiliary task uses its own task-speciﬁc parameters θs = (θ′ κ+1,...,θ ′ K). We call the unshared parameters θs the self-supervised task branch, and θm = (θκ+1,...,θ K) the main task branch . Pictorially, the joint architecture is a Y-structure with a shared bottom and two branches. For our experiments, the self-supervised task branch has the same architecture as the main branch, except for the output dimensionality of the last layer due to the different number of classes in the two tasks. Training is done in the fashion of multi-task learning (Caru- ana, 1997); the model is trained on both tasks on the same data drawn fromP. Losses for both tasks are added together, and gradients are taken for the collection of all parameters. The joint training problem is therefore min θe,θm,θs 1 n n∑ i=1 lm(xi,yi; θm,θe) + ls(xi; θs,θe). (2) Now we describe the standard version of Test-Time Training on a single test sample x. Simply put, Test-Time Training ﬁne-tunes the shared feature extractor θe by minimizing the auxiliary task loss on x. This can be formulated as min θe ls(x; θs,θe). (3) Denote θ∗ e the (approximate) minimizer of Equation 3. The model then makes a prediction using the updated parameters θ(x) = (θ∗ e,θm). Empirically, the difference is negligible between minimizing Equation 3 over θe versus over both θe and θs. Theoretically, the difference exists only when optimization is done with more than one gradient step. Test-Time Training naturally beneﬁts from standard data augmentation techniques. On each test sample x, we per- form the exact same set of random transformations as for data augmentation during training, to form a batch only con- taining these augmented copies of xfor Test-Time Training. Online Test-Time Training. In the standard version of our method, the optimization problem in Equation 3 is al- ways initialized with parameters θ= (θe,θs) obtained by minimizing Equation 2. After making a prediction on x, θ∗ e is discarded. Outside of the standard supervised learning setting, when the test samples arrive online sequentially, the online version solves the same optimization problem as in Equation 3 to update the shared feature extractor θe. How- ever, on test sample xt, θis instead initialized with θ(xt−1) updated on the previous sample xt−1. This allows θ(xt) to take advantage of the distributional information available in x1,...,x t−1 as well as xt. 3. Empirical Results We experiment with both versions of our method (standard and online) on three kinds of benchmarks for distribution shifts, presented here in the order of visually low to high- level. Our code is available at the project website. Network details. Our architecture and hyper-parameters are consistent across all experiments. We use ResNets (He et al., 2016b), which are constructed differently for CIFAR-10 (Krizhevsky & Hinton, 2009) (26-layer) and Ima- geNet (Russakovsky et al., 2015) (18-layer). The CIFAR-10 dataset contains 50K images for training, and 10K images for testing. The ImageNet contains 1.2M images for train- ing and the 50K validation images are used as the test set. ResNets on CIFAR-10 have three groups, each containing convolutional layers with the same number of channels and size of feature maps; our splitting point is the end of the second group. ResNets on ImageNet have four groups; our splitting point is the end of the third group. We use Group Normalization (GN) instead of Batch Nor- malization (BN) in our architecture, since BN has been shown to be ineffective when training with small batches, for which the estimated batch statistics are not accurate (Ioffe & Szegedy, 2015). This technicality hurts Test-Time Training since each batch only contains (augmented) copies of a single image. Different from BN, GN is not dependent on batch size and achieves similar results on our baselines.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure 1.Test error (%) on CIFAR-10-C with level 5 corruptions.We compare our approaches, Test-Time Training (TTT) and its online version (TTT-Online), with two baselines: object recognition without self-supervision, and joint training with self-supervision but keeping the model ﬁxed at test time. TTT improves over the baselines and TTT-Online improves even further. We report results with BN in Section A4 of the appendix for completeness. We directly compare our architecture to that of Hendrycks et al. (2018) in subsection A4.5. Optimization details. For joint training (Equation 2), we use stochastic gradient descent with standard hyper- parameters as (Huang et al., 2016; He et al., 2016a). For Test-Time Training (Equation 3), we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments. We set weight decay and momentum to zero during Test-Time Training, inspired by practice in (He et al., 2018; Liu et al., 2018). For the standard version of Test-Time Training, we take ten gradient steps, using batches independently gener- ated by the same image. For online version of Test-Time Training, we take only one gradient step given each new im- age. We use random crop and random horizontal ﬂip for data augmentation. See Section A2 of the appendix for computa- tional aspects of our method. In all the tables and ﬁgures, object recognition task onlyrefers to the plain ResNet model (using GN, unless otherwise speciﬁed); joint training refers to the model jointly trained on both the main task and the self-supervised task, ﬁxed at test time; this has been pro- posed as the method in Hendrycks et al. (2019a); Test-Time Training (TTT) refers to the standard version described sec- tion 2; and online Test-Time Training (TTT-Online)refers to the online version that does not discardθ(xt) for xt arriving sequentially from the same distribution. Performance for TTT-Online is calculated as the average over the entire test set; we always shufﬂe the test set before TTT-Online to avoid ordering artifacts. 3.1. Object Recognition on Corrupted Images Hendrycks & Dietterich (2019) propose to benchmark ro- bustness of object recognition with 15 types of corruptions from four broad categories: noise, blur, weather and digital. Each corruption type comes in ﬁve levels of severity, with level 5 the most severe (details and sample images in the ap- pendix). The corruptions are simulated to mimic real-world corruptions as much as possible on copies of the test set for both CIFAR-10 and ImageNet. The new test sets are named as CIFAR-10-C and ImageNet-C, respectively. In the pro- posed benchmark, training should be done on the original training set, and the diversity of corruption types should make it difﬁcult for any methods to work well across the board if it relies too much on corruption speciﬁc knowledge. For online Test-Time Training, we take the entire test set as a stream of incoming images, and update and test on each image in an online manner as it arrives. CIFAR-10-C. Our results on the level 5 corruptions (most severe) are shown in Figure 1. The results on levels 1-4 are shown in Section A4 in appendix. Across all ﬁve levels and 15 corruption types, both standard and online versions of Test-Time Training improve over the object recognition task only baseline by a large margin. The standard version always improves over joint training, and the online version often improves signiﬁcantly (>10%) over joint training and never hurts by more than 0.2%. Speciﬁcally, TTT-Online contributes >24% on the three noise types and 38% on pix- elation. For a learning problem with the seemingly unstable setup that abuses a single image, this kind of consistency is rather surprising. The baseline ResNet-26 with object recognition task only has error 8.9% on the original test set of CIFAR-10. The joint training baseline actually improves performance on the original to 8.1%. More surprisingly, unlike many other methods that trade off original performance for robustness, Test-Time Training further improves on the original test set by 0.2% consistently over multiple independent trials. This suggests that our method does not choose between speciﬁcity and generality.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Accuracy (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online 0 20000 40000 Number of samples 60 62 64 66 68 70 72 74 76Accuracy (%) Original Sliding window average 0 20000 40000 Number of samples 12 15 18 21 24 27 30 33Accuracy (%) Gaussian Noise Sliding window average 0 20000 40000 Number of samples 16 18 20 22 24 26 28 30 32Accuracy (%) Defocus Blur Sliding window average 0 20000 40000 Number of samples 28 30 32 34 36 38Accuracy (%) Zoom Blur Sliding window average 0 20000 40000 Number of samples 33 36 39 42 45 48 51 54Accuracy (%) Fog Sliding window average 0 20000 40000 Number of samples 30 33 36 39 42 45 48 51Accuracy (%) Elastic Transform Sliding window average Figure 2.Test accuracy (%) on ImageNet-C with level 5 corruptions.Upper panel: Our approaches, TTT and TTT-Online, show signiﬁcant improvements in all corruption types over the two baselines. Lower panel: We show the accuracy of TTT-Online as the average over a sliding window of 100 samples; TTT-Online generalizes better as more samples are evaluated (x-axis), without hurting on the original distribution. We use accuracy instead of error here because the baseline performance is very low for most corruptions. Separate from our method, it is interesting to note that joint training consistently improves over the single-task baseline, as discovered by Hendrycks et al. (2019a). Hendrycks & Dietterich (2019) have also experimented with various other training methods on this benchmark, and point to Adversar- ial Logit Pairing (ALP) (Kannan et al., 2018) as the most effective approach. Results of this additional baseline on all levels of CIFAR-10-C are shown in the appendix, along with its implementation details. While surprisingly robust under some of the most severe corruptions (especially the three noise types), ALP incurs a much larger error (by a factor of two) on the original distribution and some corruptions (e.g. all levels of contrast and fog), and hurts performance signiﬁcantly when the corruptions are not as severe (espe- cially on levels 1-3); this kind of tradeoff is to be expected for methods based on adversarial training. ImageNet-C. Our results on the level 5 corruptions (most severe) are shown in Figure 2. We use accuracy instead of error for this dataset because the baseline performance is very low for most corruptions. The general trend is roughly the same as on CIFAR-10-C. The standard version of TTT always improves over the baseline and joint training, while the online version only hurts on the original by 0.1% over the baseline, but signiﬁcantly improves (by a factor of more than three) on many of the corruption types. In the lower panel of Figure 2, we visualize how the accu- racy (averaged over a sliding window) of the online version changes as more images are tested. Due to space constraints, we show this plot on the original test set, as well as every third corruption type, following the same order as in the original paper. On the original test set, there is no visible trend in performance change after updating on the 50,000 samples. With corruptions, accuracy has already risen sig- niﬁcantly after 10,000 samples, but is still rising towards the end of the 50,000 samples, indicating room for additional improvements if more samples were available. Without seeing a single label, TTT-Online behaves as if we were training on the test set from the appearance of the plots.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 Table 1.Test error (%) on CIFAR-10-C with level 5 corruption.Comparison between online Test-Time Training (TTT-Online) and unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019) with access to the entire (unlabeled) test set during training. We highlight the lower error in bold. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. The reported numbers for TTT-Online are the same as in Figure 1. See complete table in Table A2. 0 2000 4000 6000 8000 Number of samples 12 16 20 24 28 32 36 40 44 48Error (%) Gaussian Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 9 12 15 18 21 24 27 30 33 36Error (%) Shot Noise Joint training TTT TTT-Online UDA-SS 0 2000 4000 6000 8000 Number of samples 15 20 25 30 35 40 45 50Error (%) Impulse Noise Joint training TTT TTT-Online UDA-SS Figure 3.Test error (%) on CIFAR-10-C, for the three noise types, with gradually changing distribution.The distribution shifts are created by increasing the standard deviation of each noise type from small to large, the further we go on the x-axis. As the samples get noisier, all methods suffer greater errors the more we evaluate into the test set, but online Test-Time Training (TTT-Online) achieves gentler slopes than joint training. For the ﬁrst two noise types, TTT-Online also achieves better results over unsupervised domain adaptation by self-supervision (UDA-SS) (Sun et al., 2019). Comparison with unsupervised domain adaptation. Table 1 empirically compares online Test-Time Training (TTT-Online) with unsupervised domain adaptation through self-supervision (UDA-SS) (Sun et al., 2019), which is sim- ilar to our method in spirit but is designed for the setting of unsupervised domain adaptation (Section 5 provides a sur- vey of other related work in this setting). Given labeled data from the training distribution and unlabeled data from the test distribution, UDA-SS hopes to ﬁnd an invariant repre- sentation that extracts useful features for both distributions by learning to perform a self-supervised task, speciﬁcally rotation prediction, simultaneously on data from both. It then learns a labeling function on top of the invariant rep- resentation using the labeled data. In our experiments, the unlabeled data given to UDA-SS is the entire test set itself without the labels. Because TTT-Online can only learn from the unlabeled test samples that have already been evaluated on, it is given less information than UDA-SS at all times. In this sense, UDA- SS should be regarded as an oracle rather than a baseline. Surprisingly, TTT-Online outperforms UDA-SS on 13 out of the 15 corruptions as well as the original distribution. Our explanation is that UDA-SS has to ﬁnd an invariant representation for both distributions, while TTT-Online only adapts the representation to be good for the current test distribution. That is, TTT-Online has the ﬂexibility to forget the training distribution representation, which is no longer relevant. This suggests that in our setting, forgetting is not harmful and perhaps should even be taken advantage of. Gradually changing distribution shifts.In our previous experiments, we have been evaluating the online version under the assumption that the test inputs xt for t= 1...nare all sampled from the same test distribution Q, which can be different from the training distribution P. This assumption is indeed satisﬁed for i.i.d. samples from a shufﬂed test set. But here we show that this assumption can in fact be relaxed to allow xt ∼Qt, where Qt is close to Qt+1 (in the sense of distributional distance). We call this the assumption of gradually changing distribution shifts. We perform experiments by simulating such distribution shifts on the three noise types of CIFAR-10-C. For each noise type, xt is corrupted with standard deviation σt, and σ1,...,σ n interpolate between the standard deviation of level 1 and level 5. So xt is more severely corrupted as we evaluate further into the test set and t grows larger. As shown in Figure 3, TTT-Online still improves upon joint training (and our standard version) with this relaxed assumption, and even upon UDA-SS for the ﬁrst two noise types.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Accuracy (%) Airplane Bird Car Dog Cat Horse Ship Average Object recognition task only 67.9 35.8 42.6 14.7 52.0 42.0 66.7 41.4 Joint training (Hendrycks et al., 2019a) 70.2 36.7 42.6 15.5 52.0 44.0 66.7 42.4 TTT (standard version) 70.2 39.2 42.6 21.6 54.7 46.0 77.8 45.2 TTT-Online 70.2 39.2 42.6 22.4 54.7 46.0 77.8 45.4 Table 2.Class-wise and average classiﬁcation accuracy (%) on CIFAR classes in VID-Robust, adapted from (Shankar et al., 2019). Test-Time Training (TTT) and online Test-Time Training (TTT-Online) improve over the two baselines on average, and by a large margin on “ship” and “dog” classes where the rotation task is more meaningful than in classes like “airplane” (sample images in Figure A7). 3.2. Object Recognition on Video Frames The Robust ImageNet Video Classiﬁcation (VID-Robust) dataset was developed by Shankar et al. (2019) from the Ima- geNet Video detection dataset (Russakovsky et al., 2015), to demonstrate how deep models for object recognition trained on ImageNet (still images) fail to adapt well to video frames. The VID-Robust dataset contains 1109 sets of video frames in 30 classes; each set is a short video clip of frames that are similar to an anchor frame. Our results are reported on the anchor frames. To map the 1000 ImageNet classes to the 30 VID-Robust classes, we use the max-conversion function in Shankar et al. (2019). Without any modiﬁcations for videos, we apply our method to VID-Robust on top of the same ImageNet model as in the previous subsection. Our classiﬁcation accuracy is reported in Table 3. In addition, we take the seven classes in VID-Robust that overlap with CIFAR-10, and re-scale those video frames to the size of CIFAR-10 images, as a new test set for the model trained on CIFAR-10 in the previous subsection. Again, we apply our method to this dataset without any modiﬁcations. Our results are shown in Table 2, with a breakdown for each class. Noticing that Test-Time Training does not improve on the airplane class, we inspect some airplane samples (Figure A7), and observe black margins on two sides of most images, which provide a trivial hint for rotation prediction. In addition, given an image of airplanes in the sky, it is often impossible even for humans to tell if it is rotated. This shows that our method requires the self-supervised task to be both well deﬁned and non-trivial. 3.3. CIFAR-10.1: Unknown Distribution Shifts CIFAR-10.1 (Recht et al., 2018) is a new test set of size 2000 modeled after CIFAR-10, with the exact same classes and image dimensionality, following the dataset creation process documented by the original CIFAR-10 paper as closely as possible. The purpose is to investigate the distribution shifts present between the two test sets, and the effect on object recognition. All models tested by the authors suffer a large performance drop on CIFAR-10.1 comparing to CIFAR-10, even though there is no human noticeable difference, and Method Accuracy (%) Object recognition task only 62.7 Joint training (Hendrycks et al., 2019a) 63.5 TTT (standard version) 63.8 TTT-Online 64.3 Table 3.Test accuracy (%) on VID-Robust dataset (Shankar et al., 2019). TTT and TTT-Online improve over the baselines. Method Error (%) Object recognition task only 17.4 Joint training (Hendrycks et al., 2019a) 16.7 TTT (standard version) 15.9 Table 4.Test error (%) on CIFAR-10.1 (Recht et al., 2018). TTT is the ﬁrst method to improve the performance of an existing model on this new test set. both have the same human accuracy. This demonstrates how insidious and ubiquitous distribution shifts are, even when researchers strive to minimize them. The distribution shifts from CIFAR-10 to CIFAR-10.1 pose an extremely difﬁcult problem, and no prior work has been able to improve the performance of an existing model on this new test set, probably because: 1) researchers cannot even identify the distribution shifts, let alone describe them mathematically; 2) the samples in CIFAR-10.1 are only revealed at test time; and even if they were revealed during training, the distribution shifts are too subtle, and the sample size is too small, for domain adaptation (Recht et al., 2018). On the original CIFAR-10 test set, the baseline with only object recognition has error 8.9%, and with joint training has 8.1%; comparing to the ﬁrst two rows of Table 4, both suffer the typical performance drop (by a factor of two). TTT yields an improvement of 0.8% (relative improvement of 4.8%) over joint training. We recognize that this improve- ment is small relative to the performance drop, but see it as an encouraging ﬁrst step for this very difﬁcult problem.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts 0 10 20 30 40 50 60 Gradient inner product 0 1 2 3 4 5Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 0 10 20 30 40 50 60 Gradient inner product 0 5 10 15 20 25 30 35Improvement (%) Level 5 Level 4 Level 3 Level 2 Level 1 Figure 4.Scatter plot of the inner product between the gradients (on the shared feature extractor θe) of the main task lm and the self- supervised task le, and the improvement in test error (%) from Test-Time Training, for the standard (left) and online (right) version. Each point is the average over a test set, and each scatter plot has 75 test sets, from all 15 types of corruptions over ﬁve levels as described in subsection 3.1. The blue lines and bands are the best linear ﬁts and the 99% conﬁdence intervals. The linear correlation coefﬁcients are 0.93 and 0.89 respectively, indicating strong positive correlation between the two quantities, as suggested by Theorem 1. 4. Theoretical Results This section contains our preliminary study of when and why Test-Time Training is expected to work. For convex models, we prove that positive gradient correlation between the loss functions leads to better performance on the main task after Test-Time Training. Equipped with this insight, we then empirically demonstrate that gradient correlation governs the success of Test-Time Training on the deep learning model discussed in Section 3. Before stating our main theoretical result, we ﬁrst illustrate the general intuition with a toy model. Consider a regression problem where x∈Rd denotes the input, y1 ∈R denotes the label, and the objective is the square loss (ˆy−y1)2/2 for a prediction ˆy. Consider a two layer linear network parametrized by A∈Rh×d and v ∈Rh (where hstands for the hidden dimension). The prediction according to this model is ˆy= v⊤Ax, and the main task loss is lm(x,y1; A,v) = 1 2 ( y1 −v⊤Ax )2 . (4) In addition, consider a self-supervised regression task that also uses the square loss and automatically generates a label ys for x. Let the self-supervised head be parametrized by w∈Rh. Then the self-supervised task loss is ls(x,y2; A,w) = 1 2 ( y2 −w⊤Ax )2 . (5) Now we apply Test-Time Training to update the shared feature extractor Aby one step of gradient descent on ls, which we can compute with y2 known. This gives us A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (6) where A′is the updated matrix and ηis the learning rate. If we set η= η∗where η∗= y1 −v⊤Ax (y2 −w⊤Ax) v⊤wx⊤x, (7) then with some simple algebra, it is easy to see that the main task loss lm(x,y1; A′,v) = 0. Concretely, Test-Time Training drives the main task loss down to zero with a single gradient step for a carefully chosen learning rate. In prac- tice, this learning rate is unknown since it depends on the unknown y1. However, since our model is convex, as long as η∗is positive, it sufﬁces to set η to be a small positive constant (see details in the appendix). If x̸= 0, one sufﬁ- cient condition for η∗to be positive (when neither loss is zero) is to have sign ( y1 −v⊤Ax ) = sign ( y2 −w⊤Ax ) (8) and v⊤w>0 . (9) For our toy model, both parts of the condition above have an intuition interpretation. The ﬁrst part says that the mistakes should be correlated, in the sense that predictions from both tasks are mistaken in the same direction. The second part, v⊤w>0, says that the decision boundaries on the feature space should be correlated. In fact, these two parts hold iff. ⟨∇lm(A),∇ls(A)⟩>0 (see a simple proof of this fact in the appendix). To summarize, if the gradients have positive correlation, Test-Time Training is guaranteed to reduce the main task loss. Our main theoretical result extends this to general smooth and convex loss functions.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Theorem 1. Let lm(x,y; θ) denote the main task loss on test instance x,y with parameters θ, and ls(x; θ) the self- supervised task loss that only depends onx. Assume that for all x,y, lm(x,y; θ) is differentiable, convex andβ-smooth in θ, and both ∥∇lm(x,y; θ)∥,∥∇ls(x,θ)∥≤ Gfor all θ. With a ﬁxed learning rate η= ϵ βG2 , for every x,y such that ⟨∇lm(x,y; θ),∇ls(x; θ)⟩>ϵ, (10) we have lm(x,y; θ) >lm(x,y; θ(x)), (11) where θ(x) = θ−η∇ls(x; θ) i.e. Test-Time Training with one step of gradient descent. The proof uses standard techniques in optimization, and is left for the appendix. Theorem 1 reveals gradient correlation as a determining factor of the success of Test-Time Training in the smooth and convex case. In Figure 4, we empirically show that our insight also holds for non-convex loss func- tions, on the deep learning model and across the diverse set of corruptions considered in Section 3; stronger gradient cor- relation clearly indicates more performance improvement over the baseline. 5. Related Work Learning on test instances. Shocher et al. (2018) pro- vide a key inspiration for our work by showing that image super-resolution could be learned at test time simply by try- ing to upsample a downsampled version of the input image. More recently, Bau et al. (2019) improve photo manipula- tion by adapting a pre-trained GAN to the statistics of the input image. One of the earlier examples of this idea comes from Jain & Learned-Miller (2011), who improve Viola- Jones face detection (Viola et al., 2001) by bootstrapping the more difﬁcult faces in an image from the more easily detected faces in that same image. The online version of our algorithm is inspired by the work of Mullapudi et al. (2018), which makes video segmentation more efﬁcient by using a student model that learns online from a teacher model. The idea of online updates has also been used in Kalal et al. (2011) for tracking and detection. A recent work in echocardiography (Zhu et al., 2019) improves the deep learning model that tracks myocardial motion and cardiac blood ﬂow with sequential updates. Lastly, we share the philosophy of transductive learning (Vapnik, 2013; Gam- merman et al., 1998), but have little in common with their classical algorithms; recent work by Tripuraneni & Mackey (2019) theoretically explores this for linear prediction, in the context of debiasing the LASSO estimator. Self-supervised learning studies how to create labels from the data, by designing various pretext tasks that can learn semantic information without human annotations, such as context prediction (Doersch et al., 2015), solving jig- saw puzzles (Noroozi & Favaro, 2016), colorization (Lars- son et al., 2017; Zhang et al., 2016), noise prediction (Bo- janowski & Joulin, 2017), feature clustering (Caron et al., 2018). Our paper uses rotation prediction (Gidaris et al., 2018). Asano et al. (2019) show that self-supervised learn- ing on only a single image, surprisingly, can produce low- level features that generalize well. Closely related to our work, Hendrycks et al. (2019a) propose that jointly training a main task and a self-supervised task (our joint training baseline in Section 3) can improve robustness on the main task. The same idea is used in few-shot learning (Su et al., 2019), domain generalization (Carlucci et al., 2019), and unsupervised domain adaptation (Sun et al., 2019). Adversarial robustness studies the robust risk RP,∆(θ) = Ex,y∼P maxδ∈∆ l(x + δ,y; θ), where l is some loss function, and ∆ is the set of perturbations; ∆ is often chosen as the Lp ball, for p ∈{1,2,∞}. Many popular algorithms formulate and solve this as a robust optimization problem (Goodfellow et al., 2014; Madry et al., 2017; Sinha et al., 2017; Raghunathan et al., 2018; Wong & Kolter, 2017; Croce et al., 2018), and the most well known technique is adversarial training. Another line of work is based on randomized smoothing (Cohen et al., 2019; Salman et al., 2019), while some other approaches, such as input transformations (Guo et al., 2017; Song et al., 2017), are shown to be less effective (Athalye et al., 2018). There are two main problems with the approaches above. First, all of them can be seen as smoothing the decision boundary. This establishes a theoretical tradeoff between accuracy and robustness (Tsipras et al., 2018; Zhang et al., 2019), which we also observe empirically with our adversarial training baseline in Section 3. Intuitively, the more diverse ∆ is, the less effective this one-boundary-ﬁts-all approach can be for a particular element of ∆. Second, adversarial methods rely heavily on the mathematical structure of ∆, which might not accurately model perturbations in the real world. Therefore, generalization remains hard outside of the ∆ we know in advance or can mathematically model, especially for non-adversarial distribution shifts. Empirically, Kang et al. (2019) shows that robustness for one ∆ might not transfer to another, and training on the L∞ball actually hurts robustness on the L1 ball. Non-adversarial robustness studies the effect of corrup- tions, perturbations, out-of-distribution examples, and real- world distribution shifts (Hendrycks et al., 2019b;a; 2018; Hendrycks & Gimpel, 2016). Geirhos et al. (2018) show that training on images corrupted by Gaussian noise makes deep learning models robust to this particular noise type, but does not improve performance on images corrupted by another noise type e.g. salt-and-pepper noise.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Unsupervised domain adaptation (a.k.a. transfer learn- ing) studies the problem of distribution shifts, when an unlabeled dataset from the test distribution (target domain) is available at training time, in addition to a labeled dataset from the training distribution (source domain) (Chen et al., 2011; Gong et al., 2012; Long et al., 2015; Ganin et al., 2016; Long et al., 2016; Tzeng et al., 2017; Hoffman et al., 2017; Csurka, 2017; Chen et al., 2018). The limitation of the problem setting, however, is that generalization might only be improved for this speciﬁc test distribution, which can be difﬁcult to anticipate in advance. Prior work try to anticipate broader distributions by using multiple and evolv- ing domains (Hoffman et al., 2018; 2012; 2014). Test-Time Training does not anticipate any test distribution, by chang- ing the setting of unsupervised domain adaptation, while taking inspiration from its algorithms. Our paper is a follow- up to Sun et al. (2019), which we explain and empirically compare with in Section 3. Our update rule can be viewed as performing one-sample unsupervised domain adaptation on the ﬂy, with the caveat that standard domain adaptation techniques might become ill-deﬁned when there is only one sample from the target domain. Domain generalization studies the setting where a meta distribution generates multiple environment distributions, some of which are available during training (source), while others are used for testing (target) (Li et al., 2018; Shankar et al., 2018; Muandet et al., 2013; Balaji et al., 2018; Ghifary et al., 2015; Motiian et al., 2017; Li et al., 2017a; Gan et al., 2016). With only a few environments, information on the meta distribution is often too scarce to be helpful, and with many environments, we are back to the i.i.d. setting where each environment can be seen as a sample, and a strong baseline is to simply train on all the environments (Li et al., 2019). The setting of domain generalization is limited by the inherent tradeoff between speciﬁcity and generality of a ﬁxed decision boundary, and the fact that generalization is again elusive outside of the meta distribution i.e. the actual P learned by the algorithm. One (few)-shot learning studies how to learn a new task or a new classiﬁcation category using only one (or a few) sample(s), on top of a general representation that has been learned on diverse samples (Snell et al., 2017; Vinyals et al., 2016; Fei-Fei et al., 2006; Ravi & Larochelle, 2016; Li et al., 2017b; Finn et al., 2017; Gidaris & Komodakis, 2018). Our update rule can be viewed as performing one-shot self- supervised learning and can potentially be improved by progress in one-shot learning. Continual learning (a.k.a. learning without forgetting) studies the setting where a model is made to learn a sequence of tasks, and not forget about the earlier ones while training for the later (Li & Hoiem, 2017; Lopez-Paz & Ranzato, 2017; Kirkpatrick et al., 2017; Santoro et al., 2016). In contrast, with Test-Time Training, we are not concerned about forgetting the past test samples since they have already been evaluated on; and if a past sample comes up by any chance, it would go through Test-Time Training again. In addition, the impact of forgetting the training set is minimal, because both tasks have already been jointly trained. Online learning (a.k.a. online optimization) is a well- studied area of learning theory (Shalev-Shwartz et al., 2012; Hazan et al., 2016). The basic setting repeats the following: receive xt, predict ˆyt, receive yt from a worst-case oracle, and learn. Final performance is evaluated using the regret, which colloquially translates to how much worse the online learning algorithm performs in comparison to the best ﬁxed model in hindsight. In contrast, our setting never reveals any yt during testing even for the online version, so we do not need to invoke the concept of the worst-case oracle or the regret. Also, due to the lack of feedback from the envi- ronment after predicting, our algorithm is motivated to learn (with self-supervision) before predicting ˆyt instead of after. Note that some of the previously covered papers (Hoffman et al., 2014; Jain & Learned-Miller, 2011; Mullapudi et al., 2018) use the term “online learning” outside of the learning theory setting, so the term can be overloaded. 6. Discussion The idea of test-time training also makes sense for other tasks, such as segmentation and detection, and in other ﬁelds, such as speech recognition and natural language process- ing. For machine learning practitioners with prior domain knowledge in their respective ﬁelds, their expertise can be leveraged to design better special-purpose self-supervised tasks for test-time training. Researchers for general-purpose self-supervised tasks can also use test-time training as an evaluation benchmark, in addition to the currently prevalent benchmark of pre-training and ﬁne-tuning. More generally, we hope this paper can encourage re- searchers to abandon the self-imposed constraint of a ﬁxed decision boundary for testing, or even the artiﬁcial division between training and testing altogether. Our work is but a small step toward a new paradigm where much of the learning happens after a model is deployed. Acknowledgements. This work is supported by NSF grant 1764033, DARPA and Berkeley DeepDrive. This paper took a long time to develop, and beneﬁted from con- versations with many of our colleagues, including Ben Recht and his students Ludwig Schmidt, Vaishaal Shanker and Becca Roelofs; Ravi Teja Mullapudi, Achal Dave and Deva Ramanan; and Armin Askari, Allan Jabri, Ashish Kumar, Angjoo Kanazawa and Jitendra Malik.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts References Asano, Y . M., Rupprecht, C., and Vedaldi, A. Surprising effectiveness of few-image unsupervised feature learning. arXiv preprint arXiv:1904.13132, 2019. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. Balaji, Y ., Sankaranarayanan, S., and Chellappa, R. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems, pp. 998–1008, 2018. Bau, D., Strobelt, H., Peebles, W., Wulff, J., Zhou, B., Zhu, J.-Y ., and Torralba, A. Semantic photo manipulation with a generative image prior. ACM Transactions on Graphics (TOG), 38(4):59, 2019. Bojanowski, P. and Joulin, A. Unsupervised learning by predicting noise. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 517– 526. JMLR. org, 2017. Carlucci, F. M., D’Innocente, A., Bucci, S., Caputo, B., and Tommasi, T. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 2229–2238, 2019. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chen, M., Weinberger, K. Q., and Blitzer, J. Co-training for domain adaptation. In Advances in neural information processing systems, pp. 2456–2464, 2011. Chen, X., Sun, Y ., Athiwaratkun, B., Cardie, C., and Wein- berger, K. Adversarial deep averaging networks for cross- lingual sentiment classiﬁcation. Transactions of the Asso- ciation for Computational Linguistics, 6:557–570, 2018. Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918, 2019. Croce, F., Andriushchenko, M., and Hein, M. Provable robustness of relu networks via maximization of linear regions. arXiv preprint arXiv:1810.07481, 2018. Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017. Ding, G. W., Wang, L., and Jin, X. AdverTorch v0.1: An adversarial robustness toolbox based on pytorch. arXiv preprint arXiv:1902.07623, 2019. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594–611, 2006. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Proceed- ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1126–1135. JMLR. org, 2017. Gammerman, A., V ovk, V ., and Vapnik, V . Learning by transduction. In Proceedings of the Fourteenth conference on Uncertainty in artiﬁcial intelligence , pp. 148–155. Morgan Kaufmann Publishers Inc., 1998. Gan, C., Yang, T., and Gong, B. Learning attributes equals multi-source domain generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 87–97, 2016. Ganin, Y ., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V . Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016. Geirhos, R., Temme, C. R., Rauber, J., Sch¨utt, H. H., Bethge, M., and Wichmann, F. A. Generalisation in humans and deep neural networks. In Advances in Neural Information Processing Systems, pp. 7538–7550, 2018. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., and Balduzzi, D. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pp. 2551– 2559, 2015. Gidaris, S. and Komodakis, N. Dynamic few-shot visual learning without forgetting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4367–4375, 2018. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, B., Shi, Y ., Sha, F., and Grauman, K. Geodesic ﬂow kernel for unsupervised domain adaptation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2066–2073. IEEE, 2012.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guo, C., Rana, M., Cisse, M., and van der Maaten, L. Coun- tering adversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017. Hazan, E. et al. Introduction to online convex optimization. Foundations and Trends® in Optimization, 2(3-4):157– 325, 2016. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016a. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630–645. Springer, 2016b. He, K., Girshick, R., and Doll ´ar, P. Rethinking imagenet pre-training. arXiv preprint arXiv:1811.08883, 2018. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. Hendrycks, D. and Gimpel, K. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016. Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K. Using trusted data to train deep networks on labels cor- rupted by severe noise. InAdvances in neural information processing systems, pp. 10456–10465, 2018. Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training can improve model robustness and uncertainty. arXiv preprint arXiv:1901.09960, 2019a. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Improving model robustness and uncertainty estimates with self-supervised learning. arXiv preprint, 2019b. Hoffman, J., Kulis, B., Darrell, T., and Saenko, K. Discover- ing latent domains for multisource domain adaptation. In European Conference on Computer Vision, pp. 702–715. Springer, 2012. Hoffman, J., Darrell, T., and Saenko, K. Continuous man- ifold based adaptation for evolving visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 867–874, 2014. Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y ., Isola, P., Saenko, K., Efros, A. A., and Darrell, T. Cycada: Cycle- consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213, 2017. Hoffman, J., Mohri, M., and Zhang, N. Algorithms and theory for multiple-source adaptation. In Advances in Neural Information Processing Systems, pp. 8246–8256, 2018. Huang, G., Sun, Y ., Liu, Z., Sedra, D., and Weinberger, K. Q. Deep networks with stochastic depth. In European conference on computer vision, pp. 646–661. Springer, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jain, V . and Learned-Miller, E. Online domain adaptation of a pre-trained cascade of classiﬁers. In CVPR 2011, pp. 577–584. IEEE, 2011. Kalal, Z., Mikolajczyk, K., and Matas, J. Tracking-learning- detection. IEEE transactions on pattern analysis and machine intelligence, 34(7):1409–1422, 2011. Kang, D., Sun, Y ., Brown, T., Hendrycks, D., and Steinhardt, J. Transfer of adversarial robustness between perturbation types. arXiv preprint arXiv:1905.01034, 2019. Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint arXiv:1803.06373, 2018. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. Larsson, G., Maire, M., and Shakhnarovich, G. Colorization as a proxy task for visual understanding. In CVPR, 2017. Li, D., Yang, Y ., Song, Y .-Z., and Hospedales, T. M. Deeper, broader and artier domain generalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 5542–5550, 2017a. Li, D., Zhang, J., Yang, Y ., Liu, C., Song, Y .-Z., and Hospedales, T. M. Episodic training for domain gen- eralization. arXiv preprint arXiv:1902.00113, 2019. Li, Y ., Tian, X., Gong, M., Liu, Y ., Liu, T., Zhang, K., and Tao, D. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 624–639, 2018.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Li, Z. and Hoiem, D. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Li, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017b. Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T. Re- thinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. Long, M., Cao, Y ., Wang, J., and Jordan, M. I. Learn- ing transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791, 2015. Long, M., Zhu, H., Wang, J., and Jordan, M. I. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pp. 136–144, 2016. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pp. 6467–6476, 2017. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017. Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Uniﬁed deep supervised domain adaptation and gen- eralization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5715–5725, 2017. Muandet, K., Balduzzi, D., and Sch ¨olkopf, B. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pp. 10– 18, 2013. Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D., and Fatahalian, K. Online model distillation for efﬁcient video inference. arXiv preprint arXiv:1812.02699, 2018. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pp. 69–84. Springer, 2016. Raghunathan, A., Steinhardt, J., and Liang, P. Certiﬁed defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. IEEE transactions on pattern analysis and machine intelligence, 2016. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V . Do cifar-10 classiﬁers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classiﬁers. arXiv preprint arXiv:1906.04584, 2019. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In International conference on machine learning, pp. 1842–1850, 2016. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Shankar, S., Piratla, V ., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi, S. Generalizing across domains via cross-gradient training. arXiv preprint arXiv:1804.10745, 2018. Shankar, V ., Dave, A., Roelofs, R., Ramanan, D., Recht, B., and Schmidt, L. Do image classiﬁers generalize across time? arXiv, 2019. Shocher, A., Cohen, N., and Irani, M. zero-shot super- resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3118–3126, 2018. Sinha, A., Namkoong, H., and Duchi, J. Certifying some dis- tributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017. Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pp. 4077–4087, 2017. Song, Y ., Kim, T., Nowozin, S., Ermon, S., and Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. arXiv preprint arXiv:1710.10766, 2017. Su, J.-C., Maji, S., and Hariharan, B. Boosting supervi- sion with self-supervision for few-shot learning. arXiv preprint arXiv:1906.07079, 2019. Sun, Y ., Tzeng, E., Darrell, T., and Efros, A. A. Unsuper- vised domain adaptation through self-supervision. arXiv preprint, 2019.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Tripuraneni, N. and Mackey, L. Debiasing linear prediction. arXiv preprint arXiv:1908.02341, 2019. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167–7176, 2017. Vapnik, V .The nature of statistical learning theory. Springer science & business media, 2013. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016. Viola, P., Jones, M., et al. Rapid object detection using a boosted cascade of simple features. CVPR (1), 1(511- 518):3, 2001. Wong, E. and Kolter, J. Z. Provable defenses against adver- sarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851, 2017. Zhang, H., Yu, Y ., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jor- dan, M. I. Theoretically principled trade-off between ro- bustness and accuracy. arXiv preprint arXiv:1901.08573, 2019. Zhang, R., Isola, P., and Efros, A. A. Colorful image col- orization. In European conference on computer vision, pp. 649–666. Springer, 2016. Zhu, W., Huang, Y ., Vannan, M. A., Liu, S., Xu, D., Fan, W., Qian, Z., and Xie, X. Neural multi-scale self-supervised registration for echocardiogram dense tracking. arXiv preprint arXiv:1906.07357, 2019.Appendix: Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A1. Informal Discussion on Our Variable Decision Boundary In the introduction, we claim that in traditional supervised learning θgives a ﬁxed decision boundary, while ourθgives a variable decision boundary. Here we informally discuss this claim. Denote the input space Xand output space Y. A decision boundary is simply a mapping f : X →Y. Let Θ be a model class e.g Rd. Now consider a family of parametrized functions gθ : X→Y , where θ∈Θ. In the context of deep learning, gis the neural network architecture and θcontains the parameters. We say that f is a ﬁxed decision boundary w.r.t. g and Θ if there exists θ ∈Θ s.t. f(x) = gθ(x) for every x ∈X , and a variable decision boundary if for every x∈X, there exists θ∈Θ s.t. f(x) = gθ(x). Note how selection of θcan depend on xfor a variable decision boundary, and cannot for a ﬁxed one. It is then trivial to verify that our claim is true under those deﬁnitions. A critical reader might say that with an arbitrarily large model class, can’t every decision boundary be ﬁxed? Yes, but this is not the end of the story. Let d = dim( X) × dim(Y), and consider the enormous model class Θ′= Rd which is capable of representing all possible mappings be- tween Xand Y. Let g′ θ′ simply be the mapping represented by θ′ ∈Θ′. A variable decision boundary w.r.t. g and Θ then indeed must be a ﬁxed decision boundary w.r.t. g′and Θ′, but we would like to note two things. First, without any prior knowledge, generalization in Θ′is impossible with any ﬁnite amount of training data; reasoning about g′and Θ′is most likely not productive from an algorithmic point of view, and the concept of a variable decision boundary is to avoid such reasoning. Second, selecting θbased on xfor a variable decision boundary can be thought of as “training” on all points x ∈Rd; however, “training” only happens when necessary, for the xthat it actually encounters. Altogether, the concept of a variable decision boundary is different from what can be described by traditional learning theory. A formal discussion is beyond the scope of this paper and might be of interest to future work. A2. Computational Aspects of Our Method At test time, our method is 2 × batch size × number of iterations times slower than regular test- ing, which only performs a single forward pass for each sample. As the ﬁrst work on Test-Time Training, this paper is not as concerned about computational efﬁciency as improving robustness, but here we provide two poten- tial solutions that might be useful, but have not been thor- oughly veriﬁed. The ﬁrst is to use the thresholding trick on ls, introduced as a solution for the small batches prob- lem in the method section. For the models considered in our experiments, roughly 80% of the test instances fall below the threshold, so Test-Time Training can only be performed on the other 20% without much effect on per- formance, because those 20% contain most of the sam- ples with wrong predictions. The second is to reduce the number of iterations of test-time updates. For the online version, the number of iterations is al- ready 1, so there is nothing to do. For the standard ver- sion, we have done some preliminary experiments setting number of iterations to 1 (instead of 10) and learn- ing rate to 0.01 (instead of 0.001), and observing results almost as good as the standard hyper-parameter setting. A more in depth discussion on efﬁciency is left for future works, which might, during training, explicitly make the model amenable to fast updates. A3. Proofs Here we prove the theoretical results in the main paper. A3.1. The Toy Problem The following setting applies to the two lemmas; this is simply the setting of our toy problem, reproduced here for ease of reference.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Consider a two layer linear network parametrized by A∈ Rh×d (shared) and v,w ∈Rh (ﬁxed) for the two heads, respectively. Denote x∈Rd the input and y1,y2 ∈R the labels for the two tasks, respectively. For the main task loss lm(A; v) = 1 2 ( y1 −v⊤Ax )2 , (12) and the self-supervised task loss ls(A; w) = 1 2 ( y2 −w⊤Ax )2 , (13) Test-Time Training yields an updated matrix A′←A−η ( y2 −w⊤Ax )( −wx⊤) , (14) where ηis the learning rate. Lemma 1. Following the exposition of the main paper, let η∗= (y1 −v⊤Ax) (y2 −w⊤Ax)v⊤wx⊤x. (15) Assume η∗∈[ϵ,∞) for some ϵ> 0. Then for any η∈(0,ϵ], we are guaranteed an improvement on the main loss i.e. lm(A′) <lm(A). Proof. From the exposition of the main paper, we know that lm(A−η∗∇lsA)) = 0, which can also be derived from simple algebra. Then by convexity, we have lm(A−η∇ls(A)) (16) = lm (( 1 − η η∗ ) A+ η η∗(A−η∗∇ls(A)) ) (17) ≤ ( 1 − η η∗ ) lm(A) + 0 (18) ≤ ( 1 −η ϵ ) lm(A) (19) <lm(A), (20) where the last inequality uses the assumption that lm(A) > 0, which holds because η∗>0. Lemma 2. Deﬁne ⟨U,V⟩= vec (U)⊤vec (V) i.e. the Frobenious inner product, then sign (η∗) = sign (⟨∇lm(A),∇ls(A)⟩) . (21) Proof. By simple algebra, ⟨∇lm(A),∇ls(A)⟩ = ⟨ ( y1 −v⊤Ax )( −vx⊤) , ( y2 −w⊤Ax )( −wx⊤) ⟩ = ( y1 −v⊤Ax )( y2 −w⊤Ax ) Tr ( xv⊤wx⊤) = ( y1 −v⊤Ax )( y2 −w⊤Ax ) v⊤wx⊤x, which has the same sign as η∗. A3.2. Proof of Theorem 1 For any η, by smoothness and convexity, lm(x,y; θ(x)) = lm(x,y; θ−η∇ls(x; θ)) ≤lm(x,y; θ) + η⟨∇lm(x,y; θ),∇ls(x,θ)⟩ + η2β 2 ∥∇ls(x; θ)∥2 . Denote η∗= ⟨∇lm(x,y; θ),∇ls(x,θ)⟩ β∥∇ls(x; θ)∥2 . Then Equation 22 becomes lm(x,y; θ−η∗∇ls(x; θ)) (22) ≤lm(x,y; θ) −⟨∇lm(x,y; θ),∇ls(x,θ)⟩2 2β∥∇ls(x; θ)∥2 . (23) And by our assumptions on the gradient norm and gradient inner product, lm(x,y; θ) −lm(x,y; θ−η∗∇ls(x; θ)) ≥ ϵ2 2βG2 . (24) Because we cannot observe η∗in practice, we instead use a ﬁxed learning rate η = ϵ βG2 , as stated in Theorem 1. Now we argue that this ﬁxed learning rate still improves performance on the main task. By our assumptions, η∗ ≥ ϵ βG2 , so η ∈(0,η∗]. Denote g= ∇ls(x; θ), then by convexity of lm, lm(x,y; θ(x)) = lm(x,y; θ−ηg) (25) = lm ( x,y; ( 1 − η η∗ ) θ+ η η∗(θ−η∗g) ) (26) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗lm(x,y; θ−η∗g) (27) Combining with Equation 24, we have lm(x,y; θ(x)) ≤ ( 1 − η η∗ ) lm(x,y; θ) + η η∗ ( lm(x,y; θ) − ϵ2 2βG2 ) = lm(x,y; θ) − η η∗ ϵ2 2βG2 Since η/η∗>0, we have shown that lm(x,y; θ) −lm(x,y; θ(x)) >0. (28)Test-Time Training with Self-Supervision for Generalization under Distribution Shifts A4. Additional Results on the Common Corruptions Dataset For table aethetics, we use the following abbreviations: B for baseline, JT for joint training, TTT for Test-Time Train- ing standard version, and TTT-Online for online Test-Time Training i.e. the online version. We have abbreviated the names of the corruptions, in order: original test set, Gaussian noise, shot noise, impulse noise, defocus blur, glass blue, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic transformation, pixelation, and JPEG compression. A4.1. Results Using Batch Normalization As discussed in the results section, Batch Normalization (BN) is ineffective for small batches, which are the inputs for Test-Time Training (both standard and online version) since there is only one sample available when forming each batch; therefore, our main results are based on a ResNet using Group Normalization (GN). Figure A2 and Table A1 show results of our method on CIFAR-10-C level 5, with a ResNet using Batch Normalization (BN). These results are only meant to be a point of reference for the curious readers. In the early stage of this project, we have experimented with two potential solutions to the small batches problem with BN. The naive solution is to ﬁx the BN layers during Test-Time Training. but this diminishes the performance gains since there are fewer shared parameters. The better solution, adopted for the results below, is hard example mining: instead of updating on all inputs, we only update on inputs that incur large self-supervised task loss ls, where the large improvements might counter the negative effects of inaccurate statistics. Test-Time Training (standard version) is still very effective with BN. In fact, some of the improvements are quite dra- matic, such as on contrast (34%), defocus blue (18%) and Gaussian noise (22% comparing to joint-training, and 16% comparing to the baseline). Performance on the original distribution is still almost the same, and the original error with BN is in fact slightly lower than with GN, and takes half as many epochs to converge. We did not further experiment with BN because of two rea- sons: 1) The online version does not work with BN, because the problem with inaccurate batch statistics is exacerbated when training online for many (e.g. 10000) steps. 2) The baseline error for almost every corruption type is signiﬁ- cantly higher with BN than with GN. Although unrelated to the main idea of our paper, we make the interesting note that GN signiﬁcantly improves model robustness. A4.2. Additional Baseline: Adversarial Logit Pairing As discussed in the results section, Hendrycks & Dietterich (2019) point to Adversarial Logit Pairing (ALP) (Kannan et al., 2018) as an effective method for improving model robustness to corruptions and perturbations, even though it was designed to defend against adversarial attacks. We take ALP as an additional baseline on all benchmarks based on CIFAR-10 (using GN), following the training proce- dure in Kannan et al. (2018) and their recommended hyper- parameters. The implementation of the adversarial attack comes from the codebase of Ding et al. (2019). We did not run ALP on ImageNet because the two papers we reference for this method, Kannan et al. (2018) and Hendrycks & Di- etterich (2019), did not run on ImageNet or make any claim or recommendation. A4.3. Results on CIFAR-10-C and ImageNet-C, Level 5 Table A2 and Table A3 correspond to the bar plots in the results section. Two rows of Table A2 have been presented as Table 1 in the main text. A4.4. Results on CIFAR-10-C, Levels 1-4 The following bar plots and tables are on levels 1-4 of CIFAR-10-C. The original distribution is the same for all levels, so are our results on the original distribution. A4.5. Direct Comparison with Hendrycks et al. (2019a) The following comparison has been requested by an anony- mous reviewer for our ﬁnal version. Our joint training baseline is based on Hendrycks et al. (2019a), but also incor- porates some architectural changes (see below). We found these changes improved the robustness of our method, and felt that it was important to give the baseline the same ben- eﬁt. Note that our joint training baseline overall performs better than Hendrycks: Compare Table S2 to Figure 3 of Hendrycks et al. (2019a) (provided by the authors), our baseline has average error of 22.8% across all corruptions and levels, while their average error is 28.6%. Summary of architectural changes: 1) Group Normalization (GN) instead of Batch Normalization (BN). For complete- ness, the results with BN are provided in Table S1; c.f. GN results in Table S2 which signiﬁcantly improves robustness, with or without self-supervision. 2) We split after the sec- ond residual group, while they split after the third residual group right before the linear layer. This consistently gives about 0.5% - 1% improvement. 3) We use a ResNet-26, while they use a 40-2 Wide ResNet. But our baseline still performs better than their method even though our network is 4x smaller, due to the two tricks above.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Gaussian Noise  Shot Noise  Impulse Noise  Defocus Blur  Frosted Glass Blur Motion Blur  Zoom Blur  Snow  Frost  Fog Brightness  Contrast  Elastic  Pixelate  JPEG Figure A1.Sample images from the Common Corruptions Benchmark, taken from the original paper by Hendrycks & Dietterich (2019). originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 20 40 60Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT Figure A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 7.9 63.9 58.8 64.3 46.3 54.6 41.6 45.9 31.9 44.0 37.5 13.0 69.2 33.8 61.4 31.7 JT 7.5 70.7 65.6 67.2 43.1 55.4 40.9 42.7 30.3 44.5 42.5 12.7 58.6 30.7 62.6 31.9 TTT 7.9 47.9 45.2 54.8 27.6 50.4 31.5 30.9 28.7 34.3 26.9 12.6 35.2 30.6 51.2 31.3 Table A1.Test error (%) on CIFAR-10-C, level 5, ResNet-26 with Batch Normalization. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 50.5 47.2 56.1 23.7 51.7 24.3 26.3 25.6 34.4 28.1 13.5 25.0 27.4 55.8 29.8 JT 8.1 49.4 45.3 53.4 24.2 48.5 24.8 26.4 25.0 32.5 27.5 12.6 25.3 24.0 51.6 28.7 TTT 7.9 45.6 41.8 50.0 21.8 46.1 23.0 23.9 23.9 30.0 25.1 12.2 23.9 22.6 47.2 27.2 TTT-Online 8.2 25.8 22.6 30.6 14.6 34.4 18.3 17.1 20.0 18.0 16.9 11.2 15.6 21.6 18.1 21.2 UDA-SS 9.0 28.2 26.5 20.8 15.6 43.7 24.5 23.8 25.0 24.9 17.2 12.7 11.6 22.1 20.3 22.6 ALP 16.5 22.7 22.9 28.3 25.0 25.6 27.4 23.1 25.2 27.2 64.8 21.7 73.6 23.0 20.2 18.9 Table A2.Test error (%) on CIFAR-10-C, level 5, ResNet-26. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 68.9 1.3 2.0 1.3 7.5 6.6 11.8 16.2 15.7 14.9 15.3 43.9 9.7 16.5 15.3 23.4 JT 69.1 2.1 3.1 2.1 8.7 6.7 12.3 16.0 15.3 15.8 17.0 45.3 11.0 18.4 19.7 22.9 TTT 69.0 3.1 4.5 3.5 10.1 6.8 13.5 18.5 17.1 17.9 20.0 47.0 14.4 20.9 22.8 25.3 TTT-Online 68.8 26.3 28.6 26.9 23.7 6.6 28.7 33.4 35.6 18.7 47.6 58.3 35.3 44.3 47.8 44.3 Table A3.Test accuracy (%) on ImageNet-C, level 5, ResNet-18.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40 50Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A3.Test error (%) on CIFAR-10-C, level 4. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 46.4 39.2 44.8 15.3 52.5 19.1 20.5 21.3 26.9 13.3 10.5 13.7 20.8 35.3 26.9 JT 8.1 45.0 38.3 42.2 16.4 50.2 20.7 20.5 21.1 25.4 14.1 10.0 14.7 19.0 33.2 25.1 TTT 7.9 41.5 35.4 39.8 15.0 47.8 19.1 18.4 20.1 24.0 13.5 10.0 14.1 17.7 29.4 24.5 TTT-Online 8.2 22.9 20.0 23.9 11.2 35.1 15.6 13.8 18.6 15.9 12.3 9.7 11.9 16.7 13.6 19.8 ALP 16.5 21.3 20.5 24.5 20.7 25.9 23.7 21.4 24.2 23.9 42.2 17.5 53.7 22.1 19.1 18.5 Table A4.Test error (%) on CIFAR-10-C, level 4, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A4.Test error (%) on CIFAR-10-C, level 3. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 42.2 35.1 30.7 12.2 41.7 18.6 17.5 19.0 25.3 10.8 9.7 11.6 15.3 21.7 24.6 JT 8.1 40.2 34.4 29.9 12.2 37.9 20.8 17.3 18.4 25.0 11.4 9.2 12.0 15.2 20.8 22.8 TTT 7.9 37.2 31.6 28.6 11.5 35.8 19.1 15.8 17.8 23.3 11.0 9.1 11.6 14.3 18.9 22.3 TTT-Online 8.2 21.3 17.7 17.9 9.0 23.4 15.3 12.5 16.4 15.8 10.9 9.0 10.7 12.8 12.2 18.7 ALP 16.5 20.0 19.3 20.5 19.2 21.2 24.0 20.5 20.9 24.2 30.1 16.6 39.6 20.9 17.8 18.0 Table A5.Test error (%) on CIFAR-10-C, level 3, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A5.Test error (%) on CIFAR-10-C, level 2. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 31.7 22.6 24.3 9.9 42.6 14.9 14.7 21.7 18.4 9.8 9.1 10.0 13.1 17.1 22.4 JT 8.1 31.0 22.6 23.4 9.1 39.2 16.4 14.2 21.2 17.5 9.4 8.3 10.6 12.8 15.9 20.5 TTT 7.9 28.8 20.7 23.0 9.0 36.6 15.4 13.1 20.2 16.9 9.2 8.3 10.2 12.5 14.8 19.7 TTT-Online 8.2 16.8 13.8 15.5 8.5 23.4 13.3 11.5 16.8 12.7 9.4 8.4 9.7 12.4 11.5 17.0 ALP 16.5 18.0 17.2 19.0 17.8 20.7 21.2 19.3 19.0 20.1 22.4 16.3 29.2 20.3 17.4 17.8 Table A6.Test error (%) on CIFAR-10-C, level 2, ResNet-26. originalgauss shot impulsedefocus glass motion zoom snow frost fog bright contrastelasticpixelate jpeg 0 10 20 30 40Error (%) Object recognition task only Joint training (Hendrycks et al. 2019) TTT TTT-Online Figure A6.Test error (%) on CIFAR-10-C, level 1. See the results section for details. orig gauss shot impul defoc glass motn zoom snow frost fog brit contr elast pixel jpeg B 8.9 21.7 17.1 17.0 9.0 44.0 12.1 13.9 14.3 13.4 9.2 8.9 9.0 13.2 12.0 17.3 JT 8.1 20.4 16.6 16.9 8.2 40.5 12.2 13.0 13.1 12.3 8.4 8.1 8.5 12.9 11.3 15.9 TTT 7.9 19.1 15.8 16.5 8.0 37.9 11.7 12.2 12.8 11.9 8.2 8.0 8.3 12.6 11.1 15.5 TTT-Online 8.2 13.8 11.9 12.2 8.5 24.4 10.5 11.5 12.4 10.7 8.5 8.3 8.6 12.4 10.7 14.4 ALP 17.0 16.8 17.6 16.8 20.9 18.7 19.0 17.3 17.5 17.4 16.1 18.4 20.4 17.0 17.2 17.5 Table A7.Test error (%) on CIFAR-10-C, level 1, ResNet-26.Test-Time Training with Self-Supervision for Generalization under Distribution Shifts Figure A7.Sample Images from the VID-Robust dataset (Shankar et al., 2019) in the results section adapted to CIFAR-10. Each row shows eight sample images from one class. The seven classes shown are, in order: airplane, bird, car, dog, cat, horse, ship.",
      "meta_data": {
        "arxiv_id": "1909.13231v3",
        "authors": [
          "Yu Sun",
          "Xiaolong Wang",
          "Zhuang Liu",
          "John Miller",
          "Alexei A. Efros",
          "Moritz Hardt"
        ],
        "published_date": "2019-09-29T08:09:15Z",
        "pdf_url": "https://arxiv.org/pdf/1909.13231v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Test-Time Training (TTT), a novel approach to enhance the generalization of predictive models under distribution shifts by converting a single unlabeled test sample into a self-supervised learning problem to update model parameters before prediction. Key contributions include demonstrating substantial performance improvements on diverse image classification benchmarks (e.g., corrupted images, video frames, unknown shifts) without degrading performance on original data distributions, and showing its effectiveness in an online streaming setting. A theoretical investigation establishes that positive gradient correlation between the main and self-supervised tasks is a sufficient condition for TTT's success in convex models, which is empirically validated for deep learning.",
        "methodology": "Test-Time Training (TTT) utilizes a Y-structured neural network with a shared feature extractor (θe) and separate branches for a main classification task (θm) and a self-supervised auxiliary task (θs). The model is initially trained jointly on both tasks using multi-task learning. At test time, for an unlabeled test sample (x), the shared feature extractor θe is fine-tuned by minimizing the self-supervised loss ls(x; θs,θe). The primary self-supervised task used is rotation prediction (classifying 0, 90, 180, or 270-degree image rotations). Data augmentation is applied to the single test sample during fine-tuning. For online Test-Time Training, parameters are initialized with those updated from the previous test sample, and only one gradient step is taken per new image. Group Normalization (GN) is employed instead of Batch Normalization (BN) to handle the small batch size during test-time updates.",
        "experimental_setup": "Experiments were conducted using ResNet architectures (26-layer for CIFAR-10, 18-layer for ImageNet). The datasets included CIFAR-10-C and ImageNet-C (benchmarking robustness to 15 corruption types at 5 severity levels), VID-Robust (video frames for object recognition, adapted for ImageNet and CIFAR-10 models), and CIFAR-10.1 (a new test set with unknown distribution shifts). Optimization utilized Stochastic Gradient Descent (SGD) with a learning rate of 0.001 for test-time updates (10 steps for standard TTT, 1 step for online TTT). Baselines included a plain ResNet (\"object recognition task only\"), an improved joint training model (fixed at test time), Unsupervised Domain Adaptation by Self-Supervision (UDA-SS), and Adversarial Logit Pairing (ALP). Performance was evaluated using test error/accuracy, and theoretical insights were empirically validated by analyzing gradient correlation.",
        "limitations": "The method incurs a significant computational overhead at test time, being \"2 × batch size × number of iterations times slower than regular testing.\" Its effectiveness is contingent on the self-supervised task being \"well defined and non-trivial\"; for example, trivial rotation cues (like black margins on airplane images) can hinder performance improvement. While theoretical results apply to convex models, their extension to non-convex deep learning relies primarily on empirical validation. The online version of TTT assumes \"gradually changing distribution shifts,\" implying test samples originate from smoothly evolving distributions. The concept of a variable decision boundary, central to TTT, is acknowledged to deviate from traditional learning theory, with a formal discussion deemed beyond the paper's scope.",
        "future_research_directions": "Future work could involve extending Test-Time Training to other tasks such as segmentation and detection, and applying it in different domains like speech recognition and natural language processing. Leveraging domain-specific knowledge to design more effective special-purpose self-supervised tasks is suggested. The authors also propose using TTT as a new evaluation benchmark for general-purpose self-supervised learning methods. Further research is needed to improve computational efficiency, including exploring faster update mechanisms during training, and more in-depth analysis of strategies like thresholding or reducing update steps. A more formal theoretical discussion on the concept of a variable decision boundary is also highlighted. The paper generally encourages abandoning the self-imposed constraint of fixed decision boundaries and the artificial division between training and testing, moving towards a paradigm where learning extensively occurs post-deployment."
      }
    },
    {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution\nshifts between training and testing data by adapting a given model on test\nsamples. However, the online model updating of TTA may be unstable and this is\noften a key obstacle preventing existing TTA methods from being deployed in the\nreal world. Specifically, TTA may fail to improve or even harm the model\nperformance when test data have: 1) mixed distribution shifts, 2) small batch\nsizes, and 3) online imbalanced label distribution shifts, which are quite\ncommon in practice. In this paper, we investigate the unstable reasons and find\nthat the batch norm layer is a crucial factor hindering TTA stability.\nConversely, TTA can perform more stably with batch-agnostic norm layers, \\ie,\ngroup or layer norm. However, we observe that TTA with group and layer norms\ndoes not always succeed and still suffers many failure cases. By digging into\nthe failure cases, we find that certain noisy test samples with large gradients\nmay disturb the model adaption and result in collapsed trivial solutions, \\ie,\nassigning the same class label for all samples. To address the above collapse\nissue, we propose a sharpness-aware and reliable entropy minimization method,\ncalled SAR, for further stabilizing TTA from two aspects: 1) remove partial\nnoisy samples with large gradients, 2) encourage model weights to go to a flat\nminimum so that the model is robust to the remaining noisy samples. Promising\nresults demonstrate that SAR performs more stably over prior methods and is\ncomputationally efficient under the above wild test scenarios.",
      "meta_data": {
        "arxiv_id": "2302.12400v1",
        "authors": [
          "Shuaicheng Niu",
          "Jiaxiang Wu",
          "Yifan Zhang",
          "Zhiquan Wen",
          "Yaofo Chen",
          "Peilin Zhao",
          "Mingkui Tan"
        ],
        "published_date": "2023-02-24T02:03:41Z",
        "pdf_url": "https://arxiv.org/pdf/2302.12400v1.pdf"
      }
    },
    {
      "title": "Active Test-Time Adaptation: Theoretical Analyses and An Algorithm",
      "abstract": "Test-time adaptation (TTA) addresses distribution shifts for streaming test\ndata in unsupervised settings. Currently, most TTA methods can only deal with\nminor shifts and rely heavily on heuristic and empirical studies.\n  To advance TTA under domain shifts, we propose the novel problem setting of\nactive test-time adaptation (ATTA) that integrates active learning within the\nfully TTA setting.\n  We provide a learning theory analysis, demonstrating that incorporating\nlimited labeled test instances enhances overall performances across test\ndomains with a theoretical guarantee. We also present a sample entropy\nbalancing for implementing ATTA while avoiding catastrophic forgetting (CF). We\nintroduce a simple yet effective ATTA algorithm, known as SimATTA, using\nreal-time sample selection techniques. Extensive experimental results confirm\nconsistency with our theoretical analyses and show that the proposed ATTA\nmethod yields substantial performance improvements over TTA methods while\nmaintaining efficiency and shares similar effectiveness to the more demanding\nactive domain adaptation (ADA) methods. Our code is available at\nhttps://github.com/divelab/ATTA",
      "full_text": "Published as a conference paper at ICLR 2024 ACTIVE TEST-TIME ADAPTATION : T HEORETICAL ANALYSES AND AN ALGORITHM Shurui Gui∗ Texas A&M University College Station, TX 77843 shurui.gui@tamu.edu Xiner Li* Texas A&M University College Station, TX 77843 lxe@tamu.edu Shuiwang Ji Texas A&M University College Station, TX 77843 sji@tamu.edu ABSTRACT Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques. Extensive experimental results confirm consistency with our theoretical analyses and show that the proposed ATTA method yields substantial performance improvements over TTA methods while maintaining efficiency and shares similar effectiveness to the more demanding active domain adaptation (ADA) methods. Our code is available at https://github.com/divelab/ATTA. 1 I NTRODUCTION Deep learning has achieved remarkable success across various fields, attaining high accuracy in numerous applications (Krizhevsky et al., 2017; Simonyan and Zisserman, 2014). Nonetheless, When training and test data follow distinct distributions, models often experience significant performance degradation during test. This phenomenon, known as the distribution shift or out-of-distribution (OOD) problem, is extensively studied within the context of both domain generalization (DG) (Gulra- jani and Lopez-Paz, 2020; Koh et al., 2021; Gui et al., 2022) and domain adaptation (DA) (Ganin et al., 2016; Sun and Saenko, 2016). While these studies involve intensive training of models with considerable generalization abilities towards target domains, they overlook an important application property; namely, continuous adaptivity to real-time streaming data under privacy, resource, and efficiency constraints. This gap leads to the emergence of test-time adaptation (TTA) tasks, targeting on-the-fly adaptation to continuous new domains during the test phase or application deployment. The study of TTA encompasses two main categories; namely test-time training (TTT) methods (Sun et al., 2020; Liu et al., 2021c) and fully test-time adaptation (FTTA) (Niu et al., 2023; Wang et al., 2021). The TTT pipeline incorporates retraining on the source data, whereas FTTA methods adapt arbitrary pre-trained models to the given test mini-batch by conducting entropy minimization, without access to the source data. Nevertheless, most TTA methods can only handle corrupted distribution shifts (Hendrycks and Dietterich, 2019b) (e.g., Gaussian noise,) and rely heavily on human intuition or empirical studies. To bridge this gap, our paper focuses on tackling significant domain distribution shifts in real time with theoretical insights. We investigate FTTA, which is more general and adaptable than TTT, particularly under data ac- cessibility, privacy, and efficiency constraints. Traditional FTTA aims at adapting a pre-trained model to streaming test-time data from diverse domains under unsupervised settings. However, recent works (Lin et al., 2022; Pearl, 2009) prove that it is theoretically infeasible to achieve OOD generalization without extra information such as environment partitions. Since utilizing environment partitions requires heavy pretraining, contradicting the nature of TTA, we are motivated to incorporate extra information in a different way,i.e., integrating a limited number of labeled test-time samples to alleviate distribution shifts, following the active learning (AL) paradigm (Settles, 2009). To this end, we propose the novel problem setting of active test-time adaptation (ATTA) by incorporating ∗Equal contributions 1 arXiv:2404.05094v1  [cs.LG]  7 Apr 2024Published as a conference paper at ICLR 2024 AL within FTTA. ATTA faces two major challenges; namely, catastrophic forgetting (CF) (Kemker et al., 2018; Li and Hoiem, 2017) and real-time active sample selection. CF problem arises when a model continually trained on a sequence of domains experiences a significant performance drop on previously learned domains, due to the inaccessibility of the source data and previous test data. Real-time active sample selection requires AL algorithms to select informative samples from a small buffer of streaming test data for annotation, without a complete view of the test distribution. In this paper, we first formally define the ATTA setting. We then provide its foundational analysis under the learning theory’s paradigm to guarantee the mitigation of distribution shifts and avoid CF. Aligned with our empirical validations, while the widely used entropy minimization (Wang et al., 2021; Grandvalet and Bengio, 2004) can cause CF, it can conversely become the key to preventing CF problems with our sample selection and balancing techniques. Building on the analyses, we then introduce a simple yet effective ATTA algorithm, SimATTA, incorporating balanced sample selections and incremental clustering. Finally, we conducted a comprehensive experimental study to evaluate the proposed ATTA settings with three different settings in the order of low to high requirement restrictiveness, i.e., TTA, Enhanced TTA, and Active Domain Adaptation (ADA). Intensive experiments indicate that ATTA jointly equips with the efficiency of TTA and the effectiveness of ADA, rendering an uncompromising real-time distribution adaptation direction. Comparison to related studies. Compared to TTA methods, ATTA requires extra active labels, but the failure of TTA methods (Sec. 5.1) and the theoretical proof of Lin et al. (2022); Pearl (2009) justify its necessity and rationality. Compared to active online learning, ATTA focuses on lightweight real-time fine-tuning without round-wise re-trainings as Saran et al. (2023) and emphasizes the importance of CF avoidance instead of resetting models and losing learned distributions. In fact, active online learning is partially similar to our enhanced TTA setting (Sec. 5.2. Compared to ADA methods (Prabhu et al., 2021; Ning et al., 2021), ATTA does not presuppose access to source data, model parameters, or pre-collected target samples. Furthermore, without this information, ATTA can still perform on par with ADA methods (Sec. 5.3). The recent source-free active domain adaptation (SFADA) method SALAD (Kothandaraman et al., 2023) still requires access to model parameter gradients, pre-collected target data, and training of additional networks. Our ATTA, in contrast, with non-regrettable active sample selection on streaming data, is a much lighter and more realistic approach distinct from ADA and SFADA. More related-work discussions are provided in Appx. C. 2 T HE ACTIVE TEST-TIME ADAPTATION FORMULATION TTA methods aim to solve distribution shifts by dynamically optimizing a pre-trained model based on streaming test data. We introduce the novel problem setting of Active Test-Time Adaptation (ATTA), which incorporates active learning during the test phase. In ATTA, the model continuously selects the most informative instances from the test batch to be labeled by an explicit or implicit oracle (e.g., human annotations, self-supervised signals) and subsequently learned by the model, aiming to improve future adaptations. Considering the labeling costs in real-world applications, a “budget” is established for labeled test instances. The model must effectively manage this budget distribution and ensure that the total number of label requests throughout the test phase does not surpass the budget. We now present a formal definition of the ATTA problem. Consider a pre-trained modelf(x; ϕ) with parameters ϕ trained on the source dataset DS = (x, y)|DS|, with each data sample x ∈ Xand a label y ∈ Y. We aim to adapt model parameters θ, initialized as ϕ, to an unlabeled test-time data stream. The streaming test data exhibit distribution shifts from the source data and varies continuously with time, forming multiple domains to which we must continuously adapt. The test phase commences at time step t = 1 and the streaming test data is formulated in batches. The samples are then actively selected, labeled (by the oracle) and collected as Dte(t) = ActAlg(Ute(t)), where ActAlg(·) denotes an active selection/labeling algorithm. The labeled samples Dte(t) are subsequently incorporated into the ATTA training setDtr(t). Finally, we conclude time step t by performing ATTA training, updating model parameters θ(t) using Dtr(t), with θ(t) initialized as the previous final state θ(t − 1). Definition 1 (The ATTA problem). Given a model f(x; θ), with parameters θ, initialized with parameters θ(0) = ϕ obtained by pre-training on source domain data, and streaming test data batches Ute(t) continually changing over time, the ATTA task aims to optimize the model at any time stept (with test phase commencing at t = 1) as θ(t)∗ := argmin θ(t) (E(x,y,t)∈Dtr(t)[ℓCE (f(x; θ(t)), y)] + E(x,t)∈Ute(t)[ℓU (f(x; θ(t)))]), (1) 2Published as a conference paper at ICLR 2024 where Dtr(t) = ( ∅, t = 0 Dtr(t − 1) ∪ Dte(t), t ≥ 1, s.t. |Dtr(t)| ≤ B, (2) Dte(t) = ActAlg(Ute(t)) is actively selected and labeled, ℓCE is the cross entropy loss, ℓU is an unsupervised learning loss, and B is the budget. 3 T HEORETICAL STUDIES In this section, we conduct an in-depth theoretical analysis of TTA based on learning theories. We mainly explore two questions: How can significant distribution shifts be effectively addressed under the TTA setting? How can we simultaneously combat the issue of CF? Sec. 3.1 provides a solution with theoretical guarantees to the first question, namely, active TTA (ATTA), along with the conditions under which distribution shifts can be well addressed. Sec. 3.2 answers the second question with an underexplored technique, i.e., selective entropy minimization, building upon the learning bounds established in Sec. 3.1. We further validate these theoretical findings through experimental analysis. Collectively, we present a theoretically supported ATTA solution that effectively tackles both distribution shift and CF. 3.1 A LLEVIATING DISTRIBUTION SHIFTS THROUGH ACTIVE TEST-TIME ADAPTATION Traditional TTA is performed in unsupervised or self-supervised context. In contrast, ATTA introduces supervision into the adaptation setting. In this subsection, we delve into learning bounds and establish generalization bounds to gauge the efficacy of ATTA in solving distribution shifts. We scrutinize the influence of active learning and evidence that the inclusion of labeled test instances markedly enhances overall performances across incremental test domains. Following Kifer et al. (2004), we examine statistical guarantees for binary classification. A hypothesis is a function h : X → {0, 1}, which can serve as the prediction function within this context. In the ATTA setting, the mapping ofh varies with time as h(x, t). We use H∆H-distance following Ben- David et al. (2010), which essentially provides a measure to quantify the distribution shift between two distributions D1 and D2, and can also be applied between datasets. The probability that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} according to distribution D is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source data is inaccessible under ATTA settings, we consider the existence of source dataset DS for accurate theoretical analysis. Thus, we initialize Dtr as Dtr(0) = DS. For every time step t, the test and training data can be expressed asUte(t) and Dtr(t) = DS ∪Dte(1) ∪Dte(2) ∪···∪ Dte(t). Building upon two lemmas (provided in Appx. D), we establish bounds on domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesish at time t. Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), ··· , Ute(t), ··· , Si are unlabeled samples of sizem sampled from each of thet+1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λ = (λ0, ··· , λt). If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w = (w0, ··· , wt) on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. The adaptation performance on a test domain is majorly bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. Further theoretical analysis are in Appx. D. 3Published as a conference paper at ICLR 2024 Figure 1: (a) Empirical validation of Thm. 1. We train a series of models on N = 2000 samples from the PACS (Li et al., 2017) dataset given differentλ0 and w0 and display the test domain loss of each model. Red points are the test loss minimums given a fixed λ0. The orange line is the reference where w0 = λ0. We observe that w0 with loss minimums are located closed to the orange line but slightly smaller than λ0, which validates our findings in Eq. (4). (b) Empirical analysis with an uncertainty balancing. Given source pre-trained models, we fine-tune the models on 500 samples with different λ0 and w0, and display the combined error surface of test and source error. Although a small λ0 is good for test domain error, it can lead to non-trivial source error exacerbation. Therefore, we can observe that the global loss minimum (green X) locates in a relatively high-λ0 region. If we consider the multiple test data distributions as a single test domain,i.e., St i=1 Ute(i), Thm. 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . Given the optimal test/source hypothesis h∗ T (t) = arg minh∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), we have |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (3b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (3.a), with approximatelyB = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (4) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. The following theorem offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (5) Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. All proofs are provided in Appx. E. Finally, we support the theoretical findings with experimental analysis and show the numerical results of applying the principles on real-world datasets, as shown in Fig. 1. For rigorous analysis, note that our theoretical results rest on the underlying condition that N should at least be of the same scale as d, according to the principles of VC-dimension theory. The empirical alignment of our experiments with the theoretical framework can be attributed to the assumption that fine-tuning a model is roughly equivalent to learning a model with a relatively small d. Experiment details and other validations can be found in Appx. H. 4Published as a conference paper at ICLR 2024 3.2 M ITIGATING CATASTROPHIC FORGETTING WITH BALANCED ENTROPY MINIMIZATION Catastrophic forgetting (CF), within the realm of Test-Time Adaptation (TTA), principally manifests as significant declines in overall performance, most notably in the source domain. Despite the lack of well-developed learning theories for analyzing training with series data, empirical studies have convincingly illustrated the crucial role of data sequential arrangement in model learning, thereby accounting for the phenomenon of CF. Traditionally, the mitigation of CF in adaptation tasks involves intricate utilization of source domain data. However, under FTTA settings, access to the source dataset is unavailable, leaving the problem of CF largely unexplored in the data-centric view. Table 1: Correlation analysis of high/low en- tropy samples and domains. We use a source pre-trained model to select samples with low- est/highest entropy, and 1.retrain the model on 2000 samples; 2.fine-tune the model on 300 sam- ples. We report losses on source/test domains for each setting, showing that low-entropy samples form distributions close to the source domain. Sample type Retrain Fine-tune ϵS ϵT ϵS ϵT Low entropy 0.5641 0.8022 0.0619 1.8838 High entropy 2.5117 0.3414 0.8539 0.7725 To overcome this challenge of source dataset ab- sence, we explore the acquisition of “source-like” data. In TTA scenarios, it is generally assumed that the amount of source data is considerably large. We also maintain this assumption in ATTA, practically assuming the volume of source data greatly surpasses the test-time budget. As a re- sult, we can safely assume that the pre-trained model is well-trained on abundant source do- main data DS. Given this adequately trained source model, we can treat it as a “true” source data labeling function f(x; ϕ). The model es- sentially describes a distribution, Dϕ,S(X, Y) = {(x, ˆy) ∈ (X, Y) | ˆy = f(x; ϕ), x∈ DS}. The entropy of the model prediction is defined as H(ˆy) = −P c p(ˆyc) logp(ˆyc), ˆy = f(x; ϕ), where c denotes the class. Lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction, which can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model recognizes the sample as being similar to those it was trained on. Thus entropy can be used as an indicator of how closely a sample x aligns with the model distribution Dϕ,S. Since the model distribution is approximately the source distribution, selecting (and labeling) low-entropy samples using f(x; ϕ) essentially provides an estimate of sampling from the source dataset. Therefore, in place of the inaccessible DS, we can feasibly include the source-like dataset into the ATTA training data at each time stept: Dϕ,S(t) = {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el}, (6) where el is the entropy threshold. The assumption that Dϕ,S(t) is an approximation of DS can be empirically validated, as shown by the numerical results on PACS in Tab. 1. In contrast, high-entropy test samples typically deviate more from the source data, from which we select Dte(t) for active labeling. Following the notations in Thm. 1, we are practically minimizing the empirical weighted error of hypothesis h(t) as ˆϵ′ w(h(t)) = tX j=0 wjˆϵj(h(t)) = w0 λ0N X x∈Dϕ,S(t) |h(x, t) − f(x; ϕ)| + tX j=1 wj λjN X x,y∈Dte(j) |h(x, t) − y|. (7) By substituting DS with Dϕ,S(t) in Thm. 1, the bounds of Thm. 1 continue to hold for the test domains. In the corollary below, we bound the source error for practical ATTA at each time stept. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Thm. 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Thm. 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Further analysis and proofs are in Appx. D and E. The following corollary provides direct theoretical support that our strategy conditionally reduces the error bound on the source domain. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Thm. 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight 5Published as a conference paper at ICLR 2024 <latexit sha1_base64=\"NxhXSyFABPQk4q8627/odirDspg=\">AAAB9XicbVDLSgMxFM34rPVVdekmWARXZab4WhbcuKzYF7S1ZNI7bWgmMyR3lDL0P9y4UMSt/+LOvzHTdqGtBwKHc87l3hw/lsKg6347K6tr6xubua389s7u3n7h4LBhokRzqPNIRrrlMwNSKKijQAmtWAMLfQlNf3ST+c1H0EZEqobjGLohGygRCM7QSg/3mIWFGtAaGOwVim7JnYIuE29OimSOaq/w1elHPAlBIZfMmLbnxthNmUbBJUzyncRAzPiIDaBtqWIhmG46vXpCT63Sp0Gk7VNIp+rviZSFxoxD3yZDhkOz6GXif147weC6mwoVJwiKzxYFiaQY0awC2hcaOMqxJYxrYW+lfMg042iLytsSvMUvL5NGueRdli7uysXK+byOHDkmJ+SMeOSKVMgtqZI64USTZ/JK3pwn58V5dz5m0RVnPnNE/sD5/AFnsJJq</latexit> Streaming Test <latexit sha1_base64=\"a41BOKrutEYSWO9+8CjkPZKHvb8=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69BIvgqSTiR48FLx4r2A9oQ9lsN+3SzSbuToQQ+ie8eFDEq3/Hm//GTZuDtj4YeLw3w8w8PxZco+N8W6W19Y3NrfJ2ZWd3b/+genjU0VGiKGvTSESq5xPNBJesjRwF68WKkdAXrOtPb3O/+8SU5pF8wDRmXkjGkgecEjRSbzAhmKWzyrBac+rOHPYqcQtSgwKtYfVrMIpoEjKJVBCt+64To5cRhZwKNqsMEs1iQqdkzPqGShIy7WXze2f2mVFGdhApUxLtufp7IiOh1mnom86Q4EQve7n4n9dPMGh4GZdxgkzSxaIgETZGdv68PeKKURSpIYQqbm616YQoQtFElIfgLr+8SjoXdfe6fnV/WWs2ijjKcAKncA4u3EAT7qAFbaAg4Ble4c16tF6sd+tj0Vqyiplj+APr8wfpIY/e</latexit> ˆy <latexit sha1_base64=\"SJEOE2ZYxLL1SU/QahOlMH6fop4=\">AAAB8HicbVBNSwMxEM3Wr1q/qh69BItQL2VX/Oix4MVjBbettEvJptk2NMkuyaxQlv4KLx4U8erP8ea/MW33oK0PBh7vzTAzL0wEN+C6305hbX1jc6u4XdrZ3ds/KB8etUycasp8GotYd0JimOCK+cBBsE6iGZGhYO1wfDvz209MGx6rB5gkLJBkqHjEKQErPfr9DNi0Cuf9csWtuXPgVeLlpIJyNPvlr94gpqlkCqggxnQ9N4EgIxo4FWxa6qWGJYSOyZB1LVVEMhNk84On+MwqAxzF2pYCPFd/T2REGjORoe2UBEZm2ZuJ/3ndFKJ6kHGVpMAUXSyKUoEhxrPv8YBrRkFMLCFUc3srpiOiCQWbUcmG4C2/vEpaFzXvunZ1f1lp1PM4iugEnaIq8tANaqA71EQ+okiiZ/SK3hztvDjvzseiteDkM8foD5zPH2KnkB4=</latexit> U te ( t ) <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model SimATTA <latexit sha1_base64=\"bhVea6W/pzUPuDRNfs2xbDF7qAk=\">AAAB73icbVC7SgNBFL3rM8ZX1NJmMAhWYTf4KgM2FhYRzAOSJcxOZpMhs7PrzF0hhPyEjYUitv6OnX/jbLKFJh4YOJxzD3PvCRIpDLrut7Oyura+sVnYKm7v7O7tlw4OmyZONeMNFstYtwNquBSKN1Cg5O1EcxoFkreC0U3mt564NiJWDzhOuB/RgRKhYBSt1L6jQRYd9Eplt+LOQJaJl5My5Kj3Sl/dfszSiCtkkhrT8dwE/QnVKJjk02I3NTyhbEQHvGOpohE3/mS275ScWqVPwljbp5DM1N+JCY2MGUeBnYwoDs2il4n/eZ0Uw2t/IlSSIlds/lGYSoIxyY4nfaE5Qzm2hDIt7K6EDammDG1FRVuCt3jyMmlWK95l5eK+Wq6d53UU4BhO4Aw8uIIa3EIdGsBAwjO8wpvz6Lw4787HfHTFyTNH8AfO5w/1SI/i</latexit> Labeling <latexit sha1_base64=\"7rdY0fXtveVAqOkqa7z+i6K3Rp0=\">AAAB+XicbVDLSsNAFJ34rPUVdelmsAh1UxLxUXBTcOOygn1AE8pkMmmHTiZh5qZQQv/EjQtF3Pon7vwbp20W2nrgwuGce7n3niAVXIPjfFtr6xubW9ulnfLu3v7BoX103NZJpihr0UQkqhsQzQSXrAUcBOumipE4EKwTjO5nfmfMlOaJfIJJyvyYDCSPOCVgpL5tR1WPhgncYQ+GDMhF3644NWcOvErcglRQgWbf/vLChGYxk0AF0brnOin4OVHAqWDTspdplhI6IgPWM1SSmGk/n18+xedGCXGUKFMS8Fz9PZGTWOtJHJjOmMBQL3sz8T+vl0FU93Mu0wyYpItFUSYwJHgWAw65YhTExBBCFTe3YjokilAwYZVNCO7yy6ukfVlzb2rXj1eVRr2Io4RO0RmqIhfdogZ6QE3UQhSN0TN6RW9Wbr1Y79bHonXNKmZO0B9Ynz9h0pLV</latexit> f ( · ; ✓ ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"DPrA95GNP27SFW5vSoLC/hYa644=\">AAAB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIj4KbghuXFewDmlgmk0k7dJIJMzdKCf0PNy4Uceu/uPNvnLZZaOuBC4dz7uXee/xEcA22/W0VVlbX1jeKm6Wt7Z3dvfL+QVvLVFHWolJI1fWJZoLHrAUcBOsmipHIF6zjj26mfueRKc1lfA/jhHkRGcQ85JSAkR7CqksDCdfYTYb8tF+u2DV7BrxMnJxUUI5mv/zlBpKmEYuBCqJ1z7ET8DKigFPBJiU31SwhdEQGrGdoTCKmvWx29QSfGCXAoVSmYsAz9fdERiKtx5FvOiMCQ73oTcX/vF4KYd3LeJykwGI6XxSmAoPE0whwwBWjIMaGEKq4uRXTIVGEggmqZEJwFl9eJu2zmnNZu7g7rzTqeRxFdISOURU56Ao10C1qohaiSKFn9IrerCfrxXq3PuatBSufOUR/YH3+AFKlkbs=</latexit> f ( · ; \u0000 ) <latexit sha1_base64=\"ipQ+JKlINPDcPjrbUYUkqyyzp40=\">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYXEQpVUvMZKLIxF0IfURpXj3LRWHSeyHVBV+iksDCDEypew8Te4aQZoOZKlo3Puy8dPOFPacb6twsrq2vpGcbO0tb2zu2eX91sqTiWFJo15LDs+UcCZgKZmmkMnkUAin0PbH13P/PYDSMVica/HCXgRGQgWMkq0kfp2+S6bdNqQoCUxQ4K+XXGqTga8TNycVFCORt/+6gUxTSMQmnKiVNd1Eu1NiNSMcpiWeqmChNARGUDXUEEiUN4kO32Kj40S4DCW5gmNM/V3x4RESo0j31RGRA/VojcT//O6qQ6vvAkTSapB0PmiMOVYx3iWAw6YBKr52BBCJTO3YjokklBt0iqZENzFLy+TVq3qXlTPb2uV+lkeRxEdoiN0glx0ieroBjVQE1H0iJ7RK3qznqwX6936mJcWrLznAP2B9fkDSAyT+w==</latexit> Source-Pretrained <latexit sha1_base64=\"ud3dFXm+F2nsLD2/MdusutzkLvU=\">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PAixchgnlAsoTZ2d5kyMzOOjMbDEu+w4sHRbz6Md78GyfJHjSxoKGo6qa7K0g408Z1v53Cyura+kZxs7S1vbO7V94/aGqZKgoNKrlU7YBo4CyGhmGGQztRQETAoRUMb6Z+awRKMxk/mHECviD9mEWMEmMlvysC+ZTdyRD4pNQrV9yqOwNeJl5OKihHvVf+6oaSpgJiQznRuuO5ifEzogyjHCalbqohIXRI+tCxNCYCtJ/Njp7gE6uEOJLKVmzwTP09kRGh9VgEtlMQM9CL3lT8z+ukJrr2MxYnqYGYzhdFKcdG4mkCOGQKqOFjSwhVzN6K6YAoQo3NaRqCt/jyMmmeVb3L6sX9eaV2nsdRREfoGJ0iD12hGrpFddRAFD2iZ/SK3pyR8+K8Ox/z1oKTzxyiP3A+fwCmlpH9</latexit> Model <latexit sha1_base64=\"5LNAmmVR/AN9Lc2T+FRV/is2yz8=\">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKewGX8eACB48RDAP2CxhdjKbDJmdWWZ6lbDkM7x4UMSrX+PNv3GS7EETCxqKqm66u8JEcAOu++0UVlbX1jeKm6Wt7Z3dvfL+QcuoVFPWpEoo3QmJYYJL1gQOgnUSzUgcCtYOR9dTv/3ItOFKPsA4YUFMBpJHnBKwkn+nnvCNBK2Sca9ccavuDHiZeDmpoByNXvmr21c0jZkEKogxvucmEGREA6eCTUrd1LCE0BEZMN9SSWJmgmx28gSfWKWPI6VtScAz9fdERmJjxnFoO2MCQ7PoTcX/PD+F6CrIuExSYJLOF0WpwKDw9H/c55pREGNLCNXc3orpkGhCwaZUsiF4iy8vk1at6l1Uz+9rlfpZHkcRHaFjdIo8dInq6BY1UBNRpNAzekVvDjgvzrvzMW8tOPnMIfoD5/MHKbiRJQ==</latexit> Low Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"wuZucU3JbeEJSquG2WgqGdYMCR8=\">AAAB83icbVDLSgMxFL3js9ZX1aWbYBFclZnia1kQocsK9gHtUDJppg3NJCHJCGXob7hxoYhbf8adf2PazkJbD1w4nHMv994TKc6M9f1vb219Y3Nru7BT3N3bPzgsHR23jEw1oU0iudSdCBvKmaBNyyynHaUpTiJO29H4bua3n6g2TIpHO1E0TPBQsJgRbJ3Uq7PhCN0Lq6Wa9Etlv+LPgVZJkJMy5Gj0S1+9gSRpQoUlHBvTDXxlwwxrywin02IvNVRhMsZD2nVU4ISaMJvfPEXnThmgWGpXwqK5+nsiw4kxkyRynQm2I7PszcT/vG5q49swY0KllgqyWBSnHFmJZgGgAdOUWD5xBBPN3K2IjLDGxLqYii6EYPnlVdKqVoLrytVDtVy7zOMowCmcwQUEcAM1qEMDmkBAwTO8wpuXei/eu/exaF3z8pkT+APv8wfIYpF9</latexit> High Entropy <latexit sha1_base64=\"vLgKkEyV9E/djVdgAkvKuOUQOTU=\">AAAB7nicbVDLSgMxFL1TX7W+qi7dBIvgqswUX8uCG5cV7QPaoWTSTBuaZEKSEcrQj3DjQhG3fo87/8a0nYW2HrhwOOde7r0nUpwZ6/vfXmFtfWNzq7hd2tnd2z8oHx61TJJqQpsk4YnuRNhQziRtWmY57ShNsYg4bUfj25nffqLasEQ+2omiocBDyWJGsHVS+wELxanplyt+1Z8DrZIgJxXI0eiXv3qDhKSCSks4NqYb+MqGGdaWEU6npV5qqMJkjIe066jEgpowm587RWdOGaA40a6kRXP190SGhTETEblOge3ILHsz8T+vm9r4JsyYVKmlkiwWxSlHNkGz39GAaUosnziCiWbuVkRGWGNiXUIlF0Kw/PIqadWqwVX18r5WqV/kcRThBE7hHAK4hjrcQQOaQGAMz/AKb57yXrx372PRWvDymWP4A+/zB19wj48=</latexit> Samples <latexit sha1_base64=\"1BO6D/gzkeZNQ7HNIaph5NqELCI=\">AAAB8nicbVDLSgMxFM3UV62vqks3wSK4KjPF17LgRncV7AOmQ8mkd9rQTDIkGaEM/Qw3LhRx69e482/MtLPQ1gOBwzn3kHtPmHCmjet+O6W19Y3NrfJ2ZWd3b/+genjU0TJVFNpUcql6IdHAmYC2YYZDL1FA4pBDN5zc5n73CZRmUjyaaQJBTEaCRYwSYyX/XlAFMQhD+KBac+vuHHiVeAWpoQKtQfWrP5Q0zdOUE619z01MkBFlGOUwq/RTDQmhEzIC31JBYtBBNl95hs+sMsSRVPYJg+fq70RGYq2ncWgnY2LGetnLxf88PzXRTZAxkaQGBF18FKUcG4nz+/GQKaCGTy0hVDG7K6Zjogg1tqWKLcFbPnmVdBp176p++dCoNS+KOsroBJ2ic+Sha9REd6iF2ogiiZ7RK3pzjPPivDsfi9GSU2SO0R84nz9y2ZFU</latexit> Incremental <latexit sha1_base64=\"Jmobmj50NeE6y3ftB4xt5xZD5Eg=\">AAAB8XicbVDLSgNBEOyNrxhfUY9eBoPgKewGX8dALh4jmAcmS5id9CZDZmeXmVkhLP6FFw+KePVvvPk3TpI9aGJBQ1HVTXdXkAiujet+O4W19Y3NreJ2aWd3b/+gfHjU1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfM7j6g0j+W9mSboR3QkecgZNVZ6aIhUG1Rcjgblilt15yCrxMtJBXI0B+Wv/jBmaYTSMEG17nluYvyMKsOZwKdSP9WYUDahI+xZKmmE2s/mFz+RM6sMSRgrW9KQufp7IqOR1tMosJ0RNWO97M3E/7xeasIbP+MySQ1KtlgUpoKYmMzeJ0OukBkxtYQyxe2thI2posymoEs2BG/55VXSrlW9q+rlXa1Sv8jjKMIJnMI5eHANdbiFJrSAgYRneIU3RzsvzrvzsWgtOPnMMfyB8/kDzgaQ+A==</latexit> Clustering <latexit sha1_base64=\"c4xrXg0yZYBSSDLHCxlf45OWNzg=\">AAAB7nicbVDLSgNBEOz1GeMr6tHLYBA8hd2Aj2PAi8eI5gHJEmYnnWTIzOwyMyuEJR/hxYMiXv0eb/6Nk2QPmljQUFR1090VJYIb6/vf3tr6xubWdmGnuLu3f3BYOjpumjjVDBssFrFuR9Sg4AoblluB7UQjlZHAVjS+nfmtJ9SGx+rRThIMJR0qPuCMWie1HqhMBJpeqexX/DnIKglyUoYc9V7pq9uPWSpRWSaoMZ3AT2yYUW05EzgtdlODCWVjOsSOo4pKNGE2P3dKzp3SJ4NYu1KWzNXfExmVxkxk5DoltSOz7M3E/7xOagc3YcZVklpUbLFokApiYzL7nfS5RmbFxBHKNHe3EjaimjLrEiq6EILll1dJs1oJriqX99VyrZrHUYBTOIMLCOAaanAHdWgAgzE8wyu8eYn34r17H4vWNS+fOYE/8D5/AF7Wj40=</latexit> Samples <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"eimCpRgfVxBfxhwCehIJdcsMsvY=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5SRMtAGssI5gOTI+xt5pIle3vH7p4QjvwLGwtFbP03dv4bN8kVmvhg4PHeDDPzgkRwbVz32ylsbe/s7hX3SweHR8cn5dOzjo5TxbDNYhGrXkA1Ci6xbbgR2EsU0igQ2A2mzYXffUKleSwfzCxBP6JjyUPOqLHSY1Ok2qDicjwsV9yquwTZJF5OKpCjNSx/DUYxSyOUhgmqdd9zE+NnVBnOBM5Lg1RjQtmUjrFvqaQRaj9bXjwnV1YZkTBWtqQhS/X3REYjrWdRYDsjaiZ63VuI/3n91IS3fsZlkhqUbLUoTAUxMVm8T0ZcITNiZgllittbCZtQRZlNQZdsCN76y5ukU6t69Wr9vlZpuHkcRbiAS7gGD26gAXfQgjYwkPAMr/DmaOfFeXc+Vq0FJ585hz9wPn8AzSSQ9Q==</latexit> Clustering <latexit sha1_base64=\"JgGHFC5oztwX6+XjDtZWQo9C1hA=\">AAAB7nicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaImxscREwAQuZG8ZYMPe7mV3z4Rc+BE2Fhpj6++x89+4wBUKvmSSl/dmMjMvSgQ31ve/vcLG5tb2TnG3tLd/cHhUPj5pG5Vqhi2mhNKPETUouMSW5VbgY6KRxpHATjS5nfudJ9SGK/lgpwmGMR1JPuSMWid1biQbK2365Ypf9Rcg6yTISQVyNPvlr95AsTRGaZmgxnQDP7FhRrXlTOCs1EsNJpRN6Ai7jkoaowmzxbkzcuGUARkq7UpaslB/T2Q0NmYaR64zpnZsVr25+J/XTe3wOsy4TFKLki0XDVNBrCLz38mAa2RWTB2hTHN3K2FjqimzLqGSCyFYfXmdtGvVoF6t39cqDT+PowhncA6XEMAVNOAOmtACBhN4hld48xLvxXv3PpatBS+fOYU/8D5/AFOaj4U=</latexit> Anchors <latexit sha1_base64=\"KzBZ8R84UC9mpPFQBWeRHFxcqjw=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKVI8FLx4rmLbQhrLZbNq1m92wuxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzwpQzbVz32yltbG5t75R3K3v7B4dH1eOTjpaZItQnkkvVC7GmnAnqG2Y47aWK4iTktBtObud+94kqzaR4MNOUBgkeCRYzgo2VOn4aYUOH1ZpbdxdA68QrSA0KtIfVr0EkSZZQYQjHWvc9NzVBjpVhhNNZZZBpmmIywSPat1TghOogX1w7QxdWiVAslS1h0EL9PZHjROtpEtrOBJuxXvXm4n9ePzPxTZAzkWaGCrJcFGccGYnmr6OIKUoMn1qCiWL2VkTGWGFibEAVG4K3+vI66TTqXrPevG/UWldFHGU4g3O4BA+uoQV30AYfCDzCM7zCmyOdF+fd+Vi2lpxi5hT+wPn8AYuwjxQ=</latexit> Update <latexit sha1_base64=\"y2NH6tDs2GygUDqZYglGwvR4SpA=\">AAAB+nicbVBNSwMxEJ2tX7V+bfXoJVgEQSi7PVSPFS8eK9oPaEvJptk2NMkuSVYpa3+KFw+KePWXePPfmLZ70NYHA4/3ZpiZF8ScaeN5305ubX1jcyu/XdjZ3ds/cIuHTR0litAGiXik2gHWlDNJG4YZTtuxolgEnLaC8fXMbz1QpVkk780kpj2Bh5KFjGBjpb5bvMMi5lSjc3QlyShSuu+WvLI3B1olfkZKkKHed7+6g4gkgkpDONa643ux6aVYGUY4nRa6iaYxJmM8pB1LJRZU99L56VN0apUBCiNlSxo0V39PpFhoPRGB7RTYjPSyNxP/8zqJCS97KZNxYqgki0VhwpGJ0CwHNGCKEsMnlmCimL0VkRFWmBibVsGG4C+/vEqalbJfLVdvK6Wal8WRh2M4gTPw4QJqcAN1aACBR3iGV3hznpwX5935WLTmnGzmCP7A+fwBUnKTWg==</latexit> Samples + Anchors <latexit sha1_base64=\"u0BDOcH87PXd3DsT+o414+7cHnI=\">AAAB7XicbZC7SgNBFIbPxluMt6ilIINBsAq7FjGdARvLBMwFkhBmZ2eTMbMzy8ysEJaU9jYWitj6Cql8CDufwZdwcik0+sPAx/+fw5xz/JgzbVz308msrK6tb2Q3c1vbO7t7+f2DhpaJIrROJJeq5WNNORO0bpjhtBUriiOf06Y/vJrmzTuqNJPixoxi2o1wX7CQEWys1eiQQBrdyxfcojsT+gveAgqX75Pa1/3xpNrLf3QCSZKICkM41rrtubHpplgZRjgd5zqJpjEmQ9ynbYsCR1R309m0Y3RqnQCFUtknDJq5PztSHGk9inxbGWEz0MvZ1PwvaycmLHdTJuLEUEHmH4UJR0ai6eooYIoSw0cWMFHMzorIACtMjD1Qzh7BW175LzTOi16pWKq5hUoZ5srCEZzAGXhwARW4hirUgcAtPMATPDvSeXRenNd5acZZ9BzCLzlv33Yvk3g=</latexit> ··· <latexit sha1_base64=\"+7L/8ObZcl+JIZaSFhVO3t+lUUE=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2bvizjWb9YcituBrJMvDkp1Y6CDPV+8as7iFgScoVMUmM6nhtjL6UaBZN8UugmhseUPdIh71iqaMhNL82unZBTqwxIEGlbCkmm/p5IaWjMOPRtZ0hxZBa9qfif10kwuOqlQsUJcsVmi4JEEozI9HUyEJozlGNLKNPC3krYiGrK0AZUsCF4iy8vk+Z5xatWqnc2jQuYIQ/HcAJl8OASanALdWgAgwd4hld4cyLnxXl3PmatOWc+cwh/4Hz+AFjYkTs=</latexit> D l ( t ) <latexit sha1_base64=\"9C0bB8PYImk9DX0HLfGvGd44PFA=\">AAAB7XicbVDLSgNBEOyNrxhf8XHzMhiEeAm7ItFjQA8eI5gHJCHMTmaT0dnZZaZXCEv+wYsHRbz6P978GyebHDSxoKGo6qa7y4+lMOi6305uZXVtfSO/Wdja3tndK+4fNE2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5j9dTv/XEtRGRusdxzHshHSoRCEbRSs2b/qiMZ/1iya24Gcgy8eakVDsKMtT7xa/uIGJJyBUySY3peG6MvZRqFEzySaGbGB5T9kiHvGOpoiE3vTS7dkJOrTIgQaRtKSSZ+nsipaEx49C3nSHFkVn0puJ/XifB4KqXChUnyBWbLQoSSTAi09fJQGjOUI4toUwLeythI6opQxtQwYbgLb68TJrnFa9aqd7ZNC5ghjwcwwmUwYNLqMEt1KEBDB7gGV7hzYmcF+fd+Zi15pz5zCH8gfP5A1K8kTc=</latexit> D h ( t ) <latexit sha1_base64=\"eNrtnhPGeU8n4BRDMStm5cjQ4ts=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDbbTbt0s4m7E6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSKQw6Lrfzsbm1vbObmGvuH9weHRcOjltmzjVjLdYLGPdDajhUijeQoGSdxPNaRRI3gkmjbnfeeLaiFg94DThfkRHSoSCUbRS1zNIGlTKQansVtwFyDrxclKGHM1B6as/jFkacYVMUmN6npugn1GNgkk+K/ZTwxPKJnTEe5YqGnHjZ4t7Z+TSKkMSxtqWQrJQf09kNDJmGgW2M6I4NqveXPzP66UY3vqZUEmKXLHlojCVBGMyf54MheYM5dQSyrSwtxI2ppoytBEVbQje6svrpF2teLVK7b5arl/ncRTgHC7gCjy4gTrcQRNawEDCM7zCm/PovDjvzseydcPJZ87gD5zPH1Naj3k=</latexit> 1st Call <latexit sha1_base64=\"mxsL+XuWb2hqFND+pzTctrB1rcY=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkmR6rHQi8cK9gPaUDababt0s4m7G6GE/gkvHhTx6t/x5r9x2+agrQ8GHu/NMDMvSATXxnW/nY3Nre2d3cJecf/g8Oi4dHLa1nGqGLZYLGLVDahGwSW2DDcCu4lCGgUCO8GkMfc7T6g0j+WDmSboR3Qk+ZAzaqzUrcqQNKgQg1LZrbgLkHXi5aQMOZqD0lc/jFkaoTRMUK17npsYP6PKcCZwVuynGhPKJnSEPUsljVD72eLeGbm0SkiGsbIlDVmovycyGmk9jQLbGVEz1qveXPzP66VmeOtnXCapQcmWi4apICYm8+dJyBUyI6aWUKa4vZWwMVWUGRtR0Ybgrb68TtrViler1O6r5fp1HkcBzuECrsCDG6jDHTShBQwEPMMrvDmPzovz7nwsWzecfOYM/sD5/AE0o49l</latexit> 2nd Call <latexit sha1_base64=\"oSA1OFmXXL9y3PJtqoVxTIG9mto=\">AAAB8HicbVA9TwJBEJ3DL8Qv1NJmIzGxIncUaElCY2UwkQ8DF7K3zMGGvb3L7p6REH6FjYXG2Ppz7Pw3LnCFgi+Z5OW9mczMCxLBtXHdbye3sbm1vZPfLeztHxweFY9PWjpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY1+d++xGV5rG8N5ME/YgOJQ85o8ZKD7f4ZEidCtEvltyyuwBZJ15GSpCh0S9+9QYxSyOUhgmqdddzE+NPqTKcCZwVeqnGhLIxHWLXUkkj1P50cfCMXFhlQMJY2ZKGLNTfE1MaaT2JAtsZUTPSq95c/M/rpia89qdcJqlByZaLwlQQE5P592TAFTIjJpZQpri9lbARVZQZm1HBhuCtvrxOWpWyVy1X7yqlWiWLIw9ncA6X4MEV1OAGGtAEBhE8wyu8Ocp5cd6dj2VrzslmTuEPnM8fSFeQCA==</latexit> Next Call Figure 2: Overview of the SimATTA framework. and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (8) Corollary 4 validates that the selected low-entropy samples can mitigate the CF problem under the assumption that these samples are source-like, which is also empirically validated in Fig. 1. Note that our strategy employs entropy minimization in a selective manner, aiming to solve CF rather than the main adaptation issue. While many FTTA works use entropy minimization to adapt across domains without guarantees, our use is more theoretically-sound. 4 A N ATTA ALGORITHM Building on our theoretical findings, we introduce a simple yet effective ATTA method, known as SimATTA, that innovatively integrates incremental clustering and selective entropy minimization techniques, as illustrated in Fig. 2. We start with an overview of our methodology, including the learning framework and the comprehensive sample selection strategies. We then proceed to discuss the details of the incremental clustering technique designed for real-time sample selections. 4.1 A LGORITHM OVERVIEW Let (x, y) be a labeled sample and f(·; θ) be our neural network, where ˆy = f(x; θ) and θ represents the parameters. We have a model pre-trained on source domains with the pre-trained parameters ϕ. We initialize model parameters as θ(0) = ϕ and aim to adapt the model f(·; θ) in real-time. During the test phase, the model continuously predicts labels for streaming-in test data and concurrently gets fine-tuned. We perform sample selection to enable active learning. As discussed in Sec. 3.2, we empirically consider informative high-entropy samples for addressing distribution shifts and source-like low-entropy samples to mitigate CF. As shown in Alg. 1, at each time step t, we first partition unlabeled test samples Ute(t) into high entropy and low entropy datasets, Uh(t) and Ul(t), using an entropy threshold. The source-pretrained model f(·; ϕ) is frozen to predict pseudo labels for low entropy data. We obtain labeled low-entropy data Dl(t) by labeling Ul(t) with f(·; ϕ) and combining it with Dl(t − 1). In contrast, the selection of high-entropy samples for active labeling is less straightforward. Since the complete test dataset is inaccessible for analyzing the target domain distribution, real-time sample selection is required. We design an incremental clustering sample selection technique to reduce sample redundancy and increase distribution coverage, detailed in Sec. 4.2. The incremental clustering algorithm outputs the labeled test samples Dh(t), also referred to as anchors, given Dh(t −1) and Uh(t). After sample selection, the model undergoes test-time training using the labeled test anchors Dh(t) and pseudo-labeled source-like anchors Dl(t). Following the analyses in Sec. 3.1, the training weights and sample numbers should satisfy w(t) ≈ λ(t) for Dh(t) and Dl(t) for optimal results. The analyses and results in Sec. 3.2 further indicate that balancing the source and target ratio is the key to mitigating CF. However, when source-like samples significantly outnumber test samples, the optimal w(t) for test domains can deviate from λ(t) according to Eq. (4). 4.2 I NCREMENTAL CLUSTERING We propose incremental clustering, a novel continual clustering technique designed to select informa- tive samples in unsupervised settings under the ATTA framework. The primary goal of this strategy is to store representative samples for distributions seen so far. Intuitively, we apply clusters to cover all seen distributions while adding new clusters to cover newly seen distributions. During this process with new clusters added, old clusters may be merged due to the limit of the cluster budget. Since 6Published as a conference paper at ICLR 2024 Algorithm 1 SIMATTA: A SIMPLE ATTA ALGORITHM Require: A fixed source pre-trained model f(·; ϕ) and a real-time adapting model f(·; θ(t)) with θ(0) = ϕ. Streaming test data Ute(t) at time step t. Entropy of predictions H(ˆy) = −P c p(ˆyc) logp(ˆyc). Low entropy and high entropy thresholds el and eh. The number of cluster centroid budget NC (t) at time step t. Centroid increase number k. Learning step size η. 1: for t = 1, . . . , Tdo 2: Model inference on Ute(t) using f(·; θ(t − 1)). 3: Dl(t) ← Dl(t − 1) ∪ {(x, f(x; ϕ))|x ∈ Ute(t), H(f(x; ϕ)) < el} 4: Uh(t) ← {x|x ∈ Ute(t), H(f(x; θ)) > eh} 5: Dh(t) ← Dh(t − 1) ∪ {(x, y)|∀x ∈ IC(Dh(t − 1), Uh(t), NC(t)), y= Oracle(x)} 6: λ(t) ← |Dl(t)|/(|Dl(t)| + |Dh(t)|), |Dh(t)|/(|Dl(t)| + |Dh(t)|) 7: w(t) ← GetW(λ(t)) ▷ Generally, GetW(λ(t)) = λ(t) is a fair choice. 8: θ(t) ← θ(t − 1) 9: for (xl, yl) in Dl and (xh, yh) in Dh do 10: θ(t) ← θ(t) − ηw0∇ℓCE (f(xl; θ(t)), yl) − η(1 − w0)∇ℓCE (f(xh; θ(t)), yh) 11: end for 12: NC (t + 1) ← UpdateCentroidNum(NC (t)) ▷ Naive choice: NC (t + 1) ← NC (t) + k. 13: end for clusters cannot be stored efficiently, we store the representative samples of clusters, named anchors, instead. In this work, we adopt weighted K-means (Krishna and Murty, 1999) as our base clustering method due to its popularity and suitability for new setting explorations. When we apply clustering with new samples, a previously selected anchor should not weigh the same as new samples since the anchor is a representation of a cluster,i.e., a representation of many samples. Instead, the anchor should be considered as a barycenter with a weight of the sum of its cluster’s sample weights. For a newly added cluster, its new anchor has the weight of the whole cluster. For clusters containing multiple old anchors, i.e., old clusters, the increased weights are distributed equally among these anchors. These increased weights are contributed by new samples that are close to these old anchors. Intuitively, this process of clustering is analogous to the process of planet formation. Where there are no planets, new planets (anchors) will be formed by the aggregation of the surrounding material (samples). Where there are planets, the matter is absorbed by the surrounding planets. This example is only for better understanding without specific technical meanings. Specifically, we provide the detailed Alg. 2 for incremental clustering. In each iteration, we apply weighted K-Means for previously selected anchors Danc and the new streaming-in unlabeled data Unew. We first extract all sample features using the model from the previous step f(·; θ(t − 1)), and then cluster these weighted features. The initial weights of the new unlabeled samples are 1, while anchors inherit weights from previous iterations. After clustering, clusters including old anchors are old clusters, while clusters only containing new samples are newly formed ones. For each new cluster, we select the centroid-closest sample as the new anchor to store. As shown in line 10 of Alg. 2, for both old and new clusters, we distribute the sample weights in this cluster as its anchors’ weights. With incremental clustering, although we can control the number of clusters in each iteration, we cannot control the number of new clusters/new anchors. This indirect control makes the increase of new anchors adaptive to the change of distributions, but it also leads to indirect budget control. Therefore, in experimental studies, we set the budget limit, but the actual anchor budget will not reach this limit. The overall extra storage requirement is O(B) since the number of saved unlabeled samples is proportional to the number of saved labeled samples (anchors). 5 E XPERIMENTAL STUDIES In this study, we aim to validate the effectiveness of our proposed method, as well as explore the various facets of the ATTA setting. Specifically, we design experiments around the following research questions: RQ1: Can TTA methods address domain distribution shifts? RQ2: Is ATTA as efficient as TTA? RQ3: How do the components of SimATTA perform? RQ4: Can ATTA perform on par with stronger Active Domain Adaptation (ADA) methods? We compare ATTA with three settings, TTA (Tab. 2), enhanced TTA (Tab. 3 and 5), and ADA (Tab. 4). Datasets. To assess the OOD performance of the TTA methods, we benchmark them using datasets from DomainBed (Gulrajani and Lopez-Paz, 2020) and Hendrycks and Dietterich (2019a). We employ PACS (Li et al., 2017), VLCS (Fang et al., 2013), Office-Home (Venkateswara et al., 2017), and Tiny-ImageNet-C datasets for our evaluations. For each dataset, we designate one domain as 7Published as a conference paper at ICLR 2024 Table 2: TTA comparisons on PACS and VLCS.This table includes the two data stream mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. PACS Domain-wise data stream Post-adaptation Random data stream Post-adaptation P →A→ →C→ →S P A C S →1→ →2→ →3→ →4 P A C S BN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 Tent (steps=1) N/A 67.29 64.59 44.67 97.60 66.85 64.08 42.58 56.35 54.09 51.83 48.58 97.19 63.53 60.75 41.56Tent (steps=10) N/A 67.38 57.85 20.23 62.63 34.52 40.57 13.59 47.36 31.01 22.84 20.33 50.78 23.68 20.95 19.62EATA N/A 67.04 64.72 50.27 98.62 66.50 62.46 48.18 57.31 56.06 58.17 59.78 98.62 69.63 65.70 54.26CoTTA N/A 65.48 62.12 53.17 98.62 65.48 63.10 53.78 56.06 54.33 57.16 57.42 98.62 65.97 62.97 54.62SAR (steps=1) N/A 66.75 63.82 49.58 98.32 66.94 62.93 45.74 56.78 56.35 56.68 56.70 98.44 68.16 64.38 52.53SAR (steps=10) N/A 69.38 68.26 49.02 96.47 62.16 56.19 54.62 53.51 51.15 51.78 45.60 94.13 56.64 56.02 36.37 SimATTA (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00SimATTA (B ≤500) N/A 77.93 76.02 76.30 98.62 88.33 83.49 83.74 68.46 78.22 80.91 85.49 99.16 86.67 84.77 87.71 VLCS Domain-wise data stream Post-adaptation Random data stream Post-adaptation C →L→ →S→ →V C L S V →1→ →2→ →3→ →4 C L S V BN w/o adapt 100.00 33.55 41.10 49.05 100.00 33.55 41.10 49.05 41.23 41.23 41.23 41.23 100.00 33.55 41.10 49.05BN w/ adapt 85.16 37.31 33.27 52.16 85.16 37.31 33.27 52.16 40.91 40.91 40.91 40.91 85.16 37.31 33.27 52.16 Tent (steps=1) N/A 38.55 34.40 53.88 84.73 43.86 33.61 53.11 44.85 44.29 47.38 44.98 85.30 43.49 37.81 53.35Tent (steps=10) N/A 45.41 31.44 32.32 42.54 37.65 27.79 33.12 46.13 42.31 43.51 39.48 52.01 40.32 33.64 40.37EATA N/A 37.24 33.15 52.58 84.10 37.69 32.39 52.49 43.77 42.48 43.34 41.55 83.32 36.67 31.47 52.55CoTTA N/A 37.39 32.54 52.25 82.12 37.65 33.12 52.90 43.69 42.14 43.21 42.32 81.98 37.99 33.52 53.23SAR (steps=1) N/A 36.18 34.43 52.46 83.96 39.72 36.53 52.37 43.64 43.04 44.20 41.93 85.09 40.70 36.44 53.02SAR (steps=10) N/A 35.32 34.10 51.66 82.12 41.49 33.94 53.08 43.56 42.05 42.53 41.16 85.09 37.58 33.12 52.01 SimATTA (B ≤300) N/A 62.61 65.08 74.38 99.93 69.50 66.67 77.34 62.33 69.33 73.20 71.93 99.93 69.43 72.46 80.39SimATTA (B ≤500) N/A 63.52 68.01 76.13 99.51 70.56 73.10 78.35 62.29 70.45 73.50 72.02 99.43 70.29 72.55 80.18 the source domain and arrange the samples from the other domains to form the test data stream. For DomainBed datasets, we adopt two stream order strategies. The first order uses a domain-wise data stream, i.e., we finish streaming samples from one domain before starting streaming another domain. The second order is random, where we shuffle samples from all target domains and partition them into four splits 1, 2, 3, and 4, as shown in Tab. 2. More dataset details are provided in Appx. G.1. Baselines. For baseline models, we start with the common source-only models, which either utilize pre-calculated batch statistics (BN w/o adapt) or test batch statistics (BN w/ adapt). For comparison with other TTA methods, we consider four state-of-the-art TTA methods: Tent (Wang et al., 2021), EATA (Niu et al., 2022), CoTTA (Wang et al., 2022a), and SAR (Niu et al., 2023). The three of them except Tent provide extra design to avoid CF. To compare with ADA methods, we select algorithms that are partially comparable with our method, i.e., they should be efficient (e.g., uncertainty-based) without the requirements of additional networks. Therefore, we adopt random, entropy (Wang and Shang, 2014), k-means (Krishna and Murty, 1999), and CLUE (Prabhu et al., 2021) for comparisons. Settings. For TTA, we compare with general TTA baselines in streaming adaptation using the two aforementioned data streaming orders, domain-wise and random. We choose P in PACS and C in VLCS as source domains. For domain-wise data stream, we use order A → C → S for PACS and L → S → V for VLCS. We report the real-time adaptation accuracy results for each split of the data stream, as well as the accuracy on each domain after all adaptations through the data stream (under “post-adaptation” columns). Enhanced TTA is built on TTA with access to extra random sample labels. TTA baselines are further fine-tuned with these random samples. To further improve enhanced TTA, we use long-term label storage and larger unlabeled sample pools. To its extreme where the model can access the whole test set samples, the setting becomes similar to ADA, thus we also use ADA methods for comparisons. ADA baselines have access to all samples in the pre-collected target datasets but not source domain data, whereas our method can only access the streaming test data. 5.1 T HE FAILURE OF TEST-TIME ADAPTATION The failure of TTA methods on domain distribution shifts is one of the main motivations of the ATTA setting. As shown in Tab. 2, TTA methods cannot consistently outperform eventhe simplest baseline \"BN w/ adapt\" which uses test time batch statistics to make predictions, evidencing that current TTA methods cannot solve domain distribution shifts (RQ1). Additionally, Tent (step=10) exhibits significant CF issues, where \"step=10\" indicates 10 test-time training updates, i.e., 10 gradient backpropagation iterations. This failure of TTA methods necessitates the position of ATTA. In contrast, SimATTA, with a budget B less than 300, outperforms all TTA methods on both source and target domains by substantial margins. Moreover, compared to the source-only baselines, our method improves the target domain performances significantly with negligible source performance loss, showing that ATTA is a more practically effective setting for real-world distribution shifts. 5.2 E FFICIENCY & ENHANCED TTA SETTING COMPARISONS To validate the efficiency of ATTA and broaden the dataset choice, we conduct this study on Tiny- ImageNet-C which, though does not focus on domain shifts, is much larger than PACS and VLCS. we 8Published as a conference paper at ICLR 2024 Table 3: Comparisons with Enhanced TTA on Tiny-ImageNet-C (severity level 5). Tiny-ImageNet-C Time (sec)Noise Blur Weather Digital Gauss. Shot Impul. Defoc. Glass Motion Zoom Snow Frost Fog Contr. Elastic Pixel JPEG Avg. Tent (step=1) 68.83 9.32 11.97 8.86 10.43 7.00 12.20 14.34 13.58 15.46 13.55 3.99 13.31 17.79 18.61 12.17Tent (step=10) 426.90 0.86 0.63 0.52 0.52 0.55 0.54 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.54EATA 93.14 3.98 3.33 2.18 4.80 2.37 11.02 11.41 14.06 15.26 9.65 1.36 9.88 14.24 12.12 8.26CoTTA 538.78 5.63 7.12 6.31 8.05 5.74 9.68 10.55 11.75 12.00 11.15 4.17 5.35 7.82 8.90 8.16SAR (step=1) 113.76 8.90 3.11 1.67 1.55 1.47 1.35 1.19 1.03 1.04 0.93 0.83 1.00 0.74 0.77 1.83SAR (step=10) 774.11 2.67 3.26 2.38 1.64 1.85 2.49 3.16 3.81 2.72 3.12 0.81 3.47 4.04 1.76 2.66 SimATTA (step=10) 736.289.68 19.40 12.14 30.28 17.03 42.36 43.10 31.96 40.08 29.243.21 34.56 45.24 45.74 28.86 enhance the TTA setting by fine-tuning baselines on randomly selected labeled samples. Specifically, the classifier of ResNet18-BN is pre-adapted to the brightness corruption (source domain) before test-time adapting. SimATTA’s label budget is around 4,000, while all other TTA methods have budget 4,500 for randomly selected labeled samples. The data stream order is shown in Tab. 3. Time is measured across all corrupted images in the Noise and Blur noise types, and the values represent the average time cost for adapting 10,000 images. The results clearly evidence the efficiency of ATTA (RQ2), while substantially outperforming all enhanced TTA baselines. Simply accessing labeled samples cannot benefit TTA methods to match ATTA. With 10 training updates (step=10) for each batch, FTTA methods would suffer from severe CF problem. In contrast, ATTA covers a statistically significant distribution, achieving stronger performances with 10 training updates or even more steps till approximate convergences. In fact, longer training on Tent (step=10) leads to worse results (compared to step=1), which further motivates the design of the ATTA setting. The reason for higher absolute time cost in Tab. 3 is due to differences in training steps. In this experiment, SimATTA has a training step of 10, and similar time cost as SAR per step. Note that if the enhanced TTA setting is further improved to maintain distributions with a balanced CF mitigation strategy and an incremental clustering design, the design approaches ATTA. Specifically, we compare SimATTA with its variants as the ablation study (RQ3) in Appx. I.2. 5.3 C OMPARISONS TO A STRONGER SETTING : ACTIVE DOMAIN ADAPTATION Table 4: Comparisons to ADA baselines. Source domains are denoted as \"(S)\". Results are average accuracies (with standard deviations). PACS P (S) A C S Random (B= 300) 96.21 (0.80) 81.19 (0.48) 80.75 (1.27) 84.34 (0.18)Entropy (B= 300) 96.31 (0.64)88.00 (1.46)82.48 (1.71) 80.55 (1.01)Kmeans (B= 300) 93.71 (1.50) 79.31 (4.01) 79.64 (1.44) 83.92 (0.65)CLUE (B= 300) 96.69 (0.17)83.97 (0.57)84.77 (0.88) 86.91 (0.26) SimATTA (B ≤300) 98.89 (0.09)84.69 (0.22)83.09 (0.83)83.76 (2.24) VLCS C (S) L S V Random (B= 300) 96.21 (1.65) 66.67 (1.70) 70.72 (0.30) 72.14 (1.71)Entropy (B= 300) 97.74 (1.56) 69.29 (2.26)69.25 (4.77) 75.26 (3.07)Kmeans (B= 300) 98.61 (0.27)67.57 (1.64)70.77 (0.01)74.49 (0.97)CLUE (B= 300) 85.70 (10.09) 65.29 (1.49) 69.42 (2.64) 69.09 (6.05) SimATTA (B ≤300) 99.93 (0.00) 69.47 (0.03)69.57 (2.90)78.87 (1.53) In addtion to the above comparisons with (en- hanced) TTA, which necessitate the requirement of extra information in the ATTA setting, we com- pare ATTA with a stronger setting Active Domain Adaptation (ADA) to demonstrate another supe- riority of ATTA, i.e., weaker requirements for comparable performances (RQ4). ADA baselines are able to choose the global best active samples, while ATTA has to choose samples from a small sample buffer (e.g., a size of 100) and discard the rest. Tab. 4 presents the post-adaptation model per- formance results. All ADA results are averaged from 3 random runs, while ATTA results are the post-adaptation performances averaged from the two data stream orders. As can be observed, despite the lack of a pre-collected target dataset, SimATTA produces better or competitive results against ADA methods. Moreover, without source data access, SimATTA’s design for CF allows it to maintain superior source domain performances over ADA methods. Further experimental studies including the Office-Home dataset are provided in Appx. I. In conclusion, the significant improvement compared to weaker settings (TTA, enhanced TTA) and the comparable performance with the stronger setting, ADA, rendering ATTA a setting that is as efficient as TTA and as effective as ADA. This implies its potential is worthy of future explorations. 6 C ONCLUSION AND DISCUSSION There’s no denying that OOD generalization can be extremely challenging without certain information, often relying on various assumptions easily compromised by different circumstances. Thus, it’s prudent to seek methods to achieve significant improvements with minimal cost, e.g., DG methods leveraging environment partitions and ATTA methods using budgeted annotations. As justified in our theoretical and experimental studies, ATTA stands as a robust approach to achieve real-time OOD generalization. Although SimATTA sets a strong baseline for ATTA, there’s considerable scope for further investigation within the ATTA setting. One potential direction involves developing alternatives to prevent CF in ATTA scenarios. While selective entropy minimization on low-entropy samples has prove to be empirically effective, it relies on the quality of the pre-trained model and training on incorrectly predicted low-entropy samples may reinforce the errors. It might not be cost-effective to expend annotation budgets on low-entropy samples, but correcting them could be a viable alternative solution. We anticipate that our work will spur numerous further explorations in this field. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS This work was supported in part by National Science Foundation grant IIS-2006861 and National Institutes of Health grant U01AG070112. REFERENCES Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. Domain- adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. Lucas Baier, Tim Schlör, Jakob Schöffer, and Niklas Kühl. Detecting concept drift with neural network model uncertainty. In Hawaii International Conference on System Sciences, 2021. URL https://api.semanticscholar.org/CorpusID:235731947. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151–175, 2010. Davide Cacciarelli and Murat Kulahci. A survey on online active learning, 2023. Cheng Chen, Quande Liu, Yueming Jin, Qi Dou, and Pheng-Ann Heng. Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part V 24, pages 225–235. Springer, 2021. Li Chen, Tutian Tang, Zhitian Cai, Yang Li, Penghao Wu, Hongyang Li, Jianping Shi, Junchi Yan, and Yu Qiao. Level 2 autonomous driving on a single device: Diving into the devils of openpilot. arXiv preprint arXiv:2206.08176, 2022a. Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, and Yueting Zhuang. Self-supervised noisy label learning for source-free unsupervised domain adaptation. In 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS) , pages 10185–10192. IEEE, 2022b. Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. Advances in Neural Information Processing Systems, 33:21061–21071, 2020. David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129–145, 1996. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. Yuhe Ding, Lijun Sheng, Jian Liang, Aihua Zheng, and Ran He. Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation. arXiv preprint arXiv:2205.14566, 2022. Cian Eastwood, Ian Mason, Christopher KI Williams, and Bernhard Schölkopf. Source-free adaptation to measurement shift via bottom-up feature restoration. arXiv preprint arXiv:2107.05446, 2021. Jiahao Fan, Hangyu Zhu, Xinyu Jiang, Long Meng, Chen Chen, Cong Fu, Huan Yu, Chenyun Dai, and Wei Chen. Unsupervised domain adaptation by statistics alignment for deep sleep staging networks. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30:205–216, 2022. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657–1664, 2013. Yuqi Fang, Pew-Thian Yap, Weili Lin, Hongtu Zhu, and Mingxia Liu. Source-free unsupervised domain adaptation: A survey. arXiv preprint arXiv:2301.00265, 2022. Francois Fleuret et al. Uncertainty reduction for model adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9613–9623, 2021. 10Published as a conference paper at ICLR 2024 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180–1189. PMLR, 2015. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096–2030, 2016. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021. Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances in neural information processing systems, 17, 2004. Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14004–14013, 2020. Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=8hHg-zs_p-h. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. March 2019a. doi: 10.48550/ARXIV .1903.12261. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019b. Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Semisupervised svm batch mode active learning with applications to image retrieval. ACM Transactions on Information Systems (TOIS), 27(3):1–29, 2009. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853–17862, 2023. Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34:3635–3649, 2021. Masato Ishii and Masashi Sugiyama. Source-free domain adaptation via distributional alignment by matching batch normalization statistics. arXiv preprint arXiv:2101.10842, 2021. Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. Advances in Neural Information Processing Systems, 34:2427–2440, 2021. Suyog Dutt Jain and Kristen Grauman. Active image segmentation propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2864–2873, 2016. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4893–4902, 2019. Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaussian processes for object categorization. In 2007 IEEE 11th international conference on computer vision, pages 1–8. IEEE, 2007. Neerav Karani, Ertunc Erdil, Krishna Chaitanya, and Ender Konukoglu. Test-time adaptable neural networks for robust medical image segmentation. Medical Image Analysis, 68:101907, 2021. 11Published as a conference paper at ICLR 2024 Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring catastrophic forgetting in neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB, volume 4, pages 180–191. Toronto, Canada, 2004. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521–3526, 2017. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal- subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021. Divya Kothandaraman, Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, and Dinesh Manocha. Salad: Source-free active label-agnostic domain adaptation for classification, segmentation and detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 382–391, 2023. K Krishna and M Narasimha Murty. Genetic k-means algorithm. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 29(3):433–439, 1999. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu- tional neural networks. Communications of the ACM, 60(6):84–90, 2017. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap- olation (REx). In International Conference on Machine Learning , pages 5815–5826. PMLR, 2021. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 615–625, 2021. David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148–156. Elsevier, 1994. Aodong Li, Alex Boyd, Padhraic Smyth, and Stephan Mandt. Detecting and adapting to irregular distribution shifts in bayesian online learning. Advances in neural information processing systems, 34:6816–6828, 2021a. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain adaptation without source data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9641–9650, 2020. Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474–8481, 2021b. Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. Jian Liang, Dapeng Hu, Ran He, and Jiashi Feng. Distill and fine-tune: Effective adaptation from a black-box source model. arXiv preprint arXiv:2104.01539, 1(3), 2021. Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Dine: Domain adaptation from single and multiple black-box predictors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8003–8013, 2022. 12Published as a conference paper at ICLR 2024 Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment partition? Advances in Neural Information Processing Systems, 35:24529–24542, 2022. Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, and Jonghye Woo. Adapting off-the- shelf source segmenter for target medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24, pages 549–559. Springer, 2021a. Xinyu Liu and Yixuan Yuan. A source-free domain adaptive polyp detection framework with style diversification flow. IEEE Transactions on Medical Imaging, 41(7):1897–1908, 2022. Yuang Liu, Wei Zhang, Jun Wang, and Jianyong Wang. Data-free knowledge transfer: A survey. arXiv preprint arXiv:2112.15278, 2021b. Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021c. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017. Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021. Xinhong Ma, Junyu Gao, and Changsheng Xu. Active universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8968–8977, 2021. Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, and Dongmei Zhang. Source free unsupervised graph domain adaptation. arXiv preprint arXiv:2112.00955, 2021. Christoforos Mavrogiannis, Francesca Baldini, Allan Wang, Dapeng Zhao, Pete Trautman, Aaron Steinfeld, and Jean Oh. Core challenges of social robot navigation: A survey. ACM Transactions on Human-Robot Interaction, 12(3):1–39, 2023. Zachary Nado, Shreyas Padhy, D Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020. Munan Ning, Donghuan Lu, Dong Wei, Cheng Bian, Chenglang Yuan, Shuang Yu, Kai Ma, and Yefeng Zheng. Multi-anchor active domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9112–9122, 2021. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In International conference on machine learning, pages 16888–16905. PMLR, 2022. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. InThe Eleventh International Con- ference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE transactions on neural networks, 22(2):199–210, 2010. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 13Published as a conference paper at ICLR 2024 Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53–69, 2015. Judea Pearl. Causality. Cambridge university press, 2009. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):947–1012, 2016. Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505–8514, 2021. Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv preprint arXiv:2010.05761, 2020. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal- ization help optimization? Advances in neural information processing systems, 31, 2018. Akanksha Saran, Safoora Yousefi, Akshay Krishnamurthy, John Langford, and Jordan T. Ash. Streaming active learning with deep neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 30005–30021. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr. press/v202/saran23a.html. Harald Schafer, Eder Santana, Andrew Haden, and Riccardo Biasini. A commute in data: The comma2k19 dataset, 2018. Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for informa- tion extraction. In Advances in Intelligent Data Analysis: 4th International Conference, IDA 2001 Cascais, Portugal, September 13–15, 2001 Proceedings 4, pages 309–318. Springer, 2001. Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 33:11539–11551, 2020. Burr Settles. Active learning literature survey. 2009. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, and Manmohan Chandraker. Active adversarial domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 739–748, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443–450. Springer, 2016. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229–9248. PMLR, 2020. 14Published as a conference paper at ICLR 2024 Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7472–7481, 2018. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068–4076, 2015. Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167–7176, 2017. Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. Sudheendra Vijayanarasimhan and Ashish Kapoor. Visual recognition and detection under bounded computational resources. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1006–1013. IEEE, 2010. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test- time adaptation by entropy minimization. InInternational Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312: 135–153, 2018. Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7201–7211, 2022a. Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. IEEE Transactions on Multimedia , 2022b. Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. ACM Transactions on Intelligent Systems and Technology (TIST), 11(5):1–46, 2020. Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, Xinjing Cheng, and Guoren Wang. Active learning for domain adaptation: An energy-based approach. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8708–8716, 2022. Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for text classification using support vector machines. In Advances in Information Retrieval: 25th European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings 25, pages 393–407. Springer, 2003. Baoyao Yang, Hao-Wei Yeh, Tatsuya Harada, and Pong C Yuen. Model-induced generalization error bound for information-theoretic representation learning in source-data-free unsupervised domain adaptation. IEEE Transactions on Image Processing, 31:419–432, 2021a. Guanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci. Transformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021b. Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, and Yang You. Divide to adapt: Mitigating confirmation bias for domain adaptation of black-box predictors. arXiv preprint arXiv:2205.14467, 2022. H Yao, Yuhong Guo, and Chunsheng Yang. Source-free unsupervised domain adaptation with surrogate data generation. In Proceedings of NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. 15Published as a conference paper at ICLR 2024 Hao-Wei Yeh, Baoyao Yang, Pong C Yuen, and Tatsuya Harada. Sofa: Source-data-free feature alignment for unsupervised domain adaptation. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 474–483, 2021. Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. Hu Yu, Jie Huang, Yajing Liu, Qi Zhu, Man Zhou, and Feng Zhao. Source-free domain adaptation for real-world image dehazing. In Proceedings of the 30th ACM International Conference on Multimedia, pages 6645–6654, 2022. Haojian Zhang, Yabin Zhang, Kui Jia, and Lei Zhang. Unsupervised domain adaptation of black-box source models. arXiv preprint arXiv:2101.02839, 2021. Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Advances in Neural Information Processing Systems, 35:38629–38642, 2022a. Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Adanpc: Exploring non-parametric classifier for test-time adaptation. In International Conference on Machine Learning, pages 41647–41676. PMLR, 2023. Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2339–2348, 2022b. Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: degradation-free fully test-time adaptation. arXiv preprint arXiv:2301.13018, 2023a. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International Conference on Machine Learning (ICML), 2023b. Chunting Zhou, Xuezhe Ma, Paul Michel, and Graham Neubig. Examining and combating spurious features under distribution shift. In International Conference on Machine Learning, pages 12857– 12867. PMLR, 2021. 16Published as a conference paper at ICLR 2024 Active Test-Time Adaptation: Foundational Analyses and An Algorithm Supplementary Material A B ROADER IMPACTS The field of domain generalization primarily concentrates on enhancing a model’s generalization abilities by preparing it thoroughly before deployment. However, it is equally important for deep learning applications to have the capacity for real-time adaptation, as no amount of preparation can account for all possible scenarios. Consequently, domain generalization and test-time adaptation are complementary strategies: the former is more weighty and extensive, while the latter is more agile, lightweight and privacy-friendly. This work delves into the development of a real-time model adaptation strategy that can be applied to any pre-trained models, including large language models, to enhance their adaptive capabilities. Our research does not involve any human subjects or dataset releases, nor does it raise any ethical concerns. Since this work does not directly tie to specific applications, we do not foresee any immediate negative societal impacts. Nonetheless, we acknowledge that any technological advancement may carry potential risks, and we encourage the continued assessment of the broader impacts of real-time adaptation methodologies in various contexts. B FAQ & D ISCUSSIONS To facilitate the reviewing process, we summarize the answers to the questions that arose during the discussion of an earlier version of this paper. The major updates of this version are reorganized theoretical studies, incremental clustering details, experimental reorganization, and additional datasets and settings . We include more related field comparisons to distinguish different settings. We also cover the position of this paper in literature and the main claims of this paper. Finally, we will frankly acknowledge the limitations of this paper, explain and justify the scope of coverage, and provide possible future directions. Q1: What is the relationship between the proposed ATTA protocol and stream based active learning (Saran et al., 2023)? A: We would like to discuss the difference between our work and the referenced work. 1. Real-time Training Distinction: Saran et al. (2023) doesn’t operate in real-time capacity. This is evident from their experiments, where their model is trained only after completing a round. In contrast, our work involves training the model post each batch. This positions Saran et al. (2023)’s work as an intrinsic active learning technique, while our approach leans towards TTA methods. 2. Continual Training Nuance: Following the point above, Saran et al. (2023) stands out of the scope of continual training. As they mentioned ‘each time new data are acquired, the ResNet is reset to the ImageNet pre-trained weights before being updated‘, Saran et al. (2023) starts afresh with each iteration and is out of scope for CF discussions. Contrarily, our model is continuously trained on varying distributions, compelling us to address the CF issue while preserving advantages derived from various stored distributions. 3. Comparative Complexity: Given the aforementioned distinctions, it’s evident that our task presents a greater challenge compared to theirs. In addition, we have included comparisons with stronger active learning settings in Sec. 5.3. Q2: What are the insights from the theoretically foundational analysis? A: 1. It sets a well-defined formulation and grounded theoretical framework for the ATTA setting. 2. While entropy minimizations can cause CF, balancing the learning rate and number of high/low entropy samples is conversely the key solution to both distribution shifts and 17Published as a conference paper at ICLR 2024 CF by corresponding benefits. Though adding low-entropy data is intuitive, it is crucial in that this simple operation can make methods either too conservative or too aggressive without the correct balancing conditions. 3. The studies in Sec. 3.1 directly present a feasible and guaranteed solution for imple- menting ATTA to tackle shifts while avoiding CF. The aligned empirical validations of Sec. 3.2 also instruct the implementation of SimATTA. Q3: In test-time adaptation, one important issue is that the number of testing samples in a batch may be small, which means the sample size m will also be very small. May it affect the theorem and make them become very loose? A: We consider this issue jointly from theoretical and empirical validations. 1. It is true that the theoretical bounds can be loose given a small size of m unlabeled test samples. This situation of the error bound is mathematically ascribed to the quotient between the VC-dimension d of the hypothesis class and m. Under the VC-dimension theory, the ResNet18 model we adopt should have d ≫ m. However, practically we perform fine-tuning on pre-trained models instead of training from scratch, which significantly reduces the scale of parameter update. In this case, an assumption can be established that fine-tuning a model is roughly equivalent to learning a model with a relatively small d (Appx. H). This assumption is potentially underpinned by the empirical alignment of our validation experiments with the theoretical framework (Fig. 1). To this end, experiments indicate thatd and m are practically of similar scale for our settings. This prevents our theoretical bounds from being very loose and meaningless in reality. 2. Regarding cases that our assumption does not apply, this issue would appear inevitable, since it is rigorously inherent in the estimation error of our streaming and varying test distributions. The distribution of a test stream can be hardly monitored when only a limited batch is allowed, which we consider as a limitation of TTA settings. Moreover, this issue directly implies the necessity of using a buffer for unlabeled samples. A good practice is to maintain a relatively comparable sample buffer scale. Q4: What distribution shifts can ATTA solve? A: We would like to follow (but not limited to) the work (Zhao et al., 2023b) to discuss the distribution shifts ATTA can solve. 1. As elucidated in Sec. 3.1 and Sec. 5, ATTA can solve domain generalization shifts. Domain generalization shifts include complex shifts on the joint data distribution P(X, Y), given X as the covariates and Y as the label variable. Since P(X, Y) = P(X)P(Y |X), ATTA can handle covariate shift (P(X)), label shift (P(Y )), and conditional shift (P(Y |X)). The shifts on both covariate and conditional distributions can cover the shift on labels, but they (covariate + conditional shifts) are more complicated than pure label shifts, where only the marginal label distribution changes while the conditional distribution remains. Note that the conditional shifts are generally caused by spurious correlations, where the independent causal mechanism assumption (Pearl, 2009) holds or no concept drifts exist. 2. In our framework, the distribution support of X at different time steps can be different, but we don’t cover the situation where the support of Y changes, i.e., class-incremental problems. Q5: It is unclear how many samples are selected in each minibatch of testing samples. How the total budget is distributed across the whole testing data stream? A: The number of selected samples for each minibatch is decided jointly by the incremental clustering and the cluster centroid number NC (t). Intuitively, this sample selection is a dynamic process, with NC (t) restricting the budget and incremental clustering performing sample selection. For each batch, we increase applicable clustering centroids as a maximum limit, while the exact number of the selected samples is given by the incremental clustering by how many clusters are located in the scope of new distributions. e.g., if the incoming batch does not introduce new data distributions, then we select zero samples even with increased NC (t). In contrast, if the incoming batch contains data located in multiple new distributions, the incremental clustering tends to select more samples than the NC (t) limit, thus forcing to merging of multiple previous clusters into one new cluster. 18Published as a conference paper at ICLR 2024 The incremental clustering is detailed in Sec. 4.2, and NC (t) is naively increased by a constant hyper-parameter k. Therefore, the budget is adaptively distributed according to the data streaming distribution with budgets controlled by k, which is also the reason why we compare methods under a budget limit. Q6: Could compared methods have access to a few ground-truth labels as well? Making other algorithms be able to use the same amount of ground-truth labels randomly will produce fairer comparisons. A: 1. The enhanced TTA setting is exactly the setup we provide to produce fairer comparisons. See Tab. 3 and Tab. 5 for comparison results. 2. ATTA also compares to a stronger setting ADA which can access the whole test datasets multiple times. Table 5: The table demonstrates the comparisons on PACS where all enhanced TTA baselines have 300 budgets to randomly select labeled samples. The training steps of these labeled samples are the same as the original TTA method training steps. For accumulated sample selection, please refer to our ablation studies. Method Domain-wise data stream A VG Random data stream A VG P→ →A→ →C→ →S P A C S 1 2 3 4 P A C S Source onlyBN w/o adapt 99.70 59.38 28.03 42.91 99.70 59.38 28.03 42.91 43.44 43.44 43.44 43.44 99.70 59.38 28.03 42.91BN w/ adapt 98.74 68.07 64.85 54.57 98.74 68.07 64.85 54.57 62.50 62.50 62.50 62.50 98.74 68.07 64.85 54.57 TTA Tent (steps=1) N/A 70.07 68.43 64.42 97.72 74.17 72.61 68.92 61.20 62.36 66.59 67.32 98.14 74.37 70.26 66.07Tent (steps=10) N/A 76.27 63.78 49.35 59.46 38.62 48.46 55.03 56.20 53.22 52.55 55.55 58.32 47.56 60.75 58.00EATA N/A 69.53 66.94 61.42 98.56 69.38 66.60 64.83 60.34 59.81 64.38 65.02 98.68 73.78 68.30 59.74CoTTA N/A 66.55 63.14 59.91 90.12 61.67 66.68 67.68 57.26 57.36 63.46 65.64 92.22 71.53 70.44 62.41SAR (steps=1) N/A 66.60 63.78 50.34 98.38 67.87 64.04 49.48 57.21 56.06 56.78 57.14 98.38 68.80 64.59 53.02SAR (steps=10) N/A 69.09 66.55 49.07 96.23 62.50 59.34 46.53 49.76 52.74 48.51 49.06 95.39 57.13 54.61 38.76 Ours (B ≤300) N/A 76.86 70.90 75.39 98.80 84.47 82.25 81.52 69.47 76.49 82.45 82.22 98.98 84.91 83.92 86.00 Q7: What is the position of ATTA? A: Comparisons with different settings are challenging. In this work, the design of our experiments (Sec. 5) is to overcome this challenge by comparing both weaker settings and stronger settings. While the significant performance over weaker settings renders the necessity of extra information, the comparable performance with stronger settings provides the potential to relax restricted requirements. Intuitively, ATTA is the most cost-effective option in the consideration of both efficiency and effectiveness. We further provide the following ATTA summary: ATTA, which incorporates active learning in FTTA, is the light, real-time, source-free, widely applicable setting to achieve high generalization performances for test-time adaptation. 1. Necessity: From the causality perspective, new information is necessary (Lin et al., 2022; Pearl, 2009; Peters et al., 2017) to attain generalizable over distribution shifts which are insurmountable within the current TTA framework. 2. Effectiveness: Compared to FTTA methods, ATTA produces substantially better perfor- mances, on-par with the costly active domain adaptation (ADA) methods as shown in Table 3 in the paper. 3. Efficiency: Relative to ADA methods, ATTA possesses superior efficiency, similar to general FTTA methods, as shown in Tab. 3. 4. Applicability: ATTA is a model-agnostic setting. (1) Compared to domain generalization methods, ATTA do not require re-training and has the potential to apply to any pre-trained models. One interesting future direction is designing ATTA methods for large language models (LLMs), where re-trainings are extremely expensive and source data may be in- accessible. (2) Compared to FTTA methods, ATTA can protect model parameters from corrupting while learning new distributions by fine-tuning pre-trained models, rendering it more feasible and practical. In comparison with existing works, ATTA is motivated to mitigate the limitations of previous settings: 1. FTTA: Limited generalization performance. 19Published as a conference paper at ICLR 2024 2. TTT: Not source-free; limited generalization performance. 3. ADA & domain adaptation/generalization: Expensive re-trainings; limited applicability to pre-trained models. 4. Online active learning: It does not maintain and protect adaptation performances for multiple distributions in one model and does not consider the CF problem. Q8: What is the potential practical utility of ATTA? A: 1. Empirically, our method can generally finish a round of sample selection/training of 100 frames in 5s, i.e., 20 frames per sec, which is more than enough to handle multiple practical situations. Experiments on time complexity are provided in Tab. 3, where SimATTA has comparable time efficiency. 2. As a case analysis, the autopilot system (Hu et al., 2023; Chen et al., 2022a) presents an application scenario requiring high-speed low-latency adaptations, while these adaptations are largely underexplored. When entering an unknown environment, e.g., a construction section, a system of ATTA setting can require the driver to take over the wheel. During the period of manual operation when the driver is handling the wheel, steering signals are generated, and the in-car system quickly adaptations. The system doesn’t need to record 60 frames per second, since only the key steering operations and the corresponding dash cam frames are necessary, which can be handled by ATTA algorithms processing at 20 frames per sec. In this case, the human annotations are necessary and indirect. ATTA makes use of this information and adapts in the short term instead of collecting videos and having a long-round fine-tuning (Schafer et al., 2018). 3. In addition, many scenarios applicable for ATTA are less speed-demanding than the case above. One example is a personalized chatbot that subtly prompts and gathers user labels during user interaction. In a home decoration setting, applications can request that users scan a few crucial areas to ensure effective adaptation. Social robots (Mavrogiannis et al., 2023), e.g., vacuum robots, often require users to label critical obstacles they’ve encountered. 4. Compared with ADA, ATTA stands out as the tailored solution for the above scenarios. It does not require intensive retraining or server-dependent fine-tuning, offering both speed and computational efficiency. Meanwhile, akin to other TTA methods, ATTA also ensures user privacy. While it might marginally exceed the cost of standard TTA methods, the superior generalization ability makes it a compelling choice and justifies the additional expense. Q9: What can be covered by this paper? A: This paper endeavors to establish the foundational framework for a novel setting referred to as ATTA. We target (1) positioning the ATTA setting, (2) solving the two major and basic challenges of ATTA,i.e., the mitigation of distribution shifts and the avoidance of catastrophic forgetting (CF). We achieve the first goal by building the problem formulation and analyses, and further providing extensive qualitative and well-organized experimental comparisons with TTA, enhanced TTA, and ADA settings. These efforts position ATTA as the most cost-effective option between TTA and ADA, where ATTA inherits the efficiency of TTA and the effectiveness of ADA. With our theoretical analyses and the consistent algorithm design, we validate the success of our second goal through significant empirical performances. Q10: What are not covered by this paper? A: Constructing a new setting involves multifaceted complexities. Although there are various potential applications discussed above including scaling this setting up for large models and datasets, we cannot cover them in this single piece of work. There are three main reasons. First, the topics covered by a single paper are limited. Formally establishing ATTA setting and addressing its major challenges of ATTA takes precedence over exploring practical applications. Secondly, given the interrelations between ATTA and other settings, our experimental investigations are predominantly comparative, utilizing the most representative datasets from TTA and domain adaptation to showcase persuasive results. Thirdly, many practical applications necessitate task-specific configurations, rendering them unsuitable for establishing a universal learning setting. While the current focus is on laying down the foundational aspects of ATTA, the exploration of more specialized applications remains a prospective avenue for future work in the ATTA domain. 20Published as a conference paper at ICLR 2024 C R ELATED WORKS The development of deep learning witnesses various applications (He et al., 2016; Gui et al., 2020). To tackle OOD problem, various domain generalization works emerge (Krueger et al., 2021; Sagawa et al., 2019). C.1 U NSUPERVISED DOMAIN ADAPTATION Unsupervised Domain Adaptation (UDA) (Pan et al., 2010; Patel et al., 2015; Wilson and Cook, 2020; Wang and Deng, 2018) aims at mitigating distribution shifts between a source domain and a target domain, given labeled source domain samples and unlabeled target samples. UDA methods generally rely on feature alignment techniques to eliminate distribution shifts by aligning feature distributions between source and target domains. Typical feature alignment techniques include discrepancy minimization (Long et al., 2015; Sun and Saenko, 2016; Kang et al., 2019) and adversarial training (Ganin and Lempitsky, 2015; Tsai et al., 2018; Ajakan et al., 2014; Ganin et al., 2016; Tzeng et al., 2015; 2017). Nevertheless, alignments are normally not guaranteed to be correct, leading to the alignment distortion problem as noted by Ning et al. (2021). Source-free Unsupervised Domain Adaptation (SFUDA) (Fang et al., 2022; Liu et al., 2021b) algorithms aim to adapt a pre-trained model to unlabeled target domain samples without access to source samples. Based on whether the algorithm can access model parameters, these algorithms are categorized into white-box and black-box methods. White-box SFUDA typically considers data recovery (generation) and fine-tuning methods. The former focuses on recovering source- like data (Ding et al., 2022; Yao et al., 2021), e.g., training a Generative Adversarial Network (GAN) (Kurmi et al., 2021; Li et al., 2020), while the latter employs various techniques (Mao et al., 2021), such as knowledge distillation (Chen et al., 2022b; Liu and Yuan, 2022; Yang et al., 2021b; Yu et al., 2022), statistics-based domain alignment (Ishii and Sugiyama, 2021; Liu et al., 2021a; Fan et al., 2022; Eastwood et al., 2021), contrastive learning (Huang et al., 2021; Wang et al., 2022b), and uncertainty-based adaptation (Gawlikowski et al., 2021; Fleuret et al., 2021; Chen et al., 2021; Li et al., 2021b). Black-box SFUDA cannot access model parameters and often relies on self-supervised knowledge distillation (Liang et al., 2022; 2021), pseudo-label denoising (Zhang et al., 2021; Yang et al., 2022), or generative distribution alignment (Yeh et al., 2021; Yang et al., 2021a). C.2 T EST-TIME ADAPTATION Test-time Adaptation (TTA), especially Fully Test-time Adaptation (FTTA) algorithms (Wang et al., 2021; Iwasawa and Matsuo, 2021; Karani et al., 2021; Nado et al., 2020; Schneider et al., 2020; Wang et al., 2022a; Zhao et al., 2023a; Niu et al., 2022; Zhang et al., 2022a; Niu et al., 2023; You et al., 2021; Zhang et al., 2022b), can be considered as realistic and lightweight methods for domain adaptation. Built upon black-box SFUDA, FTTA algorithms eliminate the requirement of a pre-collected target dataset and the corresponding training phase. Instead, they can only access an unlabeled data stream and apply real-time adaptation and training. In addition to FTTA, Test-time Training (TTT) (Sun et al., 2020; Liu et al., 2021c) often relies on appending the original network with a self-supervised task. TTT methods require retraining on the source dataset to transfer information through the self-supervised task. Although they do not access the source dataset during the test-time adaptation phase, TTT algorithms are not off-the-shelf source-free methods. TTA is a promising and critical direction for real-world applications, but current entropy minimization-based methods can be primarily considered as feature calibrations that require high-quality pseudo-labels. This requirement, however, can be easily violated under larger distribution shifts. Current TTA algorithms, inheriting UDA drawbacks, cannot promise good feature calibration results, which can be detrimental in real-world deployments. For instance, entropy minimization on wrongly predicted target domain samples with relatively low entropy can only exacerbate spurious correla- tions (Chen et al., 2020). Without extra information, this problem may be analogous to applying causal inference without intervened distributions, which is intrinsically unsolvable (Peters et al., 2016; Pearl, 2009). This paper aims to mitigate this issue with minimal labeled target domain samples. To minimize the cost, we tailor active learning techniques for TTA settings. It is worth noting that a recent work AdaNPC (Zhang et al., 2023) is essentially a domain gener- alization method with a TTA phase attached, while our ATTA is built based on the FTTA setting. Specifically, Current FTTA methods and our work cannot access the source domain. In contrast, 21Published as a conference paper at ICLR 2024 AdaNPC accesses source data to build its memory bank, circumventing the catastrophic forgetting problem. Furthermore, AdaNPC requires multiple source domains and training before performing TTA. Thus AdaNPC uses additional information on domain labels and retraining resources for its memory bank, undermining the merits of FTTA. Regarding theoretical bounds, their target domain is bounded by source domain error and model estimations (in big-O expression), while we consider active sample learning and time variables for varying test distributions. C.3 C ONTINUAL DOMAIN ADAPTATION Many domain adaptation methods focus on improving target domain performance, neglecting the performance on the source domain, which leads to the CF problem (Kemker et al., 2018; Kirkpatrick et al., 2017; Li and Hoiem, 2017; Lopez-Paz and Ranzato, 2017; De Lange et al., 2021; Wang et al., 2022a; Niu et al., 2022). This issue arises when a neural network, after being trained on a sequence of domains, experiences a significant degradation in its performance on previously learned domains as it continues to learn new domains. Continual learning, also known as lifelong learning, addresses this problem. Recent continual domain adaptation methods have made significant progress by employing gradient regularization, random parameter restoration, buffer sample mixture, and more. Although the CF problem is proposed in the continual learning field, it can occur in any source-free OOD settings since the degradation caused by CF is attributed to the network’s parameters being updated to optimize performance on new domains, which may interfere with the representations learned for previous domains. C.4 A CTIVE DOMAIN ADAPTATION Active Domain Adaptation (ADA) (Prabhu et al., 2021; Ning et al., 2021; Su et al., 2020; Ma et al., 2021; Xie et al., 2022) extends semi-supervised domain adaptation with active learning strate- gies (Cohn et al., 1996; Settles, 2009), aiming to maximize target domain performance with a limited annotation budget. Therefore, the key challenge of active learning algorithms is selecting the most informative unlabeled data in target domains (Kapoor et al., 2007). Sample selection strategies are of- ten based on uncertainty (Lewis and Catlett, 1994; Scheffer et al., 2001), diversity (Jain and Grauman, 2016; Hoi et al., 2009), representativeness (Xu et al., 2003), expected error minimization (Vijaya- narasimhan and Kapoor, 2010), etc. Among these methods, uncertainty and diversity-based methods are simple and computationally efficient, making them the most suitable choices to tailor for TTA settings. Adapting these strategies is non-trivial because, compared to typical active domain adaptation, our proposed Active Test-time Adaptation (ATTA) setting does not provide access to source data, model parameters, or pre-collected target samples. This requirement demands that our active sample selection algorithm select samples for annotation during data streaming. Consequently, this active sampling selection process is non-regrettable, i.e., we can only meet every sample once in a short period. To avoid possible confusion, compared to the recent Source-free Active Domain Adaptation (SFADA) method SALAD (Kothandaraman et al., 2023), we do not require access to model parameter gradients, training additional neural networks, or pre-collected target datasets. Therefore, our ATTA setting is quite different, much lighter, and more realistic than ADA and SFADA. C.5 A CTIVE ONLINE LEARNING The most related branch of active online learning (AOL) (Cacciarelli and Kulahci, 2023) is active online learning on drifting data stream (Zhou et al., 2021; Baier et al., 2021; Li et al., 2021a). Generally, these methods include two components, namely, detection and adaptation. Compared with ATTA, there are several distinctions. First, this line of studies largely focuses on the distribution shift detection problem, while ATTA focuses on multi-domain adaptations. Second, AOL on drifting data stream aims to detect and adapt to one current distribution in the stream, without considering preserving the adaptation abilities of multiple past distributions by maintaining and fine-tuning the original pre-trained models. In contrast, ATTA’s goal is to achieve the OOD generalization optimums adaptable across multiple source and target distributions, leading to the consideration of CF problems. Third, while AOL requires one-by-one data input and discard, ATTA maintains a buffer for incoming data before selection decisions. This is because ATTA targets maintaining the original model without corrupting and replacing it, such that making statistically meaningful and high-quality decisions is 22Published as a conference paper at ICLR 2024 critical for ATTA. In contrast, AOL allows resetting and retraining new models, whose target is more lean to cost saving and one-by-one manner. D F URTHER THEORETICAL STUDIES In this section, we refine the theoretical studies with supplement analysis and further results. We use the H-divergence and H∆H-distance definitions following (Ben-David et al., 2010). Definition 2 (H-divergence). For a function class H and two distributions D1 and D2 over a domain X, the H-divergence between D1 and D2 is defined as dH(D1, D2) = sup h∈H |Px∼D1 [h(x) = 1] − Px∼D2 [h(x) = 1]|. The H∆H-distance is defined base on H-divergence. We use the H∆H-distance definition follow- ing (Ben-David et al., 2010). Definition 3 (H∆H-distance). For two distributions D1 and D2 over a domain X and a hypothesis class H, the H∆H-distance between D1 and D2 w.r.t. H is defined as dH∆H(D1, D2) = sup h,h′∈H Px∼D1 [h(x) ̸= h′(x)] + Px∼D2 [h(x) ̸= h′(x)]. (9) The H∆H-distance essentially provides a measure to quantify the distribution shift between two distributions. It measures the maximum difference of the disagreement between two hypotheses in H for two distributions, providing a metrics to quantify the distribution shift between D1 and D2. H-divergence and H∆H-distance have the advantage that they can be applied between datasets, i.e., estimated from finite samples. Specifically, let S1, S2 be unlabeled samples of size m sampled from D1 and D2; then we have estimated H∆H-distance ˆdH(S1, S2). This estimation can be bounded based on Theorem 3.4 of Kifer et al. (2004), which we state here for completeness. Theorem 5. Let A be a collection of subsets of some domain measure space, and assume that the VC-dimension is some finite d. Let P1 and P2 be probability distributions over that domain and S1, S2 finite samples of sizes m1, m2 drawn i.i.d. according P1, P2 respectively. Then Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16, (10) where Pm1+m2 is the m1 + m2’th power of P - the probability that P induces over the choice of samples. Theorem 5 bounds the probability for relativized discrepancy, and its applications in below lemmas and Theorem 1 help us bound the quantified distribution shifts between domains. The probability, according to a distribution D, that an estimated hypothesis h disagrees with the true labeling function g : X → {0, 1} is defined as ϵ(h(t), g) = E(x)∼D[|h(x, t) − g(x)|], which we also refer to as the error or risk ϵ(h(t)). While the source domain dataset is inaccessible under ATTA settings, we consider the existence of the source dataset DS for the purpose of accurate theoretical analysis. Thus, we initialize Dtr(0) as DS, i.e., Dtr(0) = DS. For every time step t, the test and training data can be expressed as Ute(t) and Dtr(t) = DS ∪ Dte(1) ∪ Dte(2) ∪ ··· ∪Dte(t). (11) We use N to denote the total number of samples in Dtr(t) and λ = (λ0, λ1, ··· , λt) to represent the ratio of sample numbers in each component subset. In particular, we have |DS| |Dtr(t)| = λ0, |Dte(1)| |Dtr(t)| = λ1, ··· , |Dte(t)| |Dtr(t)| = λt, (12) where Pt i=0 λi = 1. Therefore, at time step t, the model has been trained on labeled data Dtr(t), which contains t + 1 components consisting of a combination of data from the source domain and multiple test-time domains. For each domain the model encounters, DS, Ute(1), Ute(2), ··· , Ute(t), let ϵj(h(t)) denote the error of hypothesis h at time t on the jth domain. Specifically, ϵ0(h(t)) = ϵS(h(t)) represents the error of h(t) on the source data DS, and ϵj(h(t)) for j ≥ 1 denotes the error of h(t) on test data Ute(j). Our optimization minimizes a convex combination of training error over the labeled samples from all domains. Formally, given the vector w = (w0, w1, ··· , wt) of domain error 23Published as a conference paper at ICLR 2024 weights with Pt j=0 wj = 1 and the sample number from each component Nj = λjN, we minimize the empirical weighted error of h(t) as ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj Nj X Nj |h(x, t) − g(x)|. (13) Note that w, λ and N are also functions of t, which we omit for simplicity. We now establish two lemmas as the preliminary for Theorem 1. In the following lemma, we bound the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)). Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. In the following lemma, we provide an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. The proofs for both lemmas are provided in Appx. E. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Lemma 6 bounds the difference between the weighted error ϵw(h(t)) and the domain error ϵj(h(t)), which is majorly influenced by the estimatedH∆H-distance and the quality of discrepancy estimation. During the ATTA process, the streaming test data can form multiple domains and distributions. However, if we consider all data during the test phase as a single test domain,i.e., St i=1 Ute(i), we can simplify Lemma 6 to obtain an upper bound for the test error ϵT as |ϵw(h(t)) − ϵT (h(t))| ≤w0  1 2 ˆdH∆H(S0, ST ) + 2 s 2d log(2m) + log 2 δ m + γ  , (14) where γ = min h∈H{ϵ0(h(t)) + ϵT (h(t))}, and ST is sampled from St i=1 Ute(i). To understand Lamma 7, we need to understand Hoeffding’s Inequality, which we state below as a Proposition for completeness. Proposition 8 (Hoeffding’s Inequality). Let X be a set, D1, . . . , Dt be probability distributions on X, and f1, . . . , ft be real-valued functions on X such that fi : X → [ai, bi] for i = 1, . . . , t. Then for any ϵ >0, P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! (15) where E[fi(x)] is the expected value of fi(x). Lamma 7 provides an upper bound on the difference between the true and empirical weighted errors ϵw(h(t)) and ˆϵw(h(t)). Thus, as wj deviates from λj, the feasible approximation ˆϵw(h(t)) with a finite number of labeled samples becomes less reliable. Building upon the two preceding lemmas, we proceed to derive bounds on the domain errors under the ATTA setting when minimizing the empirical weighted error using the hypothesis h at time t. Theorem 1 essentially bounds the performance of ATTA on the source and each test domains. The adaptation performance on a test domain is majorly 24Published as a conference paper at ICLR 2024 bounded by the composition of (labeled) training data, estimated distribution shift, and ideal joint hypothesis performance, which correspond to C, ˆdH∆H(Si, Sj), and γi, respectively. The ideal joint hypothesis error γi gauges the inherent adaptability between domains. If we consider the multiple data distributions during the test phase as a single test domain, i.e., St i=1 Ute(i), Theorem 1 can be reduced into bounds for the source domain error ϵS and test domain error ϵT . With the optimal test/source hypothesis h∗ T (t) = arg min h∈H ϵT (h(t)) and h∗ S(t) = arg minh∈H ϵS(h(t)), |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16a) |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤(1 − w0)A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (16b) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Our learning bounds demonstrates the trade-off between the small amount of budgeted test-time data and the large amount of less relevant source data. Next, we provide an approximation of the condition necessary to achieve optimal adaptation performance, which is calculable from finite samples and can be readily applied in practical ATTA scenarios. Following Eq. (16.a), with approximately B = c1 p d/N, the optimal value w∗ 0 to tighten the test error bound is a function of λ0 and A: w∗ 0 = λ0 − s A2N c2 1d − A2Nλ0(1 − λ0), for λ 0 ≥ 1 − d A2N , (17) where c1 is a constant. Note that λ0 ≥ 1 − d A2N should be the satisfied condition in practical ATTA settings, where the budget is not sufficiently big while the source data amount is relatively large. When the budget is sufficiently large or the source data amount is not sufficiently large compared to the distribution shift A, the optimal w∗ 0 for the test error bound is w∗ 0 = 0, i.e., using no source data since possible error reduction from the data addition is always less than the error increase caused by large divergence between the source data and the test data. Theorem 2 offers a direct theoretical guarantee that ATTA reduces the error bound on test domains in comparison to TTA without the integration of active learning. Following Theorem 1, when no active learning is included during TTA,i.e., w0 = λ0 = 1, the upper boundw0A+ q w2 0 λ0 + (1−w0)2 1−λ0 B ≥ A+B; when enabling ATTA, withw0 = λ0 ̸= 1, we can easily achieve an upper bound w0A + B < A+ B. Therefore, the incorporation of labeled test instances in ATTA theoretically enhances the overall performance across test domains, substantiating the significance of the ATTA setting in addressing distribution shifts. Entropy quantifies the amount of information contained in a probability distribution. In the context of a classification model, lower entropy indicates that the model assigns high probability to one of the classes, suggesting a high level of certainty or confidence in its prediction. When a model assigns low entropy to a sample, this high confidence can be interpreted as the sample being well-aligned or fitting closely with the model’s learned distribution. In other words, the model “recognizes” the sample as being similar to those it was trained on, hence the high confidence in its prediction. While entropy is not a direct measure of distributional distance, it can be used as an indicator of how closely a sample aligns with the model’s learned distribution. This interpretation is more about model confidence and the implied proximity rather than a strict mathematical measure of distributional distance. The pre-trained model is well-trained on abundant source domain data, and thus the model distribution is approximately the source distribution. Selecting low-entropy samples using essentially provides an estimate of sampling from the source dataset. Thus, Dϕ,S(t), based on well-aligned with the model’s learned distribution is an approximation of DS. When we consider the CF problem and feasibly include the source-like dataset Dϕ,S(t) into the ATTA training data in place of the inaccessible DS in Eq. (11), we can also derive bounds on the domain errors under this practical ATTA setting when minimizing the empirical weighted errorϵ′ w(h(t)) using the hypothesis h at time t, similar to Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domainsDϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is 25Published as a conference paper at ICLR 2024 N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵ′ w(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Other derived results following Theorem 1 also apply for this practical ATTA setting. Further empirical validations for our theoretical results are provided in Appx. H. E P ROOFS This section presents comprehensive proofs for all the lemmas, theorems, and corollaries mentioned in this paper, along with the derivation of key intermediate results. Lemma 6. Let H be a hypothesis space of VC-dimension d. At time step t, let the ATTA data domains be DS, Ute(1), Ute(2), ··· , Ute(t), and Si be unlabeled samples of size m sampled from each of the t + 1 domains respectively. Then for any δ ∈ (0, 1), for every h ∈ Hminimizing ϵw(h(t)) on Dtr(t), we have |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. Proof. First we prove that given unlabeled samples of size m S1, S2 sampled from two distributions D1 and D2, we have dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (18) We start with Theorem 3.4 of Kifer et al. (2004): Pm1+m2 [|ϕA(S1, S2) − ϕA(P1, P2)| > ϵ] ≤ (2m)de−m1ϵ2/16 + (2m)de−m2ϵ2/16. (19) In Eq. 19, ’d’ is the VC-dimension of a collection of subsets of some domain measure space A, while in our case, d is the VC-dimension of hypothesis space H. Following (Ben-David et al., 2010), the H∆H space is the set of disagreements between every two hypotheses inH, which can be represented as a linear threshold network of depth 2 with 2 hidden units. Therefore, the VC-dimension of H∆H is at most twice the VC-dimension of H, and the VC-dimension of our domain measure space is 2d for Eq. 19 to hold. Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2m)2de−m1ϵ2/16 + (2m)2de−m2ϵ2/16. We rewrite the inequality as δ (2m)2d = e−m1ϵ2/16 + e−m2ϵ2/16; taking the logarithm of both sides, we get log δ (2m)2d = −m1 ϵ2 16 + log(1 +e−(m1−m2) ϵ2 16 ). 26Published as a conference paper at ICLR 2024 Assuming m1 = m2 = m and defining a = ϵ2 16 , we have log δ (2m)2d = −ma + log 2; rearranging the equation, we then get ma + log(δ/2) = 2d log(2m). Now, we can solve for a: a = 2d log(2m) + log 2 δ m . Recall that a = ϵ2 16 , so we get: ϵ = 4√a ϵ = 4 s 2d log(2m) + log 2 δ m . With probability of at least 1 − δ, we have |ϕA(S1, S2) − ϕA(P1, P2)| ≤4 s 2d log(2m) + log 2 δ m ; therefore, dH∆H(D1, D2) ≤ ˆdH∆H(S1, S2) + 4 s 2d log(2m) + log 2 δ m . (20) Now we prove Lemma 6. We use the triangle inequality for classification error in the derivation. For the domain error of hypothesis h at time t on the jth domain ϵj(h(t)), given the definition of ϵw(h(t)), |ϵw(h(t)) − ϵj(h(t))| = | tX i=0 wiϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi|ϵi(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(|ϵi(h(t)) − ϵi(h(t), h∗ i (t))| + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + |ϵj(h(t), h∗ i (t)) − ϵj(h(t))|) ≤ tX i=0 wi(ϵi(h∗ i (t)) + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| + ϵj(h∗ i (t))) ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|), where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. By the definition of H∆H-distance and our proved Eq. 20, |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))| ≤sup h,h′∈H |ϵi(h(t), h′(t)) − ϵj(h(t), h′(t))| = sup h,h′∈H Px∼Di[h(x) ̸= h′(x)] + Px∼Dj [h(x) ̸= h′(x)] = 1 2dH∆H(Di, Dj) ≤ 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m , 27Published as a conference paper at ICLR 2024 where Di, Dj denote the ith and jth domain. Therefore, |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0 wi(γi + |ϵi(h(t), h∗ i (t)) − ϵj(h(t), h∗ i (t))|) ≤ tX i=0 wi(γi + 1 2dH∆H(Di, Dj)) ≤ tX i=0 wi(γi + 1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m ). Since ϵi(h(t)) − ϵj(h(t)) = 0 when i = j, we derive |ϵw(h(t)) − ϵj(h(t))| ≤ tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi  , with probability of at least 1 − δ, where γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. This completes the proof. Lemma 7. Let H be a hypothesis class. For Dtr(t) = DS ∪ Dte(1) ∪ ··· ∪Dte(t) at time t, if the total number of samples in Dtr(t) is N, and the ratio of sample numbers in each component is λj, then for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Proof. We apply Hoeffding’s Inequality in our proof: P  \f\f\f\f\f 1 t tX i=1 fi(x) − 1 t tX i=1 Ex∼Di[fi(x)] \f\f\f\f\f ≥ ϵ ! ≤ 2 exp   − 2t2ϵ2 Pt i=1(bi − ai)2 ! . (21) In the jth domain, there are λjN samples. With the true labeling function g(x), for each of the λjN samples x, let there be a real-valued function fi(x) fi(x) = wj λj |h(x, t) − g(x)|, where fi(x) ∈ [0, wj λj ]. Incorporating all the domains, we get ˆϵw(h(t)) = tX j=0 wjˆϵj(h(t)) = tX j=0 wj λjN X λjN |h(x, t) − g(x)| = 1 N tX j=0 λjNX i=1 fi(x), which corresponds to the 1 t Pt i=1 fi(x) part in Hoeffding’s Inequality. Due to the linearity of expectations, we can calculate the sum of expectations as 1 N tX j=0 λjNX i=1 E[fi(x)] = 1 N ( tX j=0 λjN wj λj ϵj(h(t))) = tX j=0 wjϵj(h(t)) = ϵw(h(t)), which corresponds to the 1 t Pt i=1 Ex∼Di[fi(x)] part in Hoeffding’s Inequality. Therefore, we can apply Hoeffding’s Inequality as P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ 2 exp   −2N2ϵ2/( NX i=0 range2(fi(x))) ! = 2 exp   −2N2ϵ2/( tX j=0 λjN(wj λj )2) ! = 2 exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . This completes the proof. 28Published as a conference paper at ICLR 2024 Theorem 1. Let H be a hypothesis class of VC-dimension d. At time step t, for ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), Si are unlabeled samples of size m sampled from each of the t + 1 domains respectively. The total number of samples in Dtr(t) is N and the ratio of sample numbers in each component is λi. If ˆh(t) ∈ Hminimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), and h∗ j (t) = arg minh∈H ϵj(h(t)) is the optimal hypothesis on the jth domain, then for any δ ∈ (0, 1), with probability of at least 1 − δ, we have ϵj(ˆh(t)) ≤ ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k (k >0), assuming k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H (D(k′), Ute(t + k)) ≤ δD, where 0 ≤ δD ≪ +∞, then ∀δ, with probability of at least 1 − δ, we have ϵt+k(ˆh(t)) ≤ ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. Proof. First we prove that for any δ ∈ (0, 1) and h ∈ H, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (22) We apply Theorem 3.2 of Kifer et al. (2004) and Lemma 7, P[|ϵw(h(t)) − ˆϵw(h(t))| ≥ϵ] ≤ (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . Given δ ∈ (0, 1), we set the upper bound of the inequality to δ, and solve for ϵ: δ = (2N)d exp   −2Nϵ2/( tX j=0 w2 j λj ) ! . We rewrite the inequality as δ (2N)d = e −2Nϵ2/(Pt j=0 w2 j λj ) , taking the logarithm of both sides, we get log δ (2N)d = −2Nϵ2/( tX j=0 w2 j λj ). Rearranging the equation, we then get ϵ2 = ( tX j=0 w2 j λj )d log(2N) − log(δ) 2N . Therefore, with probability of at least 1 − δ, we have |ϵw(h(t)) − ˆϵw(h(t))| ≤ vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 . (23) 29Published as a conference paper at ICLR 2024 Based on Eq. 23, we now prove Theorem 1. For the empirical domain error of hypothesis h at time t on the jth domain ϵj(ˆh(t)), applying Lemma 6, Eq. 23, and the definition of h∗ j (t), we get ϵj(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ j (t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵj(h∗ j (t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   = ϵj(h∗ j (t)) + 2 tX i=0,i̸=j wi  1 2 ˆdH∆H(Si, Sj) + 2 s 2d log(2m) + log 2 δ m + γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵj(h(t))}. For future test domains j = t + k where k > 0, we have the assumption that k′ = argmink′∈{0,1,...t} dH∆H(D(k′), Ute(t + k)) and min dH∆H(D(k′), Ute(t + k)) ≤ δD. Here, we slightly abuse the notation D(k′) to represent Ds if k′ = 0 and Ute(k′) if k′ > 0. Then we get ϵt+k(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, St+k) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2( ˆdH∆H(Si, Sk′ ) + ˆdH∆H(Sk′ , St+k)) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   30Published as a conference paper at ICLR 2024 ≤ ˆϵw(h∗ t+k(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵt+k(h∗ t+k(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, Sk′ ) + 1 2δD + 2 s 2d log(2m) + log 2 δ m + γi   = ϵt+k(h∗ t+k(t)) + tX i=0 wi  ˆdH∆H(Si, Sk′ ) + 4 s 2d log(2m) + log 2 δ m + δD + 2γi   + 2C. with probability of at least 1−δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 , γi = minh∈H{ϵi(h(t))+ ϵt+k(h(t))}, and 0 ≤ δD ≪ +∞. This completes the proof. Theorem 2. Let H be a hypothesis class of VC-dimension d. For ATTA data domains DS, Ute(1), Ute(2), ··· , Ute(t), considering the test-time data as a single test domain St i=1 Ute(i), if ˆh(t) ∈ H minimizes the empirical weighted error ˆϵw(h(t)) with the weight vector w on Dtr(t), let the test error be upper-bounded with |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when no active learning is included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, then for any λ ̸= λ′, there exists w s.t. EBT (w, λ, N, t) < EBT (w′, λ′, N, t). (24) Proof. From Theorem 1, we can derive the bound for the test error where the test-time data are considered as a single test domain: |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤EBT (w, λ, N, t) = w0( ˆdH∆H(S0, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N ; and we simplify the above equation as |ϵT (ˆh(t)) − ϵT (h∗ T (t))| ≤w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B, (25) where the distribution divergence termA = ˆdH∆H(S0, ST )+4 q 2d log(2m)+log 2 δ m +2γ, the empirical gap term B = 2 q d log(2N)−log(δ) 2N , ST is sampled from St i=1 Ute(i), and γ = minh∈H{ϵ0(h(t)) + ϵT (h(t))}. Since we have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (26) 31Published as a conference paper at ICLR 2024 where Formula 26 obtains the minimum value if and only if w0 = λ0; when enabling ATTA with any λ0 ̸= 1, we can get EBT (w, λ, N, t) = w0A + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≥ w0A + B, (27) where the minimum value EBT (w, λ, N, t)min = w0A + B can be obtained with condition w0 = λ0 ̸= 1. When no active learning is included, i.e., for weight and sample ratio vectors w′ and λ′, w′ 0 = λ′ 0 = 1 and w′ i = λ′ i = 0 for i ≥ 1, we have EBT (w′, λ′, N, t) = w′ 0A + s w′2 0 λ′ 0 + (1 − w′ 0)2 1 − λ′ 0 B = A + B. (28) Since for EBT (w, λ, N, t)min = w0A + B, w0 < 1 and A, B >0 hold, we derive EBT (w, λ, N, t)min = w0A + B < A+ B = EBT (w′, λ′, N, t). (29) This completes the proof. Corollary 3. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), Si are unla- beled samples of size m sampled from each of the t + 1 domains respectively, and SS is unlabeled samples of size m sampled from DS. If ˆh(t) ∈ Hminimizes ˆϵ′ w(h(t)) while other conditions remain identical to Theorem 1, then ϵS(ˆh(t)) ≤ ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C, with probability at least 1 − δ, where C follows Theorem 1 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. Proof. For the empirical source error on DS of hypothesis h at time t, similar to Theorem 1, we apply Lemma 6, Eq. 23 to get ϵS(ˆh(t)) ≤ ϵw(ˆh(t)) + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(ˆh(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ˆϵw(h∗ S(t)) + vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵw(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   ≤ ϵS(h∗ S(t)) + 2 vuut  tX i=0 w2 i λi !\u0012d log(2N) − log(δ) 2N \u0013 + 2 tX i=0 wi  1 2 ˆdH∆H(Si, SS) + 2 s 2d log(2m) + log 2 δ m + γi   32Published as a conference paper at ICLR 2024 = ϵS(h∗ S(t)) + tX i=0 wi  ˆdH∆H(Si, SS) + 4 s 2d log(2m) + log 2 δ m + 2γi   + 2C with probability of at least 1 − δ, where C = r\u0010Pt i=0 w2 i λi \u0011\u0010 d log(2N)−log(δ) 2N \u0011 and γi = minh∈H{ϵi(h(t)) + ϵS(h(t))}. This completes the proof. Corollary 4. At time step t, for ATTA data domains Dϕ,S(t), Ute(1), Ute(2), ··· , Ute(t), suppose that ˆh(t) ∈ Hminimizes ˆϵw′(h(t)) under identical conditions to Theorem 2. Let’s denote the source error upper bound with |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t). Let w′ and λ′ be the weight and sample ratio vectors when Dϕ,S(t) is not included, i.e., w′ and λ′ s.t. w′ 0 = λ′ 0 = 0 . If ˆdH∆H(DS, Dϕ,S(t)) < ˆdH∆H(DS, St i=1 Ute(i)), then for any λ ̸= λ′, there exists w s.t. EBS(w, λ, N, t) < EBS(w′, λ′, N, t). (30) Proof. From Theorem 1, considering the test-time data as a single test domain, we can derive the bound for the source error on DS: |ϵS(ˆh(t)) − ϵS(h∗ S(t))| ≤EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + 2 s w2 0 λ0 + (1 − w0)2 1 − λ0 r d log(2N) − log(δ) 2N , where ST is sampled fromSt i=1 Ute(i), γ = minh∈H{ϵ0(h(t))+ϵS(h(t))}, and γ′ = minh∈H{ϵT (h(t))+ ϵS(h(t))}. We have s w2 0 λ0 + (1 − w0)2 1 − λ0 = s (w0 − λ0)2 λ0(1 − λ0) + 1 ≥ 1, (31) where the equality and the minimum value are obtained if and only if w0 = λ0. When Dϕ,S(t) is not included,i.e., with the weight and sample ratio vectorsw′ and λ′ s.t. w′ 0 = λ′ 0 = 0, using the empirical gap term B = 2 q d log(2N)−log(δ) 2N , we have EBS(w′, λ′, N, t) = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + s w2 0 λ0 + (1 − w0)2 1 − λ0 B = ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B. When Dϕ,S(t) is included with λ0 ̸= 0, EBS(w, λ, N, t) = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + s w2 0 λ0 + (1 − w0)2 1 − λ0 B ≤ w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B, 33Published as a conference paper at ICLR 2024 Algorithm 2 INCREMENTAL CLUSTERING (IC) Require: Given previously selected anchors, new unlabeled samples, and the cluster budget as Danc, Unew, and NC . Global anchor weights wanc = (wanc 1 , . . . , wanc |Danc|)⊤. 1: For simplicity, we consider anchor weights wanc as a global vector. 2: function IC(Danc, Unew, NC ) 3: wsp ← Concat(wanc, 1⊤ |Unew|) ▷ Assign all new samples with weight 1. 4: Φ ← Extract the features from the penultimate layer of model f on x ∈ Danc ∪ Unew in order. 5: clusters ← Weighted-K-Means(Φ, wsp, NC) 6: new_clusters ← {clusteri | ∀clusteri ∈ clusters, ∀x ∈ Danc, x /∈ clustersi} 7: Xnew_anchors ← {the closest sample x to the centroid of clusteri | ∀clusteri ∈ new_clusters} 8: Xanchors ← {x ∈ Danc} ∪Xnew_anchors 9: wanc ← Concat(wanc, 0⊤ |Xnew_anchors|) ▷ Initialize new anchor weights. 10: for wanc i ∈ wanc, wanc i ← wanc i + # sample of clusterj # anchor in clusterj , wanc i ∈ clusterj ▷ Weight accumulation. 11: Return Xanchors 12: end function where the minimum value can be obtained with condition w0 = λ0 ̸= 0. In practical learning scenarios, we generally assume adaptation tasks are solvable; therefore, there should be a prediction function that performs well on two distinct domains. In this case, γ and γ′ should be relatively small, so we can assume γ ≈ γ′. If ˆdH∆H(S0, SS) < ˆdH∆H(SS, ST ), then we have EBS(w, λ, N, t)min = w0( ˆdH∆H(S0, SS) + 4 s 2d log(2m) + log 2 δ m + 2γ) + (1 − w0)( ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′) + B < ˆdH∆H(SS, ST ) + 4 s 2d log(2m) + log 2 δ m + 2γ′ + B = EBS(w′, λ′, N, t). Therefore, we derive EBS(w, λ, N, t)min < EBS(w′, λ′, N, t). (32) This completes the proof. F I NCREMENTAL CLUSTERING F.1 A LGORITHM DETAILS We provide the detailed algorithm for incremental clustering as Alg. 2. F.2 V ISUALIZATION To better illustrate the incremental clustering algorithm, we provide visualization results on PACS to demonstrate the process. As shown in Fig. 3, the initial step of IC is a normal K-Means clustering step, and ten anchors denoted as \"X\" are selected. The weights of all samples in a clusters is aggregated into the corresponding anchor’s weight. Therefore, these ten samples (anchors) are given larger sizes visually (i.e., larger weights) than that of other new test samples in the first IC step (Fig. 4). During the first IC step, several distributions are far away from the existed anchors and form clusters 1,7,9 and 10, which leads to 4 new selected anchors. While the number of cluster centroid is only increased by 1, 4 of the existing anchors are clustered into the same cluster 8 (purple). Thus IC produces 4 new anchors instead of 1. Similarly, in the second IC step (Fig. 5), the new streaming-in test samples introduce a new distribution; IC produces 3 new clusters (4, 8, and 11) and the corresponding number of anchors to cover them. The number of centroid is only increased by 1, which implies that there are two original-cluster-merging events. More IC step visualization results are provided in Fig. 6 and 7. 34Published as a conference paper at ICLR 2024 Figure 3: Initial IC step: normal clustering. Left: Clustering results. Right: Selecting new anchors. Figure 4: The first IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 5: The second IC step. Left: Weighted clustering results. Right: Selecting new anchors. 35Published as a conference paper at ICLR 2024 Figure 6: The third IC step. Left: Weighted clustering results. Right: Selecting new anchors. Figure 7: The fourth IC step. Left: Weighted clustering results. Right: Selecting new anchors. 36Published as a conference paper at ICLR 2024 G E XPERIMENT DETAILS In this section, we provide more experimental details including the details of the datasets and training settings. G.1 D ETAILS ABOUT THE DATASETS We adopt datasets PACS, VLCS, and Office-Home from DomainBed (Gulrajani and Lopez-Paz, 2020) with the same domain splits. All available licenses are mentioned below. • PACS (Li et al., 2017) includes four domains: art, cartoons, photos, and sketches. PACS is a 7-class classification dataset with 9,991 images of dimension (3, 224, 224). • VLCS (Fang et al., 2013) contains photographic domains: Caltech101, LabelMe, SUN09, and VOC2007. This dataset includes 10,729 images of dimension (3, 224, 224) with 5 classes. • Office-Home (Venkateswara et al., 2017) is a 65-class dataset, including domains: art, clipart, product, and real. VLCS includes 10,729 images of dimension (3, 224, 244). (License) • Tiny-ImageNet-C is a 200-class dataset, including 15 corrupt types. Tiny-ImageNet-C includes 150,000 images of dimension (3, 224, 244). Since the class number 200 is less than ImageNet (1000), the model’s last layer classifier needs to be adapted. In this work, we use the brightness corruption domain to adapt. In the source pretraining phase, we adopt the most ImageNet-like domain as our source domain. For PACS and Office-Home, we use domains \"photos\" and \"real\" as the source domains, respectively, while for VLCS, Caltech101 is assigned to apply the source pretraining. We freeze the random seeds to generate the sample indices order for the two test data streams, namely, the domain-wise data stream and the random data stream. For PACS, the domain-wise data stream inputs samples from domain art, cartoons, to sketches, while we shuffle all samples from these three domains in the random data stream. For VLCS, we stream the domains in the order: LabelMe, SUN09, and VOC2007, as the domain-wise data stream. For Office-Home, the domain-wise data stream order becomes art, clipart, and product. G.2 T RAINING AND OPTIMIZATION SETTINGS In this section, we extensively discuss the model architectures, optimization settings, and method settings. G.2.1 A RCHITECTURES PACS & VLCS. We adopt ResNet-18 as our model encoder followed by a linear classifier. The initial parameters of ResNet-18 are ImageNet pre-trained weights. In our experiment, we remove the Dropout layer since we empirically found that using the Dropout layer might degrade the optimization process when the sample number is small. The specific implementation of the network is closely aligned with the implementation in DomainBed (Gulrajani and Lopez-Paz, 2020). Office-Home. We employ ResNet-50 as our model encoder for Office-Home. Except for the architecture, the other model settings are aligned with the ResNet-18. Tiny-ImageNet-C ResNet-18 is adapted from ImageNet to Tiny-ImageNet-C by training the last linear layer. G.2.2 T RAINING & OPTIMIZATION In this section, we describe the training configurations for both the source domain pre-training and test-time adaptation procedures. Source domain pre-training. For the PACS and VLCS datasets, models are fine-tuned on the selected source domains for 3,000 iterations. The Adam optimizer is utilized with a learning rate 37Published as a conference paper at ICLR 2024 of 10−4. In contrast, for the Office-Home dataset, the model is fine-tuned for a longer duration of 10,000 iterations with a slightly adjusted learning rate of 5 × 10−5. Test-time adaptation. For test-time adaptation across PACS and VLCS, the pre-trained source model is further fine-tuned using the SGD optimizer with a learning rate of 10−3. While on Office-Home and Tiny-ImageNet-C, a learning rate of 10−4 is adopted. For all TTA baselines, barring specific exceptions, we faithfully adhere to the original implementation settings. A noteworthy exception is the EATA method, which requires a cosine similarity threshold. The default threshold of the original EATA implementation was not suitable for the three datasets used in our study, necessitating an adjustment. We empirically set this threshold to 0.5 for training. Unlike Tent and SAR, which only require the optimization of batch normalization layers (Santurkar et al., 2018), SimATTA allows the training of all parameters in the networks. In experiments, we use a tolerance count (tol) to control the training process. SimATTA will stop updating once the loss does not descrease for more than 5 steps. However, for Tiny-ImageNet-C, SimATTA uses ‘steps=10‘ for time comparisons since other methods apply at most 10 steps. G.2.3 M ETHOD SETTINGS Tent. In our experiments, we apply the official implementation of Tent1. Specifically, we evaluate Tent with 1 test-time training step and 10 steps, respectively. EATA.Our EATA implementation follows its official code2. In our experiments, EATA has 2000 fisher training samples, E0 = 0.4 × log(# class), ϵ <0.5. CoTTA. For CoTTA, we strictly follow all the code and settings from its official implementation3. SAR. With SAR’s official implementation4, we set E0 = 0 .4 × log(# class) and e0 = 0 .1 in our experiments. ADA baselines. For ADA baselines, we follow the architecture of the official implementation of CLUE (Prabhu et al., 2021)5. SimATTA Implementation. Our implementation largely involves straightforward hyperparameter settings. The higher entropy bound eh = 10−2 should exceed the lower entropy bound el, but equal values are acceptable. Empirically, the lower entropy bound el can be set to 10−3 for VLCS and Office-Home, or 10−4 for PACS. The choice of el is largely dependent on the number of source-like samples obtained. A lower el may yield higher-accuracy low-entropy samples, but this could lead to unstable training due to sample scarcity. Though experimentation with different hyperparameters is encouraged, our findings suggest that maintaining a non-trivial number of low-entropy samples and setting an appropriateλ0 are of primary importance. If λ0 < 0.5, CF may ensue, which may negate any potential improvement. Regarding the management of budgets, numerous strategies can be adopted. In our experiments, we utilized a simple hyperparameter k, varying from 1 to 3, to regulate the increasing rate of budget consumption. This strategy is fairly elementary and can be substituted by any adaptive techniques. G.3 S OFTWARE AND HARDWARE We conduct our experiments with PyTorch (Paszke et al., 2019) and scikit-learn (Pedregosa et al., 2011) on Ubuntu 20.04. The Ubuntu server includes 112 Intel(R) Xeon(R) Gold 6258R CPU @2.70GHz, 1.47TB memory, and NVIDIA A100 80GB PCIe graphics cards. The training process costs graphics memory less than 10GB, and it requires CPU computational resources for scikit-learn K-Means clustering calculations. Our implementation also includes a GPU-based PyTorch K-Means method for transferring calculation loads from CPUs to GPUs. However, for consistency, the results of our experiments are obtained with the original scikit-learn K-Means implementation. 1https://github.com/DequanWang/tent 2https://github.com/mr-eggplant/EATA 3https://github.com/qinenergy/cotta 4https://github.com/mr-eggplant/SAR 5https://github.com/virajprabhu/CLUE 38Published as a conference paper at ICLR 2024 Figure 8: Target loss surface on 2000 samples without source pre-training. The red points denote the loss minimum for a fixed λ0. The orange line denote the place where w0 = λ0. Figure 9: Target loss surface on 2000 samples with source pre-training. H E MPIRICAL VALIDATIONS FOR THEORETICAL ANALYSIS In this section, we undertake empirical validation of our learning theory, which encompasses multiple facets awaiting verification. In contemporary computer vision fields, pre-trained models play a pivotal role, and performance would significantly decline without the use of pre-trained features. The learning theory suggests that given the vast VC-dimension of complete ResNets, without substantial data samples, the training error cannot be theoretically tight-bounded. However, we show empirically in the following experiments that fine-tuning pre-trained models is behaviorally akin to training a model with a low VC-dimension. Training on 2000 Samples Without Source Domain Pre-training. For an ImageNet pre-trained ResNet-18 model, we trained it using 2000 samples from the PACS dataset. To ascertain the optimal value w∗ 0 in Equation 4, we trained multiple models for different w0 and λ0 pairings. For each pair, we derived the target domain loss (from art, cartoons, and sketches) post-training and plotted this loss on the z-axis. With w0 and λ0 serving as the xy-axes, we drafted the target domain loss ϵT surface in Figure 8. As the results show, given a λ0, the optimal w∗ 0 typically aligns with the line λ0 = w0, with a slight downward shift, which aligns with Equation 4. 39Published as a conference paper at ICLR 2024 Figure 10: Target loss surface on 500 samples with source pre-training. Figure 11: Source loss surface on 500 samples with source pre-training. 40Published as a conference paper at ICLR 2024 Figure 12: Target and source loss surface on 500 samples with source pre-training. Table 6: TTA comparisons on Office-Home. This table includes the two data stream settings mentioned in the dataset setup and reports performances in accuracy. Results that outperform all TTA baselines are highlighted in bold font. N/A denotes the adaptations are not applied on the source domain. Office-Home Domain-wise data stream Post-adaptation Random data stream Post-adaptation R →A→ →C→ →P R A C P 1 2 3 4 R A C P BN w/o adapt 93.78 42.93 37.62 59.90 93.78 42.93 37.62 59.90 46.82 46.82 46.82 46.82 93.78 42.93 37.62 59.90BN w/ adapt 92.38 49.69 39.43 63.53 92.38 49.69 39.43 63.53 50.88 50.88 50.88 50.88 92.38 49.69 39.43 63.53 Tent (steps=1) N/A 49.61 39.31 63.87 92.47 49.57 39.89 63.89 49.95 50.27 50.23 52.06 92.40 49.24 39.68 63.98Tent (steps=10) N/A 49.61 39.04 61.41 87.08 44.79 38.37 60.49 50.05 49.31 48.74 47.79 85.31 42.85 37.89 58.71EATA N/A 49.65 39.04 63.53 91.60 49.61 38.65 63.48 49.73 50.27 49.45 51.07 91.05 49.11 38.26 62.99CoTTA N/A 49.61 38.76 61.84 87.81 44.95 35.92 59.04 49.84 49.84 48.95 50.43 86.99 43.68 34.73 57.56SAR (steps=1) N/A 49.65 39.24 63.53 92.45 49.73 39.36 63.69 49.84 50.05 49.91 51.67 92.38 49.57 39.50 63.87SAR (steps=10) N/A 49.53 38.81 61.50 88.94 46.15 37.04 59.41 50.09 50.30 49.77 49.22 89.14 46.23 36.31 59.45 SimATTA (B ≤300) N/A 56.20 48.38 71.66 95.75 60.07 52.62 74.70 58.57 60.88 62.91 63.67 95.89 62.01 54.98 74.70SimATTA (B ≤500) N/A 58.71 51.11 74.36 96.03 62.05 57.41 76.98 58.85 62.63 63.41 64.31 95.91 63.78 57.87 77.09 Training on 2000 Samples with Source Domain Pre-training. To further assess the effects of source pre-training, we repeated the same experiment on a source pre-trained ResNet-18. The results are depicted in Figure 9. This experiment provides empirical guidance on selecting w0 in source domain pre-trained situations. The findings suggest that the optimal w∗ 0 non-trivially shifts away from the line λ0 = w0 towards lower-value regions. Considering the source pre-training process as using a greater quantity of source domain samples, it implies that when the number of source samples greatly exceeds target samples, a lower w0 can enhance target domain results. Training on 500 Samples with Source Domain Pre-training. We proceed to fine-tune the source domain pre-trained ResNet-18 using only 500 samples, thereby simulating active TTA settings. We train models with various w0 and λ0 pairings, then graph the target domain losses, source domain losses, and the combined losses. As shown in Figure 10, the target losses still comply with our theoretical deductions where the local minima are close to the line λ0 = w0 and marginally shift towards lower values. Considering the challenge of CF, the source domain results in Figure 11 suggest a reverse trend compared to the target domain, where lower λ0 and w0 values yield superior target domain results but inferior source domain results. Thus, to curb CF, the primary strategy is to maintain a relatively higher λ0. When considering both target and source domains, a balance emerges as depicted in Figure 12. The global minimum is located in the middle region, demonstrating the trade-off between the target domain and source domain performance. I A DDITIONAL EXPERIMENT RESULTS In this section, we provide additional experiment results. The Office-Home results and ablation studies will be presented in a similar way as the main paper. In the full results Sec. I.3, we will post more detailed experimental results with specific budget numbers and intermediate performance during the test-time adaptation. 41Published as a conference paper at ICLR 2024 Table 7: Comparisons to ADA baselines on Office-Home. The source domain is denoted as \"(S)\" in the table. Results are average accuracies with standard deviations). Office-Home R (S) A C P Random (B = 300) 95.04 (0.20) 57.54 (1.16) 53.43 (1.17) 73.46 (0.97) Entropy (B = 300) 94.39 (0.49) 61.21 (0.71) 56.53 (0.71) 72.31 (0.28) Kmeans (B = 300) 95.09 (0.14) 57.37 (0.90) 51.74 (1.34) 71.81 (0.39) CLUE (B = 300) 95.20 (0.23) 60.18 (0.98) 58.05 (0.43) 73.72 (0.70) Ours (B ≤300) 95.82 (0.07) 61.04 (0.97) 53.80 (1.18) 74.70 (0.00) I.1 R ESULTS ON OFFICE -HOME We conduct experiments on Office-Home and get the test-time performances and post-adaptation performances for two data streams. As shown in Tab. 6, SimATTA can outperform all TTA baselines with huge margins. Compared to ADA baselines under the source-free settings, as shown in Tab. 7, SimATTA obtains comparable results. I.2 A BLATION STUDIES Figure 13: Ablation study on PACS and VLCS.\"IC=0\" denotes removing incremental clustering (IC) selection. \"LE=0\" denotes removing the low-entropy (LE) sample training. Domain-wise stream and random stream are applied on first and second rows, respectively. The accuracy values are averaged across all splits/domains. In this section, we explore three variations of our method to examine the individual impacts of its components. The first variant replaces the incremental clustering selection with entropy selection, 42Published as a conference paper at ICLR 2024 where only the samples with the highest entropy are chosen. The second variant eliminates low- entropy sample training. The third variation combines the first and second variants. We perform this ablation study on the PACS and VLCS as outlined in Fig. 13. We denote the use of incremental clustering (IC) and low-entropy training (LE) respectively as IC=1 and LE=1. The experiments essentially reveals the effectiveness of incremental clustering and low-entropy- sample training. As we have detailed in Sec. 3.2, these techniques are designed to to select informative samples, increase distribution coverage, and mitigate catastrophic forgetting. These designs appositely serve the ATTA setting where the oracle has costs and the budget is limited. Therefore, their effectiveness is prominent particularly when the budget is small. As the results show, when the budget B ≤100 or B ≤300, removing the components observably impairs performances. When B gets large, more active samples cover a larger distribution; thus the performance gap from random selection and informative selection gets smaller. In the extreme case where B → ∞, all samples are selected and thus the superiority of our meticulously-designed techniques are not manifested. Specifically, our analysis yields several insights. First, SimATTA (LE=1, IC=1) comprehensively outperforms other variants on both datasets, different streams, and different budgets. Second, variants without low-entropy training (LE=0, IC=0/1) easily fail to produce stable results (e.g., domain-wise stream in VLCS). Third, SimATTA’s performance surpasses this variant on PACS’s domain-wise stream clearly especially when the budgets are low. This indicates these variants fail to retrieve the most informative style shift (PACS’s shifts) samples, which implies the advantage of incremental clustering when the budget is tight. In addition, these results show that IC has its unique advantage on domain-wise streams where distributions change abruptly instead of random streams. Therefore, compared to PACS’s domain- wise stream results, the reason for the smaller performance improvement of SimATTA over the variant (LE=1, IC=0) on VLCS’s domain-wise stream is that images in VLCS are all photos that do not include those severe style shifts in PACS (i.e., art, cartoons, and sketches). That is, when the shift is not severe, we don’t need IC to cover very different distributions, and selecting samples using entropy can produce good results. In brief, IC is extraordinary for severe distribution shifts and quick adaptation. It is worth mentioning that low budget comparison is essential to show the informative sample retrieval ability, since as the budget increases, all AL techniques will tend to perform closely. I.3 C OMPLETE EXPERIMENT RESULTS We provide complete experimental results in this section. As shown in Tab. 8, we present the full results for two data streams. The test-time adaptation accuracies are shown in the \"Current domain\" row, while the \"Budgets\" row denotes the used budget by the end of the domain. The rest four rows denote the four domain test results by the end of the real-time adaptation of the current domain, where the first column results are the test accuracy before the test-time adaptation phase. N/A represents \"do not apply\". Table 8: Tent (steps=1) on PACS. Tent (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.29 64.59 44.67 56.35 54.09 51.83 48.58 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.38 97.60 98.56 98.08 97.72 97.19 A 59.38 69.09 68.95 66.85 68.07 67.33 65.58 63.53 C 28.03 64.04 65.19 64.08 64.85 65.19 62.97 60.75 S 42.91 53.65 47.39 42.58 54.57 49.83 44.13 41.56 J C HALLENGES AND PERSPECTIVES Despite advancements, test-time adaptation continues to pose considerable challenges. As previously discussed, without supplementary information and assumptions, the ability to guarantee model generalization capabilities is limited. However, this is not unexpected given that recent progress 43Published as a conference paper at ICLR 2024 Table 9: Tent (steps=10) on PACS. Tent (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.38 57.85 20.23 47.36 31.01 22.84 20.33 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 95.45 87.43 62.63 93.83 81.32 65.39 50.78 A 59.38 64.94 55.03 34.52 55.32 40.28 28.27 23.68 C 28.03 55.89 56.70 40.57 54.52 39.68 27.22 20.95 S 42.91 36.96 26.27 13.59 32.25 23.16 20.95 19.62 Table 10: EATA on PACS. EATA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 67.04 64.72 50.27 57.31 56.06 58.17 59.78 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.62 98.50 98.62 98.68 98.62 98.50 98.62 A 59.38 68.90 68.16 66.50 68.65 68.95 69.34 69.63 C 28.03 63.74 65.36 62.46 65.19 66.00 65.57 65.70 S 42.91 54.01 52.89 48.18 55.71 55.64 54.09 54.26 Table 11: CoTTA on PACS. CoTTA Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 65.48 62.12 53.17 56.06 54.33 57.16 57.42 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.62 98.62 98.62 98.62 98.56 98.62 A 59.38 65.82 65.87 65.48 66.02 65.87 66.31 65.97 C 28.03 62.63 63.05 63.10 63.01 62.88 63.01 62.97 S 42.91 53.88 54.03 53.78 54.67 55.31 55.10 54.62 Table 12: SAR (steps=1) on PACS. SAR (steps=1) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 66.75 63.82 49.58 56.78 56.35 56.68 56.70 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.68 98.50 98.32 98.74 98.56 98.50 98.44 A 59.38 68.02 68.07 66.94 67.87 68.65 68.55 68.16 C 28.03 62.84 64.97 62.93 63.82 64.89 64.46 64.38 S 42.91 53.47 52.07 45.74 54.92 55.46 53.68 52.53 Table 13: SAR (steps=10) on PACS. SAR (steps=10) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 69.38 68.26 49.02 53.51 51.15 51.78 45.60 Budgets N/A N/A N/A N/A N/A N/A N/A N/A P 99.70 98.20 95.39 96.47 97.13 97.78 97.72 94.13 A 59.38 72.36 66.60 62.16 62.74 64.94 66.11 56.64 C 28.03 63.44 68.30 56.19 59.77 61.73 62.03 56.02 S 42.91 53.37 44.59 54.62 41.00 49.66 48.79 36.37 44Published as a conference paper at ICLR 2024 Table 14: SimATTA (B ≤300) on PACS. SimATTA (B ≤300) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 76.86 70.90 75.39 69.47 76.49 82.45 82.22 Budgets N/A 75 145 223 66 142 203 267 P 99.70 98.44 98.86 98.80 97.96 98.68 99.04 98.98 A 59.38 80.71 82.32 84.47 73.97 80.52 81.10 84.91 C 28.03 48.12 82.00 82.25 72.35 81.06 83.36 83.92 S 42.91 32.78 56.25 81.52 79.49 83.10 84.78 86.00 Table 15: SimATTA (B ≤500) on PACS. SimATTA (B ≤500) Domain-wise data stream Random data stream P →A→ → C→ → S 1 → → 2→ → 3→ → 4→ Current domain N/A 77.93 76.02 76.30 68.46 78.22 80.91 85.49 Budgets N/A 121 230 358 102 221 343 425 P 99.70 98.92 98.86 98.62 98.20 99.46 99.10 99.16 A 59.38 87.01 87.60 88.33 73.39 79.20 84.91 86.67 C 28.03 54.78 83.96 83.49 68.43 74.40 84.22 84.77 S 42.91 46.37 63.53 83.74 81.34 81.04 86.66 87.71 Table 16: Tent (steps=1) on VLCS. Tent (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 38.55 34.40 53.88 44.85 44.29 47.38 44.98 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.81 85.44 84.73 84.95 85.16 85.80 85.30 L 33.55 40.02 43.11 43.86 39.68 41.98 43.11 43.49 S 41.10 33.39 35.41 33.61 36.29 37.90 38.27 37.81 V 49.08 53.20 54.06 53.11 53.76 54.18 53.76 53.35 Table 17: Tent (steps=10) on VLCS. Tent (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 45.41 31.44 32.32 46.13 42.31 43.51 39.48 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 73.07 48.34 42.54 74.13 62.19 56.54 52.01 L 33.55 46.61 38.44 37.65 44.88 45.93 43.41 40.32 S 41.10 31.75 28.82 27.79 35.37 36.14 35.28 33.64 V 49.08 48.05 40.14 33.12 50.50 44.49 42.48 40.37 Table 18: EATA on VLCS. EATA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.24 33.15 52.58 43.77 42.48 43.34 41.55 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 85.16 85.02 84.10 84.73 84.52 84.10 83.32 L 33.55 37.16 37.24 37.69 37.09 36.78 36.90 36.67 S 41.10 33.39 33.49 32.39 33.33 32.54 31.84 31.47 V 49.08 51.87 52.16 52.49 52.07 52.43 52.64 52.55 45Published as a conference paper at ICLR 2024 Table 19: CoTTA on VLCS. CoTTA Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 37.39 32.54 52.25 43.69 42.14 43.21 42.32 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 81.55 81.98 82.12 82.61 82.47 82.12 81.98 L 33.55 37.20 37.91 37.65 38.48 38.22 38.40 37.99 S 41.10 30.71 32.78 33.12 34.00 33.70 33.97 33.52 V 49.08 52.01 52.64 52.90 53.64 53.14 53.08 53.23 Table 20: SAR (steps=1) on VLCS. SAR (steps=1) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 36.18 34.43 52.46 43.64 43.04 44.20 41.93 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 84.31 84.17 83.96 85.09 85.23 85.23 85.09 L 33.55 35.62 38.29 39.72 38.55 39.34 40.21 40.70 S 41.10 33.24 36.41 36.53 34.37 35.62 36.29 36.44 V 49.08 51.75 52.61 52.37 52.90 52.75 53.05 53.02 Table 21: SAR (steps=10) on VLCS. SAR (steps=10) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 35.32 34.10 51.66 43.56 42.05 42.53 41.16 Budgets N/A N/A N/A N/A N/A N/A N/A N/A C 100.00 83.96 83.04 82.12 84.03 84.24 85.23 85.09 L 33.55 34.07 35.92 41.49 39.53 38.37 37.65 37.58 S 41.10 31.93 34.89 33.94 35.19 32.94 33.88 33.12 V 49.08 51.33 51.51 53.08 52.78 52.34 51.78 52.01 Table 22: SimATTA (B ≤300) on VLCS. SimATTA (B ≤300) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 62.61 65.08 74.38 62.33 69.33 73.20 71.93 Budgets N/A 79 175 272 71 135 208 262 C 100.00 99.51 98.52 99.93 99.86 99.79 100.00 99.93 L 33.55 68.11 69.92 69.50 62.61 66.64 68.45 69.43 S 41.10 55.24 68.89 66.67 65.54 69.29 71.79 72.46 V 49.08 66.08 70.94 77.34 73.79 76.87 78.82 80.39 Table 23: SimATTA (B ≤500) on VLCS. SimATTA (B ≤500) Domain-wise data stream Random data stream C →L→ → S→ → V 1 → → 2→ → 3→ → 4→ Current domain N/A 63.52 68.01 76.13 62.29 70.45 73.50 72.02 Budgets N/A 113 266 446 107 203 283 356 C 100.00 99.29 98.59 99.51 99.93 99.86 99.86 99.43 L 33.55 62.95 70.63 70.56 66.57 67.09 67.24 70.29 S 41.10 51.31 73.83 73.10 65.33 71.79 72.91 72.55 V 49.08 59.36 71.65 78.35 73.58 77.84 80.01 80.18 46Published as a conference paper at ICLR 2024 Table 24: Tent (steps=1) on Office-Home. Tent (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.31 63.87 49.95 50.27 50.23 52.06 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.33 92.36 92.47 92.38 92.45 92.45 92.40 A 57.07 49.73 49.73 49.57 49.69 49.73 49.57 49.24 C 44.97 39.27 39.54 39.89 39.45 39.68 39.73 39.68 P 73.15 63.60 63.66 63.89 63.60 63.82 63.93 63.98 Table 25: Tent (steps=10) on Office-Home. Tent (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 39.04 61.41 50.05 49.31 48.74 47.79 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 91.99 89.14 87.08 92.08 90.80 88.59 85.31 A 57.07 49.94 46.77 44.79 49.44 48.21 45.69 42.85 C 44.97 38.58 39.11 38.37 40.18 40.02 38.63 37.89 P 73.15 63.28 61.03 60.49 64.36 63.64 61.12 58.71 Table 26: EATA on Office-Home. EATA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.04 63.53 49.73 50.27 49.45 51.07 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.36 92.17 91.60 92.38 92.22 91.71 91.05 A 57.07 49.57 49.53 49.61 49.69 49.40 49.36 49.11 C 44.97 39.08 39.01 38.65 39.27 39.01 38.42 38.26 P 73.15 63.42 63.42 63.48 63.51 63.37 63.33 62.99 Table 27: CoTTA on Office-Home. CoTTA Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.61 38.76 61.84 49.84 49.84 48.95 50.43 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 90.38 88.02 87.81 90.48 89.37 88.00 86.99 A 57.07 48.58 45.53 44.95 47.34 46.35 44.62 43.68 C 44.97 36.66 35.58 35.92 37.55 36.40 35.44 34.73 P 73.15 60.40 57.74 59.04 61.12 59.63 58.35 57.56 Table 28: SAR (steps=1) on Office-Home. SAR (steps=1) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.65 39.24 63.53 49.84 50.05 49.91 51.67 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.38 92.31 92.45 92.40 92.36 92.36 92.38 A 57.07 49.65 49.57 49.73 49.69 49.61 49.57 49.57 C 44.97 39.34 39.22 39.36 39.34 39.56 39.47 39.50 P 73.15 63.51 63.51 63.69 63.60 63.71 63.71 63.87 47Published as a conference paper at ICLR 2024 Table 29: SAR (steps=10) on Office-Home. SAR (steps=10) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 49.53 38.81 61.50 50.09 50.30 49.77 49.22 Budgets N/A N/A N/A N/A N/A N/A N/A N/A R 96.44 92.20 92.06 88.94 92.40 92.47 91.53 89.14 A 57.07 49.40 49.77 46.15 49.81 50.02 48.91 46.23 C 44.97 39.20 38.63 37.04 39.50 39.29 38.65 36.31 P 73.15 63.53 62.69 59.41 64.18 64.18 62.83 59.45 Table 30: SimATTA (B ≤300) on Office-Home. SimATTA (B ≤300) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 56.20 48.38 71.66 58.57 60.88 62.91 63.67 Budgets N/A 75 187 277 79 147 216 278 R 96.44 95.43 95.43 95.75 95.91 95.96 96.01 95.89 A 57.07 57.56 59.50 60.07 58.34 59.91 61.15 62.01 C 44.97 42.25 52.46 52.62 51.66 52.30 54.75 54.98 P 73.15 68.84 70.13 74.70 72.45 73.10 74.50 74.70 Table 31: SimATTA (B ≤500) on Office-Home. SimATTA (B ≤500) Domain-wise data stream Random data stream R →A→ → C→ → P 1 → → 2→ → 3→ → 4→ Current domain N/A 58.71 51.11 74.36 58.85 62.63 63.41 64.31 Budgets N/A 107 284 440 126 248 361 467 R 96.44 95.69 95.71 96.03 96.26 96.19 95.87 95.91 A 57.07 61.43 61.43 62.05 58.18 61.15 61.52 63.78 C 44.97 46.41 57.73 57.41 53.17 55.14 56.79 57.87 P 73.15 70.74 71.98 76.98 73.51 74.18 75.78 77.09 48Published as a conference paper at ICLR 2024 in deep learning heavily relies on large-scale data. Consequently, two promising paths emerge: establishing credible assumptions and leveraging additional information. Firstly, developing credible assumptions can lead to comprehensive comparisons across various stud- ies. Given that theoretical guarantees highlight the inherent differences between methods primarily based on the application limits of their assumptions, comparing these assumptions becomes critical. Without such comparative studies, empirical evaluations may lack precise guidance and explanation. Secondly, while we acknowledge the value of real-world data (observations), discussions surrounding the use of extra information remain pertinent. Considerations include the strategies to acquire this supplementary information and the nature of the additional data needed. Despite the myriad of works on domain generalization, domain adaptation, and test-time adaptation, a comprehensive survey or benchmark encapsulating the aforementioned comparisons remains an unmet need. Moreover, potential future directions for out-of-distribution generalization extend beyond domain generalization and test-time adaptation. One promising avenue is bridging the gap between causal inference and deep learning, for instance, through causal representation learning. In conclusion, our hope is that this work not only offers a novel practical setting and algorithm but also illuminates meaningful future directions and research methodologies that can benefit the broader scientific community. 49",
      "meta_data": {
        "arxiv_id": "2404.05094v1",
        "authors": [
          "Shurui Gui",
          "Xiner Li",
          "Shuiwang Ji"
        ],
        "published_date": "2024-04-07T22:31:34Z",
        "pdf_url": "https://arxiv.org/pdf/2404.05094v1.pdf",
        "github_url": "https://github.com/divelab/ATTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Active Test-Time Adaptation (ATTA), a novel problem setting that integrates active learning within the fully Test-Time Adaptation (FTTA) framework to address significant domain distribution shifts in real-time. It provides a learning theory analysis, demonstrating that incorporating limited labeled test instances improves overall performance with theoretical guarantees. The paper also proposes a sample entropy balancing technique to prevent catastrophic forgetting (CF) in ATTA. Finally, it presents SimATTA, a simple yet effective ATTA algorithm utilizing real-time sample selection, which shows substantial performance improvements over TTA methods and comparable effectiveness to Active Domain Adaptation (ADA) methods while maintaining efficiency.",
        "methodology": "The ATTA problem is formally defined, where a pre-trained model continuously selects informative instances from streaming test data within a budget to be labeled by an oracle and subsequently learned. The theoretical analysis establishes learning bounds for mitigating distribution shifts and avoiding catastrophic forgetting (CF). This involves scrutinizing the influence of active learning and using selective entropy minimization to identify 'source-like' low-entropy samples (pseudo-labeled by a frozen source-pretrained model) and informative high-entropy samples. The SimATTA algorithm implements this by partitioning unlabeled test samples into high and low entropy sets. High-entropy samples are actively selected using an incremental clustering technique (based on weighted K-means) to reduce redundancy and increase distribution coverage. The model is then fine-tuned using these labeled test anchors and pseudo-labeled source-like anchors, balancing training weights and sample numbers.",
        "experimental_setup": "The method is evaluated against three settings: TTA, Enhanced TTA (TTA with access to random labeled samples), and ADA. Experiments are conducted on PACS, VLCS, Office-Home (from DomainBed), and Tiny-ImageNet-C datasets to assess OOD performance and efficiency. Source domains are 'photos' for PACS, 'Caltech101' for VLCS, and 'real' for Office-Home, with brightness corruption for Tiny-ImageNet-C. Two data stream orders are used: domain-wise and random. Baselines include source-only models (BN w/o adapt, BN w/ adapt), state-of-the-art TTA methods (Tent, EATA, CoTTA, SAR), and ADA methods (random, entropy, k-means, CLUE). ResNet-18 (PACS, VLCS, Tiny-ImageNet-C) and ResNet-50 (Office-Home) are used, initialized with ImageNet pre-trained weights. Optimization uses Adam for source pre-training and SGD for test-time adaptation, with specific learning rates and training step controls. Ablation studies analyze the impact of incremental clustering and low-entropy sample training.",
        "limitations": "The theoretical bounds can be loose with small unlabeled test sample sizes, addressed by assuming fine-tuning is equivalent to learning with a small VC-dimension. The effectiveness of selective entropy minimization relies on the quality of the pre-trained model, and training on incorrectly predicted low-entropy samples might reinforce errors. It may not be cost-effective to use annotation budgets for low-entropy samples. The current work does not cover class-incremental problems where the support of labels changes. The incremental clustering's centroid update strategy is 'naive' and could be improved with adaptive techniques. The paper focuses on establishing a foundational framework rather than large-scale applications or extensive hyperparameter tuning for all possible scenarios.",
        "future_research_directions": "Future work could explore developing alternative strategies to prevent catastrophic forgetting in ATTA scenarios, such as correcting incorrectly predicted low-entropy samples instead of solely relying on pseudo-labeling. There is considerable scope for designing ATTA methods for large language models (LLMs), where retraining is computationally expensive and source data may be inaccessible. Further research could bridge the gap between causal inference and deep learning (e.g., through causal representation learning) to enhance OOD generalization. Additionally, a comprehensive survey or benchmark that systematically compares domain generalization, domain adaptation, and test-time adaptation methods based on their underlying assumptions is suggested.",
        "experimental_code": "import copy\nimport pathlib\nimport time\nfrom typing import Union\n\nimport numpy as np\n# from sklearnex import patch_sklearn, config_context\n# patch_sklearn()\n\n# from sklearn.cluster import KMeans\n# from ATTA.utils.fast_pytorch_kmeans import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom typing import Literal\n\nfrom torch import nn\nimport torch\n# import models for resnet18\nfrom munch import Munch\nfrom ATTA import register\nfrom ATTA.utils.config_reader import Conf\nfrom ATTA.data.loaders.fast_data_loader import InfiniteDataLoader, FastDataLoader\nfrom torch.utils.data import TensorDataset\nfrom tqdm import tqdm\nfrom .Base import AlgBase\nimport pandas as pd\nfrom ATTA.definitions import STORAGE_DIR\n\n\n\n@register.alg_register\nclass SimATTA(AlgBase):\n    def __init__(self, config: Conf):\n        super(SimATTA, self).__init__(config)\n\n        self.teacher = copy.deepcopy(self.model.to('cpu'))\n\n        self.model.to(config.device)\n        self.teacher.to(config.device)\n        self.update_teacher(0)  # copy student to teacher\n\n        self.budgets = 0\n        self.anchors = None\n        self.source_anchors = None\n        self.buffer = []\n        self.n_clusters = 10\n        self.nc_increase = self.config.atta.SimATTA.nc_increase\n        self.source_n_clusters = 100\n\n        self.cold_start = self.config.atta.SimATTA.cold_start\n\n        self.consistency_weight = 0\n        self.alpha_teacher = 0\n        self.accumulate_weight = True\n        self.weighted_entropy: Union[Literal['low', 'high', 'both'], None] = 'both'\n        self.aggressive = True\n        self.beta = self.config.atta.SimATTA.beta\n        self.alpha = 0.2\n\n        self.target_cluster = True if self.config.atta.SimATTA.target_cluster else False\n        self.LE = True if self.config.atta.SimATTA.LE else False\n        self.vis_round = 0\n\n\n    def __call__(self, *args, **kwargs):\n        # super(SimATTA, self).__call__()\n        self.continue_result_df = pd.DataFrame(\n            index=['Current domain', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in self.config.dataset.test_envs), 'Test AVG'], dtype=float)\n        self.random_result_df = pd.DataFrame(\n            index=['Current step', 'Budgets', *(i for i in self.config.dataset.test_envs), 'Frame AVG'],\n            columns=[*(i for i in range(4)), 'Test AVG'], dtype=float)\n\n        self.enable_bn(self.model)\n        if 'ImageNet' not in self.config.dataset.name:\n            for env_id in self.config.dataset.test_envs:\n                acc = self.test_on_env(env_id)[1]\n                self.continue_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n                self.random_result_df.loc[env_id, self.config.dataset.test_envs[0]] = acc\n\n        for adapt_id in self.config.dataset.test_envs[1:]:\n            self.continue_result_df.loc['Current domain', adapt_id] = self.adapt_on_env(self.fast_loader, adapt_id)\n            self.continue_result_df.loc['Budgets', adapt_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.continue_result_df.loc[env_id, adapt_id] = self.test_on_env(env_id)[1]\n\n        self.__init__(self.config)\n        for target_split_id in range(4):\n            self.random_result_df.loc['Current step', target_split_id] = self.adapt_on_env(self.target_loader, target_split_id)\n            self.random_result_df.loc['Budgets', target_split_id] = self.budgets\n            print(self.budgets)\n            if 'ImageNet' not in self.config.dataset.name:\n                for env_id in self.config.dataset.test_envs:\n                    self.random_result_df.loc[env_id, target_split_id] = self.test_on_env(env_id)[1]\n\n        print(f'#IM#\\n{self.continue_result_df.round(4).to_markdown()}\\n'\n              f'{self.random_result_df.round(4).to_markdown()}')\n        # print(self.random_result_df.round(4).to_markdown(), '\\n')\n        self.continue_result_df.round(4).to_csv(f'{self.config.log_file}.csv')\n        self.random_result_df.round(4).to_csv(f'{self.config.log_file}.csv', mode='a')\n\n\n    @torch.no_grad()\n    def val_anchor(self, loader):\n        self.model.eval()\n        val_loss = 0\n        val_acc = 0\n        for data, target in loader:\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            output = self.fc(self.encoder(data))\n            val_loss += self.config.metric.loss_func(output, target, reduction='sum').item()\n            val_acc += self.config.metric.score_func(target, output) * len(data)\n        val_loss /= len(loader.sampler)\n        val_acc /= len(loader.sampler)\n        return val_loss, val_acc\n\n    def update_teacher(self, alpha_teacher):  # , iteration):\n        for t_param, s_param in zip(self.teacher.parameters(), self.model.parameters()):\n            t_param.data[:] = alpha_teacher * t_param[:].data[:] + (1 - alpha_teacher) * s_param[:].data[:]\n        if not self.config.model.freeze_bn:\n            for tm, m in zip(self.teacher.modules(), self.model.modules()):\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    tm.running_mean = alpha_teacher * tm.running_mean + (1 - alpha_teacher) * m.running_mean\n                    tm.running_var = alpha_teacher * tm.running_var + (1 - alpha_teacher) * m.running_var\n\n    @torch.enable_grad()\n    def cluster_train(self, target_anchors, source_anchors):\n        self.model.train()\n\n        source_loader = InfiniteDataLoader(TensorDataset(source_anchors.data, source_anchors.target), weights=None,\n                                           batch_size=self.config.train.train_bs,\n                                           num_workers=self.config.num_workers)\n        target_loader = InfiniteDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                             batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        alpha = target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())\n        if source_anchors.num_elem() < self.cold_start:\n            alpha = min(0.2, alpha)\n\n        ST_loader = iter(zip(source_loader, target_loader))\n        val_loader = FastDataLoader(TensorDataset(target_anchors.data, target_anchors.target), weights=None,\n                                    batch_size=self.config.train.train_bs, num_workers=self.config.num_workers)\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.config.atta.SimATTA.lr, momentum=0.9)\n        # print('Cluster train')\n        delay_break = False\n        loss_window = []\n        tol = 0\n        lowest_loss = float('inf')\n        for i, ((S_data, S_targets), (T_data, T_targets)) in enumerate(ST_loader):\n            S_data, S_targets = S_data.to(self.config.device), S_targets.to(self.config.device)\n            T_data, T_targets = T_data.to(self.config.device), T_targets.to(self.config.device)\n            L_T = self.one_step_train(S_data, S_targets, T_data, T_targets, alpha, optimizer)\n            # self.update_teacher(self.alpha_teacher)\n            if len(loss_window) < self.config.atta.SimATTA.stop_tol:\n                loss_window.append(L_T.item())\n            else:\n                mean_loss = np.mean(loss_window)\n                tol += 1\n                if mean_loss < lowest_loss:\n                    lowest_loss = mean_loss\n                    tol = 0\n                if tol > 5:\n                    break\n                loss_window = []\n            if 'ImageNet' in self.config.dataset.name or 'CIFAR' in self.config.dataset.name:\n                if i > self.config.atta.SimATTA.steps:\n                    break\n\n\n    def one_step_train(self, S_data, S_targets, T_data, T_targets, alpha, optimizer):\n        # print('one step train')\n        L_S = self.config.metric.loss_func(self.model(S_data), S_targets)\n        L_T = self.config.metric.loss_func(self.model(T_data), T_targets)\n        loss = (1 - alpha) * L_S + alpha * L_T\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return L_T\n\n    def softmax_entropy(self, x: torch.Tensor, y: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Entropy of softmax distribution from logits.\"\"\"\n        if y is None:\n            if x.shape[1] == 1:\n                x = torch.cat([x, -x], dim=1)\n            return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n        else:\n            return - 0.5 * (x.softmax(1) * y.log_softmax(1)).sum(1) - 0.5 * (y.softmax(1) * x.log_softmax(1)).sum(1)\n\n    def update_anchors(self, anchors, data, target, feats, weight):\n        if anchors is None:\n            anchors = Munch()\n            anchors.data = data\n            anchors.target = target\n            anchors.feats = feats\n            anchors.weight = weight\n            anchors.num_elem = lambda: len(anchors.data)\n        else:\n            anchors.data = torch.cat([anchors.data, data])\n            anchors.target = torch.cat([anchors.target, target])\n            anchors.feats = torch.cat([anchors.feats, feats])\n            anchors.weight = torch.cat([anchors.weight, weight])\n        return anchors\n\n    def update_anchors_feats(self, anchors):\n        # sequential_data = torch.arange(200)[:, None]\n        anchors_loader = FastDataLoader(TensorDataset(anchors.data), weights=None,\n                                        batch_size=32, num_workers=self.config.num_workers, sequential=True)\n\n        anchors.feats = None\n        self.model.eval()\n        for data in anchors_loader:\n            # print(data)\n            data = data[0].to(self.config.device)\n            if anchors.feats is None:\n                anchors.feats = self.model[0](data).cpu().detach()\n            else:\n                anchors.feats = torch.cat([anchors.feats, self.model[0](data).cpu().detach()])\n\n        return anchors\n\n    @torch.no_grad()\n    def adapt_on_env(self, loader, env_id):\n        # beta_func = torch.distributions.beta.Beta(0.8, 0.8)\n        acc = 0\n        for data, target in tqdm(loader[env_id]):\n            data, target = data.to(self.config.device), target.to(self.config.device)\n            outputs, closest, self.anchors = self.sample_select(self.model, data, target, self.anchors, int(self.n_clusters), 1, ent_bound=self.config.atta.SimATTA.eh, incremental_cluster=self.target_cluster)\n            acc += self.config.metric.score_func(target, outputs).item() * data.shape[0]\n            if self.LE:\n                _, _, self.source_anchors = self.sample_select(self.teacher, data, target, self.source_anchors, self.source_n_clusters, 0,\n                                                               use_pseudo_label=True, ent_bound=self.config.atta.SimATTA.el, incremental_cluster=False)\n            else:\n                self.source_anchors = self.update_anchors(None, torch.tensor([]), None, None, None)\n            if not self.target_cluster:\n                self.n_clusters = 0\n            self.source_n_clusters = 100\n\n            self.budgets += len(closest)\n            self.n_clusters += self.nc_increase\n            self.source_n_clusters += 1\n\n            print(self.anchors.num_elem(), self.source_anchors.num_elem())\n            if self.source_anchors.num_elem() > 0:\n                self.cluster_train(self.anchors, self.source_anchors)\n            else:\n                self.cluster_train(self.anchors, self.anchors)\n            self.anchors = self.update_anchors_feats(self.anchors)\n        acc /= len(loader[env_id].sampler)\n        print(f'#IN#Env {env_id} real-time Acc.: {acc:.4f}')\n        return acc\n\n    @torch.no_grad()\n    def sample_select(self, model, data, target, anchors, n_clusters, ent_beta, use_pseudo_label=False, ent_bound=1e-2, incremental_cluster=False):\n        model.eval()\n        feats = model[0](data)\n        outputs = model[1](feats)\n        pseudo_label = outputs.argmax(1).cpu().detach()\n        data = data.cpu().detach()\n        feats = feats.cpu().detach()\n        target = target.cpu().detach()\n        entropy = self.softmax_entropy(outputs).cpu()\n        if not incremental_cluster:\n            entropy = entropy.numpy()\n            if ent_beta == 0:\n                closest = np.argsort(entropy)[: n_clusters]\n                closest = closest[entropy[closest] < ent_bound]\n            elif ent_beta == 1:\n                closest = np.argsort(entropy)[- n_clusters:]\n                closest = closest[entropy[closest] >= ent_bound]\n            else:\n                raise NotImplementedError\n            weights = torch.zeros(len(closest), dtype=torch.float)\n        else:\n            if ent_beta == 0:\n                sample_choice = entropy < ent_bound\n            elif ent_beta == 1:\n                sample_choice = entropy >= ent_bound\n            else:\n                raise NotImplementedError\n\n            data = data[sample_choice]\n            target = target[sample_choice]\n            feats = feats[sample_choice]\n            pseudo_label = pseudo_label[sample_choice]\n\n            if anchors:\n                feats4cluster = torch.cat([anchors.feats, feats])\n                sample_weight = torch.cat([anchors.weight, torch.ones(len(feats), dtype=torch.float)])\n            else:\n                feats4cluster = feats\n                sample_weight = torch.ones(len(feats), dtype=torch.float)\n\n            if self.config.atta.gpu_clustering:\n                from ATTA.utils.fast_pytorch_kmeans import KMeans\n                from joblib import parallel_backend\n                kmeans = KMeans(n_clusters=n_clusters, n_init=10, device=self.config.device).fit(\n                    feats4cluster.to(self.config.device),\n                    sample_weight=sample_weight.to(self.config.device))\n                with parallel_backend('threading', n_jobs=8):\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n            # elif self.config.atta.gpu_clustering == 'jax':\n            #     from ott.tools.k_means import k_means as KMeans\n            #     import jax\n            #     import jax.numpy as jnp\n            #     tik = time.time()\n            #     kmeans = KMeans(jnp.array(feats4cluster.numpy()), k=n_clusters, weights=jnp.array(sample_weight.numpy()), n_init=10)\n            #     mit = time.time()\n            #     print(f'#IN#Kmeans time: {mit - tik}')\n            #     @jax.jit\n            #     def jax_pairwise_distances_argmin(c, feats):\n            #         dis = lambda x, y: jnp.sqrt(((x - y) ** 2).sum())\n            #         argmin_dis = lambda x, y: jnp.argmin(jax.vmap(dis, in_axes=(None, 0))(x, y))\n            #         return jax.vmap(argmin_dis, in_axes=(0, None))(c, feats)\n            #     raw_closest = np.array(jax_pairwise_distances_argmin(kmeans.centroids, jnp.array(feats4cluster.numpy())))\n            #     print(f'#IN#Pairwise distance time: {time.time() - mit}')\n            #     kmeans_labels = np.array(kmeans.assignment)\n            else:\n                from joblib import parallel_backend\n                from sklearn.cluster import KMeans\n                with parallel_backend('threading', n_jobs=8):\n                    kmeans = KMeans(n_clusters=n_clusters, n_init=10, algorithm='elkan').fit(feats4cluster,\n                                                                                                  sample_weight=sample_weight)\n                    raw_closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, feats4cluster)\n                kmeans_labels = kmeans.labels_\n\n\n\n            if anchors:\n                num_anchors = anchors.num_elem()\n                prev_anchor_cluster = torch.tensor(kmeans_labels[:num_anchors], dtype=torch.long)\n\n                if self.accumulate_weight:\n                    # previous anchor weight accumulation\n                    # Average the weight of the previous anchor if sharing the same cluster\n                    num_prev_anchors_per_cluster = prev_anchor_cluster.unique(return_counts=True)\n                    num_prev_anchors_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_prev_anchors_per_cluster_dict[num_prev_anchors_per_cluster[0].long()] = \\\n                    num_prev_anchors_per_cluster[1]\n\n                    num_newsample_per_cluster = torch.tensor(kmeans_labels).unique(return_counts=True)\n                    num_newsample_per_cluster_dict = torch.zeros(len(raw_closest), dtype=torch.long)\n                    num_newsample_per_cluster_dict[num_newsample_per_cluster[0].long()] = num_newsample_per_cluster[1]\n                    assert (num_prev_anchors_per_cluster_dict[prev_anchor_cluster] == 0).sum() == 0\n                    # accumulate the weight of the previous anchor\n                    anchors.weight = anchors.weight + num_newsample_per_cluster_dict[prev_anchor_cluster] / \\\n                                          num_prev_anchors_per_cluster_dict[prev_anchor_cluster].float()\n\n                anchored_cluster_mask = torch.zeros(len(raw_closest), dtype=torch.bool).index_fill_(0,\n                                                                                                    prev_anchor_cluster.unique().long(),\n                                                                                                    True)\n                new_cluster_mask = ~ anchored_cluster_mask\n\n                closest = raw_closest[new_cluster_mask] - num_anchors\n                if (closest < 0).sum() != 0:\n                    # The cluster's closest sample may not belong to the cluster. It makes sense to eliminate them.\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    new_cluster_mask = torch.where(new_cluster_mask)[0]\n                    print('new_cluster_mask: ', new_cluster_mask)\n                    print(closest)\n                    print(closest >= 0)\n                    new_cluster_mask = new_cluster_mask[closest >= 0]\n                    closest = closest[closest >= 0]\n\n\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1][new_cluster_mask]\n            else:\n                num_anchors = 0\n                closest = raw_closest\n                weights = torch.tensor(kmeans_labels).unique(return_counts=True)[1]\n\n        if use_pseudo_label:\n            anchors = self.update_anchors(anchors, data[closest], pseudo_label[closest], feats[closest], weights)\n        else:\n            anchors = self.update_anchors(anchors, data[closest], target[closest], feats[closest], weights)\n\n        return outputs, closest, anchors\n\n    def enable_bn(self, model):\n        if not self.config.model.freeze_bn:\n            for m in model.modules():\n                if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                    m.momentum = 0.1",
        "experimental_info": "The SimATTA algorithm is implemented as follows:\n- **Teacher Model Initialization**: The `teacher` model is initialized as a deep copy of the `student` model (which is `self.model`). `self.update_teacher(0)` is called at initialization, effectively making the teacher a frozen snapshot of the student model at the beginning of adaptation. It does not update during the adaptation process.\n- **Budget Management**: `self.budgets` tracks the total number of actively selected samples. It increases by the number of new samples (`len(closest)`) added to `self.anchors` in each adaptation step.\n- **Cluster Parameters (High Entropy Samples)**:\n  - `self.n_clusters`: Initial number of clusters for target samples is set to 10.\n  - `self.nc_increase`: This parameter, specified by `self.config.atta.SimATTA.nc_increase`, determines how much `self.n_clusters` increases in each adaptation step. If `self.target_cluster` is false, `self.n_clusters` is explicitly set to 0.\n  - `self.config.atta.SimATTA.eh`: Entropy high threshold for selecting high-entropy samples from the current batch.\n  - `self.target_cluster`: A boolean flag (`self.config.atta.SimATTA.target_cluster`) indicating whether to use incremental clustering for target samples. If `False`, direct entropy-based selection is used.\n- **Cluster Parameters (Low Entropy Samples)**:\n  - `self.source_n_clusters`: Initial number of samples to select for source-like samples is 100. It increases by 1 in each adaptation step.\n  - `self.LE`: A boolean flag (`self.config.atta.SimATTA.LE`) enabling the selection of low-entropy source-like samples using the frozen teacher model.\n  - `self.config.atta.SimATTA.el`: Entropy low threshold for selecting low-entropy samples.\n- **Sample Selection (`sample_select`)**:\n  - The method partitions unlabeled test samples into high and low entropy sets based on `ent_bound` thresholds (`eh` for high, `el` for low entropy).\n  - For high-entropy samples (when `ent_beta=1` and `incremental_cluster=True`): It uses weighted K-Means (`sklearn.cluster.KMeans` with `algorithm='elkan'` or `ATTA.utils.fast_pytorch_kmeans.KMeans` if `gpu_clustering` is enabled) to select samples, reducing redundancy and increasing distribution coverage. `n_init=10` is used for K-Means. `sample_weight` combines existing anchor weights with new sample weights. `self.accumulate_weight` is `True` to allow previous anchor weights to be accumulated.\n  - For low-entropy samples (when `ent_beta=0` and `incremental_cluster=False`): Samples are selected purely based on entropy sorting, without clustering.\n  - `use_pseudo_label=True` is used for selecting low-entropy (source-like) anchors, where pseudo-labels are generated by the teacher model. True labels are used for high-entropy (target) anchors.\n  - `self.config.atta.gpu_clustering`: A boolean flag to determine whether to use GPU-accelerated K-Means.\n- **Model Fine-tuning (`cluster_train`)**:\n  - The model is fine-tuned using `labeled test anchors` (`self.anchors`) and `pseudo-labeled source-like anchors` (`self.source_anchors`).\n  - **Training Weights**: A dynamic `alpha` value balances the contribution of source and target anchors to the loss. `alpha` is calculated as `target_anchors.num_elem() / (target_anchors.num_elem() + source_anchors.num_elem())`.\n  - **Cold Start**: If the number of source anchors is less than `self.config.atta.SimATTA.cold_start`, `alpha` is capped at `min(0.2, alpha)`.\n  - **Optimizer**: `torch.optim.SGD` with a learning rate (`self.config.atta.SimATTA.lr`) and `momentum=0.9`.\n  - **Loss Function**: `self.config.metric.loss_func` (e.g., cross-entropy for classification) is used for both source and target losses, combined as `(1 - alpha) * L_S + alpha * L_T`.\n  - **Training Steps**: The training runs for a maximum of `self.config.atta.SimATTA.steps` iterations.\n  - **Early Stopping**: Training includes an early stopping mechanism, breaking if `tol > 5` after `loss_window` reaches `self.config.atta.SimATTA.stop_tol` without a new lowest loss.\n- **Anchor Feature Update**: After each fine-tuning step, the features of `self.anchors` are re-extracted using the currently updated student model (`self.update_anchors_feats`).\n- **Batch Normalization**: If `self.config.model.freeze_bn` is `False`, BatchNorm layers (`nn.BatchNorm1d`, `nn.BatchNorm2d`) are enabled and their `momentum` is set to `0.1`."
      }
    },
    {
      "title": "Leveraging Proxy of Training Data for Test-Time Adaptation"
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces a novel online evaluation protocol for Test-Time Adaptation (TTA) methods that accounts for computational time constraints by penalizing slower methods with fewer samples for adaptation. The core problem addressed is that existing TTA evaluation protocols overlook the computational cost, leading to an unrealistic assessment of methods' real-world applicability. The main findings are that when inference speed is considered, simpler and faster TTA approaches can outperform more sophisticated but slower methods (e.g., SHOT outperforms SAR), highlighting the importance of developing practical TTA methods that are both accurate and efficient.",
        "methodology": "The proposed methodology is a Realistic TTA evaluation protocol, which explicitly considers the relation between a TTA method's speed and the constant speed of an incoming data stream. It defines a 'relative adaptation speed' C(g) as the integer ratio of the stream's speed to the method's speed. If a method is k times slower than the stream (C(g)=k), it is allowed to adapt only every kth sample. The remaining samples are processed by the most recent adapted model (fθt+1) or the base model if no adaptation has occurred, which is assumed to run in real-time. This protocol intrinsically penalizes slower methods, as long adaptation times lead to dropped samples and thus fewer opportunities for adaptation. C(g) is computed online for each input, accounting for hardware and input-dependent variability.",
        "experimental_setup": "The evaluation was conducted using a ResNet-50-BN3 backbone pretrained on ImageNet (Deng et al., 2009). The data stream revealed batches of size 64 (except for MEMO, which used single images). Experiments were performed on multiple datasets: ImageNet-C (Hendrycks & Dietterich, 2019) with a corruption level of 5 across 15 corruptions, CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and ImageNet-3DCC (Kar et al., 2022). The study benchmarked 15 state-of-the-art TTA methods (e.g., AdaBN, SHOT, TENT, SAR, EATA, CoTTA, DDA) and evaluated them under four scenarios: (i) episodic domain shifts, (ii) continual adaptation to sequential domain shifts, (iii) varying stream speeds (ηr with η ∈ {1/16, 1/8, 1/4, 1/2, 1}), and (iv) practical TTA with label correlation (PTTA setup on CIFAR10-C). Additional evaluations included different architectures like ViT and ResNet-18, and hyper-parameter tuning for TENT.",
        "limitations": "The current evaluation protocol is limited by its assumption that the data stream waits for TTA methods to complete adaptation, which is unrealistic in real-time deployment. Many sophisticated TTA methods incur high computational overhead, making them impractical under realistic time constraints. Data-dependent approaches like MEMO and DDA are particularly inefficient, with C(g) values of 54 and 810 respectively, meaning they miss a vast majority of samples and often perform close to the non-adapted baseline. The performance of TTA methods can vary significantly with batch size, with most methods failing at small batch sizes. Also, sample rejection methods like SAR that rely on gradients for rejection suffer performance drops due to reduced speed, unlike EATA which uses a faster, forward-pass-based rejection. The paper also mentions a 'single model evaluation scheme' as a stricter scenario where concurrently deploying two models is not assumed, heavily penalizing slower methods.",
        "future_research_directions": "The paper calls for future research to focus on increasing the efficiency of data-dependent adaptation methods due to their extreme inefficiency under the proposed realistic protocol. It hopes that the new evaluation scheme will inspire the development of future TTA methods that consider inference speed as a critical dimension affecting real-world performance. Implicitly, research into TTA methods that perform well with smaller batch sizes and those with efficient sample rejection mechanisms (like EATA) is encouraged."
      }
    },
    {
      "title": "Improved Test-Time Adaptation for Domain Generalization",
      "abstract": "The main challenge in domain generalization (DG) is to handle the\ndistribution shift problem that lies between the training and test data. Recent\nstudies suggest that test-time training (TTT), which adapts the learned model\nwith test data, might be a promising solution to the problem. Generally, a TTT\nstrategy hinges its performance on two main factors: selecting an appropriate\nauxiliary TTT task for updating and identifying reliable parameters to update\nduring the test phase. Both previous arts and our experiments indicate that TTT\nmay not improve but be detrimental to the learned model if those two factors\nare not properly considered. This work addresses those two factors by proposing\nan Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically\ndefining an auxiliary objective, we propose a learnable consistency loss for\nthe TTT task, which contains learnable parameters that can be adjusted toward\nbetter alignment between our TTT task and the main prediction task. Second, we\nintroduce additional adaptive parameters for the trained model, and we suggest\nonly updating the adaptive parameters during the test phase. Through extensive\nexperiments, we show that the proposed two strategies are beneficial for the\nlearned model (see Figure 1), and ITTA could achieve superior performance to\nthe current state-of-the-art methods on several DG benchmarks. Code is\navailable at https://github.com/liangchen527/ITTA.",
      "full_text": "Improved Test-Time Adaptation for Domain Generalization Liang Chen1 Yong Zhang2* Yibing Song3 Ying Shan2 Lingqiao Liu1∗ 1 The University of Adelaide 2 Tencent AI Lab 3 AI3 Institute, Fudan University {liangchen527, zhangyong201303, yibingsong.cv}@gmail.com yingsshan@tencent.com lingqiao.liu@adelaide.edu.au Abstract The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Gen- erally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for up- dating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments in- dicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly consid- ered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, in- stead of heuristically defining an auxiliary objective, we pro- pose a learnable consistency loss for the TTT task, which con- tains learnable parameters that can be adjusted toward bet- ter alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adap- tive parameters during the test phase. Through extensive ex- periments, we show that the proposed two strategies are ben- eficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA. 1. Introduction Recent years have witnessed the rapid development of deep learning models, which often assume the training and test data are from the same domain and follow the same distribution. However, this assumption does not always hold in real-world scenarios. Distribution shift among the source and target domains is ubiquitous in related areas [35], such as autonomous driving or object recognition tasks, resulting *Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. 0.5 1.1 0.5 1.2 0.5 0.5 0.5 1.4 0.4 0.4 0.4 0.3 art cartoon photo sketch 79.9 75.4 94.4 75.8 83.3 76.0 94.4 76.7 84.7 78.0 94.5 78.2 Figure 1. Performance improvements from the proposed two strate- gies (i.e. introducing a learnable consistency loss and including additional adaptive parameters to improve TTT) for the baseline model (i.e. ResNet18 [30] with existing augmentation strategy [75]). Experiments are conducted on the PACS dataset [37] with the leave- one-out setting. Following [27], we use 60 sets of random seeds and hyper-parameters for each target domain. The reported average accuracy and error bars verify the effectiveness of our method. in poor performances for delicately designed models and hindering the further application of deep learning techniques. Domain generalization (DG) [2,8,16,23,24,31,38 –40,40, 44, 47, 51, 52, 69], designed to generalize a learned model to unseen target domains, has attracted a great deal of attention in the research community. The problem can be traced back to a decade ago [7], and various approaches have been pro- posed to push the DG boundary ever since. Those efforts in- clude invariant representation learning [28,47,49,58], adver- sarial learning [23,40,44,69], augmentation [9,41,42,66,75], or meta-learning [2, 16, 38, 39]. Despite successes on certain occasions, a recent study [27] shows that, under a rigorous evaluation protocol, most of these arts are inferior to the baseline empirical risk minimization (ERM) method [61]. This finding is not surprising, as most current arts strive to decrease the distribution shift only through the training data while overlooking the contributions from test samples. Recently, the test-time training (TTT) technique [60] has been gaining momentum for easing the distribution shift problem. TTT lies its success in enabling dynamic tuning of the pretrained model with the test samples via an auxil- iary TTT task, which seems to be a promising effort when arXiv:2304.04494v2  [cs.CV]  16 Apr 2023confronting data from different domains. However, TTT is not guaranteed to improve the performance. Previous arts [46, 63] indicate that selecting an appropriate auxiliary TTT task is crucial, and an inappropriate one that does not align with the main loss may deteriorate instead of improv- ing the performance. Meanwhile, it is pointed out in [63] that identifying reliable parameters to update is also essential for generalization, which is in line with our experimental findings in Sec. 5.3. Both of these two tasks are non-trivial, and there are limited efforts made to address them. This paper aims to improve the TTT strategy for better DG. First, different from previous works that empirically define auxiliary objectives and assume they are aligned with the main task, our work does not make such assumptions. Instead, we suggest learning an appropriate auxiliary loss for test-time updating. Specifically, encouraged by recent successes in multi-view consistency learning [13,26,29], we propose to augment the consistency loss by adding learn- able parameters based on the original implementation, where the parameters can be adjusted to assure our TTT task can be more aligned with the main task and are updated by en- forcing the two tasks share the same optimization direction. Second, considering that identifying reliable parameters to update is an everlasting job given the growing size of current deep models, we suggest introducing new adaptive param- eters after each block during the test phase, and we only tune the new parameters by the learned consistency loss while leaving the original parameters unchanged. Through extensive evaluations on the current benchmark [27], we illustrate that the learnable consistency loss performs more effectively than the self-supervised TTT tasks adopted in previous arts [60, 63], and by tuning only the new adaptive parameters, our method is superior to existing strategies that update all the parameters or part of them. This work aims to ease the distribution shift problem by improving TTT, and the main contributions are three-fold: • We introduce a learnable consistency loss for test-time adaptation, which can be enforced to be more aligned with the main loss by tuning its learnable parameters. • We introduce new adaptive parameters for the trained model and only update them during the test phase. • We conduct experiments on various DG benchmarks and illustrate that our ITTA performs competitively against current arts under the rigorous setting [27] for both the multi-source and single-source DG tasks. 2. Related Works 2.1. Domain Generalization. Being able to generalize to new environments while de- ploying is a challenging and practical requirement for cur- rent deep models. Existing DG approaches can be roughly categorized into three types. (1) Invariant representation learning: The pioneering work [5] theoretically proves that if the features remain invariant across different domains, then they are general and transferable to different domains. Guided by this finding, [47] uses maximum mean discrep- ancy (MMD) to align the learned features, and [25] proposes to use a multi-domain reconstruction auto-encoder to obtain invariant features. More recently, [58] suggests maximiz- ing the inner product of gradients from different domains to enforce invariance, and a similar idea is proposed in [52] where these gradients are expected to be similar to their mean values. (2) Optimization algorithms: Among the different optimization techniques adopted in DG, prevail- ing approaches resort to adversarial learning [23, 40, 44, 69] and meta-learning [2, 16, 38, 39]. Adversarial training is often used to enforce the learned features to be agnostic about the domain information. In [23], a domain-adversarial neural network (DANN) is implemented by asking the main- stream feature to maximize the domain classification loss. This idea is also adopted in [44], where adversarial training and an MMD constraint are employed to update an auto- encoder. Meanwhile, the meta-learning technique is used to simulate the distribution shifts between seen and unseen environments [2, 16, 38, 39], and most of these works are developed based on the MAML framework [20]. (3) Aug- mentation: Most augmentation skills applied in the general- ization tasks are operated in the feature level [34, 41, 48, 75] except for [11,66,68] which mix images [68] or its phase [66] to synthesize new data. To enable contrastive learning, we incorporate an existing augmentation strategy [75] in our framework. This method originated from AdaIN [32], which synthesizes new domain information by mixing the statistics of the features. Similar ideas can be found in [42, 48]. 2.2. Test-Time Training and Adaptation Test-Time Training (TTT) is first introduced in [60]. The basic paradigm is to employ a test-time task besides the main task during the training phase and update the pre- trained model using the test data with only the test-time objective before the final prediction step. The idea is empir- ically proved effective [60] and further developed in other related areas [3, 10, 12, 14, 21, 22, 43, 56, 63, 65, 73, 74]. Most current works focus on finding auxiliary tasks for updat- ing during the test phase, and the efforts derive from self- supervion [3, 10, 21, 22, 43, 60], meta-learning [65, 73, 74], information entropy [63], pseudo-labeling [12, 14], to name a few. However, not all empirically selected test-time tasks are effective. A recent study [46] indicates that only when the auxiliary loss aligns with the main loss can TTT improve the trained model. Inspired by that, we propose a learnable consistency loss and enforce alignment between the two ob- jectives. Results show that our strategy can be beneficial for the trained model (see Figure 1).subtract Figure 2. Training process of ITTA. We use x from the source domain as input for the feature extractor fθ(·) to obtain the repre- sentation z and its augmented version z′, where the augmentation skill from [75] is applied. The classifier fϕ(·) and weight subnet- work fw(·) are used to compute the main loss Lmain and learnable consistency loss Lwcont. Please refer to our text for details. Meanwhile, [63] suggests that auxiliary loss is not the only factor that affects the performance. Selecting reliable parameters to update is also crucial within the TTT frame- work. Given the large size of current models, correctly iden- tifying these parameters may require tremendous amounts of effort. To this end, instead of heuristically selecting candi- dates, we propose to include new adaptive parameters for up- dating during the test phase. Experimental results show that the proposed method can obtain comparable performances against existing skills. 3. Methodology In the task of DG, we are often given access to data from S (S ≥ 1) source domains Ds = {D1, D2, ..., DS} and expect a model to make good prediction on unseen target domains Dt = {D1, D2, ..., DT } (T ≥ 1). Our method aims to improve the test-time training (TTT) strategy for better DG. The improvements are two-fold. First, we pro- pose a learnable consistency loss for the TTT task, which could be enforced to align with the main objective by tuning its learnable weights. Second, we suggest including addi- tional adaptive parameters and only updating these adaptive parameters during the test phase. 3.1. A Learnable Consistency Loss for TTT The TTT strategies have shown promising performances when dealing with distribution shift problems [43, 63]. How- ever, their successes are depended on the empirically selected auxiliary TTT tasks, which may deteriorate the performances if chosen improperly. Motivated by the recent successes in multi-view consistency learning [13, 26, 29], we suggest adopting a consistency loss in our TTT task. Note that the naive consistency loss is still not guaranteed to be effective as prior art [46] indicates that only when the auxiliary loss aligns with the main loss, can TTT improves the perfor- mance. To this end, we propose to augment the auxiliary loss with learnable parameters that could be adjusted toward a better alignment between the TTT and main tasks. In our case, we make the adopted consistency loss learnable by introducing a weight subnetwork that allows flexible ways Algorithm 1 Pseudo code of the training phase of ITTA in a PyTorch-like style. # fθ, fϕ, fw: feature extractor, classifier, weight subnetwork # α, 0: weight paramter, all zero tensor # training process for x, yin training loader: # load a minibatch with N samples def forward process(x, y): z, z′ = fθ.forward(x) # computing losses Lmain = CrossEntropyLoss(fϕ.forward(z), y) Lmain+ =CrossEntropyLoss(fϕ.forward(z′), y) Lwcont = MSELoss(fw.forward(z − z′), 0) return Lmain, Lwcont # SGD update: feature extractor and classifier Lmain, Lwcont = forward process(x, y) ([fθ.params, fϕ.params]).zero grad() (Lmain + αLwcont).backward() update( \u0002 fθ.params, fϕ.params \u0003 ) # compute objectives for updating weight subnetwork Lmain, Lwcont = forward process(x, y) Lmain.backward() ˆgmain = fθ.params.grad.clone().normalize() fθ.params.zero grad() Lwcont.backward() ˆgwcont = fθ.params.grad.clone().normalize() # SGD update: weight subnetwork MSELoss(ˆgmain, ˆgwcont).backward() fw.params.zero grad() update(fw.params) to measure the consistency between two views of the same instance. We first introduce the pipeline of our training framework. Given the D dimensional representation z ∈ RD1 and its corresponding augmented version z′ that are obtained from a feature extractor (i.e. {z, z′} = fθ(x), where x is an input image from Ds, and fθ(·) is the feature extractor parame- terized by θ. In our implementation, we use the existing augmentation method [75] to obtain z′ by modifying the intermediate activation in fθ(x). We show in our supplemen- tary material that our framework can also thrive with other augmentation strategies), our learnable consistency loss is given by, Lwcont = ∥fw(z − z′)∥, (1) where ∥ · ∥denotes the L2 norm; fw(·) is the weight sub- network parameterized by w. To make the training process more stable and potentially achieve better performance, we apply a dimension-wise nonlinear function to map each di- mension of z − z′ before calculating the L2 norm. That is, ∀h ∈ RD, fw(h) is implemented by stacking layers of a nonlinear function: ReLU(a ∗ h + b), where a ∈ RD and b ∈ RD are the weight and bias from the nonlinear function, 1We omit the batch dimensions of the variables for simplicity.… … subtract Figure 3. Test adaptation process of ITTA. Different from that in the training stage, we include additional adaptive parameters fΘ after each block of the feature extractor fθ. For each test sample x, the intermediate representations zi and z′i obtained from fi θ are passed to fi Θ before going to the next block fi+1 θ . We use the learnable consistency loss Lwcont as the objective to update fΘ. Please refer to our text for details. and different layers of a, bform the parameter w in fw. In effect, this creates a piecewise-linear mapping function for h: depending on the value of h, the output could be 0, a constant, or a scaling-and-shifted version of h. More studies about the design of fw are provided in our supplementary material. Compared to the naive consistency learning with- out fw, our Lwcont can be more flexible with an adjustable fw, which we show in the following is the key for learning an appropriate loss in the improved TTT framework. Combining Lwcont with the main loss Lmain which applies the cross-entropy loss (CE) for both the origi- nal and augmented inputs ( i.e. Lmain = CE(fϕ(z), y) + CE(fϕ(z′), y), where fϕ is the classifier parameterized by ϕ, and y is the corresponding label), the objective for the feature extractor and classifier can be formulated into, min{θ,ϕ} Lmain + αLwcont, (2) where α is the weight parameter that balances the contri- butions from the two terms. A simple illustration of the workflow is shown in Figure 2. From Eq. (2), the expected gradients for the feature ex- tractor from Lmain and Lwcont can be represented as, \u001a gmain = ∇θ(CE(fϕ(z), y) + CE(fϕ(z′), y)), (3) gwcont = ∇θ∥fw(z − z′)∥. (4) We observe that the direction of gwcont is also determined by the weight subnetwork fw(·), which should be close with gmain to ensure alignment between Lmain and Lwcont [46, 60]. To this end, we propose a straightforward solution by enforcing equality between the normalized versions of gmain and gwcont, and we use this term as the objective for updating fw(·), which gives, min w Lalign, s.t. Lalign = ∥ˆgmain − ˆgwcont∥, (5) where ˆgmain = gmain−Egmain σgmain , and similar for ˆgwcont. In our implementation, we update {θ, ϕ} and w in an alternative manner. Pseudo code of the training process are shown in Algorithm 1. Algorithm 2 Pseudo code of the test phase of ITTA in a PyTorch-like style. # fθ, fϕ: feature extractor, classifier # fw, fΘ: weight subnetwork, additional adaptive blocks # m, 0: total number of blocks in fθ, all zero tensor # test process for x in test loader: # load a test batch def forward process(x): z1, z′1 = f1 Θ.forward((f1 θ .forward(x))) # first blocks for i in range(2, m + 1): # the following m − 1 blocks zi, z′i = fi θ.forward(zi−1), fi θ.forward(z′i−1) zi, z′i = fi Θ.forward(zi), fi Θ.forward(z′i) return zi, z′i # test adaptation phase: SGD update additional adaptive parameters z, z′ = forward process(x) Lwcont = MSELoss(fw.forward(z − z′), 0) fΘ.params.zero grad() Lwcont.backward() update(fΘ.params) # final prediction z, = forward process(x) result = fϕ.forward(z) 3.2. Including Additional Adaptive Parameters Selecting expressive and reliable parameters to update during the test phase is also essential in the TTT frame- work [63]. Some strategies decide to update all the parame- ters from the feature extractor [3, 43], while others use only the parameters from the specific layers for updating [63, 71]. Given the fact that the sizes of current deep models are often very large and still growing, exhaustively trying different combinations among the millions of candidates seems to be an everlasting job. As there are no consensuses on which parameter should be updated, we suggest another easy alter- native in this work. Specifically, assuming there are a total of m blocks in the pretrained feature extractor fθ(·), and the i-th block can be denoted as fi θ(·). Then the intermediate representation zi from fi θ(·) can be formulated as, zi = fi θ(zi−1), s.t. z1 = f1 θ (x). (6) We propose to include additional adaptive blockfΘ that is parameterized by Θ after each block of fθ during the test- time adaptation phase, which reformulates Eq. (6) into, zi = fi Θ(fi θ(zi−1)), s.t. z1 = f1 Θ(f1 θ (x)), (7) where fΘ(·) does not change the dimension and sizes of the intermediate representations. In our work, we use a structure similar to fw to implement fΘ. Note zm is simplified as z in this phase, and the same process is applied for obtaining z′. Then, in the test-time adaptation phase, we suggest only updating the new adaptive parameters via the learned con- sistency loss. The optimization process can be written as,Table 1. Multi sources domain generalization. Experiments are conducted on the DomainBed benchmark [27]. All methods are examined for 60 trials in each unseen domain. Top5 accumulates the number of datasets where a method achieves the top 5 performances. The score here accumulates the numbers of the dataset where a specific art obtains larger accuracy than ERM on account of the variance. Best results are colored as red. Among the 22 methods compared, less than a quarter outperforms ERM in most datasets (Score ≥ 3). PACS VLCS OfficeHome TerraInc DomainNet Avg. Top5↑ Score↑ MMD [40] 81.3 ± 0.8 74.9 ± 0.5 59.9 ± 0.4 42.0 ± 1.0 7.9 ± 6.2 53.2 1 2 RSC [33] 80.5 ± 0.2 75.4 ± 0.3 58.4 ± 0.6 39.4 ± 1.3 27.9 ± 2.0 56.3 0 1 IRM [1] 80.9 ± 0.5 75.1 ± 0.1 58.0 ± 0.1 38.4 ± 0.9 30.4 ± 1.0 56.6 0 1 ARM [72] 80.6 ± 0.5 75.9 ± 0.3 59.6 ± 0.3 37.4 ± 1.9 29.9 ± 0.1 56.7 0 0 DANN [23] 79.2 ± 0.3 76.3 ± 0.2 59.5 ± 0.5 37.9 ± 0.9 31.5 ± 0.1 56.9 1 1 GroupGRO [55] 80.7 ± 0.4 75.4 ± 1.0 60.6 ± 0.3 41.5 ± 2.0 27.5 ± 0.1 57.1 0 1 CDANN [44] 80.3 ± 0.5 76.0 ± 0.5 59.3 ± 0.4 38.6 ± 2.3 31.8 ± 0.2 57.2 0 0 VREx [36] 80.2 ± 0.5 75.3 ± 0.6 59.5 ± 0.1 43.2 ± 0.3 28.1 ± 1.0 57.3 1 1 CAD [53] 81.9 ± 0.3 75.2 ± 0.6 60.5 ± 0.3 40.5 ± 0.4 31.0 ± 0.8 57.8 1 2 CondCAD [53] 80.8 ± 0.5 76.1 ± 0.3 61.0 ± 0.4 39.7 ± 0.4 31.9 ± 0.7 57.9 0 1 MTL [6] 80.1 ± 0.8 75.2 ± 0.3 59.9 ± 0.5 40.4 ± 1.0 35.0 ± 0.0 58.1 0 0 ERM [61] 79.8 ± 0.4 75.8 ± 0.2 60.6 ± 0.2 38.8 ± 1.0 35.3 ± 0.1 58.1 1 - MixStyle [75] 82.6 ± 0.4 75.2 ± 0.7 59.6 ± 0.8 40.9 ± 1.1 33.9 ± 0.1 58.4 1 1 MLDG [38] 81.3 ± 0.2 75.2 ± 0.3 60.9 ± 0.2 40.1 ± 0.9 35.4 ± 0.0 58.6 1 1 Mixup [68] 79.2 ± 0.9 76.2 ± 0.3 61.7 ± 0.5 42.1 ± 0.7 34.0 ± 0.0 58.6 2 2 Fishr [52] 81.3 ± 0.3 76.2 ± 0.3 60.9 ± 0.3 42.6 ± 1.0 34.2 ± 0.3 59.0 2 2 SagNet [48] 81.7 ± 0.6 75.4 ± 0.8 62.5 ± 0.3 40.6 ± 1.5 35.3 ± 0.1 59.1 1 2 SelfReg [34] 81.8 ± 0.3 76.4 ± 0.7 62.4 ± 0.1 41.3 ± 0.3 34.7 ± 0.2 59.3 2 3 Fish [58] 82.0 ± 0.3 76.9 ± 0.2 62.0 ± 0.6 40.2 ± 0.6 35.5 ± 0.0 59.3 3 4 CORAL [59] 81.7 ± 0.0 75.5 ± 0.4 62.4 ± 0.4 41.4 ± 1.8 36.1 ± 0.2 59.4 2 3 SD [51] 81.9 ± 0.3 75.5 ± 0.4 62.9 ± 0.2 42.0 ± 1.0 36.3 ± 0.2 59.7 4 4 Ours 83.8 ± 0.3 76.9 ± 0.6 62.0 ± 0.2 43.2 ± 0.5 34.9 ± 0.1 60.2 4 4 min Θ ∥fw(z − z′)∥, s.t. {z, z′} = fΘ(fθ(x)). (8) Note that different from the training phase, x in this stage is from the target domain Dt, and we use the online setting in [60] for updating. A simple illustration of the test adaptation pipeline is shown in Figure 3. For the final step, we use the original representation ob- tained from the pretrained feature extractor and the adapted adaptive parameters for prediction. Pseudo code of the test stage are shown in Algorithm 2. 4. Experiments 4.1. Settings Datasets. We evalute ITTA on five benchmark datasets: PACS [37] which consists of 9,991 images from 7 cate- gories. This dataset is probably the most widely-used DG benchmark owing to its large distributional shift across 4 do- mains including art painting, cartoon, photo, and sketch; VLCS [18] contains 10,729 images of 5 classes from 4 different datasets (i.e. domains) including PASCAL VOC 2007 [17], LabelMe [54], Caltech [19], and Sun [64] where each dataset is considered a domain in DG;OfficeHome [62] is composed of 15,588 images from 65 classes in office and home environments, and those images can be categorized into 4 domains (i.e. artistic, clipart, product, and real world); TerraInc [4] has 24,788 images from 10 classes. Those images are wild animals taken from 4 different locations (i.e. domains) including L100, L38, L43, and L46; Domain- Net [50] which contains 586,575 images from 345 classes, and the images in it can be depicted in 6 styles (i.e. clipart, infograph, painting, quickdraw, real, and sketch). Implementation details. For all the experiments, we use the ImageNet [15] pretrained ResNet18 [30] backbone that with 4 blocks as the feature extractor fθ, which could en- large the gaps in DG compared to larger models [70]. Corre- spondingly, we also include 4 blocks of additional adaptive parameters (i.e. fΘ), and each block is implemented with 5 layers of learnable parameters with weight initialized as all ones and bias initialized as all zeros. For the weight subnet- work fw, we use 10 layers of learnable parameters with the initialization skill similar to that of fΘ. The classifier fϕ is an MLP layer provided by the Domainbed benchmark [27]. For the weight parameter α in Eq. (2), we set it to be 1 for all experiments (please refer to our supplementary material for analysis). The random seeds, learning rates, batch size, and augmentation skills are all dynamically set for all the compared arts according to [27].Table 2. Single source domain generalization. Experiments are conducted on the PACS dataset [37]. Here A, C, P, and S are the art, cartoon, photo, and sketch domains in PACS. A→C represents models trained on the art domain and tested on the cartoon domain, and similar for others. All methods are examined for 60 trials in each unseen domain. Best results are colored as red. A→C A →P A →S C →A C →P C →S P →A P →C P →S S →A S →C S →P Avg. RSC 66.3 ±1.3 88.2±0.6 57.2±3.1 65.8±1.5 82.4±0.6 68.7±2.5 60.5±2.0 41.3±6.0 53.1±2.8 53.8±1.6 65.9±0.7 48.4±1.9 62.6 Fish 67.1 ±0.5 89.2±1.8 57.0±0.2 66.7±1.0 85.6±0.4 64.5±3.6 55.1±2.1 33.9±2.3 51.2±4.2 59.1±3.2 67.1±0.9 58.4±1.2 62.9 CDANN 66.5±1.7 92.2±0.6 65.0±0.9 70.6±0.1 82.9±1.4 67.7±3.0 60.6±0.3 42.2±6.4 46.9±9.9 51.4±2.3 60.7±1.2 51.9±0.4 63.2 SelfReg 63.9±1.9 90.1±1.0 56.8±2.2 70.2±2.3 85.4±0.3 70.2±2.2 60.9±2.6 38.8±4.0 50.5±3.2 54.5±4.7 66.2±1.2 51.7±4.1 63.3 DANN 67.5 ±1.6 91.2±1.3 67.5±1.3 70.6±1.0 81.4±0.4 66.6±1.1 54.1±2.3 33.5±2.7 52.8±2.3 53.8±1.7 64.4±0.7 58.9±0.8 63.5 CAD 67.1 ±1.5 89.6±0.4 60.2±0.2 67.7±3.1 83.7±1.4 70.2±2.6 60.6±2.6 38.3±3.7 53.8±3.2 50.7±1.6 65.8±1.3 54.4±1.7 63.5 GroupGRO66.5±1.2 90.5±1.5 58.9±2.5 70.8±0.9 85.7±1.2 69.7±1.8 62.3±2.1 41.1±2.7 48.2±4.1 54.8±0.5 65.2±1.6 53.9±1.4 64.0 MTL 67.3 ±1.0 90.1±1.0 58.9±0.7 70.2±1.8 84.2±2.2 71.9±0.7 58.3±2.7 38.5±2.7 52.8±1.5 55.4±3.1 66.1±1.3 55.2±2.6 64.1 IRM 67.5 ±1.8 93.0±0.5 62.9±4.7 67.6±1.3 83.8±0.4 68.9±0.8 63.7±1.8 39.9±3.7 49.0±5.4 54.9±1.4 63.1±2.1 54.9±1.4 64.1 ARM 66.0 ±2.4 91.2±0.7 58.7±6.9 70.6±0.8 84.2±1.0 69.1±0.9 59.2±1.8 42.1±5.6 52.1±3.0 60.0±0.6 62.9±3.3 53.8±2.0 64.2 Mixup 65.5 ±0.8 87.8±0.3 57.2±1.0 71.4±1.1 83.1±1.8 68.0±3.0 59.6±1.7 37.2±2.7 56.5±3.8 55.0±2.2 66.2±1.5 62.7±4.2 64.2 CORAL 66.8±0.5 90.3±0.7 61.5±1.9 67.9±2.1 85.4±0.3 70.4±1.3 55.9±2.9 40.4±4.9 49.8±8.5 55.8±2.1 67.6±0.9 58.9±3.8 64.2 SD 67.1 ±1.3 91.7±1.2 63.7±4.1 70.3±0.9 84.4±0.7 69.4±2.3 57.5±2.5 42.6±0.8 47.7±1.7 55.9±2.4 65.7±0.8 55.8±2.1 64.3 MMD 67.1 ±1.4 88.0±0.8 63.6±1.6 70.0±1.1 83.6±0.2 70.2±1.0 58.8±2.6 40.3±1.0 52.3±2.4 57.4±1.9 68.7±0.9 52.7±3.7 64.4 MLDG 67.3±2.0 90.8±0.5 64.4±0.9 70.8±1.0 84.2±0.3 69.7±1.8 61.6±1.0 41.3±5.1 50.4±0.2 49.9±2.5 66.8±0.4 58.7±3.4 64.7 CondCAD66.9±1.4 92.3±0.7 60.8±4.5 71.0±0.6 84.7±1.1 72.6±0.5 61.2±1.5 40.7±3.6 55.7±1.6 52.3±1.7 64.2±0.4 55.3±1.2 64.8 ERM 67.3 ±0.7 91.7±0.9 60.1±4.7 70.4±0.6 82.3±2.7 68.1±0.9 59.6±1.8 44.7±2.8 56.5±2.7 52.8±2.3 68.1±0.7 58.4±0.9 65.0 VREx 67.1 ±1.5 91.0±1.0 62.6±3.5 71.1±2.4 84.1±0.9 71.7±1.3 62.4±3.1 37.7±3.3 53.6±2.3 60.6±1.6 66.7±0.8 57.5±1.4 65.5 Fishr 67.9 ±1.9 92.7±0.3 62.4±4.7 71.2±0.5 83.4±0.6 70.2±1.1 60.0±2.3 42.7±3.2 57.1±3.9 55.7±3.7 68.4±1.0 62.0±3.1 66.1 SagNet 67.6±1.4 92.3±0.5 59.5±1.7 71.8±0.3 82.8±0.6 69.9±1.8 62.5±2.5 45.2±2.5 64.1±2.0 55.8±1.1 65.7±1.4 55.9±3.5 66.1 MixStyle 68.5±2.0 91.2±1.6 65.1±0.7 73.2±1.3 85.0±0.8 71.7±1.5 63.6±1.7 46.3±1.1 51.6±3.7 54.2±1.5 67.0±3.4 58.3±1.4 66.3 Ours 68.9 ±0.6 92.4±0.1 62.5±0.6 75.3±0.4 85.9±0.3 70.2±1.4 66.5±1.1 52.2±2.7 63.8±1.1 57.6±3.7 68.0±1.3 57.9±2.0 68.4 Training and evaluation details. For all the compared methods, we conduct 60 trials on each source domain, and each with 5,000 iteration steps. During the training stage, we split the examples from training domains to 8:2 (train:val) where the training and validation samples are dynamically selected among different training trials. During test, we select the model that performs the best in the validation samples and test it on the target domains. The strategy is referred to as the “training-domain validate set” model selec- tion method in [27]. For each domain in different datasets, the final performance is the average accuracy from the 60 trials. 4.2. Multi-Source Generalization In these experiments, all five benchmark datasets afore- mentioned are used for evaluation, and the leave-one-out strategy is adopted for training (i.e. with S = |Ds ∪Dt|2 −1, and T = 1). Results are shown in Table 1. We note that ERM method obtains favorable performance against existing arts. In fact, as a strong baseline, ERM is superior to half of the methods in the term of average accuracy, and only 5 arts (i.e. SelfReg [34], Fish [58], CORAL [59], SD [51], and ours) among the compared 22 methods outperforms ERM in most datasets (i.e. with Score ≥ 3). In comparison, the proposed ITTA is more effective than all other models on average. In particular, ITTA achieves the best performances in 3 out of the 5 benchmarks (i.e. PACS, VLCS, and TerraInc datasets) and 4 in the top 5. Note that although our method does not obtain the best performances in the OfficeHome and DomainNet benchmarks, it still outperforms more than half 2We use | · |to denote the number of domains in the environment. of the existing models. The results validate the effectiveness of our method when tested in the multi-source setting. We present results of average accuracy in each domain from different datasets in the supplementary material. Please refer to it for details. 4.3. Single-Source Generalization In these experiments, we adopt the widely-used PACS [37] benchmark for evaluation, and the models are trained on one domain while tested on the remaining three (i.e. with S = 1, and T = 3). Although some approaches, such as MLDG [38] and Fishr [52], may require more than one domain information for their trainings, we can simu- late multi-domain information using only the source domain, and thus the experimental settings are still feasible for them. Compared to the multi-source generalization task, the single- source generalization is considered more difficult due to the limited domain information during the training phase. Evalu- ation results are presented in Table 2. We note that the ERM method outperforms most state-of-the-art models, and only 5 models, including VREx [36], Fishr [52], SagNet [48], MixStyle [75], and the proposed ITTA, can obtain better re- sults than ERM in the term of average accuracy. Meanwhile, our method achieves the best performances when trained in 5 out of the 12 source domain, and it obtains the best perfor- mance on average, leading more than 2% than the second best (i.e. MixStyle [75]) and 3% the ERM method. In line with the findings in [27], we notice that the naive ERM method [61] can indeed perform favorably against most existing models under rigorous evaluation protocol. As a matter of fact, the proposed method is the only one that consistently outperforms ERM in both the multi-sourceTable 3. Evaluations of different TTT-based models in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch Baseline 79.9 ±0.5 75.4±1.1 94.4±0.5 75.8±1.2 81.4±0.5 TTT [60] 81.5±0.8 77.6±0.6 94.3±0.2 78.4±0.7 83.0±0.2 MT3 [3] 82.0 ±1.0 76.5±1.0 94.1±0.2 77.7±1.3 82.6±0.6 TENT [63] 80.2±0.9 77.2±0.8 94.4±0.2 77.4±0.1 82.3±0.5 Ours 84.7 ±0.4 78.0±0.4 94.5±0.4 78.2±0.3 83.8±0.3 and single-source settings. These results indicate that DG remains challenging for current efforts that aim to ease the distribution shift only through training data, and using the proposed improved TTT strategy may be a promising direc- tion for solving DG. 5. Analysis All experiments in this section are conducted on the widely-used PACS benchmark [37] with the leave-one-out strategy. The experimental settings are the same as that illus- trated in Sec. 4.1. Please refer to our supplementary material for more analysis. 5.1. Compared with Other TTT-Based Models Using test-time adaptation to ease the distribution shift problem has been explored in previous works, such as the original TTT method [60] and MT3 [3]. Their differences lie in that TTT uses a rotation estimation task for the test-time objective, and MT3 adopts a contrastive loss for the task and implements the overall framework using MAML [20]. There is also a recently proposed TENT [63] that aims to minimize the entropy of the final results by tuning the parameters from the batch normalization (BN) layers. To analyze the overall effectiveness of our method, we compare ITTA with these arts using the same baseline (i.e. ResNet18 [30] backbone with the existing augmentation skill [75]). Results are shown in Table 3. We observe that all the com- pared TTT-based methods can improve the baseline model in almost all target domains except for the “Photo” domain, which might be due to the ImageNet pretraining [67]. This phenomenon demonstrates that the TTT strategy may be a promising effort for easing the distribution shift problem. Meanwhile, we observe that the proposed ITTA is superior to all other approaches in most target domains and leads in the term of average accuracy. The main reason is that compared to the empirically designed TTT tasks adopted in previous works, the proposed learnable consistency loss is enforced to be more aligned with the main loss, thus more suitable for the test-time adaptation task [46]. Meanwhile, compared to the strategies that update the original param- eters from the trained model, the adaptation of the newly included parameters is also more effective for the overall (a) Input (b) Ours w/o fw (c) Ours (d) Main Figure 4. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels from the four target do- mains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). Ours w/o fw is the naive consis- tency loss with fw disabled in Eq. (1). The proposed learnable consistency loss can align well with the main classification task. TTT framework. In the following, we provide more analysis to support these claims. 5.2. Effectiveness of the Learnable Consistency Loss To examine the effectiveness of our learnable consistency loss, we conduct ablation studies by comparing our method with the following variants. (1) Ours w/o fw: we disable fw when computing the learnable consistency loss in Eq. (1), which uses the naive consistency loss for the auxiliary TTT task. (2) Ours w/ Ent.: after training the model using the baseline settings (i.e. ResNet18 with the augmentation strat- egy [75]), we use the entropy minimization task in [63] for the TTT task. (3) Ours w/ Rot.: we use the rotation estimation task in [60] for the TTT task. To ensure fair com- parisons, we use the same baseline settings and include the same additional adaptive parameters for all the variants. Results are shown in the 4th to 6th rows Table 4. We find that the results from the naive consistency loss ( i.e. Ours w/o fw) are slightly better than that from the other two specially-designed objectives (i.e. Ours w/ Ent. and Ours w/ Rot.) on average. Besides the possibility of deteriorating the performance [46], our results indicate that empirically select- ing a TTT task may also be far from optimal. Meanwhile, we observe that when enabling fw, the proposed learnable consistency loss is superior to that withoutfw in all target do-Table 4. Comparison between different TTT tasks and parameter selecting strategies in the unseen domain from the PACS benchmark [37]. Here the “Ent.”, “Rot.”, and “Lwcont” denotes the entropy minimization task in [63], the rotation estimation task in [60], and the proposed learnable consistency objective, the “All”, “BN”, and “Ada.” are the strategies that update all the parameters, parameters from the batch normalization layer, and the proposed strategy that updates only the new additional adaptive parameters. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model TTT tasks Param selectings Target domain Avg.Ent. Rot. Lwcont All BN Ada. Art Cartoon Photo Sketch Ours − − ✓ − − ✓ 84.7±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 Ours w/ofw − − − − − ✓ 83.1±0.4 74.6 ±0.6 94.0 ±0.5 78.0 ±0.8 82.5 ±0.1 Ours w/ Ent. ✓ − − − − ✓ 79.9±2.4 77.3 ±0.3 94.8 ±0.8 77.6 ±0.4 82.4 ±0.8 Ours w/ Rot. − ✓ − − − ✓ 81.1±1.0 75.2 ±0.5 94.9 ±0.3 77.3 ±0.6 82.1 ±0.3 Ours w/o TTT − − ✓ − − − 83.3±0.5 76.0 ±0.5 94.4 ±0.5 76.7 ±1.4 82.8 ±0.3 Ours w/ All − − ✓ ✓ − − 83.0±0.7 77.0 ±1.4 94.5 ±0.7 77.4 ±0.9 83.0 ±0.2 Ours w/ BN − − ✓ − ✓ − 81.8±0.5 75.6 ±0.3 94.4 ±0.3 77.9 ±1.1 82.4 ±0.5 mains, and it leads in the term of average accuracy among the variants compared, illustrating its advantage against other adopted TTT tasks. These results are not surprising. By comparing the Grad-CAM [57] visualizations from the main classification task with the learnable and naive consistency losses in Figure 4, we find that the proposed learnable objec- tive can well align with the main loss when fw is enabled as the hot zones activated by these two tasks are similar, which guarantees the improvement for the test-time adapta- tion [46, 60]. Please refer to our supplementary material for more visualizations. 5.3. Effectiveness of the Adaptive Parameters We compare ITTA with three variants to demonstrate the effectiveness of the proposed additional adaptive parameters. (1) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model. (2) Ours w/ ALL: similar to the updating strategy in the original TTT method [60], we update all the parameters from the feature extractor during the test phase. (3) Ours w/ BN: following the suggestion from TENT [63], only parameters from the BN layers of the feature extractor are updated. Note the same pretrained model is shared for all variants in these experiments, and the objectives during the test adaptation phase are to minimize the same learned consistency loss. We list the results in the last three rows in Table 4. We observe that when only updating parameters from the BN layers, the performance is inferior to the strategy without test-time adaptation, and updating all the parameters does not ensure improvements in all target domains. The observations are in line with the findings in [63] that selecting reliable parameters to update is essential in the TTT system and may also interact with the choice of the TTT task. In comparison, when including additional adaptive parameters for updating, the pretrained model can be boosted in all environments. The results validate that our adaptive parameters are more effective than that selected with existing strategies [60, 63] when applied with the proposed learnable test-time objective. 5.4. Limitation Although the proposed learned loss can bring satisfaction improvements, we are aware that the lunch is not free. When the weight subnetwork fw is disabled, updating the joint loss in Eq. (2) only costs 1 forward and 1 backward. However, in order to update fw, we have to compute the second-order derivative in Eq. (5), which will require 1 more forward and 3 more backward processes, bringing extra burden to the system. Our future efforts aim to simplify the overall optimization process and reduce the cost for ITTA. 6. Conclusion In this paper, we aim to improve the current TTT strategy for alleviating the distribution shift problem in DG. First, given that the auxiliary TTT task plays a vital role in the over- all framework, and an empirically selecting one that does not align with the main task may potentially deteriorate instead of improving the performance, we propose a learnable con- sistency loss that can be enforced to be more aligned with the main loss by adjusting its learnable parameters. This strategy is ensured to improve the model and shows favorable perfor- mance against some specially-designed objectives. Second, considering that selecting reliable and effective parameters to update during the test phase is also essential while exhaus- tively trying different combinations may require tremendous effort, we propose a new alternative by including new ad- ditional adaptive parameters for adaptation during the test phase. This alternative is shown to outperform some pre- vious parameter selecting strategies via our experimental findings. By conducting extensive experiments under a rig- orous evaluation protocol, we show that our method can achieve superior performance against existing arts in both the multi-source and single-source DG tasks. Acknowledgements. Liang Chen is supported by the ChinaScholarship Council (CSC Student ID 202008440331). References [1] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. 5, 15, 16, 17 [2] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018. 1, 2, 14, 15 [3] Alexander Bartler, Andre B¨uhler, Felix Wiewel, Mario D¨obler, and Bin Yang. Mt3: Meta test-time training for self- supervised test-time adaption. In AISTATS, 2022. 2, 4, 7 [4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 5, 17 [5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2006. 2 [6] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017. 5, 15, 16, 17 [7] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generaliz- ing from several related classification tasks to a new unlabeled sample. In NeurIPS, 2011. 1 [8] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound domain generalization via meta- knowledge encoding. In CVPR, 2022. 1 [9] Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. Mix and reason: Reasoning over se- mantic topology with data mixing for domain generalization. In NeurIPS, 2022. 1 [10] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, 2022. 2 [11] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarial example: Towards good generalizations for deepfake detection. In CVPR, 2022. 2 [12] Liang Chen, Yong Zhang, Yibing Song, Jue Wang, and Lingqiao Liu. Ost: Improving generalization of deepfake detection via one-shot test-time training. In NeurIPS, 2022. 2, 12 [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof- frey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 2, 3 [14] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, 2022. 2 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5 [16] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019. 1, 2 [17] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010. 5 [18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013. 5, 16 [19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener- ative visual models from few training examples: An incre- mental bayesian approach tested on 101 object categories. In CVPR worksho, 2004. 5 [20] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 2, 7 [21] Francois Fleuret et al. Uncertainty reduction for model adap- tation in semantic segmentation. In CVPR, 2021. 2 [22] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 2 [23] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marc- hand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016. 1, 2, 5, 15, 16, 17 [24] Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for domain adaptation and domain generalization. IEEE TPAMI, 39(7):1414–1430, 2016. 1 [25] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In ICCV, 2015. 2 [26] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- sch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. 2, 3 [27] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021. 1, 2, 5, 6, 14, 15, 16, 17 [28] Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, et al. Unsupervised domain general- ization by learning a bridge across domains. In CVPR, 2022. 1 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. In CVPR, 2020. 2, 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 5, 7, 14 [31] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In UAI, 2020. 1 [32] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. 2 [33] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In ECCV, 2020. 5, 15, 16, 17[34] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regular- ization for domain generalization. In ICCV, 2021. 2, 5, 6, 15, 16, 17 [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribu- tion shifts. In ICML, 2021. 1 [36] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021. 5, 6, 15, 16, 17 [37] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In ICCV, 2017. 1, 5, 6, 7, 8, 12, 13, 14, 15 [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, 2018. 1, 2, 5, 6, 15, 16, 17 [39] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M Hospedales. Episodic training for domain generalization. In ICCV, 2019. 1, 2 [40] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In CVPR, 2018. 1, 2, 5, 15, 16, 17 [41] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple feature augmentation for domain generalization. In ICCV, 2021. 1, 2, 12, 14 [42] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out- of-distribution generalization. In ICLR, 2022. 1, 2 [43] Yizhuo Li, Miao Hao, Zonglin Di, Nitesh Bharadwaj Gun- davarapu, and Xiaolong Wang. Test-time personalization with a transformer for human pose estimation. In NeurIPS, 2021. 2, 3, 4 [44] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018. 1, 2, 5, 15, 16, 17 [45] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous do- main generalization. In ICML, 2019. 14, 15 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In NeurIPS, 2021. 2, 3, 4, 7, 8, 12, 14, 15 [47] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant feature representation. In ICML, 2013. 1, 2 [48] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In CVPR, 2021. 2, 5, 6, 15, 16, 17 [49] Prashant Pandey, Mrigank Raman, Sumanth Varambally, and Prathosh Ap. Generalization on unseen domains via inference- time label-preserving target projections. In CVPR, 2021. 1 [50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, 2019. 5, 17 [51] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient star- vation: A learning proclivity in neural networks. In NeurIPS, 2021. 1, 5, 6, 15, 16, 17 [52] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution gen- eralization. In ICML, 2022. 1, 2, 5, 6, 15, 16, 17 [53] Yangjun Ruan, Yann Dubois, and Chris J Maddison. Optimal representations for covariate shift. In ICLR, 2022. 5, 15, 16, 17 [54] Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 77(1):157–173, 2008. 5 [55] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. In ICLR, 2020. 5, 15, 16, 17 [56] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [57] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad- cam: Visual explanations from deep networks via gradient- based localization. In ICCV, 2017. 7, 8, 11, 13 [58] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. In ICLR, 2021. 1, 2, 5, 6, 15, 16, 17 [59] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV, 2016. 5, 6, 15, 16, 17 [60] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, 2020. 1, 2, 4, 5, 7, 8, 11, 12, 13 [61] Vladimir Vapnik. The nature of statistical learning theory . Springer science & business media, 1999. 1, 5, 6, 15, 16, 17 [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In CVPR, 2017. 5, 16 [63] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 2, 3, 4, 7, 8, 11, 12, 13 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In CVPR, 2010. 5 [65] Zehao Xiao, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Learning to generalize across domains on single test samples. In ICLR, 2022. 2 [66] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generaliza- tion. In CVPR, 2021. 1, 2 [67] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 7[68] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020. 2, 5, 15, 16, 17 [69] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu- Chiang Frank Wang. Adversarial teacher-student representa- tion learning for domain generalization. In NeurIPS, 2021. 1, 2 [70] Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and Jun Zhu. Ood-bench: Quantifying and understanding two dimensions of out-of- distribution generalization. In CVPR, 2022. 5 [71] Fuming You, Jingjing Li, and Zhou Zhao. Test-time batch statistics calibration for covariate shift. arXiv preprint arXiv:2110.04065, 2021. 4 [72] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: A meta-learning approach for tackling group distri- bution shift. arXiv preprint arXiv:2007.02931, 2020. 5, 15, 16, 17 [73] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk mini- mization: Learning to adapt to domain shift. NeurIPS, 2021. 2 [74] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-dmoe: Adapting to domain shift by meta- distillation from mixture-of-experts. In NeurIPS, 2022. 2 [75] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 1, 2, 3, 5, 6, 7, 12, 15, 16, 17 Appendix In this supplementary material, we provide, 1. Resource usage for ITTA in Section 7. 2. Grad-CAM visualizations of different loss terms in Section 8. 3. Parameter analysis of ITTA in Section 9; 4. Using a different augmentation skill for ITTA in Sec- tion 10. 5. Using different updating steps or a strategy for ITTA during the test phase in Section 11. 6. Using different network structures for the learnable consistency loss and adaptive parameters in Section 12. 7. Comparisons with other related methods in Section 13. 8. Detailed experimental results in the DomainBed bench- mark in Section 14. 7. Resource Usage Comparisons Between ITTA and the Baseline Model Requiring extra resources for our ITTA is a common lim- itation for existing test-time-based arts. To further evaluate our method, in this section, we compare FLOPS, model size, and inference time in Table 5. We compare only with ERM as most existing methods utilize the same network during in- ferences. We note that compare to the baseline model, ITTA requires extra Flops and processing time, this is because the adaptation process uses extra forward and backward steps during the test phase. While the parameters between the two models are similar because the newly included adaptive blocks are much smaller in size compared to the original model. Table 5. Resource comparisons during testing. Here inc. and exc. columns in ITTA indicate to include and exclude the TTA phase. Model Flops (G) Params (M) Time (s) Baseline 1.82 11.18 0.004 ITTA (inc.| exc.) 6.12 | 1.83 14.95 | 14.94 0.021 | 0.005 8. Grad-CAM Visualizations of Different Self- Supervised Objectives In Section 5 of the manuscript, we provide Grad-CAM [57] visualizations of our learnable consistency and the main losses to illustrate their alignment. To further show the differences between several TTT tasks [60, 63], we present more visual examples in this section. Results are shown in Figure 5. We observe that the entropy minimization [63] and rotation estimation [60] objectives do not activate the same regions as the main loss. As shown in the first row, for the class label of giraffe, both the main loss and our learned loss can correctly locate the two giraffes in the image, while the rotation estimation task can only locate one target, the same observation can be found when the learned weightsare disabled in our loss term. Meanwhile, although the two objects can be found for the entropy minimization task, the corresponding hot region does not align with that of the main loss. Similar phenomena can be observed in other samples. These visual examples demonstrate that our learned objective can better align with the main task than the TTT tasks adopted in previous works [60, 63], explaining why using the proposed learnable consistency loss can better improve TTT. 9. Parameter Analysis In this section, we analyze the hyper-parameter used in ITTA. We use the weight parameterα to balance the contri- butions from the main loss and weighted consistency loss (i.e. Lmain + αLwcont in Eq. (2) of our manuscript). To analyze the sensitivity of ITTA regarding different values of α, we conduct ablation studies in the PACS benchmark [37]. Results are listed in Table 6. We observe that the proposed ITTA can obtain favorable performances when α is in the range of 0.1 to 10, and it performs the best on average when setting as 1. We thus fix the parameter as 1 in all experi- ments. 10. A Different Augmentation Skill for ITTA In our manuscript, we use the existing augmentation strat- egy from [75] to obtain the augmented feature. In this sec- tion, we replace this implementation with that from [41] to further verify if our ITTA can still thrive with another aug- mentation skill. Different from [75] that mixes the statics of the feature to synthesize new information, [41] uses an affine transformation to create new features, where the weight for the transformation is sampled from a normal distribution with the mean value of one and standard value of zero, and the bias for the transformation is sampled from a normal distribution with the mean and standard values both zero. Experiments are conducted on the PACS benchmark [37] with the leave-one-out strategy. We compare ITTA with several different variants. (1) Ours w/o fw & TTT: this variant is the baseline model which uses the naive consistency loss for training and does not include TTT during the test phase. (2) Ours w/o fw: we disable the fw in our consistency loss, which uses the naive consistency loss for the test-time updating. (3) Ours w/o TTT: we do not update any parameters during the test phase. This variant is used to verify whether TTT can improve the pretrained model when replacing the augmentation strategy. We also compare these variants with the ERM method to show their effectivenesses. Results are listed in Table 7. We observe that ERM per- forms favorably against the baseline model, indicating that this augmentation strategy may not be beneficial for the training process. Meanwhile, we observe that when fw is disabled, the performances seem to decrease in 3 out of 4 target domains, and the average accuracy is also inferior to the baseline (i.e. Ours w/o fw & TTT). This result is in line with the finding in [46] that an inappropriate TTT task may deteriorate the performance. In comparison, we note that the performances are both improved when fw is enabled (i.e. Ours w/o TTT and Ours), which once again demonstrates that the proposed learnable consistency loss can improve the trained model. Moreover, we can also observe that when combining fw and TTT, our model is superior to other vari- ants and the ERM method. These results demonstrate that the proposed two strategies can improve the current TTT framework despite a less effective augmentation strategy. 11. Different Updating Steps or Strategies for ITTA In the manuscript, we use one TTT step for ITTA before during the testing step. In this section, we conduct experi- ments to evaluate the performances of ITTA with different TTT steps. Experiments are conducted on the PACS bench- mark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper- parameter settings. Results are listed in Table 8. We observe that the average accuracies of using more TTT steps are not improved greatly while the computational times are propor- tional to the TTT steps. To this end, we use one TTT step for ITTA as a compromise between accuracy and efficiency. We use the online setting from TTT [60] for all arts, which assumes test samples arrive sequentially and updates the adaptive blocks based on the states optimized from a previous sample. In this section, we also test ITTA in an episodic manner (i.e. Epi) [12]. Results in Table 8 suggest that while the episodic updating strategy performs slightly worse than the current scheme, and it still outperforms the baseline. 12. Different Network Structures for the Learnable Consistency Loss and Adaptive Parameters In our implementation, we use 10 layers of learnable pa- rameters for fw, and we use 5 layers of learnable parameters for fΘ after each block. In this section, we evaluate our ITTA with different network structures for these two mod- ules. Specifically, we compare the original implementation with the variants that use 1, 5, and 15 layers for fw and 1, 10, and 15 layers for fΘ to evaluate the performances of dif- ferent structures. Similarly, we conduct experiments on the PACS benchmark [37] with the leave-one-out strategy, and each target domain is examined with 60 sets of random seeds and hyper-parameter settings. Evaluation results are listed in Table 9. We observe that their differences in the average accuracy are rather subtle on account of the variances. To(a) Input (b) Entropy (c) Rotation (d) Ours w/o fw (e) Ours (f) Main Figure 5. Grad-CAM [57] visualizations from different loss terms. We use images with varying class labels (i.e. giraffe, elephant, house, and horse from top to bottom) from the four target domains of PACS [37] as inputs (i.e. art, cartoon, photo, and sketch domains from top to bottom). “Entropy” and “Rotation” here denote the entropy minimization and rotation estimation tasks in [63] and [60]. Ours w/o fw is the learnable consistency loss in Eq. (1) in the manuscript (i.e. ∥fw(z − z′)∥) when fw is disabled. The proposed learnable consistency loss can align well with the main classification task. Table 6. Sensitivity analysis of ITTA regarding different values ofα in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Values Target domain Avg.Art Cartoon Photo Sketch α = 0.1 83.9 ± 0.7 76.2 ± 1.1 94.8 ± 0.2 78.8 ± 0.8 83.4 ± 0.2 α = 1 (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 α = 10 83.9 ± 0.5 77.4 ± 0.6 94.2 ± 0.7 77.3 ± 0.8 83.2 ± 0.3 α = 100 81.5 ± 1.2 77.0 ± 0.6 92.6 ± 0.7 78.9 ± 2.1 82.5 ± 0.9 this end, we use the original implementation with 10 layers of learnable parameters for fw and 5 layers of learnable pa- rameters for fΘ, which performs relatively better than other variants. Since the adaptive blocks fΘ are attached after each layer of the network, one may wonder how the varying locations of the adaptive blocks affect the performance of ITTA. To answer this question, we further conduct experiments by adding the adaptive blocks after different layers of the orig- inal network. Denoting as Loc = lan given the n layers in the original network, we note that the model performs less effectively when the adaptive block is placed after the 1st layer of the network, and using all four adaptive blocks (i.e. ours) is more effective than other alternatives. 13. Comparisons with Other Related Methods Apart from the proposed ITTA, some other works also propose to include learnable parameters in their auxiliaryTable 7. Performances of our method with another augmentation strategy from [41] in the unseen domain from PACS [37]. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Model Target domain Avg.Art Cartoon Photo Sketch ERM 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 Ours w/o fw & TTT 74.9 ± 0.4 74.1 ± 0.8 90.6 ± 0.3 79.7 ± 0.7 79.8 ± 0.4 Ours w/o fw 77.1 ± 1.0 73.6 ± 1.1 89.9 ± 0.4 78.4 ± 0.8 79.7 ± 0.2 Ours w/o TTT 77.5 ± 0.3 73.2 ± 0.6 92.4 ± 0.4 78.0 ± 1.0 80.3 ± 0.3 Ours (w/ fw & TTT) 79.2 ± 0.8 74.9 ± 1.1 92.2 ± 0.3 76.9 ± 0.7 80.8 ± 0.4 Table 8. Evaluations of ITTA in the unseen domain from PACS [37] with different TTT steps and updating strategies during the testing phase. The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. The time consumption (TC) is computed using one image with the size of 224 × 224. Epi. denotes updating ITTA in an episodic manner. Steps Target domain Avg. TCArt Cartoon Photo Sketch 1 step (Ours) 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 2.4 ms 2 step 84.2 ± 0.9 77.5 ± 0.6 94.4 ± 0.4 79.1 ± 1.0 83.8 ± 0.1 4.2 ms 3 step 84.5 ± 1.2 77.6 ± 0.6 94.0 ± 0.6 79.3 ± 0.1 83.9 ± 0.3 6.1 ms Epi. 83.6 ± 0.7 77.9 ± 0.5 95.2 ± 0.1 76.6 ± 0.5 83.3 ± 0.4 losses. Examples include MetaReg [2] and Feature-Critic [45] which both suggest using meta-learning to produce more general models. The main difference between these arts and ITTA is that parameters in the auxiliary loss from [2,45] are gradually refined by episode training, and they are updated via a gradient alignment step in ITTA (see Sec. 3.1 in the manuscript), which is much simpler. In this sec- tion, we compare ITTA with these two arts in the PACS dataset [37] using the same settings aforementioned. Be- cause MetaReg [2] does not release codes, we thus directly cite the data from their paper in the comparison. Different from others, the results in [2] are averaged by 5 trials accord- ing to their paper, which is much less than our experimental settings. Meanwhile, we also compare with TTT++ [46] which suggests storing the momentum of the features from the source domain and enforcing the similarity between mo- mentums of features from the source and target domains. We use the same setting in Section 5.1 from the manuscript to evaluate TTT++. Results are listed in Table 10. We observe that our method consistently outperforms that from [2,45,46] for both the cases with and without TTT, indicating that the proposed learnable consistency loss and updating method is not only simpler but also more effective than the losses in [2, 45]. 14. Detailed Results in the DomainBed Bench- mark [27] this section presents the average accuracy in each domain from different datasets. As shown in Table 11, 12, 13, 14, and 15, these results are detailed illustrations of the results in Table 2 in our manuscript. For all the experiments, we use the “training-domain validate set” as the model selection method. A total of 22 methods are examined for 60 trials in each unseen domain, and all methods are trained with the leave-one-out strategy using the ResNet18 [30] backbones.Table 9. Performances of our method with different network structures for the consistency loss (i.e. fw) and adaptive parameters (i.e. fΘ) in the unseen domain from PACS [37]. Here ‘Loc=lan’ locates the adaptive block after the n-th layer of the model (‘la4’ is the last layer). The reported accuracies (%) and standard deviations are computed from 60 trials in each target domain. Structures Target domain Avg.Art Cartoon Photo Sketch Structures offw 1 layer 83.5 ±1.2 76.0 ±1.0 95.3 ±0.2 78.7 ±1.5 83.4 ±0.4 5 layers 83.7 ±0.6 76.8 ±0.9 94.6 ±0.3 78.8 ±0.3 83.5 ±0.3 10 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 15 layers 84.1 ±0.4 75.8 ±0.2 94.3 ±0.3 79.5 ±0.4 83.4 ±0.2 Structures offΘ 1 layer 84.0 ±0.6 77.4 ±0.5 94.4 ±0.5 78.3 ±0.4 83.5 ±0.3 5 layers (Ours) 84.7 ±0.4 78.0 ±0.4 94.5 ±0.4 78.2 ±0.3 83.8 ±0.3 10 layers 84.8 ±0.3 76.0 ±0.6 94.1 ±0.5 78.3 ±0.1 83.3 ±0.3 15 layers 83.9 ±0.8 76.0 ±0.5 93.8 ±0.4 78.7 ±1.4 83.1 ±0.6 Locations offΘ Loc=la1 83.4±0.7 76.8 ±0.3 94.4 ±0.3 77.8 ±0.3 83.1 ±0.3 Loc=la2 83.4±0.6 77.7 ±0.6 94.2 ±0.5 78.0 ±0.5 83.3 ±0.3 Loc=la3 84.0±0.4 77.5 ±0.3 94.4 ±0.1 77.8 ±0.1 83.4 ±0.2 Loc=la4 84.1±0.7 77.8 ±0.5 94.8 ±0.2 76.9 ±1.5 83.4 ±0.4 Table 10. Compare with learnable losses in [2, 45] in the unseen domain from PACS [37]. The reported accuracies ( %) and standard deviations are computed from 60 trials in each target domain except for [2] where the numbers are directly cited from their paper. Model Target domain Avg.Art Cartoon Photo Sketch MetaReg [2] 83.7 ± 0.2 77.2 ± 0.3 95.5 ± 0.2 70.3 ± 0.3 81.7 Feture-Critic [45] 78.4 ± 1.6 75.4 ± 1.2 92.6 ± 0.5 73.3 ± 1.4 80.0 ± 0.3 TTT++ [46] 84.3 ± 0.1 78.4 ± 0.5 93.8 ± 1.3 73.2 ± 3.2 82.4 ± 1.1 Ours w/o TTT 83.3 ± 0.5 76.0 ± 0.5 94.4 ± 0.5 76.7 ± 1.4 82.8 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3 Table 11. Average accuracies on the PACS [37] datasets using the default hyper-parameter settings in DomainBed [27]. art cartoon photo sketch Average ERM [61] 78.0 ± 1.3 73.4 ± 0.8 94.1 ± 0.4 73.6 ± 2.2 79.8 ± 0.4 IRM [1] 76.9 ± 2.6 75.1 ± 0.7 94.3 ± 0.4 77.4 ± 0.4 80.9 ± 0.5 GroupGRO [55] 77.7 ± 2.6 76.4 ± 0.3 94.0 ± 0.3 74.8 ± 1.3 80.7 ± 0.4 Mixup [68] 79.3 ± 1.1 74.2 ± 0.3 94.9 ± 0.3 68.3 ± 2.7 79.2 ± 0.9 MLDG [38] 78.4 ± 0.7 75.1 ± 0.5 94.8 ± 0.4 76.7 ± 0.8 81.3 ± 0.2 CORAL [59] 81.5 ± 0.5 75.4 ± 0.7 95.2 ± 0.5 74.8 ± 0.4 81.7 ± 0.0 MMD [40] 81.3 ± 0.6 75.5 ± 1.0 94.0 ± 0.5 74.3 ± 1.5 81.3 ± 0.8 DANN [23] 79.0 ± 0.6 72.5 ± 0.7 94.4 ± 0.5 70.8 ± 3.0 79.2 ± 0.3 CDANN [44] 80.4 ± 0.8 73.7 ± 0.3 93.1 ± 0.6 74.2 ± 1.7 80.3 ± 0.5 MTL [6] 78.7 ± 0.6 73.4 ± 1.0 94.1 ± 0.6 74.4 ± 3.0 80.1 ± 0.8 SagNet [48] 82.9 ± 0.4 73.2 ± 1.1 94.6 ± 0.5 76.1 ± 1.8 81.7 ± 0.6 ARM [72] 79.4 ± 0.6 75.0 ± 0.7 94.3 ± 0.6 73.8 ± 0.6 80.6 ± 0.5 VREx [36] 74.4 ± 0.7 75.0 ± 0.4 93.3 ± 0.3 78.1 ± 0.9 80.2 ± 0.5 RSC [33] 78.5 ± 1.1 73.3 ± 0.9 93.6 ± 0.6 76.5 ± 1.4 80.5 ± 0.2 SelfReg [34] 82.5 ± 0.8 74.4 ± 1.5 95.4 ± 0.5 74.9 ± 1.3 81.8 ± 0.3 MixStyle [75] 82.6 ± 1.2 76.3 ± 0.4 94.2 ± 0.3 77.5 ± 1.3 82.6 ± 0.4 Fish [58] 80.9 ± 1.0 75.9 ± 0.4 95.0 ± 0.4 76.2 ± 1.0 82.0 ± 0.3 SD [51] 83.2 ± 0.6 74.6 ± 0.3 94.6 ± 0.1 75.1 ± 1.6 81.9 ± 0.3 CAD [53] 83.9 ± 0.8 74.2 ± 0.4 94.6 ± 0.4 75.0 ± 1.2 81.9 ± 0.3 CondCAD [53] 79.7 ± 1.0 74.2 ± 0.9 94.6 ± 0.4 74.8 ± 1.4 80.8 ± 0.5 Fishr [52] 81.2 ± 0.4 75.8 ± 0.8 94.3 ± 0.3 73.8 ± 0.6 81.3 ± 0.3 Ours 84.7 ± 0.4 78.0 ± 0.4 94.5 ± 0.4 78.2 ± 0.3 83.8 ± 0.3Table 12. Average accuracies on the VLCS [18] datasets using the default hyper-parameter settings in DomainBed [27]. Caltech LabelMe Sun VOC Average ERM [61] 97.7 ± 0.3 62.1 ± 0.9 70.3 ± 0.9 73.2 ± 0.7 75.8 ± 0.2 IRM [1] 96.1 ± 0.8 62.5 ± 0.3 69.9 ± 0.7 72.0 ± 1.4 75.1 ± 0.1 GroupGRO [55] 96.7 ± 0.6 61.7 ± 1.5 70.2 ± 1.8 72.9 ± 0.6 75.4 ± 1.0 Mixup [68] 95.6 ± 1.5 62.7 ± 0.4 71.3 ± 0.3 75.4 ± 0.2 76.2 ± 0.3 MLDG [38] 95.8 ± 0.5 63.3 ± 0.8 68.5 ± 0.5 73.1 ± 0.8 75.2 ± 0.3 CORAL [59] 96.5 ± 0.3 62.8 ± 0.1 69.1 ± 0.6 73.8 ± 1.0 75.5 ± 0.4 MMD [40] 96.0 ± 0.8 64.3 ± 0.6 68.5 ± 0.6 70.8 ± 0.1 74.9 ± 0.5 DANN [23] 97.2 ± 0.1 63.3 ± 0.6 70.2 ± 0.9 74.4 ± 0.2 76.3 ± 0.2 CDANN [44] 95.4 ± 1.2 62.6 ± 0.6 69.9 ± 1.3 76.2 ± 0.5 76.0 ± 0.5 MTL [6] 94.4 ± 2.3 65.0 ± 0.6 69.6 ± 0.6 71.7 ± 1.3 75.2 ± 0.3 SagNet [48] 94.9 ± 0.7 61.9 ± 0.7 69.6 ± 1.3 75.2 ± 0.6 75.4 ± 0.8 ARM [72] 96.9 ± 0.5 61.9 ± 0.4 71.6 ± 0.1 73.3 ± 0.4 75.9 ± 0.3 VREx [36] 96.2 ± 0.0 62.5 ± 1.3 69.3 ± 0.9 73.1 ± 1.2 75.3 ± 0.6 RSC [33] 96.2 ± 0.0 63.6 ± 1.3 69.8 ± 1.0 72.0 ± 0.4 75.4 ± 0.3 SelfReg [34] 95.8 ± 0.6 63.4 ± 1.1 71.1 ± 0.6 75.3 ± 0.6 76.4 ± 0.7 MixStyle [75] 97.3 ± 0.3 61.6 ± 0.1 70.4 ± 0.7 71.3 ± 1.9 75.2 ± 0.7 Fish [58] 97.4 ± 0.2 63.4 ± 0.1 71.5 ± 0.4 75.2 ± 0.7 76.9 ± 0.2 SD [51] 96.5 ± 0.4 62.2 ± 0.0 69.7 ± 0.9 73.6 ± 0.4 75.5 ± 0.4 CAD [53] 94.5 ± 0.9 63.5 ± 0.6 70.4 ± 1.2 72.4 ± 1.3 75.2 ± 0.6 CondCAD [53] 96.5 ± 0.8 62.6 ± 0.4 69.1 ± 0.2 76.0 ± 0.2 76.1 ± 0.3 Fishr [52] 97.2 ± 0.6 63.3 ± 0.7 70.4 ± 0.6 74.0 ± 0.8 76.2 ± 0.3 Ours 96.9 ± 1.2 63.7 ± 1.1 72.0 ± 0.3 74.9 ± 0.8 76.9 ± 0.6 Table 13. Average accuracies on the OfficeHome [62] datasets using the default hyper-parameter settings in DomainBed [27]. art clipart product real Average ERM [61] 52.2 ± 0.2 48.7 ± 0.5 69.9 ± 0.5 71.7 ± 0.5 60.6 ± 0.2 IRM [1] 49.7 ± 0.2 46.8 ± 0.5 67.5 ± 0.4 68.1 ± 0.6 58.0 ± 0.1 GroupGRO [55] 52.6 ± 1.1 48.2 ± 0.9 69.9 ± 0.4 71.5 ± 0.8 60.6 ± 0.3 Mixup [68] 54.0 ± 0.7 49.3 ± 0.7 70.7 ± 0.7 72.6 ± 0.3 61.7 ± 0.5 MLDG [38] 53.1 ± 0.3 48.4 ± 0.3 70.5 ± 0.7 71.7 ± 0.4 60.9 ± 0.2 CORAL [59] 55.1 ± 0.7 49.7 ± 0.9 71.8 ± 0.2 73.1 ± 0.5 62.4 ± 0.4 MMD [40] 50.9 ± 1.0 48.7 ± 0.3 69.3 ± 0.7 70.7 ± 1.3 59.9 ± 0.4 DANN [23] 51.8 ± 0.5 47.1 ± 0.1 69.1 ± 0.7 70.2 ± 0.7 59.5 ± 0.5 CDANN [44] 51.4 ± 0.5 46.9 ± 0.6 68.4 ± 0.5 70.4 ± 0.4 59.3 ± 0.4 MTL [6] 51.6 ± 1.5 47.7 ± 0.5 69.1 ± 0.3 71.0 ± 0.6 59.9 ± 0.5 SagNet [48] 55.3 ± 0.4 49.6 ± 0.2 72.1 ± 0.4 73.2 ± 0.4 62.5 ± 0.3 ARM [72] 51.3 ± 0.9 48.5 ± 0.4 68.0 ± 0.3 70.6 ± 0.1 59.6 ± 0.3 VREx [36] 51.1 ± 0.3 47.4 ± 0.6 69.0 ± 0.4 70.5 ± 0.4 59.5 ± 0.1 RSC [33] 49.0 ± 0.1 46.2 ± 1.5 67.8 ± 0.7 70.6 ± 0.3 58.4 ± 0.6 SelfReg [34] 55.1 ± 0.8 49.2 ± 0.6 72.2 ± 0.3 73.0 ± 0.3 62.4 ± 0.1 MixStyle [75] 50.8 ± 0.6 51.4 ± 1.1 67.6 ± 1.3 68.8 ± 0.5 59.6 ± 0.8 Fish [58] 54.6 ± 1.0 49.6 ± 1.0 71.3 ± 0.6 72.4 ± 0.2 62.0 ± 0.6 SD [51] 55.0 ± 0.4 51.3 ± 0.5 72.5 ± 0.2 72.7 ± 0.3 62.9 ± 0.2 CAD [53] 52.1 ± 0.6 48.3 ± 0.5 69.7 ± 0.3 71.9 ± 0.4 60.5 ± 0.3 CondCAD [53] 53.3 ± 0.6 48.4 ± 0.2 69.8 ± 0.9 72.6 ± 0.1 61.0 ± 0.4 Fishr [52] 52.6 ± 0.9 48.6 ± 0.3 69.9 ± 0.6 72.4 ± 0.4 60.9 ± 0.3 Ours 54.4 ± 0.2 52.3 ± 0.8 69.5 ± 0.3 71.7 ± 0.2 62.0 ± 0.2Table 14. Average accuracies on the TerraInc [4] datasets using the default hyper-parameter settings in DomainBed [27]. L100 L38 L43 L46 Average ERM [61] 42.1 ± 2.5 30.1 ± 1.2 48.9 ± 0.6 34.0 ± 1.1 38.8 ± 1.0 IRM [1] 41.8 ± 1.8 29.0 ± 3.6 49.6 ± 2.1 33.1 ± 1.5 38.4 ± 0.9 GroupGRO [55] 45.3 ± 4.6 36.1 ± 4.4 51.0 ± 0.8 33.7 ± 0.9 41.5 ± 2.0 Mixup [68] 49.4 ± 2.0 35.9 ± 1.8 53.0 ± 0.7 30.0 ± 0.9 42.1 ± 0.7 MLDG [38] 39.6 ± 2.3 33.2 ± 2.7 52.4 ± 0.5 35.1 ± 1.5 40.1 ± 0.9 CORAL [59] 46.7 ± 3.2 36.9 ± 4.3 49.5 ± 1.9 32.5 ± 0.7 41.4 ± 1.8 MMD [40] 49.1 ± 1.2 36.4 ± 4.8 50.4 ± 2.1 32.3 ± 1.5 42.0 ± 1.0 DANN [23] 44.3 ± 3.6 28.0 ± 1.5 47.9 ± 1.0 31.3 ± 0.6 37.9 ± 0.9 CDANN [44] 36.9 ± 6.4 32.7 ± 6.2 51.1 ± 1.3 33.5 ± 0.5 38.6 ± 2.3 MTL [6] 45.2 ± 2.6 31.0 ± 1.6 50.6 ± 1.1 34.9 ± 0.4 40.4 ± 1.0 SagNet [48] 36.3 ± 4.7 40.3 ± 2.0 52.5 ± 0.6 33.3 ± 1.3 40.6 ± 1.5 ARM [72] 41.5 ± 4.5 27.7 ± 2.4 50.9 ± 1.0 29.6 ± 1.5 37.4 ± 1.9 VREx [36] 48.0 ± 1.7 41.1 ± 1.5 51.8 ± 1.5 32.0 ± 1.2 43.2 ± 0.3 RSC [33] 42.8 ± 2.4 32.2 ± 3.8 49.6 ± 0.9 32.9 ± 1.2 39.4 ± 1.3 SelfReg [34] 46.1 ± 1.5 34.5 ± 1.6 49.8 ± 0.3 34.7 ± 1.5 41.3 ± 0.3 MixStyle [75] 50.6 ± 1.9 28.0 ± 4.5 52.1 ± 0.7 33.0 ± 0.2 40.9 ± 1.1 Fish [58] 46.3 ± 3.0 29.0 ± 1.1 52.7 ± 1.2 32.8 ± 1.0 40.2 ± 0.6 SD [51] 45.5 ± 1.9 33.2 ± 3.1 52.9 ± 0.7 36.4 ± 0.8 42.0 ± 1.0 CAD [53] 43.1 ± 2.6 31.1 ± 1.9 53.1 ± 1.6 34.7 ± 1.3 40.5 ± 0.4 CondCAD [53] 44.4 ± 2.9 32.9 ± 2.5 50.5 ± 1.3 30.8 ± 0.5 39.7 ± 0.4 Fishr [52] 49.9 ± 3.3 36.6 ± 0.9 49.8 ± 0.2 34.2 ± 1.3 42.6 ± 1.0 Ours 51.7 ± 2.4 37.6 ± 0.6 49.9 ± 0.6 33.6 ± 0.6 43.2 ± 0.5 Table 15. Average accuracies on the DomainNet [50] datasets using the default hyper-parameter settings in DomainBed [27]. clip info paint quick real sketch Average ERM [61] 50.4 ± 0.2 14.0 ± 0.2 40.3 ± 0.5 11.7 ± 0.2 52.0 ± 0.2 43.2 ± 0.3 35.3 ± 0.1 IRM [1] 43.2 ± 0.9 12.6 ± 0.3 35.0 ± 1.4 9.9 ± 0.4 43.4 ± 3.0 38.4 ± 0.4 30.4 ± 1.0 GroupGRO [55] 38.2 ± 0.5 13.0 ± 0.3 28.7 ± 0.3 8.2 ± 0.1 43.4 ± 0.5 33.7 ± 0.0 27.5 ± 0.1 Mixup [68] 48.9 ± 0.3 13.6 ± 0.3 39.5 ± 0.5 10.9 ± 0.4 49.9 ± 0.2 41.2 ± 0.2 34.0 ± 0.0 MLDG [38] 51.1 ± 0.3 14.1 ± 0.3 40.7 ± 0.3 11.7 ± 0.1 52.3 ± 0.3 42.7 ± 0.2 35.4 ± 0.0 CORAL [59] 51.2 ± 0.2 15.4 ± 0.2 42.0 ± 0.2 12.7 ± 0.1 52.0 ± 0.3 43.4 ± 0.0 36.1 ± 0.2 MMD [40] 16.6 ± 13.3 0.3 ± 0.0 12.8 ± 10.4 0.3 ± 0.0 17.1 ± 13.7 0.4 ± 0.0 7.9 ± 6.2 DANN [23] 45.0 ± 0.2 12.8 ± 0.2 36.0 ± 0.2 10.4 ± 0.3 46.7 ± 0.3 38.0 ± 0.3 31.5 ± 0.1 CDANN [44] 45.3 ± 0.2 12.6 ± 0.2 36.6 ± 0.2 10.3 ± 0.4 47.5 ± 0.1 38.9 ± 0.4 31.8 ± 0.2 MTL [6] 50.6 ± 0.2 14.0 ± 0.4 39.6 ± 0.3 12.0 ± 0.3 52.1 ± 0.1 41.5 ± 0.0 35.0 ± 0.0 SagNet [48] 51.0 ± 0.1 14.6 ± 0.1 40.2 ± 0.2 12.1 ± 0.2 51.5 ± 0.3 42.4 ± 0.1 35.3 ± 0.1 ARM [72] 43.0 ± 0.2 11.7 ± 0.2 34.6 ± 0.1 9.8 ± 0.4 43.2 ± 0.3 37.0 ± 0.3 29.9 ± 0.1 VREx [36] 39.2 ± 1.6 11.9 ± 0.4 31.2 ± 1.3 10.2 ± 0.4 41.5 ± 1.8 34.8 ± 0.8 28.1 ± 1.0 RSC [33] 39.5 ± 3.7 11.4 ± 0.8 30.5 ± 3.1 10.2 ± 0.8 41.0 ± 1.4 34.7 ± 2.6 27.9 ± 2.0 SelfReg [34] 47.9 ± 0.3 15.1 ± 0.3 41.2 ± 0.2 11.7 ± 0.3 48.8 ± 0.0 43.8 ± 0.3 34.7 ± 0.2 MixStyle [75] 49.1 ± 0.4 13.4 ± 0.0 39.3 ± 0.0 11.4 ± 0.4 47.7 ± 0.3 42.7 ± 0.1 33.9 ± 0.1 Fish [58] 51.5 ± 0.3 14.5 ± 0.2 40.4 ± 0.3 11.7 ± 0.5 52.6 ± 0.2 42.1 ± 0.1 35.5 ± 0.0 SD [51] 51.3 ± 0.3 15.5 ± 0.1 41.5 ± 0.3 12.6 ± 0.2 52.9 ± 0.2 44.0 ± 0.4 36.3 ± 0.2 CAD [53] 45.4 ± 1.0 12.1 ± 0.5 34.9 ± 1.1 10.2 ± 0.6 45.1 ± 1.6 38.5 ± 0.6 31.0 ± 0.8 CondCAD [53] 46.1 ± 1.0 13.3 ± 0.4 36.1 ± 1.4 10.7 ± 0.2 46.8 ± 1.3 38.7 ± 0.7 31.9 ± 0.7 Fishr [52] 47.8 ± 0.7 14.6 ± 0.2 40.0 ± 0.3 11.9 ± 0.2 49.2 ± 0.7 41.7 ± 0.1 34.2 ± 0.3 Ours 50.7 ± 0.7 13.9 ± 0.4 39.4 ± 0.5 11.9 ± 0.2 50.2 ± 0.3 43.5 ± 0.1 34.9 ± 0.1",
      "meta_data": {
        "arxiv_id": "2304.04494v2",
        "authors": [
          "Liang Chen",
          "Yong Zhang",
          "Yibing Song",
          "Ying Shan",
          "Lingqiao Liu"
        ],
        "published_date": "2023-04-10T10:12:38Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04494v2.pdf",
        "github_url": "https://github.com/liangchen527/ITTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the distribution shift problem in Domain Generalization (DG) by proposing an Improved Test-Time Adaptation (ITTA) method. The main contributions are: 1) Introducing a learnable consistency loss for the test-time training (TTT) task, which features learnable parameters that can be adjusted to align better with the main prediction task by enforcing shared optimization directions. 2) Proposing the inclusion of additional adaptive parameters within the trained model, with the strategy of updating only these new parameters during the test phase, leaving the original model parameters unchanged. Extensive experiments demonstrate that ITTA achieves superior performance compared to state-of-the-art methods on various DG benchmarks for both multi-source and single-source DG tasks.",
        "methodology": "ITTA improves Test-Time Training (TTT) through two main strategies. First, a learnable consistency loss (Lwcont) is introduced for the TTT auxiliary task. This loss is defined as the L2 norm of the output of a weight subnetwork (fw) applied to the difference between original and augmented feature representations (z - z'). The fw subnetwork, composed of stacked ReLU(a * h + b) layers, allows for flexible consistency measurement. During training, the feature extractor and classifier are optimized using a combined main loss and learnable consistency loss (Lmain + αLwcont). The fw subnetwork is updated by minimizing the difference between the normalized gradients of Lmain and Lwcont with respect to the feature extractor parameters, ensuring alignment. Second, additional adaptive parameter blocks (fΘ) are introduced after each block of the pretrained feature extractor during the test-time adaptation phase. Only these fΘ parameters are updated using the learned consistency loss from the target domain data, following an online adaptation setting. The augmentation strategy used is an existing method that modifies intermediate activations to create augmented feature representations.",
        "experimental_setup": "ITTA was evaluated on five benchmark datasets: PACS, VLCS, OfficeHome, TerraInc, and DomainNet, covering diverse image categories and domain shifts. The backbone model for all experiments was an ImageNet-pretrained ResNet18 with 4 blocks as the feature extractor. The proposed adaptive parameters (fΘ) consisted of 4 blocks, each with 5 learnable layers, while the weight subnetwork (fw) used 10 learnable layers. The classifier was an MLP layer. Hyperparameters, including random seeds, learning rates, batch size, and augmentation skills, were dynamically set according to the rigorous DomainBed benchmark protocol. Evaluations included multi-source domain generalization (using a leave-one-out strategy) and single-source domain generalization (trained on one domain, tested on three others on PACS). Each setting involved 60 trials, with model selection based on the 'training-domain validate set' method, and final performance reported as average accuracy. Comparisons were made against 22 existing DG methods and several TTT-based approaches (TTT, MT3, TENT, MetaReg, Feature-Critic, TTT++).",
        "limitations": "The proposed ITTA method incurs increased computational costs. While updating the model without the weight subnetwork (fw disabled) requires 1 forward and 1 backward pass, enabling fw for the learnable consistency loss and its alignment objective necessitates computing second-order derivatives, which demands 1 additional forward and 3 additional backward passes. This significantly increases the burden on the system during training. During testing, the adaptation process involves extra forward and backward steps, leading to increased FLOPS and inference time compared to the baseline model.",
        "future_research_directions": "Future efforts will focus on simplifying the overall optimization process of ITTA and reducing its computational cost to alleviate the extra burden on the system caused by the learnable consistency loss and its gradient alignment step.",
        "experimental_code": "class ITTA(Algorithm):\n    \"\"\"\n    Improved Test-Time Adaptation (ITTA)\n    \"\"\"\n\n    def __init__(self, input_shape, num_classes, num_domains, hparams):\n        super(ITTA, self).__init__(input_shape, num_classes, num_domains,\n                                  hparams)\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.featurizer = networks.ResNet_ITTA(input_shape, self.hparams)\n        self.classifier = networks.Classifier(\n            self.featurizer.n_outputs,\n            num_classes,\n            self.hparams['nonlinear_classifier'])\n        self.test_mapping = networks.MappingNetwork() #specialized for resnet18\n        self.test_optimizer = torch.optim.Adam(self.test_mapping.parameters(), lr=self.hparams[\"lr\"]*0.1)\n        self.optimizer = torch.optim.Adam([\n            {'params': self.featurizer.parameters()},\n            {'params': self.classifier.parameters()}],\n            lr=self.hparams[\"lr\"],\n            weight_decay=self.hparams['weight_decay']\n        )\n        self.MSEloss = nn.MSELoss()\n        self.adaparams = networks.Adaparams() #specialized for resnet18\n        self.adaparams_optimizer = torch.optim.Adam(self.adaparams.parameters(), lr=self.hparams[\"lr\"]*0.1)\n\n    def _get_grads(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward(inputs=list(self.featurizer.parameters()),\n                          retain_graph=True, create_graph=True)\n        dict = OrderedDict(\n            [\n                (name, weights.grad.clone().view(weights.grad.size(0),-1))\n                for name, weights in self.featurizer.named_parameters()\n            ]\n        )\n\n        return dict\n\n    def update(self, minibatches, unlabeled=None):\n        all_x = torch.cat([x for x,y in minibatches])\n        all_y = torch.cat([y for x,y in minibatches])\n        ############################# this is for network update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        loss = loss_reg + loss_cla\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        ############################# this is for adaparams update\n        #############################\n        z_ori, z_aug = self.featurizer(all_x)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.featurizer.fea_forward(z_ori), self.featurizer.fea_forward(z_aug)\n        loss_reg = self.MSEloss(self.adaparams(z_aug - z_ori), torch.zeros_like(z_aug))\n        loss_cla = F.cross_entropy(self.classifier(z_ori), all_y) + \\\n                   F.cross_entropy(self.classifier(z_aug), all_y)\n        dict_reg = self._get_grads(loss_reg)\n        dict_cla = self._get_grads(loss_cla)\n        penalty = l2_between_dicts(dict_reg, dict_cla, normalize=True) * 0.1\n        self.adaparams_optimizer.zero_grad()\n        penalty.backward(inputs=list(self.adaparams.parameters()))\n        self.adaparams_optimizer.step()\n\n        return {'loss': loss_cla.item(), 'reg': loss_reg.item()}\n\n    def test_adapt(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori, z_aug = self.test_mapping.fea1(z_ori), self.test_mapping.fea1(z_aug)\n        z_ori, z_aug = self.featurizer.fea2(z_ori, z_aug)\n        z_ori, z_aug = self.test_mapping.fea2(z_ori), self.test_mapping.fea2(z_aug)\n        z_ori, z_aug = self.featurizer.fea3(z_ori), self.featurizer.fea3(z_aug)\n        z_ori, z_aug = self.test_mapping.fea3(z_ori), self.test_mapping.fea3(z_aug)\n        z_ori, z_aug = self.featurizer.fea4(z_ori), self.featurizer.fea4(z_aug)\n        z_ori, z_aug = self.test_mapping.fea4(z_ori), self.test_mapping.fea4(z_aug)\n        z_ori, z_aug = self.featurizer.flat(z_ori), self.featurizer.flat(z_aug)\n        ########## small lr for large datasets\n        loss_reg = self.MSEloss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']\n        self.test_optimizer.zero_grad()\n        loss_reg.backward(inputs=list(self.test_mapping.parameters()))\n        self.test_optimizer.step()\n\n    def predict(self, x):\n        z_ori, z_aug = self.featurizer(x)\n        z_ori = self.test_mapping.fea1(z_ori)\n        z_ori, z_aug = self.featurizer.fea2(z_ori,z_aug)\n        z_ori = self.test_mapping.fea2(z_ori)\n        z_ori = self.featurizer.fea3(z_ori)\n        z_ori = self.test_mapping.fea3(z_ori)\n        z_ori = self.featurizer.fea4(z_ori)\n        z_ori = self.test_mapping.fea4(z_ori)\n        z_ori = self.featurizer.flat(z_ori)\n        return self.classifier(z_ori)\n\nclass MappingNetwork(torch.nn.Module):\n    def __init__(self, depth=5):\n        super().__init__()\n        self.depth = depth\n        self.weight1 = nn.ParameterList()\n        self.bias1 = nn.ParameterList()\n        self.weight2 = nn.ParameterList()\n        self.bias2 = nn.ParameterList()\n        self.weight3 = nn.ParameterList()\n        self.bias3 = nn.ParameterList()\n        self.weight4 = nn.ParameterList()\n        self.bias4 = nn.ParameterList()\n        for i in range(depth):\n            self.weight1.append(nn.Parameter(torch.ones((64,56,56))))\n            self.bias1.append(nn.Parameter(torch.zeros((64,56,56))))\n\n            self.weight2.append(nn.Parameter(torch.ones((128,28,28))))\n            self.bias2.append(nn.Parameter(torch.zeros((128,28,28))))\n\n            self.weight3.append(nn.Parameter(torch.ones((256,14,14))))\n            self.bias3.append(nn.Parameter(torch.zeros((256,14,14))))\n\n            self.weight4.append(nn.Parameter(torch.ones((512, 7, 7))))\n            self.bias4.append(nn.Parameter(torch.zeros((512, 7, 7))))\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def fea1(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight1[i] * x + self.bias1[i])\n        x = self.weight1[i+1] * x + self.bias1[i+1]\n        return x\n\n    def fea2(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight2[i] * x + self.bias2[i])\n        x = self.weight2[i + 1] * x + self.bias2[i + 1]\n        return x\n\n    def fea3(self, x):\n        for i in range(self.depth - 1):\n            x = self.relu(self.weight3[i] * x + self.bias3[i])\n        x = self.weight3[i + 1] * x + self.bias3[i + 1]\n        return x\n\n    def fea4(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight4[i] * x + self.bias4[i])\n        x = self.weight4[i+1] * x + self.bias4[i+1]\n        return x\n\n\nclass Adaparams(nn.Module):\n    def __init__(self, depth=10):\n        super(Adaparams, self).__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.depth = depth\n        self.weight = nn.ParameterList()\n        self.bias = nn.ParameterList()\n        for i in range(depth):\n            self.weight.append(nn.Parameter(torch.ones(512)))\n            self.bias.append(nn.Parameter(torch.zeros(512)))\n\n    def forward(self, x):\n        for i in range(self.depth-1):\n            x = self.relu(self.weight[i] * x + self.bias[i])\n        x = self.weight[i+1] * x + self.bias[i+1]\n        return x\n        \nclass ResNet_ITTA(torch.nn.Module):\n    \"\"\"ResNet with the softmax chopped off and the batchnorm frozen\"\"\"\n    def __init__(self, input_shape, hparams):\n        super(ResNet_ITTA, self).__init__()\n        if hparams['resnet18']:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 512\n        else:\n            self.network = torchvision.models.resnet18(pretrained=True)\n            self.n_outputs = 2048\n\n        nc = input_shape[0]\n        if nc != 3:\n            tmp = self.network.conv1.weight.data.clone()\n\n            self.network.conv1 = nn.Conv2d(\n                nc, 64, kernel_size=(7, 7),\n                stride=(2, 2), padding=(3, 3), bias=False)\n\n            for i in range(nc):\n                self.network.conv1.weight.data[:, i, :, :] = tmp[:, i % 3, :, :]\n\n        # save memory\n        self.network.fc = Identity()\n        self.isaug = True\n        self.freeze_bn()\n        self.hparams = hparams\n        self.dropout = nn.Dropout(hparams['resnet_dropout'])\n        self.eps = 1e-6\n\n    def mixstyle(self, x):\n        alpha = 0.1\n        beta = torch.distributions.Beta(alpha, alpha)\n        B = x.size(0)\n        mu = x.mean(dim=[2, 3], keepdim=True)\n        var = x.var(dim=[2, 3], keepdim=True)\n        sig = (var + self.eps).sqrt()\n        mu, sig = mu.detach(), sig.detach()\n        x_normed = (x - mu) / sig\n        lmda = beta.sample((B, 1, 1, 1))\n        lmda = lmda.to(x.device)\n        perm = torch.randperm(B)\n        mu2, sig2 = mu[perm], sig[perm]\n        mu_mix = mu * lmda + mu2 * (1 - lmda)\n        sig_mix = sig * lmda + sig2 * (1 - lmda)\n        return x_normed * sig_mix + mu_mix\n\n    def fea_forward(self, x):\n        x = self.fea3(x)\n        x = self.fea4(x)\n\n        x = self.flat(x)\n        return x\n\n    def fea2(self, x, aug_x):\n        x = self.network.layer2(x)\n        aug_x = self.network.layer2(aug_x)\n        if not self.isaug:\n            aug_x = self.mixstyle(aug_x)\n        return x, aug_x\n\n    def fea3(self, x):\n        x = self.network.layer3(x)\n        return x\n\n    def fea4(self, x):\n        x = self.network.layer4(x)\n        return x\n\n    def flat(self, x):\n        x = self.network.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.network.fc(x)\n        x = self.dropout(x)\n        return x\n\n    def forward(self, x):\n        \"\"\"Encode x into a feature vector of size n_outputs.\"\"\"\n        x = self.network.conv1(x)\n        x = self.network.bn1(x)\n        x = self.network.relu(x)\n        x = self.network.maxpool(x)\n\n        x = self.network.layer1(x)\n        if random.random() > 0.5:\n            self.isaug = True\n            aug_x = self.mixstyle(x)\n        else:\n            self.isaug = False\n            aug_x = x\n\n        return x, aug_x\n\n    def train(self, mode=True):\n        \"\"\"\n        Override the default train() to freeze the BN parameters\n        \"\"\"\n        super().train(mode)\n        self.freeze_bn()\n\n    def freeze_bn(self):\n        for m in self.network.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()",
        "experimental_info": "ITTA improves Test-Time Training (TTT) using a learnable consistency loss (Lwcont) and adaptive parameter blocks (fΘ). The architecture comprises a ResNet_ITTA featurizer (ResNet18, ImageNet pretrained weights, Batch Normalization layers frozen during training), a linear classifier, a MappingNetwork for adaptive parameter blocks (fΘ, depth 5, specialized for ResNet18), and an Adaparams subnetwork for the learnable consistency loss (fw, depth 10, MLP-like for 512-dim features).\n\nDuring training, the main network (featurizer + classifier) is optimized with an Adam optimizer (learning rate: `hparams[\"lr\"]`, weight decay: `hparams['weight_decay']`). The `fw` subnetwork (Adaparams) is optimized with an Adam optimizer (learning rate: `hparams[\"lr\"] * 0.1`). The loss functions include a main classification loss (`Lmain = F.cross_entropy` for both original and augmented features) and the learnable consistency loss (`Lwcont = nn.MSELoss()` applied to `fw(z_aug - z_ori)` and zero tensor). The total loss for the main network update is `Lmain + Lwcont`. The `fw` subnetwork is updated by minimizing a penalty calculated as `l2_between_dicts(normalized_gradients(Lwcont), normalized_gradients(Lmain)) * 0.1`.\n\nThe augmentation strategy involves `mixstyle` applied to intermediate feature activations (either after `layer1` or `layer2` of ResNet_ITTA), with a 50% probability of initial application. The `mixstyle` parameters include `alpha=0.1` for the Beta distribution and `eps=1e-6` for numerical stability.\n\nFor test-time adaptation, adaptive parameter blocks (`fΘ` / MappingNetwork) are updated using an Adam optimizer (learning rate: `hparams[\"lr\"] * 0.1`). These `fΘ` parameters are updated by minimizing `Lwcont`, specifically `nn.MSELoss(self.adaparams(z_aug-z_ori), torch.zeros_like(z_ori)) * self.hparams['ada_lr']`. This adaptation is performed for 1 iteration on test data before prediction. The `ada_lr` hyperparameter is set to 0.1 for the DomainNet dataset and 1e-6 for other datasets. General hyperparameters include a learning rate (`lr`) of 5e-5 for non-small image datasets and 1e-3 for small image datasets, and a weight decay of 0. `resnet18` is set to True (default), implying 512 `n_outputs` for the featurizer. `nonlinear_classifier` and `resnet_dropout` are both 0.0 by default. `batch_size` is typically 32 for non-small image datasets (e.g., DomainNet) and 64 for small image datasets."
      }
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of selecting an effective Test-Time Adaptation (TTA) loss for neural networks facing distribution shifts. It presents a surprising phenomenon: meta-learning the 'best' TTA loss recovers a temperature-scaled softmax-entropy for cross-entropy trained classifiers and a negative squared error for squared loss trained classifiers. To explain this, the paper proposes a novel framework analyzing TTA through the lens of the training loss's convex conjugate. This framework provides a generic recipe to derive TTA losses, recovering existing successful methods like TENT and suggesting new ones for various training losses (e.g., PolyLoss). The approach is interpreted as self-training with specific 'conjugate pseudo-labels' (∇f(h(x))). Empirically, this method consistently outperforms other TTA alternatives across a wide range of domain adaptation benchmarks.",
        "methodology": "The methodology involves two main parts: an exploratory meta-learning phase and a principled conjugate function framework. Initially, the authors meta-learn a TTA loss function, parameterized by a neural network (Transformer + MLP), by differentiating through the adaptation process to find parameters that yield optimal performance on distribution shifts. This meta-learning revealed the dependence of the optimal TTA loss on the source classifier's training loss. Subsequently, the paper introduces a framework based on the convex conjugate function. For a general class of loss functions L(h(x),y) = f(h(x)) - yᵀh(x), the conjugate adaptation loss is defined as Lconj(hθ(x)) = -f⋆(∇f(hθ(x))), which can also be expressed as f(hθ(x)) - ∇f(hθ(x))ᵀhθ(x). This loss is shown to be equivalent to self-training with 'conjugate pseudo-labels' ˜yCPLθ(x) = ∇f(hθ(x)). The adaptation process (Algorithm 1) involves iterative updates of the model parameters by optimizing this conjugate pseudo-labeling loss on batches of unlabeled test data, typically fine-tuning only the learnable scale and shift parameters of batch normalization layers, often with an additional temperature scaling.",
        "experimental_setup": "The evaluation was conducted on three common corruption benchmarks: CIFAR-10-C, CIFAR-100-C, and ImageNet-C, reporting averaged errors. Additionally, three domain adaptation datasets were used: SVHN to MNIST, ImageNet to ImageNet-R, and VISDA-C (synthetic to real). Source classifiers included ResNet-26 for CIFAR, ResNet-18 for SVHN, and ResNet-50 for ImageNet and VisDA-C, trained with cross-entropy, PolyLoss, and squared loss. Baselines for comparison included self-training with 'hard' pseudo-labels (argmax with confidence thresholding) and 'soft' pseudo-labels (softmax), Entropy Minimization (TENT), Robust Pseudo-Label, and MEMO (batch version). Test-time adaptation involved updating only batch normalization parameters and statistics. Hyperparameters (learning rate from {1e-1 to 1e-4} and temperature from {1 to 5}) were tuned via grid-search on validation noises for corruption benchmarks, while fixed values (T=1, specific LRs) were used for domain adaptation tasks without validation sets. Experiments were performed on A6000 GPUs.",
        "limitations": "The meta-learning framework used in this work was restricted to learning functions over the logits of individual inputs, suggesting that more complex functions involving intermediate representations or batch interactions could be explored. The paper provides strong evidence for the effectiveness of the conjugate loss but does not offer a complete theoretical answer to what constitutes the 'optimal' test-time adaptation loss. Furthermore, the role and impact of common TTA heuristics, such as updating only batch normalization parameters and temperature scaling, still require more concrete understanding. The applicability of self-training based approaches to various real-world distribution shifts remains an open problem.",
        "future_research_directions": "Future research directions include expanding the meta-learning framework to incorporate intermediate representations and learning functions that account for interactions within a batch of inputs. A more concrete theoretical understanding of the role of heuristics like batch normalization parameter updates and temperature scaling in TTA is also a promising avenue. Further work is needed to identify the types of real-world distribution shifts where self-training based approaches, particularly those utilizing conjugate pseudo-labels, are most beneficial. Additionally, extending and applying the conjugate pseudo-labeling framework to other related settings, such as semi-supervised learning, is suggested."
      }
    },
    {
      "title": "Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering",
      "abstract": "Deploying models on target domain data subject to distribution shift requires\nadaptation. Test-time training (TTT) emerges as a solution to this adaptation\nunder a realistic scenario where access to full source domain data is not\navailable and instant inference on target domain is required. Despite many\nefforts into TTT, there is a confusion over the experimental settings, thus\nleading to unfair comparisons. In this work, we first revisit TTT assumptions\nand categorize TTT protocols by two key factors. Among the multiple protocols,\nwe adopt a realistic sequential test-time training (sTTT) protocol, under which\nwe further develop a test-time anchored clustering (TTAC) approach to enable\nstronger test-time feature learning. TTAC discovers clusters in both source and\ntarget domain and match the target clusters to the source ones to improve\ngeneralization. Pseudo label filtering and iterative updating are developed to\nimprove the effectiveness and efficiency of anchored clustering. We demonstrate\nthat under all TTT protocols TTAC consistently outperforms the state-of-the-art\nmethods on six TTT datasets. We hope this work will provide a fair benchmarking\nof TTT methods and future research should be compared within respective\nprotocols. A demo code is available at\nhttps://github.com/Gorilla-Lab-SCUT/TTAC.",
      "full_text": "Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering Yongyi Su1 Xun Xu2 Kui Jia13 1South China University of Technology 2Institute for Infocomm Research 3Peng Cheng Laboratory eesuyongyi@mail.scut.edu.cn alex.xun.xu@gmail.com kuijia@scut.edu.cn Abstract Deploying models on target domain data subject to distribution shift requires adaptation. Test-time training (TTT) emerges as a solution to this adaptation under a realistic scenario where access to full source domain data is not available and instant inference on target domain is required. Despite many efforts into TTT, there is a confusion over the experimental settings, thus leading to unfair comparisons. In this work, we ﬁrst revisit TTT assumptions and categorize TTT protocols by two key factors. Among the multiple protocols, we adopt a realistic sequential test-time training (sTTT) protocol, under which we further develop a test-time anchored clustering (TTAC)approach to enable stronger test-time feature learning. TTAC discovers clusters in both source and target domain and match the target clusters to the source ones to improve generalization. Pseudo label ﬁltering and iterative updating are developed to improve the effectiveness and efﬁciency of anchored clustering. We demonstrate that under all TTT protocols TTAC consistently outperforms the state-of-the-art methods on six TTT datasets. We hope this work will provide a fair benchmarking of TTT methods and future research should be compared within respective protocols. A demo code is available at https://github.com/Gorilla-Lab-SCUT/TTAC. 1 Introduction The recent success in deep learning is attributed to the availability of large labeled data [16, 43] and the assumption of i.i.d. between training and test datasets. Such assumptions could be violated when test data features a drastic difference from the training data, e.g. training on synthetic images and test on real ones, and this is often referred to as domain shift [ 24, 2]. To tackle this issue, domain adaptation (DA) [34] emerges and the labeled training data and unlabeled testing data are often referred to as source and target data/domains respectively. The existing DA works either require the access to both source and target domain data during training [6] or training on multiple domains simultaneously [ 42]. The former approach renders the methods restrictive to limited scenarios where source domain data is always available during adaptation while the latter ones are computationally more expensive. To alleviate the reliance on source domain data, which may be inaccessible due to privacy issues or storage overhead, source-free domain adaptation (SFDA) emerges which handles DA on target data without access to source data [21, 17, 39, 37, 22]. SFDA is often achieved through self-training [21], self-supervised learning [22] or introducing prior knowledge [ 21] and it requires multiple training epochs on the full target data to allow convergence. Despite easing the dependence on source data, SFDA has major drawbacks in a more realistic domain adaptation scenario where test data arrives in a stream 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.02721v2  [cs.CV]  14 Oct 2022and inference or prediction must be taken instantly, and this setting is often referred to as test-time training (TTT) or adaptation (TTA) [29, 33, 14, 22]. Despite the attractive feature of adaption at time test, we notice a confusion of what deﬁnes a test-time training and as a result comparing apples and oranges happens frequently in the community. In this work, we ﬁrst categorize TTT by two key factors after summarizing various deﬁnitions made in existing works. First, under a realistic TTT setting, test samples are sequentially streamed and prediction must be made instantly upon the arrival of a new test sample. More speciﬁcally, the prediction of test sample XT , arriving at time stamp T, should not be affected by any subsequent samples, {Xt}t=T+1···∞. Throughout this work, we refer to the sequential streaming as one-pass adaptation protocol and any other protocols violating this assumption are called multi-pass adaptation (model may be updated on all test data for multiple epochs before inference). Second, we notice some recent works must modify source domain training loss, e.g. by introducing additional self-supervised branch, to allow more effective TTT [29, 22]. This will introduce additional overhead in the deployment of TTT because re-training on some source dataset, e.g. ImageNet, is computationally expensive. In this work, we aim to tackle on the most realistic and challenging TTT protocol, i.e. one-pass test time training with no modiﬁcations to training objective. This setting is similar to TTA proposed in [ 33] except for not restricting access to a light-weight information from the source domain. Given the objective of TTT being efﬁcient adaptation at test-time, this assumption is computationally efﬁcient and improves TTT performance substantially. We name this new TTT protocol as sequential test time training (sTTT). We propose two techniques to enable efﬁcient and accurate sTTT. i) We are inspired by the recent progresses in unsupervised domain adaptation [30] that encourages testing samples to form clusters in the feature space. However, separately learning to cluster in the target domain without regularization from source domain does not guarantee improved adaptation [30]. To overcome this challenge, we identify clusters in both the source and target domains through a mixture of Gaussians with each component Gaussian corresponding to one category. Provided with the category-wise statistics from source domain as anchors, we match the target domain clusters to the anchors by minimizing the KL-Divergence as the training objective for sTTT. Therefore, we name the proposed methodtest-time anchored clustering (TTAC). Since test samples are sequentially streamed, we develop an exponential moving averaging strategy to update the target domain cluster statistics to allow gradient-based optimization. ii) Each component Gaussian in the target domain is updated by the test sample features that are assigned to the corresponding category. Thus, incorrect assignments (pseudo labels) will harm the estimation of component Gaussian. To tackle this issue, we are inspired by the correlation between network’s stability and conﬁdence and pseudo label accuracy [19, 26], and propose to ﬁlter out potentially incorrect pseudo labels. Component Gaussians are then updated by the samples that have passed the ﬁltering. To exploit the ﬁltered out samples, we incorporate a global feature alignment [22] objective. We also demonstrate TTAC is compatible with existing TTT techniques, e.g. contrastive learning branch [22], if source training loss is allowed to be modiﬁed. The contributions of this work are summarized as below. • In light of the confusions within TTT works, we provide a categorization of TTT protocols by two key factors. Comparison of TTT methods is now fair within each category. • We adopt a realistic TTT setting, namely sTTT. To improve test-time feature learning, we propose TTAC by matching the statistics of the target clusters to the source ones. The target statistics are updated through moving averaging with ﬁltered pseudo labels. • The proposed method is complementary to existing TTT method and is demonstrated on six TTT datasets, achieving the state-of-the-art performance under all categories of TTT protocols. 2 Related Work Unsupervised Domain Adaptation . Domain adaptation aims to improve model generalization when source and target data are not drawn i.i.d. When target data are unlabeled, unsupervised domain adaptation (UDA) [6, 31] learns domain invariant feature representations on both source and target domains to improve generalization. Follow-up works improve UDA by minimizing a divergence [7, 27, 40], adversarial training [12] or discovering cluster structures in the target data [30]. Apart from formulating UDA as a task-speciﬁc model, re-weighting has been adopted for domain adaptation by selectively up-weighting conducive samples in the source domain [ 15, 38]. During 2model training, the existing approaches often require access to the source domain data which, however, may be not accessible due to privacy issues, storage overhead, etc. Therefore, deploying UDA in more realistic scenarios has inspired research into source-free domain adaptation and test-time training/adaptation. Source-Free Domain Adaptation. Without the access to source data, source-free domain adaptation (SFDA) develops domain adaptation through self-training [21, 17, 14], self-supervised training [22], clustering in the target domain [39] and feature restoration [5]. It has been demonstrated that SFDA performs well on seminal domain adaptation datasets even compared against UDA methods [ 30]. Nevertheless, SFDA requires access to all testing data beforehand and model training must be carried out iteratively on the testing data. In a more realistic DA scenario where inference and adaptation must be implemented simultaneously, SFDA will no longer be effective. Moreover, some statistical information on the source domain does not pose privacy issues and can be exploited to further improve adaptation on target data. Test-Time Training. Collecting enough samples from target domain and adapt models in an ofﬂine manner restricts the application to adapting to a static target domain. To allow fast and online adaptation, test-time training (TTT) [29, 35] or adaptation (TTA) [33] emerges. Despite many recent works claiming to be test-time training, we notice a severe confusion over the deﬁnition of TTT. In particular, whether training objective must be modiﬁed [29, 22] and whether sequential inference on target domain data is possible [33, 14]. Therefore, to reﬂect the key challenges in TTT, we deﬁne a setting called sequential test-time training (sTTT) which neither modiﬁes the training objective nor violates sequential inference. Under the more clear deﬁnition, some existing works, e.g. TTT [29] and TTT++ [22] is more likely to be categorized into SFDA. Several existing works [33, 14] can be adapted to the sTTT protocol. Tent [33] proposed to adjust afﬁne parameters in the batchnorm layers to adapt to target domain data. Nevertheless, updating only a fraction of model weights inevitably leads to limited performance gain on the target domain. T3A [14] further proposed to update classiﬁer prototype through pseudo labeling. Despite being efﬁcient, updating classiﬁer prototype alone does not affect feature representation for the target domain. Target feature may not form clusters at all when the distribution mismatch between source and target is large enough. In this work we propose to simultaneously cluster on the target domain and match target clusters to source domain classes, namely anchored clustering. To further constrain feature update, we introduce additional global feature alignment and pseudo label ﬁltering. Through the introduced anchored clustering, we achieve test-time training of more network parameters and achieve the state-of-the-art performance. 3 Methodology In this section we ﬁrst introduce the anchored clustering objective for test-time training through pseudo labeling and then describe an efﬁcient iterative updating strategy. An overview of the proposed pipeline is illustrated in Fig. 1. 3.1 Anchored Clustering for Test-Time Training Discovering cluster structures in the target domain has been demonstrated effective for unsupervised domain adaptation [30] and we develop an anchored clustering on the test data alone. We ﬁrst use a mixture of Gaussians to model the clusters in the target domain, here each component Gaussian represents one discovered cluster. We further use the distributions of each category in the source domain as anchors for the target distribution to match against. In this way, test data features can simultaneously form clusters and the clusters are associated with source domain categories, resulting in improved generalization to target domain. Formally, we ﬁrst write the mixture of Gaussians in the source and target domains ps(x) = ∑ k αkN(µsk,Σsk), p t(x) = ∑ k βkN(µtk,Σtk), where {µk ∈Rd,Σk ∈Rd×d}represent one cluster in the source/target domain and dis the dimension of feature embedding. Anchored clustering can be achieved by matching the above two distributions and one may directly minimize the KL-Divergence between the two distribution. Nevertheless, this is non-trivial because the KL-Divergence between two mixture of Gaussians has no closed-form solution which prohibits efﬁcient gradient-based optimization. Despite some approximations exist [11], without knowing the semantic labels for each Gaussian component, even a good match between two mixture of Gaussians does not guarantee target clusters are aligned to the correct source ones and this will severely harm the 3𝑁(𝜇!,Σ!) Source Domain(offline) 𝑓(⋅) Iteratively update Streaming Testing DataPseudo Label Filter(P.L.F) temporal changing direction (t) (t -1)  (t + 1) Fixed-LengthQueue Push 𝑓!(⋅) ℎ(⋅) P.L.F pseudo labelfilter out𝑁(µ\",Σ\") 𝑁(µ\"#,Σ\"#) 𝑁(µ\"$,Σ\"$) 𝑁(µ\"%,Σ\"%) Iteratively update𝑁(𝜇!,Σ!) Iterativelyupdate𝑁𝜇$!,Σ$!𝑁𝜇%!,Σ%!𝑁𝜇#!,Σ#! 𝑁(µ\",Σ\") 𝑁(µ\"#,Σ\"#) 𝑁(µ\"$,Σ\"$) 𝑁(µ\"%,Σ\"%) 𝑁(𝜇!,Σ!)𝑁(µ#!,Σ#&)𝑁(µ%!,Σ%!) 𝑁(µ$!,Σ$!) (t + 2) Frozen ℒ\"#ℒ$\" 𝑁(µ#!,Σ#&) 𝑁(µ%!,Σ%!) 𝑁(µ$!,Σ$!) 𝑁(µ\",Σ\")𝑁(µ\"#,Σ\"#) 𝑁(µ\"$,Σ\"$) 𝑁(µ\"%,Σ\"%) Anchor. Cluster.& Global Feat. Align. Anchored Clustering Figure 1: Overview of TTAC pipeline. i) In the source domain, we calculate category-wise and global statistics as anchors. ii) In the testing stage, samples are sequentially streamed and pushed into a ﬁxed-length queue. Clusters in target domain are identiﬁed through anchored clustering with pseudo label ﬁltering. Target clusters are then matched to the anchors in source domain to achieve test-time training. performance of test-time training. In light of these challenges, we propose a category-wise alignment. Speciﬁcally, we allocate the same number of clusters in both source and target domains and each target cluster is assigned to one source cluster. We can then minimize the KL-Divergence between each pair of clusters as in Eq. 1. Lac = ∑ k DKL(N(µsk,Σsk)||N(µtk,Σtk)) = ∑ k −H(N(µsk,Σsk)) + H(N(µsk,Σsk),N(µtk,Σtk)) (1) The KL-Divergence can be further decomposed into the entropy H(N(µsk,Σsk)) and cross-entropy H(N(µsk,Σsk),N(µtk,Σtk)). It is commonly true that the source reference distribution Ps(x) is ﬁxed thus the entropy term is a constant Cand only the cross-entropy term is to be optimized. Given the closed-form solution to the KL-Divergence between two Gaussian distributions, we now write the anchored clustering objective as, Lac = ∑ k {log √ 2πd|Σtk|+ 1 2(µtk −µsk)⊤Σ−1 tk (µtk −µsk) + tr(Σ−1 tk Σsk)}+ C (2) The source cluster parameters can be estimated in an ofﬂine manner. These information will not cause any privacy leakage and only introduces a small computation and storage overheads. In the next section, we elaborate clustering in the target domain. 3.2 Clustering through Pseudo Labeling In order to test-time train network with anchored clustering loss, one must obtain target cluster parameters {µtk,Σtk}. For a minibatch of target test samples Bt = {xi}i=1...NB at timestamp t, we ﬁrst denote the predicted posterior as Pt = softmax(h(f(xi))) ∈[0,1]B×K where softmax(·), h(·) and f(·) respectively denote a standard softmax function, the classiﬁer head and backbone network. The pseudo labels are obtained via ˆyi = arg maxk Pt ik. Given the predicted pseudo labels we could estimate the mean and covariance for each component Gaussian with the pseudo labeled testing samples. However, pseudo labels are always subject to model’s discrimination ability. The error rate for pseudo labels is often high when the domain shift between source and target is large, directly updating the component Gaussian is subject to erroneous pseudo labels, a.k.a. conﬁrmation bias [1]. To reduce the impact of incorrect pseudo labels, we ﬁrst adopt a light-weight temporal consistency (TC) pseudo label ﬁltering approach. Compared to co-teaching [ 8] or meta-learning [20] based methods, this light-weight method does not introduce additional computation overhead and is therefore more suitable for test-time training. Speciﬁcally, to alleviate the impact from the noisy 4predictions, we calculate the temporal exponential moving averaging posteriors ˜Pt ∈[0,1]N×K as below, ˜Pt i = (1 −ξ) ∗˜Pt−1 i + ξ∗Pt i , s.t. ˜P0 i = P0 i (3) The temporal consistency ﬁltering is realized as in Eq. 4 where τTC is a threshold determining the maximally allowed difference in the most probable prediction over time. If the posterior deviate from historical value too much, it will be excluded from target domain clustering. FTC i = 1 ((Pt iˆk −˜Pt−1 iˆk ) >τTC ), s.t. ˆk= arg max k (Pt ik) (4) Due to the sequential inference, test samples without enough historical predictions may still pass the TC ﬁltering. So, we further introduce an additional pseudo label ﬁlter directly based on the posterior probability as, FPP i = 1 ( ˜Pt iˆk >τPP ) (5) By ﬁltering out potential incorrect pseudo labels, we update the component Gaussian only with the leftover target samples as below. µtk = ∑ i FTC i FPP i 1 (ˆyi = k)f(xi) ∑ i FTC i FPP i 1 (ˆyi = k) , Σtk = ∑ i FTC i FPP i 1 (ˆyi = k)(f(xi) −µtk)⊤(f(xi) −µtk) ∑ i FTC i FPP i 1 (ˆyi = k) (6) 3.3 Global Feature Alignment As discussed above, test samples that do not pass the ﬁltering will not contribute to the estimation of target clusters. Hence, anchored clustering may not reach its full potential without the ﬁltered test samples. To exploit all available test samples, we propose to align global target data distribution to the source one. We deﬁne the global feature distribution of the source data asˆps(x) = N(µs,Σs) and the target data as ˆpt(x) = N(µt,Σt). To align two distributions, we again minimize the KL-Divergence as, Lga = DKL(ˆps(x)||ˆpt(x)) (7) Similar idea has appeared in [22] which directly matches the moments between source and target domains [40] by minimizing the F-norm for the mean and covariance, i.e.||µt −µs||2 2 + ||Σt −Σs||2 F . However, designed for matching complex distributions represented as drawn samples, central moment discrepancy [40] requires summing inﬁnite central moment discrepancies and the ratios between different order moments are hard to estimate. For matching two parameterized Gaussian distributions KL-Divergence is more convenient with good explanation from a probabilistic point of view. Finally, we add a small constant to the diagonal of Σ for both source and target domains to increase the condition number for better numerical stability. 3.4 Efﬁcient Iterative Updating Despite the distribution for source data can be trivially estimated from all available training data in a totally ofﬂine manner, estimating the distribution for target domain data is not equally trivial, in particular under the sTTT protocol. In a related research [ 22], a dynamic queue of test data features are preserved to dynamically estimate the statistics, which will introduce additional memory footprint [22]. To alleviate the memory cost we propose to iteratively update the running statistics for Gaussian distribution. Formally, we deﬁne t-th test minibatch as Bt = {xi}i=1···NB . Denoting the running mean and covariance at step tas µt and Σt, we present the rules to update the mean and covariance in Eq. 8. More detailed derivations and update rules for per cluster statistics are deferred to the Appendix. µt = µt−1 + δt, Σt = Σt−1 + at ∑ xi∈B {(f(xi) −µt−1)⊤(f(xi) −µt−1) −Σt−1}−δt⊤ δt δt = at ∑ xi∈B (f(xi) −µt−1), N t = Nt−1 + |Bt|, a t = 1 Nt (8) 5Additionally, Nt grows larger overtime. New test samples will have smaller contribution to the update of target domain statistics when Nt is large enough. As a result, the gradient calculated from current minibatch will vanish. To alleviate this issue, we impose a clip on the value ofαt as below. As such, the gradient can maintain a minimal scale even if Nt is very large. at = { 1 Nt Nt <Nclip 1 Nclip others (9) 3.5 TTAC Training Algorithm We summarize the training algorithm for the TTAC in Algo. 1. For effective clustering in target domain, we allocate a ﬁxed length memory space, denoted as C∈ RNC×H×W×3, to store the recent testing samples. In the sTTT protocol, we ﬁrst make instant prediction on each testing sample, and only update the model when NB testing samples are accumulated. TTAC can be efﬁciently implemented, e.g. with two devices, one is for continuous inference and another is for model updating. Algorithm 1: Test-Time Anchored Clustering Training Algorithm input : A new testing sample batch Bt = {xi}i=1...NB . # Update the testing sample queue C. Ct = Ct \\Bt−NC/NB , Ct = Ct ⋃Bt for 1 to Nitr do for minibatch {xt k}N k=1 in Ct do # Obtain the predicted posterior and pseudo labels Pt i = softmax(h(f(xt i))), ˆyt i = arg maxk(Pt ik) # Calculate the global and per-cluster running mean and covariance by Eq. 8 µt, Σt, {µt k}, {Σt k} # Optimize the combined loss by Eq. 2 and Eq. 7 L= Lac + λLga update network f to minimize L 4 Experiment In this section, we ﬁrst compare various existing methods based on the two key factors. Evaluation is then carried out on six test-time training datasets. We then ablate the components of TTAC. Further analysis on the cumulative performance, qualitative insights, etc. are provided at the end. 4.1 Datasets We evaluate on 5 test-time training datasets and report the classiﬁcation error rate (%) throughout the experiment section. To evaluate the test-time training efﬁcacy on corrupted target images, we use CIFAR10-C/CIFAR100-C[10], each consisting of 10/100 classes with 50,000 training samples of clean data and 10,000 corrupted test samples. We further evaluate test-time training on hard target domain samples with CIFAR10.1 [25], which contains around 2,000 difﬁcult testing images sampled over years of research on the original CIFAR-10 dataset. To demonstrate the ability to do test-time training for synthetic data to real data transfer we further use VisDA-C [23], which is a challenging large-scale synthetic-to-real object classiﬁcation dataset, consisting of 12 classes, 152,397 synthetic training images and 55,388 real testing images. To evaluate large-scale test-time training, we use ImageNet-C [10] which consists of 1,000 classes and 15 types of corruptions on the 50,000 testing samples. Finally, to evaluate test-time training on 3D point cloud data, we choose ModelNet40-C [28], which consists of 15 common and realistic corruptions of point cloud data, with 9,843 training samples and 2,468 test samples. 4.2 Experiment Settings Hyperparameters. We use the ResNet-50 [ 9] for image datasets and the DGCNN [ 36] on ModelNet40-C. We optimize the backbone networkf(·) by SGD with momentum on all datasets. On CIFAR10-C/CIFAR100-C and CIFAR10.1, we use (batchsize) BS = 256 and (learning rate) LR = 0.01, 0.0001, 0.01 respectively. On VisDA-C we use BS = 128 and LR = 0.0001, and on ModelNet40-C we use BS = 64 and LR = 0.001. More details of hyperparameters can be found in the Appendix. Test-Time Training Protocols. We categorize test-time training based on two key factors. First, whether the training objective must be changed during training on the source domain, we use Y and N 6to indicate if training objective is allowed to be changed or not respectively. Second, whether testing data is sequentially streamed and predicted, we use O to indicate a sequential One-pass inference and M to indicate non-sequential inference, a.k.a. Multi-pass inference. With the above criteria, we summarize 4 test-time training protocols, namely N-O, Y-O, N-M and Y-M, and the strength of the assumption increases from the ﬁrst to the last protocols. Our sTTT setting makes the weakest assumption, i.e. N-O. Existing methods are categorized by the four TTT protocols, we notice that some methods can operate under multiple protocols Competing Methods. We compare the following test-time training methods. Direct testing (TEST) without adaptation simply do inference on target domain with source domain model. Test-time training (TTT-R) [29] jointly trains the rotation-based self-supervised task and the classiﬁcation task in the source domain, and then only train the rotation-based self-supervised task in the streaming test samples and make the predictions instantly. The default method is classiﬁed into the Y-M protocol. Test-time normalization (BN) [13] moving average updates the batch normalization statistics by streamed data. The default method follows N-M protocol and can be adapted to N-O protocol. Test-time entropy minimization (TENT) [33] updates the parameters of all batch normalization by minimizing the entropy of the model predictions in the streaming data. By default, TENT follows the N-O protocol and can be adapted to N-M protocol. Test-time classiﬁer adjustment (T3A) [14] computes target prototype representation for each category using streamed data and make predictions with updated prototypes. T3A follows the N-O protocol by default. Source Hypothesis Transfer (SHOT) [21] freezes the linear classiﬁcation head and trains the target-speciﬁc feature extraction module by exploiting balanced category assumption and self-supervised pseudo-labeling in the target domain. SHOT by default follows the N-M protocol and we adapt it to N-O protocol. TTT++ [22] aligns source domain feature distribution, whose statistics are calculated ofﬂine, and target domain feature distribution by minimizing the F-norm between the mean covariance. TTT++ follows the Y-M protocol and we adapt it to N-O (removing contrastive learning branch) and Y-O protocols. Finally, we present our own approach, TTAC, which only requires a single pass on the target domain and does not have to modify the source training objective. We further modify TTAC for Y-O, N-M and Y-M protocols, for Y-O and Y-M we incorporate an additional contrastive learning branch [22]. We could further combine TTAC with additional diversity loss and entropy minimization loss introduced in SHOT [21], denoted as TTAC+SHOT. 4.3 Test-Time Training on Corrupted Target Domain We present the test-time training results on CIFAR10/100-C and ModelNet40-C datasets in Tab. 1, and the results on ImageNet-C dataset in Tab. 2. We make the following observations from the results. sTTT (N-O) Protocol. We ﬁrst analyze the results under the proposed sTTT (N-O) protocol. Our method outperforms all competing ones by a large margin. For example,3% improvement is observed on both CIFAR10-C and CIFAR100-C from the previous best (TTT++) and 5-13% improvement is observed on ImageNet-C compared with BN and TENT, and TTAC is superior in average accuracy and outperforms on 9 out of 15 types of corruptions compared with SHOT on ImageNet-C. We further combine TTAC with the class balance assumption made in SHOT (TTAC+SHOT). With the stronger assumptions out method can further improve upon TTAC alone, in particular on ModelNet40-C dataset. This result demonstrates TTAC’s compatibility with existing methods. Alternative Protocols. We further compare different methods under N-M, Y-O and Y-M protocols. Under the Y-O protocol, TTT++ [22] modiﬁes the source domain training objective by incorporating a contrastive learning branch [3]. To compare with TTT++, we also include the contrastive branch and observe a clear improvement on both CIFAR10-C and CIFAR100-C datasets. More TTT methods can be adapted to the N-M protocol which allows training on the whole target domain data multiple epochs. Speciﬁcally, we compared with BN, TENT and SHOT. With TTAC alone we observe substantial improvement on all three datasets and TTAC can be further combined with SHOT demonstrating additional improvement. Finally, under the Y-M protocol, we demonstrate very strong performance compared to TTT-R and TTT++. It is also worth noting that TTAC under the N-O protocol can already yield results close to TTT++ under the Y-M protocol, suggesting the strong test-time training ability of TTAC even under the most challenging TTT protocol. 4.4 Additional Datasets TTT on Hard Samples. CIFAR10.1 contains roughly 2,000 new test images that were re-sampled after the research on original CIFAR-10 dataset, which consists of some hard samples and reﬂects the 7Table 1: Comparison under different TTT protocols. Y/N indicates modifying source domain training objective or not. O/M indicate one pass or multiple passes test-time training. C10-C, C100-C and MN40-C refer to CIFAR10-C, CIFAR100-C and ModelNet40-C datasets respectively. All numbers indicate error rate in percentage. Method TTT Protocol Assum. StrengthC10-C C100-C MN40-C TEST - - 29.15 60.34 34.62 BN [13] N-O Weak 15.49 43.38 26.53 TENT [33] N-O Weak 14.27 40.72 26.38 T3A [14] N-O Weak 15.44 42.72 24.57 SHOT [21] N-O Weak 13.95 39.10 19.71 TTT++ [22] N-O Weak 13.69 40.32 - TTAC (Ours) N-O Weak 10.94 36.64 22.30 TTAC+SHOT (Ours) N-O Weak 10.99 36.39 19.21 TTT++ [22] Y-O Medium 13.00 35.23 - TTAC (Ours) Y-O Medium 10.69 34.82 - BN [13] N-M Medium 15.70 43.30 26.49 TENT [33] N-M Medium 12.60 36.30 21.23 SHOT [21] N-M Medium 14.70 38.10 15.99 TTAC (Ours) N-M Medium 9.42 33.55 16.77 TTAC+SHOT (Ours) N-M Medium 9.54 32.89 15.04 TTT-R [29] Y-M Strong 14.30 40.40 - TTT++ [22] Y-M Strong 9.80 34.10 - TTAC (Ours) Y-M Strong 8.52 30.57 - Table 2: Test-time training on ImageNet-C under the sTTT (N-O) protocol. Method Birt Contr Defoc Elast Fog Frost Gauss Glass Impul Jpeg Motn Pixel Shot Snow ZoomAvg TEST 38.82 89.55 82.23 87.13 64.84 76.83 97.34 90.50 97.76 68.31 83.60 80.37 96.74 82.22 74.3180.70BN (N-O)32.33 50.93 81.28 52.98 42.21 64.13 83.25 83.64 82.52 59.18 66.23 49.45 82.59 62.34 52.5163.04TENT (N-O)31.39 40.27 75.68 42.03 35.38 64.32 84.92 84.96 81.43 46.84 49.48 39.77 84.21 49.23 43.4956.89SHOT (N-O)30.6937.69 61.9741.3034.7454.19 76.33 71.94 74.24 46.5047.98 38.8870.60 46.0940.7451.59TTAC (N-O)30.3638.84 69.0639.6736.0150.20 66.18 70.17 64.36 45.5951.77 39.7262.43 44.5642.8050.11 normal domain shift in our life. The results in Table. 3 demonstrate our method is better able to adapt to the normal domain shift. TTT on Synthetic to Real Adaptation . VisDA-C is a large-scale benchmark of synthetic-to-real object classiﬁcation dataset. The setting of training on a synthetic dataset and testing on real data ﬁts well with the real application scenario. On this dataset, we conduct experiments with our method under the N-O, Y-O and Y-M protocols and other methods under respective protocols, results are presented in Table. 5. We make the following observations. First, our method (TTAC Y-O) outperforms all methods except TTT++ under the Y-M protocol. This suggests TTAC is able to be deployed in the realistic test-time training protocol. Moreover, if training on the whole target data is allowed, TTAC (Y-M) further beats TTT++ by a large margin, suggesting the effectiveness of TTAC under a wide range of TTT protocols. Table 3: Test-time training on CIFAR10.1. TEST BN TTT-R TENT SHOT TTT++ TTAC 12.1 14.1 11.0 13.4 11.1 9.5 9.2 Table 4: Source-free sTTT on CIFAR10-C. TEST BN TENT T3A SHOT TTAC TTAC+SHOT 29.15 15.49 14.27 15.44 13.95 13.74 13.35 4.5 Ablation Study We conduct ablation study on CIFAR10-C dataset for individual components, including anchored clus- tering, pseudo label ﬁltering, global feature alignment and ﬁnally the compatibility with contrastive branch [22]. For anchored clustering alone, we use all testing samples to update cluster statistics. For pseudo label ﬁltering alone, we implement as predicting pseudo labels followed by ﬁltering, then pseudo labels are used for self-training. We make the following observations from Tab. 6. Under both N-O and N-M protocols, introducing anchored clustering or pseudo label ﬁltering alone improves over the baseline, e.g. under N-O 29.15% →14.32% for anchored clustering and 29.15% →15.00% for pseudo label ﬁltering. When anchored clustering is combined with pseudo label ﬁltering, we observe a signiﬁcant boost in performance. This is due to more accurate estimation of category-wise cluster in the target domain and this reﬂects matching directly in the feature space may be better than minimizing cross-entropy with pseudo labels. We further evaluate aligning global features alone 8Table 5: Test-time training on VisDA. The numbers for competing methods are inherited from [22]. Method Plane Bcycl Bus Car Horse Knife Mcycl Person Plant Sktbrd Train Truck Per-class TEST 56.52 88.71 62.77 30.56 81.88 99.03 17.53 95.85 51.66 77.86 20.44 99.51 65.19BN (N-M) [13]44.38 56.98 33.24 55.28 37.45 66.60 16.55 59.02 43.55 60.72 31.07 82.98 48.99TENT (N-M) [33]13.43 77.98 20.17 48.15 21.72 82.45 12.37 35.78 21.06 76.41 34.11 98.93 45.21SHOT (N-M) [21]5.73 13.64 23.33 42.69 7.93 86.99 19.17 19.97 11.63 11.09 15.06 43.26 25.04TFA (N-M) [22]28.25 32.03 33.67 64.77 20.49 56.63 22.52 36.30 24.84 35.20 25.31 64.24 37.02TTT++ (Y-M) [22]4.13 26.20 21.60 31.70 7.43 83.30 7.83 21.10 7.03 7.73 6.91 51.40 23.03 TTAC (N-O) 18.54 40.20 35.84 63.11 23.83 39.61 15.51 41.35 22.97 46.56 25.24 67.81 36.71TTAC (Y-O) 7.19 29.99 22.52 56.58 8.14 18.41 8.25 22.28 10.18 23.98 13.55 67.02 24.01TTAC (Y-M) 2.74 17.73 18.91 43.12 5.54 12.24 4.66 15.90 4.77 10.78 9.75 62.45 17.38 TESTBN (N-O)TENT (N-O)SHOT (N-O)TTT++ (Y-O)T3A (N-O)OURS(N-O)OURS (Y-O) CIFAR10-C CIFAR100-C (a) Test-time cumulative error  (b) TTT++ Feature  (c) TTAC Feature Figure 2: (a) Comparison of test-time cumulative error under one-pass protocol. (b) T-SNE visualiza- tion of TTT++ feature embedding. (c) T-SNE visualization of TTAC feature embedding. with KL-Divergence. This achieves relatively good performance and obviously outperforms the L2 distance alignment adopted in [22]. Finally, we combine all three components and the full model yields the best performance. When contrast learning branch is included, TTAC achieves even better results. Table 6: Ablation study for individual components on CIFAR10-C dataset. TTT Protocol - N-O Y-O N-M Y-M Anchored Cluster. - ✓ - ✓ - ✓ ✓ ✓ ✓ - - ✓ ✓Pseudo Label Filter. - - ✓ ✓ - ✓ ✓ - ✓ - - ✓ ✓Global Feat. Align. - - - - KLD KLD KLD - - L2 Dist.[22] KLD KLD KLDContrast. Branch [22] - - - - - - ✓ - - - - - ✓Avg Acc 29.15 14.32 15.00 11.33 11.72 10.94 10.69 11.11 10.01 11.87 10.8 9.42 8.52 4.6 Additional Analysis Cumulative performance under sTTT. We illustrate the cumulative error under the sTTT protocol in Fig. 2 (a). For both datasets TTAC outperforms competing methods from the early stage of test-time training. The advantage is consistent throughout the TTT procedure. TSNE Visualization of TTAC features. We provide qualitative results for test-time training by visualizing the adapted features through T-SNE [32]. In Fig. 2 (b) and Fig. 2 (c), we compared the features learned by TTT++ [22] and TTAC (Ours). We observe a better separation between classes by TTAC, implying an improved classiﬁcation accuracy. Source-Free Test-Time Training. TTT aims to adapt model to target domain data by doing simulta- neous training and sequential inference. It has been demonstrated some light-weight information, e.g. statistics, from source domain will greatly improve the efﬁcacy of TTT. Nevertheless, under a more strict scenario where source domain information is strictly blind, TTAC can still exploit classiﬁer prototypes to facilitate anchored clustering. Speciﬁcally, we normalize the category-wise weight vector with the norm of corresponding target domain cluster center as prototypes. Then, we build source domain mixture of Gaussians by taking prototypes as mean with a ﬁxed covariance matrix. The results on CIFAR10-C are presented in Tab. 4. It is clear that even without any statistical information from source domain, TTAC still outperforms all competing methods. Test Sample Queue and Update Epochs. Under the sTTT protocol, we allow all competing methods to maintain the same test sample queue and multiple update epochs on the queue. To analyse the signiﬁcance of the sample queue and update epochs, we evaluate BN, TENT, SHOT and TTAC 9on CIFAR10-C and ImageNet-C level 5 snow corruption evaluation set under different number of update epochs on test sample queue and under a without queue protocol, i.e. only update model w.r.t. the current test sample batch. As the results presented in Tab. 7, we make the following observations. i) Maintaining a sample queue can substantially improve the performance of methods that estimate target distribution, e.g. TTAC ( 11.91 →10.88 on CIFAR10-C) and SHOT ( 15.18 →13.96 on CIFAR10-C). This is due to more test samples giving a better estimation of true distribution. ii) Consistent improvement can be observed with increasing update epochs for SHOT and TTAC. We ascribe this to iterative pseudo labeling beneﬁting from more update epochs. Table 7: Comparing with and without test sample queue and different numbers of model update epochs. w/ Queue maintains a test sample queue with 4096 samples; w/o Queue maintains a single mini-batch with 256 and 128 samples on CIFAR10-C and ImageNet-C respectively. CIFAR10-C ImageNet-C w/ Queue w/o Queue w/ Queue w/o Queue #Epochs 1 2 3 4* 1 1 2* 1 BN 15.84 15.99 16.04 16.00 15.44 62.34 62.34 62.59 TENT 13.35 13.83 13.85 13.87 13.48 47.82 49.23 48.39 SHOT 13.96 13.93 13.83 13.75 15.18 46.91 46.09 51.46 TTAC 10.88 10.80 10.58 9.96 11.91 45.44 44.56 46.64 Computation Cost Measured in Wall-Clock Time. Test sample queue and multiple update epochs introduce additional computation overhead. To investigate the impact on efﬁciency, we measure the overall wall time as the time elapsed from the beginning to the end of test-time training, including all I/O overheads. The per-sample wall time is then calculated as the overall wall time divided by the number of test samples. We report the per-sample wall time (in seconds) for BN, TENT, SHOT and TTAC in Tab. 8 under different update epoch settings and without queue setting. The Inference row indicates the per-sample wall time in a single forward pass including the data I/O overhead. We observe that, under the same experiment setting, BN and TENT are more computational efﬁcient, but TTAC is only twice more expensive than BN and TENT if no test sample queue is preserved (0.0083 v.s. 0.0030/0.0041) while the performance of TTAC w/o queue is still better than TENT (11.91 v.s. 13.48). In summary, TTAC is able to strike a balance between computation efﬁciency and performance depending on how much computation resource is available. This suggests allocating a separate device is only necessary when securing best performance is the priority. Table 8: The per-sample wall time (measured in seconds) on CIFAR10-C under sTTT protocol. w/ Queue w/o Queue #Epochs 1 2 3 4 1 BN 0.0136 0.0220 0.0293 0.0362 0.0030 TENT 0.0269 0.0399 0.0537 0.0663 0.0041 SHOT 0.0479 0.0709 0.0942 0.1183 0.0067 TTAC 0.0516 0.0822 0.1233 0.1524 0.0083 Inference0.0030 0.0030 0.0030 0.0030 0.0030 5 Conclusion Test-time training (TTT) tackles the realistic challenges of deploying domain adaptation on-the-ﬂy. In this work, we are ﬁrst motivated by the confused evaluation protocols for TTT and propose two key criteria, namely modifying source training objective and sequential inference, to further categorize existing methods into four TTT protocols. Under the most realistic protocol, i.e. sequential test-time training (sTTT), we develop a test-time anchored clustering (TTAC) approach to align target domain features to the source ones. Unlike batchnorm and classiﬁer prototype updates, anchored clustering allows all network parameters to be trainable, thus demonstrating stronger test-time training ability. We further propose pseudo label ﬁltering and an iterative update method to improve anchored clustering and save memory footprint respectively. Experiments on six datasets veriﬁed the effectiveness of TTAC under sTTT as well as other TTT protocols. Acknowledgement This work was supported in part by the National Natural Science Founda- tion of China (NSFC) under Grant 62106078, Guangdong R&D key project of China (No.: 2019B010155001), and the Program for Guangdong Introducing Innovative and Enterpreneurial Teams (No.: 2017ZT07X183). 10References [1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and conﬁrmation bias in deep semi-supervised learning. In International Joint Conference on Neural Networks, 2020. [2] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 2010. [3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, 2020. [4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [5] Cian Eastwood, Ian Mason, Chris Williams, and Bernhard Schölkopf. Source-free adaptation to measure- ment shift via bottom-up feature restoration. In International Conference on Learning Representations, 2022. [6] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Interna- tional conference on machine learning, 2015. [7] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 2012. [8] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in neural information processing systems, 2018. [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. [10] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [11] John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between gaussian mixture models. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2007. [12] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, 2018. [13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 2015. [14] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [15] Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In ACL, 2007. [16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, 2012. [17] Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. [18] Gerhard Kurz, Florian Pfaff, and Uwe D. Hanebeck. Kullback-leibler divergence and moment matching for hyperspherical probability distributions. In 2016 19th International Conference on Information Fusion (FUSION), 2016. [19] Dong-Hyun Lee et al. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, 2013. [20] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy labeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. [21] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, 2020. 11[22] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [23] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017. [24] Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning. Mit Press, 2008. [25] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize to imagenet? In International Conference on Machine Learning, 2019. [26] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence. Advances in Neural Information Processing Systems, 2020. [27] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, 2016. [28] Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, and Z Morley Mao. Benchmarking robustness of 3d point cloud recognition against common corruptions. arXiv preprint arXiv:2201.12296, 2022. [29] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning, 2020. [30] Hui Tang, Ke Chen, and Kui Jia. Unsupervised domain adaptation via structurally regularized deep clustering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020. [31] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [32] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 2008. [33] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [34] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 2018. [35] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2022. [36] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 2019. [37] Haifeng Xia, Handong Zhao, and Zhengming Ding. Adaptive adversarial network for source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. [38] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, and Wangmeng Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. [39] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. [40] Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne Saminger- Platz. Central moment discrepancy (cmd) for domain-invariant representation learning. In International Conference on Learning Representations, 2016. [41] Werner Zellinger, Bernhard A. Moser, Thomas Grubinger, Edwin Lughofer, Thomas Natschläger, and Susanne Saminger-Platz. Robust unsupervised domain adaptation for neural networks via moment alignment. Information Sciences, 2019. [42] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. arXiv e-prints, pages arXiv–2103, 2021. [43] Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 2018. 12Appendix of \"Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering\" In this appendix, we ﬁrst provide more details for the derivation of iterative updating target domain cluster parameters. We further provide more details of the hyperparameters used in TTAC. Finally, we present evaluation of TTAC with transformer backbone, ViT [4], additional evaluation of TTAC update epochs, the stability of TTAC under different data streaming orders and compared alternative target clustering updating strategies. A Derivations of Efﬁcient Iterative Updating The mean and covariance for each target domain cluster can be naively estimated through Maximum Likelihood Estimation (MLE) as below. The existing solution in TTT++ [22] stores the recent one thousand testing samples and their features for MLE. µ= 1 N N∑ i=1 f(xi), Σ = 1 N N∑ i=1 (f(xi) −µ)⊤(f(xi) −µ) (10) When N is very large, it is inevitable that a very large memory space must be allocated to store all features F ∈RN×D, e.g. the VisDA dataset has 55k testing samples and a naive MLE prohibits efﬁcient test-time training. In the manuscript, we propose to online update target domain feature distribution parameters without caching sample features as Eq. 8. The detailed derivations are now presented as follows. Formally, we denote the running mean and covariance at step t−1 as µt−1 and Σt−1, and the test minibatch at step tas Bt = {xi}i=1···NB . The following is the derivation of µt. µt = 1 Nt Nt ∑ i=1 f(xi), s.t. N t = Nt−1 + |Bt| (11) µt = 1 Nt ( Nt−1 ∑ i=1 f(xi) + Nt ∑ i=Nt−1+1 f(xi)) = 1 Nt (Nt−1 ·µt−1 + Nt ∑ i=Nt−1+1 f(xi)) = µt−1 + 1 Nt ∑ xi∈Bt (f(xi) −µt−1) (12) 13To simplify the expression, we denote δt = 1 Nt ∑ xi∈Bt (f(xi) −µt−1), so µt = µt−1 + δt. The following is the derivation ofΣt. For the ease of calculation, we use the asymptotic unbiased estimator of Σt as shown as below. Σt = 1 Nt Nt ∑ i=1 (f(xi) −µt)⊤(f(xi) −µt) = 1 Nt Nt ∑ i=1 (f(xi) −µt−1 −δt)⊤(f(xi) −µt−1 −δt) = 1 Nt Nt ∑ i=1 {(f(xi) −µt−1)⊤(f(xi) −µt−1) −δt⊤ (f(xi) −µt−1) −(f(xi) −µt−1)⊤δt + δt⊤ δt} = 1 Nt ( Nt−1 ∑ i=1 (f(xi) −µt−1)⊤(f(xi) −µt−1) + ∑ xi∈Bt (f(xi) −µt−1)⊤(f(xi) −µt−1) + Nt ∑ i=1 {−δt⊤ (f(xi) −µt−1) −(f(xi) −µt−1)⊤δt}) + δt⊤ δt = 1 Nt (Nt−1 ·Σt−1 + ∑ xi∈Bt (f(xi) −µt−1)⊤(f(xi) −µt−1)) −δt⊤ δt = Σt−1 + 1 Nt ∑ xi∈Bt {(f(xi) −µt−1)⊤(f(xi) −µt−1) −Σt−1}−δt⊤ δt (13) Furthermore, we give the formulations of the running mean µt k and covariance Σt k for the kth target domain cluster as below. δt k = 1 Nt k ∑ xi∈Bt FTC i FPP i 1 (ˆyi = k)(f(xi) −µt−1 k ), s.t. N t k = Nt−1 k + ∑ xi∈Bt FTC i FPP i 1 (ˆyi = k) µt k = µt−1 k + δt k, Σt k = Σt−1 k + 1 Nt k ∑ xi∈Bt FTC i FPP i 1 (ˆyi = k){(f(xi) −µt−1 k )⊤(f(xi) −µt−1 k ) −Σt−1 k }−δt k ⊤ δt k (14) Similarly to Nclip for the threshold used to clip the Nt protecting the gradient of new test samples, we use Nclip_k as the threshold to clip the Nt k for each target domain cluster. B Hyperparameter Values We provide the details of hyperparameters in this section. Hyperparameters are shared across multiple TTT protocols except for NC and Nitr which are only applicable under one-pass adaptation protocols. The details are shown as Tab. 9. αk and βk respectively represent the prevalence of each category, here we set them to 1 over the number of categories. NC indicates the length of the testing sample queue Cunder the sTTT protocol, and Nitr controls the update epochs on this queue. τTC and τPP are the thresholds used for pseudo label ﬁltering. Nclip and Nclip_k are the upper bounds of sample counts in the iterative updating of global statistics and target cluster statistics respectively. Finally λis the coefﬁcient of Lga, which takes the default value of 1. All models are implemented by the PyTorch 1.10.2 framework, CUDA 11.3 with an NVIDIA RTX 3090 GPU. 14Table 9: Hyper-parameters are used in our method. Dataset αk βk NC Nitr ξ τ TC τPP Nclip Nclip_k λ CIFAR10-C 0.1 0.1 4096 4 0.9 -0.001 0.9 1280 128 1.0 CIFAR100-C 0.01 0.01 4096 4 0.9 -0.001 0.9 1280 64 1.0 CIFAR10.1 0.1 0.1 4096 4 0.9 -0.001 0.9 1280 128 1.0 VisDA-C 1 12 1 12 4096 4 0.9 -0.01 0.9 1536 128 1.0 ModelNet40-C 0.025 0.025 4096 6 0.9 -0.1 0.5 1280 128 1.0 ImageNet-C 0.001 0.001 4096 2 0.9 -0.01 0.9 1280 64 1.0 Table 10: The results using ViT backbone on CIFAR10-C dataset. Method Bird Contr Defoc Elast Fog Frost Gauss Glass Impul Jpeg Motn Pixel Shot Snow ZoomAvg Std TEST 2.29 16.24 4.83 9.45 13.60 6.73 24.52 18.23 24.48 12.63 7.63 14.57 23.02 5.29 3.5012.47 7.36BN 2.29 16.24 4.83 9.45 13.60 6.73 24.52 18.23 24.48 12.63 7.63 14.57 23.02 5.29 3.5012.47 7.36TENT 1.84 3.55 3.31 7.01 5.57 4.09 60.97 10.20 61.12 9.72 4.93 3.87 22.47 4.552.64 13.72 19.19SHOT 2.00 3.13 3.46 6.63 5.79 4.06 11.65 9.39 10.58 9.69 5.03 3.63 10.05 4.35 2.706.14 3.15TTT++ 1.91 4.14 3.88 6.58 6.27 4.00 10.08 8.59 8.85 9.66 4.68 3.62 9.17 4.28 2.74 5.90 2.64TTAC (Ours)2.15 4.05 3.91 6.62 5.67 3.75 9.26 7.95 7.97 8.55 4.75 3.87 8.24 3.93 2.94 5.57 2.24 C Additional Evaluation C.1 Evaluation of TTAC with Transformer Backbone In this section, we provide additional evaluation of TTAC with a transformer backbone, ViT [4]. In speciﬁc, we pre-train ViT on CIFAR10 clean dataset and then follow the sTTT protocol to do test- time training on CIFAR10-C. The results are presented in Tab. 10. We report the average (Avg) and standard deviation (Std) of accuracy over all 15 categories of corruptions. Again, TTAC consistently outperform all competing methods with transformer backbone. C.2 Impact of TTAC Update Epochs on Cached Testing Sample Under the sTTT protocol, we perform multiple iterations of adaptation on cached testing sample queue. Preserving a history of testing samples is a commonly practice in test-time training. For example, T3A [14] preserves a support set, which contains testing samples and the pseudo labels, to update classiﬁer prototypes. TTT++ [22] preserves a testing sample queue to estimate global feature distribution. For these methods, both raw testing samples and features must be cached simultaneously, in comparison, we only cache the raw data samples and target domain clusters are estimated in an online fashion. Here, we analyze the impact of TTAC update epochs on cached testing samples. The results are presented in Tab. 11, where we make the following observations. First, the error rate is decreasing as the number of epochs increases, while at the cost of more computation time. But this can be solved by allocating a separate device for model adaptation. Second, the error rate saturates at Nitr = 4 suggesting only a few epochs is necessary to achieve good test-time training on target domain. Table 11: The impact of TTAC update epochs under the sTTT protocol. Nitr Bird Contr Defoc Elast Fog Frost Gauss Glass Impul Jpeg Motn Pixel Shot Snow ZoomAvg 1 6.57 8.20 8.57 15.82 11.61 11.60 17.46 22.66 20.99 11.97 10.44 13.79 15.40 10.96 7.4912.902 6.82 8.12 8.77 15.96 11.79 11.17 15.49 23.53 19.78 12.28 10.19 13.22 16.28 10.84 7.4912.783 6.80 8.11 8.53 15.94 11.36 10.89 14.87 22.67 18.94 11.77 9.83 12.51 15.91 10.58 7.35 12.404 6.41 8.05 7.85 14.8110.28 10.51 13.0618.3617.35 10.808.97 9.34 11.61 10.01 6.68 10.946 6.42 7.64 7.97 14.6610.66 10.59 13.3018.2917.61 10.868.94 9.36 11.76 10.03 6.73 10.98 C.3 Impact of Data Streaming Order The proposed sTTT protocols assumes test samples arrive in a stream and inference is made instantly on each test sample. The result for each test sample will not be affected by any following ones. In this section, we investigate how the data streaming order will affect the results. Speciﬁcally, we randomly shufﬂe all testing samples in CIFAR10-C for 10 times with different seeds and calculate the mean 15Table 12: The performance of TTAC under different data streaming orders. Random Seed 0 10 20 200 300 3000 4000 40000 50000 500000 Avg Error (%) 10.01 10.06 10.05 10.29 10.20 10.03 10.31 10.36 10.37 10.13 10.18±0.13 Table 13: Comparison of alternative strategies for updating target domain clusters. Strategy Bird Contr Defoc Elast Fog Frost Gauss Glass Impul Jpeg Motn Pixel Shot Snow ZoomAvg i. Without ﬁltering7.19 8.98 9.29 17.28 11.90 11.72 17.19 22.47 20.83 12.27 10.11 12.39 13.85 11.56 7.9713.00ii. Soft Assignment6.77 8.02 7.93 14.7710.87 10.68 13.65 18.69 17.58 11.26 9.33 9.54 11.70 10.56 6.9311.22Filtering (Ours)6.41 8.05 7.85 14.8110.28 10.51 13.06 18.36 17.35 10.80 8.97 9.34 11.61 10.01 6.6810.94 and standard deviation of test accuracy under sTTT protocol. The results in Tab. 12 suggest TTAC maintains consistent performance regardless of data streaming order. C.4 Alternative Strategies for Updating Target Domain Clusters In the manuscript, we presented target domain clustering through pseudo labeling. A temporal consistency approach is adopted to ﬁlter out conﬁdent samples to update target clusters. In this section, we discuss two alternative strategies for updating target domain clusters. Firstly, each target cluster can be updated with all samples assigned with respective pseudo label (Without Filtering). This strategy will introduce many noisy samples into cluster updating and potentially harm test-time feature learning. Secondly, we use a soft assignment of testing samples to each target cluster to update target clusters (Soft Assignment). This strategy is equivalent to ﬁtting a mixture of Gaussian through EM algorithm. Finally, we compare these two alternative strategies with our temporal consistency based ﬁltering approach. The results are presented in Tab. 13. We ﬁnd the results with temporal consistency based ﬁltering outperforms the other two strategies on 13 out of 15 categories of corruptions, suggesting pseudo label ﬁltering is necessary for estimating more accurate target clusters. C.5 Sensitivity to Hyperparameters We evaluate the sensitivity to two thresholds during pseudo label ﬁltering, namely the temporal smoothness threshold τTC and posterior threshold τPP . τTC controls how much the maximal probability deviate from the historical exponential moving average. If the current value is lower than the ema below a threshold, we believe the prediction is not conﬁdent and the sample should be excluded from estimating target domain cluster. τPP controls the the minimal maximal probability and below this threshold is considered as not conﬁdent enough. We evaluate τTC in the interval between 0 and -1.0 and τPP in the interval from 0.5 to 0.95 with results on CIFAR10-C level 5 glass blur corruption presented in Tab. 14. We draw the following conclusions on the evaluations. i) There is a wide range of hyperparameters that give stable performance, e.g. τTC ∈[0.5,0.0.9] and τPP ∈[−0.0001,−0.01]. ii) When temporal consistency ﬁltering is turn off, i.e. τTC = −1.0, because the probability is normalized to between 0 and 1, the performance drops substantially, suggesting the necessity to apply temporal consistency ﬁltering. Table 14: Evaluation of pseudo labeling thresholds on CIFAR10-C level 5 glass blur corruption. Numbers are reported as classiﬁcation error (%). τTC \\τPP 0.5 0.6 0.7 0.8 0.9 0.95 0.0 23.03 22.26 21.96 22.50 21.14 28.55 -0.0001 20.03 20.53 20.45 20.40 19.49 27.00 -0.001 19.66 20.51 19.49 20.48 19.42 26.83 -0.01 20.71 20.78 20.73 20.65 20.29 27.58 -0.1 24.10 21.47 21.46 22.36 21.45 28.71 -1.0 30.75 24.08 23.40 24.33 22.21 28.77 16C.6 Improvement by KL-Divergence Minimizing KL-Divergence between two Gaussian distributions is equivalent to matching the ﬁrst two moments of the true distributions [ 18]. TFA or TTT++ aligns the ﬁrst two moments through minimizing the L2/F norm, referred to as L2 alignment hereafter. Although L2 alignment is derived from Central Moment Discrepancy [41], the original CMD advocates a higher order moment matching and the weight applied to each moment is hard to estimate on real-world datasets. An empirical weight could be applied to balance the mean and covariance terms in TTT++, at the cost of introducing additional hyperparameters. We also provide a comparison between KL-Divergence and L2 alignment on CIFAR10-C level 5 snow corruption in Tab. 15 using the original code released by TTT++. The performance gap empirically demonstrates the superiority of KL-Divergence. Nevertheless, we believe a theoretical analysis into why KL-Divergence is superior under test-time training would be inspirational and we leave it for future work. Table 15: Comparing KL-Divergence and L2 alignment as test-time training loss with the original code released by TTT++ (Y-M) on CIFAR10 level 5 snow corruption. Feature Alignment Strategy Error (%) L2 alignment (original TTT++) 9.85 KL-Divergence 8.43 D Limitations and Failure Cases We discuss the limitations of our method from two perspectives. First, we point out that TTAC implements backpropagation to update models at test stage, therefore additional computation overhead is required. Speciﬁcally, as Tab. 8, we carried out additional evaluations on the per-sample wall clock time. Basically, we discovered that TTAC is 2-5 times computationally more expensive than BN and TENT. However, contrary to usual recognition, BN and TENT are also very expensive compared with no adaptation at all. Eventually, most test-time training methods might require an additional device for test-time adaptation. We further discuss the limitations on test-time training under more severe corruptions. Speciﬁcally, we evaluate TENT, SHOT and TTAC under 1-5 levels of corruptions on CIFAR10-C with results reported in Tab. 16. We observe generally a drop of performance from 1-5 level of corruption. Despite consistently outperforming TENT and SHOT at all levels of corruptions, TTAC’s performance at higher corruption levels are relatively worse, suggesting more attention must be paid to more severely corrupted scenarios. Table 16: Classiﬁcation error under different levels of snow corruption on CIFAR10-C dataset. Level 1 2 3 4 5 TEST 9.46 18.34 16.89 19.31 21.93 TENT 8.76 11.39 13.37 15.18 13.93 SHOT 8.70 11.21 13.16 15.12 13.76 TTAC 6.54 8.19 9.82 10.61 9.98 E Detailed results We further provide details of test-time training on CIFAR10-C, CIFAR100-C and ModelNet40-C datasets in Tab. 17, 18 and 19 respectively. The results in Tab. 17 and 18 suggest TTAC has a powerful ability to adapt to the corrupted images, and obtains the state-of-the-art performances on almost all corruption categories. 17Table 17: The results of CIFAR10-C under the sTTT protocol Method Bird Contr Defoc Elast Fog Frost Gauss Glass Impul Jpeg Motn Pixel Shot Snow ZoomAvg TEST 7.00 13.28 11.84 23.38 29.42 28.25 48.73 50.79 57.01 19.46 23.38 47.88 44.00 21.93 10.8429.15BN 8.21 8.36 9.73 19.43 20.16 13.72 17.46 26.34 28.11 14.00 13.90 12.22 16.64 16.00 8.0315.49TENT 8.22 8.07 9.93 18.29 15.65 14.14 16.60 24.10 25.80 13.39 12.34 11.06 14.75 13.87 7.8714.27T3A 8.33 8.70 9.70 19.51 20.26 13.83 17.27 25.61 27.63 14.05 14.26 12.12 16.37 15.78 8.1315.44SHOT 7.58 7.78 9.12 17.76 16.90 12.56 15.99 23.30 24.99 13.19 12.59 11.37 14.85 13.75 7.5113.95TTT++ 7.70 7.91 9.24 17.55 16.39 12.74 15.49 22.57 22.86 13.02 12.52 11.46 14.45 13.90 7.5113.69TTAC (Ours) 6.41 8.05 7.85 14.8110.28 10.51 13.0618.3617.35 10.808.97 9.34 11.61 10.01 6.6810.94TTAC+SHOT (Ours)6.37 6.98 7.79 14.8011.04 10.52 13.5818.3417.68 10.948.93 9.20 11.81 10.01 6.7910.99 Table 18: The results of CIFAR100-C under the sTTT protocol Method Bird Contr Defoc Elast Fog Frost Gauss Glass Impul Jpeg Motn Pixel Shot Snow ZoomAvg TEST 28.84 50.87 39.61 59.53 68.10 60.21 80.77 82.27 87.75 49.98 54.20 72.27 77.84 54.57 38.3660.34BN 31.78 33.06 33.86 48.65 54.23 42.28 48.02 57.08 60.14 39.09 40.72 37.76 45.83 46.31 31.9143.38TENT 30.45 31.47 32.48 45.84 44.85 41.39 45.59 52.31 56.16 38.94 38.41 35.55 43.40 42.89 31.1040.72T3A 31.66 32.63 33.62 47.60 53.06 41.95 46.63 55.51 58.92 38.89 40.26 37.21 45.32 46.08 31.4342.72SHOT 29.3630.4931.33 43.41 45.14 39.31 43.35 50.98 53.75 36.07 36.11 34.54 42.16 40.99 29.5239.10TTT++ 30.79 31.48 33.04 44.95 47.74 40.19 43.94 52.06 54.08 37.26 38.10 35.40 42.28 42.97 30.5840.32TTAC (Ours) 28.13 32.55 29.45 41.54 39.07 36.95 40.01 48.30 49.21 34.55 33.29 32.69 38.62 37.69 27.6136.64TTAC+SHOT (Ours)27.7332.1929.25 41.26 38.67 36.67 40.01 47.87 49.21 34.13 32.98 32.52 38.62 37.35 27.3636.39 Table 19: The results of ModelNet40-C under the sTTT protocol Method Background Cutout Density Inc. Density Dec. Inv. RBF RBF FFD Gaussian Impulse LiDAR Occlusion Rotation Shear Uniform UpsamplingAvgTEST 57.41 23.82 16.17 27.59 21.19 22.85 19.89 27.07 37.48 85.21 65.24 41.61 16.33 22.93 34.4434.62BN 52.88 18.07 13.25 20.42 16.57 17.50 17.75 17.30 18.60 70.75 58.51 26.94 14.51 15.48 19.3726.53TENT 51.94 17.38 13.25 17.99 14.14 16.65 15.68 16.49 17.10 81.44 64.18 22.33 13.29 14.59 19.2526.38T3A 52.51 16.37 13.09 18.23 14.26 15.48 15.88 14.14 15.68 69.12 54.82 24.80 13.01 14.14 17.0624.57SHOT 15.64 14.3412.24 15.48 13.3713.82 12.6413.1313.4366.0547.41 18.80 11.7912.44 15.1119.71TTAC (Ours)24.88 17.14 12.44 19.12 15.07 16.29 16.45 14.95 16.37 63.49 52.19 22.41 13.70 13.78 16.2122.30TTAC+SHOT (Ours)18.67 14.8910.88 15.58 13.1214.19 14.0412.1514.0857.3547.48 18.93 11.9911.92 12.8819.21 18",
      "meta_data": {
        "arxiv_id": "2206.02721v2",
        "authors": [
          "Yongyi Su",
          "Xun Xu",
          "Kui Jia"
        ],
        "published_date": "2022-06-06T16:23:05Z",
        "pdf_url": "https://arxiv.org/pdf/2206.02721v2.pdf",
        "github_url": "https://github.com/Gorilla-Lab-SCUT/TTAC"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the confusion in Test-Time Training (TTT) experimental settings by categorizing TTT protocols based on two key factors: modification of source training objective and sequential inference. It proposes a novel Test-Time Anchored Clustering (TTAC) approach under the most realistic protocol, sequential TTT (sTTT), to enable stronger test-time feature learning. TTAC discovers and matches target domain clusters to source domain anchors, improving generalization. It also introduces pseudo label filtering and iterative updating for efficiency and effectiveness. The main finding is that TTAC consistently outperforms state-of-the-art methods on six TTT datasets across all TTT protocols, providing a fair benchmarking framework.",
        "methodology": "TTAC employs a mixture of Gaussians to model clusters in the target domain, using category-wise source domain statistics as anchors. It minimizes the KL-Divergence between target and source component Gaussians for category-wise alignment. For robustness against noisy predictions, a pseudo label filtering mechanism is developed, combining temporal consistency filtering (based on exponential moving average posteriors) and posterior probability filtering. Component Gaussians are updated only by samples passing these filters. To utilize all test samples, a global feature alignment objective minimizes the KL-Divergence between global source and target feature distributions. An efficient iterative updating strategy is proposed for running statistics of target domain Gaussians, avoiding storing all features and incorporating a clipping mechanism to prevent gradient vanishing. The overall TTAC algorithm uses a fixed-length queue for recent test samples and optimizes a combined anchored clustering and global feature alignment loss.",
        "experimental_setup": "The method is evaluated on six TTT datasets: CIFAR10-C, CIFAR100-C, ImageNet-C (for corrupted images), CIFAR10.1 (for hard samples), VisDA-C (for synthetic-to-real adaptation), and ModelNet40-C (for 3D point clouds). Classification error rate (%) is the primary benchmark. ResNet-50 is used for image datasets and DGCNN for ModelNet40-C, optimized with SGD with momentum. Specific hyperparameters (batch size, learning rate) are detailed for each dataset. The evaluation compares TTAC against direct testing (TEST) and state-of-the-art methods (BN, TENT, T3A, SHOT, TTT-R, TTT++) across four categorized TTT protocols (N-O, Y-O, N-M, Y-M). Ablation studies are performed on individual TTAC components (anchored clustering, pseudo label filtering, global feature alignment). Further analyses include cumulative performance, T-SNE visualizations of features, source-free TTT, impact of test sample queue length and update epochs, data streaming order, alternative clustering strategies, hyperparameter sensitivity, and wall-clock computation cost.",
        "limitations": "The main limitation is the computational overhead, as TTAC requires backpropagation at test time, making it 2-5 times more computationally expensive than methods like BN and TENT. This implies that dedicated additional hardware might be necessary for test-time adaptation. Another limitation is that while TTAC consistently outperforms baselines, its performance on severely corrupted target data, particularly at higher corruption levels, is relatively weaker, indicating a need for further improvement in such extreme scenarios. A theoretical analysis for the observed superiority of KL-Divergence over L2 alignment for feature alignment is also noted as absent.",
        "future_research_directions": "Future research could focus on providing a theoretical analysis to explain why KL-Divergence is empirically superior to L2 alignment for feature alignment in test-time training. Further work is also needed to improve performance under more severely corrupted scenarios. Additionally, ongoing efforts could explore methods to reduce the computational overhead of test-time training, potentially mitigating the need for additional dedicated devices for adaptation.",
        "experimental_code": "parser = argparse.ArgumentParser()\nparser.add_argument('--dataset', default='cifar10')\nparser.add_argument('--dataroot', default='./data')\nparser.add_argument('--batch_size', default=128, type=int)\nparser.add_argument('--batch_size_align', default=512, type=int)\nparser.add_argument('--workers', default=0, type=int)\nparser.add_argument('--num_sample', default=1000000, type=int)\nparser.add_argument('--lr', default=0.001, type=float)\nparser.add_argument('--iters', default=4, type=int)\nparser.add_argument('--outf', default='.')\nparser.add_argument('--level', default=5, type=int)\nparser.add_argument('--corruption', default='snow')\nparser.add_argument('--resume', default=None, help='directory of pretrained model')\nparser.add_argument('--ckpt', default=None, type=int)\nparser.add_argument('--ssl', default='contrastive', help='self-supervised task')\nparser.add_argument('--temperature', default=0.5, type=float)\nparser.add_argument('--align_ext', action='store_true')\nparser.add_argument('--align_ssh', action='store_true')\nparser.add_argument('--fix_ssh', action='store_true')\nparser.add_argument('--with_ssl', action='store_true', default=False)\nparser.add_argument('--with_shot', action='store_true', default=False)\nparser.add_argument('--without_global', action='store_true', default=False)\nparser.add_argument('--without_mixture', action='store_true', default=False)\nparser.add_argument('--filter', default='ours', choices=['ours', 'posterior', 'none'])\nparser.add_argument('--model', default='resnet50', help='resnet50')\nparser.add_argument('--seed', default=0, type=int)\nargs = parser.parse_args()\n\nclass_num = 10 if args.dataset == 'cifar10' else 100\n\n# Offline Feature Summarization (Source Domain Statistics)\next_src_mu, ext_src_cov, ssh_src_mu, ssh_src_cov, mu_src_ext, cov_src_ext, mu_src_ssh, cov_src_ssh = offline(offlineloader, ext, classifier, head, class_num)\nbias = cov_src_ext.max().item() / 30.\nbias2 = cov_src_ssh.max().item() / 30.\ntemplate_ext_cov = torch.eye(2048).cuda() * bias\ntemplate_ssh_cov = torch.eye(128).cuda() * bias2\n\n# Initialization of Target Domain Gaussians and EMA for Pseudo-labels\next_src_mu = torch.stack(ext_src_mu)\next_src_cov = torch.stack(ext_src_cov) + template_ext_cov[None, :, :]\nsource_component_distribution = torch.distributions.MultivariateNormal(ext_src_mu, ext_src_cov)\ntarget_compoent_distribution = torch.distributions.MultivariateNormal(ext_src_mu, ext_src_cov)\n\nsample_predict_ema_logit = torch.zeros(len(tr_dataset), class_num, dtype=torch.float)\nsample_predict_alpha = torch.ones(len(tr_dataset), dtype=torch.float)\nema_alpha = 0.9\n\nema_n = torch.zeros(class_num).cuda()\nema_ext_mu = ext_src_mu.clone()\nema_ext_cov = ext_src_cov.clone()\n\nema_ext_total_mu = torch.zeros(2048).float()\nema_ext_total_cov = torch.zeros(2048, 2048).float()\n\nema_ssh_total_mu = torch.zeros(128).float()\nema_ssh_total_cov = torch.zeros(128, 128).float()\n\nema_total_n = 0.\nif class_num == 10:\n    ema_length = 128\n    mini_batch_length = 4096\nelse:\n    ema_length = 64\n    mini_batch_length = 4096\n\nif class_num == 10:\n    loss_scale = 0.05\nelse:\n    loss_scale = 0.5\n\nmini_batch_indices = []\n\n# Main adaptation loop for each incoming test batch\nfor te_batch_idx, (te_inputs, te_labels) in enumerate(teloader):\n    # Fixed-length queue for recent test samples\n    mini_batch_indices.extend(te_inputs[-1].tolist())\n    mini_batch_indices = mini_batch_indices[-mini_batch_length:]\n\n    # Dataloader re-creation for subset of recent samples\n    # (Code for tr_dataset_subset, tr_dataloader, tr_dataset_extra_subset, tr_extra_dataloader, tr_extra_dataloader_iter omitted for brevity)\n\n    for iter_id in range(min(args.iters, int(len(mini_batch_indices) / 256) + 1) + 1):\n        if iter_id > 0:\n            sample_predict_alpha = torch.where(sample_predict_alpha < 1, sample_predict_alpha + 0.2, torch.ones_like(sample_predict_alpha))\n\n        for batch_idx, (inputs_dummy, labels_dummy) in enumerate(tr_dataloader): # inputs_dummy/labels_dummy are for SSL, actual inputs for TTAC from tr_extra_dataloader_iter\n            optimizer.zero_grad()\n\n            # If args.with_ssl is True, SSL loss computation happens here (omitted for TTAC core method focus)\n\n            if iter_id > 0: # Actual TTAC adaptation begins after first iteration\n                loss = 0.\n                # Fetch inputs from the extra dataloader (containing current mini-batch samples from queue)\n                try:\n                    inputs, indexes = next(tr_extra_dataloader_iter)\n                except StopIteration:\n                    # Re-initialize iterator if exhausted\n                    tr_extra_dataloader_iter = iter(tr_extra_dataloader)\n                    inputs, indexes = next(tr_extra_dataloader_iter)\n\n                inputs = inputs.cuda()\n\n                feat_ext = ext(inputs)\n                logit = classifier(feat_ext)\n                feat_ssh = head(feat_ext)\n\n                # Pseudo-label generation with temporal consistency filtering\n                with torch.no_grad():\n                    ext.eval()\n                    origin_images = inputs\n                    origin_image_index = indexes\n                    predict_logit = net(origin_images)\n                    softmax_logit = predict_logit.softmax(dim=1).cpu()\n\n                    old_logit = sample_predict_ema_logit[origin_image_index, :]\n                    max_val, max_pos = softmax_logit.max(dim=1)\n                    old_max_val = old_logit[torch.arange(max_pos.shape[0]), max_pos]\n                    accept_mask = max_val > (old_max_val - 0.001)\n\n                    sample_predict_alpha[origin_image_index] = torch.where(accept_mask, sample_predict_alpha[origin_image_index], torch.zeros_like(accept_mask).float())\n\n                    sample_predict_ema_logit[origin_image_index, :] = \\\n                        torch.where(sample_predict_ema_logit[origin_image_index, :] == torch.zeros(class_num), \\\n                                    softmax_logit, \\\n                                    (1 - ema_alpha) * sample_predict_ema_logit[origin_image_index, :] + ema_alpha * softmax_logit)\n\n                    pro, pseudo_label = sample_predict_ema_logit[origin_image_index].max(dim=1)\n                    ext.train()\n                    del predict_logit\n\n                # Pseudo-label filtering mechanism\n                if args.filter == 'ours':\n                    pseudo_label_mask = (sample_predict_alpha[origin_image_index] == 1) & (pro > 0.9)\n                    feat_ext2 = feat_ext[pseudo_label_mask]\n                    feat_ssh2 = feat_ssh[pseudo_label_mask]\n                    pseudo_label2 = pseudo_label[pseudo_label_mask].cuda()\n                elif args.filter == 'posterior':\n                    with torch.no_grad():\n                        posterior = target_compoent_distribution.log_prob(feat_ext[:, None, :]) # log prob\n                        posterior_tmp = posterior.max(dim=1, keepdim=True)[0] - math.log((2 ** 127) / 10) # B, K\n                        posterior -= posterior_tmp\n                        posterior = posterior.exp() # prob / exp(posterior_tmp)\n                        posterior /= posterior.sum(dim=1, keepdim=True)\n                        posterior = posterior.transpose(0, 1).detach()  # K, N\n                # else: args.filter == 'none'\n\n                if args.align_ext:\n                    if not args.without_mixture:\n                        # Mixture Gaussian Alignment (Category-wise KL-Divergence & Iterative Update with Clipping)\n                        if args.filter != 'posterior': # Using pseudo_labels directly\n                            b, d = feat_ext2.shape\n                            feat_ext2_categories = torch.zeros(class_num, b, d).cuda() # K, N, D\n                            feat_ext2_categories.scatter_add_(dim=0, index=pseudo_label2[None, :, None].expand(-1, -1, d), src=feat_ext2[None, :, :])\n\n                            num_categories = torch.zeros(class_num, b, dtype=torch.int).cuda() # K, N\n                            num_categories.scatter_add_(dim=0, index=pseudo_label2[None, :], src=torch.ones_like(pseudo_label2[None, :], dtype=torch.int))\n\n                            ema_n += num_categories.sum(dim=1) # K\n                            alpha = torch.where(ema_n > ema_length, torch.ones(class_num, dtype=torch.float).cuda() / ema_length, 1. / (ema_n + 1e-10))\n\n                            delta_pre = (feat_ext2_categories - ema_ext_mu[:, None, :]) * num_categories[:, :, None] # K, N, D\n                            delta = alpha[:, None] * delta_pre.sum(dim=1) # K, D\n                            new_component_mean = ema_ext_mu + delta\n                            new_component_cov = ema_ext_cov \\\n                                                + alpha[:, None, None] * ((delta_pre.permute(0, 2, 1) @ delta_pre) - num_categories.sum(dim=1)[:, None, None] * ema_ext_cov) \\\n                                                - delta[:, :, None] @ delta[:, None, :]\n\n                            with torch.no_grad():\n                                ema_ext_mu = new_component_mean.detach()\n                                ema_ext_cov = new_component_cov.detach()\n\n                            if (class_num == 10 or len(mini_batch_indices) >= 4096) and (iter_id > int(args.iters / 2) or args.filter == 'none'):\n                                target_compoent_distribution.loc = new_component_mean\n                                target_compoent_distribution.covariance_matrix = new_component_cov + template_ext_cov\n                                target_compoent_distribution._unbroadcasted_scale_tril = torch.linalg.cholesky(new_component_cov + template_ext_cov)\n                                loss += (torch.distributions.kl_divergence(source_component_distribution, target_compoent_distribution) \\\n                                        + torch.distributions.kl_divergence(target_compoent_distribution, source_component_distribution)).mean() * loss_scale\n                        else: # Using posterior probabilities from target_compoent_distribution\n                            feat_ext2_categories = feat_ext[None, :, :].expand(class_num, -1, -1) # K, N, D\n                            num_categories = posterior # K, N (posterior is like soft counts)\n                            ema_n += num_categories.sum(dim=1) # K\n                            alpha = torch.where(ema_n > ema_length, torch.ones(class_num, dtype=torch.float).cuda() / ema_length, 1. / (ema_n + 1e-10))\n\n                            delta_pre = (feat_ext2_categories - ema_ext_mu[:, None, :]) * num_categories[:, :, None] # K, N, D\n                            delta = alpha[:, None] * delta_pre.sum(dim=1) # K, D\n                            new_component_mean = ema_ext_mu + delta\n                            new_component_cov = ema_ext_cov \\\n                                                + alpha[:, None, None] * ((delta_pre.permute(0, 2, 1) @ delta_pre) - num_categories.sum(dim=1)[:, None, None] * ema_ext_cov) \\\n                                                - delta[:, :, None] @ delta[:, None, :]\n\n                            with torch.no_grad():\n                                ema_ext_mu = new_component_mean.detach()\n                                ema_ext_cov = new_component_cov.detach()\n\n                            if (class_num == 10 or len(mini_batch_indices) >= 4096) and iter_id > int(args.iters / 2):\n                                target_compoent_distribution.loc = new_component_mean\n                                target_compoent_distribution.covariance_matrix = new_component_cov + template_ext_cov\n                                target_compoent_distribution._unbroadcasted_scale_tril = torch.linalg.cholesky(new_component_cov + template_ext_cov)\n                                loss += (torch.distributions.kl_divergence(source_component_distribution, target_compoent_distribution) \\\n                                        + torch.distributions.kl_divergence(target_compoent_distribution, source_component_distribution)).mean() * loss_scale\n\n                    if not args.without_global:\n                        # Global Feature Alignment (KL-Divergence & Iterative Update with Clipping)\n                        b = feat_ext.shape[0]\n                        ema_total_n += b\n                        alpha = 1. / 1280 if ema_total_n > 1280 else 1. / ema_total_n\n                        delta_pre = (feat_ext - ema_ext_total_mu.cuda())\n                        delta = alpha * delta_pre.sum(dim=0)\n                        tmp_mu = ema_ext_total_mu.cuda() + delta\n                        tmp_cov = ema_ext_total_cov.cuda() + alpha * (delta_pre.t() @ delta_pre - b * ema_ext_total_cov.cuda()) - delta[:, None] @ delta[None, :]\n                        with torch.no_grad():\n                            ema_ext_total_mu = tmp_mu.detach().cpu()\n                            ema_ext_total_cov = tmp_cov.detach().cpu()\n\n                        source_domain = torch.distributions.MultivariateNormal(mu_src_ext, cov_src_ext + template_ext_cov)\n                        target_domain = torch.distributions.MultivariateNormal(tmp_mu, tmp_cov + template_ext_cov)\n                        loss += (torch.distributions.kl_divergence(source_domain, target_domain) + torch.distributions.kl_divergence(target_domain, source_domain)) * loss_scale\n\n                    if args.without_mixture and args.without_global:\n                        # Fallback: simple cross-entropy if both alignment objectives are disabled\n                        logit2 = logit[pseudo_label_mask.cuda()]\n                        loss += F.cross_entropy(logit2, pseudo_label2) * loss_scale * 2\n\n                if args.align_ssh:\n                    # SSH Global Feature Alignment (similar logic as ext global alignment, omitted for primary focus)\n                    pass\n\n                if args.with_shot:\n                    # SHOT entropy minimization loss (omitted for primary focus)\n                    pass\n\n                # Backpropagation and Optimization\n                try:\n                    loss.backward()\n                except:\n                    pass\n                finally:\n                    del loss\n\n            if iter_id > 0:\n                optimizer.step()\n                optimizer.zero_grad()\n",
        "experimental_info": "Dataset: cifar10 (default), dataroot='./data'\nBatch sizes: 128 (for adaptation training `tr_dataloader`), 512 (for `tr_extra_dataloader` used for feature alignment)\nWorkers: 0\nNumber of samples: 1,000,000 (default, for `tr_dataset` which tracks all samples for EMA logit/alpha)\nLearning rate: 0.001\nIterations per test batch: 4 (`args.iters`)\nCorruption type: snow (default), level: 5 (default)\nModel: resnet50 (default)\nSeed: 0 (default)\nSSL task: contrastive (default), temperature: 0.5 (for `SupConLoss` if `with_ssl` is True)\nAlignments activated: `--align_ext` is `action='store_true'` (default False in command line, but typically enabled for TTAC), `--align_ssh` is `action='store_true'` (default False)\n\nLoss components: Mixture Gaussian Alignment and Global Feature Alignment (both use KL-Divergence).\nPseudo-label filtering (`args.filter`): 'ours' (default). This combines:\n  - Temporal consistency filtering: based on exponential moving average posteriors (`sample_predict_ema_logit`) with `ema_alpha = 0.9`.\n  - Posterior probability filtering: `pro > 0.9` (pseudo-label confidence threshold) and `sample_predict_alpha[origin_image_index] == 1` (acceptance mask from temporal consistency).\n  - An alternative `posterior` filter mode exists, which uses component posterior probabilities directly.\n\nIterative updating strategy for running statistics:\n  - Category-wise EMA length (`ema_length`): 128 for cifar10, 64 for cifar100 (for `ema_n`).\n  - Global EMA length (implicit in `ema_total_n` clipping): `alpha = 1. / 1280` if `ema_total_n > 1280`.\n  - Initialization bias for covariance matrix (`template_ext_cov`, `template_ssh_cov`): `cov_src_ext.max().item() / 30.` and `cov_src_ssh.max().item() / 30.` respectively.\n\nLoss scale (`loss_scale`): 0.05 for cifar10, 0.5 for cifar100 (applied to KL-Divergence terms).\nFixed-length queue for recent test samples: `mini_batch_length = 4096`. This queue stores indices of `mini_batch_length` most recent test samples. Adaptation is performed using only these samples in each test batch iteration."
      }
    },
    {
      "title": "L-TTA: Lightweight Test-Time Adaptation Using a Versatile Stem Layer"
    },
    {
      "title": "AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation",
      "abstract": "Test-time adaptation (TTA) has emerged as a viable solution to adapt\npre-trained models to domain shifts using unlabeled test data. However, TTA\nfaces challenges of adaptation failures due to its reliance on blind adaptation\nto unknown test samples in dynamic scenarios. Traditional methods for\nout-of-distribution performance estimation are limited by unrealistic\nassumptions in the TTA context, such as requiring labeled data or re-training\nmodels. To address this issue, we propose AETTA, a label-free accuracy\nestimation algorithm for TTA. We propose the prediction disagreement as the\naccuracy estimate, calculated by comparing the target model prediction with\ndropout inferences. We then improve the prediction disagreement to extend the\napplicability of AETTA under adaptation failures. Our extensive evaluation with\nfour baselines and six TTA methods demonstrates that AETTA shows an average of\n19.8%p more accurate estimation compared with the baselines. We further\ndemonstrate the effectiveness of accuracy estimation with a model recovery case\nstudy, showcasing the practicality of our model recovery based on accuracy\nestimation. The source code is available at https://github.com/taeckyung/AETTA.",
      "full_text": "AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation Taeckyung Lee† Sorn Chottananurak† Taesik Gong‡ Sung-Ju Lee† †KAIST ‡Nokia Bell Labs {taeckyung,sorn111930,profsj}@kaist.ac.kr, taesik.gong@nokia-bell-labs.com Abstract Test-time adaptation (TTA) has emerged as a viable solu- tion to adapt pre-trained models to domain shifts using unla- beled test data. However, TTA faces challenges of adaptation failures due to its reliance on blind adaptation to unknown test samples in dynamic scenarios. Traditional methods for out-of-distribution performance estimation are limited by unrealistic assumptions in the TTA context, such as requir- ing labeled data or re-training models. To address this issue, we propose AETTA, a label-free accuracy estimation algo- rithm for TTA. We propose the prediction disagreement as the accuracy estimate, calculated by comparing the target model prediction with dropout inferences. We then improve the prediction disagreement to extend the applicability of AETTA under adaptation failures. Our extensive evaluation with four baselines and six TTA methods demonstrates that AETTA shows an average of 19.8%p more accurate estima- tion compared with the baselines. We further demonstrate the effectiveness of accuracy estimation with a model recovery case study, showcasing the practicality of our model recovery based on accuracy estimation. The source code is available at https://github.com/taeckyung/AETTA. 1. Introduction The rise of deep learning has impacted various fields with remarkable achievements [4, 13, 17, 32, 33]. In real-world deep learning applications, the divergence between training and test data, known as domain shifts, often leads to poor accuracy. For instance, object detection models encountering previously unseen data ( e.g., variations of objects) or dis- tributional shifts (e.g., weather changes) might suffer from performance degradation. To overcome this challenge, Test- Time Adaptation (TTA) [2, 11, 12, 28, 29, 34–36] has been regarded as a promising solution recently and actively stud- ied. TTA aims to adapt pre-trained models to domain shifts on the fly with only unlabeled test data. Despite recent advances in TTA, significant challenges hinder its practical applications. The core issue is that TTA’s Adapted Model Dropout  Inferences Prediction Disagreement Model A Model B A B Accuracy Estimation True acc. Estimated acc. Proposed Method Online Test Streams Figure 1. AETTA estimates the model’s accuracy after adaptation using unlabeled test data without needing source data or ground- truth labels. AETTA can be integrated into existing TTA methods to estimate their accuracy under various scenarios. reliance on unlabeled test-domain samples makes TTA sus- ceptible to adaptation failures, especially in dynamic envi- ronments where the domain continuously changes [29, 30]. Although recent TTA studies deal with dynamic test streams in TTA [ 11, 12, 29, 35, 36], the inherent risk of TTA– blind adaptation to unseen test samples without ground- truth labels–remains a critical vulnerability. Notably, the absence of ground-truth labels makes it difficult to moni- tor the correctness of the adaptation. While various out-of- distribution performance estimation approaches have been proposed [1, 5, 15, 27], such methods necessitate labeled train data for accuracy estimation, which is impractical for TTA scenarios. In light of these challenges, we propose AETTA (Accu- racy Estimation for Test-Time Adaptation), a novel accuracy estimation method designed for TTA without reliance on labeled data or source data access (Figure 1). AETTA lever- ages prediction disagreement with dropout inferences, where the prediction disagreement between the adapted model and dropout inferences serves as a basis for performance estima- tion. To enhance AETTA’s robustness to adaptation failure scenarios, we propose robust disagreement equality that dy- namically adjust the accuracy estimates based on model 1 arXiv:2404.01351v1  [cs.LG]  1 Apr 2024failures. The key idea is to extend the well-calibration as- sumption (i.e., predicted probabilities of expected model predictions are neither over-/under-confident [21]) to cover over-confident models (e.g., adaptation failures) via adaptive scaling of the predicted probability. In addition, we provide theoretical analysis on how AETTA can estimate accuracy with unlabeled test data. We evaluate AETTA on three TTA benchmarks (CIFAR10-C, CIFAR100-C, and ImageNet-C [ 18]) with two scenarios of fully TTA ( i.e., adapting to each corrup- tion) [34] and continual TTA (i.e., continuously adapting to 15 corruptions) [35]. We evaluate the accuracy estimation of AETTA integrated with six state-of-the-art TTA algo- rithms [12, 28, 29, 34–36]. We compare AETTA with four baselines that could be applied in the TTA setting. The result illustrates that AETTA shows an average of 19.8%p more accurate estimation compared with the baselines in various TTA methods and evaluation scenarios. Furthermore, we explore the impact of performance es- timation in TTA through a case study where we avoided undesirable accuracy drops in TTA based on AETTA. We propose a simple model recovery algorithm, which resets the model when consecutive estimated accuracy degradation or sudden accuracy drop are observed. Our case study shows that our model recovery algorithm with accuracy estimation achieved 11.7%p performance improvement, outperforming the best baseline that knows when distribution changes by 3.0%p. The result shows an example where accuracy estima- tion could benefit TTA in practice. 2. Preliminaries 2.1. Test-Time Adaptation (TTA) Consider the source data distribution DS, and the target data distribution DT and its random variable (X, Y), where Y is typically unknown to the learning algorithm, and K is total number of classes. The covariate shift assump- tion [ 31] asserts a disparity between the source and tar- get data distributions, defined by DS(x) ̸= DT (x) while maintaining consistency in the conditional label distribution: DS(y|x) = DT (y|x). Let h ∼ HA denote a hypothesis that predicts a single class for a single input and f denote a corresponding soft- max value before class prediction. We define the hypothesis space HA as a hypothesis space H induced by a stochastic training algorithm A [21]. The stochasticity could arise from a different random initialization or data ordering. Assuming an off-the-shelf model h0 ∼ HA pre-trained on DS, the goal of (fully) test-time adaptation (TTA) [34] is to adapt h0 for the target distribution DT to produce h, using a batch of the unlabeled test set in an online manner. 2.2. Accuracy Estimation in TTA We adopt a common TTA setup where source data is unavail- able and target test data lacks labels [12, 28, 29, 34–36]. The objective of TTA accuracy estimation is to predict the test accuracy (or error) with unlabeled test streams. Given an adapted model h(·; Θ)at time t, we denote the test error of model h(·; Θ)by: ErrDT (h) ≜ EDT [1(h(X) ̸= Y )]. (1) Note that we use the terms test accuracy and test error de- pending on the context, and the sum of them is 1. Given the temporal nature of TTA, we consider estimating the accuracy of the model h(·; Θ)–which has been updated before time t–with the test batch Xt. Following the estimation, the test batch Xt is used for adaptation. 3. Methodology 3.1. Disagreement Equality We introduce an approach for estimating the test error of a model that is adapted at test time. The key idea is to compare the model’s output against outputs generated through dropout inference. Remarkably, this estimation process does not rely on access to the original training or labeled test data, which contrasts with existing accuracy estimation methods [1, 5, 15, 21, 27]. For example, generalization disagreement equality (GDE) [21] proposes a theoretical ground for estimating model error by measuring the disagreement rate between two networks. However, GDE requires multiple pre-trained models from different training procedures to calculate the disagreement rate. Instead of multiple pre-trained models, our strategy uti- lizes dropout inference sampling, a technique where random parts of a model’s intermediate layer outputs are omitted dur- ing the inference process [9]. From a single adapted model, we simulate the behavior of independent and identically dis- tributed (i.i.d.) models by dropout inference sampling. Definition 3.1. The hypothesis space HA satisfies the dropout independence if for anyh ∼ HA, h and its dropout inference samples are i.i.d. over HA. To estimate the accuracy of the model, we proposepre- diction disagreement with dropout inferences (PDD) that calculates a disagreement between the adapted modelh(·; Θ) and the dropout inferences h(·; Θdropout) with respect to test samples as: PDDDT(h) ≜ EDT   1 N NX i=1 1 \u0002 h(X; Θ) ̸= h(X; Θdropouti) \u0003  , (2) where N is the number of dropout inferences. 2We now provide the theoretical background to estimate test error with PDD. We first define the expectation function ˜h [21] over hypothesis space HA, which produces proba- bility vector of size K. For k-th element ˜hk(x), we define: ˜hk(x) ≜ Eh∼HA[1[h(x) = k]], (3) which indicates the probability of a sample x sampled from DT being classed as the class k. Note that the expectation function does not represent the model’s accuracy; it indicates the probability of the input being classified as a particular class, regardless of the ground truth labels. Then, we define a confidence-prediction calibration as- sumption, indicating that the value of ˜h for a particular class equals the probability of the sample having the same ground- truth label [21]. Definition 3.2. The hypothesis space HA and correspond- ing expectation function ˜h satisfies confidence-prediction calibration1 on DT if for any confidence value q ∈ [0, 1] and class k ∈ [1, ··· , K]: p(Y = k|˜hk(X) = q) = q. (4) With PDD and the assumption of dropout independence and confidence-prediction calibration, we are able to esti- mate the model’s prediction error h (Theorem 3.1). Detailed proof is provided in the Appendix A.1. Theorem 3.1 (Disagreement Equality). If the hypothesis space HA and corresponding expectation function˜h satisfies dropout independence and confidence-prediction calibration, prediction disagreement with dropouts (PDD) approximates the test error over HA: Eh∼HA[ErrDT (h)] = Eh∼HA[PDDDT (h)]. (5) 3.2. Robust Disagreement Equality Adaptation failures in TTA are often coupled with over- confident incorrect predictions. Figure 2 shows an illustra- tive example of this case; as the expectation function’s ac- curacy drops, the confidence increases, and predictions are skewed towards a few classes2. This violates the confidence- prediction calibration, leading to a high misalignment be- tween test error and PDD (red lines in Figure 3). To tackle the issue, we propose a robust confidence- prediction calibration to provide the theoretical ground of accuracy estimation for both well-calibrated or over- confident expectation function ˜h. 1We rename the term from class-wise calibration [21] to clearly state the purpose of the calibration. 2Using the probabilistic property of expectation of dropout infer- ences [9], we approximate ˜h(X) as EHA[Edropout[h(X; Θdropout)]]. /uni00000012/uni00000013/uni00000012/uni00000012/uni00000012/uni00000014/uni00000012/uni00000012/uni00000012 /uni00000031/uni00000050/uni0000004e/uni0000004b/uni00000050/uni00000047/uni00000002/uni00000024/uni00000043/uni00000056/uni00000045/uni0000004a /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) (a) Test batch accuracy and confidence. 0 1000 2000 Online batch 0 25 50 75 100Predicted class  (b) Predicted class distribution. Figure 2. Batch-wise accuracy, confidence, and prediction distribu- tion when a model failed to adapt. TENT [34] is used on CIFAR100- C with continually changing domains. The model becomes over- confident, and predictions are skewed. Definition 3.3. The hypothesis space HA and correspond- ing expectation function ˜h satisfies robust confidence- prediction calibration on DT if for any confidence value q ∈ [0, 1], any class k ∈ [1, ··· , K], and the over-confident class k′, there exists a weighting constant b ≥ 1 and corre- sponding 0 ≤ a ≤ 1 that satisfies: p(Y = k′|˜hk′(X) = q) = aq, (6) and p(Y = k|˜hk(X) = q) = bq for k ̸= k′. (7) Robust confidence-prediction calibration adjusts the over- confident expectation function ˜h to have a lower probability on the misclassified class k′ via multiplying a ≤ 1. Note that we can easily expand Definition 3.3 for multiple over- confident classes. Then, we estimate the test error with The- orem 3.2 (detailed proof in the Appendix A.2). Theorem 3.2 (Robust Disagreement Equality) . If the hy- pothesis space HA and corresponding expectation function ˜h satisfies dropout independence and robust confidence- prediction calibration with a weighting constant b, predic- tion disagreement with dropouts (PDD) approximates the test error over HA: Eh∼HA[ErrDT (h)] = b Eh∼HA[PDDDT (h)] − C, (8) where C = Z q∈[0,1] (b − a) q(1 − q) p(˜hk′(X) = q)dq. (9) 3.3. Accuracy Estimation for TTA With Theorem 3.2, we propose an empirical approach to estimate the single model test error. Our experiments show that a single model’s disagreement (and the test error) lies close to the robust disagreement equality. This aligns with the previous finding that a single pair of differently-trained models’ disagreement rate (and the test error) lies close to 3/uni00000012/uni00000010/uni00000012/uni00000012/uni00000012/uni00000010/uni00000014/uni00000017/uni00000012/uni00000010/uni00000017/uni00000012/uni00000012/uni00000010/uni00000019/uni00000017/uni00000013/uni00000010/uni00000012/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni0000000f/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000036/uni00000027/uni00000030/uni00000036 /uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a /uni00000025/uni00000032/uni00000025 /uni00000034/uni00000025/uni00000032/uni00000025 /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000032/uni00000054/uni00000051/uni00000044/uni00000043/uni00000044/uni0000004b/uni0000004e/uni0000004b/uni00000056/uni0000005b/uni00000002p /uni00000012/uni00000010/uni00000012/uni00000012/uni00000012/uni00000010/uni00000014/uni00000017/uni00000012/uni00000010/uni00000017/uni00000012/uni00000012/uni00000010/uni00000019/uni00000017/uni00000013/uni00000010/uni00000012/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni0000000f/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000027/uni00000023/uni00000036/uni00000023 /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000032/uni00000054/uni00000051/uni00000044/uni00000043/uni00000044/uni0000004b/uni0000004e/uni0000004b/uni00000056/uni0000005b/uni00000002p /uni00000012/uni00000010/uni00000012/uni00000012/uni00000012/uni00000010/uni00000014/uni00000017/uni00000012/uni00000010/uni00000017/uni00000012/uni00000012/uni00000010/uni00000019/uni00000017/uni00000013/uni00000010/uni00000012/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni0000000f/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000035/uni00000023/uni00000034 /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000032/uni00000054/uni00000051/uni00000044/uni00000043/uni00000044/uni0000004b/uni0000004e/uni0000004b/uni00000056/uni0000005b/uni00000002p /uni00000012/uni00000010/uni00000012/uni00000012/uni00000012/uni00000010/uni00000014/uni00000017/uni00000012/uni00000010/uni00000017/uni00000012/uni00000012/uni00000010/uni00000019/uni00000017/uni00000013/uni00000010/uni00000012/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni0000000f/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000025/uni00000051/uni00000036/uni00000036/uni00000023 /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000032/uni00000054/uni00000051/uni00000044/uni00000043/uni00000044/uni0000004b/uni0000004e/uni0000004b/uni00000056/uni0000005b/uni00000002p /uni00000012/uni00000010/uni00000012/uni00000012/uni00000012/uni00000010/uni00000014/uni00000017/uni00000012/uni00000010/uni00000017/uni00000012/uni00000012/uni00000010/uni00000019/uni00000017/uni00000013/uni00000010/uni00000012/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni0000000f/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000034/uni00000051/uni00000036/uni00000036/uni00000023 /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000032/uni00000054/uni00000051/uni00000044/uni00000043/uni00000044/uni0000004b/uni0000004e/uni0000004b/uni00000056/uni0000005b/uni00000002p /uni00000012/uni00000010/uni00000012/uni00000012/uni00000012/uni00000010/uni00000014/uni00000017/uni00000012/uni00000010/uni00000017/uni00000012/uni00000012/uni00000010/uni00000019/uni00000017/uni00000013/uni00000010/uni00000012/uni00000012 /uni00000025/uni00000051/uni00000050/uni00000048/uni0000004b/uni00000046/uni00000047/uni00000050/uni00000045/uni00000047/uni00000002hk′(X) /uni00000012 /uni00000014/uni00000012 /uni00000016/uni00000012 /uni00000018/uni00000012 /uni0000001a/uni00000012 /uni00000013/uni00000012/uni00000012/uni00000029/uni00000054/uni00000051/uni00000057/uni00000050/uni00000046/uni0000000f/uni00000036/uni00000054/uni00000057/uni00000056/uni0000004a/uni00000002/uni00000023/uni00000045/uni00000045/uni00000057/uni00000054/uni00000043/uni00000045/uni0000005b/uni00000002/uni0000000a/uni00000007/uni0000000b /uni00000035/uni00000051/uni00000036/uni00000036/uni00000023 /uni00000012/uni00000010/uni00000012 /uni00000012/uni00000010/uni00000014 /uni00000012/uni00000010/uni00000016 /uni00000012/uni00000010/uni00000018 /uni00000012/uni00000010/uni0000001a /uni00000013/uni00000010/uni00000012 /uni00000032/uni00000054/uni00000051/uni00000044/uni00000043/uni00000044/uni0000004b/uni0000004e/uni0000004b/uni00000056/uni0000005b/uni00000002p Figure 3. Correlations between the confidence value of estimated expectation function ˜h and (1) ground-truth accuracy (GroundTruth), (2) conditional probability p(Y = k′|˜hk′(X) =q) of confidence-prediction calibration (CPC), and (3) robust confidence-prediction calibration (RCPC). We used six TTA methods in CIFAR100-C with continual domain changes. We observed accuracy degradation in TENT and EATA and improvement in SAR, CoTTA, RoTTA, and SoTTA. When models failed to adapt, the original CPC misaligned with the ground truth. In contrast, our WCPC dynamically scaled the probability p, thus showing better alignment. the disagreement equality [21]. Therefore, we approximate a single model test error as: ErrDT (h) ≈ b PDDDT (h), (10) where we omit C due to the insufficient information regard- ing the true value of p(˜hk′(X) = q). Note that C ≈ 0 for models with calibration. Now, we discuss selecting a proper weighting constant b. Note that a desirable b should dynamically suppress the over- confident expectation function depending on the context so that the confidence-prediction calibration assumption holds. To this end, we use the skewness of the predicted outputs as an indicator of model over-confidence. Out intuition is based on the observation that the predicted class distribution is highly skewed when the adaptation fails (Figure 2b), which aligns with the findings from prior studies [19, 24]. Specifi- cally, we estimate the skewness of predictions by calculating the entropy (Ent) of the batch-aggregated softmax values from the dropout inferences over a test batch Xt: Eavg = Ent   1 N NX i=1 1 |Xt| X x∈Xt f(x; Θdropouti)  , (11) where Eavg would maximize as Emax = Ent(⃗1K/K) with uniform predictions among the batch (e.g., no failures); while the minimum value would be 0 when entire batch predicts a single class (e.g., adaptation failures). We then model b with Eavg as: b = \u0012Eavg Emax \u0013−α , (12) where α ∈ [0, ∞) is a hyperparameter. If the adaptation does not fail, predictions are uniformly distributed as Eavg = Emax and b = 1. Note that a = b = 1 drives Theorem 3.2 to be equivalent to Theorem 3.1. We found that modelingb with the average batch-wise entropy effectively corrects the correlation between confidence and prediction probability, as illustrated in Figure 3 (blue dots). Finally, with Equation 10 and Equation 12, we propose Accuracy Estimation for TTA (AETTA): ErrDT (h) ≈ \u0012Eavg Emax \u0013−α PDDDT (h). (13) Observe that α = 0 and ∞ result in ErrDT (h) = PDDDT (h) and ErrDT (h) = 1, respectively. Setting a small α would result in a lesser penalty with adaptation failures. On the other hand, choosing a high α would undesirably penalize model improvement cases. Our experiment found that accuracy estimation is not too sensitive to α (Figure 5b), and we chose α = 3 for the other experiments. Algorithm 1 AETTA: batchwise TTA accuracy estimation Input: Test batch Xt, model f, number of dropout infer- ences N PDD ← 0 Yavg ← ⃗0 ˆY ← f(Xt; Θ) for i ∈ {1, ··· , N} do ˆYd ← f(Xt; Θdropouti) Yavg ← Yavg + Avg( ˆYd) PDD ← PDD + Avg(1[arg max(ˆY) ̸= arg max(ˆYd)]) Yavg ← 1 N Yavg PDD ← 1 N PDD ▷ Avg. over dropouts Eavg ← Ent(Yavg) ▷ Entropy of avg. batch Err ← \u0010 Eavg Emax \u0011−α PDD ▷ ErrDT (h) Acc ← 1 − Err We summarize the accuracy estimation procedure in Algo- rithm 1. We first infer with the adapted model for the current test batch Xt. Then, we repeatedly perform dropout infer- ence sampling. WithN samples from dropout inferences, we estimate the entropy of the batch-aggregated softmax output Eavg. Finally, we calculated the expected error of the model by AETTA. We apply the exponential moving average to the final accuracy estimation for stable error estimation. 4Table 1. Mean absolute error (MAE) (%) of the accuracy estimation on fully TTA (adapting to each corruption type).Bold numbers are the lowest error. Averaged over three different random seeds for 15 types of corruption. TTA Method Dataset Method TENT [34] EATA [28] SAR [29] CoTTA [35] RoTTA [36] SoTTA [12] Avg. (↓) SrcValid 18.37 ± 0.29 14.37 ± 0.33 21.28 ± 0.27 18.43 ± 0.16 20.35 ± 1.31 13.13 ± 0.85 17.66 ± 0.24 SoftmaxScore [7] 6.26 ± 0.49 4.78 ± 0.12 5.21 ± 0.22 10.96 ± 0.28 6.01 ± 0.23 4.97 ± 0.50 6.37 ± 0.10 GDE [21] 18.69 ± 0.28 16.95 ± 0.22 21.25 ± 0.27 14.50 ± 0.03 23.27 ± 0.43 16.45 ± 0.21 18.52 ± 0.13 AdvPerturb [23] 23.06 ± 1.17 24.97 ± 1.00 21.89 ± 0.95 18.00 ± 0.82 19.35 ± 0.99 23.68 ± 0.85 21.83 ± 0.92 Fully CIFAR10-C AETTA 4.00 ± 0.03 3.87 ± 0.14 3.89 ± 0.07 6.83 ± 0.47 6.44 ± 1.35 5.28 ± 0.87 5.05 ± 0.46 SrcValid 38.96 ± 0.22 10.71 ± 0.31 42.68 ± 0.21 44.58 ± 0.30 23.50 ± 0.51 19.34 ± 0.63 29.96 ± 0.09 SoftmaxScore [7] 17.34 ± 0.10 27.86 ± 1.11 24.56 ± 0.25 34.50 ± 0.35 24.18 ± 0.19 23.98 ± 0.21 25.40 ± 0.23 GDE [21] 40.11 ± 0.05 71.53 ± 2.12 42.51 ± 0.23 33.21 ± 0.24 48.02 ± 0.56 34.24 ± 0.12 44.94 ± 0.23 AdvPerturb [23] 24.17 ± 0.41 8.22 ± 0.56 22.91 ± 0.60 20.53 ± 0.14 17.84 ± 0.65 25.77 ± 0.47 19.91 ± 0.26 Fully CIFAR100-C AETTA 6.89 ± 0.15 20.15 ± 1.70 6.54 ± 0.15 6.05 ± 0.12 6.88 ± 0.10 5.29 ± 0.18 8.63 ± 0.24 SrcValid 39.13 ± 0.89 35.89 ± 0.79 29.77 ± 0.94 41.09 ± 0.53 10.28 ± 0.28 16.00 ± 0.33 28.69 ± 0.54 SoftmaxScore [7] 20.67 ± 0.01 21.06 ± 0.03 24.42 ± 0.08 19.62 ± 0.02 21.03 ± 0.04 23.60 ± 0.07 21.73 ± 0.03 GDE [21] 70.58 ± 0.01 66.17 ± 0.07 63.48 ± 0.03 72.76 ± 0.02 66.39 ± 0.04 52.74 ± 0.02 65.35 ± 0.02 AdvPerturb [23] 12.56 ± 0.03 14.52 ± 0.01 18.76 ± 0.06 11.05 ± 0.02 12.93 ± 0.04 22.90 ± 0.02 15.45 ± 0.02 Fully ImageNet-C AETTA 6.14 ± 0.03 6.48 ± 0.02 6.43 ± 0.09 6.02 ± 0.03 14.82 ± 0.01 17.40 ± 0.26 9.55 ± 0.07 Table 2. Mean absolute error (MAE) (%) of the accuracy estimation on continual TTA (continuously adapting to 15 consecutive corruptions). Bold numbers are the lowest error. Averaged over three different random seeds for 15 types of corruption. TTA Method Dataset Method TENT [34] EATA [28] SAR [29] CoTTA [35] RoTTA [36] SoTTA [12] Avg. (↓) SrcValid 10.84 ± 1.83 11.06 ± 0.11 21.29 ± 0.26 18.30 ± 0.25 13.37 ± 0.89 9.40 ± 0.85 14.04 ± 0.58 SoftmaxScore [7] 41.10 ± 11.66 15.40 ± 4.73 5.21 ± 0.22 12.96 ± 0.37 12.57 ± 0.43 4.37 ± 0.09 15.27 ± 2.51 GDE [21] 46.29 ± 10.93 26.44 ± 5.16 21.25 ± 0.27 14.69 ± 0.15 17.50 ± 0.30 17.03 ± 0.70 23.87 ± 2.43 AdvPerturb [23] 15.56 ± 1.53 20.93 ± 2.83 21.88 ± 0.93 17.79 ± 0.74 22.95 ± 0.82 23.63 ± 0.78 20.45 ± 1.17 Continual CIFAR10-C AETTA 9.05 ± 1.02 7.13 ± 3.33 3.89 ± 0.06 5.82 ± 0.30 5.36 ± 1.22 4.73 ± 0.34 6.00 ± 0.35 SrcValid 11.00 ± 0.58 1.68 ± 0.18 38.20 ± 0.22 46.09 ± 0.38 19.43 ± 1.17 17.16 ± 1.57 22.32 ± 0.52 SoftmaxScore [7] 58.29 ± 1.82 76.58 ± 0.71 24.05 ± 0.29 36.27 ± 0.68 27.19 ± 0.12 21.89 ± 0.35 40.71 ± 0.43 GDE [21] 80.87 ± 1.29 94.01 ± 0.43 39.21 ± 0.22 35.43 ± 0.30 41.68 ± 0.45 35.29 ± 0.27 54.41 ± 0.18 AdvPerturb [23] 10.12 ± 0.24 1.97 ± 0.33 24.93 ± 0.57 19.62 ± 0.15 21.18 ± 0.71 25.12 ± 0.39 17.16 ± 0.32 Continual CIFAR100-C AETTA 5.85 ± 0.36 4.18 ± 0.82 6.67 ± 0.12 6.55 ± 0.17 5.86 ± 0.10 5.32 ± 0.18 5.74 ± 0.13 SrcValid 33.30 ± 0.93 36.42 ± 0.76 22.30 ± 0.55 41.06 ± 0.54 9.56 ± 0.26 14.28 ± 0.28 26.15 ± 0.53 SoftmaxScore [7] 19.34 ± 0.02 20.16 ± 0.05 21.91 ± 0.16 19.63 ± 0.01 17.56 ± 0.08 19.67 ± 0.50 19.71 ± 0.53 GDE [21] 68.30 ± 0.01 66.58 ± 0.03 64.36 ± 0.15 72.81 ± 0.07 73.76 ± 0.22 55.76 ± 0.45 66.93 ± 0.14 AdvPerturb [23] 14.82 ± 0.02 14.15 ± 0.06 19.17 ± 0.14 11.06 ± 0.02 11.05 ± 0.05 20.83 ± 0.39 15.18 ± 0.09 Continual ImageNet-C AETTA 5.66 ± 0.05 6.73 ± 0.03 6.68 ± 0.04 5.98 ± 0.04 11.19 ± 0.12 19.22 ± 0.79 9.24 ± 0.14 4. Experiments We describe our experimental setup and present the results. Please refer to the Appendix D for further details. Scenario. We consider both fully (non-continual) and contin- ual test-time adaptation scenarios. In the fully TTA setting, target domains are each corruption type [ 34], while in the continual setting, the target domain continually changes to 15 different corruptions [35]. During adaptation, we calcu- late the accuracy estimation for every batch and report the mean absolute error between the ground-truth batch-wise accuracy. We ran experiments with three random seeds (0, 1, 2) and reported the average values. We use the test batch size 64 for all TTA baselines, with a memory size 64 for RoTTA [36] and SoTTA [12]. We specify further details of the hyperparameters in the Appendix D.2. Datasets. We use three standard benchmarks for test-time adaptation: CIFAR10-C, CIFAR100-C, and ImageNet- C [18]. Each dataset contains 15 different corruptions with five levels of corruption, where we use corruption level 5. CIFAR10-C/CIFAR100-C/ImageNet-C contains 10/100/1,000 classes with 10,000/10,000/50,000 test data, respectively. We use pre-trained ResNet18 [17] as an adapta- tion target, following a recent study [12]. TTA Methods. We consider six state-of-the-art TTA meth- ods. TENT [34] updates BN parameters with entropy min- imization. EATA [28] utilizes entropy thresholding-based sample filtering and anti-forgetting regularization. SAR [29] also adapts sample filtering with sharpness minimization [8]. CoTTA [35] addresses the continual setting by augmenta- tions and stochastic restoration of model weights to avoid catastrophic forgetting. RoTTA [36] adapts with robust batch normalization and category-balanced sampling with timeli- ness and uncertainty. SoTTA [12] utilizes high-confidence uniform-sampling and entropy-sharpness minimization for robust adaptation in noisy data streams [8]. 50 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) TENT GroundTruth SrcValid SoftmaxScore GDE AdvPerturb AETTA (Ours) 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) EATA 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) SAR 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) CoTTA (a) CIFAR10-C. 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) TENT 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) EATA 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) SAR 0 1000 2000 Online Batch 0 20 40 60 80 100Accuracy (%) CoTTA (b) CIFAR100-C. 0 5000 10000 Online Batch 0 20 40 60 80 100Accuracy (%) TENT 0 5000 10000 Online Batch 0 20 40 60 80 100Accuracy (%) EATA 0 5000 10000 Online Batch 0 20 40 60 80 100Accuracy (%) SAR 0 5000 10000 Online Batch 0 20 40 60 80 100Accuracy (%) CoTTA (c) ImageNet-C. Figure 4. Qualitative results on continual CIFAR10-C, CIFAR100-C, and ImageNet-C. Accuracy Estimation Baselines. We evaluate four distinct accuracy estimation baselines that could be applied to TTA settings: SrcValid, SoftmaxScore, GDE, and AdvPerturb. • SrcValid is a widely used technique that validates per- formance by leveraging labeled source data. It computes the accuracy using a hold-out labeled source dataset to estimate the target performance. Importantly, the hold-out source data for validation were not used for training in other baselines to ensure they do not affect the model per- formance. Note that TTA usually assumes that source data are unavailable during test time; hence, this baseline is unrealistic in TTA. We nonetheless include SrcValid as one of our baselines to understand its performance when the source data are accessible. • SoftmaxScore [7] utilizes the confidence scores derived from the last softmax layer as the model’s accuracy, which is also a widely used baseline [5, 6]. It estimates the target domain accuracy by averaging softmax confidence scores computed from the current test batch. In addition, we apply temperature scaling [16] to improve the estimation performance [10]. • Generalization disagreement equality (GDE) [21] aims to estimate test accuracy by quantifying the (dis)agreement rate between predictions on a test batch generated by a pair of models. Since training multiple models is impractical, we compare the current adapted model and the previous model right before the adaptation. We also report a com- parison with the original GDE and multiple pre-trained models in Appendix B. • Adversarial perturbation ( AdvPerturb) [ 23] also aims to estimate the OOD accuracy by calculating the agree- ment between the domain-adapted model and the source model, where adversarial perturbations on a test batch are applied to penalize the unconfident samples near the de- cision boundary. We note that the original paper aims to predict the accuracy of the source model, while our goal is to predict the accuracy of the adapted model. Results. Table 1 and Table 2 show the results on the fully and continual TTA settings. We observe that none of the baselines could reliably predict the accuracy among different scenarios. On the other hand, AETTA achieves the lowest mean absolute error, including adaptation failure cases (e.g., TENT in continual CIFAR10/100-C). On average, AETTA outperforms baselines by 19.8%p, validating the effective- ness of our robust prediction disagreement in diverse scenar- ios. More details are in the Appendix F. 6TENT EATA SARCoTTARoTTASoTTA 0 2 4 6 8 10MAE (%) 5 10 15 (a) Number of dropout inferences N. TENTEATA SARCoTTARoTTASoTTA 0 20 40 60 80 100MAE (%) 0 (PDD) 1 2 3 4 5 (b) Scaling hyperparameter α. Figure 5. Impact of hyperparameters on the accuracy estimation performance. Qualitative Analysis. We qualitatively analyze the results of the baselines and AETTA to understand the behavior. Figure 4 visualizes the ground-truth accuracy and the estimated accuracy from the baselines and AETTA under adaptation failure and non-failure cases. The Gaussian filter is applied for visualization. We observe that AETTA generally shows a reliable estimation of the ground-truth accuracy in diverse scenarios (fully and continual) and datasets (CIFAR10/100-C and ImageNet-C). SrcValid cor- rectly estimated when model accuracy decreases; however, it consistently predicted high accuracy when the adaptation did not fail. This limitation might be due to the distributional gap between source and target data. SoftmaxScore [ 7] captures the trend of ground-truth accuracy in some cases, but it overestimates the accuracy when the model accuracy drops. This is mostly due to the over-confident predictions from the model. GDE [ 21] showed to constantly predict high values among different TTA methods. Note that GDE was originally designed to utilize various pre-trained models. To use GDE in TTA, we utilize adapted models sampled at different stages of adaptation. The result suggests that utilizing multiple models from the single stochastic learning process might not be sufficient to consist of independent and identically distributed (i.i.d.) ensembles, leading to inaccurate estimation. AdvPerturb [ 23] shows accuracy estimations when ground-truth accuracy decreases but shows high errors in other cases. We believe this happens because it aims to evaluate the performance of the source model, not the adapted model. We found similar patterns were observed with different TTA methods. Impact of Hyperparameter N. The number of dropout in- ferences, N, is a hyperparameter for calculating the test error. We conducted an ablation study in continual CIFAR100-C with varying N ∈ {5, 10, 15}. As shown in Figure 5a, we found the effect of hyperparameter N is negligible. We inter- pret this result as the effect of calculating prediction disagree- ment over sufficient batch size with dropout independence, which could reduce the probabilistic variances from dropout inference sampling. We adopt a single value of N = 10 for the other experiments. Impact of Hyperparameter α. We investigate the im- pact of α, a hyperparameter to control the strength of ro- bust confidence-prediction calibration. We conduct an ab- lation study in continual CIFAR100-C with varying α ∈ {0, . . . ,5}, where α = 0 indicates no weighting, thus Err = PDD. Figure 5b shows the result. Note that estima- tions are often inaccurate when α = 0 , which shows the importance of our robust equality. Setting a reasonable α is important to predict failed adaptation cases (TENT and EATA) properly, but it is generally robust after certain values. We adopt α = 3 for the other experiments. 5. Case Study: Model Recovery The deployment of TTA algorithms encounters a significant challenge when exposed to extreme test streams, such as continuously changing corruptions [ 35]. Several TTA algorithms (e.g., TENT [34]) were not designed to exhibit robustness under such extreme conditions. Consequently, the model weights are poorly updated, leading to perfor- mance degradation, even worse than the source model. Although recent studies attempt to manage dynamic test streams [11, 12, 35], TTA algorithms are still susceptible to adaptation failures [30]. To tackle the issue, we perform a case study of model recovery based on the accuracy estimation. Recovery Algorithm. We introduce a simple reset algorithm based on our accuracy estimation with AETTA. Our reset algorithm detects two cases: (1) consecutive low accuracies and (2) sudden accuracy drop. First, we reset the model if the five recent consecutive estimated accuracies ( e.g., t − 4, ··· , t) are lower than the five previous consecutive estimations (e.g., t − 9, ··· , t− 5). This way, we can detect the gradual degradation of TTA accuracy. Second, we apply hard lower-bound thresholding, which resets the model if the estimated accuracy is below the threshold ( e.g., 0.2). This could prevent catastrophic failure of TTA algorithms. Baselines. Some TTA studies covered the model recov- ery/reset as a part of the TTA algorithm: Episodic resetting (Episodic) [37], where the model resets after every batch; Model Recovery Scheme (MRS) [29], where the model re- sets when the moving average of entropy loss falls below a certain threshold; Stochastic restoration (Stochastic) [35], where a small number of model weights are stochastically restored to the initial weight of the source model; and Fisher information based restoration (FisherStochastic) [3], which applies stochastic restoration for layer importance measured by Fisher information matrix. We also include a baseline (DistShift), which assumes that the model knows when the distribution changes and thus acts as an oracle. DistShift resets the model when the test data distribution (corruption) changes, which is not feasible in practice. 7Table 3. Average accuracy improvement (%p) with model recovery.Bold number is the highest improvement. Averaged over three different random seeds for 15 types of corruption. TTA Method Method TENT [34] EATA [28] SAR [29] CoTTA [35] RoTTA [36] SoTTA [12] Avg. (↑) Episodic [37] 33.58 ± 1.04 51.28 ± 0.52 -7.00 ± 0.26 1.65 ± 0.10 -22.57 ± 0.85 -26.40 ± 0.51 5.09 ± 0.24 MRS [29] 24.12 ± 2.11 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 -1.97 ± 2.23 0.00 ± 0.00 3.69 ± 0.22 Stochastic [35] 35.93 ± 0.78 -0.01 ± 0.47 -2.00 ± 0.48 0.00 ± 0.00 -2.55 ± 0.49 0.35 ± 0.51 5.29 ± 0.19 FisherStochastic [3] 40.27 ± 1.29 0.12 ± 1.16 -4.85 ± 0.13 0.13 ± 0.03 -2.89 ± 0.13 -1.36 ± 0.51 5.24 ± 0.29 DistShift 38.93 ± 1.15 22.17 ± 2.38 -3.25 ± 0.10 1.51 ± 0.09 -7.63 ± 0.23 0.68 ± 0.19 8.74 ± 0.55 AETTA 36.79 ± 1.20 48.64 ± 0.74 -5.66 ± 0.20 1.64 ± 0.11 -6.03 ± 0.89 -4.97 ± 1.58 11.73 ± 0.34 0 50 100 150 200 Online Batch 0 20 40 60 80 100Accuracy (%) AETTA Acc. w\\o Recovery Acc. w\\ Recovery Reset 0 50 100 150 200 Online Batch 0 20 40 60 80 100Accuracy (%) DistShift Figure 6. An example of model recovery compared with DistShift. Reset points are marked over the x-axis. Results. Our simple recovery algorithm outperforms the baselines, including DistShift, which relies on an impractical assumption of knowing when the corruption changes. Episodic [37] showed high accuracy improvements under adaptation failures; however, it prevents continuous adaptation, even without adaptation failures. MRS [ 29] fails to recover among various TTA methods due to the hard-coded threshold of loss value. Stochastic [ 35] and FisherStochastic [ 3] show marginal improvements while failing to recover EATA. Our proposed reset algorithm successfully recovers from adaptation failures while minimizing the negative effect on TTA without failures. Qualitative Analysis. Figure 6 shows an example of our model recovery compared with DistShift. Notably, our re- covery algorithm resets only when an accuracy degradation trend is detected. On the other hand, DistShift failed to re- cover in the early steps since it resets the model only on distribution shifts. This implies that estimating performance degradation is more beneficial than knowing when the do- main changes to improve TTA performance. 6. Related Work Test-Time Adaptation. Recent progress in the field of test-time adaptation (TTA) has focused on improving model robustness [2, 11, 12, 28, 29, 35, 36] and addressing novel forms of domain shifts [11, 12, 35]. On the other hand, an analysis [30] pointed out the conventional TTA approaches remain prone to adaptation failures and demonstrated the importance of model recovery. In alignment with this insight, our work not only showcases the feasibility of accuracy estimation for TTA but also investigates a promising model recovery solution to enhance the robustness of TTA. Accuracy Estimation. Existing accuracy estimation ap- proaches mainly focus on the ensemble of multiple pre- trained models [1, 5, 15, 21, 27]. Accuracy-on-the-line [27] and Agreement-on-the-line [1] have demonstrated a notable linear relationship between performances in a wide range of models and distribution shifts, relying on the consistency of model predictions between in-distribution (ID) and out- of-distribution (OOD) data. The Difference of Confidence (DoC) [15] leverages differences in the model’s confidence between ID and OOD data to estimate the accuracy gap under distribution shifts for calculating the final OOD accu- racy. Self-training ensemble [5] estimates the accuracy of the pre-trained classifier by iteratively learning an ensemble of models with a training dataset, unlabeled test dataset, and wrongly classified samples. All these methods require la- beled ID data to estimate OOD accuracy. To our knowledge, no existing studies target the accuracy estimation in TTA where source data and labels are unavailable. 7. Conclusion We proposed a label-free TTA performance estimation method without access to source data and target labels. Based on the dropout inference sampling, we proposed calculating the prediction disagreement to estimate the TTA accuracy. We further improved the method with robust disagreement equality by utilizing the batch-aggregated distribution to pe- nalize skewed predictions. Our method outperformed the baselines in diverse scenarios and datasets. Finally, our case study of model recovery showed the practicality of accuracy estimation. Our findings suggest that accuracy estimation is not only feasible but also a valuable tool in advancing the field of TTA without the need for labeled data. Acknowledgements This work was supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0- 00495, On-Device V oice Phishing Call Detection). 8References [1] Christina Baek, Yiding Jiang, Aditi Raghunathan, and J. Zico Kolter. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. In Advances in Neural Information Processing Systems, pages 19274–19289. Curran Associates, Inc., 2022. 1, 2, 8 [2] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8344–8353, 2022. 1, 8 [3] Dhanajit Brahma and Piyush Rai. A probabilistic frame- work for lifelong test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3582–3591, 2023. 7, 8, 17, 24 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jef- frey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few- shot learners. In Advances in Neural Information Processing Systems, pages 1877–1901. Curran Associates, Inc., 2020. 1 [5] Jiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and estimating accuracy on unlabeled data with self-training ensembles. In Advances in Neural Information Processing Systems, pages 14980–14992. Curran Associates, Inc., 2021. 1, 2, 6, 8 [6] Ching-Yao Chuang, Antonio Torralba, and Stefanie Jegelka. Estimating generalization under distribution shifts via domain- invariant representations. In Proceedings of the 37th Interna- tional Conference on Machine Learning, pages 1984–1994. PMLR, 2020. 6 [7] Hady Elsahar and Matthias Gall´e. To annotate or not? predict- ing performance drop under domain shift. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2163–2173, 2019. 5, 6, 7, 14, 15, 18, 19, 20, 21, 22, 23 [8] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. 5, 16 [9] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learn- ing. In Proceedings of The 33rd International Conference on Machine Learning, pages 1050–1059, New York, New York, USA, 2016. PMLR. 2, 3 [10] Saurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lip- ton, Behnam Neyshabur, and Hanie Sedghi. Leveraging un- labeled data to predict out-of-distribution performance. In International Conference on Learning Representations, 2022. 6 [11] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test- time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 1, 7, 8, 16 [12] Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananu- rak, and Sung-Ju Lee. SoTTA: Robust test-time adaptation on noisy data streams. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 1, 2, 5, 7, 8, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 63(11):139–144, 2020. 1 [14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 15 [15] Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig Schmidt. Predicting with confidence on unseen distributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1134–1144, 2021. 1, 2, 8 [16] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, pages 1321–1330. PMLR, 2017. 6, 15 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 1, 5, 15, 16 [18] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations , 2019. 2, 5, 16 [19] Dan Hendrycks and Kevin Gimpel. A baseline for detect- ing misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Repre- sentations, 2017. 4 [20] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. 15 [21] Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of sgd via disagree- ment. In International Conference on Learning Representa- tions, 2021. 2, 3, 4, 5, 6, 7, 8, 11, 12, 14, 15, 18, 19, 20, 21, 22, 23 [22] Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. 16 [23] JoonHo Lee, Jae Oh Woo, Hankyu Moon, and Kwonho Lee. Unsupervised accuracy estimation of deep visual models us- ing domain-adaptive adversarial perturbation without source samples. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV) , pages 16443–16452, 2023. 5, 6, 7, 14, 15, 18, 19, 20, 21, 22, 23 [24] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. In Advances in 9Neural Information Processing Systems, pages 21464–21475. Curran Associates, Inc., 2020. 4 [25] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2017. 16 [26] TorchVision maintainers and contributors. Torchvision: Py- torch’s computer vision library. https://github.com/ pytorch/vision, 2016. 16 [27] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in- distribution generalization. In Proceedings of the 38th Inter- national Conference on Machine Learning, pages 7721–7735. PMLR, 2021. 1, 2, 8 [28] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning, pages 16888–16905. PMLR, 2022. 1, 2, 5, 8, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24 [29] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. 1, 2, 5, 7, 8, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24 [30] Ori Press, Steffen Schneider, Matthias K ¨ummerer, and Matthias Bethge. Rdumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty- seventh Conference on Neural Information Processing Sys- tems, 2023. 1, 7, 8 [31] Joaquin Qui ˜nonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. Mit Press, 2008. 2 [32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmenta- tion. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, pages 234–241, Cham, 2015. Springer International Publishing. 1 [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neu- ral Information Processing Systems. Curran Associates, Inc., 2017. 1 [34] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. 1, 2, 3, 5, 7, 8, 14, 16, 18, 19, 20, 21, 22, 23, 24 [35] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, 2022. 1, 2, 5, 7, 8, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24 [36] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test- time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15922–15932, 2023. 1, 2, 5, 8, 14, 16, 18, 19, 20, 21, 22, 23, 24 [37] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, pages 38629–38642. Curran Associates, Inc., 2022. 7, 8, 17, 24 10AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation Supplementary Material A. Proof of Theorems A.1. Proof of Theorem 3.1 We start expanding test error Err with few modifications from GDE [21]: Eh∼HA[ErrDT (h)] (14) ≜ EHA[EDT [1(h(X; Θ) ̸= Y )]] (15) = EDT [EHA[1(h(X; Θ) ̸= Y )] (exchanging expectations) (16) = EDT [1 − ˜hY (X)] (17) = K−1X k=0 Z x (1 − ˜hk(x)) p(X = x, Y= k)dx (by definition of expectation) (18) = Z q∈∆K K−1X k=0 Z x (1 − ˜hk(x)) p(X = x, Y= k, ˜h(X) = q)dxdq (introducing ˜h as a r.v.) (19) = Z q∈∆K K−1X k=0 Z x (1 − ˜hk(x)) p(Y = k, ˜h(X) = q)p(X = x|Y = k, ˜h(X) = q)dxdq (20) = Z q∈∆K K−1X k=0 p(Y = k, ˜h(X) = q) Z x (1 − ˜hk(x)| {z } =qk ) p(X = x|Y = k, ˜h(X) = q)dxdq (21) = Z q∈∆K K−1X k=0 p(Y = k, ˜h(X) = q) Z x (1 − qk)| {z } constant w.r.t. R x p(X = x|Y = k, ˜h(X) = q)dxdq (22) = Z q∈∆K K−1X k=0 p(Y = k, ˜h(X) = q)(1 − qk) Z x p(X = x|Y = k, ˜h(X) = q)dx | {z } =1 dq (23) = Z q∈∆K K−1X k=0 p(Y = k, ˜h(X) = q)(1 − qk)dq (24) = Z q∈[0,1] K−1X k=0 p(Y = k, ˜hk(X) = q)(1 − q)dq (refer [21]) (25) = Z q∈[0,1] K−1X k=0 p(Y = k|˜hk(X) = q)| {z } =q p(˜hk(X) = q)(1 − q)dq (26) = Z q∈[0,1] q(1 − q) K−1X k=0 p(˜hk(X) = q)dq. (confidence-prediction calibration) (27) 11Then, we expand the prediction disagreement with dropouts (PDD) from its definition: Eh∼HA[PDDDT (h)] (28) ≜ EHA  EDT   1 N NX i=1 1[h(X; Θ) ̸= h(X; Θdropouti)]     (29) = EDT  EHA   1 N NX i=1 1[h(X; Θ) ̸= h(X; Θdropouti)]     (exchanging expectations) (30) = EDT  EHA   1 N NX i=1 K−1X k=0 1[h(X; Θ) = k](1 − 1[h(X; Θdropouti) = k])     (31) = EDT   K−1X k=0 1 N NX i=1 EHA \u0002 1[h(X; Θ) = k](1 − 1[h(X; Θdropouti) = k]) \u0003   (32) = EDT   K−1X k=0 EHA \u0002 1[h(X; Θ) = k] \u0003 EHA[1 − 1 N NX i=1 1[h(X; Θdropouti) = k]]   (Dropout independence (Definition 3.1)) (33) = EDT   K−1X k=0 ˜hk(X)(1 − ˜hk(X))   (34) = Z x K−1X k=0 ˜hk(x)(1 − ˜hk(x))p(X = x)dx (by definition of expectation) (35) = Z q∈∆K Z x K−1X k=0 ˜hk(x)(1 − ˜hk(x))p \u0010 X = x, ˜h(X) = q \u0011 dxdq (introducing ˜h as a r.v.) (36) = Z q∈∆K p(˜h(X) = q) Z x K−1X k=0 ˜hk(x)(1 − ˜hk(x))| {z } ˜hk(x)=qk p \u0010 X = x|˜h(X) = q \u0011 dxdq (37) = Z q∈∆K p(˜h(X) = q) Z x K−1X k=0|{z} bring to the front qk(1 − qk)p(X = x|˜h(X) = q)dxdq (38) = K−1X k=0 Z q∈∆K p(˜h(X) = q) Z x qk(1 − qk)| {z } constant w.r.t. R x p(X = x|˜h(X) = q)dxdq (39) = K−1X k=0 Z q∈∆K | {z } swap p(˜h(X) = q)qk(1 − qk) Z x p(X = x|˜h(X) = q)dx | {z } =1 dq (40) = Z q∈∆K K−1X k=0 qk(1 − qk)p(˜h(X) = q)dq (41) = Z q∈[0,1] q(1 − q) K−1X k=0 p(˜hk(X) = q)dq. (refer [21]) (42) 12Equation 27 is equivalent to Equation 42: Eh∼HA[ErrDT (h)] = Eh∼HA[PDDDT (h)], (43) which concludes the proof of Theorem 3.1. A.2. Proof of Theorem 3.2 From robust confidence-prediction calibration, the over-confident model’s conditional probability of the major class k′ is scaled by a, while other classes’ conditional probabilities are equally scaled up by b. Then, Equation 27 now becomes: Eh∼HA[ErrDT (h)] (44) = Z q∈[0,1] K−1X k=0 p(Y = k|˜hk(X) = q)p(˜hk(X) = q)(1 − q)dq (45) = Z q∈[0,1] p(Y = k′|˜hk′(X) = q)p(˜hk′(X) = q)(1 − q) + X k̸=k′ p(Y = k|˜hk(X) = q)p(˜hk(X) = q)(1 − q)dq (46) = Z q∈[0,1] aq p(˜hk′(X) = q)(1 − q) + X k̸=k′ bq p(˜hk(X) = q)(1 − q)dq (robust confidence-prediction calibration) (47) = Z q∈[0,1] aq(1 − q) p(˜hk′(X) = q)dq + b Z q∈[0,1] X k̸=k′ q(1 − q) p(˜hk(X) = q)dq. (48) We rewrite Equation 48 as: Z q∈[0,1] X k̸=k′ q(1 − q) p(˜hk(X) = q)dq = 1 b Eh∼HA[ErrDT (h)] − Z q∈[0,1] a b q(1 − q) p(˜hk′(X) = q)dq. (49) Then, we rewrite PDD (Equation 42): Eh∼HA[PDDDT (h)] (50) = Z q∈[0,1] q(1 − q) K−1X k=0 p(˜hk(X) = q)dq (51) = Z q∈[0,1] q(1 − q) p(˜hk′(X) = q) + q(1 − q) X k̸=k′ p(˜hk(X) = q)dq (52) = Z q∈[0,1] q(1 − q) p(˜hk′(X) = q)dq + Z q∈[0,1] q(1 − q) X k̸=k′ p(˜hk(X) = q)dq (53) = Z q∈[0,1] b − a b q(1 − q) p(˜hk′(X) = q)dq + 1 b Eh∼HA[ErrDT (h)]. (Equation 49) (54) Finally, we obtain the equality between Err and PDD: Eh∼HA[ErrDT (h)] (55) = b Eh∼HA[PDDDT (h)] − Z q∈[0,1] (b − a) q(1 − q) p(˜hk′(X) = q)dq, (56) which concludes the proof of Theorem 3.2. Note that without weighting (a = b = 1), the result is identical to Theorem 3.1. 13B. Additional Experiments GDE with multiple pre-trained models. We compare AETTA with the original version of GDE (denoted as GDE*), utilizing multiple pre-trained models with access to training data. We report the result in Table 4. Due to the misalignment of confidence-prediction calibration, GDE* underperforms AETTA even with full access to source data. Table 4. Mean absolute error (MAE) (%) of the accuracy estimation on continual CIFAR100-C. TTA Method Dataset Method TENT [34] EATA [28] SAR [29] CoTTA [35] RoTTA [36] SoTTA [12] Avg. (↓) GDE* [21] 14.54 ± 8.14 4.11 ± 2.43 7.27 ± 0.16 9.89 ± 0.29 7.44 ± 0.13 5.79 ± 0.23 8.17 ± 0.87Continual CIFAR100-C AETTA 5.85 ± 0.36 4.18 ± 0.82 6.67 ± 0.12 6.55 ± 0.17 5.86 ± 0.10 5.32 ± 0.18 5.74 ± 0.13 ImageNet-R. To demonstrate the dataset generality of AETTA, we report the accuracy estimation result on ResNet18 architecture on ImageNet-R (Table 5). AETTA outperformed the baselines in all TTA methods, showing AETTA is applicable in various datasets (e.g., CIFAR10-C, CIFAR100-C, ImageNet-C, and ImageNet-R). Table 5. Mean absolute error (MAE) (%) of the accuracy estimation on ResNet18 on ImageNet-R. Bold numbers are the lowest error. Averaged over three different random seeds. TTA Method Dataset Method TENT [34] EATA [28] SAR [29] CoTTA [35] RoTTA [36] SoTTA [12] Avg. (↓) SrcValid 37.00 ± 0.14 37.91 ± 0.18 36.58 ± 0.24 35.05 ± 0.16 34.43 ± 0.05 37.82 ± 0.41 36.46 ± 0.05 SoftmaxScore [7] 10.79 ± 0.17 13.87 ± 0.09 15.02 ± 0.14 14.76 ± 0.07 14.08 ± 0.04 12.25 ± 0.42 13.46 ± 0.06 GDE [21] 62.81 ± 0.12 61.36 ± 0.14 63.27 ± 0.18 64.86 ± 0.10 62.64 ± 0.12 55.23 ± 0.29 61.70 ± 0.02 AdvPerturb [23] 13.42 ± 0.28 16.04 ± 0.36 17.90 ± 0.28 21.19 ± 0.22 31.12 ± 0.12 9.91 ± 0.73 18.26 ± 0.03 ImageNet-R AETTA 8.02 ± 0.12 6.87 ± 0.08 7.06 ± 0.19 7.07 ± 0.11 8.63 ± 0.19 6.79 ± 0.29 7.41 ± 0.05 ResNet50. To demonstrate the model generality of AETTA, we report the accuracy estimation result on ResNet50 architecture on ImageNet-C (Table 6). AETTA outperformed the baselines in general, showing AETTA is applicable to diverse model architectures. Table 6. Mean absolute error (MAE) (%) of the accuracy estimation on ResNet50 on ImageNet-C. Bold numbers are the lowest error. Averaged over three different random seeds for 15 types of corruption. TTA Method Dataset Method TENT [34] EATA [28] SAR [29] CoTTA [35] RoTTA [36] SoTTA [12] Avg. (↓) SrcValid 46.46 ± 0.15 34.19 ± 0.67 30.35 ± 0.75 46.47 ± 0.17 12.28 ± 0.11 19.28 ± 0.18 31.50 ± 0.26 SoftmaxScore [7] 23.58 ± 0.03 24.14 ± 0.04 26.22 ± 0.05 23.57 ± 0.04 24.32 ± 0.02 17.87 ± 0.24 23.28 ± 0.03 GDE [21] 68.39 ± 0.04 57.08 ± 0.08 55.81 ± 0.06 68.36 ± 0.04 58.69 ± 0.09 48.15 ± 0.33 59.41 ± 0.04 AdvPerturb [23] 12.77 ± 0.04 21.16 ± 0.05 23.66 ± 0.08 12.77 ± 0.05 16.44 ± 0.00 25.28 ± 0.32 18.68 ± 0.05 Fully ImageNet-C AETTA 6.14 ± 0.05 9.15 ± 0.03 8.50 ± 0.07 6.15 ± 0.04 28.28 ± 0.03 36.90 ± 0.36 15.85 ± 0.09 SrcValid 46.38 ± 0.10 35.83 ± 0.74 24.35 ± 1.86 46.46 ± 0.22 13.79 ± 0.16 5.12 ± 0.29 28.65 ± 0.46 SoftmaxScore [7] 23.58 ± 0.03 21.34 ± 0.06 16.64 ± 0.25 23.61 ± 0.01 19.99 ± 0.25 51.60 ± 0.75 26.13 ± 0.12 GDE [21] 68.36 ± 0.03 58.41 ± 0.14 60.20 ± 0.24 68.38 ± 0.01 68.98 ± 0.52 86.08 ± 0.36 68.40 ± 0.09 AdvPerturb [23] 12.80 ± 0.04 19.82 ± 0.12 21.50 ± 0.14 12.77 ± 0.02 13.77 ± 0.35 4.79 ± 0.17 14.24 ± 0.07 Continual ImageNet-C AETTA 6.15 ± 0.05 10.81 ± 0.01 6.41 ± 0.08 6.00 ± 0.04 14.90 ± 0.30 4.21 ± 0.12 8.08 ± 0.04 14C. Discussion Potential Societal Impact. The computational overheads associated with test-time adaptation (TTA) could raise environ- mental concerns, particularly regarding carbon emissions. Our algorithm introduces N extra model inferences for accuracy estimation. Importantly, our approach of utilizing dropout inference is computationally lightweight compared to baseline methods involving model retraining [21] and adversarial backpropagation [23]. Recent advancements, such as the memory- economic TTA [20], are anticipated to tackle these challenges effectively. This implies that, despite the computational demands, the environmental impact of our approach could be mitigated by integrating emerging strategies for resource-efficient TTA implementations. Limitations and Future Directions. Our research investigates the possibility of accuracy estimation for TTA with only unlabeled data. A promising direction for further improvements is the (1) optimization of the weighting constant b (or corresponding a), which stands to fine-tune the calibration process, and (2) estimation of the variable C for more precise error estimates. Also, we presented a case study on model recovery to demonstrate the practicality of accuracy estimation. While we chose a heuristic method to reset the model for the simplicity of analysis, there exists room for improvement to be more effective. Beyond model recovery, we also envision the potential of accuracy estimation in broader applications, such as model refinement and maintenance processes, and enhancing the dynamics of human-AI interactions, which we leave as future work. D. Experiment Details We conducted all experiments under three random seeds (0, 1, 2) and reported the average values with standard deviations. The experiments were performed on NVIDIA GeForce RTX 3090 and NVIDIA TITAN RTX GPUs. D.1. Accuracy Estimation Details AETTA (Ours). We used the number of dropout inference samples N = 10 and prediction disagreement weighting hyperparameter α = 3 for all experiments. The maximum entropy for the model Emax is calculated as Emax = Ent(⃗1K/K) where K is a number of classes and ⃗1K is one vector with size K; which results in 2.3, 4.6, and 6.9 for 10, 100, and 1,000 classes. We applied the Dropout module for each residual block layer in ResNet18 [17], where the dropout rate is 0.4, 0.3, and 0.2 for 10, 100, and 1,000 classes, following previous studies which apply different hyperparameters for different numbers of classes [12, 28, 35]. SrcValid. For SrcValid, we used labeled source-domain validation data and calculated the accuracy. We used 1,000 random samples from the validation set of the source dataset. SoftmaxScore. For SoftmaxScore [7], we utilized the average softmax score for the current test batch as the estimated accuracy. We additionally applied temperature scaling [16] with temperature value T = 2, which showed the best estimation performance on CIFAR10-C, to enhance the estimation performance. GDE. For generalization disagreement equality (GDE) [ 21], we calculated the (dis)agreement rate between predictions of the test batch over a pair of models. Unlike the setting in domain adaptation of utilizing multiple pre-trained models, we utilized the models in different adaptation stages. Specifically, we compared the two models: (1) the currently adapted model and (2) the previous model right before the adaptation. This follows the suggestion that utilizing only two models is sufficient to calculate disagreement [21]. AdvPerturb. Adversarial perturbation [23] estimates the source model accuracy by calculating the agreement between the domain-adapted and source models by applying adversarial perturbation on the source model side. In the TTA setting, we compared the test-time-adapted model with the source model and applied the FGSM [14] adversarial attack with attack size following the original paper (ϵ = 1/255). D.2. TTA Method Details In this study, we followed the official implementation of TTA methods. To maintain consistency, we adopted the optimal hyperparameters reported in the corresponding papers or source code repositories. We also provide additional implementation details and the use of hyperparameters if not specified in the original paper or the source code. 15TENT. For TENT [34], we configured the learning rate as LR = 0.001 for CIFAR10-C/CIFAR100-C andLR = 0.00025 for ImageNet-C, aligning with the guidelines outlined in the original paper. The implementation followed the official code.3 EATA. For EATA [28], we followed the original configuration of LR = 0.005/0.005/0.00025 for CIFAR10-C/CIFAR100- C/ImageNet-C, entropy constant E0 = 0.4 × ln K, where K represents the number of classes. Additionally, we set the cosine sample similarity threshold ϵ = 0.4/0.4/0.05, trade-off parameter β = 1/1/2, 000, and moving average factor α = 0.1. The Fisher importance calculation involved 2,000 samples, as recommended. The implementation followed the official code.4 SAR. For SAR [29], we selected a batch size 64 for fair comparisons. We set a learning rate of LR = 0.00025, sharpness threshold ρ = 0.5, and entropy threshold E0 = 0.4 × lnK, following the recommendations from the original paper. The top layer (layer 4 for ResNet18) was frozen, consistent with the original paper. The implementation followed the official code.5 CoTTA. For CoTTA [35], we set the restoration factor p = 0.01, and exponential moving average (EMA) factor α = 0.999. For augmentation confidence threshold pth, we followed the authors’ guidelines as pth = 0.92 for CIFAR10-C, pth = 0.72 for CIFAR100-C, and pth = 0.1 for ImageNet-C. The implementation followed the official code.6 RoTTA. For RoTTA [36], we utilized the Adam optimizer [ 22] with a learning rate of LR = 0.001 and β = 0.9. We followed the original hyperparameters, including BN-statistic exponential moving average updating rateα = 0.05, Teacher model’s exponential moving average updating rate ν = 0.001, timeliness parameter λt = 1.0, and uncertainty parameter λu = 1.0. The implementation followed the original code.7 SoTTA. For SoTTA [12], the Adam optimizer [22] was employed, featuring a BN momentum ofm = 0.2 and a learning rate of LR = 0.001 with a single adaptation epoch. The memory size was set to 64, with the confidence threshold C0 configured as 0.99 for CIFAR10-C (10 classes), 0.66 for CIFAR100-C (100 classes), and 0.33 for ImageNet-C (1,000 classes). The entropy-sharpness L2-norm constraint ρ was set to 0.5, aligning with the suggestion [8]. The top layer was frozen following the original paper. The implementation followed the original code.8 D.3. Experiment Setting Details Datasets. CIFAR10-C/CIFAR100-C/ImageNet-C [18] are the most widely used benchmarks for test-time adaptation (TTA) [11, 12, 28, 29, 34–36]. All datasets contain 15 corruption types, including Gaussian, Snow, Frost, Fog, Brightness, Contrast, Elastic Transformation, Pixelate, and JPEG Compression. Each corruption is applied in 5 levels of severity, where we adopt the highest severity level of 5. CIFAR10-C and CIFAR100-C consist of 50,000 train images and 10,000 test images for 10 and 100 classes. ImageNet-C consists of 1,281,167 train images and 50,000 test images for 1,000 classes. Pre-Training. We employed the ResNet18 [ 17] as the backbone network. The model is trained for each CIFAR10- C/CIFAR100-C/ImageNet-C on the training dataset. For CIFAR10-C/CIFAR100-C, we utilized the stochastic gradient descent with a batch size of 128, a learning rate of 0.1, and a momentum of 0.9, with cosine annealing learning rate scheduling [25] for 200 epochs. For ImageNet-C, we utilized the pre-trained model from TorchVision [26]. Test-Time Adaptation. For the fully TTA, each TTA method adapts to one corruption at a time. For thecontinual TTA, each TTA method continually adapts to 15 corruptions in the predefined order of [Gaussian, Snow, Frost, Fog, Brightness, Contrast, Elastic Transformation, Pixelate, and JPEG Compression], following the previous study [35]. For all experiments, we use the batch size of 64, with memory size 64 for RoTTA [36] and SoTTA [12] for a fair comparison. 3https://github.com/DequanWang/tent 4https://github.com/mr-eggplant/EATA 5https://github.com/mr-eggplant/SAR 6https://github.com/qinenergy/cotta 7https://github.com/BIT-DA/RoTTA 8https://github.com/taeckyung/sotta 16D.4. Model Recovery Details (Section 5) AETTA (Ours). With AETTA, our reset algorithm detects two cases: (1) consecutive low accuracies and (2) sudden accuracy drops. For consecutive low accuracies, we utilize the information of estimated accuracy from each 5 batches. Regarding hard lower-bound thresholding, we employ a threshold value of 0.2. We reset both the model’s weights to those from the source model and the optimizer’s state to its initialization value. Episodic. Episodic resetting was first introduced by MEMO [37], where the model resets after every batch. We reset both the model’s weights and the optimizer’s state to its value before adaptation. MRS. The Model Recovery Scheme (MRS) was initially introduced by SAR [ 29] to recover the model from collapsing. The reset occurs when the moving average of entropy loss falls below a certain threshold. We utilized the threshold value of 0.2 introduced in the original paper. We reset both the model’s weights to those from the source model and the optimizer’s state to its initialization value. Stochastic. Stochastic restoration was first introduced by CoTTA [ 35]. A small number of model weights are stochastically restored to the initial weights of the source model, with a certain probability specified by the restoration factor. We use the restoration factor 0.01, as introduced in the original work. FisherStochastic. Fisher information based restoration was proposed by PETAL [3], based on the stochastic restoration [35]. It applies stochastic restoration based on layer importance measured by the Fisher information matrix (FIM). We use an FIM-based parameter restoration quantile value of 0.03 for CIFAR100-C, as recommended in the original paper. The parameter with an FIM value less than 0.03-quantile would be restored to the original source weight. DistShift. DistShift assumes that the model knows when the distribution changes and thus acts as an oracle. Resetting occurs when the test data distribution (corruption) changes. We reset both the model’s weights to those from the source model and the optimizer’s state to its initialization value. E. License of Assets Datasets. CIFAR10/CIFAR100 (MIT License), CIFAR10-C/CIFAR100-C (Creative Commons Attribution 4.0 International) and ImageNet-C (Apache 2.0). Codes. Torchvision for ResNet18 and ResNet50 (Apache 2.0), the official repository of TENT (MIT License), the official repository of EATA (MIT License), the official repository of SAR (BSD 3-Clause License), the official repository of CoTTA (MIT License), the official repository of RoTTA (MIT License), and the official repository of SoTTA (MIT License). 17F. Result Details We report the detailed results per corruption in the main experiments. Table 1 in the main paper is detailed in Table 7, Table 8, and Table 9. Table 2 in the main paper is detailed in Table 10, Table 11, and Table 12. Table 3 in the main paper is detailed in Table 13. Table 7. Mean absolute error (MAE) (%) of the accuracy estimation on fully CIFAR10-C. Averaged over three different random seeds. Noise Blur Weather Digital TTA Method Acc. Estimation Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↓) SrcValid 24.85 ± 0.83 21.82 ± 0.74 32.41 ± 0.90 11.60 ± 0.53 32.49 ± 1.50 12.78 ± 0.45 11.16 ± 0.62 15.90 ± 0.57 17.90 ± 1.04 13.12 ± 0.78 8.46 ± 0.35 12.62 ± 0.33 21.53 ± 1.26 16.35 ± 0.68 22.57 ± 1.26 18.37 ± 0.29 SoftmaxScore [7] 8.40 ± 0.10 6.57 ± 0.91 12.95 ± 1.53 3.96 ± 0.50 14.30 ± 2.12 3.76 ± 0.12 3.52 ± 0.36 4.40 ± 0.08 6.20 ± 0.94 4.08 ± 0.50 3.13 ± 0.08 4.06 ± 0.36 6.62 ± 1.00 4.63 ± 0.47 7.32 ± 1.40 6.26 ± 0.49 GDE [21] 25.29 ± 0.67 22.16 ± 0.78 32.75 ± 1.04 12.07 ± 0.92 33.26 ± 1.83 12.78 ± 0.55 11.25 ± 0.84 16.02 ± 0.55 18.57 ± 1.07 13.56 ± 1.14 8.81 ± 0.38 12.72 ± 0.25 21.58 ± 1.23 16.64 ± 1.03 22.97 ± 1.34 18.69 ± 0.28 AdvPerturb [23] 48.04 ± 2.26 44.69 ± 3.91 41.20 ± 5.55 28.88 ± 3.38 10.96 ± 2.02 18.75 ± 2.07 22.50 ± 1.34 5.56 ± 0.43 14.27 ± 2.42 10.58 ± 2.22 2.48 ± 0.08 52.38 ± 1.67 4.73 ± 0.25 35.74 ± 1.12 5.20 ± 0.10 23.06 ± 1.17 TENT [34] AETTA 4.12 ± 0.45 4.67 ± 0.42 4.59 ± 0.45 3.14 ± 0.11 7.05 ± 0.49 3.20 ± 0.03 3.07 ± 0.35 3.66 ± 0.11 4.07 ± 0.58 3.32 ± 0.26 2.70 ± 0.05 4.29 ± 0.19 4.01 ± 0.38 3.69 ± 0.13 4.41 ± 0.35 4.00 ± 0.03 SrcValid 18.53 ± 0.43 16.06 ± 1.16 24.04 ± 1.75 10.55 ± 0.25 23.95 ± 2.07 11.72 ± 0.61 10.26 ± 0.11 12.81 ± 0.31 12.85 ± 0.48 10.27 ± 0.36 7.78 ± 0.32 8.88 ± 0.82 19.00 ± 1.34 12.62 ± 0.56 16.18 ± 1.01 14.37 ± 0.33 SoftmaxScore [7] 4.75 ± 0.62 3.85 ± 0.37 9.08 ± 0.24 4.14 ± 0.41 8.77 ± 2.09 3.92 ± 0.19 4.44 ± 0.42 3.50 ± 0.30 3.54 ± 0.39 4.02 ± 0.24 4.79 ± 0.48 4.22 ± 0.47 4.38 ± 0.40 3.58 ± 0.37 4.70 ± 0.42 4.78 ± 0.12 GDE [21] 22.88 ± 0.72 20.42 ± 0.72 30.68 ± 0.46 11.07 ± 0.19 30.04 ± 2.50 12.58 ± 0.56 10.78 ± 0.38 14.99 ± 0.28 14.52 ± 0.56 11.48 ± 0.13 8.27 ± 0.46 9.32 ± 0.48 21.01 ± 0.92 14.85 ± 0.11 21.31 ± 0.97 16.95 ± 0.22 AdvPerturb [23] 50.21 ± 4.36 45.60 ± 3.84 44.13 ± 4.72 31.34 ± 2.85 16.32 ± 0.62 19.30 ± 1.78 22.95 ± 2.10 6.43 ± 0.19 17.52 ± 3.03 13.62 ± 0.85 2.73 ± 0.15 56.00 ± 1.18 4.93 ± 0.06 37.56 ± 0.69 5.99 ± 0.70 24.97 ± 1.00 EATA [28] AETTA 3.86 ± 0.28 4.09 ± 0.25 5.56 ± 0.77 3.07 ± 0.23 6.88 ± 1.05 3.42 ± 0.20 3.07 ± 0.29 3.25 ± 0.03 3.48 ± 0.12 3.31 ± 0.33 2.73 ± 0.11 2.76 ± 0.23 4.45 ± 0.10 3.34 ± 0.35 4.72 ± 0.75 3.87 ± 0.14 SrcValid 32.05 ± 1.03 30.47 ± 0.81 37.29 ± 0.79 12.22 ± 0.20 33.83 ± 0.43 13.71 ± 0.10 12.62 ± 0.36 18.37 ± 0.36 19.73 ± 0.52 14.61 ± 0.57 9.26 ± 0.24 13.12 ± 0.43 23.28 ± 0.20 20.67 ± 0.02 28.02 ± 0.54 21.28 ± 0.27 SoftmaxScore [7] 4.21 ± 0.33 4.08 ± 0.17 5.52 ± 0.59 6.28 ± 0.31 4.89 ± 0.20 5.92 ± 0.26 6.49 ± 0.52 4.85 ± 0.27 4.86 ± 0.29 5.80 ± 0.59 7.11 ± 0.48 5.34 ± 0.30 4.26 ± 0.24 4.55 ± 0.09 3.94 ± 0.11 5.21 ± 0.22 GDE [21] 31.88 ± 1.08 30.38 ± 0.85 37.17 ± 0.78 12.22 ± 0.20 33.72 ± 0.46 13.71 ± 0.10 12.62 ± 0.36 18.37 ± 0.36 19.73 ± 0.52 14.61 ± 0.57 9.26 ± 0.24 13.12 ± 0.43 23.28 ± 0.20 20.67 ± 0.02 27.99 ± 0.55 21.25 ± 0.27 AdvPerturb [23] 42.25 ± 2.34 37.73 ± 2.89 38.52 ± 4.53 30.55 ± 3.11 9.85 ± 1.85 18.17 ± 1.49 21.66 ± 2.37 4.60 ± 0.60 14.48 ± 2.98 11.73 ± 0.52 2.71 ± 0.14 52.81 ± 2.08 4.92 ± 0.04 31.36 ± 0.89 6.98 ± 0.44 21.89 ± 0.95 SAR [29] AETTA 4.91 ± 0.84 5.15 ± 0.89 4.75 ± 0.19 2.92 ± 0.14 5.42 ± 0.47 3.09 ± 0.06 3.18 ± 0.21 3.55 ± 0.37 3.65 ± 0.17 3.25 ± 0.34 2.81 ± 0.06 3.58 ± 0.61 3.87 ± 0.15 3.69 ± 0.43 4.47 ± 0.23 3.89 ± 0.07 SrcValid 23.70 ± 0.75 21.84 ± 0.28 28.79 ± 0.21 12.46 ± 0.33 29.57 ± 0.62 13.92 ± 0.06 12.75 ± 0.25 17.30 ± 0.41 17.21 ± 0.30 14.75 ± 0.51 9.26 ± 0.21 15.14 ± 0.06 21.29 ± 0.43 17.69 ± 0.14 20.78 ± 0.38 18.43 ± 0.16 SoftmaxScore [7] 16.82 ± 0.51 17.21 ± 0.63 16.33 ± 0.22 6.71 ± 0.42 12.30 ± 0.19 7.02 ± 0.43 7.30 ± 0.71 9.69 ± 0.48 12.00 ± 0.63 7.54 ± 0.45 7.18 ± 0.56 7.90 ± 0.61 12.01 ± 0.38 11.76 ± 0.74 12.69 ± 0.63 10.96 ± 0.28 GDE [21] 15.65 ± 0.77 14.29 ± 0.35 19.35 ± 0.33 12.08 ± 0.17 21.18 ± 0.52 13.08 ± 0.16 12.20 ± 0.07 14.43 ± 0.15 13.44 ± 0.17 13.60 ± 0.49 9.20 ± 0.16 13.16 ± 0.30 16.40 ± 0.06 13.91 ± 0.58 15.46 ± 0.45 14.50 ± 0.03 AdvPerturb [23] 16.79 ± 0.32 15.09 ± 0.89 18.84 ± 3.83 31.44 ± 3.23 6.81 ± 0.51 20.83 ± 2.16 23.18 ± 2.46 6.05 ± 0.33 11.83 ± 1.56 17.25 ± 1.20 2.64 ± 0.06 55.25 ± 1.82 14.63 ± 1.63 21.96 ± 1.58 7.41 ± 0.68 18.00 ± 0.82 CoTTA [35] AETTA 15.34 ± 1.06 15.26 ± 1.54 14.98 ± 0.87 3.02 ± 0.06 6.45 ± 0.60 3.22 ± 0.17 3.20 ± 0.28 4.11 ± 0.24 5.57 ± 0.48 3.48 ± 0.39 2.79 ± 0.05 4.24 ± 0.10 5.56 ± 0.63 6.10 ± 0.92 9.18 ± 0.89 6.83 ± 0.47 SrcValid 27.12 ± 7.07 27.75 ± 6.21 14.88 ± 3.34 12.12 ± 3.16 25.35 ± 1.09 5.02 ± 0.23 4.88 ± 0.72 14.33 ± 3.02 36.52 ± 1.99 11.62 ± 1.44 35.55 ± 2.35 35.93 ± 0.82 20.00 ± 0.14 7.96 ± 0.72 26.23 ± 0.98 20.35 ± 1.31 SoftmaxScore [7] 4.68 ± 0.46 4.64 ± 0.15 5.19 ± 0.31 7.29 ± 0.50 4.77 ± 0.27 7.12 ± 0.50 7.73 ± 0.65 6.49 ± 0.31 6.28 ± 0.47 7.32 ± 0.76 8.40 ± 0.59 4.71 ± 0.47 5.25 ± 0.04 5.89 ± 0.43 4.39 ± 0.14 6.01 ± 0.23 GDE [21] 32.94 ± 0.75 30.87 ± 0.91 39.40 ± 0.72 12.02 ± 0.22 34.18 ± 0.90 13.48 ± 0.48 12.01 ± 0.26 17.90 ± 0.80 21.73 ± 0.82 13.93 ± 0.51 8.90 ± 0.42 40.52 ± 2.50 22.68 ± 0.44 21.22 ± 0.37 27.31 ± 0.69 23.27 ± 0.43 AdvPerturb [23] 40.38 ± 2.57 36.59 ± 2.88 35.02 ± 3.67 29.64 ± 2.94 9.29 ± 1.72 17.31 ± 1.01 21.45 ± 2.59 4.81 ± 0.35 11.96 ± 2.41 11.24 ± 0.41 2.70 ± 0.09 26.49 ± 4.84 5.42 ± 0.24 29.92 ± 0.45 8.06 ± 0.59 19.35 ± 0.99 RoTTA [36] AETTA 13.47 ± 5.78 13.35 ± 5.69 9.74 ± 3.87 3.55 ± 0.44 5.42 ± 1.13 3.68 ± 0.27 3.88 ± 0.43 4.45 ± 0.93 4.93 ± 1.02 4.51 ± 0.62 3.31 ± 0.33 12.13 ± 6.65 4.66 ± 1.17 4.97 ± 1.13 4.57 ± 0.24 6.44 ± 1.35 SrcValid 11.98 ± 4.00 10.86 ± 0.74 8.25 ± 0.47 9.73 ± 2.10 23.16 ± 0.58 4.54 ± 0.68 4.74 ± 1.04 5.55 ± 0.66 16.12 ± 2.74 4.56 ± 0.17 13.62 ± 2.37 38.68 ± 6.48 19.00 ± 0.45 5.57 ± 0.67 20.67 ± 0.57 13.13 ± 0.85 SoftmaxScore [7] 4.10 ± 0.13 4.46 ± 0.27 4.50 ± 0.17 5.45 ± 1.04 5.05 ± 0.65 5.47 ± 0.69 6.14 ± 0.62 4.82 ± 0.53 4.91 ± 0.97 5.61 ± 0.82 6.17 ± 1.00 4.36 ± 0.85 4.23 ± 0.41 5.25 ± 0.70 4.07 ± 0.26 4.97 ± 0.50 GDE [21] 23.46 ± 0.77 20.15 ± 0.46 29.27 ± 0.45 10.60 ± 0.38 28.84 ± 0.86 11.03 ± 0.11 10.00 ± 0.56 13.53 ± 0.58 14.42 ± 0.16 10.67 ± 0.40 7.09 ± 0.09 13.81 ± 1.96 19.08 ± 0.52 14.25 ± 0.50 20.51 ± 0.82 16.45 ± 0.21 AdvPerturb [23] 47.94 ± 3.36 43.98 ± 3.57 44.74 ± 4.05 30.14 ± 2.33 12.47 ± 2.81 18.81 ± 1.68 22.85 ± 2.61 5.48 ± 0.77 16.01 ± 2.86 12.78 ± 1.13 2.68 ± 0.20 49.76 ± 2.32 4.95 ± 0.33 37.03 ± 1.34 5.63 ± 0.43 23.68 ± 0.85 SoTTA [12] AETTA 9.08 ± 2.79 9.51 ± 2.49 9.23 ± 2.73 3.58 ± 0.27 5.01 ± 0.52 3.63 ± 0.45 3.77 ± 0.57 3.86 ± 0.42 4.99 ± 1.17 4.24 ± 0.90 3.05 ± 0.07 5.15 ± 1.28 4.33 ± 0.63 5.43 ± 1.39 4.41 ± 0.51 5.28 ± 0.87 18Table 8. Mean absolute error (MAE) (%) of the accuracy estimation on fully CIFAR100-C. Averaged over three different random seeds. Noise Blur Weather Digital TTA Method Acc. Estimation Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↓) SrcValid 46.38 ± 1.17 45.01 ± 1.16 51.81 ± 1.79 31.42 ± 0.67 49.43 ± 0.58 33.51 ± 0.64 31.37 ± 0.35 39.05 ± 0.33 38.89 ± 0.28 34.25 ± 0.38 28.72 ± 0.33 33.04 ± 0.61 41.81 ± 0.75 35.52 ± 0.51 44.24 ± 0.66 38.96 ± 0.22 SoftmaxScore [7] 13.70 ± 0.41 14.53 ± 0.58 11.33 ± 0.70 20.57 ± 0.40 13.53 ± 0.49 19.91 ± 0.60 21.14 ± 0.25 17.30 ± 0.27 17.10 ± 0.23 18.95 ± 0.23 21.15 ± 0.12 17.26 ± 0.66 18.07 ± 0.57 19.69 ± 0.13 15.89 ± 0.14 17.34 ± 0.10 GDE [21] 49.21 ± 0.79 47.38 ± 0.87 54.91 ± 0.53 31.71 ± 0.36 50.52 ± 0.75 33.61 ± 0.70 31.61 ± 0.37 40.02 ± 0.45 40.32 ± 0.13 36.41 ± 0.33 28.89 ± 0.17 32.74 ± 0.61 42.20 ± 0.70 36.20 ± 0.11 45.87 ± 0.35 40.11 ± 0.05 AdvPerturb [23] 36.92 ± 1.76 38.54 ± 0.24 35.93 ± 0.49 31.08 ± 0.42 26.47 ± 1.53 18.05 ± 0.82 24.02 ± 0.77 8.33 ± 0.53 21.92 ± 1.07 19.26 ± 0.03 4.58 ± 0.38 48.38 ± 0.25 5.43 ± 0.10 37.17 ± 2.42 6.42 ± 0.25 24.17 ± 0.41 TENT [34] AETTA 6.55 ± 0.59 6.09 ± 0.17 7.60 ± 0.31 5.57 ± 0.18 10.27 ± 0.36 6.05 ± 0.37 5.60 ± 0.53 8.26 ± 0.09 7.21 ± 0.03 5.81 ± 0.09 5.54 ± 0.28 7.32 ± 0.56 6.99 ± 0.66 5.50 ± 0.23 8.96 ± 0.08 6.89 ± 0.15 SrcValid 7.65 ± 0.94 7.33 ± 0.33 7.51 ± 0.42 15.32 ± 1.91 8.35 ± 0.45 13.56 ± 1.08 12.54 ± 1.32 9.21 ± 1.68 8.91 ± 0.48 9.31 ± 0.73 17.52 ± 1.75 15.60 ± 0.77 9.99 ± 1.31 9.80 ± 0.28 8.09 ± 0.44 10.71 ± 0.31 SoftmaxScore [7] 36.65 ± 1.55 35.32 ± 1.08 40.06 ± 2.16 15.38 ± 1.48 35.65 ± 2.20 19.64 ± 2.12 20.05 ± 5.94 27.09 ± 4.80 31.54 ± 4.28 24.67 ± 4.45 15.80 ± 2.67 25.68 ± 7.41 31.88 ± 2.77 26.64 ± 2.28 31.81 ± 0.85 27.86 ± 1.11 GDE [21] 83.95 ± 1.83 83.36 ± 1.31 88.21 ± 0.66 55.73 ± 3.68 84.37 ± 1.49 62.93 ± 2.76 60.31 ± 7.35 73.24 ± 4.38 77.51 ± 2.86 70.26 ± 4.24 40.71 ± 3.37 62.07 ± 9.58 79.02 ± 2.84 71.39 ± 1.01 79.95 ± 1.07 71.53 ± 2.12 AdvPerturb [23] 9.32 ± 0.83 8.30 ± 0.59 5.08 ± 0.16 16.05 ± 1.45 5.75 ± 0.87 7.72 ± 0.40 10.48 ± 1.96 4.03 ± 0.44 6.07 ± 1.16 6.79 ± 1.07 3.90 ± 0.18 21.65 ± 6.92 2.93 ± 0.09 12.38 ± 0.61 2.87 ± 0.07 8.22 ± 0.56 EATA [28] AETTA 18.86 ± 2.19 19.47 ± 1.46 18.76 ± 5.27 17.54 ± 1.56 26.65 ± 2.14 21.51 ± 1.48 19.45 ± 2.91 20.39 ± 2.07 18.76 ± 1.79 19.32 ± 0.12 11.18 ± 1.82 23.44 ± 4.90 24.58 ± 0.69 20.48 ± 2.46 21.82 ± 5.09 20.15 ± 1.70 SrcValid 53.44 ± 0.56 51.46 ± 0.76 59.19 ± 0.77 32.52 ± 0.17 53.90 ± 0.52 35.00 ± 0.71 33.63 ± 0.21 43.03 ± 0.48 43.55 ± 0.30 38.69 ± 0.40 30.12 ± 0.20 33.17 ± 0.40 43.77 ± 0.40 39.64 ± 0.44 49.16 ± 0.23 42.68 ± 0.21 SoftmaxScore [7] 20.82 ± 0.65 21.98 ± 0.41 18.42 ± 0.24 27.91 ± 0.39 20.67 ± 0.57 26.77 ± 0.56 27.81 ± 0.20 24.33 ± 0.51 24.01 ± 0.40 26.76 ± 0.48 27.92 ± 0.16 25.97 ± 0.20 25.72 ± 0.41 26.20 ± 0.38 23.11 ± 0.08 24.56 ± 0.25 GDE [21] 53.09 ± 0.53 51.11 ± 0.69 58.81 ± 0.77 32.45 ± 0.21 53.63 ± 0.55 34.91 ± 0.76 33.60 ± 0.26 42.90 ± 0.57 43.42 ± 0.32 38.63 ± 0.43 30.05 ± 0.19 33.08 ± 0.44 43.65 ± 0.37 39.50 ± 0.44 48.87 ± 0.21 42.51 ± 0.23 AdvPerturb [23] 35.15 ± 1.78 36.42 ± 1.64 32.87 ± 1.13 30.78 ± 0.58 23.87 ± 1.07 16.74 ± 0.22 22.52 ± 0.46 7.21 ± 0.46 21.01 ± 0.61 18.54 ± 0.59 4.30 ± 0.42 48.28 ± 0.16 5.65 ± 0.37 34.21 ± 2.97 6.09 ± 0.46 22.91 ± 0.60 SAR [29] AETTA 5.75 ± 0.45 5.33 ± 0.29 6.90 ± 0.24 5.19 ± 0.22 9.56 ± 0.54 6.43 ± 0.32 5.82 ± 0.30 8.34 ± 0.28 7.15 ± 0.38 5.47 ± 0.18 5.37 ± 0.19 6.04 ± 0.23 6.57 ± 0.50 5.72 ± 0.33 8.50 ± 0.24 6.54 ± 0.15 SrcValid 53.11 ± 0.39 51.88 ± 0.44 57.18 ± 0.18 36.41 ± 0.61 53.90 ± 0.59 38.59 ± 0.67 37.33 ± 0.46 44.93 ± 0.62 44.30 ± 0.32 43.99 ± 0.46 32.17 ± 0.16 41.50 ± 1.22 45.74 ± 0.20 40.52 ± 0.10 47.13 ± 0.13 44.58 ± 0.30 SoftmaxScore [7] 34.06 ± 0.49 35.00 ± 0.50 32.31 ± 0.35 32.88 ± 0.53 32.16 ± 0.58 33.73 ± 0.76 34.70 ± 0.60 36.25 ± 0.96 36.42 ± 0.36 36.35 ± 0.55 30.59 ± 0.77 32.11 ± 0.49 36.57 ± 0.30 38.82 ± 0.21 35.60 ± 0.09 34.50 ± 0.35 GDE [21] 36.44 ± 0.63 35.59 ± 0.34 38.06 ± 0.50 31.10 ± 0.04 38.74 ± 0.55 31.56 ± 0.59 30.66 ± 0.31 32.25 ± 0.84 31.99 ± 0.24 32.19 ± 0.47 29.77 ± 0.36 33.19 ± 0.08 33.02 ± 0.17 29.45 ± 0.39 34.16 ± 0.50 33.21 ± 0.24 AdvPerturb [23] 26.80 ± 0.88 26.32 ± 0.95 26.90 ± 0.45 32.56 ± 0.56 12.67 ± 0.28 24.33 ± 0.97 27.17 ± 0.14 5.68 ± 0.18 11.58 ± 0.31 30.58 ± 0.42 5.21 ± 0.30 47.59 ± 0.76 10.03 ± 0.60 14.24 ± 1.60 6.28 ± 0.19 20.53 ± 0.14 CoTTA [35] AETTA 9.24 ± 0.38 9.97 ± 0.55 8.79 ± 0.53 5.03 ± 0.20 4.83 ± 0.22 4.73 ± 0.21 4.92 ± 0.11 4.66 ± 0.11 4.76 ± 0.43 4.41 ± 0.16 5.04 ± 0.36 5.68 ± 0.24 4.78 ± 0.22 6.27 ± 0.28 7.65 ± 0.15 6.05 ± 0.12 SrcValid 28.06 ± 2.60 29.10 ± 2.49 16.22 ± 0.43 34.38 ± 1.94 10.57 ± 1.13 7.34 ± 0.25 17.58 ± 2.21 14.31 ± 1.55 30.17 ± 1.28 15.75 ± 2.45 30.84 ± 2.27 28.00 ± 1.54 31.30 ± 1.65 15.53 ± 1.36 43.38 ± 1.76 23.50 ± 0.51 SoftmaxScore [7] 18.70 ± 0.62 19.63 ± 0.63 17.09 ± 0.46 29.83 ± 0.32 21.67 ± 0.49 28.86 ± 0.09 30.26 ± 0.35 25.59 ± 0.36 21.84 ± 0.54 28.33 ± 0.32 29.95 ± 0.16 13.55 ± 0.61 27.63 ± 0.23 26.73 ± 0.37 23.09 ± 0.49 24.18 ± 0.19 GDE [21] 59.99 ± 1.07 59.43 ± 1.11 63.72 ± 0.99 33.71 ± 0.18 56.11 ± 0.24 36.10 ± 0.55 34.68 ± 0.67 46.27 ± 0.67 52.01 ± 0.59 41.07 ± 0.17 31.91 ± 0.62 62.02 ± 0.22 45.38 ± 0.57 44.14 ± 1.05 53.79 ± 0.67 48.02 ± 0.56 AdvPerturb [23] 25.47 ± 1.49 26.56 ± 1.30 25.08 ± 0.90 28.92 ± 0.25 19.97 ± 0.92 15.41 ± 0.66 21.68 ± 0.69 6.37 ± 0.19 13.92 ± 0.74 15.02 ± 0.93 4.50 ± 0.35 20.75 ± 1.25 6.84 ± 0.53 28.80 ± 3.46 8.34 ± 0.62 17.84 ± 0.65 RoTTA [36] AETTA 6.37 ± 1.21 6.88 ± 0.64 4.81 ± 0.43 4.74 ± 0.23 6.28 ± 0.34 5.29 ± 0.11 5.18 ± 0.42 7.22 ± 0.43 7.58 ± 1.46 4.74 ± 0.26 4.79 ± 0.48 21.01 ± 3.35 5.53 ± 0.49 5.28 ± 0.56 7.43 ± 1.21 6.88 ± 0.10 SrcValid 13.43 ± 2.25 12.55 ± 1.41 8.28 ± 1.68 37.61 ± 3.05 12.41 ± 3.74 6.47 ± 0.88 12.99 ± 1.60 16.44 ± 1.42 13.05 ± 1.58 7.58 ± 0.80 10.12 ± 1.70 51.02 ± 4.34 35.45 ± 0.57 11.12 ± 4.35 41.55 ± 0.87 19.34 ± 0.63 SoftmaxScore [7] 21.66 ± 0.49 22.18 ± 0.45 19.31 ± 0.05 25.96 ± 0.20 21.08 ± 0.62 25.26 ± 0.47 26.54 ± 0.43 24.57 ± 0.52 24.12 ± 0.19 25.49 ± 0.26 26.25 ± 0.47 23.62 ± 0.52 25.30 ± 0.45 25.17 ± 0.29 23.23 ± 0.39 23.98 ± 0.21 GDE [21] 41.62 ± 0.27 40.60 ± 0.39 48.16 ± 0.43 26.49 ± 0.24 44.28 ± 0.48 28.68 ± 0.42 26.67 ± 0.34 33.51 ± 0.51 34.05 ± 0.18 30.44 ± 0.51 24.30 ± 0.32 27.73 ± 0.69 36.20 ± 0.21 31.28 ± 0.44 39.52 ± 0.24 34.24 ± 0.12 AdvPerturb [23] 41.39 ± 1.33 41.35 ± 1.19 38.06 ± 0.94 32.19 ± 0.31 27.92 ± 1.51 18.64 ± 0.14 24.83 ± 0.76 10.19 ± 0.29 24.05 ± 0.89 21.34 ± 0.74 4.84 ± 0.51 48.66 ± 0.26 6.74 ± 0.18 38.48 ± 1.95 7.88 ± 0.29 25.77 ± 0.47 SoTTA [12] AETTA 8.11 ± 0.91 7.95 ± 0.39 6.35 ± 0.55 4.67 ± 0.32 5.03 ± 0.26 4.28 ± 0.18 4.50 ± 0.28 4.68 ± 0.21 4.73 ± 0.42 5.00 ± 0.37 4.18 ± 0.04 5.30 ± 0.20 4.58 ± 0.07 4.94 ± 0.48 5.02 ± 0.19 5.29 ± 0.18 19Table 9. Mean absolute error (MAE) (%) of the accuracy estimation on fully ImageNet-C. Averaged over three different random seeds. Noise Blur Weather Digital TTA Method Acc. Estimation Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↓) SrcValid 53.54 ± 1.08 52.12 ± 1.12 53.28 ± 0.95 54.72 ± 0.95 53.66 ± 1.07 42.96 ± 0.92 32.78 ± 0.83 37.25 ± 0.96 38.77 ± 0.97 25.06 ± 0.94 10.47 ± 0.67 53.56 ± 0.77 27.86 ± 0.85 22.35 ± 0.70 28.55 ± 0.71 39.13 ± 0.89 SoftmaxScore [7] 11.11 ± 0.14 11.97 ± 0.06 11.35 ± 0.03 9.82 ± 0.09 10.59 ± 0.07 19.16 ± 0.09 26.76 ± 0.09 22.44 ± 0.05 21.08 ± 0.06 31.41 ± 0.05 34.87 ± 0.01 9.52 ± 0.04 28.83 ± 0.05 32.26 ± 0.14 28.88 ± 0.11 20.67 ± 0.01 GDE [21] 85.03 ± 0.14 83.75 ± 0.06 84.81 ± 0.04 85.74 ± 0.09 84.87 ± 0.08 74.26 ± 0.10 64.29 ± 0.08 68.95 ± 0.06 70.47 ± 0.07 56.71 ± 0.02 41.78 ± 0.02 84.43 ± 0.09 59.48 ± 0.07 53.93 ± 0.12 60.29 ± 0.14 70.58 ± 0.01 AdvPerturb [23] 13.38 ± 0.12 14.08 ± 0.05 13.44 ± 0.04 4.43 ± 0.15 5.21 ± 0.14 11.48 ± 0.05 11.47 ± 0.13 17.17 ± 0.13 8.67 ± 0.10 26.80 ± 0.03 6.56 ± 0.16 11.99 ± 0.03 17.97 ± 0.03 19.11 ± 0.14 6.69 ± 0.19 12.56 ± 0.03 TENT [34] AETTA 4.88 ± 0.10 4.27 ± 0.02 4.93 ± 0.09 7.23 ± 0.11 5.03 ± 0.15 4.53 ± 0.09 4.72 ± 0.12 4.35 ± 0.13 4.37 ± 0.11 9.50 ± 0.12 8.88 ± 0.05 6.24 ± 0.02 7.34 ± 0.06 8.95 ± 0.21 6.82 ± 0.17 6.14 ± 0.03 SrcValid 50.34 ± 1.01 48.89 ± 0.88 49.88 ± 0.76 52.47 ± 0.59 51.11 ± 0.57 40.01 ± 0.83 28.80 ± 0.84 32.41 ± 0.71 35.23 ± 0.95 21.78 ± 0.93 8.88 ± 0.53 51.16 ± 0.97 23.11 ± 1.18 19.03 ± 0.72 25.30 ± 0.92 35.89 ± 0.79 SoftmaxScore [7] 12.31 ± 0.08 13.18 ± 0.15 12.71 ± 0.04 10.26 ± 0.14 10.97 ± 0.05 19.72 ± 0.03 27.54 ± 0.10 23.79 ± 0.12 21.46 ± 0.08 30.90 ± 0.11 32.96 ± 0.08 10.51 ± 0.08 29.11 ± 0.07 31.61 ± 0.08 28.90 ± 0.04 21.06 ± 0.03 GDE [21] 81.21 ± 0.16 79.99 ± 0.27 80.82 ± 0.03 77.04 ± 0.50 77.95 ± 0.07 70.22 ± 0.13 60.30 ± 0.02 64.37 ± 0.15 67.25 ± 0.16 53.43 ± 0.10 40.26 ± 0.01 76.64 ± 0.03 55.33 ± 0.09 50.67 ± 0.07 57.02 ± 0.03 66.17 ± 0.07 AdvPerturb [23] 15.44 ± 0.07 16.26 ± 0.19 15.70 ± 0.07 4.89 ± 0.07 6.05 ± 0.14 13.36 ± 0.11 14.92 ± 0.09 20.93 ± 0.14 10.72 ± 0.22 29.38 ± 0.13 5.97 ± 0.17 13.31 ± 0.14 22.32 ± 0.09 21.23 ± 0.19 7.34 ± 0.14 14.52 ± 0.01 EATA [28] AETTA 5.28 ± 0.12 4.72 ± 0.05 5.28 ± 0.07 7.45 ± 0.09 5.12 ± 0.10 4.37 ± 0.07 5.23 ± 0.18 4.60 ± 0.13 4.73 ± 0.13 10.46 ± 0.12 9.22 ± 0.09 5.38 ± 0.09 8.25 ± 0.16 9.47 ± 0.10 7.68 ± 0.05 6.48 ± 0.02 SrcValid 39.53 ± 1.22 37.69 ± 1.46 39.79 ± 1.36 43.89 ± 1.00 43.75 ± 0.82 33.12 ± 0.77 25.52 ± 1.06 27.90 ± 0.86 31.43 ± 1.03 17.14 ± 0.46 9.03 ± 0.71 40.82 ± 0.33 19.89 ± 1.25 16.74 ± 0.89 20.27 ± 1.14 29.77 ± 0.94 SoftmaxScore [7] 18.07 ± 0.26 19.20 ± 0.20 18.08 ± 0.21 14.90 ± 0.25 15.23 ± 0.29 24.02 ± 0.06 29.50 ± 0.06 26.57 ± 0.09 23.87 ± 0.16 33.16 ± 0.11 34.57 ± 0.09 12.88 ± 1.75 31.62 ± 0.09 33.20 ± 0.20 31.47 ± 0.02 24.42 ± 0.08 GDE [21] 75.10 ± 0.28 73.20 ± 0.21 75.19 ± 0.25 77.65 ± 0.21 77.44 ± 0.28 66.24 ± 0.07 58.62 ± 0.07 61.48 ± 0.10 65.23 ± 0.16 50.56 ± 0.07 40.51 ± 0.12 74.62 ± 0.99 53.29 ± 0.08 49.16 ± 0.20 53.97 ± 0.02 63.48 ± 0.03 AdvPerturb [23] 22.95 ± 0.27 24.15 ± 0.14 22.86 ± 0.25 7.84 ± 0.11 10.46 ± 0.36 18.99 ± 0.17 17.74 ± 0.13 24.62 ± 0.12 13.19 ± 0.07 32.89 ± 0.05 6.06 ± 0.06 21.28 ± 0.94 25.45 ± 0.13 23.36 ± 0.15 9.55 ± 0.11 18.76 ± 0.06 SAR [29] AETTA 5.38 ± 0.27 5.24 ± 0.16 5.11 ± 0.15 8.06 ± 0.04 6.19 ± 0.27 4.53 ± 0.08 5.17 ± 0.14 4.97 ± 0.11 4.71 ± 0.04 9.73 ± 0.16 8.68 ± 0.10 5.39 ± 0.33 7.76 ± 0.17 8.66 ± 0.19 6.89 ± 0.03 6.43 ± 0.09 SrcValid 55.20 ± 0.50 54.09 ± 0.55 54.87 ± 0.49 56.70 ± 0.86 55.43 ± 0.45 45.10 ± 0.55 34.91 ± 0.54 39.13 ± 0.55 40.12 ± 0.54 27.92 ± 0.56 10.68 ± 0.44 56.19 ± 0.65 29.77 ± 0.49 24.40 ± 0.41 31.80 ± 0.56 41.09 ± 0.53 SoftmaxScore [7] 9.93 ± 0.07 10.66 ± 0.11 10.24 ± 0.03 8.36 ± 0.10 9.36 ± 0.05 17.69 ± 0.14 25.68 ± 0.03 21.47 ± 0.07 20.57 ± 0.05 30.40 ± 0.06 35.07 ± 0.08 7.79 ± 0.13 28.04 ± 0.03 31.69 ± 0.11 27.39 ± 0.08 19.62 ± 0.02 GDE [21] 86.85 ± 0.06 85.76 ± 0.13 86.52 ± 0.02 88.18 ± 0.12 87.04 ± 0.05 76.83 ± 0.14 66.63 ± 0.03 70.86 ± 0.07 71.81 ± 0.06 59.64 ± 0.06 42.23 ± 0.09 87.87 ± 0.13 61.50 ± 0.03 56.13 ± 0.11 63.53 ± 0.08 72.76 ± 0.02 AdvPerturb [23] 11.73 ± 0.07 12.19 ± 0.09 11.84 ± 0.03 3.81 ± 0.06 4.33 ± 0.03 9.70 ± 0.09 9.46 ± 0.06 15.50 ± 0.13 7.86 ± 0.13 24.07 ± 0.09 6.72 ± 0.20 9.11 ± 0.13 15.96 ± 0.09 17.56 ± 0.16 5.96 ± 0.34 11.05 ± 0.02 CoTTA [35] AETTA 4.76 ± 0.07 4.00 ± 0.04 4.79 ± 0.07 7.69 ± 0.18 5.04 ± 0.06 4.63 ± 0.08 4.49 ± 0.15 4.39 ± 0.08 4.25 ± 0.10 8.97 ± 0.12 8.81 ± 0.09 6.70 ± 0.18 7.00 ± 0.02 8.44 ± 0.19 6.27 ± 0.19 6.02 ± 0.03 SrcValid 15.32 ± 0.10 13.73 ± 0.68 15.71 ± 0.14 4.79 ± 0.54 17.18 ± 1.34 5.81 ± 0.55 9.04 ± 1.50 8.44 ± 0.80 5.54 ± 0.33 8.84 ± 1.21 10.68 ± 0.77 14.01 ± 0.55 7.27 ± 0.34 5.59 ± 0.27 12.32 ± 0.91 10.28 ± 0.28 SoftmaxScore [7] 11.98 ± 0.07 12.84 ± 0.28 12.31 ± 0.13 9.96 ± 0.18 10.93 ± 0.12 19.51 ± 0.09 27.15 ± 0.18 23.28 ± 0.02 20.61 ± 0.11 31.75 ± 0.10 33.58 ± 0.05 11.05 ± 0.14 29.18 ± 0.04 32.08 ± 0.09 29.22 ± 0.10 21.03 ± 0.04 GDE [21] 80.31 ± 0.05 79.07 ± 0.33 80.18 ± 0.21 82.25 ± 0.26 81.56 ± 0.12 70.30 ± 0.10 60.45 ± 0.17 64.30 ± 0.07 67.01 ± 0.25 52.64 ± 0.18 38.45 ± 0.03 77.50 ± 0.21 55.15 ± 0.07 50.24 ± 0.01 56.48 ± 0.20 66.39 ± 0.04 AdvPerturb [23] 13.96 ± 0.07 14.55 ± 0.30 14.24 ± 0.13 4.68 ± 0.09 5.18 ± 0.05 11.83 ± 0.05 12.11 ± 0.12 17.54 ± 0.06 7.91 ± 0.12 27.54 ± 0.11 6.71 ± 0.08 12.27 ± 0.11 18.72 ± 0.11 19.66 ± 0.02 7.09 ± 0.19 12.93 ± 0.04 RoTTA [36] AETTA 14.33 ± 0.14 14.97 ± 0.22 14.49 ± 0.10 8.62 ± 0.48 7.53 ± 0.33 14.33 ± 0.27 13.02 ± 0.16 11.06 ± 0.16 12.36 ± 0.13 17.18 ± 0.13 16.82 ± 0.13 8.27 ± 0.27 15.24 ± 0.05 32.03 ± 0.07 22.07 ± 0.11 14.82 ± 0.01 SrcValid 28.39 ± 0.39 25.90 ± 0.41 28.46 ± 0.79 9.76 ± 2.10 7.02 ± 0.68 21.34 ± 2.88 12.78 ± 1.76 18.59 ± 0.86 6.08 ± 0.71 18.38 ± 0.76 12.63 ± 0.92 22.00 ± 0.98 13.36 ± 1.19 9.43 ± 0.81 5.84 ± 0.67 16.00 ± 0.33 SoftmaxScore [7] 19.01 ± 0.14 20.58 ± 0.17 19.52 ± 0.27 16.57 ± 0.42 17.93 ± 0.07 24.41 ± 0.09 27.95 ± 0.15 26.76 ± 0.28 23.75 ± 0.09 30.23 ± 0.10 30.71 ± 0.14 6.66 ± 0.36 30.04 ± 0.14 30.52 ± 0.09 29.35 ± 0.21 23.60 ± 0.07 GDE [21] 62.46 ± 0.16 60.09 ± 0.20 62.04 ± 0.18 64.31 ± 0.25 63.47 ± 0.16 53.92 ± 0.34 48.60 ± 0.35 50.02 ± 0.29 54.63 ± 0.18 42.15 ± 0.15 35.24 ± 0.17 64.78 ± 0.07 43.55 ± 0.18 40.82 ± 0.15 45.04 ± 0.07 52.74 ± 0.02 AdvPerturb [23] 27.73 ± 0.10 29.55 ± 0.21 28.38 ± 0.31 12.10 ± 0.29 17.45 ± 0.17 24.33 ± 0.18 23.11 ± 0.23 30.31 ± 0.26 17.63 ± 0.08 36.11 ± 0.08 5.52 ± 0.12 21.22 ± 0.32 31.03 ± 0.08 26.34 ± 0.10 12.61 ± 0.06 22.90 ± 0.02 SoTTA [12] AETTA 17.92 ± 0.25 18.73 ± 0.95 16.32 ± 2.09 14.69 ± 1.33 7.64 ± 0.29 18.94 ± 0.59 17.21 ± 0.54 16.92 ± 0.51 14.49 ± 0.37 20.84 ± 0.02 18.54 ± 0.44 12.70 ± 1.06 20.46 ± 0.39 25.44 ± 0.42 20.13 ± 0.17 17.40 ± 0.26 20Table 10. Mean absolute error (MAE) (%) of the accuracy estimation on continual CIFAR10-C. Averaged over three different random seeds. t TTA Method Acc. Estimation Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↓) SrcValid 24.85 ± 0.83 17.83 ± 1.52 22.28 ± 0.17 8.89 ± 1.17 19.20 ± 3.28 8.62 ± 3.45 7.16 ± 2.71 9.00 ± 3.73 8.13 ± 2.54 7.34 ± 2.32 4.44 ± 0.57 6.28 ± 1.89 7.70 ± 3.52 5.22 ± 1.67 5.68 ± 2.27 10.84 ± 1.83 SoftmaxScore [7] 8.40 ± 0.10 12.84 ± 2.07 24.30 ± 4.53 20.45 ± 7.09 38.03 ± 11.59 37.45 ± 14.78 37.21 ± 16.96 42.53 ± 19.10 45.88 ± 18.10 49.95 ± 15.87 49.10 ± 14.58 56.89 ± 12.63 62.16 ± 13.50 63.30 ± 12.88 68.01 ± 12.02 41.10 ± 11.66 GDE [21] 25.29 ± 0.67 22.33 ± 1.77 33.67 ± 4.42 25.64 ± 6.92 45.16 ± 10.64 41.70 ± 13.71 40.88 ± 16.32 46.23 ± 18.18 49.25 ± 16.91 52.99 ± 14.83 51.55 ± 13.76 59.15 ± 12.01 65.02 ± 12.36 65.40 ± 11.94 70.16 ± 11.23 46.29 ± 10.93 AdvPerturb [23] 48.04 ± 2.26 44.46 ± 5.59 44.82 ± 7.11 15.98 ± 7.94 10.29 ± 2.90 6.10 ± 0.85 7.51 ± 4.79 4.17 ± 0.50 6.54 ± 2.69 9.16 ± 3.21 2.32 ± 0.35 17.84 ± 10.01 3.97 ± 0.58 8.40 ± 3.06 3.74 ± 0.23 15.56 ± 1.53 TENT [34] AETTA 4.12 ± 0.45 4.02 ± 0.26 4.63 ± 0.31 8.37 ± 3.30 6.92 ± 0.72 11.18 ± 4.48 10.40 ± 6.13 9.48 ± 2.58 12.23 ± 5.64 11.40 ± 5.47 13.30 ± 0.62 14.01 ± 5.38 14.17 ± 6.47 13.96 ± 4.59 12.58 ± 5.01 10.05 ± 1.69 SrcValid 18.53 ± 0.43 11.25 ± 0.30 17.43 ± 1.46 7.76 ± 0.72 20.95 ± 1.86 8.86 ± 0.99 7.02 ± 0.97 10.91 ± 1.06 9.68 ± 1.26 7.36 ± 1.45 4.34 ± 0.70 6.45 ± 0.16 16.05 ± 1.85 8.15 ± 0.85 11.16 ± 2.45 11.06 ± 0.11 SoftmaxScore [7] 4.75 ± 0.62 6.20 ± 1.19 15.17 ± 1.79 10.01 ± 3.73 21.53 ± 3.27 18.50 ± 7.97 15.43 ± 6.98 16.81 ± 4.13 17.71 ± 7.04 15.33 ± 7.57 12.30 ± 5.92 14.49 ± 5.54 21.08 ± 4.63 18.07 ± 6.59 23.67 ± 6.32 15.40 ± 4.73 GDE [21] 22.88 ± 0.72 20.96 ± 1.47 31.37 ± 2.07 21.28 ± 4.63 36.43 ± 3.32 29.58 ± 8.64 25.03 ± 7.81 26.76 ± 4.68 27.17 ± 7.62 23.91 ± 8.61 19.29 ± 6.98 21.44 ± 6.17 31.55 ± 4.68 26.22 ± 6.77 32.68 ± 5.87 26.44 ± 5.16 AdvPerturb [23] 50.21 ± 4.36 45.51 ± 5.20 42.95 ± 5.29 23.78 ± 5.30 12.43 ± 0.75 11.88 ± 4.12 16.14 ± 4.75 4.38 ± 0.46 12.59 ± 3.26 9.28 ± 2.72 2.47 ± 0.07 44.14 ± 6.38 4.71 ± 0.28 28.64 ± 6.81 4.84 ± 0.67 20.93 ± 2.83 EATA [28] AETTA 3.86 ± 0.28 3.98 ± 0.12 5.96 ± 1.90 5.97 ± 2.48 9.89 ± 1.84 10.47 ± 6.25 8.03 ± 5.24 7.47 ± 3.19 7.79 ± 5.72 7.02 ± 4.80 5.59 ± 3.33 6.80 ± 3.37 6.69 ± 3.13 7.13 ± 4.33 10.22 ± 4.52 7.13 ± 3.33 SrcValid 32.05 ± 1.03 30.47 ± 0.76 37.42 ± 0.42 12.20 ± 0.20 33.88 ± 0.52 13.69 ± 0.12 12.62 ± 0.36 18.37 ± 0.36 19.73 ± 0.52 14.61 ± 0.57 9.26 ± 0.24 13.12 ± 0.43 23.28 ± 0.20 20.67 ± 0.02 28.03 ± 0.53 21.29 ± 0.26 SoftmaxScore [7] 4.21 ± 0.33 4.04 ± 0.20 5.54 ± 0.53 6.28 ± 0.32 4.91 ± 0.14 5.93 ± 0.25 6.49 ± 0.52 4.85 ± 0.27 4.86 ± 0.29 5.80 ± 0.59 7.11 ± 0.48 5.34 ± 0.30 4.26 ± 0.24 4.55 ± 0.09 3.94 ± 0.11 5.21 ± 0.22 GDE [21] 31.88 ± 1.08 30.35 ± 0.80 37.25 ± 0.49 12.19 ± 0.20 33.80 ± 0.52 13.68 ± 0.13 12.62 ± 0.36 18.37 ± 0.36 19.73 ± 0.52 14.61 ± 0.57 9.26 ± 0.24 13.12 ± 0.43 23.28 ± 0.20 20.67 ± 0.02 28.01 ± 0.52 21.25 ± 0.27 AdvPerturb [23] 42.25 ± 2.34 37.75 ± 2.91 38.40 ± 4.24 30.55 ± 3.11 9.77 ± 1.93 18.20 ± 1.52 21.66 ± 2.37 4.60 ± 0.60 14.48 ± 2.98 11.73 ± 0.52 2.71 ± 0.14 52.81 ± 2.08 4.92 ± 0.04 31.36 ± 0.89 6.98 ± 0.45 21.88 ± 0.93 SAR [29] AETTA 4.91 ± 0.84 5.14 ± 0.85 4.77 ± 0.15 2.91 ± 0.14 5.45 ± 0.50 3.08 ± 0.06 3.18 ± 0.21 3.55 ± 0.37 3.65 ± 0.17 3.25 ± 0.34 2.81 ± 0.06 3.58 ± 0.61 3.87 ± 0.15 3.69 ± 0.43 4.48 ± 0.23 3.89 ± 0.06 SrcValid 23.70 ± 0.75 21.71 ± 0.16 28.09 ± 0.28 12.26 ± 0.05 28.88 ± 0.55 13.78 ± 0.09 12.46 ± 0.35 17.09 ± 0.59 17.19 ± 0.48 15.34 ± 0.36 9.18 ± 0.14 16.33 ± 0.42 21.34 ± 0.68 16.88 ± 0.25 20.29 ± 0.38 18.30 ± 0.25 SoftmaxScore [7] 16.82 ± 0.51 16.82 ± 0.61 16.38 ± 0.27 8.79 ± 0.51 13.43 ± 0.32 9.90 ± 0.53 10.29 ± 0.48 12.04 ± 0.84 14.12 ± 0.52 10.76 ± 0.50 10.49 ± 0.65 9.82 ± 0.61 13.90 ± 0.44 15.08 ± 0.74 15.73 ± 0.81 12.96 ± 0.37 GDE [21] 15.65 ± 0.77 14.46 ± 0.13 19.48 ± 0.59 11.93 ± 0.06 21.35 ± 0.41 13.05 ± 0.12 12.00 ± 0.21 14.71 ± 0.35 13.73 ± 0.30 14.20 ± 0.32 9.14 ± 0.13 14.62 ± 0.20 16.83 ± 0.33 13.66 ± 0.33 15.56 ± 0.37 14.69 ± 0.15 AdvPerturb [23] 16.79 ± 0.32 15.00 ± 0.78 19.37 ± 3.92 31.13 ± 3.19 7.05 ± 0.41 20.30 ± 2.25 23.01 ± 2.44 5.86 ± 0.42 11.50 ± 1.57 16.32 ± 1.37 2.55 ± 0.09 53.39 ± 1.32 13.96 ± 1.62 23.41 ± 1.77 7.14 ± 0.62 17.79 ± 0.74 CoTTA [35] AETTA 15.34 ± 1.06 13.13 ± 1.39 12.06 ± 0.89 3.15 ± 0.12 5.08 ± 0.30 3.29 ± 0.04 3.18 ± 0.30 3.45 ± 0.05 3.92 ± 0.27 3.51 ± 0.21 2.75 ± 0.02 5.01 ± 0.15 4.15 ± 0.21 4.18 ± 0.37 5.06 ± 0.57 5.82 ± 0.30 SrcValid 27.12 ± 7.07 26.34 ± 6.79 9.26 ± 2.75 7.15 ± 1.06 24.37 ± 0.72 5.96 ± 0.94 4.82 ± 1.05 4.60 ± 0.29 17.57 ± 1.70 4.44 ± 0.77 4.69 ± 0.63 21.73 ± 4.14 16.11 ± 0.21 8.78 ± 0.75 17.58 ± 0.24 13.37 ± 0.89 SoftmaxScore [7] 4.68 ± 0.46 5.10 ± 0.20 5.00 ± 0.09 11.86 ± 0.78 8.45 ± 0.59 13.92 ± 1.06 15.37 ± 0.84 14.95 ± 0.47 15.51 ± 0.36 16.15 ± 0.63 15.69 ± 0.45 15.48 ± 0.47 15.65 ± 0.13 15.52 ± 0.31 15.31 ± 0.30 12.57 ± 0.43 GDE [21] 32.94 ± 0.75 27.97 ± 1.10 33.95 ± 0.74 13.61 ± 0.39 28.49 ± 0.73 11.81 ± 0.16 9.71 ± 0.21 13.20 ± 0.20 12.86 ± 0.46 11.37 ± 0.26 7.00 ± 0.28 11.11 ± 0.41 16.67 ± 0.31 13.76 ± 0.19 18.09 ± 0.36 17.50 ± 0.30 AdvPerturb [23] 40.38 ± 2.57 39.03 ± 3.20 40.39 ± 3.88 29.63 ± 2.87 13.44 ± 2.20 18.72 ± 1.44 23.67 ± 3.12 6.03 ± 0.60 17.61 ± 3.15 13.01 ± 0.57 2.73 ± 0.09 53.28 ± 0.74 4.84 ± 0.13 36.60 ± 1.21 4.90 ± 0.44 22.95 ± 0.82 RoTTA [36] AETTA 13.47 ± 5.78 12.40 ± 5.05 10.05 ± 3.52 3.69 ± 0.21 5.45 ± 0.92 3.39 ± 0.15 3.24 ± 0.16 3.67 ± 0.69 3.90 ± 0.61 3.75 ± 0.44 2.77 ± 0.03 3.37 ± 0.36 4.00 ± 0.14 3.48 ± 0.22 3.73 ± 0.35 5.36 ± 1.22 SrcValid 11.98 ± 4.00 5.70 ± 1.02 9.03 ± 3.17 6.60 ± 2.08 21.16 ± 1.40 6.41 ± 0.73 3.82 ± 0.36 9.00 ± 1.38 5.61 ± 0.97 5.74 ± 0.40 3.94 ± 0.35 15.30 ± 2.22 15.65 ± 0.40 5.94 ± 0.52 15.07 ± 0.88 9.40 ± 0.85 SoftmaxScore [7] 4.10 ± 0.13 4.13 ± 0.63 3.96 ± 0.62 4.79 ± 0.18 5.39 ± 1.27 3.72 ± 0.26 4.37 ± 0.89 3.67 ± 0.24 3.55 ± 0.36 4.41 ± 0.53 4.39 ± 0.81 6.63 ± 0.32 3.88 ± 0.22 4.35 ± 0.22 4.15 ± 0.57 4.37 ± 0.09 GDE [21] 23.46 ± 0.77 17.63 ± 0.94 24.00 ± 0.97 14.12 ± 1.41 26.42 ± 1.21 15.42 ± 0.99 11.27 ± 0.58 15.05 ± 0.48 14.27 ± 0.48 13.33 ± 0.37 8.85 ± 0.15 15.49 ± 1.19 19.61 ± 1.30 15.91 ± 1.09 20.61 ± 1.00 17.03 ± 0.70 AdvPerturb [23] 47.94 ± 3.36 47.49 ± 4.59 50.19 ± 5.41 26.66 ± 1.29 16.53 ± 1.72 15.64 ± 0.97 21.76 ± 2.19 4.96 ± 0.25 15.67 ± 3.26 10.46 ± 0.47 2.61 ± 0.19 48.72 ± 0.34 4.51 ± 0.34 35.66 ± 0.51 5.64 ± 0.52 23.63 ± 0.78 SoTTA [12] AETTA 7.91 ± 1.16 5.83 ± 0.56 5.76 ± 0.77 3.27 ± 0.06 4.58 ± 0.11 3.57 ± 0.19 3.28 ± 0.17 3.49 ± 0.10 3.66 ± 0.08 3.79 ± 0.13 2.94 ± 0.02 3.81 ± 0.21 4.20 ± 0.01 4.07 ± 0.07 3.87 ± 0.02 4.27 ± 0.12 21Table 11. Mean absolute error (MAE) (%) of the accuracy estimation on continual CIFAR100-C. Averaged over three different random seeds. t TTA Method Acc. Estimation Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↓) SrcValid 46.38 ± 1.17 32.85 ± 3.59 28.45 ± 3.99 12.51 ± 0.84 16.92 ± 2.40 5.75 ± 0.60 3.84 ± 0.73 3.88 ± 1.45 2.77 ± 0.87 2.32 ± 0.35 2.12 ± 0.20 2.00 ± 0.13 1.87 ± 0.15 1.73 ± 0.30 1.65 ± 0.26 11.00 ± 0.58 SoftmaxScore [7] 13.70 ± 0.41 5.34 ± 0.21 13.91 ± 0.58 21.16 ± 0.94 38.51 ± 1.59 50.42 ± 4.50 59.40 ± 6.35 70.82 ± 5.52 78.56 ± 3.43 82.11 ± 1.88 84.99 ± 1.14 89.52 ± 0.24 87.46 ± 1.72 88.87 ± 1.65 89.56 ± 0.63 58.29 ± 1.82 GDE [21] 49.21 ± 0.79 48.12 ± 0.60 60.77 ± 0.20 56.99 ± 1.30 74.34 ± 1.15 77.88 ± 4.24 82.28 ± 5.11 90.14 ± 3.30 93.71 ± 1.63 95.33 ± 0.77 95.86 ± 0.45 96.93 ± 0.43 96.88 ± 0.62 97.25 ± 0.75 97.31 ± 0.82 80.87 ± 1.29 AdvPerturb [23] 36.92 ± 1.76 36.25 ± 0.61 29.26 ± 1.31 13.91 ± 0.54 8.56 ± 2.77 3.79 ± 0.85 3.68 ± 1.01 3.08 ± 0.81 3.96 ± 2.12 2.58 ± 0.56 1.47 ± 0.13 2.10 ± 0.28 2.26 ± 1.07 2.46 ± 0.66 1.53 ± 0.06 10.12 ± 0.24 TENT [34] AETTA 6.55 ± 0.59 7.40 ± 0.10 7.52 ± 0.54 13.45 ± 0.40 6.30 ± 0.56 6.87 ± 0.81 8.36 ± 1.09 8.09 ± 2.80 5.08 ± 1.37 3.84 ± 0.58 3.53 ± 0.33 2.97 ± 0.32 2.73 ± 0.35 2.54 ± 0.51 2.51 ± 0.42 5.85 ± 0.36 SrcValid 7.65 ± 0.94 1.97 ± 0.17 1.59 ± 0.33 1.47 ± 0.43 1.24 ± 0.22 1.36 ± 0.26 1.31 ± 0.23 1.22 ± 0.20 1.07 ± 0.10 1.01 ± 0.08 0.94 ± 0.04 0.98 ± 0.11 1.13 ± 0.20 1.11 ± 0.11 1.09 ± 0.10 1.68 ± 0.18 SoftmaxScore [7] 36.65 ± 1.55 64.29 ± 1.84 70.97 ± 1.45 75.02 ± 1.73 77.61 ± 1.85 78.47 ± 2.34 78.46 ± 0.61 79.04 ± 0.36 82.42 ± 0.71 81.35 ± 0.18 84.35 ± 0.31 89.83 ± 0.75 82.75 ± 0.91 83.33 ± 1.63 84.09 ± 1.09 76.58 ± 0.71 GDE [21] 83.95 ± 1.83 94.00 ± 0.88 94.99 ± 0.74 94.52 ± 0.94 95.15 ± 0.50 94.73 ± 1.31 94.57 ± 1.16 93.63 ± 1.11 95.31 ± 1.51 95.46 ± 0.38 95.31 ± 0.51 94.60 ± 1.34 94.55 ± 0.70 94.51 ± 0.48 94.81 ± 1.49 94.01 ± 0.43 AdvPerturb [23] 9.32 ± 0.83 4.85 ± 1.55 2.07 ± 0.54 1.55 ± 0.70 1.41 ± 0.20 1.31 ± 0.43 1.22 ± 0.16 1.21 ± 0.52 0.88 ± 0.36 0.89 ± 0.27 0.52 ± 0.07 1.08 ± 0.57 0.98 ± 0.25 1.27 ± 0.22 0.93 ± 0.14 1.97 ± 0.33 EATA [28] AETTA 18.86 ± 2.19 7.14 ± 2.67 2.59 ± 0.40 4.21 ± 3.24 3.33 ± 2.13 2.64 ± 1.35 2.03 ± 0.25 1.90 ± 0.14 1.78 ± 0.06 1.74 ± 0.06 1.97 ± 0.34 8.42 ± 10.00 2.10 ± 0.09 1.92 ± 0.10 2.04 ± 0.48 4.18 ± 0.82 SrcValid 53.44 ± 0.56 44.48 ± 0.19 50.08 ± 0.66 32.15 ± 0.18 47.60 ± 0.38 33.57 ± 0.57 30.57 ± 0.27 38.18 ± 0.81 36.38 ± 0.46 35.00 ± 0.48 27.36 ± 0.32 29.30 ± 0.55 39.09 ± 0.36 33.58 ± 0.22 42.22 ± 0.34 38.20 ± 0.22 SoftmaxScore [7] 20.82 ± 0.65 23.48 ± 0.20 20.68 ± 0.05 25.76 ± 0.19 21.30 ± 0.50 24.92 ± 0.54 26.40 ± 0.30 23.71 ± 0.68 23.90 ± 0.26 25.12 ± 0.32 26.21 ± 0.19 25.49 ± 0.24 24.85 ± 0.45 25.13 ± 0.06 22.95 ± 0.60 24.05 ± 0.29 GDE [21] 53.09 ± 0.53 44.65 ± 0.17 51.31 ± 0.41 33.64 ± 0.17 48.74 ± 0.41 34.68 ± 0.61 31.44 ± 0.31 39.11 ± 0.72 37.62 ± 0.29 36.45 ± 0.28 28.86 ± 0.08 30.48 ± 0.33 40.06 ± 0.27 34.55 ± 0.24 43.42 ± 0.28 39.21 ± 0.22 AdvPerturb [23] 35.15 ± 1.78 42.48 ± 1.22 40.57 ± 0.60 29.46 ± 0.65 28.50 ± 1.19 16.57 ± 0.38 23.68 ± 0.46 8.75 ± 0.68 25.17 ± 0.46 18.91 ± 0.75 4.57 ± 0.42 49.75 ± 0.52 5.52 ± 0.24 38.29 ± 2.90 6.54 ± 0.50 24.93 ± 0.57 SAR [29] AETTA 5.75 ± 0.45 5.35 ± 0.22 6.31 ± 0.08 6.59 ± 0.22 9.11 ± 0.29 7.66 ± 0.31 6.16 ± 0.37 8.37 ± 0.47 7.06 ± 0.21 6.12 ± 0.09 5.78 ± 0.30 5.65 ± 0.01 6.32 ± 0.36 5.90 ± 0.35 7.93 ± 0.15 6.67 ± 0.12 SrcValid 53.11 ± 0.39 51.92 ± 0.46 57.00 ± 0.31 36.42 ± 0.36 54.13 ± 0.72 39.30 ± 0.46 38.28 ± 0.42 46.48 ± 0.48 46.81 ± 0.58 47.73 ± 0.89 33.94 ± 0.43 44.55 ± 1.25 49.18 ± 0.38 42.97 ± 0.08 49.48 ± 0.27 46.09 ± 0.38 SoftmaxScore [7] 34.06 ± 0.49 34.58 ± 0.52 32.05 ± 0.53 33.14 ± 0.76 31.92 ± 0.80 34.68 ± 0.79 35.83 ± 0.82 36.94 ± 1.04 37.46 ± 0.85 36.13 ± 0.91 36.94 ± 1.03 40.90 ± 0.70 38.04 ± 0.86 42.67 ± 0.64 38.70 ± 0.68 36.27 ± 0.68 GDE [21] 36.44 ± 0.63 36.25 ± 0.17 39.37 ± 0.40 31.48 ± 0.46 40.05 ± 0.54 32.73 ± 0.55 32.12 ± 0.19 34.87 ± 0.38 35.40 ± 0.79 36.28 ± 0.78 31.54 ± 0.36 37.31 ± 0.43 36.98 ± 0.26 32.82 ± 0.43 37.87 ± 0.69 35.43 ± 0.30 AdvPerturb [23] 26.80 ± 0.88 26.08 ± 0.93 27.02 ± 0.54 32.75 ± 0.77 11.69 ± 0.89 23.72 ± 1.09 26.36 ± 0.40 5.63 ± 0.17 9.87 ± 0.12 27.10 ± 0.57 4.87 ± 0.27 44.56 ± 1.41 7.57 ± 0.44 14.56 ± 1.39 5.74 ± 0.25 19.62 ± 0.15 CoTTA [35] AETTA 9.24 ± 0.38 8.52 ± 0.55 6.75 ± 0.35 5.20 ± 0.13 5.06 ± 0.25 5.54 ± 0.16 5.70 ± 0.29 5.49 ± 0.31 5.88 ± 0.33 6.71 ± 0.59 7.38 ± 0.10 9.70 ± 0.72 6.33 ± 0.14 5.14 ± 0.27 5.66 ± 0.45 6.55 ± 0.17 SrcValid 28.06 ± 2.60 29.86 ± 2.79 13.52 ± 1.12 22.72 ± 0.61 18.32 ± 1.73 12.14 ± 0.85 5.78 ± 0.25 8.58 ± 0.39 23.32 ± 0.81 10.68 ± 2.60 6.52 ± 1.30 25.88 ± 4.11 33.45 ± 0.64 13.98 ± 3.80 38.61 ± 0.73 19.43 ± 1.17 SoftmaxScore [7] 18.70 ± 0.62 21.28 ± 0.40 19.45 ± 0.34 28.25 ± 0.20 22.90 ± 0.62 29.05 ± 0.26 31.75 ± 0.27 28.49 ± 0.44 29.37 ± 0.19 30.03 ± 0.28 31.39 ± 0.69 28.52 ± 0.44 30.99 ± 0.70 29.72 ± 0.36 28.03 ± 0.41 27.19 ± 0.12 GDE [21] 59.99 ± 1.07 57.35 ± 0.80 61.31 ± 0.96 36.40 ± 0.92 53.91 ± 0.73 34.73 ± 0.63 31.10 ± 0.63 39.01 ± 0.31 37.56 ± 0.48 37.15 ± 0.45 26.90 ± 0.24 33.56 ± 0.20 38.31 ± 0.64 34.69 ± 0.46 43.29 ± 0.95 41.68 ± 0.45 AdvPerturb [23] 25.47 ± 1.49 28.60 ± 0.91 27.71 ± 0.81 26.38 ± 0.74 22.80 ± 1.70 15.78 ± 0.72 23.44 ± 0.96 8.93 ± 0.85 24.12 ± 0.74 16.68 ± 0.95 4.56 ± 0.43 44.85 ± 0.74 5.96 ± 0.33 36.17 ± 3.16 6.18 ± 0.16 21.18 ± 0.71 RoTTA [36] AETTA 6.37 ± 1.21 7.00 ± 0.80 4.77 ± 0.44 6.20 ± 0.32 6.54 ± 0.45 5.69 ± 0.27 4.89 ± 0.24 6.59 ± 0.15 4.83 ± 0.55 5.52 ± 0.25 4.78 ± 0.26 7.60 ± 0.80 5.40 ± 0.16 4.92 ± 0.69 6.84 ± 0.76 5.86 ± 0.10 SrcValid 13.43 ± 2.25 8.45 ± 1.26 9.32 ± 2.50 14.85 ± 2.02 23.51 ± 3.62 9.03 ± 1.89 6.22 ± 1.14 26.37 ± 2.50 10.83 ± 3.76 8.66 ± 1.84 20.27 ± 0.21 35.17 ± 2.57 29.05 ± 3.38 14.48 ± 4.68 33.72 ± 0.71 17.56 ± 1.57 SoftmaxScore [7] 21.66 ± 0.49 22.56 ± 0.75 18.88 ± 0.55 21.70 ± 0.73 18.70 ± 1.01 22.58 ± 0.47 24.48 ± 0.34 21.62 ± 0.61 22.08 ± 0.10 22.18 ± 0.08 23.67 ± 0.48 21.86 ± 0.55 22.45 ± 0.65 22.42 ± 0.63 21.45 ± 0.49 21.89 ± 0.35 GDE [21] 41.62 ± 0.27 37.46 ± 0.91 46.03 ± 0.18 33.39 ± 0.87 44.99 ± 1.03 32.05 ± 0.66 28.16 ± 0.28 35.19 ± 0.80 33.80 ± 0.28 32.74 ± 0.13 26.34 ± 0.48 29.23 ± 0.80 37.40 ± 0.78 31.99 ± 1.11 38.42 ± 0.57 35.25 ± 0.27 AdvPerturb [23] 41.39 ± 1.33 45.03 ± 0.81 40.28 ± 1.21 25.84 ± 0.41 26.78 ± 1.90 16.58 ± 0.85 23.72 ± 0.36 9.68 ± 0.69 24.71 ± 0.74 19.14 ± 0.83 4.52 ± 0.20 47.12 ± 1.27 5.70 ± 0.02 37.97 ± 2.21 8.41 ± 0.72 25.12 ± 0.39 SoTTA [12] AETTA 8.11 ± 0.91 7.19 ± 0.44 7.34 ± 0.79 4.99 ± 0.31 4.97 ± 0.25 4.30 ± 0.19 4.33 ± 0.14 4.86 ± 0.24 4.90 ± 0.34 5.23 ± 0.46 4.24 ± 0.19 5.20 ± 0.32 4.61 ± 0.10 5.01 ± 0.27 4.58 ± 0.13 5.32 ± 0.18 22Table 12. Mean absolute error (MAE) (%) of the accuracy estimation on continual ImageNet-C. Averaged over three different random seeds. t TTA Method Acc. Estimation Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↓) SrcValid 53.54 ± 1.08 48.82 ± 1.21 47.13 ± 1.16 48.84 ± 1.07 44.94 ± 1.16 34.83 ± 1.25 26.16 ± 1.18 32.41 ± 1.21 31.80 ± 1.02 21.17 ± 1.05 8.93 ± 0.73 43.25 ± 0.72 20.57 ± 0.66 17.02 ± 0.49 20.10 ± 0.27 33.30 ± 0.93 SoftmaxScore [7] 11.11 ± 0.14 13.76 ± 0.03 14.42 ± 0.09 11.60 ± 0.01 13.15 ± 0.12 19.04 ± 0.18 24.78 ± 0.06 19.82 ± 0.06 19.35 ± 0.11 26.70 ± 0.07 28.68 ± 0.08 9.97 ± 0.05 25.38 ± 0.09 26.50 ± 0.13 25.86 ± 0.07 19.34 ± 0.02 GDE [21] 85.03 ± 0.14 80.80 ± 0.02 79.82 ± 0.06 82.06 ± 0.02 79.46 ± 0.13 70.43 ± 0.17 61.92 ± 0.06 68.34 ± 0.08 68.10 ± 0.10 57.45 ± 0.07 44.38 ± 0.07 79.03 ± 0.07 57.35 ± 0.07 53.54 ± 0.08 56.74 ± 0.08 68.30 ± 0.01 AdvPerturb [23] 13.38 ± 0.12 16.92 ± 0.07 18.51 ± 0.08 5.92 ± 0.07 9.30 ± 0.16 15.03 ± 0.20 14.79 ± 0.06 18.37 ± 0.09 12.01 ± 0.04 26.75 ± 0.06 5.86 ± 0.09 17.02 ± 0.05 21.98 ± 0.12 18.40 ± 0.10 8.03 ± 0.09 14.82 ± 0.02 TENT [34] AETTA 4.88 ± 0.10 4.53 ± 0.06 5.19 ± 0.22 8.64 ± 0.11 5.99 ± 0.06 5.60 ± 0.25 4.74 ± 0.16 5.75 ± 0.20 4.81 ± 0.07 6.50 ± 0.13 5.62 ± 0.12 6.85 ± 0.13 5.43 ± 0.19 5.27 ± 0.12 5.12 ± 0.15 5.66 ± 0.05 SrcValid 50.34 ± 1.01 48.73 ± 0.91 49.54 ± 0.80 51.63 ± 0.70 50.92 ± 0.57 39.99 ± 0.75 30.20 ± 1.06 34.05 ± 0.72 36.29 ± 0.93 23.21 ± 0.69 9.74 ± 0.59 49.05 ± 0.79 25.37 ± 0.63 20.56 ± 0.80 26.64 ± 0.92 36.42 ± 0.76 SoftmaxScore [7] 12.31 ± 0.08 12.95 ± 0.12 12.55 ± 0.06 10.51 ± 0.20 10.60 ± 0.07 19.20 ± 0.17 26.02 ± 0.02 22.11 ± 0.06 20.15 ± 0.11 29.26 ± 0.11 31.42 ± 0.05 11.22 ± 0.46 27.09 ± 0.14 29.88 ± 0.08 27.14 ± 0.07 20.16 ± 0.05 GDE [21] 81.21 ± 0.16 80.02 ± 0.13 80.73 ± 0.07 76.16 ± 0.24 77.51 ± 0.44 69.92 ± 0.12 61.37 ± 0.06 65.81 ± 0.11 68.10 ± 0.07 54.61 ± 0.10 41.10 ± 0.05 74.94 ± 0.60 57.03 ± 0.17 51.96 ± 0.08 58.28 ± 0.06 66.58 ± 0.03 AdvPerturb [23] 15.44 ± 0.07 16.25 ± 0.13 15.88 ± 0.10 5.13 ± 0.06 6.00 ± 0.03 13.33 ± 0.14 13.52 ± 0.19 19.48 ± 0.04 10.13 ± 0.08 28.15 ± 0.04 6.26 ± 0.18 15.43 ± 0.63 20.22 ± 0.19 20.08 ± 0.17 6.95 ± 0.11 14.15 ± 0.06 EATA [28] AETTA 5.28 ± 0.12 4.75 ± 0.05 5.43 ± 0.05 7.19 ± 0.11 4.93 ± 0.06 4.28 ± 0.10 5.39 ± 0.18 4.53 ± 0.09 5.01 ± 0.06 11.19 ± 0.08 10.40 ± 0.13 4.79 ± 0.20 8.85 ± 0.21 10.46 ± 0.22 8.41 ± 0.07 6.73 ± 0.03 SrcValid 39.53 ± 1.22 26.07 ± 1.15 24.30 ± 1.02 33.06 ± 0.50 26.55 ± 1.30 23.04 ± 0.42 18.50 ± 0.11 24.48 ± 0.14 24.45 ± 0.62 15.04 ± 0.55 6.99 ± 0.44 32.56 ± 0.97 14.51 ± 1.34 11.73 ± 0.83 13.62 ± 0.68 22.30 ± 0.55 SoftmaxScore [7] 18.07 ± 0.26 22.21 ± 0.25 21.62 ± 0.20 13.92 ± 0.16 16.91 ± 0.29 19.36 ± 0.28 23.96 ± 0.21 20.32 ± 0.16 20.23 ± 0.08 27.65 ± 0.07 29.54 ± 0.02 11.81 ± 0.70 26.78 ± 0.17 28.10 ± 0.06 28.13 ± 0.07 21.91 ± 0.16 GDE [21] 75.10 ± 0.28 67.51 ± 0.30 68.29 ± 0.23 77.94 ± 0.10 73.27 ± 0.31 69.21 ± 0.32 62.70 ± 0.22 67.53 ± 0.16 66.89 ± 0.10 55.50 ± 0.09 45.27 ± 0.05 73.24 ± 0.29 56.67 ± 0.16 51.98 ± 0.09 54.24 ± 0.09 64.36 ± 0.15 AdvPerturb [23] 22.95 ± 0.27 30.05 ± 0.34 30.19 ± 0.24 7.97 ± 0.03 16.05 ± 0.38 17.26 ± 0.23 16.44 ± 0.14 20.51 ± 0.21 14.11 ± 0.12 28.50 ± 0.04 5.37 ± 0.20 22.67 ± 0.25 24.31 ± 0.11 20.91 ± 0.10 10.33 ± 0.17 19.17 ± 0.14 SAR [29] AETTA 5.38 ± 0.27 4.83 ± 0.05 4.67 ± 0.12 13.98 ± 0.07 9.91 ± 0.30 9.50 ± 0.15 6.54 ± 0.15 7.50 ± 0.05 5.88 ± 0.14 5.58 ± 0.11 4.96 ± 0.02 6.59 ± 0.11 4.97 ± 0.01 5.01 ± 0.06 4.94 ± 0.02 6.68 ± 0.04 SrcValid 55.20 ± 0.50 54.08 ± 0.55 54.85 ± 0.50 56.48 ± 0.54 55.35 ± 0.61 45.25 ± 0.55 34.90 ± 0.55 39.10 ± 0.60 40.10 ± 0.57 27.96 ± 0.51 10.73 ± 0.40 56.36 ± 0.45 29.61 ± 0.71 24.28 ± 0.56 31.70 ± 0.66 41.06 ± 0.54 SoftmaxScore [7] 9.92 ± 0.07 10.66 ± 0.11 10.25 ± 0.03 8.45 ± 0.02 9.39 ± 0.03 17.61 ± 0.12 25.68 ± 0.05 21.49 ± 0.12 20.60 ± 0.10 30.40 ± 0.06 35.08 ± 0.08 7.74 ± 0.18 28.08 ± 0.08 31.71 ± 0.09 27.43 ± 0.07 19.63 ± 0.01 GDE [21] 86.92 ± 0.11 85.84 ± 0.09 86.59 ± 0.05 88.22 ± 0.10 87.15 ± 0.11 77.00 ± 0.10 66.67 ± 0.08 70.86 ± 0.17 71.83 ± 0.17 59.67 ± 0.13 42.25 ± 0.07 88.09 ± 0.04 61.51 ± 0.10 56.11 ± 0.10 63.50 ± 0.14 72.81 ± 0.07 AdvPerturb [23] 11.72 ± 0.07 12.20 ± 0.09 11.85 ± 0.03 3.88 ± 0.15 4.36 ± 0.10 9.61 ± 0.09 9.48 ± 0.11 15.52 ± 0.17 7.89 ± 0.14 24.08 ± 0.08 6.72 ± 0.21 9.07 ± 0.17 15.97 ± 0.09 17.59 ± 0.15 5.98 ± 0.33 11.06 ± 0.02 CoTTA [35] AETTA 4.75 ± 0.07 4.01 ± 0.03 4.80 ± 0.07 7.44 ± 0.20 5.06 ± 0.22 4.66 ± 0.06 4.51 ± 0.13 4.42 ± 0.03 4.24 ± 0.08 8.98 ± 0.13 8.83 ± 0.09 6.46 ± 0.44 6.96 ± 0.07 8.36 ± 0.24 6.26 ± 0.19 5.98 ± 0.04 SrcValid 15.32 ± 0.10 18.63 ± 0.24 21.33 ± 0.24 7.87 ± 0.64 4.26 ± 0.10 4.70 ± 0.33 9.33 ± 0.78 7.02 ± 0.35 4.60 ± 0.14 7.37 ± 0.56 5.56 ± 0.51 19.87 ± 3.35 5.95 ± 0.65 5.32 ± 0.25 6.34 ± 0.60 9.56 ± 0.26 SoftmaxScore [7] 11.98 ± 0.07 16.57 ± 0.16 17.30 ± 0.14 13.34 ± 0.22 16.79 ± 0.35 17.72 ± 0.39 20.62 ± 0.14 17.70 ± 0.17 17.19 ± 0.08 18.56 ± 0.23 26.66 ± 0.07 7.63 ± 1.41 21.56 ± 0.24 20.65 ± 0.15 19.14 ± 0.11 17.56 ± 0.08 GDE [21] 80.31 ± 0.05 75.64 ± 0.34 75.58 ± 0.12 78.75 ± 0.45 75.50 ± 0.41 73.86 ± 0.52 70.37 ± 0.27 73.65 ± 0.16 73.73 ± 0.24 73.21 ± 0.33 60.84 ± 0.08 85.80 ± 2.29 68.56 ± 0.12 69.35 ± 0.37 71.30 ± 0.26 73.76 ± 0.22 AdvPerturb [23] 13.96 ± 0.07 18.59 ± 0.21 20.19 ± 0.19 6.39 ± 0.09 12.37 ± 0.49 10.70 ± 0.43 9.35 ± 0.04 12.77 ± 0.09 9.00 ± 0.05 12.01 ± 0.13 4.08 ± 0.04 9.25 ± 1.09 15.19 ± 0.13 7.64 ± 0.24 4.28 ± 0.21 11.05 ± 0.05 RoTTA [36] AETTA 14.33 ± 0.14 12.64 ± 0.40 9.38 ± 0.71 7.12 ± 0.97 5.32 ± 0.83 6.73 ± 0.13 9.97 ± 0.43 14.78 ± 0.30 13.99 ± 0.19 10.84 ± 0.07 12.30 ± 0.07 11.51 ± 2.42 11.92 ± 0.35 11.48 ± 1.03 15.47 ± 0.56 11.19 ± 0.12 SrcValid 28.39 ± 0.39 24.64 ± 2.41 21.33 ± 2.05 11.91 ± 0.91 6.16 ± 0.22 12.34 ± 1.06 7.23 ± 1.43 14.57 ± 1.82 8.24 ± 0.60 19.17 ± 0.97 9.67 ± 0.83 23.36 ± 1.46 11.31 ± 0.68 9.89 ± 0.84 5.92 ± 0.43 14.28 ± 0.28 SoftmaxScore [7] 19.01 ± 0.14 20.35 ± 0.14 19.30 ± 0.32 10.82 ± 0.52 10.91 ± 1.19 11.64 ± 2.29 19.47 ± 1.35 20.00 ± 0.74 19.98 ± 0.16 27.16 ± 0.13 28.53 ± 0.16 6.86 ± 0.57 25.73 ± 0.50 27.97 ± 0.24 27.39 ± 0.25 19.67 ± 0.50 GDE [21] 62.46 ± 0.16 58.40 ± 0.17 59.83 ± 0.27 68.57 ± 0.46 67.82 ± 1.14 65.51 ± 2.04 56.52 ± 1.44 56.27 ± 0.64 57.53 ± 0.16 44.74 ± 0.34 38.05 ± 0.20 63.36 ± 0.63 48.33 ± 0.37 43.13 ± 0.26 45.89 ± 0.28 55.76 ± 0.45 AdvPerturb [23] 27.73 ± 0.10 31.55 ± 0.15 30.82 ± 0.31 9.56 ± 0.42 14.54 ± 0.85 14.59 ± 1.72 17.39 ± 0.85 24.67 ± 0.45 16.20 ± 0.32 33.31 ± 0.21 5.19 ± 0.08 23.24 ± 0.93 27.07 ± 0.24 24.12 ± 0.30 12.45 ± 0.09 20.83 ± 0.39 SoTTA [12] AETTA 17.92 ± 0.25 16.51 ± 0.62 18.41 ± 0.09 20.63 ± 0.51 16.50 ± 0.53 18.76 ± 4.82 16.27 ± 4.31 18.90 ± 1.16 19.99 ± 1.58 25.06 ± 1.90 21.37 ± 1.10 19.28 ± 1.51 17.33 ± 1.16 23.02 ± 1.12 18.30 ± 0.88 19.22 ± 0.79 23Table 13. Average accuracy improvement (%p) with model recovery. Averaged over three different random seeds. t Gau. Shot Imp. Def. Gla. Mot. Zoom Snow Fro. Fog Brit. Cont. Elas. Pix. JPEG Avg.(↑) Episodic [37] -10.41 ± 0.85 -10.27 ± 0.40 -4.58 ± 0.93 22.41 ± 1.13 14.47 ± 0.72 38.62 ± 2.00 44.44 ± 2.82 41.68 ± 2.66 44.55 ± 3.61 51.44 ± 2.92 62.95 ± 1.48 61.34 ± 1.23 50.74 ± 1.07 53.72 ± 0.45 42.53 ± 0.57 33.58 ± 1.04 MRS [29] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 20.38 ± 18.28 44.40 ± 13.41 53.45 ± 1.36 59.25 ± 1.04 58.27 ± 3.48 44.38 ± 3.80 46.87 ± 5.07 34.81 ± 6.96 24.12 ± 2.11 Stochastic [35] -0.48 ± 0.48 2.40 ± 0.63 6.59 ± 0.56 20.81 ± 1.10 19.07 ± 1.31 37.15 ± 1.75 44.49 ± 2.82 43.04 ± 1.88 46.81 ± 2.67 51.22 ± 1.81 61.12 ± 1.34 59.45 ± 0.53 48.60 ± 1.16 53.83 ± 1.09 44.89 ± 1.82 35.93 ± 0.78 FisherStochastic [3] 0.45 ± 0.59 4.13 ± 0.91 7.91 ± 0.88 23.24 ± 0.83 21.79 ± 0.45 41.00 ± 1.72 47.98 ± 2.94 47.45 ± 3.31 51.60 ± 3.74 57.52 ± 3.38 65.52 ± 1.51 66.03 ± 0.45 55.12 ± 1.64 61.53 ± 0.59 52.85 ± 0.45 40.27 ± 1.29 DistShift 0.00 ± 0.00 0.52 ± 0.73 5.11 ± 0.08 23.92 ± 0.83 20.31 ± 0.73 40.73 ± 2.29 47.28 ± 2.44 46.85 ± 3.02 49.87 ± 3.51 56.18 ± 2.86 65.19 ± 1.73 63.53 ± 1.44 53.79 ± 1.10 60.17 ± 0.53 50.53 ± 0.41 38.93 ± 1.15 TENT [34] AETTA -3.27 ± 0.91 -1.57 ± 0.76 1.50 ± 0.45 23.05 ± 0.89 17.29 ± 1.24 39.66 ± 2.39 46.02 ± 2.71 44.59 ± 2.61 46.55 ± 3.93 54.21 ± 2.72 63.87 ± 1.58 62.71 ± 1.21 52.67 ± 0.81 56.91 ± 1.16 47.67 ± 1.46 36.79 ± 1.20 Episodic [37] 28.27 ± 2.76 37.60 ± 1.31 31.76 ± 0.81 63.90 ± 0.48 40.46 ± 0.52 61.49 ± 1.03 63.09 ± 0.42 52.08 ± 0.59 52.18 ± 0.62 56.85 ± 0.84 66.88 ± 0.36 63.27 ± 0.17 52.45 ± 0.56 55.18 ± 0.54 43.79 ± 0.46 51.28 ± 0.52 MRS [29] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 Stochastic [35] 1.28 ± 0.79 -0.43 ± 1.00 0.01 ± 1.27 -0.03 ± 0.76 -0.06 ± 0.75 -0.74 ± 0.46 0.03 ± 0.34 -0.20 ± 0.80 0.35 ± 0.70 0.16 ± 0.77 0.14 ± 0.89 0.10 ± 0.66 -0.21 ± 0.31 -0.21 ± 0.49 -0.29 ± 0.06 -0.01 ± 0.47 FisherStochastic [3] 1.27 ± 1.08 0.16 ± 1.79 0.55 ± 1.53 -0.09 ± 1.35 -0.30 ± 1.15 -0.44 ± 1.27 -0.14 ± 1.14 -0.07 ± 1.11 0.00 ± 1.27 0.11 ± 1.38 0.39 ± 1.54 0.47 ± 1.13 0.06 ± 0.81 0.01 ± 1.07 -0.17 ± 0.62 0.12 ± 1.16 DistShift 0.00 ± 0.00 8.72 ± 1.54 5.02 ± 1.00 34.33 ± 7.43 8.69 ± 1.25 30.29 ± 8.46 37.89 ± 1.08 18.97 ± 1.48 16.83 ± 0.86 24.31 ± 3.47 54.64 ± 4.55 43.38 ± 8.46 13.47 ± 1.74 22.46 ± 3.63 13.61 ± 2.30 22.17 ± 2.38 EATA [28] AETTA 25.52 ± 2.94 35.73 ± 1.87 27.52 ± 1.66 59.91 ± 2.41 32.61 ± 3.46 57.93 ± 1.86 60.50 ± 0.21 50.05 ± 0.52 49.48 ± 0.59 56.00 ± 0.55 64.97 ± 0.95 62.84 ± 0.28 49.02 ± 0.75 54.48 ± 1.30 43.01 ± 0.97 48.64 ± 0.74 Episodic [37] -7.28 ± 0.41 -14.44 ± 0.47 -14.37 ± 0.37 -0.14 ± 0.27 -8.55 ± 0.48 -1.79 ± 0.11 -3.65 ± 0.26 -6.93 ± 0.67 -8.78 ± 0.34 -5.24 ± 0.20 -2.85 ± 0.37 -4.94 ± 0.27 -5.93 ± 0.54 -8.75 ± 0.63 -11.33 ± 0.51 -7.00 ± 0.26 MRS [29] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 Stochastic [35] -2.16 ± 0.29 -5.38 ± 0.65 -4.81 ± 0.57 1.07 ± 0.26 -3.35 ± 0.76 0.10 ± 0.50 -0.77 ± 0.30 -1.89 ± 1.06 -3.17 ± 0.82 -1.22 ± 0.40 0.05 ± 0.45 -1.98 ± 0.21 -1.26 ± 0.06 -2.40 ± 0.91 -2.77 ± 0.92 -2.00 ± 0.48 FisherStochastic [3] -3.68 ± 0.55 -9.44 ± 0.30 -9.95 ± 0.22 0.84 ± 0.29 -6.57 ± 0.48 -0.80 ± 0.28 -2.69 ± 0.28 -5.41 ± 0.41 -7.03 ± 0.28 -3.79 ± 0.27 -1.84 ± 0.37 -3.63 ± 0.14 -4.48 ± 0.47 -6.21 ± 0.36 -8.02 ± 0.59 -4.85 ± 0.13 DistShift 0.00 ± 0.00 -6.23 ± 0.40 -7.32 ± 0.23 1.20 ± 0.05 -4.93 ± 0.43 -0.34 ± 0.27 -2.05 ± 0.31 -3.68 ± 0.37 -5.70 ± 0.31 -1.99 ± 0.27 -1.20 ± 0.34 -2.58 ± 0.19 -3.51 ± 0.43 -4.84 ± 0.41 -5.58 ± 0.38 -3.25 ± 0.10 SAR [29] AETTA -4.81 ± 0.79 -10.45 ± 0.79 -11.13 ± 0.30 0.26 ± 0.32 -7.17 ± 0.81 -1.19 ± 0.29 -3.23 ± 0.31 -6.24 ± 0.67 -8.02 ± 0.39 -4.33 ± 0.10 -2.33 ± 0.33 -4.51 ± 0.23 -4.73 ± 0.63 -7.71 ± 0.90 -9.28 ± 0.75 -5.66 ± 0.20 Episodic [37] 0.04 ± 0.15 0.04 ± 0.08 -0.16 ± 0.10 0.11 ± 0.16 0.40 ± 0.54 0.86 ± 0.14 1.09 ± 0.07 1.77 ± 0.17 2.67 ± 0.42 4.14 ± 0.60 1.77 ± 0.40 3.28 ± 0.47 3.55 ± 0.39 2.71 ± 0.24 2.44 ± 0.24 1.65 ± 0.10 MRS [29] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 Stochastic [35] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 FisherStochastic [3] 0.00 ± 0.01 -0.01 ± 0.10 0.01 ± 0.06 0.08 ± 0.08 -0.12 ± 0.12 0.10 ± 0.12 0.06 ± 0.14 0.13 ± 0.10 0.17 ± 0.18 0.25 ± 0.09 0.11 ± 0.08 0.26 ± 0.11 0.16 ± 0.13 0.39 ± 0.08 0.34 ± 0.08 0.13 ± 0.03 DistShift 0.00 ± 0.00 0.03 ± 0.12 -0.15 ± 0.11 0.06 ± 0.13 0.23 ± 0.50 0.70 ± 0.14 0.94 ± 0.06 1.58 ± 0.12 2.49 ± 0.40 3.75 ± 0.67 1.75 ± 0.39 3.03 ± 0.53 3.33 ± 0.35 2.51 ± 0.13 2.39 ± 0.34 1.51 ± 0.09 CoTTA [35] AETTA 0.04 ± 0.12 0.01 ± 0.08 -0.20 ± 0.10 0.10 ± 0.17 0.36 ± 0.54 0.84 ± 0.11 1.08 ± 0.08 1.77 ± 0.15 2.66 ± 0.40 4.12 ± 0.62 1.75 ± 0.40 3.28 ± 0.50 3.57 ± 0.40 2.71 ± 0.25 2.46 ± 0.25 1.64 ± 0.11 Episodic [37] -24.93 ± 2.06 -27.94 ± 1.45 -28.25 ± 1.39 -25.30 ± 1.05 -22.52 ± 1.12 -18.68 ± 0.41 -25.71 ± 0.90 -12.13 ± 0.62 -26.39 ± 0.71 -19.78 ± 1.40 -4.26 ± 0.02 -45.54 ± 0.81 -9.18 ± 0.69 -38.08 ± 2.90 -9.80 ± 0.68 -22.57 ± 0.85 MRS [29] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 -1.45 ± 2.50 -0.71 ± 1.24 -0.71 ± 1.23 -1.32 ± 2.29 -1.61 ± 2.79 -0.26 ± 0.45 -1.80 ± 3.24 -10.03 ± 15.10 -4.13 ± 6.75 -3.90 ± 6.62 -3.60 ± 5.15 -1.97 ± 2.23 Stochastic [35] -0.01 ± 0.05 -1.11 ± 0.41 -1.49 ± 0.48 -0.80 ± 0.40 -2.59 ± 0.80 -1.11 ± 0.44 -1.34 ± 0.14 -3.34 ± 0.16 -4.90 ± 1.24 -2.12 ± 0.61 -1.24 ± 0.42 -8.43 ± 2.16 -2.90 ± 0.73 -3.71 ± 1.02 -3.13 ± 0.14 -2.55 ± 0.49 FisherStochastic [3] 0.06 ± 0.02 -0.53 ± 0.20 -1.09 ± 0.08 -0.43 ± 0.17 -1.67 ± 0.10 -0.79 ± 0.19 -0.92 ± 0.26 -2.74 ± 0.30 -4.86 ± 0.49 -1.10 ± 0.59 -1.30 ± 0.58 -14.10 ± 3.20 -5.25 ± 1.02 -4.39 ± 0.55 -4.26 ± 0.17 -2.89 ± 0.13 DistShift 0.00 ± 0.00 -3.39 ± 0.63 -4.32 ± 0.33 2.12 ± 0.80 -3.61 ± 0.31 -1.38 ± 0.26 -4.71 ± 0.45 -7.69 ± 0.11 -16.56 ± 0.46 -4.50 ± 0.68 -5.47 ± 0.53 -34.53 ± 2.13 -7.52 ± 1.34 -11.48 ± 0.46 -11.39 ± 0.32 -7.63 ± 0.23 RoTTA [36] AETTA -1.65 ± 1.34 -4.18 ± 0.72 -21.72 ± 11.69 -0.19 ± 1.08 -4.32 ± 0.81 -2.65 ± 0.86 -4.63 ± 1.03 -4.04 ± 0.70 -7.17 ± 0.41 -4.57 ± 0.57 -0.09 ± 0.23 -9.63 ± 1.20 -4.96 ± 0.95 -13.39 ± 0.64 -7.30 ± 0.71 -6.03 ± 0.89 Episodic [37] -41.21 ± 1.71 -45.52 ± 0.96 -41.25 ± 0.82 -26.70 ± 0.44 -28.23 ± 1.84 -18.73 ± 0.68 -26.11 ± 0.25 -13.13 ± 0.53 -27.12 ± 0.69 -21.52 ± 0.24 -2.35 ± 0.35 -47.36 ± 1.01 -7.10 ± 0.90 -37.60 ± 2.26 -12.16 ± 0.42 -26.40 ± 0.51 MRS [29] 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 Stochastic [35] -1.39 ± 0.46 -1.88 ± 0.81 -1.05 ± 0.13 3.72 ± 0.28 -1.21 ± 1.64 2.05 ± 0.76 1.04 ± 0.54 0.36 ± 0.96 -0.15 ± 0.61 1.43 ± 1.19 1.48 ± 0.34 1.35 ± 0.51 -0.11 ± 0.29 0.50 ± 1.04 -0.83 ± 0.49 0.35 ± 0.51 FisherStochastic [3] -3.33 ± 1.05 -5.23 ± 0.91 -4.51 ± 0.40 4.22 ± 0.43 -4.15 ± 1.61 1.65 ± 0.71 0.42 ± 1.03 -1.37 ± 0.24 -2.01 ± 0.21 -0.24 ± 0.72 1.21 ± 0.84 -0.55 ± 1.59 -1.85 ± 0.84 -1.01 ± 1.07 -3.73 ± 0.11 -1.36 ± 0.51 DistShift 0.00 ± 0.00 -4.06 ± 0.34 -3.60 ± 0.37 6.17 ± 0.58 0.01 ± 1.65 3.94 ± 0.76 0.90 ± 0.38 1.75 ± 0.13 -0.92 ± 0.26 2.28 ± 0.42 2.83 ± 0.66 1.94 ± 0.58 0.52 ± 0.95 0.25 ± 0.82 -1.87 ± 0.54 0.68 ± 0.19 SoTTA [12] AETTA -7.58 ± 1.91 -10.51 ± 2.83 -18.71 ± 18.80 1.54 ± 0.35 -5.16 ± 1.00 0.43 ± 1.14 -2.90 ± 1.49 -2.25 ± 1.29 -6.29 ± 0.51 -2.56 ± 0.85 0.88 ± 0.32 -8.40 ± 0.60 -1.87 ± 0.78 -4.69 ± 0.23 -6.53 ± 0.75 -4.97 ± 1.58 24",
      "meta_data": {
        "arxiv_id": "2404.01351v1",
        "authors": [
          "Taeckyung Lee",
          "Sorn Chottananurak",
          "Taesik Gong",
          "Sung-Ju Lee"
        ],
        "published_date": "2024-04-01T04:21:49Z",
        "pdf_url": "https://arxiv.org/pdf/2404.01351v1.pdf",
        "github_url": "https://github.com/taeckyung/AETTA"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces AETTA, a novel label-free accuracy estimation algorithm for Test-Time Adaptation (TTA). The primary problem addressed is the susceptibility of TTA to adaptation failures in dynamic environments due to its blind reliance on unlabeled test data, and the limitations of existing performance estimation methods that require labeled data or re-training. AETTA proposes using prediction disagreement calculated by comparing the target model prediction with dropout inferences. It further refines this by introducing robust disagreement equality, which dynamically adjusts accuracy estimates to account for adaptation failures characterized by over-confident incorrect predictions. The main contributions include theoretical analysis supporting the method, extensive evaluation demonstrating AETTA's superior accuracy estimation (average 19.8%p more accurate than baselines), and a case study showcasing its practical utility in a model recovery algorithm that improved performance by 11.7%p.",
        "methodology": "AETTA's methodology is grounded in comparing a model's prediction with predictions from its dropout inferences, termed Prediction Disagreement with Dropouts (PDD). It assumes 'dropout independence' (dropout inferences simulate i.i.d. models) and 'confidence-prediction calibration' to theoretically approximate test error with PDD. To address adaptation failures, which often lead to over-confident incorrect predictions and violate standard calibration, AETTA proposes 'robust confidence-prediction calibration'. This is achieved by introducing a weighting constant 'b' that dynamically scales predicted probabilities. The constant 'b' is determined by the skewness of predicted outputs, specifically using the entropy of batch-aggregated softmax values from dropout inferences (Eavg), modeled as b = (Eavg / Emax)^(-α). The final accuracy estimation (Err) is approximated as ErrDT(h) ≈ b PDDDT(h), with C omitted due to lack of information. The process involves performing N dropout inferences, calculating PDD and Eavg, and then applying the scaling factor 'b' to estimate the error. Exponential moving average is applied for stable error estimation.",
        "experimental_setup": "AETTA was evaluated on three standard TTA benchmarks: CIFAR10-C, CIFAR100-C, and ImageNet-C, using 15 corruption types at severity level 5. Experiments were conducted in two scenarios: fully TTA (adapting to each corruption type) and continual TTA (continuously adapting to 15 corruptions). The evaluation metric was Mean Absolute Error (MAE) between estimated and ground-truth batch-wise accuracy, averaged over three random seeds. A pre-trained ResNet18 model served as the adaptation target. Six state-of-the-art TTA methods (TENT, EATA, SAR, CoTTA, RoTTA, SoTTA) were used to integrate and evaluate AETTA. AETTA was compared against four baselines: SrcValid (using labeled source data), SoftmaxScore (using average softmax confidence), GDE (Generalization Disagreement Equality, comparing current and previous adapted models), and AdvPerturb (Adversarial Perturbation, comparing adapted and source models with FGSM attacks). Hyperparameters for AETTA were N=10 (dropout inferences) and α=3 (scaling hyperparameter), selected after ablation studies. A case study on model recovery was performed, comparing AETTA's reset algorithm (based on consecutive low accuracies or sudden drops) against baselines like Episodic, MRS, Stochastic, FisherStochastic, and an oracle DistShift.",
        "limitations": "The research identifies several limitations and areas for improvement. While AETTA's use of dropout inference is computationally lightweight, the overall computational overheads associated with TTA and accuracy estimation could raise environmental concerns. More specifically to AETTA, there is room for optimization of the weighting constant 'b' (or its related 'a') for finer calibration. The current empirical approach also omits the constant 'C' from the theoretical robust disagreement equality due to insufficient information, suggesting a potential for more precise error estimates if 'C' could be estimated. Furthermore, the model recovery algorithm presented in the case study is described as heuristic and could be made more effective.",
        "future_research_directions": "Future research directions include optimizing the weighting constant 'b' (or 'a') for fine-tuning the robust confidence-prediction calibration process. Another direction is the estimation of the variable 'C' from the robust disagreement equality, which could lead to more precise error estimates. Improvements to the heuristic model recovery algorithm presented in the case study are also suggested. Beyond model recovery, the authors envision broader applications for accuracy estimation, such as enhancing model refinement and maintenance processes, and improving the dynamics of human-AI interactions. The integration of AETTA with emerging memory-economic TTA advancements is also suggested to mitigate computational demands and environmental impact.",
        "experimental_code": "def evaluate_dropout(self, feats, net, n_iter=10, dropout=0.5):\n    if net is None:\n        net = self.net\n        \n    curr_pred, curr_conf, _, _, _, curr_softmax, _ = self.model_inference(feats, net=net)\n\n    if dropout < 0:\n        if conf.args.dataset == \"cifar10outdist\":\n            dropout = 0.4\n        elif conf.args.dataset == \"cifar100outdist\":\n            dropout = 0.3\n        elif conf.args.dataset == \"imagenetoutdist\":\n            dropout = 0.2\n        elif conf.args.dataset == \"imagenetR\":\n            dropout = 0.3\n        else:\n            raise NotImplementedError\n\n    # Dropout inference sampling\n    predictions = []\n    with torch.no_grad():\n        for _ in range(n_iter):\n            pred = net[1]((net[0](feats)), dropout=dropout)  # batch_size, n_classes\n            pred = F.softmax(pred, dim=1)\n            predictions.append(pred)\n    predictions = torch.stack(predictions, dim=1)  # batch_size, n_iter, n_classes\n    pred = torch.argmax(predictions, dim=2)\n    mean = torch.mean(predictions, dim=1)\n    #mean_pred_class = torch.argmax(mean_pred, dim=1)\n    std = torch.std(predictions, dim=1)\n\n    conf_mean = mean[:, curr_pred].diagonal()\n    conf_std = std[:, curr_pred].diagonal()\n    mean_for_curr_pred = conf_mean.mean()\n    std_for_curr_pred = conf_std.mean()\n\n    total_avg_softmax = torch.mean(mean, dim=0)\n    e_avg = (-total_avg_softmax * torch.log(total_avg_softmax + 1e-6)).sum()\n\n    # Prediction disagreement with dropouts\n    match_ratio = (curr_pred.unsqueeze(dim=1).repeat(1, n_iter) == pred).sum(dim=1, dtype=float) / n_iter\n    acc = match_ratio.mean()\n    return acc.item(), mean_for_curr_pred.item(), std_for_curr_pred.item(), e_avg.item()\n\ndef aetta(self, feats, y_pred):\n    est_acc, mean, std, e_avg = self.evaluate_dropout(feats, self.net, dropout=conf.args.dropout_rate)\n    self.acc_est_json['est_dropout'] += [est_acc]\n    self.acc_est_json['est_dropout_avg_entropy'] += [e_avg]\n    self.acc_est_json['est_dropout_softmax_mean'] += [mean]\n    self.acc_est_json['est_dropout_softmax_std'] += [std]\n\n    est_err = 1 - est_acc\n    if self.est_ema_dropout is None:\n        self.est_ema_dropout = est_err\n\n    if conf.args.dataset == \"cifar10outdist\":\n        MAX_ENTROPY = 2.3026  # cifar10\n        N_CLASS = 10\n    elif conf.args.dataset == \"cifar100outdist\":\n        MAX_ENTROPY = 4.6052  # cifar100\n        N_CLASS = 100\n    elif conf.args.dataset == \"imagenetR\" :\n        MAX_ENTROPY = 5.2983  # imagenetR\n        N_CLASS = 200\n    else: # imagenet\n        MAX_ENTROPY = 6.9078\n        N_CLASS = 1000\n\n    updated = est_err / (e_avg / MAX_ENTROPY) ** 3\n    updated = max(0., min(1. - 1. / N_CLASS, updated))\n\n    updated = self.est_ema_dropout * 0.6 + updated * 0.4\n    self.est_ema_dropout = updated\n\n    self.acc_est_json['aetta'] += [100 * (1. - updated)]",
        "experimental_info": "AETTA's methodology is implemented in the `aetta` and `evaluate_dropout` functions. The core idea is to estimate the test error using Prediction Disagreement with Dropouts (PDD) and the Entropy of Batch-aggregated Softmax values from Dropout inferences (Eavg).\n\n**1. Prediction Disagreement with Dropouts (PDD) and Eavg Calculation (`evaluate_dropout` function):**\n*   **Number of Dropout Inferences (N):** 10 iterations are performed.\n*   **Dropout Rate:** The dropout probability varies based on the dataset:\n    *   0.4 for CIFAR-10-C/OutDist\n    *   0.3 for CIFAR-100-C/OutDist and ImageNet-R\n    *   0.2 for ImageNet-C/OutDist\n*   `e_avg` (Eavg) is calculated as the sum of `-(total_avg_softmax * torch.log(total_avg_softmax + 1e-6))`.\n*   `est_acc` (agreement rate) is calculated as the mean of `match_ratio`, where `match_ratio` is the element-wise agreement between the current prediction and predictions from dropout inferences.\n*   `est_err` (PDD) is then derived as `1 - est_acc`.\n\n**2. Robust Confidence-Prediction Calibration (scaling constant 'b') and Error Estimation (`aetta` function):**\n*   **Maximum Entropy (Emax) and Number of Classes (N_CLASS):** These constants are defined based on the dataset:\n    *   CIFAR-10-C/OutDist: Emax = 2.3026, N_CLASS = 10\n    *   CIFAR-100-C/OutDist: Emax = 4.6052, N_CLASS = 100\n    *   ImageNet-R: Emax = 5.2983, N_CLASS = 200\n    *   ImageNet-C/OutDist: Emax = 6.9078, N_CLASS = 1000\n*   **Scaling Constant 'b':** The estimated error (`updated`) is approximated as `est_err / (e_avg / MAX_ENTROPY) ** 3`. This implies that the exponent `α` for `b = (Eavg / Emax)^(-α)` is `3` (since `ErrDT(h) ≈ b PDDDT(h)` and `est_err` is `PDDDT(h)`).\n*   **Clamping:** The `updated` estimated error is clamped between `0` and `1 - 1/N_CLASS`.\n*   **Exponential Moving Average (EMA):** An EMA is applied to stabilize the error estimation. The update rule is `updated = self.est_ema_dropout * 0.6 + updated * 0.4`, meaning the current estimate has a weight of `0.4` and the previous EMA has a weight of `0.6`.\n\n**3. Reset Mechanism (when `conf.args.reset_function` is \"aetta\"):**\n*   The model performs a hard reset if the estimated accuracy (`100 * (1. - updated)`) falls below 20%.\n*   Additionally, if the average of the last 5 estimated accuracies (from a window of the last 10 samples) is more than 2 percentage points lower than the average of the preceding 5 estimated accuracies in that window, a hard reset is triggered. This indicates a significant drop in performance."
      }
    },
    {
      "title": "A Bayesian Perspective on Training Speed and Model Selection",
      "abstract": "We take a Bayesian perspective to illustrate a connection between training\nspeed and the marginal likelihood in linear models. This provides two major\ninsights: first, that a measure of a model's training speed can be used to\nestimate its marginal likelihood. Second, that this measure, under certain\nconditions, predicts the relative weighting of models in linear model\ncombinations trained to minimize a regression loss. We verify our results in\nmodel selection tasks for linear models and for the infinite-width limit of\ndeep neural networks. We further provide encouraging empirical evidence that\nthe intuition developed in these settings also holds for deep neural networks\ntrained with stochastic gradient descent. Our results suggest a promising new\ndirection towards explaining why neural networks trained with stochastic\ngradient descent are biased towards functions that generalize well.",
      "full_text": "A Bayesian Perspective on Training Speed and Model Selection Clare Lyle † Lisa Schut† Binxin Ru† Yarin Gal† Mark van der Wilk‡ Abstract We take a Bayesian perspective to illustrate a connection between training speed and the marginal likelihood in linear models. This provides two major insights: ﬁrst, that a measure of a model’s training speed can be used to estimate its marginal likelihood. Second, that this measure, under certain conditions, predicts the relative weighting of models in linear model combinations trained to minimize a regression loss. We verify our results in model selection tasks for linear models and for the inﬁnite-width limit of deep neural networks. We further provide encouraging empirical evidence that the intuition developed in these settings also holds for deep neural networks trained with stochastic gradient descent. Our results suggest a promising new direction towards explaining why neural networks trained with stochastic gradient descent are biased towards functions that generalize well. 1 Introduction Choosing the right inductive bias for a machine learning model, such as convolutional structure for an image dataset, is critical for good generalization. The problem of model selection concerns itself with identifying good inductive biases for a given dataset. In Bayesian inference, the marginal likelihood (ML) provides a principled tool for model selection. In contrast to cross-validation, for which computing gradients is cumbersome, the ML can be conveniently maximised using gradients when its computation is feasible. Unfortunately, computing the marginal likelihood for complex models such as neural networks is typically intractable. Workarounds such as variational inference suffer from expensive optimization of many parameters in the variational distribution and differ signiﬁcantly from standard training methods for Deep Neural Networks (DNNs), which optimize a single parameter sample from initialization. A method for estimating the ML that closely follows standard optimization schemes would pave the way for new practical model selection procedures, yet remains an open problem. A separate line of work aims to perform model selection by predicting a model’s test set performance. This has led to theoretical and empirical results connecting training speed and generalization error [17, 21]. This connection has yet to be fully explained, as most generalization bounds in the literature depend only on the ﬁnal weights obtained by optimization, rather than on the trajectory taken during training, and therefore are unable to capture this relationship. Understanding the link between training speed, optimization and generalization thus presents a promising step towards developing a theory of generalization which can explain the empirical performance of neural networks. In this work, we show that the above two lines of inquiry are in fact deeply connected. We investigate the connection between the log ML and the sum of predictive log likelihoods of datapoints, condi- tioned on preceding data in the dataset. This perspective reveals a family of estimators of the log †OATML Group, University of Oxford. Correspondence toclare.lyle@cs.ox.ac.uk ‡Imperial College London 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2010.14499v1  [cs.LG]  27 Oct 2020ML which depend only on predictions sampled from the posterior of an iterative Bayesian updating procedure. We study the proposed estimator family in the context of linear models, where we can conclusively analyze its theoretical properties. Leveraging the fact that gradient descent can produce exact posterior samples for linear models [31] and the inﬁnite-width limit of deep neural networks [7, 26], we show that this estimator can be viewed as the sum of a subset of the model’s training losses in an iterative optimization procedure. This immediately yields an interpretation of marginal likelihood estimation as measuring a notion of training speed in linear models. We further show that this notion of training speed is predictive of the weight assigned to a model in a linear model combination trained with gradient descent, hinting at a potential explanation for the bias of gradient descent towards models that generalize well in more complex settings. We demonstrate the utility of the estimator through empirical evaluations on a range of model selection problems, conﬁrming that it can effectively approximate the marginal likelihood of a model. Finally, we empirically evaluate whether our theoretical results for linear models may have explanatory power for more complex models. We ﬁnd that an analogue of our estimator for DNNs trained with stochastic gradient descent (SGD) is predictive of both ﬁnal test accuracy and the ﬁnal weight assigned to the model after training a linear model combination. Our ﬁndings in the deep learning setting hint at a promising avenue of future work in explaining the empirical generalization performance of DNNs. 2 Background and Related Work 2.1 Bayesian Parameter Inference A Bayesian model Mis deﬁned by a prior distribution over parameters θ, P(θ|M), and a prediction map from parameters θto a likelihood over the dataD, P(D|θ,M). Parameter ﬁtting in the Bayesian framework entails ﬁnding the posterior distribution P(θ|D), which yields robust and principled uncertainty estimates. Though exact inference is possible for certain models like Gaussian processes (GPs) [38], it is intractable for DNNs. Here approximations such as variational inference [4] are used [14, 5, 27, 16, 9], to improve robustness and obtain useful uncertainty estimates. Variational approximations require optimisation over the parameters of the approximate posterior distribution. This optimization over distributions changes the loss landscape, and is signiﬁcantly slower than the pointwise optimization used in standard DNNs. Pointwise optimization methods inspired by Bayesian posterior sampling can produce similar variation and uncertainty estimates as variational inference, while improving computational efﬁciency [45, 30, 29]. An appealing example of this is ensembling [25], which works by training a collection models in the usual pointwise manner, starting from kindependently initialized points. In the case of linear models, this is exactly equivalent to Bayesian inference, as this sample-then- optimize approach yields exact posterior samples [ 31, 36]. He et al. [18] extend this approach to obtain posterior samples from DNNs in the inﬁnite-width limit. 2.2 Bayesian Model Selection In addition to ﬁnding model parameters, Bayesian inference can also perform model selection over different inductive biases, which are speciﬁed through both model structure (e.g. convolutional vs fully connected) and the prior distribution on parameters. The Bayesian approach relies on ﬁnding the posterior over models P(M|D), which uses the marginal likelihood (ML) as its likelihood function: P(D|M) = ∫ θ P(D|θ)P(θ|Mi)dθ= EP(θ|M)P(D|θ) . (1) Instead of computing the full posterior, it is common to select the model with the highest marginal likelihood. This is known as type-II maximum likelihood [ 27, 28] and is less prone to overﬁtting than performing maximum likelihood over the parameters and model combined. This is because the marginal likelihood is able to trade off between model ﬁt and model complexity [39]. Maximising the ML is standard procedure when it is easy to compute. For example, in Gaussian processes it used to set simple model parameters like smoothness [38], while recent work has demonstrated that complex inductive biases in the form of invariances can also be learned [44]. For many deep models, computing Equation 1 is intractable, and obtaining approximations that are accurate enough for model selection and that scale to complex models is an active area of research [23]. In general, variational lower bounds that scale are too loose when applied to DNNs [5]. Deep 2Gaussian processes provide a case where the bounds do work [6, 8], but heavy computational load holds performance several years behind deep learning. While ensembling methods provide useful uncertainty estimates and improve the computational efﬁciency of the variational approach, they have not yet provided a solution for Bayesian model selection. 2.3 Generalization and Risk Minimization Bayesian model selection addresses a subtly different problem from the risk minimization framework used in many learning problems. Nonetheless, the two are closely related; Germain et al. [15] show that in some cases optimizing a PAC-Bayesian risk bound is equivalent to maximizing the marginal likelihood of a Bayesian model. In practice, maximizing an approximation of the marginal likelihood in DNNs trained with SGD can improve generalization performance [41]. More recently, Arora et al. [1] computed a data-dependent complexity measure which resembles the data-ﬁt term in the marginal likelihood of a Bayesian model and which relates to optimization speed, hinting at a potential connection between the two. At the same time, generalization in deep neural networks (DNNs) remains mysterious, with classical learning-theoretic bounds failing to predict the impressive generalization performance of DNNs [47, 33]. Recent work has shown that DNNs are biased towards functions that are ‘simple’, for various deﬁnitions of simplicity [22, 13, 43, 42]. PAC-Bayesian generalization bounds, which can quantify a broad range of deﬁnitions of complexity, can attain non-vacuous values [32, 10, 11], but nonetheless exhibit only modest correlation with generalization error [21]. These bounds depend only on the ﬁnal distribution over parameters after training; promising alternatives consider properties of the trajectory taken by a model during optimization [17, 35]. This trajectory-based perspective is a promising step towards explaining the correlation between the number of training steps required for a model to minimize its objective function and its ﬁnal generalization performance observed in a broad range of empirical analyses [21, 3, 34, 40]. 3 Marginal Likelihood Estimation with Training Statistics In this section, we investigate the equivalence between the marginal likelihood (ML) and a notion of training speed in models trained with an exact Bayesian updating procedure. For linear models and inﬁnitely wide neural networks, exact Bayesian updating can be done using gradient descent optimisation. For these cases, we derive an estimator of the marginal likelihood which 1) is related to how quickly a model learns from data, 2) only depends on statistics that can be measured during pointwise gradient-based parameter estimation, and 3) becomes tighter for ensembles consisting of multiple parameter samples. We also investigate how gradient-based optimization of a linear model combination can implicitly perform approximate Bayesian model selection in Section 3.3. 3.1 Training Speed and the Marginal Likelihood Let Ddenote a dataset of the form D = ( Di)n i=1 = ( xi,yi)n i=1, and let D<i = ( Dj)i−1 j=1 with D<1 = ∅. We will abbreviate P(D|M) := P(D) when considering a single model M. Observe that P(D) = ∏n i=1 P(Di|D<i) to get the following form of the log marginal likelihood: log P(D) = log n∏ i=1 P(Di|D<i) = n∑ i=1 log P(Di|D<i) = n∑ i=1 log[EP(θ|D<i)P(Di|θ)]. (2) If we deﬁne training speed as the number of data points required by a model to form an accurate posterior, then models which train faster – i.e. whose posteriors assign high likelihood to the data after conditioning on only a few data points – will obtain a higher marginal likelihood. Interpreting the negative log posterior predictive probability log P(Di|D<i) of each data point as a loss function, the log ML then takes the form of the sum over the losses incurred by each data point during training, i.e. the area under a training curve deﬁned by a Bayesian updating procedure. 3.2 Unbiased Estimation of a Lower Bound In practice, computing log P(Di|D<i) may be intractable, necessitating approximate methods to estimate the model evidence. In our analysis, we are interested in estimators of log P(D) computed 3by drawing ksamples of θ ∼P(θ|D<i) for each i = 1,...,n . We can directly estimate a lower bound L(D) = ∑n i=1 E[log P(Di|D<i) using the log likelihoods of these samples ˆL(D) = n∑ i=1 1 k k∑ j=1 log P(Di|θi j). (3) This will produce a biased estimate of the log marginal likelihood due to Jensen’s inequality. We can get a tighter lower bound by ﬁrst estimating E[log P(Di|θ)] using our posterior samples before applying the logarithm, obtaining ˆLk(D) = n∑ i=1 log 1 k k∑ j=1 P(Di|θi j). (4) Proposition 3.1. Both ˆLand ˆLk as deﬁned in Equation 4 are estimators of lower bounds on the log marginal likelihood; that is E[ ˆL(D)] = L(D) ≤log P(D) and E[ ˆLk(D)] = Lk(D) ≤log P(D) . (5) Further, the bias term in Lcan be quantiﬁed as follows. L(D) = log P(D) − n∑ i=1 KL(P(θ|D<i)||P(θ|D<i+1)) (6) We include the proof of this and future results in Appendix A. We observe that both lower bound estimators exhibit decreased variance when using multiple posterior samples; however, ˆLk also exhibits decreasing bias (with respect to the log ML) as kincreases; each kdeﬁnes a distinct lower bound Lk = E[ ˆLk] on log P(D). The gap induced by the lower bound L(D) is characterized by the information gain each data point provides to the model about the posterior, as given by the Kullback-Leibler (KL) divergence [24] between the posterior at time iand the posterior at time i+ 1. Thus, while Lhas a Bayesian interpretation it is arguably more closely aligned with the minimum description length notion of model complexity [19]. When the posterior predictive distribution of our model is Gaussian, we consider a third approach which, unlike the previous two methods, also applies to noiseless models. Let D= (Xi,Yi)n i=1, and (θi j)k j=1 be kparameter samples from P(θ|D<i). We assume a mapping f : Θ ×X →Y such that sampling parameters θ and computing f(θ,Xi) is equivalent to sampling from the posterior P(·|D<i,Xi). We can then obtain the following estimator of a lower bound on log P(D). Proposition 3.2. Let P(Yi|D<i,Xi) = N(µi,σ2 i) for some µi,σ2 i. Deﬁne the standard mean and variance estimators ˆµi = 1 N ∑N j=1 f(θi j,xi) and ˆσ2 i = 1 N−1 ∑(f(θi j,xi) −ˆµ)2. Then the estimator ˆLS(D) = n∑ i=1 log P(Yi|ˆµi,ˆσ2 i) (7) is a lower bound on the log ML: i.e. E[ ˆLS(D)] ≤log P(D). We provide an empirical evaluation of the rankings provided by the different estimators in Section 4. We ﬁnd that ˆLS exhibits the least bias in the presence of limited samples from the posterior, though we emphasize its limitation to Gaussian posteriors; for more general posterior distributions, ˆLk minimizes bias while still estimating a lower bound. 3.2.1 Lower bounds via gradient descent trajectories The bounds on the marginal likelihood we introduced in the previous section required samples from the sequence of posteriors as data points were incrementally added p(θ|D<i). Ensembles of linear models trained with gradient descent yield samples from the model posterior. We now show that we can use these samples to estimate the log ML using the estimators introduced in the previous section. We will consider the Bayesian linear regression problem of modelling dataD= (Xi,Yi)n i=1 assumed to be generated by the process Y = θ⊤Φ(X) + ϵ∼N(0,σ2 NI) for some unknown θ, known σ2 N, 4and feature map Φ. Typically, a Gaussian prior is placed onθ; this prior is then updated as data points are seen to obtain a posterior over parameters. In the overparmeterised, noiseless linear regression setting, Matthews et al. [31] show that the distribution over parameters θobtained by sampling from the prior on θ0 and running gradient descent to convergence on the dataD<i is equivalent to sampling from the posterior conditioned on D<i. Osband et al. [36] extend this result to posteriors which include observation noise σ2 N ̸= 0 under the assumption that the targets Yi are themselves noiseless observations. Algorithm 1: Marginal Likelihood Estimation for Linear Models Input: A dataset D= (xi,yi)n i=1, parameters µ0,σ2 0,σ2 N Result: An estimate of L(D) θt ←θ0 ∼N(µ0,σ2 0); ˜Y ←Y + ϵ∼N(0,σ2 N); sumLoss ←0 ; ℓ(D≤i,w) ←∥˜Y≤i −θ⊤X≤i∥2 2 + σ2 N θ2 0 ∥θ−θ0∥2 2; for Di ∈D do sumLoss = sumLoss + (θ⊤ t xi−yi)2 2σ2 N ; θt ←GradientDescent(ℓ,θt,D≤i) ; end return sumLoss We can use this procedure to obtain posterior samples for our estimators by iteratively running sample- then-optimize on the sets D<i. Algorithm 1 outlines our approach, which uses sample-then-optimize on iterative subsets of the data to obtain the necessary posterior samples for our estimator. Theorem 3.3 shows that this procedure yields an unbiased estimate of L(D) when a single prior sample is used, and an unbiased estimate of Lk(D) when an ensemble of kmodels are trained in parallel. Theorem 3.3. Let D= (Xi,Yi)n i=1 and let (θi j)n,J i,j=1 be generated by the procedure outlined above. Then the estimators ˆL, ˆLS,and ˆLk, applied to the collection (θi j), are lower bounds on log P(D). Further, expressing −log P(Di|θ) as the ℓ2 regression loss plus a constant, we then obtain log P(D) ≥ n∑ i=1 Eθi∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,θi) + c= L(D) (8) We highlight that Theorem 3.3 precisely characterizes the lower bound on the marginal likelihood as a sum of ‘training losses’ based on the regression lossℓ2(Di,θi). 3.2.2 From Linear Models to Inﬁnite Neural Networks Beyond linear models, our estimators can further perform model selection in the inﬁnite-width limit of neural networks. Using the optimization procedure described by He et al. [18], we can obtain an exact posterior sample from a GP given by the neural tangent kernel [20]. The iterative training procedure described in Algorithm 1 will thus yield a lower bound on the marginal likelihood of this GP using sampled losses from the optimization trajectory of the neural network. We evaluate this bound in Section 4, and formalize this argument in the following corollary. Corollary 3.4. Let Dbe a dataset indexed by our standard notation. Let f0 be sampled from an inﬁnitely wide neural network architecture Funder some initialization distribution, and let fi ∞be the limiting solution under the training dynamics deﬁned by He et al. [18] applied to the initialization f0 and using data D<i. Let K∞denote the neural tangent kernel for F, and M= GP(0,K∞) the induced Gaussian Process. Then fi ∞∼P(f|D<i,M), and in the limit of inﬁnite training time, the iterative sample-then-optimize procedure yields an unbiased estimate of L(D|M). Letting ℓ2 denote the scaled squared ℓ2 regression loss and cbe a constant, we obtain as a direct corollary of Theorem 3.3 P(D) ≥Efi∞∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,fi) + c= L(D) . (9) This result provides an additional view on the link between training speed and generalisation in wide neural networks noted by Arora et al. [1], who analysed the convergence of gradient descent. They 5compute a PAC generalization bound which a features the data complexity term equal to that in the marginal likelihood of a Gaussian process Rasmussen [38]. This term provides a bound on the rate of convergence of gradient descent, whereas our notion of training speed is more closely related to sample complexity and makes the connection to the marginal likelihood more explicit. It is natural to ask if such a Bayesian interpretation of the sum over training losses can be extended to non-linear models trained with stochastic gradient descent. Although SGD lacks the exact posterior sampling interpretation of our algorithm, we conjecture a similar underlying mechanism connecting the sum over training losses and generalization. Just as the marginal likelihood measures how well model updates based on previous data points generalize to a new unseen data point, the sum of training losses measures how well parameter updates based on one mini-batch generalize to the rest of the training data. If the update generalizes well, we expect to see a sharper decrease in the training loss, i.e. for the model to train more quickly and exhibit a lower sum over training losses. This intuition can be related to the notion of ‘stiffness’ proposed by Fort et al.[12]. We provide empirical evidence supporting our hypothesis in Section 4.2. 3.3 Bayesian Model Selection and Optimization The estimator L(D) reveals an intriguing connection between pruning in linear model combinations and Bayesian model selection. We assume a data set D= (Xi,Yi)n i=1 and a collection of kmodels M1,..., Mk. A linear regressor wis trained to ﬁt the posterior predictive distributions of the models to the target Yi; i.e. to regress on the dataset (Φ,Y ) = ( φi = ( ˆYi 1 ,..., ˆYi n),Yi )n i=1 with ˆYi j ∼P( ˆY|D<i,Xi,Mj). (10) The following result shows that the optimal linear regressor on this data generating distribution assigns the highest weight to the model with the highest L(D) whenever the model errors are independent. This shows that magnitude pruning in a linear model combination is equivalent to approximate Bayesian model selection, under certain assumptions on the models. Proposition 3.5. Let M1,..., Mk be Bayesian linear regression models with ﬁxed noise variance σ2 N and Gaussian likelihoods. Let Φ be a (random) matrix of posterior prediction samples, of the form Φ[i,j] = ˆyj i ∼P(yj|D<j,xj,Mi). Suppose the following two conditions on the columns of Φ are satisﬁed: E⟨Φ[:,i],y⟩= E⟨Φ[:,j],y⟩for all i,j, and E⟨Πy⊥φi,Πy⊥φj⟩= 0. Let w∗denote the least-squares solution to the regression problem minwEΦ∥Φw−y∥2. Then the following holds arg max i w∗ i = arg max i L(D|Mi) ∀w∗= arg min w E∥Φw−y∥2 . (11) The assumption on the independence of model errors is crucial in the proof of this result: families of models with large and complementary systematic biases may not exhibit this behaviour. We observe in Section 4 that the conditions of Proposition 1 are approximately satisﬁed in a variety of model comparison problems, and running SGD on a linear combination of Bayesian models still leads to solutions that approximate Bayesian model selection. We conjecture that analogous phenomena occur during training within a neural network. The proof of Proposition 3.5 depends on the observation that, given a collection of features, the best least-squares predictor will assign the greatest weight to the feature that best predicts the training data. While neural networks are not linear ensembles of ﬁxed models, we conjecture that, especially for later layers of the network, a similar phenomenon will occur wherein weights from nodes that are more predictive of the target values over the course of training will be assigned higher magnitudes. We empirically investigate this hypothesis in Section 4.2. 4 Empirical Evaluation Section 3 focused on two key ideas: that training statistics can be used as an estimator for a Bayesian model’s marginal likelihood (or a lower bound thereof), and that gradient descent on a linear ensemble implicitly arrives at the same ranking as this estimator in the inﬁnite-sample, inﬁnite-training-time limit. We further conjectured that similar phenomena may also hold for deep neural networks. We now illustrate these ideas in a range of settings. Section 4.1 provides conﬁrmation and quantiﬁcation of our results for linear models, the model class for which we have theoretical guarantees, while Section 4.2 provides preliminary empirical conﬁrmation that the mechanisms at work in linear models also appear in DNNs. 6Figure 1: Left: ranking according to log P(D), L(D) with exact posterior samples, and L(D) computed on samples generated by gradient descent. Right: gap between true marginal likelihood and Lk(D) estimator shrinks as a function of kfor both exact and gradient descent-generated samples. 4.1 Bayesian Model Selection While we have shown that our estimators correspond to lower bounds on the marginal likelihood, we would also like the relative rankings of models given by our estimator to correlate with those assigned by the marginal likelihood. We evaluate this correlation in a variety of linear model selection problems. We consider three model selection problems; for space we focus on one, feature dimension selection, and provide full details and evaluations on the other two tasks in Appendix B.1. For the feature dimension selection task, we construct a synthetic dataset inspired by Wilson and Izmailov [46] of the form (X,y), where xi = (yi+ ϵ1,yi+ ...,y i+ ϵ15,ϵ16,...,ϵ 30), and consider a set of models {Mk}with feature embeddings φk(xi) = xi[1,...,k ]. The optimal model in this setting is the one which uses exactly the set of ‘informative’ featuresx[1,..., 15]. We ﬁrst evaluate the relative rankings given by the true marginal likelihood with those given by our estimators. We compare LS, Land Lk; we ﬁrst observe that all methods agree on the optimal model: this is a consistent ﬁnding across all of the model selection tasks we considered. While all methods lower bound the log marginal likelihood,Lk(D) and LS(D) exhibit a reduced gap compared to the naive lower bound. In the rightmost plot of Figure 1, we further quantify the reduction in the bias of the estimator Lk(D) described in Section 3. We use exact posterior samples (which we denote in the ﬁgure simply as posterior samples) and approximate posterior samples generated by the gradient descent procedure outlined in Algorithm 1 using a ﬁxed step size and thus inducing some approximation error. We ﬁnd that both sampling procedures exhibit decreasing bias as the number of samples kis increased, with the exact sampling procedure exhibiting a slightly smaller gap than the approximate sampling procedure. We next empirically evaluate the claims of Proposition 3.5 in settings with relaxed assumptions. We compare the ranking given by the true log marginal likelihood, the estimated L(D), and the weight assigned to each model by the trained linear regressor. We consider three variations on how sampled predictions from each model are drawn to generate the features φi: sampling the prediction for point ˆYi from P( ˆYi|D<i) (‘concurrent sampling’ – this is the setting of Proposition 3.5), as well as two baselines: the posterior P( ˆYi|D) (‘posterior sampling’), and the priorP( ˆYi) (‘prior sampling’). We ﬁnd that the rankings of the marginal likelihood, its lower bound, and of the ranking given by concurrent optimization all agree on the best model in all three of the model selection problems outlined previously, while the prior and posterior sampling procedure baselines do not exhibit a consistent ranking with the log ML. We visualize these results for the feature dimension selection problem in Figure 2; full results are shown in Figure 5. We further illustrate how the L(D) estimator can select inductive biases in the inﬁnite-width neural network regime in Figure 2. Here we evaluate the relative change in the log ML of a Gaussian Process induced by a fully-connected MLP (MLP-NTK-GP) and a convolutional neural network (Conv-NTK-GP) which performs regression on the MNIST dataset. The fully-connected model sees a consistent decrease in its log ML with each additional data point added to the dataset, whereas the convolutional model sees the incremental change in its log ML become less negative as more data 7Figure 2: Left: Relative rankings given by optimize-then-prune, ML, and estimated L(D) on the feature selection problem. Right: visualizing the interpretation of L(D) as the ‘area under the curve’ of training losses: we plot the relative change in the estimator L(D≤i) −L(D<i) for convolutional and fully-connected NTK-GP models, and shade their area. Figure 3: Linear combinations of DNNs on FashionMNIST trained. Left: ensemble weights versus the test loss for concurrent training. Middle: sum over training losses (SOTL), standardized by the number of training samples, versus test loss for parallel training. Right: training curves for the different models trained in parallel. All results are averaged over 10 runs, and standard deviations are shown by the shaded regions around each observation. The model parameters, given in the parentheses, are the number of layers (l), nodes per layer (n) and kernel size (k), respectively. points are added as a result of its implicit bias, as well as a much higher incremental change in its log ML from the start of training. This leads to the Conv-NTK-GP having a higher value for L(D) than the MLP-NTK-GP. We provide an analogous plot evaluatinglog P(D) in the appendix. 4.2 Training Speed, Ensemble Weight, and Generalization in DNNs We now address our conjectures from Section 3, which aim to generalize our results for linear models to deep neural networks trained with SGD. Recall that our hypothesis involves translatingiterative posterior samples to minibatch training losses over an SGD trajectory, and bayesian model evidence to generalization error; we conjectured that just as the sum of the log posterior likelihoods is useful for Bayesian model selection, the sum of minibatch training losses will be useful to predict generalization error. In this section, we evaluate whether this conjecture holds for a simple convolutional neural network trained on the FashionMNIST dataset. Our results provide preliminary evidence in support of this claim, and suggest that further work investigating this relationship may reveal valuable insights into how and why neural networks generalize. 4.2.1 Linear Combination of DNN Architectures We ﬁrst evaluate whether the sum over training losses (SOTL) obtained over an SGD trajectory correlates with a model’s generalization error, and whether SOTL predicts the weight assigned to a model by a linear ensemble. To do so, we train a linear combination of DNNs with SGD to determine whether SGD upweights NNs that generalize better. Further details of the experiment can be found in Appendix B.2. Our results are summarized in Figure 3. 8Figure 4: Weight assigned to subnetwork by SGD in a deep neural network (x-axis) versus the subnet- work performance (estimated by the sum of cross-entropy, on the y-axis) for different FashionMNIST classes. The light blue ovals denote depict 95% conﬁdence intervals, estimated over 10 seeds (i.e. 2σ for both the weight and SOTL). The orange line depicts the general trend. We observe a strong correlation between SOTL and average test cross-entropy (see Figure 3 middle column), validating that the SOTL is correlated with generalization. Further, we ﬁnd that architectures with lower test error (when trained individually) are given higher weight by the linear ensembling layer – as can be seen from the left plot in Figure 3. This supports our hypothesis that SGD favours models that generalize well. 4.2.2 Subnetwork Selection in Neural Networks Finally, we evaluate whether our previous insights apply to submodels within a neural network, suggesting a potential mechanism which may bias SGD towards parameters with better generalization performance. Based on the previous experiments, we expect that nodes that have a lower sum over training errors (if evaluated as a classiﬁer on their own) are favoured by gradient descent and therefore have a larger ﬁnal weight than those which are less predictive of the data. If so, we can then view SGD followed by pruning (in the ﬁnal linear layer of the network) as performing an approximation of a Bayesian model selection procedure. We replicate the model selection problem of the previous setting, but replace the individual models with the activations of the penultimate layer of a neural network, and replace the linear ensemble with the ﬁnal linear layer of the network. Full details on the experimental set-up can be found in Appendix B.3. We ﬁnd that our hypotheses hold here: SGD assigns larger weights to subnetworks that perform well, as can be seen in Figure 4. This suggests that SGD is biased towards functions that generalize well, even within a network. We ﬁnd the same trend holds for CIFAR-10, which is shown in Appendix B.3. 5 Conclusion In this paper, we have proposed a family of estimators of the marginal likelihood which illustrate the connection between training speed and Bayesian model selection. Because gradient descent can produce exact posterior samples in linear models, our result shows that Bayesian model selection can be done by training a linear model with gradient descent and tracking how quickly it learns. This approach also applies to the inﬁnite-width limit of deep neural networks, whose dynamics resemble those of linear models. We further highlight a connection between magnitude-based pruning and model selection, showing that models for which our lower bound is high will be assigned more weight by an optimal linear model combination. This raises the question of whether similar mechanisms exist in ﬁnitely wide neural networks, which do not behave as linear models. We provide preliminary empirical evidence that the connections shown in linear models have predictive power towards explaining generalization and training dynamics in DNNs, suggesting a promising avenue for future work. 96 Broader Impact Due to the theoretical nature of this paper, we do not foresee any immediate applications (positive or negative) that may arise from our work. However, improvement in our understanding of generalization in deep learning may lead to a host of downstream impacts which we outline brieﬂy here for completeness, noting that the marginal effect of this paper on such broad societal and environmental impacts is likely to be very small. 1. Safety and robustness. Developing a stronger theoretical understanding of generalization will plausibly lead to training procedures which improve the test-set performance of deep neural networks. Improving generalization performance is crucial to ensuring that deep learning systems applied in practice behave as expected based on their training performance. 2. Training efﬁciency and environmental impacts. In principle, obtaining better estimates of model and sub-model performance could lead to more efﬁcient training schemes, thus potentially reducing the carbon footprint of machine learning research. 3. Bias and Fairness. The setting of our paper, like much of the related work on generalization, does not consider out-of-distribution inputs or training under constraints. If the training dataset is biased, then a method which improves the generalization performance of the model under the i.i.d. assumption will be prone to perpetuating this bias. Acknowledgements Lisa Schut was supported by the Accenture Labs and Alan Turing Institute. 10References [1] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019. [2] D. Basu. On statistics independent of a complete sufﬁcient statistic. Sankhy¯a: The Indian Journal of Statistics (1933-1960), 15(4):377–380, 1955. ISSN 00364452. URL http://www. jstor.org/stable/25048259. [3] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018. [4] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017. [5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pages 1613–1622, 2015. [6] Andreas Damianou and Neil Lawrence. Deep gaussian processes. volume 31 of Proceedings of Machine Learning Research, pages 207–215, Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR. URL http://proceedings.mlr.press/v31/damianou13a.html. [7] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=H1-nGgWC-. [8] Vincent Dutordoir, Mark van der Wilk, Artem Artemev, and James Hensman. Bayesian image classiﬁcation with deep convolutional gaussian processes. volume 108 of Proceedings of Machine Learning Research, pages 1529–1539, Online, 26–28 Aug 2020. PMLR. URL http://proceedings.mlr.press/v108/dutordoir20a.html. [9] David Duvenaud, Dougal Maclaurin, and Ryan Adams. Early stopping as nonparametric variational inference. In Artiﬁcial Intelligence and Statistics, pages 1070–1077, 2016. [10] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. [11] Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent PAC-Bayes priors via differential privacy. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, NeurIPS 31, pages 8430–8441. 2018. [12] Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019. [13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=rJl-b3RcF7. [14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Ininternational conference on machine learning, pages 1050–1059, 2016. [15] Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. PAC-Bayesian theory meets Bayesian inference. In Advances in Neural Information Processing Systems, pages 1884–1892, 2016. [16] Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors,Advances in Neural Information Process- ing Systems 24, pages 2348–2356. Curran Associates, Inc., 2011. URL http://papers.nips. cc/paper/4329-practical-variational-inference-for-neural-networks.pdf . [17] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent, 2015. [18] Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the neural tangent kernel. arXiv preprint arXiv:2007.05864, 2020. 11[19] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5–13, 1993. [20] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571–8580, 2018. [21] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to ﬁnd them, 2019. [22] Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. In Advances in Neural Information Processing Systems, pages 3491–3501, 2019. [23] Mohammad Emtiyaz E Khan, Alexander Immer, Ehsan Abedi, and Maciej Ko- rzepa. Approximate inference turns deep networks into gaussian processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32 , pages 3094– 3104. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/ 8573-approximate-inference-turns-deep-networks-into-gaussian-processes. pdf. [24] S. Kullback and R. A. Leibler. On information and sufﬁciency. Ann. Math. Statist., 22(1): 79–86, 03 1951. doi: 10.1214/aoms/1177729694. URL https://doi.org/10.1214/aoms/ 1177729694. [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems, pages 6402–6413, 2017. [26] Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z. [27] David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992. [28] David JC MacKay.Information theory, inference and learning algorithms. Cambridge university press, 2003. [29] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems, pages 13132–13143, 2019. [30] Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research, 18(1):4873–4907, 2017. [31] Alexander G de G Matthews, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Sample- then-optimize posterior sampling for bayesian linear models. Neural Information Processing Systems, 2017. [32] David A. McAllester. Some PAC-Bayesian Theorems. Machine Learning, 37(3):355–363, 1999. [33] Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 11615–11626. Curran Associates, Inc., 2019. [34] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019. [35] Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy. Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances in Neural Information Processing Systems, pages 11015–11025, 2019. 12[36] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep rein- forcement learning. In Advances in Neural Information Processing Systems, pages 8617–8629, 2018. [37] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 1177–1184, 2008. [38] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine Learning, pages 63–71. Springer, 2003. [39] Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. In Advances in neural information processing systems, pages 294–300, 2001. [40] Binxin Ru, Clare Lyle, Lisa Schut, Mark van der Wilk, and Yarin Gal. Revisiting the train loss: an efﬁcient performance estimator for neural architecture search, 2020. [41] Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2017. [42] Samuel L. Smith and Quoc V . Le. A bayesian perspective on generalization and stochastic gradient descent. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=BJij4yg0Z. [43] Guillermo Valle-Pérez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018. [44] M. van der Wilk, M. Bauer, S. John, and J. Hensman. Learning Invariances using the Marginal Likelihood. arXiv e-prints, August 2018. _eprint: 1808.05563. [45] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11) , pages 681–688, 2011. [46] Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. arXiv preprint arXiv:2002.08791, 2020. [47] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. 13A Proofs of Theoretical Results Proposition 3.1. Both ˆLand ˆLk as deﬁned in Equation 4 are estimators of lower bounds on the log marginal likelihood; that is E[ ˆL(D)] = L(D) ≤log P(D) and E[ ˆLk(D)] = Lk(D) ≤log P(D) . (5) Further, the bias term in Lcan be quantiﬁed as follows. L(D) = log P(D) − n∑ i=1 KL(P(θ|D<i)||P(θ|D<i+1)) (6) Proof. The result for Lfollows from a straightforward derivation: L(D) = ∑∫ log P(Di|θ)dP(θ|D<i) (12) = ∑∫ log[P(Di|θ)P(θ|D<i)P(Di|D<i) P(θ|D<i)P(Di|D<i) ]dP(θ|D<i) (13) = ∑∫ log P(θ|D≤i)) P(θ|D<i) dP(θ|D<i) + ∑ log P(Di|D<i) (14) = ∑( log P(Di|D<i) −KL(P(θ|D<i)||P(θ|D≤i)) ) (15) = log P(D) − n∑ i=1 KL(P(θ|D<i)||P(θ|D≤i)). (16) The result for ˆLk follows immediately from Jensen’s inequality, yielding ∑ E[log k∑ j=1 1 kp(Di|θj)] ≤ ∑ log E[ k∑ j=1 1 kp(Di|θj)] = ∑ log E[p(Di|θj)] = log P(D) . (17) Because Lk applies Jensen’s inequality to a random variable with decreasing variance as a function of k, we expect the bias of Lk to decrease as kgrows, an observation characterized in Section 4. Proposition 3.2. Let P(Yi|D<i,Xi) = N(µi,σ2 i) for some µi,σ2 i. Deﬁne the standard mean and variance estimators ˆµi = 1 N ∑N j=1 f(θi j,xi) and ˆσ2 i = 1 N−1 ∑(f(θi j,xi) −ˆµ)2. Then the estimator ˆLS(D) = n∑ i=1 log P(Yi|ˆµi,ˆσ2 i) (7) is a lower bound on the log ML: i.e. E[ ˆLS(D)] ≤log P(D). Proof. To show that the sum of the estimated log likelihoods is a lower bound on the log marginal likelihood, it sufﬁces to show that each term in the sum of the estimates is a lower bound on the corresponding term in log marginal likelihood expression. Thus, without loss of generality we consider a single data point Di = (x,y) and posterior distribution p(y|x,D<i) = N(µ,σ2). Let y∈R, ˆµ,ˆσthe standard estimators for sample mean and variance given sample ˆY ∈Rk sampled from N(µ,σ2). We want to show EˆY∼N(µ,σ2)[ln p(y|ˆµ,ˆσ2)] ≤ln p(y|µ,σ2). (18) We ﬁrst note that ˆµ( ˆY) ⊥ˆσ( ˆY) for ˆY a collection of i.i.d. Gaussian random variables [ 2]. We also take advantage of the fact that the log likelihood of a Gaussian is concave with respect to itsµ parameter and its σ2 parameter. Notably, the log likelihood is not concave w.r.t. the joint pair (µ,σ2), but because the our estimators are independent, this will not be a problem for us. We proceed as follows by ﬁrst decomposing the expectation over the samples ˆY into an expectation over ˆµand ˆσ2 EX∼N(µ,σ2)[ln p(y|ˆµ,ˆσ2)] = Eˆµ,Y2,...,YN ln p(y|ˆµ,ˆσ2) (19) = EˆµEˆσ2 ln p(y|ˆµ,ˆσ2) (20) 14We apply Jensen’s inequality ﬁrst to the inner expectation, then to the outer. ≤Eˆµln p(y|ˆµ,E[ˆσ2]) = Eˆµln p(y|ˆµ,σ2) (21) ≤ln p(y|µ,σ2) (22) So we obtain our lower bound. Theorem 3.3. Let D= (Xi,Yi)n i=1 and let (θi j)n,J i,j=1 be generated by the procedure outlined above. Then the estimators ˆL, ˆLS,and ˆLk, applied to the collection (θi j), are lower bounds on log P(D). Further, expressing −log P(Di|θ) as the ℓ2 regression loss plus a constant, we then obtain log P(D) ≥ n∑ i=1 Eθi∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,θi) + c= L(D) (8) Proof. The heavy lifting for this result has largely been achieved by Propositions 3.1 and 3.2, which state that provided the samples θi j are distributed according to the posterior, the inequalities will hold. It therefore remains only to show that the sample-then-optimize procedure yields samples from the posterior. The proof of this result can be found in Lemma 3.8 of Osband et al. [36], who show that the optimum for the gradient descent procedure described in Algorithm 1 does indeed correspond to the posterior distribution for each subset D<i. Finally, it is straightforward to express the lower bound estimator ˆLas the sum of regression losses. We obtain this result by showing that the inequality holds for each term log P(Di|θi) in the summation. log P(Di|θ) = log[exp ( −(θ⊤xi −yi)2 2σ2 ) 1√ 2πσ] (23) = −(θ⊤xi −yi)2 2σ2 −1 2 log(2πσ2) (24) = c1ℓ2(Di,θ) + c2 (25) We note that in practice, the solutions found by gradient descent for ﬁnite step size and ﬁnite number of steps will not necessarily correspond to the exact local optimum. However, it is straightforward to bound the error obtained from this approximate sampling in terms of the distance of θfrom the optimum θ∗. Denoting the difference |θ−θ∗|by δ, we get |log P(Di|θ∗) −log P(Di|θ)|= |((θ∗)⊤xi −yi)2 2σ2 −((θ)⊤xi −yi)2 2σ2 | (26) ≤ 1 2σ2 |(θ∗)⊤xi −θ⊤xi|2 (27) ≤|((θ∗)⊤xi)2 −(θ⊤xi)2|+ |2y||θ⊤x−(θ∗)⊤x| (28) ≤|(θ∗−θ)⊤x+ 2((θ∗)⊤x)((θ∗−θ)⊤x)|+ |2y||θ⊤x−(θ∗)⊤x| (29) ≤|θ∗−θ||x|+ 2|θ∗x||θ∗−θ||x|+ |2y||x||θ−θ∗| (30) and so the error in the estimate of log P(D|θ) will be proportional to the distance |θ−θ∗|induced by the approximate optimization procedure. Corollary 3.4. Let Dbe a dataset indexed by our standard notation. Let f0 be sampled from an inﬁnitely wide neural network architecture Funder some initialization distribution, and let fi ∞be the limiting solution under the training dynamics deﬁned by He et al. [18] applied to the initialization f0 and using data D<i. Let K∞denote the neural tangent kernel for F, and M= GP(0,K∞) the induced Gaussian Process. Then fi ∞∼P(f|D<i,M), and in the limit of inﬁnite training time, the iterative sample-then-optimize procedure yields an unbiased estimate of L(D|M). Letting ℓ2 denote the scaled squared ℓ2 regression loss and cbe a constant, we obtain as a direct corollary of Theorem 3.3 P(D) ≥Efi∞∼P(·|D<i)[log P(Di|θi)] = E n∑ i=1 −ℓ2(Di,fi) + c= L(D) . (9) 15Proof. Follows immediately from the results of He et al. [18] stating that the the limiting distribution of fk ∞is precisely P(f|Dn ≤k,M). We therefore obtain the same result as for Theorem 3.3, plugging in the kernel gradient descent procedure on f for the parameter-space gradient descent procedure on θ. The following Lemma will be useful in order to prove Proposition 3.5. Intuitively, this result states that in a linear regression problem in which each feature φi is ‘normalized’ (the dot product ⟨φi,y⟩= ⟨φj,y⟩= αfor some αand all i,j) and ‘independent’ (i.e.⟨Πy⊥φi,Πy⊥φj⟩= 0), then the optimal linear regression solution assigns highest weight to the feature which obtains the least error in predicting yon its own. Lemma A.1. Let y∈Rn, and Φ ∈Rd×d be a design matrix such that Φ[:,j] = αy+ ϵj∀jfor some ﬁxed α ≥0, with ϵ ∈y⊥, and ϵ⊤ i ϵj = 0 for all i ̸= j. Let w∗be the solution to the least squares regression problem on Φ and y. Then min i wi = min i ∥fi(x) −y∥2 = max i L(Mi) (31) Proof. We express the minimization problem as follows. We letφ(x) = (f1(x),...,f k(x)), where fi(x) = αy+ ϵi, with ϵi ⊥ϵj. We denote by 1 the vector containing all ones (of length k). We observe that we can decompose the design matrixΦ into one component whose columns are parallel to y, denoted Φy, and one component whose columns are orthogonal to y, denoted Φ⊥. Let σ2 i = ∥ϵi∥2. By assumption, Φy = αy1 ⊤, and Φ⊤ ⊥Φ⊥= diag(σ2 1,...,σ 2 n) = Σ. We then observe the following decomposition of the squared error loss of a weight vector w, denoted ℓ(w). ℓ(w) = ∥Φw−y∥2 = (Φw−y)⊤(Φw−y) = ((Φy + Φ⊥)w−y)⊤((Φy + Φ⊥)w−y) = (Φyw−y)⊤(Φyw−y) + w⊤Φ⊤ ⊥Φ⊥w = ∥y∥2∥1 −α1 ⊤w∥2 + ∑ σ2 iwi In particular, the loss decomposes into a term which depends on the sum of the wi, and another term which will depend on the norm of the component of each model’s predictions orthogonal to the targets y. As this is a quadratic optimization problem, it is clear that an optimal wexists, and so w⊤1 will take some ﬁnite value, say β. We will show that for any ﬁxed β, the solution to the minimization problem min w ∑ wiσ2 i : w⊤1 = β (32) is such that the argmax over i of wi is equal to that of the minimum variance. This follows by applying the method of Lagrange multipliers to obtain that the optimal wsatisﬁes w∗ i = α∑σ−2 i 1 σ2 i . (33) In particular, w∗ i is inversely proportional to the variance of fi, and so is maximized for i = arg miniE∥fi(x) −y∥2. Proposition 3.5. Let M1,..., Mk be Bayesian linear regression models with ﬁxed noise variance σ2 N and Gaussian likelihoods. Let Φ be a (random) matrix of posterior prediction samples, of the form Φ[i,j] = ˆyj i ∼P(yj|D<j,xj,Mi). Suppose the following two conditions on the columns of Φ are satisﬁed: E⟨Φ[:,i],y⟩= E⟨Φ[:,j],y⟩for all i,j, and E⟨Πy⊥φi,Πy⊥φj⟩= 0. Let w∗denote the least-squares solution to the regression problem minwEΦ∥Φw−y∥2. Then the following holds arg max i w∗ i = arg max i L(D|Mi) ∀w∗= arg min w E∥Φw−y∥2 . (11) 16Proof. We ﬁrst clarify the independence assumptions as they pertain to the assumptions of the previous lemma: writing Φ[:,i] as fi(x) + ζi = αy+ ϵi + ζi with ζi ∼N (0,Σi) corresponding to the noise from the posterior distribution and fi its mean, the ﬁrst independence assumption is equivalent to the requirement that fi = α′y+ ϵi with ϵi ⊥yfor all i. The second independence assumption is an intuitive expression of the constraint that ϵi ⊥ϵj in the linear-algebraic sense of independence, and that ζj i is sampled independently (in the probabilistic sense) for all iand j. We note that our lower bound for each model in the linear regression setting is equal to E∑N i=1 ∥fk(xi) + ζi −yi∥2 + c where c is a ﬁxed normalizing constant. By the previous Lemma, we know that the linear regression solution w∗ based on the posterior means satisﬁes, maxiw∗ i = maxiL(Mi). It is then straightforward to extend this result to the noisy setting. E[∥Φw−y∥2] = E[∥(Φy + Φ⊥+ ζ)w−y∥2] (34) = E[((Φy + Φ⊥+ ζ)w−y)⊤((Φy + Φ⊥+ ζ)w−y)] (35) = ∥Φyw−y∥2 + w⊤Φ⊤ ⊥Φ⊥w+ E[w⊤ζ⊤ζw] (36) = (w⊤1 −α)2∥y∥2 + w⊤Φ⊤ ⊥Φ⊥w+ E[w⊤ζ⊤ζw] (37) = (w⊤1 −α)2∥y∥2 + ∑ w2 i(∥Φ⊥[:,i]∥2 + ∥ζi∥2) (38) We again note via the same reasoning as in the previous Lemma that the model with the greatest lower bound will be the one which minimizes ∥Φ⊥[:,i]∥2 + ∥ζi∥2, and that the weight given to index iwill be inversely proportional to this term. It only remains to show that for each model i, the model which maximizes L(Mi) will also minimize ∥Φ⊥[:,i]∥2 + ∥ζi∥2. This follows precisely from the Gaussian likelihood assumption. As we showed previously L(D|Mi) = E[ ∑ log P(yi|D<i)] ∝− ∑ E[ℓ2(yi −ˆyi] (39) = [∥y−µ∥2 + E[∥ˆy−µ∥2] (40) = α∥y∥2 + ∥Φ⊥[:,i]∥2 + E[∥ζi∥2] (41) and so ﬁnding the model Mi which maximizes L(D,Mi) is equivalent to picking the maximal index iof w∗which optimizes the expected loss of the least squares regression problem. 17Figure 5: Relative rankings given by optimize-then-prune, ML, and estimated L(D). Left: feature selection. Middle: prior variance selection. Right: RFF frequency selection. Rankings are consistent with what our theoretical results predict. Results are averaged over 5 runs. B Experiments B.1 Experimental details: Model Selection using Trajectory Statistics We consider 3 model selection settings in which to evaluate the practical performance of our estimators. In prior variance selection we evaluate a set of BLR models on a synthetic linear regression data set. Each model Mi has a prior distribution over the dparameters of the form w∼N(0,σ2 iId) for some σ2 i, and the goal is to select the optimal prior variance (in other words, the optimal regularization coefﬁcient). We additionally evaluate an analogous initialization variance selection method on an NTK network trained on a toy regression dataset. In frequency (lengthscale) selection we use as input a subset of the handwritten digits dataset MNIST given by all inputs labeled with a 0 or a 1. We compute random Fourier features (RFF) of the input to obtain the features for a Bayesian linear regression model, and perform model selection over the frequency of the features (full details on this in the appendix). This is equivalent to obtaining the lengthscale of an approximate radial basis function kernel. In feature dimension selection, we use a synthetic dataset [46] of the form (X,y), where xi = (yi + ϵ1,yi + ...,y i + ϵ15,ϵ16,...,ϵ 30). We then consider a set of models {Mk}with feature embeddings φk(xi) = xi[1,...,k ]. The optimal model in this setting is the one which uses exactly the set of ‘informative’ featuresx[1,..., 15]. The synthetic data simulation used in this experiment is identical to that used in [ 46]. Below, we provide the details. Let k be the number of informative features and d the total number of features. We generate a datapoint Di = {xi,yi}as follows: 1. Sample yi: yi ∼U([0,1]) 2. Sample kinformative features: xi,j ∼N(yi,σ0) ∀j ∈1,...k 3. Sample max(d−k,0) noise features: xi,k+j ∼N(0,σ1) ∀j ∈1,...d −k 4. Concatenate the features: Xi = [xi,1,...x i,d] We set σ0 = σ1 = 1, k= 15, n= 30, and let dvary from 5 to n. We then run our estimators on the Bayesian linear regression problem for each feature dimension, and ﬁnd that all estimators agree on the optimal number of features, k. To compute the random fourier features used for MNIST classiﬁcation, we vectorize the MNIST input images and follow the procedure outlined by [37] (Algorithm 1) to produce RFF features, which are then used for standard Bayesian linear regression against the binarized labels. The frequency parameter (which can also be interpreted as a transformation of the lengthscale of the RBF kernel approximated by the RFF model) is the parameter of interest for model selection. Results can be found in Figure 5. We additionally provide an analogue to our evaluation of model selection in NTK-GPs, with the change in the log marginal likelihood plotted instead of L(D). We obtain analogous results, as can be seen in Figure 6. 18Figure 6: Evaluation of change in log ML after data point iis added for NTK-GPs on a random subset of MNIST. B.2 Experimental details: Bayesian model comparison Here we provide further detail of the experiment in Section 4.2.1. The goal of the experiment is to determine whether the connection between sum-over-training losses (SOTL) and model evidence observed in the linear regression setting extends to DNNs. In particular, the two sub-questions are: 1. Do models with a lower SOTL generalize better? 2. Are these models favoured by SGD? To answer these questions, we train a linear combination of NNs. We can answer subquestion [1] by plotting the correlation between SOTL and test performance of an individual model. Further, we address subquestion [2] by considering the correlation between test loss and linear weights assigned to each model. Below we explain the set-up of the linear combination in more detail. We train a variety of deep neural networks along with a linear ‘ensemble’ layer that performs a linear transformation of the concatenated logit outputs3 of the classiﬁcation models. Let hm(xi) be logit output of model mfor input xi, ℓ(yi,hi) be the loss for point i(where hi is a logit) and wm,t be the weight corresponding to model mat time step t. We consider two training strategies: we ﬁrst train models individually using the cross-entropy loss between each model’s prediction and the true label, only cross-entropy loss of the ﬁnal ensemble prediction to train the linear weights. Mathematically, we update the models using the gradients δ δθm ℓ(yi,hm(xi)), (42) and the ‘ensemble’ weights using δ δwm ℓ(yi, ∑ m wmhm(xi)). (43) We refer to this training scheme as Parallel Trainingas the models are trained in parallel. We also consider the setting in which the models are trained using the cross entropy loss from the ensemble prediction backpropagated through the linear ensemble layer, i.e. the model parameters are now updated using: δ δθm ℓ(yi, ∑ m wmhm(xi)). (44) 3These are pre-softmax outputs. To obtain the predicted probability of a class, they are fed through a softmax function. 19We refer to this scheme as the Concurrent Training. We train a variety of different MLPs (with varying layers,and nodes) and convolutional neural networks (with varying layers, nodes and kernels) on FashionMNIST using SGD until convergence. B.3 Experimental Details: SGD upweights submodels that perform well Below we provide further details of the experiment in Section 4.2.2. The goal of the experiment is to determine whether SGD upweights sub-models that ﬁt the data better. We train a MLP network (with units 200,200,10) on FashionMMIST using SGD until convergence. After training is completed, for every class of y, we rank all nodes in the penultimate layer by the norm of their absolute weight (in the ﬁnal dense layer). We group the points into submodels according to their ranking – the knodes with the highest weights are grouped together, next the k+ 1,... 2k ranked nodes are grouped, etc. We set k= 10. We determine the performance of a submodels by training a simple logistic classiﬁer to predict the class of an input, based on the output of the submodel. To measure the performance of the classiﬁer, we use the cross-entropy loss. To capture the equivalent notion of the AUC, we estimate the performance of the sub-models throughout training, and sum over the estimated cross-entropy losses. Below, we show additional plots for theparallel and concurrent training schemes. The results are the same to those presented in the main text, and we observe [1] a negative correlation between test performance and ensemble weights and [2] a strong correlation between SOTL and average test cross-entropy. Figure 7: Linear combinations of DNNs on FashionMNIST. Left: ensemble weights versus the test loss for parallel training; we observe a negative correlation. Middle: SOTL (standardized by the number of training samples) versus test loss for concurrent and concurrent training. We observe a strong correlation indicating that the SOTL generalizes well. Right: training curves for the different models in concurrent training schemes. All results are averaged over 10 runs, and standard deviations are shown by the shaded regions around each observation. The model parameters, given in the parentheses, are the number of layers (l), nodes per layer (n) and kernel size (k), respectively. However, similarly to the linear setting, the difference in assigned weights is magniﬁed in the concurrent training scheme. Here we ﬁnd that in the concurrent training scheme, the ensemble focuses on training the CNNs (as can be seen from the training curve in Figure 3 in the main text). This is likely because CNNs are able to learn more easily, leading to larger weights earlier on. 20Figure 8: Weight assigned to subnetwork by SGD in a deep neural network (x-axis) versus the subnet- work performance (estimated by the sum of cross-entropy, on the y-axis) for different FashionMNIST classes. The light blue ovals denote depict 95% conﬁdence intervals, estimated over 10 seeds (i.e. 2σ for both the weight and SOTL). The orange line depicts the general trend. Figure 9: Weight assigned to subnetwork by SGD in a deep neural network (x-axis) versus the subnetwork performance (estimated by the sum of cross-entropy, on the y-axis) for different CIFAR- 10 classes. The light blue ovals denote depict 95% conﬁdence intervals, estimated over 10 seeds (i.e. 2σfor both the weight and SOTL). The orange line depicts the general trend. Above, we show additional plots to those shown in Figure 4, Section 4.2.2. Figure 8 shows the results for the all FashionMNIST classes, and Figure 9 shows the results for experiment on CIFAR-10. From both, we see that SGD assigns higher weights to subnetworks that perform better. 21",
      "meta_data": {
        "arxiv_id": "2010.14499v1",
        "authors": [
          "Clare Lyle",
          "Lisa Schut",
          "Binxin Ru",
          "Yarin Gal",
          "Mark van der Wilk"
        ],
        "published_date": "2020-10-27T17:56:14Z",
        "pdf_url": "https://arxiv.org/pdf/2010.14499v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper establishes a connection between a model's training speed and its marginal likelihood, offering a new method for model selection and insights into generalization. It shows that a measure of training speed can estimate marginal likelihood and predict model weightings in linear combinations. The core contribution is a family of marginal likelihood estimators derived from sums of predictive log likelihoods (interpreted as training losses) from an iterative Bayesian updating procedure. For linear models and infinite-width neural networks, where gradient descent provides exact posterior samples, these estimators can be computed directly from optimization trajectories. Empirical evidence suggests this intuition extends to deep neural networks (DNNs) trained with stochastic gradient descent (SGD), indicating a promising direction for explaining why SGD-trained neural networks generalize well. The study also highlights a link between magnitude-based pruning in linear model combinations and approximate Bayesian model selection.",
        "methodology": "The methodology revolves around a Bayesian perspective on marginal likelihood (ML) estimation. The log ML is re-expressed as the sum of log posterior predictive probabilities of data points, conditioned on preceding data (sum of 'training losses' over a Bayesian updating curve). Three estimators for lower bounds on the log ML are proposed: L(D) (based on expected log likelihoods), Lk(D) (a tighter bound using k posterior samples), and LS(D) (for Gaussian posteriors, using estimated mean and variance). For linear models and infinite-width neural networks (approximated by Neural Tangent Kernel GPs), gradient descent is leveraged to produce exact posterior samples iteratively, allowing these estimators to be computed from optimization trajectories. The paper also analyzes linear model combinations, showing that the optimal weights in such combinations align with the proposed ML estimators under certain conditions. For DNNs, the methodology involves evaluating the 'sum over training losses' (SOTL) obtained during SGD, correlating it with generalization error and ensemble weights in linear combinations of DNNs, and investigating subnetwork selection within a single DNN.",
        "experimental_setup": "The experimental evaluation covered linear models, infinite-width neural networks (NTK-GPs), and deep neural networks (DNNs) trained with SGD. For linear models, synthetic datasets were used for three model selection tasks: feature dimension selection (identifying informative features), prior variance selection (optimal regularization), and frequency (lengthscale) selection for Random Fourier Features (RFFs) on binarized MNIST. Validation involved comparing rankings from true ML, the proposed estimators (L, Lk, LS), and weights from optimized linear regressors. For NTK-GPs, fully-connected and convolutional architectures were tested on MNIST, visualizing the incremental change in the L(D) estimator. For DNNs, experiments were conducted on FashionMNIST and CIFAR-10 datasets using various MLP and CNN architectures. This included training linear combinations of DNNs with SGD (parallel and concurrent training schemes) to analyze the correlation between SOTL and test loss, and between individual test loss and ensemble weights. Additionally, subnetwork selection within a single MLP was explored by relating the final weights of penultimate layer nodes to their performance (SOTL of cross-entropy). All DNN experiments averaged results over multiple runs (10 seeds).",
        "limitations": "The primary theoretical guarantees for the proposed marginal likelihood estimators and the connection to training speed hold strictly for linear models and the infinite-width limit of neural networks. For finite-width, non-linear deep neural networks trained with SGD, the results are presented as empirical evidence and conjectures, as SGD lacks the exact posterior sampling interpretation of the algorithm. The LS estimator is specifically limited to models with Gaussian posterior predictive distributions. Furthermore, the theoretical result connecting linear model combination weights to marginal likelihood (Proposition 3.5) relies on strong independence assumptions on model errors which are only approximately satisfied in practice. The paper also acknowledges that, due to its theoretical nature and focus on i.i.d. assumptions, it does not immediately address issues like out-of-distribution inputs or inherent biases in training datasets.",
        "future_research_directions": "The paper suggests several promising avenues for future research. A key direction is to further formalize and explain the empirical generalization performance of deep neural networks, particularly the bias of SGD towards functions that generalize well, by extending the intuition developed in linear models and infinite-width settings to non-linear, finite-width DNNs. This includes investigating whether the analogue of the sum of training losses can reliably predict generalization error in SGD-trained models. Another direction is to explore if similar mechanisms (where more predictive components receive higher weights) occur within the internal layers of a neural network, beyond the final linear layer. More broadly, the work aims to pave the way for new practical model selection procedures that integrate closely with standard optimization schemes. Improved understanding of generalization could also lead to more efficient training methods (reducing environmental impact) and enhance the safety and robustness of deep learning systems."
      }
    },
    {
      "title": "The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent",
      "abstract": "This paper studies how neural network architecture affects the speed of\ntraining. We introduce a simple concept called gradient confusion to help\nformally analyze this. When gradient confusion is high, stochastic gradients\nproduced by different data samples may be negatively correlated, slowing down\nconvergence. But when gradient confusion is low, data samples interact\nharmoniously, and training proceeds quickly. Through theoretical and\nexperimental results, we demonstrate how the neural network architecture\naffects gradient confusion, and thus the efficiency of training. Our results\nshow that, for popular initialization techniques, increasing the width of\nneural networks leads to lower gradient confusion, and thus faster model\ntraining. On the other hand, increasing the depth of neural networks has the\nopposite effect. Our results indicate that alternate initialization techniques\nor networks using both batch normalization and skip connections help reduce the\ntraining burden of very deep networks.",
      "full_text": "The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Karthik A. Sankararaman* 1 2 Soham De* 3 Zheng Xu 2 W. Ronny Huang2 Tom Goldstein2 Abstract This paper studies how neural network architec- ture affects the speed of training. We introduce a simple concept called gradient confusion to help formally analyze this. When gradient confusion is high, stochastic gradients produced by different data samples may be negatively correlated, slow- ing down convergence. But when gradient confu- sion is low, data samples interact harmoniously, and training proceeds quickly. Through theoreti- cal and experimental results, we demonstrate how the neural network architecture affects gradient confusion, and thus the efﬁciency of training. Our results show that, for popular initialization tech- niques, increasing the width of neural networks leads to lower gradient confusion, and thus faster model training. On the other hand, increasing the depth of neural networks has the opposite effect. Our results indicate that alternate initial- ization techniques or networks using both batch normalization and skip connections help reduce the training burden of very deep networks. 1. Introduction Stochastic gradient descent (SGD) (Robbins & Monro, 1951) and its variants with momentum have become the standard optimization routine for neural networks due to their fast convergence and good generalization properties (Wilson et al., 2017; Sutskever et al., 2013; Smith et al., 2020). Yet the convergence behavior of SGD on neural networks still eludes full theoretical understanding. Fur- thermore, it is not well understood how design choices on neural network architecture affect training performance. In this paper, we make progress on these open questions. *Equal contribution 1Facebook. 2University of Maryland, Col- lege Park. 3DeepMind, London.. Correspondence to: Karthik A. Sankararaman <karthikabinavs@gmail.com>, Soham De <so- hamde@google.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Classical stochastic optimization theory predicts that the learning rate of SGD needs to decrease over time for con- vergence to be guaranteed to the minimizer of a convex function (Shamir & Zhang, 2013; Bertsekas, 2011). For strongly convex functions for example, such results show that a decreasing learning rate schedule of O(1/k) is re- quired to guarantee convergence to within ϵ-accuracy of the minimizer in O(1/ϵ) iterations, where kdenotes the itera- tion number. Such decay schemes, however, typically lead to poor performance on standard neural network problems. Neural networks operate in a regime where the number of parameters is much larger than the number of training data. In this “over-parameterized” regime, SGD seems to converge quickly with constant learning rates. Most neu- ral network practitioners use a constant learning rate for the majority of training (with exponential decay only to- wards the end of training) without seeing the method stall (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016; Zagoruyko & Komodakis, 2016). With constant learning rates, theoretical guarantees show that SGD con- verges quickly to a neighborhood of the minimizer, but then reaches a noise ﬂoor beyond which it stops converging; this noise ﬂoor depends on the learning rate and the variance of the gradients (Moulines & Bach, 2011; Needell et al., 2014). Recent results show that convergence without a noise ﬂoor is possible without decaying the learning rate, provided the model is strongly convex and overﬁtting occurs (Schmidt & Roux, 2013; Ma et al., 2017; Vaswani et al., 2018). While these results do give important insights, they do not fully explain the dynamics of SGD on neural networks, and how they relate to over-parameterization. Furthermore, training performance is strongly inﬂuenced by network ar- chitecture. It is common knowledge among practitioners that, under standard Gaussian initialization techniques (Glo- rot & Bengio, 2010; He et al., 2015), deeper networks train slower (Bengio et al., 1994; Saxe et al., 2013). This has led to several innovations over the years to get deeper nets to train more easily, such as careful initialization strategies (Xiao et al., 2018), residual connections (He et al., 2016), and normalization schemes like batch normalization (Ioffe & Szegedy, 2015). Furthermore, there is evidence to indi- cate that wider networks are faster to train (Zagoruyko & Komodakis, 2016; Nguyen & Hein, 2017), and recent the- arXiv:1904.06963v5  [cs.LG]  6 Jul 2020The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent oretical results suggest that the dynamics of SGD simplify considerably for very wide networks (Jacot et al., 2018; Lee et al., 2019). In this paper, we make progress on the- oretically understanding these empirical observations and unifying existing theoretical results. To this end, we identify and analyze a condition that enables us to establish direct relationships between layer width, network depth, problem dimensionality, initialization schemes, and trainability and SGD dynamics for over-parameterized networks. Our contributions. Typical neural networks are over- parameterized (i.e., the number of parameters exceed the number of training points). In this paper, we ask how this over-parameterization, and more speciﬁcally the network ar- chitecture, affects the trainability of neural networks and the dynamics of SGD. Through extensive theoretical and exper- imental studies, we show how layer width, network depth, initialization schemes, and other architecture choices affect the dynamics. The following are our main contributions.1 • We identify a condition, termed gradient confusion, that impacts the convergence properties of SGD on over-parameterized models. We prove that high gradi- ent confusion may lead to slower convergence, while convergence is accelerated (and could be faster than predicted by existing theory) if confusion is low, in- dicating a regime where constant learning rates work well in practice (sections 2 and 3). We use the gradi- ent confusion condition to study the effect of various architecture choices on trainability and convergence. • We study the effect of neural network architecture on gradient confusion at standard Gaussian initialization schemes (section 4), and prove (a) gradient confusion increases as the network depth increases, and (b) wider networks have lower gradient confusion. These indi- cate that deeper networks are more difﬁcult to train and wider networks can improve trainability of networks. Directly analyzing the gradient confusion bound en- ables us to derive results on the effect of depth and width, without requiring restrictive assumptions like large layer widths (Du et al., 2018; Allen-Zhu et al., 2018). Our results hold for a large class of neural networks with different non-linear activations and loss- functions. In section 5, we present a more general result on the effect of depth on the trainability of net- works without assuming the network is at initialization. • We prove that for linear neural networks, gradient con- fusion is independent of depth when using orthogonal initialization schemes (section 6) (Saxe et al., 2013; Schoenholz et al., 2016). This indicates a way forward in developing techniques for training deeper models. 1To keep the main text of the paper concise, all proofs and sev- eral additional experimental results are delegated to the appendix. • We test our theoretical predictions using extensive experiments on wide residual networks (WRNs) (Zagoruyko & Komodakis, 2016), convolutional net- works (CNNs) and multi-layer perceptrons (MLPs) for image classiﬁcation tasks on CIFAR-10, CIFAR-100 and MNIST (section 7 and appendix A). We ﬁnd that our theoretical results consistently hold across all our experiments. We further show that the combination of batch normalization and skip connections in residual networks help lower gradient confusion, thus indicat- ing why SGD can efﬁciently train deep neural networks that employ such techniques. 2. Gradient confusion Notations. We denote vectors in bold lower-case and ma- trices in bold upper-case. We use (W)i,j to indicate the (i,j) cell in matrix W and (W)i for the ith row of matrix W. ∥W∥denotes the operator norm of W. [N] denotes {1,2,...,N }and [N]0 denotes {0,1,...,N }. Preliminaries. Given N training points (speciﬁed by the corresponding loss functions {fi}i∈[N]), we use SGD to solve empirical risk minimization problems of the form, minw∈Rd F(w) := minw∈Rd 1 N ∑N i=1 fi(w), (1) using the following iterative update rule for T rounds: wk+1 = wk −α∇˜fk(wk). (2) Here α is the learning rate and ˜fk is a function chosen uniformly at random from {fi}i∈[N] at iteration k ∈[T]. w⋆ = arg minw F(w) denotes the optimal solution. Gradient confusion. SGD works by iteratively selecting a random function ˜fk, and modifying the parameters to move in the direction of the negative gradient of˜fk. It may happen that the selected gradient ∇˜fk is negatively correlated with the gradient of another term ∇fj.When the gradients of dif- ferent mini-batches are negatively correlated, the objective terms disagree on which direction the parameters should move, and we say that there is gradient confusion.2 Deﬁnition 2.1. A set of objective functions {fi}i∈[N] has gradient confusion bound η≥0 if the pair-wise inner prod- ucts between gradients satisfy, for a ﬁxed w ∈Rd, ⟨∇fi(w),∇fj(w)⟩≥− η, ∀i̸= j ∈[N]. (3) Observations in simpliﬁed settings. SGD converges fast when gradient confusion is low along its path. To see why, 2Gradient confusion is related to both gradient variance and gradient diversity (Yin et al., 2017), but with important differences, which we discuss in section 9. We also discuss alternate deﬁnitions of the gradient confusion condition in section 8.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Figure 1.Linear regression on an over-parameterized (d = 120) and under-parameterized (d= 80) model with N = 100samples generated randomly from a Gaussian, trained using SGD with mini- batch size 1. Plots are averaged over 3 independent runs. Gradient cosine similarities were calculated over all pairs of gradients. consider the case of training a logistic regression model on a dataset with orthogonal vectors. We have fi(w) = L(yix⊤ i w),where L: R →R is the logistic loss,{xi}i∈[N] is a set of orthogonal training vectors, and yi ∈{−1,1} is the label for xi. We then have ∇fi(w) = ζixi,where ζi = yiL′(yi ·x⊤ i w).Note that the gradient confusion is 0 since ⟨∇fi(w),∇fj(w)⟩= ζiζj⟨xi,xj⟩= 0, ∀i,j ∈[N] and i ̸= j. Thus, an update in the gradient direction fi has no effect on the loss value of fj for i ̸= j. In this case, SGD decouples into (deterministic) gradient descent on each objective term separately, and we can expect to see the fast convergence rates attained by gradient descent. Can we expect a problem to have low gradient confusion in practice? From the logistic regression problem, we have: |⟨∇fi(w),∇fj(w)⟩|= |⟨xi,xj⟩|·|ζiζj|.This inner prod- uct is expected to be small for allw; the logistic loss satisﬁes |ζiζj|< 1, and for ﬁxed N the quantity maxij|⟨xj,xi⟩| is O(1/ √ d) whenever {xi}are randomly sampled from a sphere (see lemma B.1 for the formal statement).3 Thus, we would expect a random linear model to have nearly orthog- onal gradients, when the number of parameters is \"large\" and the number of training data is \"small\", i.e., when the model is over-parameterized. This is further evidenced by a toy example in ﬁgure 1, where we show a slightly over- parameterized linear regression model can have much faster convergence rates, as well as lower gradient confusion. One can prove a similar result for problems that have random and low-rank Hessians, which suggests that one might expect gradient to be small near the minimizer for many standard neural nets (see appendix C for more discussion). The above arguments are a bit simplistic, considering toy scenarios and ignoring issues like the effect of network structure. In the following sections, we rigorously analyze the effect of gradient confusion on the speed of convergence on non-convex problems, and the effect of width and depth of the neural network architecture on the gradient confusion. 3Generally, this is true whenever xi = 1√ dyi,where yi is an isotropic random vector (Vershynin, 2018). 3. SGD is fast when gradient confusion is low Several prior papers have analyzed the convergence rates of constant learning rate SGD (Nedi ´c & Bertsekas, 2001; Moulines & Bach, 2011; Needell et al., 2014). These re- sults show that for strongly convex and Lipschitz smooth functions, SGD with a constant learning rate αconverges linearly to a neighborhood of the minimizer. The noise ﬂoor it converges to depends on the learning rate αand the vari- ance of the gradients at the minimizer, i.e., Ei∥∇fi(w⋆)∥2. To guarantee convergence to ϵ-accuracy in such a setting, the learning rate needs to be small, i.e., α = O(ϵ), and the method requires T = O(1/ϵ) iterations. Some more recent results show convergence of constant learning rate SGD without a noise ﬂoor and without small step sizes for models that can completely ﬁt the data (Schmidt & Roux, 2013; Ma et al., 2017; Vaswani et al., 2018). Gradient confusion is related to these results. Cauchy- Schwarz inequality implies that if Ei∥∇fi(w⋆)∥2 = O(ϵ), then Ei,j|⟨∇fi(w⋆),∇fj(w⋆)⟩|= O(ϵ), ∀i,j. Thus the gradient confusion at the minimizer is small when the vari- ance of the gradients at the minimizer is small. Further note that when the variance of the gradients at the mini- mizer is O(ϵ), a direct application of the results in Moulines & Bach (2011) and Needell et al. (2014) shows that con- stant learning rate SGD has fast convergence to ϵ-accuracy in T = O(log(1/ϵ)) iterations, without the learning rate needing to be small. Generally however, bounded gradient confusion does not provide a bound on the variance of the gradients (see section 9). Thus, it is instructive to derive con- vergence bounds of SGD explicitly in terms of the gradient confusion to properly understand its effect. We ﬁrst consider functions satisfying the Polyak- Lojasiewicz (PL) inequality (Lojasiewicz, 1965), a condi- tion related to, but weaker than, strong convexity, and used in recent work (Karimi et al., 2016; De et al., 2017). We provide bounds on the rate of convergence in terms of the optimality gap. We start with two standard assumptions. (A1) {fi}i∈[N] are Lipschitz smooth: fi(w′) ≤fi(w)+ ∇fi(w)⊤(w′−w)+ L 2 ∥w′−w∥2. (A2) {fi}i∈[N] satisfy the PL inequality: 1 2 ∥∇fi(w)∥2 ≥µ(fi(w) −f⋆ i ),f ⋆ i = minw fi(w). We now state a convergence result of constant learning rate SGD in terms of the gradient confusion. Theorem 3.1. If the objective function satisﬁes (A1) and (A2), and has gradient confusion η, SGD converges linearly to a neighborhood of the minima of problem (1) as: E[F(wT) −F⋆] ≤ρT(F(w0) −F⋆) + αη 1−ρ, where α< 2 NL, ρ= 1−2µ N ( α−NLα2 2 ) , F⋆ = minw F(w) and w0 is the initialized weights.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent This result shows that SGD converges linearly to a neigh- borhood of a minimizer, and the size of this neighborhood depends on the level of gradient confusion. When the gra- dient confusion is small, i.e., η= O(ϵ), SGD has fast con- vergence to O(ϵ)-accuracy in T = O(log(1/ϵ)) iterations, without requiring the learning rate to be vanishingly small. We now extend this to general smooth functions. Theorem 3.2. If the objective satisﬁes (A1) and has gradi- ent confusion η, then SGD converges to a neighborhood of a stationary point of problem (1) as: mink=1,...,T E∥∇F(wk)∥2 ≤ρ(F(w1)−F⋆) T + ρη, for α< 2 NL, ρ= 2N 2−NLα, and F⋆ = minw F(w). Thus, as long as η = O(1/T), SGD has fast O(1/T) con- vergence on smooth non-convex functions. Theorems 3.1 and 3.2 predict an initial phase of optimization with fast con- vergence to the neighborhood of a minimizer or a stationary point. This behavior is often observed when optimizing neural nets (Darken & Moody, 1992; Sutskever et al., 2013), where a constant learning rate reaches a high level of ac- curacy on the model. As we show in subsequent sections, this is expected since for neural networks typically used, the gradient confusion is expected to be low. See section 9 for more discussion on the above results and how they relate to previous work. We stress that our goal is not to study convergence rates per se, nor is it to prove state-of-the-art rate bounds for this class of problems. Rather, we show the direct effect that the gradient confusion bound has on the convergence rate and the noise ﬂoor for constant learning rate SGD. As we show in the following sections, this new perspective in terms of the gradient confusion helps us more directly understand how neural network architecture design affects SGD dynamics and why. 4. Effect of neural network architecture at Gaussian initializations To draw a connection between neural network architecture and training performance, we analyze gradient confusion for generic (i.e., random) model problems using methods from high-dimensional probability. In this section, we analyze the effect of neural network architecture at the beginning of training, when using standard Gaussian initialization tech- niques. Analyzing these models at initialization is important to understand which architectures are more easily trainable than others. Our results cover a wide range of scenarios compared to prior work, require minimal additional assump- tions, and hold for a large family of neural networks with different non-linear activation functions and loss-functions. In particular, our results hold for fully connected networks (and can be extended to convolutional networks) with the square-loss and logistic-loss functions, and commonly used non-linear activations such as sigmoid, tanh and ReLU. We consider both the case where the input data is arbitrary but bounded (theorem 4.1, part 1), as well as where the input data is randomly drawn from the surface of a unit sphere (theorem 4.1, part 2). Setting. We consider training dataD= {(xi,C(xi))}i∈[N], with labeling function C: Rd →[−1,1]. For some of our results, we consider that the data points {xi}are drawn uniformly at random from the surface of a d-dimensional unit sphere. The labeling function satisﬁes |C(x)|≤ 1 and ∥∇xC(x)∥2 ≤1 for ∥x∥≤ 1.Note that this automatically holds for every model considered in this paper where the labeling function is realizable (i.e., where the model can express the labeling function using its parameters). More generally, this assumes a Lipschitz condition on the labels (i.e., the labels don’t change too quickly with the inputs). We consider two loss-functions: square-loss for regres- sion and logistic loss for classiﬁcation. The square-loss function is deﬁned as fi(w) = 1 2 (C(xi) −gw(xi))2 and the logistic function is deﬁned as fi(w) = log(1 + exp(−C(xi)gw(xi))). Here, gw : Rd →R denotes the parameterized function we ﬁt to the training data and fi(w) denotes the loss-function of hypothesis gw on data point xi. Let W0 ∈Rℓ1×d and {Wp}p∈[β] where Wp ∈Rℓp×ℓp−1 are weight matrices. Let W denote the tuple (Wp)p∈[β]0 . Deﬁne ℓ := maxp∈[β] ℓp to be the width and β to be the depth of the network. Then, the model gW is deﬁned as gW(x) :=σ(Wβσ(Wβ−1 ...σ (W1σ(W0x)) ... )), where σdenotes the non-linear activation function applied point-wise to its arguments. We assume that the activation is given by a function σ(x) with the following properties. • (P1) Boundedness: |σ(x)|≤ 1 for x∈[−1,1]. • (P2) Bounded differentials: Let σ′(x) and σ′′(x) de- note the ﬁrst and second sub-differentials respectively. Then, |σ′(x)|≤ 1 and |σ′′(x)|≤ 1 for all x∈[−1,1]. When ∥x∥≤ 1, activation functions such as sigmoid, tanh, softmax and ReLU satisfy these requirements. Furthermore, in this section, we consider the following Gaussian weight initialization strategy. Strategy 4.1. W0 ∈Rℓ×d has independent N(0,1 d) en- tries. For every p∈[β], the weights Wp ∈Rℓp×ℓp−1 have independent N ( 0, 1 κℓp−1 ) entries for some constant κ> 0. This initialization strategy with different settings of κare used almost universally for neural networks (Glorot & Ben- gio, 2010; LeCun et al., 2012; He et al., 2015). For instance,The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent typically κ= 1 2 when ReLU activations are used, andκ= 1 when tanh activations are used. Main result. The following theorem shows how the width ℓ:= maxp∈[β] ℓp and the depth βaffect the gradient confu- sion condition at standard initializations. We show that as width increases (for ﬁxed depth) or depth decreases (for ﬁxed width) the probability that the gradient confusion bound (equation 3) holds increases. Thus, as the depth in- creases (with ﬁxed width), training a model becomes harder, while as the width increases (with ﬁxed depth), training a model becomes easier. Furthermore, note that this result also implies that training very deep linear neural networks (with identity activation functions) with standard Gaussian initializations is hard. Throughout the paper, we deﬁne the parameter ζ0 := 2√β. See the appendix (Lemma D.1) for a more careful deﬁnition of this quantity. Theorem 4.1. Let W0,W1,..., Wβ be weight matrices chosen according to strategy 4.1. There exists ﬁxed con- stants c1,c2 >0 such that we have the following. 1. Consider a ﬁxed but arbitrary dataset x1,x2,..., xN with ∥xi∥ ≤1 for every i ∈[N]. For η > 4, the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −cℓ2β(η−4)2 64ζ4 0 (β+2)4 ) . 2. If the dataset {xi}i∈[N] is such that each xi is an i.i.d. sample from the surface of d-dimensional unit sphere, then for every η >0 the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −c2(ℓd+ℓ2β)η2 16ζ4 0 (β+2)4 ) . Theorem 4.1 shows that under popular Gaussian initializa- tions used, training becomes harder as networks get deeper. The result however also shows a way forward: layer width improves the trainability of deep networks. Other related work supports this showing that when the layers are in- ﬁnitely wide, the learning dynamics of gradient descent simpliﬁes considerably (Jacot et al., 2018; Lee et al., 2019). Hanin & Rolnick (2018) also suggest that the width should increase linearly with depth in a neural network to help dynamics at the beginning of training. In section 7 and appendix A, we show substantial empirical evidence that, given a sufﬁciently deep network, increasing the layer width often helps in lowering gradient confusion and speeding up convergence for a range of models. 5. A more general result on the effect of depth While our results in section 4 hold at standard initialization schemes, in this section we derive a more general version of the result. In particular, we assume the setting where the data is drawn uniformly at random from a unit sphere and the weights lie in a ball around a local minimizer. Our results hold for both fully connected networks and convolutional networks with the square-loss and logistic-loss functions, and commonly-used non-linear activations such as sigmoid, tanh, softmax and ReLU. We consider the same setup as in the previous section, and assume additionally that the data points {xi}are drawn uniformly from the surface of a d-dimensional unit sphere. Additionally, instead of studying the network at initializa- tion, we make the following assumption on the weights. Assumption 1 (Small Weights). We assume that the oper- ator norm of the weight matrices {Wi}i∈[β]0 are bounded above by 1, i.e., for every i∈[β]0 we have ∥Wi∥≤ 1. The operator norm of the weight matrices ∥W∥being close to 1 is important for the trainability of neural networks, as it ensures that the input signal is passed through the net- work without exploding or shrinking across layers (Glorot & Bengio, 2010). Proving non-vacuous bounds in case of such blow-ups in magnitude of the signal or the gradient is not possible in general, and thus, we consider this restricted class of weights. Most standard neural networks are trained using weight decay regularizers of the form ∑ i∥Wi∥2 F. This biases the weights to be small when training neural networks in practice. See appendix F for further discussion on the small weights assumption. We now present a more general version of theorem 4.1. Theorem 5.1. Let W0,W1,..., Wβ satisfy assumption 1. For some ﬁxed constantc> 0, the gradient confusion bound (equation 3) holds with probability at least 1 −N2 exp ( −cdη2 16ζ4 0 (β+2)4 ) . Theorem 5.1 shows that (for ﬁxed dimension dand number of samples N) when the depth β decreases, the probabil- ity that the gradient confusion bound in equation 3 holds increases, and vice versa. Thus, our results indicate that in the general case when the weights are small, increasing the network depth will typically lead to slower model training. Note that on assuming ∥W∥≤ 1 for each weight matrix W, the dependence of gradient confusion on the width goes away. To see why this, consider an example where each weight matrix in the neural network has exactly one non-zero element, which is set to 1. The operator norm of each such weight matrix is 1, but the forward or backward propagated signals would not depend on the width. Note that the convergence rate results of SGD in section 3 assume that the gradient confusion bound holds at every point along the path of SGD. On the other hand, theoremThe Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 5.1 shows concentration bounds for the gradient confusion at a ﬁxed weight W. Thus, to make the above result more relevant for the convergence of SGD on neural networks, we now make the concentration bound in theorem 5.1 uniform over all weights inside a ball Br of radius r. Corollary 5.1. Select a point W = (W0,W1,..., Wβ), satisfying assumption 1. Consider a ball Br centered at W of radius r > 0. If the data {xi}i∈[N] are sampled uniformly from a unit sphere, then the gradient confusion bound in equation 3 holds uniformly at all points W′∈Br with probability at least 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 ) , if r≤η/4ζ2 0 , 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 + 8dζ2 0 r η ) , otherwise. Corollary 5.1 shows that the probability that the gradient confusion bound holds decreases with increasing depth, for all weights in a ball around the minimizer.4 This explains why, in the general case, training very deep models might always be hard. This raises the question why most deep neural networks used in practice are so efﬁciently trained using SGD. While careful Gaussian initialization strategies prevent vanishing or exploding gradients, these strategies still suffer from high gradient confusion for very deep net- works unless the width is also increased with the depth, as we show in section 4. Practitioners over the years, however, have achieved state-of-the-art results by making networks deeper, without necessarily making networks wider. Thus, in section 7, we empirically study how popular techniques used in these models like skip connections and batch nor- malization affect gradient confusion. We ﬁnd that these techniques drastically lower gradient confusion, making deep networks signiﬁcantly easier to train. Furthermore, in the next section, we show how deep linear nets are train- able when used with orthogonal initialization techniques, indicating a way forward for training deeper models. 6. Gradient confusion is independent of depth for orthogonal initializations In this section, we show that for deep linear neural networks, gradient confusion is independent of depth when the weight matrices are initialized as orthogonal matrices.5 Consider the following linear neural network: gW(x) :=γWβ ·Wβ−1 ·... ·W1 ·x, (4) where the rescaling parameter γ = 1√2β, and assume we use the squared loss function. Then we have the following. 4The above results automatically hold for convolutional net- works, since a convolution operation on x can be represented as a matrix multiplication Ux for an appropriate Toeplitz matrix U. 5An orthogonal matrix A satisﬁes AT ·A = A ·AT = I. Theorem 6.1. Let {Wi}i∈[β] be arbitrary orthogonal ma- trices that satisfy assumption 1. Let the dataset {xi}i∈[N] be such that each xi is an i.i.d. sample from the surface of d-dimensional unit sphere. Consider the linear neural network in equation 4 that minimizes the empirical square loss function. For some ﬁxed constant c> 0, the gradient confusion bound (equation 3) holds with probability at least 1 −N2 exp ( −cdη2) . From Theorem 6.1, we see that the probability does not depend on the depth βor maximum width ℓ. Thus, trainabil- ity does not get worse with depth when using orthogonal initializations. This result matches previous theoretical and empirical results showing the efﬁciency of orthogonal ini- tialization techniques for training very deep linear or tanh networks (Saxe et al., 2013; Schoenholz et al., 2016; Xiao et al., 2018). However, orthogonal initializations are not compatable with non-linear activation functions like sig- moids or ReLUs, which limit their use in practice. Nonethe- less, this result suggests a promising direction in developing techniques for training deeper models. 7. Experimental results To test our theoretical results and to probe why standard neural networks are efﬁciently trained with SGD, we now present experimental results showing the effect of the neu- ral network architecture on the convergence of SGD and gradient confusion. It is worth noting that theorems 3.1 and 3.2 indicate that we would expect the effect of gradient confusion to be most prominent closer to the end of training. We performed experiments on wide residual networks (WRNs) (Zagoruyko & Komodakis, 2016), convolutional networks (CNNs) and multi-layer perceptrons (MLPs) for image classiﬁcation tasks on CIFAR-10, CIFAR-100 and MNIST. We present results for CNNs on CIFAR-10 in this section, and present all other results in appendix A. We use CNN-β-ℓto denote WRNs that have no skip connections or batch normalization, with a depth β and width factor ℓ.6 We turned off dropout and weight decay for all our experiments. We used SGD as the optimizer without any momentum. Following Zagoruyko & Komodakis (2016), we ran all experiments for 200 epochs with minibatches of size 128, and reduced the initial learning rate by a factor of 10 at epochs 80 and 160. We used the MSRA initializer (He et al., 2015) for the weights as is standard for this model, and used the same preprocessing steps for the CIFAR-10 im- ages as described in Zagoruyko & Komodakis (2016). We ran each experiment 5 times, and we show the standard de- viation across runs in our plots. We tuned the optimal initial 6The width factor denotes the number of ﬁlters relative to the original ResNet model (Zagoruyko & Komodakis, 2016).The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 0 25 50 75 100 125 150 175 200 epochs 0.0 0.5 1.0 1.5 2.0training loss depth 16 depth 22 depth 28 depth 34 depth 40 (a) 20 25 30 35 40 layer width −0.9 −0.8 −0.7 −0.6 −0.5 −0.4 −0.3 −0.2 −0.1min grad cosine similarity  (b) −0.4 −0.2 0.0 0.2 0.4 pairwise gradient cosine similarity 0 1 2 3 4 5 6 7density depth 16 depth 22 depth 28 depth 34 depth 40 (c) Figure 2.The effect of network depth with CNN-β-2 on CIFAR-10 for depths β = 16, 22, 28, 34 and 40. Plots show the (a) convergence curves for SGD, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training (over all independent runs). learning rate for each model over a logarithmically-spaced grid and selected the run that achieved the lowest training loss value. To measure gradient confusion, at the end of every training epoch, we sampled 100 pairs of mini-batches each of size 128 (the same size as the training batch). We calculated gradients on each mini-batch, and then computed pairwise cosine similarities. See appendix A.2 for more details on the experimental setup and architectures used. Effect of depth. To test our theoretical results, we consider CNNs with a ﬁxed width factor of 2 and varying network depth. From ﬁgure 2, we see that our theoretical results are backed by the experiments: increasing depth slows down convergence, and increases gradient confusion. We also notice that with increasing depth, the density of pairwise gradient cosine similarities concentrates less sharply around 0, which makes the network harder to train. Effect of width. We now consider CNNs with a ﬁxed depth of 16 and varying width factors. From ﬁgure 3, we see that increasing width results in faster convergence and lower gradient confusion. We further see that gradient co- sine similarities concentrate around 0 with growing width, indicating that SGD decouples across the training samples with growing width. Note that the smallest network consid- ered (CNN-16-2) is still over-parameterized and achieves a high level of performance (see appendix A.3). Effect of batch normalization and skip connections. Al- most all state-of-the-art neural networks currently contain both skip connections and normalization layers. To help un- derstand why such neural networks are so efﬁciently trained using SGD with constant learning rates, we test the effect of adding skip connections and batch normalization to CNNs of ﬁxed width and varying depth. Figure 4 shows that adding skip connections or batch normalization individually help in training deeper models, but these models still suffer from worsening results and increasing gradient confusion as the network gets deeper. When these techniques are used to- gether, the model has relatively low gradient confusion even for very deep networks, signiﬁcantly improving trainability of deep models. Note that our observations are consistent with prior work (De & Smith, 2020; Yang et al., 2019). 8. Alternate deﬁnitions of gradient confusion Note that the gradient confusion bound ηin equation 3 is deﬁned for the worst-case gradient inner product. However, all the results in this paper can be trivially extended to using a bound on the average gradient inner product of the form: ∑ N i,j=1⟨∇fi(w),∇fj(w)⟩/N2 ≥−η. In this case, all theoretical results would remain the same up to constants. We can also deﬁne a normalized variant of the gradient confusion condition: ⟨∇fi(w),∇fj(w)⟩/(∥∇fi(w)∥∥∇fj(w)∥) ≥−η. This condition inherently makes an additional assumption that the norm of the stochastic gradients, ∥∇fi(w)∥, is bounded, and thus the gradient variance is also bounded (see discussion in section 9). Thus, while all our theoretical results would qualitatively remain the same under this con- dition, we can prove tighter versions of our current results. Finally, note that gradient confusion condition in equation 3 is applicable even when the stochastic gradients are av- eraged over minibatches of size B. The variance of the gradient inner product scales down as 1/B2 in this case, and thus ηis expected to decrease as Bgrows. 9. Related work The gradient confusion bound and our theoretical results have interesting connections to prior work. In this section, we brieﬂy discuss some of these connections.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 0 25 50 75 100 125 150 175 200 epochs 10−5 10−4 10−3 10−2 10−1 100 training loss width 2 width 4 width 6 (a) 2 3 4 5 6 layer width −0.18 −0.16 −0.14 −0.12 −0.10 −0.08min grad cosine similarity  (b) −0.20 −0.15 −0.10 −0.05 0.00 0.05 0.10 0.15 0.20 pairwise gradient cosine similarity 0 2 4 6 8 10 12density width 2 width 3 width 4 width 5 width 6 (c) Figure 3.The effect of width with CNN-16-ℓon CIFAR-10 for width factorsℓ= 2, 3, 4, 5 and 6. Plots show the (a) convergence curves of SGD (for cleaner ﬁgures, we plot results for width factors 2, 4 and 6 here), (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training (over all independent runs). 20 30 40 50 60 70 80 90 100 network depth 10−4 10−3 10−2 10−1 100 final training loss no BN, no skip with BN, no skip no BN, with skip with BN & skip (a) 20 30 40 50 60 70 80 90 100 network depth −0.7 −0.6 −0.5 −0.4 −0.3 −0.2 −0.1 0.0 0.1 min grad cosine similarity no BN, no skip with BN, no skip no BN, with skip with BN & skip (b) 20 30 40 50 60 70 80 90 100 network depth 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9final test set accuracy no BN, no skip with BN, no skip no BN, with skip with BN & skip (c) Figure 4.The effect of adding skip connections and batch normalization to CNN-β-2 on CIFAR-10 for depths β = 16, 22, 28, 34, 40, 52, 76 and 100. Plots show the (a) optimal training losses, (b) minimum pairwise gradient cosine similarities, and the (c) test set accuracies at the end of training. Connections to the gradient variance : If we assume bounded gradient variance Ei∥∇fi(w) −∇F(w)∥2 ≤σ2, we can bound the gradient confusion parameter ηin terms of other quantities. For example, suppose the true gradient ∇F(w) = ∇f1(w)/2 +∇f2(w)/2. Then we can write: |⟨∇f1(w),∇f2(w)⟩| ≤σ2 + ∥∇F(w)∥2. However, in general one cannot bound the gradient variance in terms of the gradient confusion parameter. As a counter-example, consider a problem with the following distribution on the gradients: 1 1−p samples with gradient 1 ϵ and 1 p samples with gradient ϵ, where p = ϵ →0. In this case, the gradients are positive, so gradient confusion η= 0. The mean of the gradients is given by1+ ϵ(1−ϵ), which remains bounded as ϵ→0. On the other hand, the variance (and thus the squared norm of the stochastic gradients) is unbounded (O(1/ϵ) as ϵ→0). A consequence of this is that in theorems 3.1 and 3.2, the \"noise term\" (i.e., the second term in the RHS of the convergence bounds) does not depend on the learning rate in the general case. If gradients have unbounded variance, lowering the learning rate does not reduce the variance of the SGD updates, and thus does not reduce the noise term. Connections to gradient diversity: Gradient diversity (Yin et al., 2017) also measures the degree to which in- dividual gradients at different data samples are different from each other. However, the gradient diversity measure gets larger as the individual gradients become orthogonal to each other, and further increases as the gradients start pointing in opposite directions. On the other hand, gradient confusion between two individual gradients is zero unless the inner product between them is negative. As we show in this paper, this has important implications when we study the convergence of SGD in the over-parameterized setting: increased width makes gradients more orthogonal to each other improving trainability, while increased depth result in gradients pointing in opposite directions making networks harder to train. Thus, we view our papers to be complemen- tary, providing insights about different issues (large batch distributed training vs. small batch convergence). Related work on the impact of network architecture: Balduzzi et al. (2017) studied neural networks with ReLU activations at Gaussian initializations, and showed that gra- dients become increasingly negatively correlated with depth.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Hanin (2018) showed that the variance of gradients in fully connected networks with ReLU activations is exponential in the sum of the reciprocals of the hidden layer widths at Gaussian initializations. In a follow-up work, Hanin & Rol- nick (2018) showed that this sum of the reciprocals of the hidden layer widths determines the variance of the sizes of the activations at each layer. When this sum of reciprocals is too large, early training dynamics are very slow, suggesting the difﬁculties of starting training on deeper networks, as well as the beneﬁts of increased width. Other work on SGD convergence: There has recently been a lot of interest in analyzing conditions under which SGD converges to global minimizers of over-parameterized linear and non-linear neural networks. Arora et al. (2018) shows SGD converges linearly to global minimizers for linear neural networks under certain conditions. Du et al. (2018); Allen-Zhu et al. (2018); Zou et al. (2018); Brutzkus et al. (2017) also show convergence to global minimizers of SGD for non-linear networks. This paper complements these recent results by studying how low gradient confusion contributes to SGD’s success on over-parameterized neural networks used in practice. 10. Discussion In this paper, we study how neural network architecture af- fects the trainability of networks and the dynamics of SGD. To rigorously analyze this, we introduce a concept called gra- dient confusion, and show that when gradient confusion is low, SGD has fast convergence. We show at standard Gaus- sian initializations, increasing layer width leads to lower gradient confusion, making the model easier to train. In con- trast, increasing depth results in higher gradient confusion, making models harder to train. These results indicate that increasing the layer width with the network depth is impor- tant to maintain trainability of the neural network. This is supported by other recent work that suggest that the width should increase linearly with depth in a Gaussian-initialized neural network to help dynamics early in training (Hanin, 2018; Hanin & Rolnick, 2018). Many previous results have shown how deeper models are more efﬁcient at modeling higher complexity function classes than wider models, and thus depth is essential for the success of neural networks (Eldan & Shamir, 2016; Tel- garsky, 2016). Indeed, practitioners over the years have achieved state-of-the-art results on various tasks by mak- ing networks deeper, without necessarily making networks wider. We thus study techniques that enable us to train deep models without requiring us to increase the width with depth. Most state-of-the-art neural networks currently contain both skip connections and normalization layers. We thus, empir- ically study the effect of introducing batch normalization and skip connections to a neural network. We show that the combination of batch normalization and skip connections lower gradient confusion and help train very deep models, explaining why many neural networks used in practice are so efﬁciently trained. Furthermore, we show how orthogo- nal initialization techniques provide a promising direction for improving the trainability of very deep networks. Our results provide a number of important insights that can be used for neural network model design. We demonstrate that the gradient confusion condition could be useful as a measure of trainability of networks, and thus could po- tentially be used to develop algorithms for more efﬁcient training. Additionally, the correlation between gradient confusion and the test set accuracies shown in appendix A suggest that an interesting topic for future work would be to investigate the connection between gradient confusion and generalization (Fort et al., 2019). Our results also suggest the importance of further work on orthogonal initialization schemes for neural networks with non-linear activations that make training very deep models possible. Acknowledgements The authors thank Brendan Oâ ˘A ´ZDonoghue, Aleksandar Botev, James Martens, Sudha Rao, and Samuel L. Smith for helpful discussions and for reviewing earlier versions of this manuscript. This paper was supported by the ONR MURI program, AFOSR MURI Program, and the National Science Foundation DMS directorate. References Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for deep learning via over-parameterization. arXiv preprint arXiv:1811.03962, 2018. Arora, S., Cohen, N., Golowich, N., and Hu, W. A conver- gence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018. Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D., and McWilliams, B. The shattered gradients problem: If resnets are the answer, then what is the question? arXiv preprint arXiv:1702.08591, 2017. Bengio, Y ., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is difﬁcult. IEEE transactions on neural networks, 5(2):157–166, 1994. Bertsekas, D. P. Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. Optimization for Machine Learning, 2010(1-38):3, 2011. Boucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence . Oxford university press, 2013.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Brutzkus, A., Globerson, A., Malach, E., and Shalev- Shwartz, S. Sgd learns over-parameterized networks that provably generalize on linearly separable data. arXiv preprint arXiv:1710.10174, 2017. Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y ., Bal- dassi, C., Borgs, C., Chayes, J., Sagun, L., and Zecchina, R. Entropy-sgd: Biasing gradient descent into wide val- leys. arXiv preprint arXiv:1611.01838, 2016. Cooper, Y . The loss landscape of overparameterized neural networks. arXiv preprint arXiv:1804.10200, 2018. Darken, C. and Moody, J. Towards faster stochastic gradient search. In Advances in neural information processing systems, pp. 1009–1016, 1992. De, S. and Smith, S. L. Batch normalization biases residual blocks towards the identity function in deep networks. arXiv preprint arXiv:2002.10444, 2020. De, S., Yadav, A., Jacobs, D., and Goldstein, T. Automated inference with adaptive batches. In Artiﬁcial Intelligence and Statistics, pp. 1504–1513, 2017. Du, S. S., Zhai, X., Poczos, B., and Singh, A. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018. Dziugaite, G. K. and Roy, D. M. Computing nonvacuous generalization bounds for deep (stochastic) neural net- works with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017. Eldan, R. and Shamir, O. The power of depth for feedfor- ward neural networks. In Conference on Learning Theory, pp. 907–940, 2016. Fort, S., Nowak, P. K., and Narayanan, S. Stiffness: A new perspective on generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019. Ghorbani, B., Krishnan, S., and Xiao, Y . An investiga- tion into neural net optimization via hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019. Glorot, X. and Bengio, Y . Understanding the difﬁculty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artiﬁcial intelligence and statistics, pp. 249–256, 2010. Hanin, B. Which neural net architectures give rise to ex- ploding and vanishing gradients? In Advances in Neural Information Processing Systems, pp. 582–591, 2018. Hanin, B. and Rolnick, D. How to start training: The effect of initialization and architecture. In Advances in Neural Information Processing Systems, pp. 571–581, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE inter- national conference on computer vision, pp. 1026–1034, 2015. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8580–8589, 2018. Karimi, H., Nutini, J., and Schmidt, M. Linear conver- gence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition. InJoint European Con- ference on Machine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems , pp. 1097–1105, 2012. LeCun, Y . A., Bottou, L., Orr, G. B., and Müller, K.-R. Efﬁcient backprop. In Neural networks: Tricks of the trade, pp. 9–48. Springer, 2012. Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y ., Sohl- Dickstein, J., and Pennington, J. Wide neural networks of any depth evolve as linear models under gradient descent. arXiv preprint arXiv:1902.06720, 2019. Lojasiewicz, S. Ensembles semi-analytiques. Lectures Notes IHES (Bures-sur-Yvette), 1965. Ma, S., Bassily, R., and Belkin, M. The power of in- terpolation: Understanding the effectiveness of sgd in modern over-parametrized learning. arXiv preprint arXiv:1712.06559, 2017. Martens, J. Second-order optimization for neural networks. University of Toronto (Canada), 2016. Milman, V . D. and Schechtman, G. Asymptotic Theory of Finite Dimensional Normed Spaces. Springer-Verlag, Berlin, Heidelberg, 1986. ISBN 0-387-16769-2. Moulines, E. and Bach, F. R. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems, pp. 451–459, 2011.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Nagarajan, V . and Kolter, J. Z. Generalization in deep networks: The role of distance from initialization. arXiv preprint arXiv:1901.01672, 2019. Nedi´c, A. and Bertsekas, D. Convergence rate of incre- mental subgradient algorithms. In Stochastic optimiza- tion: algorithms and applications, pp. 223–264. Springer, 2001. Needell, D., Ward, R., and Srebro, N. Stochastic gradient de- scent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems, pp. 1017–1025, 2014. Nesterov, Y .Lectures on convex optimization, volume 137. Springer, 2018. Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y ., and Srebro, N. Towards understanding the role of over- parametrization in generalization of neural networks. arXiv preprint arXiv:1805.12076, 2018. Nguyen, Q. and Hein, M. The loss surface of deep and wide neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2603– 2612. JMLR. org, 2017. Oymak, S. and Soltanolkotabi, M. Overparameterized non- linear learning: Gradient descent takes the shortest path? arXiv preprint arXiv:1812.10004, 2018. Robbins, H. and Monro, S. A stochastic approximation method. The annals of mathematical statistics, pp. 400– 407, 1951. Sagun, L., Evci, U., Guney, V . U., Dauphin, Y ., and Bottou, L. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Schmidt, M. and Roux, N. L. Fast convergence of stochastic gradient descent under a strong growth condition. arXiv preprint arXiv:1308.6370, 2013. Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl- Dickstein, J. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016. Sedghi, H., Gupta, V ., and Long, P. M. The singular values of convolutional layers. arXiv preprint arXiv:1805.10408, 2018. Shamir, O. and Zhang, T. Stochastic gradient descent for non-smooth optimization: Convergence results and opti- mal averaging schemes. In International Conference on Machine Learning, pp. 71–79, 2013. Simonyan, K. and Zisserman, A. Very deep convolu- tional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Smith, S. L., Elsen, E., and De, S. On the generalization beneﬁt of noise in stochastic gradient descent. arXiv preprint arXiv:2006.15081, 2020. Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the importance of initialization and momentum in deep learn- ing. In International conference on machine learning, pp. 1139–1147, 2013. Tao, T. Topics in random matrix theory, volume 132. Amer- ican Mathematical Soc., 2012. Telgarsky, M. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016. Vaswani, S., Bach, F., and Schmidt, M. Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron. arXiv preprint arXiv:1810.07288, 2018. Vershynin, R. High-dimensional probability: An introduc- tion with applications in data science, volume 47. Cam- bridge University Press, 2018. Wilson, A. C., Roelofs, R., Stern, M., Srebro, N., and Recht, B. The marginal value of adaptive gradient methods in machine learning. In Advances in Neural Information Processing Systems, pp. 4151–4161, 2017. Wu, L., Zhu, Z., et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Xiao, L., Bahri, Y ., Sohl-Dickstein, J., Schoenholz, S. S., and Pennington, J. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla convolu- tional neural networks. arXiv preprint arXiv:1806.05393, 2018. Yang, G., Pennington, J., Rao, V ., Sohl-Dickstein, J., and Schoenholz, S. S. A mean ﬁeld theory of batch normal- ization. arXiv preprint arXiv:1902.08129, 2019. Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ram- chandran, K., and Bartlett, P. Gradient diversity: a key ingredient for scalable distributed learning.arXiv preprint arXiv:1706.05699, 2017. Zagoruyko, S. and Komodakis, N. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Appendix We ﬁrst brieﬂy outline the different sections in the appendix. • In appendix A, we provide details of our experimental setup, and provide additional empirical results on fully connected networks, convolutional networks and residual networks with the MNIST, CIFAR-10 and CIFAR-100 datasets. • In appendix B, we state and prove a lemma on the near orthogonality of random vectors, which we refer to in the main text. This result is often attributed to Milman & Schechtman (1986). • In appendix C, we provide some intuition on why many standard over-parameterized neural networks with low-rank Hessians might have low gradient confusion for a large set of weights near the minimizer. • In appendix D, we provide the proofs of the theorems presented in the main section. In appendix D.1, we provide proofs of theorems 3.1 and 3.2. In appendix D.2, we provide the proof of lemma D.1, which we refer to in the main text. In appendix D.3, we provide proofs of theorem 5.1 and corollary 5.1. In appendix D.4, we provide the proof of theorem 4.1. In appendix D.5, we provide the proof of theorem 6.1. • In appendix E, we brieﬂy describe a few lemmas that we require in our analysis. • In appendix F, we discuss the small weights assumption (assumption 1), which is required for theorem 5.1, corollary 5.1 and theorem 6.1 in the main text. A. Additional experimental results In this section, we present more details about our experimental setup, as well as, additional experimental results on a range of models (MLPs, CNNs and Wide ResNets) and a range of datasets (MNIST, CIFAR-10, CIFAR-100). A.1. MLPs on MNIST To further test the main claims in the paper, we performed additional experiments on an image classiﬁcation problem on the MNIST dataset using fully connected neural networks. We iterated over neural networks of varying depth and width, and considered both the identity activation function (i.e., linear neural networks) and the tanh activation function. We also considered two different weight initializations that are popularly used and appropriate for these activation functions: • The Glorot normal initializer (Glorot & Bengio, 2010) with weights initialized by sampling from the distribution N ( 0,2/(fan-in + fan-out) ) , where fan-in denotes the number of input units in the weight matrix, and fan-out denotes the number of output units in the weight matrix. • The LeCun normal initializer (LeCun et al., 2012) with weights initialized by sampling from the distribution N ( 0,1/fan-in ) . We considered the simpliﬁed case where all hidden layers have the same width ℓ. Thus, the ﬁrst weight matrix W0 ∈Rℓ×d, where d= 784for the 28 ×28-sized images of MNIST; all intermediate weight matrices {Wp}p∈[β−1] ∈Rℓ×ℓ; and the ﬁnal layer Wβ ∈R10×ℓ for the 10 image classes in MNIST. We added biases to each layer, which we initialized to 0. We used softmax cross entropy as the loss function. We use MLP-β-ℓto denote this fully connected network of depth βand width ℓ. We used the standard train-valid-test splits of 40000-10000-10000 for MNIST. This relatively simple model gave us the ability to iterate over a large number of combinations of network architectures of varying width and depth, and different activation functions and weight initializations. Linear neural networks are an efﬁcient way to directly understand the effect of changing depth and width without increasing model complexity over linear regression. Thus, we considered both linear and non-linear neural networks in our experiments. We used SGD with constant learning rates for training with a mini-batch size of 128 and trained each model for 40000 iterations (more than 100 epochs). The constant learning rate αwas tuned over a logarithmically-spaced grid: α∈{100,10−1,10−2,10−3,10−4,10−5,10−6}.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent We ran each experiment 10 times (making sure at least 8 of them ran till completion), and picked the learning rate that achieved the lowest training loss value on average at the end of training. Our grid search was such that the optimal learning rate never occurred at one of the extreme values tested. To measure gradient confusion at the end training, we sampled 1000 pairs of mini-batches each of size 128 (the same size as the training batch size). We calculated gradients on each of these pairs of mini-batches, and then calculated the cosine similarity between them. To measure the worse-case gradient confusion, we computed the lowest gradient cosine similarity among all pairs. We explored the effect of changing depth and changing width on the different activation functions and weight initializations. We plot the ﬁnal training loss achieved for each model and the minimum gradient cosine similarities calculated over the 1000 pairs of gradients at the end of training. For each point, we plot both the mean and the standard deviation over the 10 independent runs. The effect of depth. We ﬁrst present results showing the effect of network depth. We considered a ﬁxed width of ℓ= 100, and varied the depth of the neural network, on the log scale, as: β ∈{3,10,30,100,300,1000}. Figure 5 shows results on neural networks with identity and tanh activation functions for the two weight initializations considered (Glorot normal and LeCun normal). Similar to the experimental results in section 7, and matching our theoretical results in sections 4 and 5, we notice the consistent trend of gradient confusion increasing with increasing depth. This makes the networks harder to train with increasing depth, and this is evidenced by an increase in the ﬁnal training loss value. By depth β = 1000, the increased gradient confusion effectively makes the network untrainable when using tanh non-linearities. The effect of width. We explored the effect of width by varying the width of the neural network while keeping the depth ﬁxed at β = 300. We chose a very deep model, which is essentially untrainable for small widths (with standard initialization techniques) and helps better illustrate the effects of increasing width. We varied the width of the network, again on the log scale, as: ℓ∈{10,30,100,300,1000}. Crucially, note that the smallest network considered here, MLP-300-10, still has more than 50000 parameters (i.e., more than the number of training samples), and the network with width ℓ= 30has almost three times the number of parameters as the high-performing MLP-3-100 network considered in the previous section. Figure 6 show results on linear neural networks and neural networks with tanh activations for both the Glorot normal and LeCun normal initializations. As in the experimental results of section 7, we see the consistent trend of gradient confusion decreasing with increasing width. Thus, wider networks become easier to train and improve the ﬁnal training loss value. We further see that when the width is too small (ℓ= 30), the gradient confusion becomes drastically high and the network becomes completely untrainable. A.2. Additional experimental details for CNNs and WRNs In this section, we review the details of our setup for the image classiﬁcation experiments on CNNs and WRNs on the CIFAR-10 and CIFAR-100 datasets. WIDE RESIDUAL NETWORKS The Wide ResNet (WRN) architecture (Zagoruyko & Komodakis, 2016) for CIFAR datasets is a stack of three groups of residual blocks. There is a downsampling layer between two blocks, and the number of channels (width of a convolutional layer) is doubled after downsampling. In the three groups, the width of convolutional layers is {16ℓ,32ℓ,64ℓ}, respectively. Each group contains βr residual blocks, and each residual block contains two 3 ×3 convolutional layers equipped with ReLU activation, batch normalization and dropout. There is a 3 ×3 convolutional layer with 16 channels before the three groups of residual blocks. And there is a global average pooling, a fully-connected layer and a softmax layer after the three groups. The depth of WRN is β = 6βr + 4. For our experiments, we turned off dropout. Unless otherwise speciﬁed, we also turned off batch normalization. We added biases to the convolutional layers when not using batch normalization to maintain model expressivity. We used the MSRA initializer (He et al., 2015) for the weights as is standard for this model, and used the same preprocessing steps for the CIFAR images as described in Zagoruyko & Komodakis (2016). This preprocessing step involves normalizing the images and doing data augmentation (Zagoruyko & Komodakis, 2016). We denote this network as WRN-β-ℓ, where βrepresents the depth and ℓrepresents the width factor of the network.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 101 102 103 depth 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6final training loss value (a) Linear NN, Glorot init 101 102 103 depth 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 min grad cosine similarity  (b) Linear NN, Glorot init 101 102 103 depth 0.0 0.5 1.0 1.5final training loss value  (c) Linear NN, LeCun init 101 102 103 depth 1.0 0.8 0.6 0.4 0.2 min grad cosine similarity  (d) Linear NN, LeCun init 101 102 103 depth 0.0 0.5 1.0 1.5 2.0 2.5final training loss value (e) Tanh NN, Glorot init 101 102 103 depth 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 min grad cosine similarity  (f) Tanh NN, Glorot init 101 102 103 depth 0.0 0.5 1.0 1.5 2.0 2.5final training loss value  (g) Tanh NN, LeCun init 101 102 103 depth 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 min grad cosine similarity  (h) Tanh NN, LeCun init Figure 5.Effect of varying depth on MLP-β-100. 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value (a) Linear NN, Glorot init 101 102 103 width 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 min grad cosine similarity  (b) Linear NN, Glorot init 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value  (c) Linear NN, LeCun init 101 102 103 width 1.2 1.0 0.8 0.6 0.4 0.2 0.0 min grad cosine similarity  (d) Linear NN, LeCun init 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value (e) Tanh NN, Glorot init 101 102 103 width 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 min grad cosine similarity  (f) Tanh NN, Glorot init 101 102 103 width 0.0 0.5 1.0 1.5 2.0 2.5final training loss value  (g) Tanh NN, LeCun init 101 102 103 width 1.2 1.0 0.8 0.6 0.4 min grad cosine similarity  (h) Tanh NN, LeCun init Figure 6.Effect of varying width on MLP-300-ℓ. To study the effect of depth, we considered WRNs with width factor ℓ= 2and depth varying as: β ∈{16,22,28,34,40,52,76,100}. For cleaner ﬁgures, we sometimes plot a subset of these results: β ∈{16,28,40,52,76,100}.To study the effect of width, we considered WRNs with depth β = 16and width factor varying as: ℓ∈{2,3,4,5,6}.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent CONVOLUTIONAL NEURAL NETWORKS The WRN architecture contains skip connections that, as we show, help in training deep networks. To consider VGG-like convolutional networks, we consider a family of networks where we remove the skip connections from WRNs. Following the WRN convention, we denote these networks as CNN-β-ℓ, where βdenotes the depth and ℓdenotes the width factor. To study the effect of depth, we considered CNNs with width factor ℓ= 2and depth varying as: β ∈{16,22,28,34,40}. To study the effect of width, we considered CNNs with depth β = 16and width factor varying as: ℓ∈{2,3,4,5,6}. HYPERPARAMETER TUNING AND OTHER DETAILS We used SGD as the optimizer without any momentum. Following Zagoruyko & Komodakis (2016), we ran all experiments for 200 epochs with minibatches of size 128, and reduced the initial learning rate by a factor of 10 at epochs 80 and 160. We turned off weight decay for all our experiments. We ran each individual experiment 5 times. We ignored any runs that were unable to decrease the loss from its initial value. We also made sure at least 4 out of the 5 independent runs ran till completion. When the learning rate is close to the threshold at which training is still possible, some runs may converge, while others may fail to converge. Thus, these checks ensure that we pick a learning rate that converges reliably in most cases on each problem. We show the standard deviation across runs in our plots. We tuned the optimal initial learning rate for each model over a logarithmically-spaced grid: α∈{101,3 ×100,100,3 ×10−1,10−1,3 ×10−2,10−2,3 ×10−3,10−3,3 ×10−4,10−4,3 ×10−5}, and selected the run that achieved the lowest ﬁnal training loss value (averaged over the independent runs). Our grid search was such that the optimal learning rate never occurred at one of the extreme values tested. We used the standard train-valid-test splits of 40000-10000-10000 for CIFAR-10 and CIFAR-100. To measure gradient confusion, at the end of every training epoch, we sampled 100 pairs of mini-batches each of size 128 (the same size as the training batch size). We calculated gradients on each mini-batch, and then computed pairwise cosine similarities. To measure the worse-case gradient confusion, we computed the lowest gradient cosine similarity among all pairs. We also show the kernel density estimation of the pairwise gradient cosine similarities of the 100 minibatches sampled at the end of training (after 200 epochs), to see the concentration of the distribution. To do this, we combine together the 100 samples for each independent run and then perform kernel density estimation with a gaussian kernel on this data. A.3. Additional plots for CIFAR-10 on CNNs In section 7, we showed results for image classiﬁcation using CNNs on CIFAR-10. In this section, we show some additional plots for this experiment. Figure 7 shows the effect of changing the depth, while ﬁgure 8 shows the effect of changing the width factor of the CNN. We see that the ﬁnal training loss and test set accuracy values show the same trends as in section 7: deeper networks are harder to train, while wider networks are easier to train. As mentioned previously, theorems 3.1 and 3.2 indicate that we would expect the effect of gradient confusion to be more prominent near the end of training. From the plots we see that deeper networks have higher gradient confusion close to minimum, while wider networks have lower gradient confusion close to the minimum. A.4. CIFAR-100 on CNNs We now consider image classiﬁcations tasks with CNNs on the CIFAR-100 dataset. Figure 9 shows the effect of varying depth, while ﬁgure 10 shows the effect of varying width. We notice the same trends as in our results with CNNs on CIFAR-10. Interestingly, from the width results in ﬁgure 10, we see that while there is no perceptible change to the minimum pairwise gradient cosine similarity, the distribution still sharply concentrates around 0 with increasing width. Thus more gradients become orthogonal to each other with increasing width, implying that SGD on very wide networks becomes closer to decoupling over the data samples.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 15 20 25 30 35 40 network depth 0.0 0.5 1.0 1.5 2.0 2.5final training loss (a) 15 20 25 30 35 40 network depth 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9final test set accuracy  (b) 0 50 100 150 200 epochs 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 min grad cosine similarity depth 16 depth 22 depth 28 depth 34 depth 40 (c) Figure 7.The effect of network depth with CNN-β-2 on CIFAR-10. The plots show the (a) ﬁnal training loss values at the end of training, (b) ﬁnal test set accuracy values at the end of training, and (c) the minimum of pairwise gradient cosine similarities during training. 2 3 4 5 6 layer width 10-4 10-3 10-2 final training loss (a) 2 3 4 5 6 layer width 0.80 0.82 0.84 0.86 0.88 0.90 0.92final test set accuracy  (b) 0 50 100 150 200 epochs 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15 0.20 min grad cosine similarity width 2 width 4 width 6 (c) Figure 8.The effect of width with CNN-16-ℓon CIFAR-10. The plots show the (a) ﬁnal training loss values at the end of training, (b) ﬁnal test set accuracy values at the end of training, and the (c) minimum of pairwise gradient cosine similarities during training. A.5. Image classiﬁcation with WRNs on CIFAR-10 and CIFAR-100 We now show results for image classiﬁcation problems using wide residual networks (WRNs) on CIFAR-10 and CIFAR- 100. The WRNs we consider do not have any batch normalization. Later we show results on the effect of adding batch normalization to these networks. Figures 11 and 12 show results on the effect of depth using WRNs on CIFAR-10 and CIFAR-100 respectively. We again see the consistent trend of deeper networks having higher gradient confusion, making them harder to train. We further see that increasing depth results in the pairwise gradient cosine similarities concentrating less around 0. Figures 13 and 14 show results on the effect of width using WRNs on CIFAR-10 and CIFAR-100 respectively. We see that increasing width typically lowers gradient confusion and helps the network achieve lower loss values. The pairwise gradient cosine similarities also typically concentrate around 0 with higher width. We also notice from these ﬁgures that in some cases, increasing width might lead to diminishing returns, i.e., the beneﬁts of increased width diminish after a certain point, as one would expect. A.6. Effect of batch normalization and skip connections In section 7 we showed results on the effect of adding batch normalization and skip connections to CNNs and WRNs on an image classiﬁcation task on CIFAR-10. In this section, we present similar results for image classiﬁcation on CIFAR-100. Similar to section 7, ﬁgure 15 shows that adding skip connections or batch normalization individually help in training deeper models, but these models still suffer from worsening results and increasing gradient confusion as the network gets deeper. Both these techniques together keep the gradient confusion relatively low even for very deep networks, signiﬁcantly improving trainability of deep models.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 15 20 25 30 35 network depth 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5final training loss (a) 15 20 25 30 35 network depth 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 min grad cosine similarity  (b) 0.4  0.2  0.0 0.2 0.4 pairwise gradient cosine similarity 0 2 4 6 8 10 12density depth 16 depth 22 depth 28 depth 34 (c) Figure 9.The effect of network depth with CNN-β-2 on CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. 2 3 4 5 6 layer width 10-4 10-3 10-2 10-1 final training loss (a) 2 3 4 5 6 layer width 0.16 0.14 0.12 0.10 0.08 0.06 0.04 0.02 min grad cosine similarity  (b) 0.10  0.05  0.00 0.05 0.10 pairwise gradient cosine similarity 0 5 10 15 20 25density width 2 width 3 width 4 width 5 width 6 (c) Figure 10.The effect of width with CNN-16- ℓon CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. B. Near orthogonality of random vectors For completeness, we state and prove below a lemma on the near orthogonality of random vectors. This result is often attributed to Milman & Schechtman (1986). Lemma B.1 (Near orthogonality of random vectors) . For vectors {xi}i∈[N] drawn uniformly from a unit sphere in d dimensions, and ν >0, Pr [ ∃i,j |x⊤ i xj|>ν ] ≤N2√ π 8 exp ( −d−1 2 ν2) . Proof. Given a ﬁxed vector x,a uniform random vector y satisﬁes |x⊤y|≥ νonly if y lies in one of two spherical caps: one centered at x and the other at −x,and both with angular radius cos−1(ν) ≤π 2 −ν.A simple result often attributed to Milman & Schechtman (1986) bounds the probability of lying in either of these caps as Pr[|x⊤y|≥ ν] ≤ √ π 2 exp ( −d−1 2 ν2 ) . (5) Because of rotational symmetry, the bound (5) holds if both x and y are chosen uniformly at random. We next apply a union bound to control the probability that |x⊤ i xj|≥ νfor some pair (i,j).There are fewer than N2/2 such pairs, and so the probability of this condition is Pr[|x⊤ i xj|≥ ν,for some i,j] ≤N2 2 √ π 2 exp ( −d−1 2 ν2 ) .The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 30 40 50 60 70 80 90 100 network depth 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 final training loss (a) 30 40 50 60 70 80 90 100 network depth 0.35 0.30 0.25 0.20 0.15 0.10 0.05 min grad cosine similarity  (b) 0.4  0.2  0.0 0.2 0.4 pairwise gradient cosine similarity 0 2 4 6 8 10 12 14 16density depth 28 depth 40 depth 52 depth 76 depth 100 (c) Figure 11.The effect of depth with WRN-β-2 (no batch normalization) on CIFAR-10. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. 30 40 50 60 70 80 90 100 network depth 10-4 10-3 10-2 10-1 100 101 final training loss (a) 30 40 50 60 70 80 90 100 network depth 0.22 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 0.04 min grad cosine similarity  (b) 0.20  0.15  0.10  0.05  0.00 0.05 0.10 0.15 0.20 pairwise gradient cosine similarity 0 5 10 15 20 25density depth 28 depth 40 depth 52 depth 76 depth 100 (c) Figure 12.The effect of depth with WRN-β-2 (no batch normalization) on CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. C. Low-rank Hessians lead to low gradient confusion In this section, we show that low-rank random Hessians result in low gradient confusion. For clarity in presentation, suppose each fi has a minimizer at the origin (the same argument can be easily extended to the more general case). Suppose also that there is a Lipschitz constant for the Hessian of each function fi that satisﬁes ∥Hi(w) −Hi(w′)∥≤ LH∥w −w′∥(note that this is a standard optimization assumption (Nesterov, 2018), with evidence that it is applicable for neural networks (Martens, 2016)). Then ∇fi(w) = Hiw + e, where e is an error term bounded as: ∥e∥≤ 1 2 LH∥w∥2,and we use the shorthand Hi to denote Hi(0).Then we have: |⟨∇fi(w),∇fj(w)⟩|= |⟨Hiw,Hjw⟩|+ ⟨e,Hiw + Hjw⟩+ ∥e∥2 ≤∥w∥2∥Hi∥∥Hj∥+ ∥e∥∥w∥(∥Hi∥+ ∥Hj∥) +∥e∥2 ≤∥w∥2∥Hi∥∥Hj∥+ 1 2LH∥w∥3(∥Hi∥+ ∥Hj∥) +1 4L2 H∥w∥4. If the Hessians are sufﬁciently random and low-rank (e.g., of the form Hi = aia⊤ i where ai ∈RN×r are randomly sampled from a unit sphere), then one would expect the terms in this expression to be small for all w within a neighborhood of the minimizer. There is evidence that the Hessian at the minimizer is very low rank for many standard over-parameterized neural network models (Sagun et al., 2017; Cooper, 2018; Chaudhari et al., 2016; Wu et al., 2017; Ghorbani et al., 2019). While a bit non-rigorous, the above result nonetheless suggests that for many standard neural network models, the gradient confusion might be small for a large class of weights near the minimizer.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 2 3 4 5 6 layer width 10-4 10-3 10-2 final training loss (a) 2 3 4 5 6 layer width 0.24 0.22 0.20 0.18 0.16 0.14 0.12 0.10 0.08 0.06 min grad cosine similarity  (b) 0.20  0.15  0.10  0.05  0.00 0.05 0.10 0.15 0.20 pairwise gradient cosine similarity 0 2 4 6 8 10 12 14 16 18density width 2 width 3 width 4 width 5 width 6 (c) Figure 13.The effect of width with WRN-16-ℓ(no batch normalization) on CIFAR-10. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. 2 3 4 5 6 layer width 10-4 10-3 10-2 10-1 final training loss (a) 2 3 4 5 6 layer width 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00 min grad cosine similarity  (b) 0.08  0.06  0.04  0.02  0.00 0.02 0.04 0.06 0.08 pairwise gradient cosine similarity 0 5 10 15 20 25 30 35density width 2 width 3 width 4 width 5 width 6 (c) Figure 14.The effect of width with WRN-16-ℓ(no batch normalization) on CIFAR-100. The plots show the (a) training loss values at the end of training, (b) minimum of pairwise gradient cosine similarities at the end of training, and the (c) kernel density estimate of the pairwise gradient cosine similarities at the end of training. D. Missing proofs D.1. Proofs of theorems 3.1 and 3.2 This section presents proofs for the convergence theorems of SGD presented in section 3, under the assumption of low gradient confusion. For clarity of presentation, we re-state each theorem before its proof. Theorem 3.1. If the objective function satisﬁes (A1) and (A2), and has gradient confusion η, SGD converges linearly to a neighborhood of the minima of problem (1) as: E[F(wT) −F⋆] ≤ρT(F(w0) −F⋆) + αη 1−ρ, where α< 2 NL, ρ= 1−2µ N ( α−NLα2 2 ) , F⋆ = minw F(w) and w0 is the initialized weights. Proof. Let ˜i∈[N] denote the index of the realized function ˜fk in the uniform sampling from {fi}i∈[N] at step k. From assumption (A1), we have F(wk+1) ≤F(wk) +⟨∇F(wk), wk+1 −wk⟩+ L 2 ∥wk+1 −wk∥2 = F(wk) −α⟨∇F(wk), ∇˜fk(wk)⟩+ Lα2 2 ∥∇˜fk(wk)∥2 = F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 −α N ∑ ∀i:i̸=˜i ⟨∇fi(wk), ∇˜fk(wk)⟩The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent 20 30 40 50 60 70 80 90 100 network depth 10-5 10-4 10-3 10-2 10-1 100 101 final training loss no BN, no skip with BN, no skip no BN, with skip with BN & skip (a) 20 30 40 50 60 70 80 90 100 network depth 0.6 0.5 0.4 0.3 0.2 0.1 0.0 min grad cosine similarity no BN, no skip with BN, no skip no BN, with skip with BN & skip (b) 20 30 40 50 60 70 80 90 100 network depth 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8final test set accuracy no BN, no skip with BN, no skip no BN, with skip with BN & skip (c) Figure 15.The effect of adding skip connections and batch normalization to CNN-β-2 on CIFAR-100. Plots show the (a) training loss, (b) minimum pairwise gradient cosine similarities, and the (c) test accuracies at the end of training. ≤F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 + α(N −1)η N , ≤F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 + αη, where the second-last inequality follows from deﬁnition 2.1. Let the learning rate α< 2/NL. Then, using assumption (A2) and subtracting by F⋆ = minw F(w) on both sides, we get F(wk+1) −F⋆ ≤F(wk) −F⋆ −2µ (α N −Lα2 2 ) ( ˜fk(wk) −˜f⋆ k) +αη, where ˜f⋆ k = minw ˜fk(w). It is easy to see that by deﬁnition we have, Ei[f⋆ i ] ≤F⋆. Moreover, from assumption that α< 2 NL, it implies that ( α N −Lα2 2 ) >0. Therefore, taking expectation on both sides we get, E[F(wk+1) −F⋆] ≤ ( 1 −2µα N + µLα2 ) E[F(wk) −F⋆] +αη. Writing ρ= 1−2µα N + µLα2, and unrolling the iterations, we get E[F(wk+1) −F⋆] ≤ρk+1(F(w0) −F⋆) + k∑ i=0 ρiαη ≤ρk+1(F(w0) −F⋆) + ∞∑ i=0 ρiαη = ρk+1(F(w0) −F⋆) + αη 1 −ρ. Theorem 3.2. If the objective satisﬁes (A1) and has gradient confusion η, then SGD converges to a neighborhood of a stationary point of problem (1) as: mink=1,...,T E∥∇F(wk)∥2 ≤ρ(F(w1)−F⋆) T + ρη, for α< 2 NL, ρ= 2N 2−NLα, and F⋆ = minw F(w). Proof. From theorem 3.1, we have: F(wk+1) ≤F(wk) − (α N −Lα2 2 ) ∥∇˜fk(wk)∥2 + αη. (6)The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Now we know that: E∥∇˜fk(wk)∥2 = E∥∇˜fk(wk) −∇F(wk)∥2 + E∥∇F(wk)∥2 ≥E∥∇F(wk)∥2. Thus, taking expectation and assuming the step size α< 2/(NL), we can rewrite equation 6 as: E∥∇F(wk)∥2 ≤ 2N 2α−NLα2 E[F(wk) −F(wk+1)] + 2Nη 2 −NLα. Taking an average overT iterations, and using F⋆ = minw F(w), we get: min k=1,...,T E∥∇F(wk)∥2 ≤ 1 T ⊤∑ k=1 E∥∇F(wk)∥2 ≤ 2N 2α−NLα2 F(w1) −F⋆ T + 2Nη 2 −NLα. D.2. Proof of lemma D.1 Lemma D.1. Consider the set of loss-functions {fi(W)}i∈[N] where all fi are either the square-loss function or the logistic-loss function. Recall that fi(W) :=f(W,xi). Consider a feed-forward neural network as deﬁned in equation 4 whose weights W satisfy assumption 1. Consider the gradient ∇Wfi(W) of each function fi. From deﬁnition we have that ∇Wfi(W) =ζxi(W)∇WgW(xi), where we deﬁne ζxi(W) =∂fi(W)/∂gW. Then we have the following properties. 1. When ∥x∥≤ 1 for every p∈[β] we have ∥∇WpgW(xi)∥≤ 1. 2. There exists 0 <ζ0 ≤2√β, such that |ζxi(W)|≤ 2 , ∥∇xiζxi(W)∥2 ≤ζ0 , ∥∇Wζxi(W)∥2 ≤ζ0. Proof. The ﬁrst property is a direct consequence of assumption 1 and property (P2) of the activation function. Let W denote the tuple (Wp)p∈[β]0 . Consider |ζxi(W)|= |∂fi(W)/∂gW|. In the case of square-loss function this evaluates to |gW(x) −C(x)|≤ 2. In case of logistic regression, this evaluates to | −1 1+exp(C(xi)gW(xi)) |≤ 1. Now we consider ∥∇xiζxi(W)∥. Consider the squared loss function. We then have the following. ∥∇xiζxi(W)∥= ∥∇xif′(W)∥ = ∥∇xigW(xi) −C(xi)∥ ≤∥∇xigW(xi)∥+ 1. Likewise, consider the logistic-loss function. We then have the following. ∥∇xiζxi(W)∥≤  C(xi)2 (1 + exp(C(xi)gW(xi)))2 exp(C(xi)gW(xi)) ∥∇xigW(xi)∥ ≤∥∇xigW(xi)∥. Thus, it sufﬁces to bound ∥∇xigW(xi)∥. Using assumption 1 and the properties (P1), (P2) of σ, this can be upper-bounded by 1. Consider ∇Wpζxi(W) for some layer index p ∈[β]0. We will show that ∥∇Wpζxi(W)∥2 ≤2. Then it immediately follows that ∥∇Wζxi(W)∥2 ≤2√β. In the case of a squared loss function. We have the following. ∥∇Wpζxi(W)∥= ∥∇Wpf′(W)∥ = ∥∇WpgW(xi) −C(xi)∥ ≤∥∇WpgW(xi)∥+ 1. Likewise, consider the logistic-loss function. We then have the following. ∥∇Wpζxi(W)∥≤  C(xi)2 (1 + exp(C(xi)gW(xi)))2 exp(C(xi)gW(xi)) ∥∇WpgW(xi)∥ ≤∥∇WpgW(xi)∥. Since ∥∇WpgW(xi)∥≤ 1, we have that ∥∇Wpζxi(W)∥≤ 2 in both the cases. Thus, ζ0 = 2√β.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent D.3. Proofs of theorem 5.1 and corollary 5.1 In this section, we will present the proofs of theorem 5.1 and corollary 5.1. Theorem 5.1. Let W0,W1,..., Wβ satisfy assumption 1. For some ﬁxed constant c> 0, the gradient confusion bound (equation 3) holds with probability at least 1 −N2 exp ( −cdη2 16ζ4 0 (β+2)4 ) . Proof. We show two key properties, namely bounded gradient and non negative expectation. We will then use both these properties to complete the proof. Bounded gradient. For every i∈[n] deﬁne ζxi(W) :=f′(W). For every p∈[β] deﬁne Hp as follows. Hp(x) :=σ(Wp ·σ(Wp−1 ·σ(... ·σ(W0 ·x) ... ). Fix an i∈[N]. Then we have the following recurrence gβ(xi) :=σ′(Hβ(xi)) gp(xi) := (W⊤ p+1 ·gp+1(xi)) ·Diag(σ′(Hp(xi))) ∀p∈{0,1,...,β −1}. Then the gradients can be written in terms of the above quantities as follows. ∇Wpfi(W) =gp(xi) ·Hp−1(xi)⊤ ∀p∈[β]0. We can write, the gradient confusion denote by hW(xi,xj), as follows. ζxi(W)ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  . (7) We will now bound ∥∇(xi,xj)hW(xi,xj)∥2. Consider ∇xihW(xi,xj). This can be written as follows. (∇xiζxi(W))ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  + ζxi(W)ζxj(W) ∑ p∈[β]0 [ ∇xi ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) )]⊤ . (8) Observe that each of the entries in the diagonal matrix Diag(σ′(Hp(xi))) is at most 1. Thus, we have that ∥Diag(σ′(Hp(xi)))∥≤ 1. We have the following relationship. ∥gβ(xi)∥≤ 1 ∥gp(xi)∥≤∥ W⊤ p+1∥∥gp+1(xi))∥∥Diag(σ′(Hp(xi)))∥≤ 1 ∀p∈{0,1,...,β −1}. Moreover we have, ∥Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]∥≤∥ Hp−1(xi)∥∥gp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)⊤∥≤ 1. Consider ∥∇xi ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) ) ∥for every p∈[β]0. This can be upper-bounded by, ∥∇xiHp−1(xi)∥∥gp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥+ ∥Hp−1(xi)∥∥∇xigp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Note that ∇xiHp−1(xi) =g1(xi) ·Diag(σ′(W0 ·xi)) ·W⊤ 0 ·gp(xi)⊤. Thus, ∥∇xiHp−1(xi)∥≤ 1. We will now show that ∥∇xigp(xi)∥≤ β−p+ 1. We prove this inductively. Consider the base case when p= β. ∥∇xigβ(xi)∥= ∥∇xiσ′(Hβ(xi))∥≤ 1 =β−β+ 1. Now, the inductive step. ∥∇xigp(xi)∥≤∥∇ xigp+1(xi)∥+ ∥∇xi Diag(σ′(Hp(xi)))∥≤ β−p≤β−p+ 1. Thus, using equation 8 and the above arguments, we obtain, ∥∇xihW(xi,xj)∥2 ≤ζ2 0 (β+ 1) +ζ2 0 (β+ 1)(β+ 2)≤ 2ζ2 0 (β+ 2)2 and thus, ∥∇(xi,xj)hW(xi,xj)∥2 ≤4ζ2 0 (β+ 2)2. Non-negative expectation. Exi,xj[h(xi,xj)] =Exi,xj[⟨∇fi(W),∇fj(W)⟩] = ⟨Exi[∇fi(W)],Exj[∇fj(W)]⟩ = ∥Exi[∇fi(W)]∥2 ≥0. (9) We have used the fact that∇fi(W) and ∇fj(W) are identically distributed and independent. Concentration of Measure. We combine the two properties as follows. From Non-negative Expectation property and equation 26, we have that Pr[hW(xi,xj) ≤−η] ≤Pr[hW(xi,xj) ≤E(xi,xj)[hW(xi,xj)] −η] ≤exp ( −cdη2 16ζ4 0 (β+ 2)4 ) . (10) To obtain the probability that some value of hw(∇wfi,∇wfj) lies below −η,we use a union bound. There are N(N − 1)/2 <N 2/2 possible pairs of data points to consider, and so this probability is bounded above by N2 exp ( −cdη2 16ζ4 0 (β+2)4 ) . D.3.1. P ROOF OF COROLLARY 5.1 Before we prove corollary 5.1 we ﬁrst prove the following helper lemma. Lemma D.2. Suppose maxW ∥∇Wfi(W)∥≤ M,and both ∇Wfi(w) and ∇Wfj(W) are Lipschitz in W with constant L. Then hW(xi,xj) is Lipschitz in W with constant 2LM. Proof. We view W as ﬂattened vector. We now prove the above result for these two vectors. For two vectorsw,w′, |hw(xi,xj) −hw′(xi,xj)| = |⟨∇wfi(w),∇wfj(w)⟩−⟨∇w′fi(w′),∇w′fj(w′)⟩| = |⟨∇wfi(w) −∇w′fi(w′) +∇w′fi(w′),∇wfj(w)⟩ −⟨∇w′fi(w′),∇w′fj(w′) −∇wfj(w) +∇wfj(w)⟩| = |⟨∇wfi(w) −∇w′fi(w′),∇wfj(w)⟩−⟨∇w′fi(w′),∇w′fj(w′) −∇wfj(w)⟩| ≤|⟨∇wfi(w) −∇w′fi(w′),∇wfj(w)⟩|+ |⟨∇w′fi(w′),∇w′fj(w′) −∇wfj(w)⟩| ≤∥∇wfi(w) −∇w′fi(w′)∥∥∇wfj(w)∥+ ∥∇w′fi(w′)∥∥∇w′fj(w′) −∇wfj(w)∥ ≤L∥w −w′∥∥∇wfj(w)∥+ ∥∇w′fi(w′)∥L∥w′−w∥ ≤2LM∥w −w′∥. Here the ﬁrst inequality uses the triangle inequality, the second inequality uses the Cauchy-Schwartz inequality, and the third and fourth inequalities use the assumptions that ∇wfi(w) and ∇wfj(w) are Lipschitz in w and have bounded norm. We are now ready to prove the corollary, which we restate here. The proof uses a standard \"epsilon-net\" argument; we identify a ﬁne net of points within the ball Br.If the gradient confusion is small at every point in this discrete set, and the gradient confusion varies slowly enough with W,when we can guarantee small gradient confusion at every point in Br.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Corollary 5.1. Select a point W = (W0,W1,..., Wβ), satisfying assumption 1. Consider a ball Br centered at W of radius r> 0. If the data {xi}i∈[N] are sampled uniformly from a unit sphere, then the gradient confusion bound in equation 3 holds uniformly at all points W′∈Br with probability at least 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 ) , if r≤η/4ζ2 0 , 1 −N2 exp ( − cdη2 64ζ4 0 (β+2)4 + 8dζ2 0 r η ) , otherwise. Proof. Deﬁne the function h+(W) = maxijhW(xi,xj).Our goal is to ﬁnd conditions under which h+(W) >−ηfor all W in a large set. To derive such conditions, we will need a Lipschitz constant for h+(W),which is no larger than the maximal Lipschitz constant of hW(xi,xj) for all i,j. We have that ∥∇Wfi∥= ∥ζxi(W)xi∥≤ ζ0.Now we need to get a W-Lipschitz constants for ∇xifi = ζxi(W)xi.By lemma D.1, we have ∥∇W(ζxi(W)xi)∥= ∥(∇Wζxi(W))xi∥≤ ζ0. Using lemma D.2, we see that 2ζ2 0 is a Lipschitz constant for hW(xi,xj),and thus also h+(W). Now, consider a minimizer W of the objective, and a ball Br around this point of radius r. Deﬁne the constant ϵ= η 4ζ2 0 ,and create an ϵ-net of points Nϵ = {Wi}inside the ball. This net is sufﬁciently dense that any W′∈Br is at most ϵunits away from some Wi ∈Nϵ.Furthermore, because h+(W) is Lipschitz in W,|h+(W′) −h+(Wi)|≤ 2ζ2 0 ϵ= η/2. We now know the following: if we can guarantee that h+(Wi) ≥−η/2, for all Wi ∈Nϵ, (11) then we also know that h+(W′) ≥−ηfor all W′∈Br. For this reason, we prove the result by bounding the probability that (11) holds. It is known that Nϵ can be constructed so that |Nϵ|≤ (2r/ϵ+ 1)d = (8ζ2 0 r/η+ 1)d (see Vershynin (2018), corollary 4.1.13). Theorem 5.1 provides a bound on the probability that each individual point in the net satisﬁes condition (11). Using a union bound, we see that all points in the net satisfy this condition with probability at least 1 −N2 (8ζ2 0 r η + 1 )d exp ( −cd(η/2)2 16ζ4 0 ) (12) = 1−N2 exp(dlog(8ζ2 0 r/η+ 1)) exp ( −cdη2 64ζ4 0 ) (13) ≥1 −N2 exp(8dζ2 0 r/η) exp ( −cdη2 64ζ4 0 ) (14) = 1−N2 exp ( −cdη2 64ζ4 0 + 8dζ2 0 r η ) . (15) Finally, note that, if r<ϵ, then we can form a net with |Nϵ|= 1. In this case, the probability of satisfying (11) is at least 1 −N2 exp ( −cd(η/2)2 64ζ4 0 ) . D.4. Proof of theorem 4.1 Theorem 4.1. Let W0,W1,..., Wβ be weight matrices chosen according to strategy 4.1. There exists ﬁxed constants c1,c2 >0 such that we have the following. 1. Consider a ﬁxed but arbitrary dataset x1,x2,..., xN with ∥xi∥≤ 1 for every i ∈[N]. For η >4, the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −cℓ2β(η−4)2 64ζ4 0 (β+2)4 ) . 2. If the dataset {xi}i∈[N] is such that each xi is an i.i.d. sample from the surface of d-dimensional unit sphere, then for every η >0 the gradient confusion bound in equation 3 holds with probability at least 1 −βexp ( −c1κ2ℓ2) −N2 exp ( −c2(ℓd+ℓ2β)η2 16ζ4 0 (β+2)4 ) .The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Both parts in theorem 4.1 depend on the following argument. From theorem 2.3.8 and Proposition 2.3.10 in Tao (2012) with appropriate scaling7, we have for every p= 1,...,β we have that the matrix norm ∥Wp∥≤ 1 with probability at least 1 −βexp ( −c1κ2ℓ2) and ∥W0∥≤ 1 with probability at least 1 −exp ( −c1κ2d2) when the weight matrices are initialized according to strategy 4.1. Thus, conditioning on this event it implies that these matrices satisfy assumption 1. The proof strategy is similar to that of theorem 5.1. We will ﬁrst show that the gradient of the function h(.,.) as deﬁned in equation (7) with respect to the weights is bounded. Note that in part (1) the random variable is the set of weight matrices {Wp}p∈[β]. Thus, the dimension used to invoke theorem E.1 is at most ℓ2β. In part (2) along with the weights, the data x ∈Rd is also random. Thus, the dimension used to invoke theorem E.1 is at most ℓd+ ℓ2β. Combining this with theorem E.1, the bound on the gradient of h(.,.) and taking a union bound, we get the respective parts of the theorem. Thus, all it remains to prove is the bound on the gradient of the function h(.,.) as deﬁned in equation (7) with respect to the weights conditioning on the event that ∥Wp∥≤ 1 for every p∈{0,1,...,β }. We obtain the following analogue of equation (8). (∇Wζxi(W))ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  + (∇Wζxj(W))ζxi(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]  + ζxi(W)ζxj(W) ∑ p∈[β]0 [ ∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) )]⊤ . (16) As in the case of the proof for theorem 5.1, we will upper-bound the ℓ2-norm of the above expression. In particular, we show the following. (∇Wζxi(W))ζxj(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]    2 ≤2ζ2 0 (β+ 2)2. (17) (∇Wζxj(W))ζxi(W)   ∑ p∈[β]0 Tr[Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤]    2 ≤2ζ2 0 (β+ 2)2. (18) ζxi(W)ζxj(W) ∑ p∈[β]0 [ ∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) )]⊤ 2 ≤4ζ2 0 (β+ 2)2. (19) Equations (17) and 18 follow from the the fact that ∥(∇Wζxi(W))∥2 ≤ ζ0 and the arguments in the proof for theorem 5.1. We will now show the proof sketch for equation (19). For every p ∈ [β]0, consider ∥∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) ) ∥. Using the symmetry between xi and xj, the expression can be upper- bounded by, 2∥∇WHp−1(xi)∥∥gp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥+ 2∥Hp−1(xi)∥∥∇Wgp(xi)⊤∥∥gp(xj)∥∥Hp−1(xi)∥. As before we can use an inductive argument to ﬁnd the upper-bound and thus, we obtain the following which implies equation (19). ∥∇W ( Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi) ) ∥≤ 4(β+ 2)2. Next, we show that the expected value can be lower-bounded by−4 as in the case of theorem 4.1 above. Combining these two gives us the desired result. Consider EW[hW(xi,xj)]. We compute this expectation iteratively as follows. EW[hW(xi,xj)] = EW0 [EW1 [... EWβ[hW(xi,xj)] 7In particular, each entry has to be scaled by 1 ℓ for matrices {Wp}p∈[β] and 1 d for the matrix W0.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent ≥−4EW0  EW1  ... EWβ   ∑ p∈[β]0 Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤)      . The inequality combines equation 7 with Lemma D.1. We now prove the following inequality. EW0  EW1  ... EWβ   ∑ p∈[β]0 Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤)      ≤1. (20) Consider the inner-most expectation. Note that the only random variable is Wβ. Moreover, the term inside the trace is scalar. Note that the activation function σsatisﬁes |σ′(x)|≤ 1. Using the linearity of expectation, the LHS in equation (20) can be upper-bounded by the following. EW0 [ EW1 [ ... EWβ−1 [ Tr(Hβ−1(xi) ·Hβ−1(xi)⊤) ]]] (21) + EW0 [ EW1 [ ... EWβ [ ∑ p∈[β]0\\{β} Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤) ]]] . (22) The ﬁrst sum in the above expression can be upper-bounded by 1, since |σ(x)|≤ 1. We will now show that the second sum is 0. Consider the inner-most expectation. The weights Wβ appears only in the expression gp(xi)⊤·gp(xj). Moreover, note that every entry in Wβ is an i.i.d. normal random variable with mean 0. Thus, the second summand simpliﬁes to, EW0 [ EW1 [ ... EWβ−1 [ ∑ p∈[β]0\\{β,β−1} Tr(Hp−1(xi) ·gp(xi)⊤·gp(xj) ·Hp−1(xi)⊤) ]]] . Applying the above argument repeatedly we obtain that the second summand (equation (22)) is 0. Thus, we obtain the inequality in equation (20) which implies that EW[hW(xi,xj)] ≥−4. D.5. Proof of Theorem 6.1 In this section, we prove Theorem 6.1. The proof follows similar to those in previous sub-sections; we prove a bound on the gradient of the gradient inner-product and show that the expectation is non-negative. Combining these two with an argument similar to equation 10 we get the theorem. Note that the dataset is obtained by considering i.i.d. samples from a d-dimensional unit sphere. Thus, the lower-bound on the expectation (i.e., non-negative expectation of the gradient inner-product) follows from equation 9. Thus, it remains to prove an upper-bound on the norm of the gradient of the gradient inner-product term. Throughout this proof, we will use g(x) as a short-hand to denote gW(x). Consider the gradient ∇Wg(x). The the ith component of this can be written as follows. [∇Wg(x)]i = γ2ζx(W) ( WT β ·... WT i+1 ·xT ·WT 1 ·... WT i−1 ) . (23) Now consider, the gradient inner-product hW(xi,xj). We want to upper-bound the quantity ∥∇(xi,xj)hW(xi,xj)∥. From symmetry, this can be upper-bounded by 2∥∇xihW(xi,xj)∥. Consider the kth coordinate of ∇xihW(xi,xj). Using equation 23, the assumption that {Wi}i∈[β] are orthogonal matrices and taking the gradient, this can be written as, [∇xihW(xi,xj)]k = γ2ζxi(W)xj + α2 ( WT β ·... WT i+1 ·xT ·WT 1 ·... WT i−1 ) (∇xiζxi(W)) . (24) Combining assumption 1 with equation 24 we have that ∥∇xihW(xi,xj)∥is at most 2γ2β∥xj∥≤ 2γ2β. For the deﬁnition of the scaling factor γ = 1√2β, we have that 2γ2β = 1. Thus, ∥∇(xi,xj)hW(xi,xj)∥≤ 2. E. Technical lemmas We will brieﬂy describe some technical lemmas we require in our analysis. The following Chernoff-style concentration bound is proved in Chapter 5 of Vershynin (2018).The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent Lemma E.1 (Concentration of Lipshitz function over a sphere). Let x ∈Rd be sampled uniformly from the surface of a d-dimensional sphere. Consider a Lipshitz function ℓ : Rd →R which is differentiable everywhere. Let ∥∇ℓ∥2 denote supx∈Rd ∥∇ℓ(x)∥2. Then for any t≥0 and some ﬁxed constant c≥0, we have the following. Pr [⏐⏐⏐ℓ(x) −E[ℓ(x)] ⏐⏐⏐≥t ] ≤2 exp ( −cdt2 ρ2 ) , (25) where ρ≥∥∇ℓ∥2. We will rely on the following generalization of lemma E.1. We would like to point out that the underlying metric is the Euclidean metric and thus we use the ∥.∥2-norm. Corollary E.1. Let x,y ∈Rd be two mutually independent vectors sampled uniformly from the surface of a d-dimensional sphere. Consider a Lipshitz function ℓ : Rd ×Rd → R which is differentiable everywhere. Let ∥∇ℓ∥2 denote sup(x,y)∈Rd×Rd ∥∇ℓ(x,y)∥2. Then for any t≥0 and some ﬁxed constant c≥0, we have the following. Pr [⏐⏐⏐ℓ(x,y) −E[ℓ(x,y)] ⏐⏐⏐≥t ] ≤2 exp ( −cdt2 ρ2 ) , (26) where ρ≥∥∇ℓ∥2. Proof. This corollary can be derived from lemma E.1 as follows. Note that for every ﬁxed ˜y ∈Rd, equation 25 holds. Additionally, we have that the vectors x and y are mutually independent. Hence we can write the LHS of equation 26 as the following. ∫ (˜y)1=∞ (˜y)1=−∞ ... ∫ (˜y)d=∞ (˜y)d=−∞ Pr [⏐⏐⏐ℓ(x,y) −E[ℓ(x,y)] ⏐⏐⏐≥t ⏐⏐⏐⏐⏐y = ˜y ⏐⏐⏐⏐⏐ ] φ(˜y)d(˜y)1 ...d (˜y)d. Here φ(˜y) refers to the pdf of the distribution of y. From independence, the inner term in the integral evaluates to Pr [⏐⏐⏐ℓ(x,˜y) −E[ℓ(x,˜y)] ⏐⏐⏐≥t ] . We know this is less than or equal to 2 exp ( −cdt2 ∥∇ℓ∥2 2 ) . Therefore, the integral can be upper bounded by the following. ∫ (˜y)1=∞ (˜y)1=−∞ ... ∫ (˜y)d=∞ (˜y)d=−∞ 2 exp ( − cdt2 ∥∇ℓ∥2 2 ) φ(˜y)d(˜y)1 ...d (˜y)d. Since φ(˜y) is a valid pdf, we get the required equation 26. Additionally, we will use the following facts about a normalized Gaussian random variable. Lemma E.2. For a normalized Gaussian x (i.e., an x sampled uniformly from the surface of a unit d-dimensional sphere) the following statements are true. 1. ∀p∈[d] we have that E[(x)p] = 0. 2. ∀p∈[d] we have that E[(x)2 p] = 1/d. Proof. Part (1) can be proved by observing that the normalized Gaussian random variable is spherically symmetric about the origin. In other words, for every p∈[d] the vectors (x1,x2,...,x p,...,x d) and (x1,x2,..., −xp,...,x d) are identically distributed. Hence E[xp] =E[−xp] which implies that E[xp] = 0. Part (2) can be proved by observing that for any p,p′∈[d], xp and xp′ are identically distributed. Fix any p∈[d]. We have that ∑ p′∈[d] E[x2 p′] =d×E[x2 p]. Note that we have ∑ p′∈[d] E[x2 p′] = ∫ (x)1=∞ (x)1=−∞ ... ∫ (x)d=∞ (x)d=−∞ ∑ p′∈[d] x2 p′ ∑ p′′∈[d] x2 p′′ φ(x)d(x)1 ...d (x)d = 1. Therefore E[x2 p] = 1/d.The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent We use the following well-known Gaussian concentration inequality in our proofs ( e.g., Chapter 5 in Boucheron et al. (2013)). Lemma E.3 (Gaussian Concentration). Let x = (x1,x2,...,x d) be i.i.d. N(0,ν2) random variables. Consider a Lipshitz function ℓ: Rd →R which is differentiable everywhere. Let ∥∇ℓ∥2 denote supx∈Rd ∥∇ℓ(x)∥2. Then for any t≥0, we have the following. Pr [⏐⏐⏐ℓ(x) −E[ℓ(x)] ⏐⏐⏐≥t ] ≤2 exp ( − t2 2ν2ρ2 ) , (27) where ρ≥∥∇ℓ∥2. F. Additional discussion of the small weights assumption (assumption 1) Without the small-weights assumption, the signal propagated forward or the gradients ∇Wfi could potentially blow up in magnitude, making the network untrainable. Proving non-vacuous bounds in case of such blow-ups in magnitude of the signal or the gradient is not possible in general, and thus, we assume this restricted class of weights. Note that the small-weights assumption is not just a theoretical concern, but also usually holds in practice. Neural networks are often trained with weight decay regularizers of the form ∑ i∥Wi∥2 F, which keep the weights small during optimization. The operator norm of convolutional layers have also recently been used as an effective regularizer for image classiﬁcation tasks by Sedghi et al. (2018). In the proof of theorem 4.1 we showed that assumption 1 holds with high probability at standard Gaussian initializations used in practice. While, in general, there is no reason to believe that such a small-weights assumption would continue to hold during optimization without explicit regularizers like weight decay, some recent work has shown evidence that the weights do not move too far away during training from the random initialization point for overparameterized neural networks (Neyshabur et al., 2018; Dziugaite & Roy, 2017; Nagarajan & Kolter, 2019; Zou et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018; Oymak & Soltanolkotabi, 2018). It is worth noting though that all these results have been shown under some restrictive assumptions, such as the width requiring to be much larger than generally used by practitioners.",
      "meta_data": {
        "arxiv_id": "1904.06963v5",
        "authors": [
          "Karthik A. Sankararaman",
          "Soham De",
          "Zheng Xu",
          "W. Ronny Huang",
          "Tom Goldstein"
        ],
        "published_date": "2019-04-15T11:02:22Z",
        "pdf_url": "https://arxiv.org/pdf/1904.06963v5.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates how neural network architecture influences training speed by introducing 'gradient confusion,' a condition where stochastic gradients from different data samples may be negatively correlated, hindering convergence. It formally proves that high gradient confusion slows down convergence, while low confusion accelerates it. The study demonstrates theoretically and experimentally that increasing network width leads to lower gradient confusion and faster training, whereas increasing depth has the opposite effect under standard Gaussian initializations. It also shows that orthogonal initialization schemes make gradient confusion independent of depth for linear networks, and that the combination of batch normalization and skip connections in residual networks significantly reduces gradient confusion, enabling efficient training of very deep models.",
        "methodology": "The core methodology involves defining 'gradient confusion' as the pair-wise inner products between gradients of different objective functions, where negative correlation indicates high confusion. Theoretical analysis is conducted on SGD convergence rates for functions satisfying Polyak-Lojasiewicz (PL) inequality and general smooth functions, showing the direct effect of gradient confusion on convergence speed and noise floor. High-dimensional probability methods are used to analyze the effect of network width and depth on gradient confusion at standard Gaussian initializations (N(0, 1/d) or N(0, 1/κℓp−1)). A more general result on depth effect with a 'small weights assumption' is derived. The paper also theoretically explores orthogonal initialization for linear networks to decouple gradient confusion from depth. Empirically, it involves calculating pairwise gradient cosine similarities over mini-batches.",
        "experimental_setup": "Experiments were conducted on Wide Residual Networks (WRNs), Convolutional Neural Networks (CNNs), and Multi-Layer Perceptrons (MLPs) for image classification tasks. Datasets included CIFAR-10, CIFAR-100, and MNIST. SGD without momentum was used as the optimizer, with mini-batch size 128, trained for 200 (CNNs/WRNs) or 40000 iterations/100 epochs (MLPs). Learning rates were constant for most training and reduced by a factor of 10 at specific epochs (80 and 160 for CNNs/WRNs). The MSRA initializer (He et al., 2015) was used for CNNs/WRNs, and Glorot/LeCun normal initializers for MLPs. Dropout and weight decay were turned off. Gradient confusion was measured by sampling 100-1000 pairs of mini-batches and computing pairwise cosine similarities. Performance was evaluated using training loss, minimum pairwise gradient cosine similarity, and test set accuracy.",
        "limitations": "The theoretical convergence results for SGD assume conditions like Lipschitz smoothness and the Polyak-Lojasiewicz inequality, or general smooth functions. The analysis of gradient confusion for general networks (Theorem 5.1 and Corollary 5.1) relies on a 'small weights assumption' (operator norm of weight matrices bounded by 1), which is argued to hold in practice due to weight decay but might not be universally true without explicit regularizers. Proving non-vacuous bounds for signal or gradient blow-ups without this assumption is not generally possible. While orthogonal initializations show promise for deep linear networks, they are not compatible with common non-linear activation functions like sigmoid or ReLU, limiting their practical use. The convergence rates presented are not state-of-the-art for these problem classes but aim to directly show the effect of gradient confusion.",
        "future_research_directions": "Future research could investigate the connection between gradient confusion and generalization properties of neural networks, as suggested by the correlation between gradient confusion and test set accuracies. Developing orthogonal initialization schemes that are compatible with non-linear activation functions (like sigmoids or ReLUs) is identified as a promising direction for improving the trainability of very deep networks. The gradient confusion condition itself could be explored as a measure of trainability, potentially leading to new algorithms for more efficient training."
      }
    },
    {
      "title": "Gradient-Based Feature Learning under Structured Data",
      "abstract": "Recent works have demonstrated that the sample complexity of gradient-based\nlearning of single index models, i.e. functions that depend on a 1-dimensional\nprojection of the input data, is governed by their information exponent.\nHowever, these results are only concerned with isotropic data, while in\npractice the input often contains additional structure which can implicitly\nguide the algorithm. In this work, we investigate the effect of a spiked\ncovariance structure and reveal several interesting phenomena. First, we show\nthat in the anisotropic setting, the commonly used spherical gradient dynamics\nmay fail to recover the true direction, even when the spike is perfectly\naligned with the target direction. Next, we show that appropriate weight\nnormalization that is reminiscent of batch normalization can alleviate this\nissue. Further, by exploiting the alignment between the (spiked) input\ncovariance and the target, we obtain improved sample complexity compared to the\nisotropic case. In particular, under the spiked model with a suitably large\nspike, the sample complexity of gradient-based training can be made independent\nof the information exponent while also outperforming lower bounds for\nrotationally invariant kernel methods.",
      "full_text": "Gradient-Based Feature Learning under Structured Data Alireza Mousavi-Hosseini∗ Denny Wu† Taiji Suzuki‡ Murat A. Erdogdu∗ September 8, 2023 Abstract Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In particular, under the spiked model with a suitably large spike, the sample complexity of gradient-based training can be made independent of the information exponent while also outperforming lower bounds for rotationally invariant kernel methods. 1 Introduction A fundamental feature of neural networks is their adaptivity to learn unknown statistical models. For instance, when the learning problem exhibits certain low-dimensional structure or sparsity, it is expected that neural networks optimized by gradient-based algorithms can efficiently adapt to such structure via feature/representation learning. A considerable amount of research has been dedicated to understanding this phenomenon under various assumptions and to demonstrate the superiority of neural networks over non-adaptive methods such as kernel models [ GMMM19, WLLM19, BES+19, LMZ20, AAM22, BES+22, DLS22, Tel23, MHPG+23]. A particular relevant problem setting for feature learning is the estimation of single index models, where the response y ∈ R depends on the input x ∈ Rd via y = g(⟨u, x⟩) + ϵ, where g : R → R is the nonlinear link function and u is the unit target direction. Here, learning corresponds to recovering the unknowns u and g, which requires the model to extract and adapt to the low-dimensional target direction. Recent works have shown that the sample complexity is determined by certain properties of the link function g. In particular, the complexity of gradient-based optimization is captured by the information exponent of g introduced by [BAGJ21]. Intuitively, a larger information exponent s corresponds to a more complex g (for gradient-based learning), and it has been proven that when the input is isotropic x ∼ N(0, Id), gradient flow can learn the single index model with ˜O(ds) sample complexity [BBSS22]. In practice, however, real data always exhibits certain structures such as low intrinsic dimensionality, and isotropic data assumptions fail to capture this fact. In statistics methodology, it is known that the directions along which the input x has high variance are often good predictors of the target y [HTFF09]; indeed, this is the main reason principal component analysis is used in pre-training [ JWHT13]. A fundamental model that captures such a structure is the spiked matrix model in which x ∼ N(0, Id + κθθ⊤) for some unit direction θ ∈ Rd and κ >0 [Joh01]. Along the direction θ, data has higher variability and predictive power. In single index models, such predictive power translates to a non-trivial alignment between the vectors u and θ — our focus is to investigate the effect of such alignment on the sample complexity of gradient-based training. ∗University of Toronto and Vector Institute for Artificial Intelligence. {mousavi,erdogdu}@cs.toronto.edu. †New York University and Flatiron Institute. dennywu@nyu.edu. ‡University of Tokyo and RIKEN Center for Advanced Intelligence Project. taiji@mist.i.u-tokyo.ac.jp. 1 arXiv:2309.03843v1  [stat.ML]  7 Sep 20231.1 Contributions: learning single index models under spiked covariance <latexit sha1_base64=\"pm3g0z5LaNpvQMiezRYzcYp7sMI=\">AAACq3icdVHbbhMxEHWWWwm3Fh55sYiQeCHaUEqKhFBbFKmPSyFtRRxVtnc2seK1V/YsarTKD/De1/Jd/A3e3VBRLiNZOjpzxmd0RhRaeYzjH53oxs1bt+9s3O3eu//g4aPNrcfH3pZOwlhabd2p4B60MjBGhRpOCwc8FxpOxOJD3T/5Cs4raz7jsoBpzmdGZUpyDNSEIZxjdchdujrb7MX9t8Od3eE2/RsM+nFTPbKu5Gyr842lVpY5GJSaez8ZxAVOK+5QSQ2rLis9FFwu+AwmARqeg59Wzc4r+jwwKc2sC88gbdjfJyqee7/MRVDmHOf+z15N/qs3KTHbnVbKFCWCka1RVmqKltYB0FQ5kKiXAXDpVNiVyjl3XGKIqdtlKWRsVLH6YyGq0Sr8QEfnRZhpIqP1Ula3uuRKlzS6xFnBhdIKl9d07yqmIUOmuZmFZIJyvwZUBNsFoG9V7yvm1GyOzLWylg3X1P7K56jxOQocNWUuwK19wvV+nYj+Hxy/6g/e9Hc+vu7tHazvuEGekmfkBRmQIdkjhyQhYyKJJRfkknyPXkafoi8Ra6VRZz3zhFyrCH4CbhLUng==</latexit> Hard <latexit sha1_base64=\"uSGAAOacUUfWA0XX52jhK1p+ncg=\">AAACrnicdVHbjtMwEHXDbSm3XXjkxaJC4qlKgaWLhNACqsRjWNHuSnW2sp1Ja9WxI3uCqKL+AF/AK/wVf4OTlBXLZSRLR2fOzBnPiFIrj3H8oxdduXrt+o29m/1bt+/cvbd/cH/mbeUkTKXV1p0J7kErA1NUqOGsdMALoeFUrN81+dNP4Lyy5iNuSkgLvjQqV5JjoM4NK50t0dLsvPbbxf4gHr4cHx6Nn9G/wWgYtzEgu0gWB70vLLOyKsCg1Nz7+SguMa25QyU1bPus8lByueZLmAdoeAE+rduxt/RxYDKaWxeeQdqyv1fUvPB+U4igLDiu/J+5hvxXbl5hfpTWypQVgpGdUV5pGv7Z7IBmyoFEvQmAS6fCrFSuuOMSw6b6fZZBziY1axoLUU+2oQOdfC5DTbs12gxldadLLnRJq0ucFVworXBzSfeqZhpyZJqbZdhMUL5pABXBdg3oO9Xrmjm1XCFznaxjw0G1v/A5aX1OAkdNVQhwO59wvV8nov8Hs6fD0Yvh4Yfng+O3uzvukYfkEXlCRmRMjsl7kpApkcSRr+Qb+R7F0SxKo0UnjXq7mgfkUkSrn3jz1d8=</latexit> n / d s <latexit sha1_base64=\"jfyNx8buYaYzJyZHx+s/XDObDBw=\">AAACwHicbZFNbxMxEIad5auErxSOSMgiRSqXaLeCwgGhUhSJ41KRtlI2imzvbGLFa2/tWdSwyo0TP4Ur/Bn+Dd4PVbRlJEuv3nnGY8/wQkmHYfinF9y4eev2na27/Xv3Hzx8NNh+fOxMaQVMhFHGnnLmQEkNE5So4LSwwHKu4ISvPtb5k69gnTT6C64LmOVsoWUmBUNvzQfPEoRzrHbsfG+H7rpCroA2CJYpvNzMB8NwFDZBr4uoE0PSRTzf7v1IUiPKHDQKxZybRmGBs4pZlELBpp+UDgomVmwBUy81y8HNquYjG/rCOynNjPVHI23cfysqlju3zrknc4ZLdzVXm//LTUvM3s4qqYsSQYu2UVYqiobWU6GptCBQrb1gwkr/ViqWzDKBfnb9fpJCloyrpL6Y82q88TfQ8Xnha5o50vpRRrVcfMHFDRdbwxmXSuL6EveuShRkmCimF34ynvxQC8p92xWga6n3VWLlYomJbbHW9StW7qLPUdPnyHtUlzkH2/Xx24uu7uq6ON4bRfuj159fDQ8Ouz1ukafkOdklEXlDDsgnEpMJEeQ7+Ul+kd/BYbAMTHDWokGvq3lCLkXw7S87e9tP</latexit> r 2 (spike magnitude) <latexit sha1_base64=\"QEa2FcEqYgcSB42jNB9ns7urGdU=\">AAACtXicdVHbattAEF2rt9S9Oc1jX5aaQp+M3CZ1CqGkLYb2TQ11ErCE2V2N7MWrXbE7ChHCP9Cf6Gv7Sf2briQ3NL0MLBzOnNkznOGFkg7D8EcvuHHz1u07O3f79+4/ePhosPv41JnSCpgJo4w958yBkhpmKFHBeWGB5VzBGV+/b/pnF2CdNPozVgUkOVtqmUnB0FOLwV6McIn1R41gc0glQ9gsBsNw9HpycDh5Sf8G41HY1pBsK1rs9r7EqRFlDhqFYs7Nx2GBSc0sSqFg049LBwUTa7aEuYea5eCSut1+Q595JqWZsf5ppC37+0TNcueqnHtlznDl/uw15L968xKzw6SWuigRtOiMslJRNLSJgqbSgkBVecCElX5XKlbMMuGzcP1+nEIWT+u4+ZjzerrxP9DpZeFn2vBos5RRnS660kWtLrKGMy6VxOqa7qiOFWQYK6aXPhmvfNsAyr3tGtB1qjd1bOVyhbHtZB3r76rclc9J63PiOarLnIPd+vjr/ToR/T84fTEavxodfNofHr/b3nGHPCFPyXMyJhNyTD6QiMyIIBX5Sr6R78EkSII0yDpp0NvO7JFrFZif4F7Yew==</latexit> Intermediate <latexit sha1_base64=\"ya8KaCvfl0SqS+knYGE8FrxUxDY=\">AAACx3icbVFLbxMxEHaWVwmvFI5cLNJK5UC0i3gdECqgSHBbKtJWykaR7cxurHjtlT1bJVrlwBV+DVf4JfwbvA9VtGUkS5+++Wa+8QwvlHQYhn96wbXrN27e2rndv3P33v0Hg92Hx86UVsBEGGXsKWcOlNQwQYkKTgsLLOcKTvjqY50/OQPrpNFfcVPALGeZlqkUDD01H+wnCGus9uw82qMHrpAreIbMZoCUKZnpHDQ+3c4Hw3AUNkGvgqgDQ9JFPN/tfU8WRpR1uVDMuWkUFjirmEUpFGz7SemgYGLFMph6qFkOblY1/9nSfc8saGqsfxppw/5bUbHcuU3OvTJnuHSXczX5v9y0xPTNrJK6KBG0aI3SUlE0tF4OXUgLAtXGAyas9LNSsWSWCfQr7PeTBaTJuErqxpxX463vQMfrwtc066T1UEa1uvhcFze62BrOuFQSNxd0b6tEQYqJYjrzm/HK9zWg3NuuAF2relclVmZLTGwra1l/aeXOfY4anyPPUV3mHGzn468XXb7VVXD8fBS9Gr388mJ4+KG74w55TJ6QAxKR1+SQfCIxmRBBfpCf5Bf5HXwOTHAWrFtp0OtqHpELEXz7C6Yu3kk=</latexit> r 1 (spike-target alignment) <latexit sha1_base64=\"abTloeRNywfcdyC2tVbf49jhhJo=\">AAACq3icdVHbbhMxEHW2XEq49fLIi0WExAvRBigpUlUVUCQel0LaijiqbGc2seK1V/Zs1dUqP8A7r/Bd/A3e3VBRLiNZOjpzxmd0RuRaeYzjH51o48bNW7c373Tv3rv/4OHW9s6Jt4WTMJZWW3cmuAetDIxRoYaz3AHPhIZTsXxX908vwHllzScsc5hmfG5UqiTHQE0YwiVWI+7L1flWL+6/Hu7tD1/Qv8GgHzfVI+tKzrc7X9jMyiIDg1Jz7yeDOMdpxR0qqWHVZYWHnMsln8MkQMMz8NOq2XlFnwRmRlPrwjNIG/b3iYpn3peZCMqM48L/2avJf/UmBab700qZvEAwsjVKC03R0joAOlMOJOoyAC6dCrtSueCOSwwxdbtsBikbVaz+WIhqtAo/0NFlHmaayGi9lNWtLrnSJY0ucVZwobTC8pruoGIaUmSam3lIJijf1ICKYLsE9K3qsGJOzRfIXCtr2XBN7a98jhuf48BRU2QC3NonXO/Xiej/wcnz/uBVf+/Dy97R2/UdN8kj8pg8JQMyJEfkPUnImEhiyVfyjXyPnkUfo88Ra6VRZz2zS65VBD8BmKTUsQ==</latexit> Easy <latexit sha1_base64=\"pFgFGq0kMOGgScQV9rSQd8zFv+o=\">AAACunicdVHbbhMxEHWWWwmXpvCCxItFhMRTtAHaFKlCLSgSj0tF2kpxFNnObGLFa6/sWZRoFT6A7+C1/A9/g3c3VJTLSNYenTnj4z0jcq08xvGPVnTj5q3bd3butu/df/Bwt7P36MzbwkkYSautuxDcg1YGRqhQw0XugGdCw7lYvq/655/BeWXNJ1znMMn43KhUSY6BmnaeMIQVlp5nuQYqbfVZKVxvpp1u3Hsz2D8cvKJ/g34vrqtLtpVM91pf2czKIgODUnPvx/04x0nJHSqpYdNmhYecyyWfwzhAwzPwk7L+hQ19HpgZTa0LxyCt2d8nSp55v85EUGYcF/7PXkX+qzcuMD2clMrkBYKRjVFaaIqWVnnQmXIgUa8D4NKp8FYqF9xxiSG1dpvNIGXDklUXC1EON+EGOlzlYaZOkFaPsrrRJVe6pNYlzgoulA55XtMdlUxDikxzMw/JBOVJBagItktA36jelsyp+QKZa2QNG5ar/ZXPae1zGjhqikyA2/qE7f1aEf0/OHvZ6x/09j++7h6/2+5xhzwlz8gL0icDckw+kISMiCRfyDdySb5HR5GIVLRspFFrO/OYXKsIfwJyFdrC</latexit> sample complexity <latexit sha1_base64=\"/Ot55LaCnRl8T+uW6RmzS6P2iaA=\">AAACq3icdVHbbhMxEHWWWwm3Fh55sYiQeCHaUEqKhFABRerjUkhbEUeV7cwmVrz2yp5Fjaz8AO+8wnfxN3h3Q0W5jGTp6MwZn9EcUWrlMU1/dJIrV69dv7F1s3vr9p2797Z37h97WzkJY2m1daeCe9DKwBgVajgtHfBCaDgRy3d1/+QzOK+s+YirEqYFnxuVK8kxUhOGcI7hUM0X67PtXtp/OdzbH+7Sv8GgnzbVI5vKznY6X9jMyqoAg1Jz7yeDtMRp4A6V1LDusspDyeWSz2ESoeEF+Glodl7Tx5GZ0dy6+AzShv19IvDC+1UhorLguPB/9mryX71Jhfn+NChTVghGtkZ5pSlaWh+AzpQDiXoVAZdOxV2pXHDHJcYzdbtsBjkbBVZ/LEQYreMPdHRexpnmZLReyupWl13oskaXOSu4UFrh6pLuVWAacmSam3m8TFS+qQEV0XYJ6FvV68BcTAKZa2UtG9PU/sLnqPE5ihw1VSHAbXxier8iov8Hx8/6gxf9vffPewdvNzlukYfkEXlCBmRIDsghyciYSGLJV/KNfE+eJh+STwlrpUlnM/OAXKoEfgJwVdSf</latexit> High <latexit sha1_base64=\"sGJO3wn9npoNn2RPkkujRYj/U2c=\">AAACqnicdVFNbxMxEHWWrxK+WjhysYiQOKBog0pLpKoqoEgcOCwRaQtxVNne2cSK117Zs9BolT/AmSv8L/4N3t1QUT5GsvT05s288YwotPIYxz860ZWr167f2LrZvXX7zt172zv3j70tnYSJtNq6U8E9aGVgggo1nBYOeC40nIjl6zp/8gmcV9a8x1UBs5zPjcqU5BiojwzhHKu39vP6bLsX94fDeLi/R/8Gg37cRI9sIjnb6XxhqZVlDgal5t5PB3GBs4o7VFLDustKDwWXSz6HaYCG5+BnVTPymj4OTEoz68IzSBv294qK596vchGUOceF/zNXk//KTUvMXswqZYoSwcjWKCs1RUvr/9NUOZCoVwFw6VSYlcoFd1xi2FK3y1LI2KhidWMhqtE6dKCj8yLUNBuj9VBWt7rkQpc0usRZwYXSCleXdAcV05Ah09zMw2aC8mUNqAi2S0Dfqg4r5tR8gcy1spYNx9T+wmfc+IwDR02ZC3Abn3C9Xyei/wfHz/qDvf7zd7u9o1ebO26Rh+QReUIGZJ8ckTckIRMiiSFfyTfyPXoajaMP0bSVRp1NzQNyKaL0J5551Eo=</latexit> Low <latexit sha1_base64=\"GvKaGa8m5OKWDIpAXcByWQs0OvY=\">AAACv3icdVFdaxQxFM2OX3X96FYfRQguwhbpMlnbbQWRVlnwcSxuW9isS5LJzIbNZEKSKQ7DPvnmP/FVf43/xszMWqzohcDh3HPvubmXaimsC8OfneDGzVu372zd7d67/+Dhdm/n0ZnNC8P4lOUyNxeUWC6F4lMnnOQX2nCSUcnP6epdnT+/5MaKXH10pebzjKRKJIIR56lF76nC2uTa5TD+VCH4Ag7sHtodoD2zGO2uF71+OHz1cjxCB7AGYThGNdgfocMxRMOwiT7YRLTY6XzFcc6KjCvHJLF2hkLt5hUxTjDJ111cWK4JW5GUzzxUJON2XjX/WMPnnolhkhv/lIMN+2dFRTJry4x6ZUbc0v6dq8l/5WaFS47mlVC6cFyx1igpJPTfrpcCY2E4c7L0gDAj/KyQLYkhzPnVdbs45gmeVLhuTGk1WfsOcPJZ+5pmjbAeKpetLrrSRY0uMjklVEjhymu61xWWPHFYEpX6zXjlSQ0g9bYr7myrelNhI9Klw6aVtay/sLRXPqeNz6nnoCoyys3Gx1/v94ng/8HZaIjGw4MP+/3jt5s7boEn4BkYAAQOwTF4DyIwBQx8Ad/Ad/AjOAnSQAW6lQadTc1jcC2C8hf1Ldl4</latexit> n / d 1+( s \u0000 1)(1 \u0000 r 2 ) <latexit sha1_base64=\"FKgR94YHl3xPTG0HxZope/ea0rk=\">AAACxnicdVFdixMxFE3Hr7V+bFcffQlWoUVaJuNuV0FkVQr7OC52d6GpJclk2tBMMiQZtQwFn/fX+Kr/xH9jZqYuruiFwOHcc++5uZfmUlgXhj9bwbXrN27e2rndvnP33v3dzt6DU6sLw/iEaanNOSWWS6H4xAkn+XluOMmo5Gd09a7Kn33ixgqtPrh1zmcZWSiRCkacp+adJwrnRudOw+RjieAz2LMD1O+hQdQz82hg5qjf38w73XD48vkoQgewAmE4QhXYj9DhCKJhWEcXbCOe77UucKJZkXHlmCTWTlGYu1lJjBNM8k0bF5bnhK3Igk89VCTjdlbW39nAp55JYKqNf8rBmv2zoiSZteuMemVG3NL+navIf+WmhUtfzEqh8sJxxRqjtJDQ/77aDUyE4czJtQeEGeFnhWxJDGHOb7DdxglP8bjEVWNKy/HGd4DjL7mvqbcJq6G0bHTxpS6udbHRlFAhhVtf0b0qseSpw5Kohd+MV76pAKTedsWdbVSvS2zEYumwaWQN6w8t7aXPSe1z4jmoioxys/Xx1/t9Ivh/cBoN0Wh48H6/e/R2e8cd8Ag8Bj2AwCE4AscgBhPAwAX4Br6DH8FxoIIi+NxIg9a25iG4EsHXXwqe23A=</latexit> n / d 1+( s \u0000 1)(1 \u0000 2( r 2 \u0000 r 1 )) <latexit sha1_base64=\"5bqFYjz2DXH+6wmpfJIH5pk/gX0=\">AAACqXicdVFdi9NAFJ3Gr7V+7K4++jJYBEEISUjdFVRWpeBjdrG71aaUmelNO3QyCTM3Ygn9A776qj/Mf+MkqYsremHgcO6599y5l5dKWgyCnz3v2vUbN2/t3e7fuXvv/v7B4YNzW1RGwFgUqjATziwoqWGMEhVMSgMs5wou+Ppdk7/4DMbKQn/ATQmznC21zKRg6KiPZh7SV9TMo/nBIPBfHMdBENHAj6OjeBg3II6DaEhDP2hjQHaRzA97X9NFIaocNArFrJ2GQYmzmhmUQsG2n1YWSibWbAlTBzXLwc7qduItfeKYBc0K455G2rJ/VtQst3aTc6fMGa7s37mG/FduWmF2PKulLisELTqjrFIUC9p8ny6kAYFq4wATRrpZqVgxwwS6JfX76QKydFSnTWPO69HWdaCjL6WraRdGm6EK1emSS13S6hJTcMalkri5ontZpwoyTBXTS7cZp3zTAMqd7RrQdqrXdWrkcoWp6WQd626p7KXPWetz5jiqq5yD2fm46/0+Ef0/OI/88Lk/PI0HJ293d9wjj8hj8pSE5IickPckIWMiSE6+ke/kh/fMO/Um3qdO6vV2NQ/JlfDELwC00kM=</latexit> r 1 = r 2 <latexit sha1_base64=\"0eh5UZ+YK09LwZ9fJ0k0Tye9JQo=\">AAACqnicdVFNj9MwEHXD11K+duHIxaJC4oCiNErZRdpFC6gSx1DR3YWmqmxn0lp1nMieIKqof4AzV/hf/BucpKxYBCNZenrzZt54hpdKWgyCnz3v2vUbN2/t3e7fuXvv/oP9g4dntqiMgKkoVGEuOLOgpIYpSlRwURpgOVdwztdvm/z5ZzBWFvoDbkqY52ypZSYFQ0d9Cs1iSE+oWYSL/UHgvzyKgiCkgR+Fh9EoakAUBeGIDv2gjQHZRbw46H1N0kJUOWgUilk7GwYlzmtmUAoF235SWSiZWLMlzBzULAc7r9uRt/SpY1KaFcY9jbRl/6yoWW7tJudOmTNc2b9zDfmv3KzC7GheS11WCFp0RlmlKBa0+T9NpQGBauMAE0a6WalYMcMEui31+0kKWTKuk6Yx5/V46zrQ8ZfS1bQbo81Qhep08aUubnWxKTjjUkncXNEd14mCDBPF9NJtxilfN4ByZ7sGtJ3qVZ0YuVxhYjpZx7pjKnvpM2l9Jo6juso5mJ2Pu97vE9H/g7PQH77wR++jwemb3R33yGPyhDwjQ3JITsk7EpMpEUSTb+Q7+eE99ybeR2/WSb3eruYRuRJe+guYDdJ/</latexit> 2 r 1 = r 2 Figure 1: Sample complexity to learn u and g under the spiked model. Smaller r1 denotes a better spike-target alignment, while larger r2 denotes a larger spike magnitude. The sample complexities are based on Corollary 8. In this paper, we study the sample complexity of learning a single index model using a two-layer neural network and show that it is determined by an interplay between • spike-target alignment: ⟨u, θ⟩ ≍d−r1 , r1 ∈ [0, 1/2], • spike magnitude: κ ≍ dr2 , for r2 ∈ [0, 1]. Our contributions can be summarized as follows. 1. We show that even in the case of perfect spike-target align- ment (r1 = 0), the spherical gradient flow commonly em- ployed in recent literature (see e.g. [ BAGJ21, BBSS22]) cannot recover the target direction for moderate spike magnitudes in the population limit. The failure of this covariance-agnostic procedure under anisotropic structure insinuates the necessity of an appropriate covariance-aware normalization to effectively learn the single index model. 2. We show that a covariance-aware normalization that resem- bles batch normalization resolves this issue. Indeed, the resulting gradient flow can successfully recover the target direction u in this case, and depending on the amount of spike-target alignment, the sample complexity can significantly improve compared to the isotropic case. 3. Under the spiked covariance model, we prove a three-stage phase transition for the sample complexity depending on the quantities r1 and r2. For a suitable direction and magnitude of the spike, the sample complexity can be made ˜O(d3+ν) for any ν >0 which is independent of the information exponent s. This should be compared against the known complexity of ˜O(ds) under isotropic data. 4. We finally show that preconditioning the training dynamics with the inverse covariance improves the sample complexity. This is particularly significant for the spiked covariance model where ˜O(d3+ν) sample complexity can be reduced to ˜O(d1+ν) for any ν >0 which is almost linear in dimension. The three-stage phase transition also emerges, as illustrated in Figure 1: in the “hard” regime, the complexity remains ˜O(ds) regardless of the magnitude and direction of the spike, while in the “easy” regime the complexity only depends on the spike magnitude r2 and not its direction, hence it is independent of r1. The “intermediate” regime interpolates between these two; smaller r1 and larger r2 improve the sample complexity. The rest of the paper is organized as follows. We discuss the notation and the related work in the remainder of this section. We provide preliminaries on the statistical model and the training procedure in Section 2, and provide a negative result on the covariance-agnostic gradient flow in Section 2.1. Our main sample complexity result on a single neuron is presented in Section 3.2. We provide our results on multi-neuron neural networks in Section 4 and also discuss extensions such as preconditioning and its implications. We provide a summary of our proof techniques in Section 5 and conclude with an overview and future work in Section 6. Notation. We use ⟨·, ·⟩ and ∥·∥ to denote Euclidean inner product and norm. For matrices, ∥·∥ denotes the usual operator norm, and λmax(·) and λmin(·) denote the largest and smallest eigenvalues respectively. We reserve γ for the standard Gaussian distribution on R, and let ∥·∥γ denote the L2(γ) norm. Sd−1 is the unit d-dimensional sphere. For quantities a and b, we will use a ≲ b to convey there exists a constant C (a universal constant unless stated otherwise, in which case may depend on polylogarithmic factors of d) such that a ≤ Cb, and a ≍ b signifies that a ≲ b and b ≲ a. 1.2 Further related work Non-Linear Feature Learning with Neural Networks. Recently, two popular scaling regimes of neural networks have emerged for theoretical studies. A large initialization variance leads to the lazy training regime, where the weights do not move significantly, and the training dynamics is captured by the neural tangent kernel 2(NTK) [JGH18, COB19]. However, there are many instances of function classes that are efficiently learnable by neural networks and not efficiently learnable by the NTK [YS19, GMMM19]. Under a smaller initialization scaling, gradient descent on infinite-width neural networks becomes equivalent to Wasserstein gradient flow on the space of measures, known as the mean-field limit [ CB18, RVE18, MMM19, NWS22, Chi22], which can learn certain low-dimensional target functions efficiently [WLLM19, AAM22, HC22, ASKL23]. As for neural networks with smaller width, recent works have shown that a two-stage feature learning procedure can outperform the NTK when the data is sampled uniformly from the hypercube [ BEG+22] or isotropic Gaussian [DLS22, BES+22, BBSS22, MHPG+23, ABAM23]. However, these results only consider the isotropic case and do not take into account the additional structure that might be present in the covariance matrix of the input data. Two notable exceptions are [ GMMM20, RGKZ21], where the authors analyzed a spiked covariance and Gaussian mixture data, respectively. Our setting is closer to [ GMMM20], however, they do not provide optimization guarantees through gradient-based training. Finally, in a companion work [BES+23], we zoom into the setting where the spike and target direction are perfectly aligned ( r1 = 0), and prove learnability in the n ≍ d regime for both kernel ridge regression and two-layer neural network. Learning Single Index Models. The problem of estimating the relevant direction in a single index model is classical in statistics [ LD89], with efficient dedicated algorithms ([ KKSK11, CM20] among others). However, these algorithms are non-standard and instead, we are concerned with standard iterative algorithms like training neural networks with gradient descent. Recently, [ DH18] considered an iterative optimization procedure for learning such models with a polynomial sample complexity that is controlled by the smoothness of the link function. [ BES+22] considered the effect of taking a single gradient step on the ability of a two-layer neural network to learn a single index model, and [ BBSS22] considered training a special two-layer neural network architecture where all neurons share the same weight with gradient flow. Finally, [ MHPG+23] considered learning a monotone single index model using a two-layer neural network with online SGD. However, these works only consider the isotropic Gaussian input, and the effect of anisotropy in the covariance matrix when training a neural network to learn a single index model has remained unclear. Training a Single Neuron with Gradient Descent. When training the first layer, we consider a setting where there is only one effective neuron. A large body of works exists on training a single neuron using variants of gradient descent. In the realizable setting (i.e. identical link and activation), the typical assumptions on the activation correspond to information exponent 1 as the activations are required to be monotone or have similar properties, see e.g. [ Sol17, YO20, DKTZ22]. In the agnostic setting, [ FCG20] considered initializing from the origin which is a saddle point for information exponent larger than 1. [ ATV22] also considered the agnostic learning of a ReLU activation, albeit their sample complexity is not explicit other than being polynomial in dimension. 2 Preliminaries: Statistical Model and Training Procedure For a d-dimensional input x and a link function g ∈ L2(γ), consider the single index model y = g \u0010 ⟨u,x⟩ ∥Σ1/2u∥ \u0011 + ϵ with x ∼ N(0, Σ), (2.1) where ϵ is a zero-mean noise with O(1) sub-Gaussian norm and u ∈ Sd−1. Learning the model (2.1) corresponds to approximately recovering the unknown link g and the unknown direction u. Note that a normalization is needed to make this problem well-defined; without loss of generality, we write⟨u, x⟩/∥Σ1/2u∥ to ensure that the input variance and the scaling of g both remain independent of the conditioning of Σ. For this learning task, we will use a two-layer neural network of the form ˆy(x; W, a, b) := mX i=1 aiϕ(⟨wi, x⟩ + bi), (2.2) where W = {wi}m i=1 is the m×d matrix whose rows corresponds to first-layer weights wi, a = {ai}m i=1 denote the second-layer weights, b = {bi}m i=1 denote the biases, and ϕ is the non-linear activation function. We assume g and ϕ are weakly differentiable with weak derivatives g′ and ϕ′ respectively, and g, g′, ϕ, ϕ′ ∈ L2(γ). 3We are interested in the high-dimensional regime; thus, d is assumed to be sufficiently large throughout the paper. Our ultimate goal is to learn both unknowns g and u by minimizing the population risk R(W, a, b) := 1 2 E \u0002 (ˆy(x; W, a, b) − y)2\u0003 , (2.3) using a gradient-based training method such as gradient flow as in recent works. We follow the two-step training procedure that is frequently employed in recent works [BES+22, MHPG+23, BBSS22, DLS22]: First, we train the first-layer weights W to learn the unknown direction u using a gradient flow; at the end of this stage, the neurons wi align with u. Here, the goal is to recover only the direction; thus, the magnitudes are not relevant. Next, using random biases and training the second-layer weights, we obtain a good approximation for the unknown link function g. In the majority of this work, we focus on the first part of this two-stage procedure as the alignment between wi’s and u is essentially determining the sample complexity of the overall procedure. This problem is somewhat equivalent to the simplified problem of minimizing (2.3) with m = 1, a1 = 1, b1 = 0, i.e., ˆy(x; W, a, b) is replaced with ˆy(x; w) := ϕ(⟨w, x⟩) and we write R(w) := R(W, a, b) for simplicity. We emphasize that unless ϕ = g (i.e. the link function is known), the first stage of training only recovers the relevant direction u and is not able to approximate g. Indeed, m >1 is often needed to learn the non-linear link function; we will provide such results in Section 4.2 where we derive a complete learnability result for a non-trivial two-layer neural network with m >1. Characteristics of the link function play an important role in the complexity of learning the model. As such, a central part of our analysis will rely on a particular property based on the Hermite expansion of functions in a basis defined by the normalized Hermite polynomials {hj}j≥0 given as hj(z) = (−1)jez2/2 √j! dj dzj e−z2/2. (2.4) These polynomials form an orthonormal basis in the space L2(γ), and the resulting expansion yields the following measure of complexity for g, which is termed as the information exponent . Definition 1 (Information exponent). Let g = P j≥0 αjhj be the Hermite expansion of g. The information exponent of g is defined to be s := inf{j >0 : αj ̸= 0}. This concept was introduced in [ BAGJ21] in a more general framework, and our definition is more in line with the setting in [ BBSS22]. We remark that the definition of [ BAGJ21] can be modified to handle anisotropy in which case one arrives at Definition 1. We provide a detailed discussion on this concept together with some properties of the Hermite expansion in Appendix A. Throughout the paper, we assume that the information exponent does not grow with dimension. In the case where the d-dimensional input data is isotropic, [ BBSS22] showed that learning a single index target with full-batch gradient flow requires a sample complexity of ˜O(ds) for s ≥ 3 where s is the information exponent of g. We will show that this sample complexity can be improved under anisotropy. More specifically, if the input covariance Σ has non-trivial alignment with the unknown direction u, we prove in Section 3 that the resulting sample complexity can be even made independent of the information exponent if we use a certain normalization in the training. In what follows, we prove that such a normalization in training procedure is indeed necessary. 2.1 Spiked model and limitations of covariance-agnostic training In practice, data often exhibit a certain structure which may have a profound impact on the statistical procedure. A well-known model that captures such a structure is the spiked model [Joh01] for which one or several large eigenvalues of the input covariance matrix Σ are separated from the bulk of the spectrum (see also [BBAP05, BS06]). Although our results hold for generic covariance matrices, they reveal interesting phenomena under the following spiked model assumption. Assumption 1. The covariance Σ follows the (κ, θ)-spiked model if Σ = Id+κθθ⊤ 1+κ where ∥θ∥ = 1. 4In pursuit of the target (unit) direction u, the magnitude of the neuron w is immaterial; thus, recent works take advantage of this and simplify the optimization trajectory by projecting w onto unit sphere Sd−1 throughout the training process [ BAGJ21, BBSS22]. In the sequel, we study the same dynamics which is agnostic to the input covariance in order to motivate our investigation of normalized gradient flow in Section 3. More specifically, we consider the spherical population gradient flow dwt dt = −∇SR(wt) where ∇SR(w) = ∇R(w) − ⟨∇R(w), w⟩w. (2.5) where ∇S is the spherical gradient at the current iterate. It is straightforward to see that when the initialization w0 is on the unit sphere, the entire flow will remain on the unit sphere, i.e. wt ∈ Sd−1 for all t ≥ 0. The flow (2.5) has been proven useful for learning the direction u [BBSS22] in the isotropic case Σ = Id when the activation ϕ is ReLU. In contrast, when Σ follows a spiked model, we show that it can get stuck at stationary points that are almost orthogonal to u. Indeed, when the input covariance Σ has a spike in the target direction u, i.e. θ = u, one expects that the training procedure benefits from this as the input x contains information about the sought unknown u without even querying the response y. The following result proves the contrary; for moderate spike magnitudes, the alignment between the first-layer weights and target ⟨wt, u⟩ will be insignificant for all t. Theorem 2. Let s >2 be the information exponent of g with E[g] = 0, and further assume Σ follows the (κ, u)-spiked model with Ω(1) ≤ κ ≤ O(d s−2 s−1 ). For the ReLU activation, if wt denotes the solution to the population flow (2.5) initialized uniformly at random over Sd−1, then, sup t≥0 \f\f wt, u \u000b\f\f ≲ 1/ √ d, (2.6) with probability at least 0.99 over the random initialization. A non-trivial alignment between the first-layer weights wt and the target direction u is required to learn the single index model (2.1). However, the above result implies that in high dimensions when d ≫ 1, the alignment is negligible in the population limit (when the number of samples goes to infinity). We remark that when the spike magnitude is large, i.e. κ ≥ Ω(d), the flow (2.5) can achieve alignment as the problem essentially becomes one-dimensional, as we demonstrate in Appendix B. To see why the flow (2.5) gets stuck at saddle points and fails to recover the true direction, notice that R(w) = 1 2 E h (ϕ(⟨w, x⟩) − y)2 i = 1 2 E \u0002 ϕ(⟨w, x⟩)2\u0003 − E[ϕ(⟨w, x⟩)y] + 1 2 E \u0002 y2\u0003 . (2.7) If the input was isotropic, i.e. x ∼ N(0, Id), the first term in (2.7) would be equal to ∥ϕ2∥γ, which is independent of w. Thus, minimizing R(w) in this case is equivalent to maximizing the “correlation” term E[ϕ(⟨w, x⟩)y]. However, under the spiked model, the alignment between w and u breaks the symmetry; consequently, the first term in the decomposition grows with ⟨w, u⟩, creating a repulsive force that traps the dynamics around the equator where w is almost orthogonal to u. 3 Main Results: Alignment via Normalized Dynamics Having established that the covariance-agnostic training dynamics (2.5) is likely to fail, we consider a covariance-aware normalized flow in this section and show that it can achieve alignment with the unknown target and enjoy better sample complexity compared to the existing results [BAGJ21, BBSS22] in the isotropic case. We start with the population dynamics. 3.1 Warm-up: Population dynamics To simplify the exposition, we define z := Σ−1/2x, w := Σ1/2w/∥Σ1/2w∥ and similarly define u, and consider the prediction function ˆy(x; w) := ϕ(⟨w, z⟩). Due to symmetry, the second moment of the prediction is 5E \u0002 ˆy(x; w)2\u0003 = ∥ϕ∥2 γ which is independent of w; thus, the population risk reads R(w) := 1 2 E h (ˆy(x; w) − y)2 i = 1 2∥ϕ∥2 γ + 1 2 E \u0002 y2\u0003 − E[ϕ(⟨w, z⟩)y]. (3.1) In (3.1), the only term that depends on the weights w is the correlation term and the source of the repulsive force in (2.7) is eliminated; we have ∇wR(w) = −∇w E[ϕ(⟨w, z⟩)y]. Based on this, we use the following normalized gradient flow for training dwt dt = −η(wt)∇wR(wt) where η(w) = ∥Σ1/2w∥2. (3.2) We remark that, although not identical, this normalization is closely related to batch normalization which is commonly employed in practice [ IS15]. Under the invariance provided by the current normalization, minimizing R(w) corresponds to maximizing E[ϕ(⟨w, z⟩)y]. Thus, instead of w, it will be more useful to track the dynamics of its properly normalized counterpart w, which is made possible by the following intermediary result that follows from Stein’s lemma; also see e.g. [EDB16, MHPG +23]. Lemma 3. Suppose we train wt using the gradient flow (3.2). Then wt solves the following ODE dwt dt = −ζϕ,g(  wt, u \u000b )(Id − wtwt⊤ )Σ(Id − wtwt⊤ )u, (3.3) where ζϕ,g(⟨w, u⟩) := −E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)]. We will investigate if the modified flow (3.3) achieves alignment; in this context, alignment corresponds to  wt, u \u000b ≈ 1. Towards that end, we make the following assumption. Assumption 2. Let g = P j≥0 αjhj and ϕ = P j≥0 βjhj be the Hermite decomposition of g and ϕ respectively. For some universal constant c >0, we assume that ζϕ,g(ω) = − X j>0 jαjβj ωj−1 ≤ −c ωs−1, ∀ω ∈ (0, 1) where s is the information exponent of g. There are several important examples that readily satisfy Assumption 2. The obvious example is when the link function is known as in [ BAGJ21], i.e. ϕ = g. A more interesting example is when ϕ is an activation with degree s non-zero Hermite coefficient (e.g. ReLU when s is even, see [ GKK19, Claim 1]) and g is a degree s Hermite polynomial, which for s = 2 corresponds to the phase retrieval problem. In this case, the assumption is satisfied if αs and βs have the same sign, which occurs with probability 0 .5 if we randomly choose the sign of the second layer. Under this condition, the following result shows that the population flow (3.3) can achieve alignment. Proposition 4. Suppose Assumption 2 holds and consider the gradient flow given by (3.3) with initialization satisfying  w0, u \u000b > 0. Then, we have  wT , u \u000b ≥ 1 − ε as soon as T ≍ τs \u0000 w0, u \u000b\u0001 + ln(1/ε) λmin(Σ) where τs(z) :=    1 s = 1 ln(1/z) s = 2 (1/z)s−2 s >2 . (3.4) We remark that the information exponent enters the rate in (3.4) through the function τs, and time needed to achieve ε alignment gets worse with larger information exponent. Indeed, it is understood that this quantity serves as a measure of complexity for the target function being learned. 63.2 Empirical dynamics and sample complexity Given n i.i.d. samples {(x(i), y(i))}n i=1 from the single index model (2.1), we consider the flow dwt dt = −η(wt)∇ ˆR(wt) with ∇ ˆR(w) := −∇w ( 1 n nX i=1 ϕ   w, x(i)\u000b ∥ˆΣ 1/2 w∥ ! y(i) ) , (3.5) where we estimate the covariance matrix Σ using the sample mean ˆΣ := 1 n′ Pn′ i=1 x(i)x(i)⊤ over n′ i.i.d. samples; the above dynamics defines an empirical gradient flow. Notice that we ignored the gradient associated with the term ϕ2 since the population dynamics ensures that its gradient will concentrate around zero; thus, it is redundant to estimate this term. Taking this term into account would come at no extra cost for the ReLU activation, yet for arbitrary smooth ϕ, the concentration argument would require n′ ≳ d2. Below, we will use n′ = n for smooth activations, i.e. the same dataset can be used for covariance estimation. For ReLU, however, we require a more accurate covariance estimator, thus, we use n′ ≳ n2 by assuming access to an additional n′ − n unlabeled input data points. This setting is still not far from practice where the number of unlabeled samples is typically much larger than that of labeled ones. Similar to the previous section, we track the dynamics of normalized w by defining w := ˆΣ1/2w/∥ˆΣ1/2w∥ (and leave u unchanged from Section 3.1). The same arguments as in Lemma 3 allow us to track the evolution of w, which ultimately yields the following alignment result under general covariance structure. Theorem 5. Let s be the information exponent of g, and assume it satisfies |g(·)| ≲ 1 + |·|p for some p >0. For ϕ denoting either the ReLU activation or a smooth activation satisfying |ϕ′|∨|ϕ′′| ≲ 1, suppose Assumption 2 holds. For any ε >0, suppose we run the finite sample gradient flow (3.5) with η(w) = ∥ˆΣ1/2w∥2, initialized such that  w0, u \u000b > 0, and with number of samples n ≳ dκ(Σ)2 n w0, u \u000b2(1−s) ∨ ε−2 o , where κ(Σ) is the condition number of Σ. Then, for T ≍ τs(⟨w0,u⟩)+ln(1/ε) λmin(Σ) , we have  wT, u \u000b ≥ 1 − ε, (3.6) with probability at least 1 − c1d−c2 for some universal constants c1, c2 > 0 over the randomness of the dataset. Here, τs is defined in (3.4) and ≳ hides poly-logarithmic factors. Remark. The initial condition  w0, u \u000b > 0 is required when we have odd information exponent. When w0 is initialized uniformly over Sd−1, the condition holds with probability 0 .5 over the initialization. See [BAGJ21, Remark 1.8] for further discussion on this condition. Remark. Notice the discrepancy in the definition of w := ˆΣ 1/2 w/∥ˆΣ 1/2 w∥ and u := Σ1/2u/∥Σ1/2u∥, where the former uses the empirical covariance while the latter uses the population. This choice is made to simplify the proof, and in fact this type of recovering u is sufficient to approximate the target function g (c.f. Theorem 9). More precisely, we need the arguments of ϕ and g to be sufficiently close, i.e. Ex \"\u0010 ⟨w, x⟩ ∥ˆΣ 1/2 w∥ − ⟨u, x⟩ ∥Σ1/2u∥ \u00112 # ≲ ∥w − u∥2 + ∥Σ1/2 ˆΣ −1/2 − Id∥2 ≲ ε + d/n′, where we used the concentration of ˆΣ due to Lemma 24. Theorem 5 follows from ensuring that the finite-sample estimation error of population quantities, which is of order p d/n by concentration, remains smaller than the signal (the time derivative of the alignment in the population limit) near initialization. The strength of this signal is controlled by the initial alignment and is of order  w0, u \u000bs−1 /κ(Σ). We highlight that the improvement in the sample complexity compared to the isotropic setting occurs whenever the covariance structure induces a stronger initial alignment and consequently stronger signal. The following corollary demonstrates a concrete example of such improvement by specializing Theorem 5 for a spiked covariance model. 7Corollary 6. Consider the setting of Theorem 5 with Σ following the (κ, θ)-spiked model, where ⟨u, θ⟩ ≍d−r1 and κ ≍ dr2 with r1 ∈ [0, 1/2] and r2 ∈ [0, 1]. Suppose w0 is sampled uniformly from Sd−1. Then, when conditioned on  w0, u \u000b > 0, the sample complexity in Theorem 5 reads n ≳    d1+2r2 \u0000 ds−1 ∨ ε−2\u0001 0 < r2 < r1 d1+2r2 \u0000 d(s−1)(1−2(r2−r1)) ∨ ε−2\u0001 r1 < r2 < 2r1 d1+2r2 \u0000 d(s−1)(1−r2) ∨ ε−2\u0001 2r1 < r2 < 1 , (3.7) where ≳ hides poly-logarithmic factors of d. Remark. Notice that r1 = 1/2 implies u and θ are relatively randomly oriented and reveal no non-trivial information about each other. Further, r2 = 1 corresponds to a setting where kernel lower bounds indicate their sample complexity may be dimension-free [DWY21]. We recall that in the isotropic setting where Σ = Id, the sample complexity of learning a target g with information exponent s using the full-batch gradient flow is ˜O(ds) for s ≥ 3 [BBSS22]. In this regime, the sample complexity in Corollary 6 is strictly smaller than ˜O(ds) as soon as (s − 1)r1/(s − 2) < r2. Furthermore, for any ν > 0 it is at most ˜O(d3+ν) as soon as r2 ≥ 1 − ν/(s − 3) and 2 r1 < r2, in which case the sample complexity becomes independent of the information exponent. Interestingly, the complexity becomes independent of r1 when r2 > 2r1 or r2 < r1, i.e. the direction of the spike becomes irrelevant when the spike magnitude is sufficiently large or small. Corollary 6 demonstrates that structured data can lead to better sample complexity when the right normalization is used during training. This complements Theorem 2 where we recall that spherical training dynamics ignores the structure in data, in which case the target direction cannot be recovered. The three-step phase transition of Corollary 6 is due to the different behaviour of the inner product w0, u \u000b in different regimes of r1 and r2. When r2 < r1, we have  w0, u \u000b ≍  w0, u \u000b , thus the initial alignment is just as uninformative as the isotropic case providing no improvement. Moreover, a potentially large condition number may hurt the sample complexity in this case. On the other hand, when r1 < r2 < 2r1 we have  w0, u \u000b ≍ κ⟨u, θ⟩  w0, θ \u000b , and r2 > 2r1 leads to  w0, u \u000b ≍ √κ  w0, θ \u000b , thus large κ or ⟨u, θ⟩ in this regime may improve the sample complexity. In the case where g is a polynomial of degree p, the lower bound for rotationally invariant kernels (including the neural tangent kernel at initialization) implies a complexity of at least dΩ((1−r2)p) [DWY21]. Thus the sample complexity of Corollary 6 can always outperform the kernel lower bound when p is sufficiently large and s remains constant. Finally note that as the information exponent grows, achieving the same sample complexity requires larger magnitudes of the spike, which is intuitive as the target becomes more complex. 4 Implications to Neural Networks and Further Improvements 4.1 Improving Sample Complexity via Preconditioning We now demonstrate that preconditioning the training dynamics with ˆΣ−1 can remove the dependency on κ(Σ), ultimately improving the sample complexity. Consider the preconditioned gradient flow dwt dt = −η(wt) ˆΣ −1 ∇ ˆR(wt) with η(w) = ∥ˆΣ 1/2 w∥2. (4.1) We have the following alignment result. Theorem 7. Consider the same setting as Theorem 5, and assume we run the preconditioned empirical gradient flow (4.1) with number of samples n ≳ d n w0, u \u000b2(1−s) ∨ ε−2 o , where ≳ hides poly-logarithmic factors of d. Then, for T ≍ τs \u0000 w0, u \u000b\u0001 + ln(1/ε), we have  wT , u \u000b ≥ 1 − ε, 8with probability at least 1 − c1d−c2 for some universal constants c1, c2 > 0. Preconditioning removes the condition number dependence, which is particularly important in the spiked model case where this quantity can be large. Corollary 8. Consider the setting of Theorem 7, and assume we run the preconditioned empirical gradient flow (4.1) for the (κ, θ)-spiked model where ⟨u, θ⟩ ≍d−r1 and κ ≍ dr2 with r1 ∈ [0, 1/2] and r2 ∈ [0, 1]. Suppose w0 is sampled uniformly from Sd−1. Then, when conditioned on  w0, u \u000b > 0, the sample complexity of Theorem 7 reads n ≳    d \u0000 ds−1 ∨ ε−2\u0001 0 < r2 < r1 d \u0000 d(s−1)(1−2(r2−r1)) ∨ ε−2\u0001 r1 < r2 < 2r1 d \u0000 d(s−1)(1−r2) ∨ ε−2\u0001 2r1 < r2 < 1 , (4.2) where ≳ hides poly-logarithmic factors of d. The above result improves upon Corollary 6; thus, making a case for preconditioning in practice. The complexity results also strictly improve upon the ˜O(ds) complexity in the isotropic case [ BBSS22] when r2 > r1. Further, for any ν >0, we can obtain the complexity of ˜O(d1+ν) (nearly linear in dimension) when r2 > 1−ν/(s−1) and r2 > 2r1 or r1 +1/2(1−ν/(s−1)) < r2 < 2r1. In addition to the remarks of Corollary 6, we note that the complexity is independent of both r1 and r2 when r2 < r1 (cf. Figure 1 hard regime), i.e. the spike magnitude and the spike-target alignment have no effect on the complexity unless r2 ≥ r1. Under the spiked covariance model, one could improve the above results by instead using spectral initialization, i.e. initializing at θ, which can be estimated from unlabeled data. Assuming perfect access to θ, using the statement of Theorems 5 and 7, this initialization would imply a sample complexity of ˜O(d1+2r2+((s−1)(2r1−r2)∨0)) without preconditioning and that of ˜O(d1+((s−1)(2r1−r2)∨0)) with preconditioning. 4.2 Two-layer neural networks and learning the link function Our main focus so far was learning the target direction u. Next, we consider learning the unknown link function with a neural network, providing a complete learnability result for single index models. We use Algorithm 1 and train the first-layer of the neural network with either the empirical gradient flow (3.5) or the preconditioned version (4.1). Then, we randomly choose the bias units and minimize the second layer weights using another gradient flow. Our goal is to track the sample complexity n needed to learn the single index target which we compare against the results of [ BBSS22]. We highlight that layer-wise training in Algorithm 1 is frequently employed in literature [ BES+22, BBSS22, DLS22, MHPG+23] and in particular [BBSS22] also used gradient flow for training. Algorithm 1 Layer-wise training of a two-layer ReLU network with gradient flow (GF). Input: w0 ∈ Rd, T, T′, ∆ ∈ R+ and data {(x(i), y(i))}n i=1. 1: Train the first layer weights WT j using the GF (3.5) or the preconditioned GF (4.1). 2: Normalize the weights WT j := WT j /∥ˆΣ 1/2 WT j ∥ for every 1 ≤ j ≤ m. 3: Let bj i.i.d. ∼ Unif(−∆, ∆) and a0 j = 1/m for 1 ≤ j ≤ m. 4: Train the second layer weights aT′ via the gradient flow dat dt = −∇a ( 1 2n nX i=1 (ˆy(x(i); WT , at, b) − y(i))2 + λ∥at∥2 2 ) . 5: return (WT , aT′ , b). Theorem 9. Let g be twice weakly differentiable with information exponent s and assume g′′ has at most polynomial growth. Suppose ϕ is the ReLU activation, Assumption 2 holds and we run Algorithm 1 with 9w0 initialized uniformly over Sd−1. For any ε >0, let n and T be chosen according to Theorem 5 when we run the gradient flow (3.5) and Theorem 7 when we run the preconditioned gradient flow (4.1). Then, for ∆ ≍ p ln(nd), some regime of λ given by (D.3) and sufficiently large T′ given by (D.4), we have E(x,y) \u0014\u0010 ˆy(x; WT , aT′ , b) − y \u00112\u0015 ≤ C1 E \u0002 ϵ2\u0003 + C2(ε + 1/m), (4.3) conditioned on  w0, u \u000b > 0 with probability at least 0.99 over the randomness of the dataset, biases, and initialization, where C1 is a universal constant and C2 hides polylog(m, n, d) factors. The next result immediately follows from the previous theorem together with Corollaries 6 & 8. Corollary 10. In the setting of Theorem 9, if Σ follows the (κ, θ)-spiked model, the sample complexity n is given by (3.7) if we use the empirical gradient flow and (4.2) if we use the preconditioned version. We remark that for fixed ε, the sample complexity to learn g in the isotropic case is ˜O(ds) [BBSS22]. Under the spiked model, if we assume that r2 is sufficiently large and r1 is sufficiently small as discussed in the previous section, Corollary 10 improves this rate to either (3.7) when the empirical gradient flow is used without preconditioning or to (4.2) with preconditioning. 5 Technical Overview In this section, we briefly discuss the key intuitions that lead to the proof of our main results. We first review the case Σ = Id, where we have the following decomposition for population loss R(w) := 1 2 E \u0002 (ϕ(⟨w, x⟩) − y)2\u0003 = 1 2 ∥ϕ∥2 γ + 1 2 E \u0002 y2\u0003 − E[ϕ(⟨w, x⟩)g(⟨u, x⟩)]. (5.1) Notice that the only term contributing to the population gradient is the last term which measures the correlation between ϕ and g. Following the gradient follow and applying Stein’s lemma yields d⟨wt, u⟩ dt = E \u0002 ϕ′(  wt, x \u000b )g′(⟨u, x⟩) \u0003 (1 −  wt, u \u000b2 ) = (1 −  wt, u \u000b2 ) X j≥s jαjβj  wt, u \u000bj−1 , where the second identity follows from the Hermite expansion; see also [ EDB16, EBD19]. Assume αsβs > 0 to ensure that the population dynamics will move towards u at least near initialization. When replacing the population gradient with a full-batch gradient, we need the estimation noise to be smaller than the signal existing in the gradient. When  w0, u \u000b ≪ 1, this signal is roughly of the order  w0, u \u000bs−1 . As the uniform concentration error over Sd−1 scales with p d/n, we need n ≍ d  w0, u \u000b2(s−1) to ensure the signal remains dominant and wt moves towards u. When w0 is initialized uniformly over Sd−1 this translates to a sample complexity of n ≍ ds, which is indeed the sample complexity obtained by [ BBSS22] who follow a similar argument (and use a more complicated architecture to remove the requirement of αsβs > 0). However, the behavior of the spherical dynamics changes significantly when we move to the anisotropic case. Suppose Σ follows a (κ, u)-spiked model and ϕ is the ReLU activation. Using Lemma 12, it is easy to show that with the spherical gradient flow, the alignment obeys the following ODE d⟨wt, u⟩ dt = ( E \u0002 ϕ′(  wt, z \u000b )g′(⟨u, z⟩) \u0003 − κ ˜ψϕ,g(wt) 1 + κ  wt, u \u000b ) (1 −  wt, u \u000b2 ), where ˜ψϕ,g(wt) is introduced in Lemma 12. The additional ˜ψϕ,g(wt) term creates a force towards the equator ⟨wt, u⟩ = 0. The presence of this term is due to the fact that unlike (5.1), the term E \u0002 ϕ(⟨wt, u⟩)2\u0003 is no longer independent of w and cannot be replaced by ∥ϕ∥2 γ. When w0 is initialized uniformly over Sd−1 and Ω(1) ≤ κ ≤ O(d), we have  w0, u \u000b ≍ √κ  w0, u \u000b . Furthermore, at this initialization ˜ψϕ,g(w0) ≈ 1/2. Therefore, d⟨wt, u⟩ dt ≈ ( sαsβs(√κ  w0, u \u000b )s−1 −  w0, u \u000b 2 ) . 10Then, we can observe that as  w0, u \u000b ≍ 1/ √ d, when κ = O(d1−1/(s−1)) the term pointing towards u is dominated by the term pointing towards the equator, and thus moving beyond ⟨w, u⟩ = O(1/ √ d) is impossible. To remove the repulsive force in the spherical dynamics, we can directly normalize the input of ϕ. As demonstrated by (3.1), once again the only term that varies with w would be the correlation loss. Specifically, using the result of Lemma 3, in the population limit we can track  wt, u \u000b via d  wt, u \u000b dt = E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)]  ut ⊥, Σut ⊥ \u000b , (5.2) where ut ⊥ := u −  wt, u \u000b wt. Thus, the strength of the signal at initialization is of order  w0, u \u000bs−1 /κ(Σ), which after controlling the error in the estimate of ˆΣ and in the estimate of population gradient using finitely many samples, leads to the sample complexity n ≍ dκ(Σ)2 w0, u \u000b2(1−s) . Therefore, the improvement in comparison to the isotropic case comes from the fact that at a random initialization, the initial alignment w0, u \u000b can be much stronger than the isotropic alignment  w0, u \u000b , which is emphasized in Corollary 6. Additionally, using preconditioning as in Section 4.1 will modify the dynamics of  wt, u \u000b to d  wt, u \u000b dt = E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)]∥ut ⊥∥2, (5.3) thus removing the dependence on the condition number of Σ in the sample complexity. 6 Conclusion We studied the dynamics of gradient flow to learn single index models when the input data is not necessarily isotropic and its covariance has additional structure. Under a spiked model for the covariance matrix, we showed that using spherical gradient flow, as an example of a covariance-agnostic training mechanism employed in the recent literature, is unable to learn the target direction of the single index model even when the spike and the target directions are identical. In contrast, we showed that an appropriate normalization of the weights removes this problem, in which case the target direction will be successfully recovered. Moreover, depending on the alignment between the covariance structure and the target direction, the sample complexity of the learning task can improve upon that of its isotropic counterpart, while also outperforming lower bounds for rotationally-invariant kernels. This phenomenon is due to the additional information about the target direction contained in the covariance matrix which improves the effective alignment at initialization. Additionally, we showed that a simple preconditioning of the gradient flow using the inverse empirical covariance can improve the sample complexity, achieving almost linear rate in certain settings. We outline a few limitations of our current work and discuss directions for future research. • While studying single index models provides a pathway to a general understanding of feature learning with structured covariance, considering multi-index models can provide a more complete picture [ PSE22]. Specifically, learning multiple-index models requires extensions of information exponent to more general complexity measures and establishing incremental learning dynamics [ ABAM23]. We leave the interplay between these concepts and a structured covariance matrix as an interesting open problem to be studied in future work. • Gradient flow for learning a single index model can be seen as an example of a Correlational Statistical Query (CSQ) algorithm [ BF02, Rey20], i.e. an algorithm that only accesses noisy estimates of expected correlation queries from the model. Understanding the limitations of learning single index models under a structured input covariance through a CSQ lower bound perspective is an important future direction that would complement our results in this paper. • When training the first layer using a gradient flow, we considered a somewhat unconventional initialization and relied on the symmetry it induces. It is interesting to consider cases where we train a network with multiple neurons starting from a more standard initialization. This analysis is challenging due to the interactions between the neurons, but can potentially relax Assumption 2. 11Acknowledgments The authors thank Alberto Bietti and Zhichao Wang for discussions and feedback on the manuscript. TS was partially supported by JSPS KAKENHI (20H00576) and JST CREST. MAE was partially supported by NSERC Grant [2019-06167], CIFAR AI Chairs program, CIFAR AI Catalyst grant. References [AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz, The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks, Conference on Learning Theory, 2022. [ABAM23] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz,Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics , arXiv preprint arXiv:2302.11055 (2023). [ASKL23] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro,From high-dimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks , arXiv preprint arXiv:2302.05882 (2023). [ATV22] Pranjal Awasthi, Alex Tang, and Aravindan Vijayaraghavan, Agnostic learning of general relu activation using gradient descent , arXiv preprint arXiv:2208.02711 (2022). [BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath, Online stochastic gradient descent on non-convex losses from high-dimensional inference. , J. Mach. Learn. Res. 22 (2021), 106–1. [BBAP05] Jinho Baik, G´ erard Ben Arous, and Sandrine P´ ech´ e,Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices . [BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song, Learning single-index models with shallow neural networks , Advances in Neural Information Processing Systems, 2022. [BEG+22] Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang, Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit , arXiv preprint arXiv:2207.08799 (2022). [BES+19] Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang, Generalization of two-layer neural networks: An asymptotic viewpoint , International Conference on Learning Representations, 2019. [BES+22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang, High- dimensional asymptotics of feature learning: How one gradient step improves the representation , arXiv preprint arXiv:2205.01445 (2022). [BES+23] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu, Learning in the presence of low-dimensional structure: a spiked random matrix perspective , in preparation (2023). [BF02] Nader H Bshouty and Vitaly Feldman, On using extended statistical queries to avoid membership queries, Journal of Machine Learning Research 2 (2002), no. Feb, 359–395. [BS06] Jinho Baik and Jack W Silverstein, Eigenvalues of large sample covariance matrices of spiked population models, Journal of multivariate analysis 97 (2006), no. 6, 1382–1408. [CB18] Lenaic Chizat and Francis Bach, On the Global Convergence of Gradient Descent for Over- parameterized Models using Optimal Transport , Advances in Neural Information Processing Systems, 2018. [Chi22] L´ ena¨ ıc Chizat,Mean-field langevin dynamics: Exponential convergence and annealing , arXiv preprint arXiv:2202.01009 (2022). 12[CM20] Sitan Chen and Raghu Meka, Learning polynomials in few relevant dimensions , Conference on Learning Theory, 2020. [COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach, On Lazy Training in Differentiable Pro- gramming, Advances in Neural Information Processing Systems, 2019. [DH18] Rishabh Dudeja and Daniel Hsu, Learning single-index models in gaussian space , Conference On Learning Theory, PMLR, 2018, pp. 1887–1930. [DKTZ22] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis, Learning a single neuron with adversarial label noise via gradient descent , Conference on Learning Theory, PMLR, 2022, pp. 4313–4361. [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi, Neural Networks can Learn Repre- sentations with Gradient Descent , Conference on Learning Theory, 2022. [DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang, How rotational invariance of common ker- nels prevents generalization in high dimensions , International Conference on Machine Learning, 2021. [EBD19] Murat A. Erdogdu, Mohsen Bayati, and Lee H. Dicker, Scalable Approximations to Generalized Linear Problems, Journal of Machine Learning Research (2019). [EDB16] Murat A Erdogdu, Lee H Dicker, and Mohsen Bayati, Scaled least squares estimator for glms in large-scale problems, Advances in Neural Information Processing Systems 29 (2016). [Erd15] Murat A Erdogdu, Newton-stein method: a second order method for glms via stein’s lemma , Proceedings of Advances in Neural Information Processing Systems, 2015, pp. 1216–1224. [FCG20] Spencer Frei, Yuan Cao, and Quanquan Gu, Agnostic learning of a single neuron with gradient descent, Advances in Neural Information Processing Systems, vol. 33, Curran Associates, Inc., 2020, pp. 5417–5428. [GKK19] Surbhi Goel, Sushrut Karmalkar, and Adam Klivans, Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals , Advances in neural information processing systems 32 (2019). [GMMM19] B. Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Limitations of Lazy Training of Two-layers Neural Networks , Advances in Neural Information Processing Systems, 2019. [GMMM20] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, When Do Neural Networks Outperform Kernel Methods? , Advances in Neural Information Processing Systems, 2020. [HC22] Karl Hajjar and Lenaic Chizat, Symmetries in the dynamics of wide two-layer neural networks , arXiv preprint arXiv:2211.08771 (2022). [HTFF09] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman, The elements of statistical learning: data mining, inference, and prediction , vol. 2, Springer, 2009. [IS15] Sergey Ioffe and Christian Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift , International conference on machine learning, pmlr, 2015, pp. 448–456. [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler, Neural Tangent Kernel: Convergence and Generalization in Neural Networks , Advances in Neural Information Processing Systems, 2018. [Joh01] Iain M Johnstone, On the distribution of the largest eigenvalue in principal components analysis , The Annals of statistics 29 (2001), no. 2, 295–327. 13[JWHT13] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, An introduction to statistical learning, vol. 112, Springer, 2013. [KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai, Efficient learning of general- ized linear and single index models with isotonic regression , Advances in Neural Information Processing Systems 24 (2011). [LD89] Ker-Chau Li and Naihua Duan, Regression Analysis Under Link Violation , The Annals of Statistics (1989). [LMZ20] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang, Learning over-parametrized two-layer neural networks beyond NTK , Conference on Learning Theory, 2020. [MHPG+23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu, Neural networks efficiently learn low-dimensional representations with SGD , The Eleventh International Conference on Learning Representations, 2023. [MMM19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari, Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit , Conference on Learning Theory, 2019. [NWS22] Atsushi Nitanda, Denny Wu, and Taiji Suzuki, Convex analysis of the mean field langevin dynamics, International Conference on Artificial Intelligence and Statistics, PMLR, 2022, pp. 9741–9757. [O’D14] Ryan O’Donnell, Analysis of boolean functions, Cambridge University Press, 2014. [PSE22] Sejun Park, Umut Simsekli, and Murat A. Erdogdu, Generalization Bounds for Stochastic Gradient Descent via Localized ε-Covers, arXiv preprint arXiv:2209.08951 (2022). [Rey20] Lev Reyzin, Statistical queries and statistical algorithms: Foundations and applications , arXiv preprint arXiv:2004.00557 (2020). [RGKZ21] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov´ a,Classifying high- dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed , Inter- national Conference on Machine Learning, 2021. [RVE18] Grant M Rotskoff and Eric Vanden-Eijnden, Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error , arXiv preprint arXiv:1805.00915 (2018). [Sol17] Mahdi Soltanolkotabi, Learning relus via gradient descent , Advances in neural information processing systems 30 (2017). [Tel23] Matus Telgarsky, Feature selection and low test error in shallow low-rotation relu networks , The Eleventh International Conference on Learning Representations, 2023. [Ver18] Roman Vershynin, High-dimensional probability: An introduction with applications in data science, Cambridge University Press, 2018. [VH16] Ramon Van Handel, Probability in high dimension , 2016. [Wai19] Martin J. Wainwright, High-dimensional statistics: A non-asymptotic viewpoint , Cambridge University Press, 2019. [WLLM19] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma, Regularization matters: Generalization and optimization of neural nets vs their induced kernel , Advances in Neural Information Processing Systems 32 (2019). [YO20] Gilad Yehudai and Shamir Ohad, Learning a single neuron with gradient methods , Proceedings of Thirty Third Conference on Learning Theory, Proceedings of Machine Learning Research, vol. 125, PMLR, 2020, pp. 3756–3786. 14[YS19] Gilad Yehudai and Ohad Shamir, On the Power and Limitations of Random Features for Understanding Neural Networks , Advances in Neural Information Processing Systems, 2019. 15A Background on Hermite Expansion The normalized Hermite polynomials {hj}j≥0 given by (2.4) provide an orthonormal basis for L2(γ), thus for every f ∈ L2(γ) we have f = ∞X j=0 ⟨f, hj⟩γhj, where ⟨f, hj⟩γ := Ez∼N(0,1)[f(z)hj(z)]. We will commonly invoke the following well-known properties of Hermite polynomials. If j ≥ 1, then h′ j = √jhj−1, where h′ j stands for the derivative of hj. Furthermore, if z1 and z2 are two standard Gaussian random variables with E[z1z2] = ρ, then E[hi(z1)hj(z2)] = δijρj where δij is the Kronecker delta. We refer the interested reader to [ O’D14, Chapter 11.2] for additional discussions and properties of these polynomials. We will now discuss how our Definition 1 relates to the original definition of information exponent of [BAGJ21]. In their setting, they assume the true data distribution Pu is parameterized by some unit vector u ∈ Sd−1, and we know the parametric family {Pw}w∈Sd−1 ; thus the problem is to estimate the direction u. Furthermore, they assume the population loss, which is the expectation of some per-sample loss, has spherical symmetry, i.e. the population loss R(w) can be written as R(w) = ˜R(⟨w, u⟩). Then, [ BAGJ21, Definition 1.2] defines the information exponent to be the degree of the first non-zero coefficient of ˜R in its Taylor expansion around the origin. In other words, we say R has information exponent s if    dk ˜R dzk (0) = 0 1 ≤ k < s dk ˜R dzk (0) = −c <0 k = s\f\f\fdk ˜R dzk (z) \f\f\f ≤ C k > s, ∀z ∈ [−1, 1] , where C, c >0 are universal constants. To specialize the above abstract definition to the Gaussian case, consider the setting where the input data is standard Gaussian x ∼ N(0, Id) and the problem is to estimate u ∈ Sd−1 given a response variable y = f(⟨u, x⟩) with known f. Via the Hermite expansion of f, one can write ˜R(⟨w, u⟩) := 1 2 E \u0002 (f(⟨w, x⟩) − f(⟨u, x⟩))2\u0003 = − X j≥1 ⟨f, hj⟩2 γ⟨w, u⟩j + const. Thus, the information exponent of ˜R is indeed the degree of the first non-zero term in the Hermite expansion of f. Now consider the general case where x ∼ N(0, Σ). The spherical symmetry assumed in [ BAGJ21] no longer holds. However, after proper normalization of weights, if we consider the population loss R(w) := 1 2 E     f   ⟨w, x⟩ ∥Σ1/2w∥ ! − f   ⟨u, x⟩ ∥Σ1/2u∥ !!2 , then R(w) = ˜R \u0010 ⟨w,Σu⟩ ∥Σ1/2w∥∥Σ1/2u∥ \u0011 . Indeed, a close examination of the arguments of [ BAGJ21] reveals that for their results to hold, the proper symmetry to consider is the ellipsoidal symmetry, and the proper definition of information exponent is the degree of the first non-zero term in the Hermite expansion of ˜R, which reads ˜R(z) = − X j≥1 ⟨f, hj⟩2zj + const. Once again, we can consistently define the information exponent to be the degree of the first non-zero term in the Hermite expansion of f, as long as the input is Gaussian (potentially anisotropic). B Proofs of Section 2.1 Before beginning our main discussions, we state the following lemma which is a generalization of Stein’s lemma (Gaussian integration by parts), and will help obtain a closed-form expression for the population gradient. We refer to [Erd15, MHPG +23] for similar statements. 16Lemma 11. Let f, g: R → R with g weakly differentiable. Suppose z ∼ N(0, Id). Then, for any w, u ∈ Sd−1, we have E[f(⟨w, z⟩)g(⟨u, z⟩)z] = E[f(⟨w, z⟩)g′(⟨u, z⟩)]u + E[f(⟨w, z⟩){g(⟨u, z⟩)⟨w, z⟩ −g′(⟨u, z⟩)⟨u, w⟩}]w. Proof. Consider the conditional distribution z|⟨w, z⟩ ∼ N(µ, Σ), where µ = ⟨w, z⟩w and Σ = Id − w w⊤. Recall that Stein’s lemma (Gaussian integration by parts) states that when z ∼ N(µ, Σ), then E[g(z)z] = E[g(z)]µ + Σ E[∇g(z)]. Hence, E[g(⟨u, z⟩)z |⟨w, z⟩] = E[g(⟨u, z⟩) |⟨w, z⟩]⟨w, z⟩w + (Id − ww⊤) E[g′(⟨u, z⟩) |⟨w, z⟩]u. Applying the tower property of conditional expectation and rearranging the terms yields the desired result. We are now ready to state and prove the expression for the population gradient when using the ReLU activation. Lemma 12. Suppose ϕ is the ReLU activation and g is weakly differentiable. Let z ∼ N(0, Id). Define w := Σ1/2w ∥Σ1/2w∥ and similarly define u. Then ∇R(w) = Σ n ˜ψϕ,g(w)w + ˜ζϕ,g(w)u o where ˜ψϕ,g(w) := E[−ϕ(⟨w, z⟩)g(⟨u, z⟩) + ϕ′(⟨w, z⟩)g′(⟨u, z⟩)⟨u, w⟩] ∥Σ1/2w∥ + 1 2, and ˜ζϕ,g(w) := −E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)] ∥Σ1/2u∥ . Proof. Notice that the population risk is given by R(w) = 1 2 E     ϕ(⟨w, x⟩) − g   ⟨u, x⟩ ∥Σ1/2u∥ !!2  + E \u0002 ϵ2\u0003 2 = 1 2 E \u0002 ϕ(⟨w, x⟩)2\u0003 − E[ϕ(⟨w, x⟩)g] + E \u0002 y2\u0003 2 , Notice that x = Σ1/2z. By the homogeneity of ReLU, we can rewrite the first term as E \u0002 ϕ(⟨w, x⟩)2\u0003 = w⊤Σw∥ϕ∥2 γ = w⊤Σw 2 . Then we have, ∇R(w) = Σw 2 − E \" ϕ(⟨w, x⟩)′g   ⟨u, x⟩ ∥Σ1/2u∥ ! x # | {z } =:υ(w) Note that ϕ′(⟨w, x⟩) = ϕ′(⟨w, z⟩). Then, υ(w) = Σ1/2 E[ϕ′(⟨w, z⟩)′g(⟨u, z⟩)z] = Σ1/2{E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)]u + E[ϕ(⟨w, z⟩g(⟨u, z⟩) − ϕ′(⟨w, z⟩)g′(⟨u, z⟩)⟨u, w⟩]w} = Σ ( E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)] ∥Σ1/2u∥ u + E[ϕ(⟨w, z⟩g(⟨u, z⟩) − ϕ′(⟨w, z⟩)g′(⟨u, z⟩)⟨u, w⟩] ∥Σ1/2w∥ w ) 17where we used Lemma 11 and the fact that ϕ′(z)z = ϕ(z). Therefore, ∇R(w) = Σ n ˜ψϕ,g(w)w + ˜ζϕ,g(w)u o which concludes the proof. Particularly, the above lemma yields the following corollary for the spherical dynamics in the population limit. Corollary 13. Suppose {wt}t≥0 is a solution to the population spherical gradient flow (2.5), ϕ is the ReLU activation, and Σ follows the (κ, θ)-spiked model. Then, d⟨wt, u⟩ dt = − ˜ζϕ,g(wt)(1 − ⟨wt, u⟩2) 1 + κ − κ n ˜ψϕ,g(wt)⟨wt, θ⟩ + ˜ζϕ,g(wt)⟨u, θ⟩ o 1 + κ (⟨θ, u⟩ −  wt, θ \u000b wt, u \u000b ). (B.1) B.1 Proof of Theorem 2 Plugging in θ = u in Corollary 13, we obtain d⟨wt, u⟩ dt = − ( ˜ζϕ,g(wt) + κ ˜ψϕ,g(wt)⟨wt, u⟩ 1 + κ ) (1 −  u, wt\u000b2 ). (B.2) To prove the statement of the theorem, we will show that whenever \f\f wt, u \u000b\f\f ≤ C√ d , we will have d⟨wt,u⟩ 2 dt < 0, thus when initialized from O(1/ √ d), ⟨wt, u⟩ can never escape the saddle point near the equator ⟨w, u⟩ = 0. Recall from the properties of the Hermite expansion in Appendix A that g′ = X j≥1 p jαjhj−1 and ϕ′ = X j≥1 p jβjhj−1. Since we additionally assume E[g] = α0 = 0, by the definition of ˜ψϕ,g in Lemma 12 and the properties of Hermite expansion discussed in Appendix A, we have ˜ψϕ,g(wt) = P j≥s(j − 1)αjβj  wt, u \u000bj ∥Σ1/2w∥ + 1 2, and similarly ˜ζϕ,g(wt) := − X j≥s jαjβj  wt, u \u000bj−1 . Thus we obtain d⟨wt, u⟩2 dt = 2F(wt)(1 −  wt, u \u000b2 ), where F(wt) := X j≥s jαjβj  wt, u \u000bj−1 wt, u \u000b − κ 1 + κ X j≥s (j − 1)αjβj  wt, u \u000bj+1 wt, u \u000b − κ⟨wt, u⟩2 2(1 + κ) . 18We proceed by upper bounding F. To do so, first note that ∥Σ1/2w∥ = s 1 + κ⟨w, u⟩2 1 + κ ≥ r 1 1 + κ. To bound the first term of F, we have X j≥s jαjβj  wt, u \u000bj−1 wt, u \u000b ≤ \f\f wt, u \u000b\f\f\f\f wt, u \u000b\f\fs−1 X j≥s j|αjβj| \f\f wt, u \u000b\f\fj−s ≤ ∥ϕ′∥γ∥g′∥γ \f\f wt, u \u000b\f\fs−1\f\f wt, u \u000b\f\f ≤ ∥ϕ′∥γ∥g′∥γ(1 + κ)(s−1)/2\f\f wt, u \u000b\f\fs . Similarly, − κ 1 + κ X j≥s (j − 1)αjβj  wt, u \u000bj+1 wt, u \u000b ≤ ∥ϕ′∥γ∥g′∥γ(1 + κ)(s+1)/2\f\f wt, u \u000b\f\fs+2 . Hence, for κ ≥ 1, F(wt) ≤ ∥ϕ′∥γ∥g′∥γ(1 + κ)(s−1)/2\f\f wt, u \u000b\f\fs\u0010 1 + (1 +κ)  wt, u \u000b2\u0011 − ⟨wt, u⟩2 4 . Suppose κ < d/C2 − 1, then F(wt) ≤  wt, u \u000b2\u0010 2∥ϕ′∥γ∥g′∥γ(1 + κ)(s−1)/2\f\f wt, u \u000b\f\fs−2 − 1/4 \u0011 ≤  wt, u \u000b2   2∥ϕ′∥γ∥g′∥γCs−2 r (1 + κ)s−1 ds−2 − 1/4 ! . Thus, for any κ such that 1 ≤ κ ≤    d s−2 s−1 (8Cs−2∥ϕ′∥γ∥g′∥γ) 2 s−1 ∧ d C2    − 1 and any wt such that |⟨wt, u⟩| ≤C/ √ d, we have d⟨wt,u⟩ 2 dt ≤ 0, hence supt≥0|⟨wt, u⟩| ≤C/ √ d, as long as the above holds true at initialization. Finally, we will show \f\f w0, u \u000b\f\f ≤ C/ √ d with probability at least 0 .99 for a suitable choice of constant C. Indeed, this is an elementary concentration of measure result on the unit sphere. For simplicity, we avoid performing sharp probability of failure analysis and only remark that E h w0, u \u000b2i = 1/d, thus by the Markov inequality P \u0010 w0, u \u000b2 ≥ C2/d \u0011 ≤ 1/C2, hence a choice of C ≥ 10 suffices, and the proof is complete. B.2 Extremely Large Spike In this section, we will show that under extremely large spike, the spherical gradient flow (2.5) can potentially recover the true direction. Namely, we will prove the following proposition. Proposition 14. Suppose we initialize the spherical population gradient flow (2.5) from w0. Let ϕ be the ReLU activation and assume ⟨ϕ, g⟩γ := Ez∼N(0,1)[ϕ(z)g(z)] = α >1/2, 19and κ ≥ C ⟨w0,u⟩2 for a sufficiently large constant C >0 depending only on g. Then, the gradient flow on the sphere satisfies ( wT , u \u000b ≥ 1 − ε if  w0, u \u000b > 0 wT , u \u000b ≤ −1 + ε if  w0, u \u000b < 0 (B.3) whenever T ≥ 1 α − 1/2 ln(2/ε). (B.4) Before proceeding to the proof, we notice that if we uniformly initialize w0 over Sd−1, then the typical value for  w0, u \u000b is of order d−1/2, meaning that the above proposition asks for κ = Ω(d). This is a regime where lower bounds for the sample complexity of kernel methods are Ω(1) [ DWY21], thus no meaningful separation in terms of dimension dependency of the sample complexity between neural networks and kernel methods is possible, as the problem becomes effectively one-dimensional. Proof. The cases where  w0, u \u000b > 0 and  w0, u \u000b < 0 are symmetric, thus we only present the proof for the former. Using (B.2), we can write the dynamics on the sphere more explicitly as d⟨wt, u⟩ dt =    E \u0002 ϕ′(  wt, z \u000b )g′(⟨u, z⟩) \u0003 1 + κ⟨wt, u⟩2 | {z } =:B1 + κ⟨wt, u⟩E \u0002 ϕ(  wt, z \u000b )g(⟨u, z⟩) \u0003 √1 + κ q 1 + κ⟨wt, u⟩2 | {z } =:B2 −κ⟨wt, u⟩ 2(1 + κ)    (1 −  wt, u \u000b2 ). Our goal is to study the regime of large κ, therefore we will bound how much B1 and B2 can deviate from their corresponding κ = ∞ values. In particular, we have B1 ≥ −∥ϕ′∥γ∥g′∥γ 1 + κ⟨wt, u⟩2 = −∥g′∥γ/2 1 + κ⟨wt, u⟩2 . Furthermore, assuming ⟨wt, u⟩ > 0 and ⟨ϕ, g⟩γ > 0, let cκ(wt) := κ⟨wt,u⟩ √1+κ √ 1+κ⟨wt,u⟩2 . Then, by the Lipschitz- ness of ϕ, B2 = cκ(wt)⟨ϕ, g⟩γ + cκ(wt) E \u0002\u0000 ϕ(  wt, z \u000b ) − ϕ(⟨u, z⟩) \u0001 g(⟨u, z⟩) \u0003 ≥ cκ(wt)⟨ϕ, g⟩γ − \f\fcκ(wt) \f\f∥g∥γ∥wt − u∥ ≥ cκ(wt)⟨ϕ, g⟩γ − \f\fcκ(wt) \f\f∥g∥γ q 2(1 −  wt, u \u000b2 ) ≥ cκ(wt)⟨ϕ, g⟩γ − √ 2κ∥g∥γ|⟨wt, u⟩| 1 + κ⟨wt, u⟩2 . where we used wt = Σ1/2wt ∥Σ1/2wt∥ in the last step. Suppose ⟨wt, u⟩ > 0 (which holds at least on a neighborhood around initialization, and as we will see below holds for all t >0), then, cκ(wt) ≥ κ⟨wt, u⟩2 1 + κ⟨wt, u⟩2 . As a result, we obtain B1 + B2 − κ⟨wt, u⟩ 2(1 + κ) ≥ ⟨ϕ, g⟩γ   1 − 1 κ⟨wt, u⟩2 ! − ∥g′∥γ/2 + √ 2∥g∥γq κ⟨wt, u⟩2 − 1 2. Consequently, the lower bound of the time derivative of⟨wt, u⟩ becomes larger as ⟨wt, u⟩ increases. Therefore, assuming  w0, u \u000b > 0, we only need to control this lower bound at initialization. Assume κ ≥ 4 ⟨w0, u⟩2(α − 1/2) ( α ∨ (∥g′∥γ + √ 8∥g∥γ)2 α − 1/2 ) . 20From this, we conclude that when ⟨wt, u⟩ > 0 and ⟨ϕ, g⟩γ = α >1/2, we have d⟨wt, u⟩ dt ≥ α − 1/2 2 (1 −  wt, u \u000b2 ), integration yields the desired result. C Proofs of Section 3 We begin by stating the closed-form expression for the population gradient, i.e. the counterpart of Lemma 12 in the normalized setting. Lemma 15. Consider the population risk R(w) defined by (3.1), recall that R(w) = E \" −ϕ   ⟨w, x⟩ ∥Σ1/2w∥ ! g(⟨u, z⟩) # + 1 2∥ϕ∥2 γ + 1 2 E \u0002 y2\u0003 . Then, ∇R(w) = Σ1/2(Id − w w⊤)ζϕ,g(⟨w, u⟩)u ∥Σ1/2w∥ , (C.1) where ζϕ,g(⟨w, u⟩) := −E[ϕ′(⟨w, z⟩)g′(⟨u, z⟩)] = − X j≥s jαjβj⟨w, u⟩j−1. (C.2) Proof. Recall from (3.1) that ∇R(w) = ∇w E \" −ϕ   ⟨w, x⟩ ∥Σ1/2w∥ ! g(⟨u, z⟩) # = − 1 ∥Σ1/2w∥   Id − Σww⊤ ∥Σ1/2w∥2 ! Σ1/2 E[ϕ′(⟨w, z⟩)g(⟨u, z⟩)z] = Σ1/2\u0000 Id − w w⊤\u0001 ∥Σ1/2w∥ {ζϕ,g(⟨w, u⟩)u + ψϕ,g(⟨w, u⟩)w} (by Lemma 11) = Σ1/2(Id − w w⊤)ζϕ,g(⟨w, u⟩)u ∥Σ1/2w∥ , where ψϕ,g(⟨w, u⟩) := −E[ϕ′(⟨w, z⟩)g(⟨u, z⟩)⟨w, z⟩ −ϕ′(⟨w, z⟩)g′(⟨u, z⟩)⟨w, u⟩] (C.3) (the above is only a function of ⟨w, u⟩ due to the Hermite expansion). Given the closed form of the population gradient, the proof of Lemma 3 is immediate by noticing that dwt dt = (Id − wtwt⊤ ) ∥Σ1/2w∥ Σ1/2 dwt dt . (C.4) Next, we move on to prove Proposition 4. C.1 Proof of Proposition 4 From Lemma 3, we have d  wt, u \u000b dt = −ζϕ,g(  wt, u \u000b )∥Σ1/2ut ⊥∥2 ≥ cλmin(Σ)  wt, u \u000bs−1 (1 −  wt, u \u000b2 ), 21where ut ⊥ := u −  wt, u \u000b wt. The above inequality and the fact that  w0, u \u000b > 0 imply that  wt, u \u000b is non-decreasing in time. Let T1 := sup{t >0 :  wt, u \u000b < 1/2}. Then, on t ∈ [0, T1], we have d  wt, u \u000b dt ≥ 3cλmin(Σ) 4  wt, u \u000bs−1 , and integration yields T1 ≤ 0 ∨ 4 3cλmin(Σ)    1/2 −  w0, u \u000b s = 1 ln(1/(2  w0, u \u000b )) s = 2 1 s−2 \u0000 (1/  w0, u \u000b )s−2 − 2s−2\u0001 s >2 . Therefore, T1 ≲ τs(  w0, u \u000b )/λmin(Σ). For t > T1, we have d  wt, u \u000b dt ≥ cλmin(Σ) 2s−1 (1 −  wt, u \u000b2 ). Let T2 = sup \b t >0 :  wt, u \u000b < 1 − ε \t . Once again, integration implies T2 ≤ T1 + 2s−2 cλmin(Σ) ln(2/(3ε)), which completes the proof. C.2 Preliminary Lemmas for proving Theorem 5 We first introduce a number of concentration (and anti-concentration) lemmas that will be useful for proving Theorem 5. Lemma 16. Suppose {z(i)}n i=1 i.i.d. ∼ N(0, Id), and g : R → R satisfies |g(·)| ≤C(1 +|·|p) for some C >0 and p ≥ 1. Additionally, suppose {ϵ(i)}i=1 are i.i.d. σ-sub-Gaussian zero-mean noise independent of {z(i)}n i=1. Let y(i) := g(  u, z(i)\u000b ) + ϵ(i) for some u ∈ Sd−1. Then, for any q >0, with probability at least 1 − 4d−q, we have \f\f\fy(i) \f\f\f ≤ C + C(2 ln(ndq))p/2 + σ p 2 ln(ndq) ≲ ln(ndq)p/2, for all i. Proof. Notice that  u, z(i)\u000b ∼ N(0, 1), thus P \u0010\f\f\f D u, z(i) E\f\f\f ≥ p 2 ln(ndq) \u0011 ≤ 2n−1d−q. Similarly, by the sub-Gaussian and zero-mean property of ϵ(i), P \u0010\f\f\fϵ(i) \f\f\f ≥ σ p 2 ln(ndq) \u0011 ≤ 2n−1d−q. Thus, by a union bound, we have \f\f\f D u, z(i) E\f\f\f ≤ p 2 ln(ndq) and \f\f\fϵ(i) \f\f\f ≤ σ p 2 ln(ndq), for all 1 ≤ i ≤ n, with probability at least 1 − 4d−q. Using the upper bound on |g| finishes the proof. 22Lemma 17. Suppose {z(i)}n i=1 are i.i.d. samples drawn uniformly from Sd−1. Then, P   sup w∈Sd−1 nX i=1 1  \f\f\f D w, z(i) E\f\f\f ≤ 3 √ d 8n ! ≥ 3d \u0010 2 + ln(8n/ √ d) \u0011! ≤ e−d. Proof. Fix some ϵ ∈ (0, 1). Let Nϵ be a minimal ϵ-covering of Sd−1. Let ˆw be the projection of w onto Nϵ. Notice that by the triangle inequality and the union bound P   sup w∈Sd−1 nX i=1 1 \u0010\f\f\f D w, z(i) E\f\f\f ≤ ϵ \u0011 ≥ α ! ≤ P   sup ˆw∈Nϵ nX i=1 1 \u0010\f\f\f D ˆw, z(i) E\f\f\f ≤ 2ϵ \u0011 ≥ α ! ≤ |Nϵ|P  nX i=1 1 \u0010\f\f\f D ˆw, z(i) E\f\f\f ≤ 2ϵ \u0011 ≥ α ! ≤ (3/ϵ)dP  nX i=1 1 \u0010\f\f\f D ˆw, z(i) E\f\f\f ≤ 2ϵ \u0011 ≥ α ! . Moreover, due to [BBSS22, Lemma A.7], E[1(|⟨ ˆw, z⟩| ≤2ϵ)] = P(⟨ ˆw, z⟩ ≤2ϵ) ≤ 8 √ dϵ. Choose ϵ = 3 √ d 8n . By Lemma 25 P  nX i=1 1 \u0010\f\f\f D ˆw, z(i) E\f\f\f ≤ 2ϵ \u0011 ≥ 3d \u0010 2 + ln(8n/ √ d) \u0011! ≤ (3/ϵ)de−d, which completes the proof. We summarize the above statements into a “good event”, as characterized by the following lemma. Lemma 18. Let {z(i)}n i=1 i.i.d. ∼ N(0, Id), and z(i) := z(i)/∥z(i)∥ for every i. We say event G occurs whenever: 1. |y|(i) ≲ ln(ndq)p/2 for all 1 ≤ i ≤ n. 2. supw∈Sd−1 Pn i=1 1 \u0010D w, z(i) E ≲ √ d/n \u0011 ≲ d ln(n/ √ d). 3. λmax \u0010 1 n Pn i=1 z(i)z(i)⊤\u0011 − 1 ≲ p d/n. 4. 1 − λmin \u0010 1 n Pn i=1 z(i)z(i)⊤\u0011 ≲ p d/n. For n ≳ d, event G occurs with probability at least 1 − O(d−q). Proof. The first and second statements of the lemma follow from Lemmas 16 and 17 respectively. The third and fourth statements are standard Gaussian covariance concentration bounds (see e.g. [ Wai19, Theorem 6.1] where both statements hold with probability at least 1 − 2e−d). C.3 Proof of Theorem 5 We begin by recalling the definition w := ˆΣ 1/2 w ∥ˆΣ 1/2 w∥ and the finite-samples dynamics (3.5), which we copy here for the reader’s convenience, dwt dt = η(wt)∇wt ( 1 n nX i=1 ϕ   wt, x(i)\u000b ∥ˆΣ 1/2 wt∥ ! y(i) ) , 23where η(wt) = ∥ˆΣ 1/2 wt∥2. Moreover, via chain rule, we obtain dwt dt = (Id − wtwt⊤ ) ˆΣ 1/2 ∥ˆΣ 1/2 wt∥ dwt dt . (C.5) Let ˜z(i) := ˆΣ −1/2 x(i) = ˆΣ −1/2 Σ1/2z(i). Then, dwt dt = (Id − wtwt⊤ ) ˆΣ(Id − wtwt⊤ ) ( 1 n nX i=1 ϕ′   wt, x(i)\u000b ∥ˆΣ 1/2 wt∥ ! y(i)˜z(i) ) To simplify the notation, define ν(w) := (Id − w w⊤) ˆΣ(Id − w w⊤)u. Then d  wt, u \u000b dt = * ν(wt), 1 n nX i=1 ϕ′ \u0010D wt, ˜z(i) E\u0011 y(i)˜z(i) + . We can decompose the above dynamics into a population term and three different error terms in the following manner: d  wt, u \u000b dt =  ν(wt), Ez,y \u0002 ϕ′\u0000 wt, z \u000b\u0001 yz \u0003\u000b + * ν(wt), 1 n nX i=1 ϕ′ \u0010D wt, z(i) E\u0011 y(i)z(i) − Ez,y \u0002 ϕ′\u0000 wt, z \u000b\u0001 yz \u0003 + | {z } =:E1 + 1 n nX i=1 n ϕ′ \u0010D wt, ˜z(i) E\u0011 − ϕ′ \u0010D wt, z(i) E\u0011o y(i) D z(i), ν(wt) E | {z } =:E2 + 1 n nX i=1 ϕ′ \u0010D w, ˜z(i) E\u0011 y(i) D ˜z(i) − z(i), ν(wt) E | {z } =:E3 . We will proceed in three steps. In the first, we bound E1, the concentration error. In the second, we bound E2 and E3, the errors due to estimating Σ with ˆΣ (i.e. replacing z(i) with ˜z(i)). Finally, we will analyze the convergence time similar to that of Proposition 4. Throughout the proof, we will assume that the event G of Lemma 18 occurs. Step 1. Controlling the concentration error E1. Let K ≍ ln(ndq)p/2, and notice that on event G we have \f\fy(i)\f\f ≲ K for all i. Let yK := y1(|y| ≤K). On the event G, we have y(i) K = y(i) for all i, and E1 =  ν(wt), ∆n \u000b ≥ −∥∆n∥∥ν(wt)∥, where ∆n := 1 n nX i=1 ϕ′ \u0010D w, z(i) E\u0011 yK (i)z(i) − Ez,y[ϕ′(⟨w, z⟩)yz]. Thus, our objective is to bound ∥∆n∥ uniformly for all w ∈ Sd−1. To that end, we first modify the expectation in the above definition so that the empirical average and expected value match in terms of their random variables. Specifically, sup w,v∈Sd−1 ⟨∆n, v⟩ = sup w,v∈Sd−1 1 n nX i=1 ϕ′ \u0010D w, z(i) E\u0011 y(i) K D z(i), v E − Ez,y[ϕ′(⟨w, z⟩)yK⟨z, v⟩] − Ez,y[ϕ′(⟨w, z⟩)y⟨z, v⟩1(|y| > K)]. 24By the Cauchy-Schwartz inequality, |Ez,y[ϕ′(⟨w, z⟩)y⟨z, v⟩1(|y| > K)]| ≤Ez,y h ϕ′(⟨w, z⟩)2y2⟨z, v⟩2 i1/2 E[1(|y| > K)]1/2 ≲ E \u0002 y4\u00031/4 Ez h ⟨z, v⟩4 i1/4 P(|y| > K)1/2 ≲ d−q/2, where the last inequality follows from Lemma 16. Hence, sup w,v∈Sd−1 ⟨∆n, v⟩ ≤ sup w,v∈Sd−1 1 n nX i=1 ϕ′ \u0010D w, z(i) E\u0011 y(i) K D z(i), v E − E[ϕ′(⟨w, z⟩)yK⟨z, v⟩] + O(d−q/2). Next, we need to establish high-probability bounds via a covering argument. To simplify the exposition, define the stochastic process indexed by w ∈ Sd−1 and v ∈ Sd−1 via X(i) w,v := ϕ′ \u0010D w, z(i) E\u0011 y(i) K D z(i), v E . Fix some ϵw, ϵv > 0. Let Θ w and Θv be ϵw and ϵv coverings of Sd−1, and let ˆw and ˆv denote the projection of w onto Θw and of v onto Θv respectively, then sup w,v∈Sd−1 1 n nX i=1 X(i) w,v − E[Xw,v] = sup w,v∈Sd−1 1 n nX i=1 \u0010 X(i) w,v − X(i) w,ˆv \u0011 + 1 n nX i=1 \u0010 X(i) w,ˆv − X(i) ˆw,ˆv \u0011 + Ez,y[Xw,ˆv − Xw,v] + Ez,y[Xˆw,ˆv − Xw,ˆv] + 1 n nX i=1 X(i) ˆw,ˆv − Ez,y[Xˆw,ˆv]. We bound each of the terms using Cauchy-Schwartz. Specifically, 1 n nX i=1 \u0010 X(i) w,v − X(i) w,ˆv \u0011 ≤ vuut1 n nX i=1 ϕ′(  w, z(i)\u000b )2y(i) K 2 vuut1 n nX i=1  z(i), v − ˆv \u000b2 ≲ Kϵv, where we used the upper bound on the operator norm of 1 n Pn i=1 z(i)z(i)⊤ from Lemma 18 together with the fact that n ≳ d. Similarly, Ez,y[Xw,ˆv − Xw,v] ≤ Ez,y \u0002 ϕ′(⟨w, z⟩)2y2 K \u00031/2 Ez h ⟨z, v − ˆv⟩2 i1/2 ≲ Kϵv. To bound the differences when we replace w with ˆw, we need to make a distinction between ReLU and smooth activations as the respective arguments are to some extent different. When ϕ′ is Lipschitz, 1 n nX i=1 \u0010 X(i) w,ˆv − X(i) ˆw,ˆv \u0011 ≤ vuut1 n nX i=1 (y(i) K )2\u0000 ϕ′(  w, z(i)\u000b ) − ϕ′(  ˆw, z(i)\u000b ) \u00012 vuut1 n nX i=1  z(i), ˆv2\u000b ≲ Kϵw, and E[Xˆw,ˆv − Xw,ˆv] ≤ E h y2 K(ϕ′(⟨w, z⟩) − ϕ′(⟨ ˆw, z⟩))2i1/2 E h ⟨z, ˆv⟩2 i1/2 ≲ Kϵw. Therefore, for a smooth activation ϕ we choose ϵv = ϵw = p d/n, and obtain sup w,v∈Sd−1 1 n nX i=1 X(i) w,v − Ez,y[Xw,v] ≤ sup ˆw,ˆv 1 n nX i=1 X(i) ˆw,ˆv − Ez,y[Xˆw,ˆv] + ˜O( p d/n). 25When ϕ is the ReLU activation, we need to show that the sign of the preactivation changes only for a small number of samples when we change the weight w to ˆw. Notice that sign \u0010D w, z(i) E\u0011 ̸= sign \u0010D ˆw, z(i) E\u0011 =⇒ \f\f\f D w, z(i) E\f\f\f ≤ \f\f\f D ˆw − w, z(i) E\f\f\f =⇒ \f\f\f D w, z(i) E\f\f\f ≤ ϵw. Recall that z(i) := z(i)/∥z(i)∥. Choose ϵw ≍ √ d/n. On event G, we know from Lemma 18 that at most O(d ln(n/ √ d)) samples can satisfy the above condition. Therefore, 1 n nX i=1 \u0010 X(i) w,ˆv − X(i) ˆw,ˆv \u0011 ≤ vuut1 n nX i=1 (y(i) K )2\u0000 ϕ′(  w, z(i)\u000b ) − ϕ′(  ˆw, z(i)\u000b ) \u00012 vuut1 n nX i=1  z(i), ˆv \u000b2 ≲ K q d ln(n/ √ d) n . and E[Xˆw,ˆv − Xw,ˆv] ≤ E h y2 K(ϕ′(⟨w, z⟩) − ϕ′(⟨ ˆw, z⟩))2i1/2 E h ⟨z, ˆv⟩2 i1/2 ≤ KP(sign(⟨w, z⟩) ̸= sign(⟨ ˆw, z⟩))1/2 ≤ KP(|⟨w, z⟩| ≤ϵw)1/2 ≤ 2K p d1/2ϵw, where the last inequality follows from the anti-concentration on the sphere [ BBSS22, Lemma A.7]. Thus, for ReLU we choose ϵv ≍ p d/n and ϵw ≍ √ d/n, and once again obtain sup w,v∈Sd−1 1 n nX i=1 X(i) w,v − Ez,y[Xw,v] ≤ sup ˆw,ˆv 1 n nX i=1 X(i) ˆw,ˆv − Ez,y[Xˆw,ˆv] + ˜O( p d/n). It remains to bound the term sup ˆw,ˆv 1 n nX i=1 X(i) ˆw,ˆv − E[Xˆw,ˆv]. Notice that for fixed ˆw, ˆv, Xˆw,ˆv is sub-Gaussian with sub-Gaussian norm O(K). Thus, via the sub-Gaussian maximal inequality [VH16, Lemma 5.2], sup ˆw,ˆv 1 n nX i=1 X(i) ˆw,ˆv − E[Xˆw,ˆv] ≲ p K2d/n ln(1/(ϵwϵv)), with probability at least 1 − e−d. Consequently, we have sup w∈Sd−1 ∥∆n∥ ≤˜O( p d/n + d−q/2), with probability at least 1 −O (d−q). Assuming that n grows at most polynomially in dimension and choosing a sufficiently large q, we have supw∈Sd−1 ∥∆n∥ ≤˜O( p d/n) with probability at least 1 − O(d−q). Finally, by Lemma 23, ∥ν(wt)∥ ≤λmax \u0010 ˆΣ \u0011 ≲ λmax(Σ), (C.6) with probability at least 1 − e−n′/2. Combining the above with the bound on ∥∆n∥, we have E1 ≥ −λmax(Σ) ˜O( p d/n) with probability at least 1 − O(d−q), which concludes the first step of the proof. 26Step 2. Bounding the error due to the estimation of Σ, i.e. E2 and E3. Recall that we are considering the event G, thus y(i) = y(i) K . We can control each of the error terms separately. We begin by E2, where by Cauchy-Schwartz E2 = 1 n nX i=1 n ϕ′ \u0010D wt, ˜z(i) E\u0011 − ϕ′ \u0010D wt, z(i) E\u0011o y(i) K D z(i), ν(wt) E ≥ − vuut1 n nX i=1 n ϕ′\u0000 wt, z(i)\u000b\u0001 − ϕ′ \u0010D wt, ˜z(i) E\u0011o2 vuut1 n nX i=1 y(i) K 2 z(i), ν(wt) \u000b2 ≥ −K∥ν(wt)∥ vuut1 n nX i=1 n ϕ′\u0000 wt, z(i)\u000b\u0001 − ϕ′ \u0010D wt, ˜z(i) E\u0011o2 , where the last line follows from Lemma 18 and the fact that n ≳ d. When ϕ′ is additionally Lipschitz, we have E2 ≳ −K∥ν(wt)∥ vuut1 n nX i=1 D wt, z(i) − ˜z(i) E2 . Moreover, for any w ∈ Sd−1, 1 n nX i=1 D w, z(i) − ˜z(i) E2 ≤ ∥1 n nX i=1 z(i)z(i)⊤ ∥2∥(Id − ˆΣ −1/2 Σ1/2)w∥2 ≤ ∥1 n nX i=1 z(i)z(i)⊤ ∥2∥Id − ˆΣ −1/2 Σ1/2∥2 ≲ d n′ , where the last inequality holds with probability at least 1 − 2e−d on the event of Lemma 24. Hence for smooth activations we conclude E2 ≳ −K∥ν(wt)∥ p d/n′. When ϕ is the ReLU activation, we need a more involved argument to control E2. In particular, we will show that for any w, at most only ˜O(d) datapoints can have sign \u0000 w, z(i)\u000b\u0001 ̸= sign \u0010D w, ˜z(i) E\u0011 . Notice that sign \u0010D w, z(i) E\u0011 ̸= sign \u0010D w, ˜z(i) E\u0011 =⇒ \f\f\f D w, z(i) E\f\f\f ≤ \f\f\f D w, z(i) − ˜z(i) E\f\f\f =⇒ \f\f\f D w, z(i) E\f\f\f ≤ ∥Id − ˆΣ −1/2 Σ1/2∥ (C.7) where z(i) := z(i) ∥z(i)∥ is distributed uniformly over Sd−1. From Lemma 24 we have ∥Id − ˆΣ −1/2 Σ1/2∥ ≲ q d n′ with probability at least 1 − 2e−d. On the other hand, from Lemma 17 we know with probability at least 1 − e−d, for any w ∈ Sd−1 at most ˜O(d) of the labeled samples have \f\f w, z(i)\u000b\f\f ≲ √ d/n. Recall that n′ ≳ n2 when using the ReLU activation. This is precisely why we make this choice for the ReLU activation, as we need to balance the RHS of (C.7) which is of order p d/n′ with the LHS of (C.7) which should at most be of order √ d/n if we want to ensure only ˜O(d) samples satisfy the bound. When n′ = n2 we can balance these two terms, thus with probability at least 1 − 3e−d the sign change can occur for at most ˜O(d) many samples, and 1 n nX i=1 n ϕ′ \u0010D wt, z(i) E\u0011 − ϕ′ \u0010D wt, ˜z(i) E\u0011o2 ≤ ˜O \u0012d n \u0013 . In this case, we end up with E2 ≥ −K∥ν(wt)∥ ˜O( p d/n). 27Bounding E3 for ReLU and Lipschitz ϕ′ is identical. In both cases, by Cauchy-Schwartz, E3 ≥ − vuut1 n nX i=1 ϕ′(  w, z(i)\u000b )2y(i) K 2 vuut1 n nX i=1 D z(i) − ˜z(i), ν(wt) E2 ≳ −K∥1 n nX i=1 z(i)z(i)⊤ ∥∥Id − ˆΣ −1/2 Σ1/2∥∥ν(wt)∥ ≳ −K∥ν(wt)∥ p d/n′, which holds on the intersection of event G and of Lemma 23. At last, using the bound on ∥ν(wt)∥ from (C.6), we obtain E2 ∧ E3 ≥ −λmax(Σ) ˜O( p d/n), with probability at least 1 − O(d−q). Step 3. Analyzing the Convergence. As a result of the previous steps, we have established d  wt, u \u000b dt ≥  ν(wt), E \u0002 ϕ′(  wt, z \u000b )yz \u0003\u000b − λmax(Σ) ˜O( p d/n). Thanks to Lemma 11, we can write E[ϕ′(⟨w, z⟩)yz] = E[ϕ′(⟨w, z⟩)g(⟨u, z⟩)z] = −ζϕ,g(⟨w, u⟩)u − ψϕ,g(  wt, u \u000b )w, where ζϕ,g and ψϕ,g were introduced in (C.2) and (C.3) respectively. Recall the definition of ν(wt), ν(wt) := (Id − wtwt⊤ ) ˆΣ(Id − wtwt⊤ )u. Therefore, d  wt, u \u000b dt ≥ −ζϕ,g \u0000 wt, u \u000b\u0001 u⊤(Id − wtwt⊤ ) ˆΣ(Id − wtwtT )u − λmax(Σ) ˜O( p d/n) ≥ c  wt, u \u000bs−1D ut ⊥, ˆΣut ⊥ E − λmax(Σ) ˜O( p d/n) (By Assumption 2) ≥ cλmin \u0010 ˆΣ \u0011 wt, u \u000bs−1 (1 −  wt, u \u000b2 ) − λmax(Σ) ˜O( p d/n), where ut ⊥ := u −  u, wt\u000b wt. Moreover, from Lemma 23, we have λmin \u0010 ˆΣ \u0011 ≥ λmin(Σ)   1 2 − s tr(Σ) n′λmin(Σ) ! ≥ λmin(Σ)   1 2 − r dκ(Σ) n′ ! , with probability at least 1 − e−n′/8. Hence, for n′ ≳ dκ(Σ) we have λmin \u0010 ˆΣ \u0011 ≳ λmin(Σ), and consequently, d  wt, u \u000b dt ≥ c′λmin(Σ)  wt, u \u000bs−1 (1 −  wt, u \u000b2 ) − λmax(Σ) ˜O( p d/n), where c′ is a universal constant. Notice that the first term in the RHS above denotes the signal, while the second term denotes the noise. We want to ensure the noise remains smaller than the signal throughout the trajectory, which leads to the convergence of wt to u. Notice that the signal term is first increasing, then decreasing for  wt, u \u000b ∈ [0, 1]. Thus, it suffices to ensure the noise is smaller than the signal on the two ends 28of the interval, i.e. at time t = 0 and at time t = T where  wT , u \u000b = 1 − ε. At initialization, this condition leads to n ≳ Cdκ(Σ)2 w0, u \u000b2(1−s) , and at time t = T, leads to n ≳ Cdκ(Σ)2 ε2 , where C hides constant depending only on s and at most polylogarithmic factors of d. Thus, we have established the sample complexity as presented by Theorem 5. It remains to obtain the convergence time. With the above sample complexity, we have d  wt, u \u000b dt ≥ c′′λmin(Σ)  wt, u \u000bs−1 (1 −  wt, u \u000b2 ), where c′′ is a universal constant. The rest of the proof follows by integration and is identical to the proof of Proposition 4 in Appendix C.1. C.4 Proof of Corollary 6 The proof follows immediately from Theorem 5 and the following lemma which describes how  w0, u \u000b behaves under different regimes of r1 and r2. Lemma 19. Suppose Σ follows the (κ, θ)-spiked model, w0 is sampled uniformly from Sd−1, n′ ≳ d, and there exist universal constants C2, C′ 2, C3, C′ 3 > 0 such that C2dr2 ≤ κ ≤ C′ 2dr2 and C3d−r1 ≤ ⟨u, θ⟩ ≤C′ 3d−r1 for r1 ∈ [0, 1/2] and r2 ∈ [0, 1]. Then, conditioned on  w0, u \u000b > 0, with any arbitrarily large constant probability 1 − δ, for sufficiently large d (that depends on δ) we have  w0, u \u000b ≳    d−1/2 0 ≤ r2 < r1 dr2−r1−1/2 r1 < r2 < 2r1 d(r2−1)/2 2r1 < r2 < 1 . (C.8) Proof. By definition,  w0, u \u000b = D ˆΣ 1/2 w0, Σ1/2u E ∥ˆΣ 1/2 w0∥∥Σ1/2u∥ . Recall that we are conditioning our arguments on  w0, u \u000b > 0, hence the numerator of the above fraction is positive. To translate the sample complexities of Theorems 5 and 7 to the spiked model, our goal is to lower bound  w0, u \u000b in terms of d, r1, and r2. We begin by observing that ∥ˆΣ 1/2 w∥ ≤ ∥ˆΣ 1/2 Σ−1/2∥∥Σ1/2w∥ ≲ ∥Σ1/2w∥, where the last inequality holds on the event of Lemma 24, which happens with probability at least 1 − 2e−d. Consequently,  w0, u \u000b ≳ D ˆΣ 1/2 w0, Σ1/2u E ∥Σ1/2w0∥∥Σ1/2u∥ =  w0, Σu \u000b + D w0, ( ˆΣ 1/2 − Σ1/2)Σ1/2u E ∥Σ1/2w0∥∥Σ1/2u∥ . Furthermore, due to the Markov inequality, P \u0012 w0, θ \u000b2 ≥ C1 d \u0013 ≤ 1/C1. 29Similarly (by conditioning on ˆΣ) P  D w0, ( ˆΣ 1/2 − Σ1/2)Σ1/2u E2 ≥ C1∥( ˆΣ 1/2 − Σ1/2)Σ1/2u∥2 d ! ≤ 1/C1. Additionally, on the event of Lemma 24, ∥( ˆΣ 1/2 − Σ1/2)Σ1/2u∥ ≤ ∥ˆΣ 1/2 Σ−1/2 − Id∥∥Σu∥ ≲ p d/n′∥Σu∥. Therefore, on the above events, for some absolute constant C >0  w0, u \u000b ≳  w0, Σu \u000b − C∥Σu∥ p 1/n′ q 1+C′ 2C1 1+κ ∥Σu∥ ≳  w0, u \u000b + κ  w0, θ \u000b ⟨u, θ⟩ −C(1 + κ|⟨u, θ⟩|) p 1/n′ p 1 + C′ 2C1 q 1 + κ⟨u, θ⟩2 . Recall that C2dr2 ≤ κ ≤ C′ 2dr2 and C3d−r1 ≤ ⟨u, θ⟩ ≤C′ 3d−r1 (notice that changing θ to −θ does not change the spiked model of Assumption 1, thus we can assume ⟨u, θ⟩ ≥0 without loss of generality). Then,  w0, u \u000b ≳  w0, u \u000b + κ  w0, θ \u000b ⟨u, θ⟩ −C(1 + κ⟨u, θ⟩) p 1/n′ p 1 + C′ 2C1 q 1 + C′ 2C′ 3 2dr2−2r1 . (C.9) The last term in the numerator can be made arbitrarily small by sufficiently large n′, hence we focus on other terms for now. Intuitively, when r2 < 2r1, the denominator is of constant order. If additionally r2 < r1, the dominant term in the numerator is  w0, u \u000b and  w0, u \u000b ≍ 1/ √ d, otherwise the dominant term is κ  w0, θ \u000b ⟨u, θ⟩ and  w0, u \u000b ≍ dr2−r1−1/2. On the other hand, when r2 > 2r1, the denominator is of order dr2/2−r1 , and once again the dominant term of the numerator is κ  w0, θ \u000b ⟨u, θ⟩, therefore  w0, u \u000b ≍ d(r2−1)/2. Using this intuition, we analyze each of the following regimes separately. Case 1. 0 < r2 < r1: In this case, by [ BBSS22, Lemma A.7] we have \f\f w0, u \u000b\f\f ≥ c/ √ d with probability at least 1 − 4c. On the intersection of all considered events with  w0, u \u000b > 0, and for sufficiently large d and n′ ≳ d, we must have  w0, u \u000b > 0 (otherwise  w0, u \u000b < 0). Thus by plugging the values in (C.9),  w0, u \u000b ≳ c − √C1C′ 2C′ 3dr2−r1 − C(1 + C′ 2C′ 3dr2−r1 ) p d/n′ √ d p 1 + C′ 2C1 q 1 + C′ 2C′ 3 2dr2−2r1 ≳ 1√ d . (C.10) The intersection of all desired events and  w0, u \u000b > 0 happens with probability at least 1 2 −4c −2/C1 −2e−d, thus conditioned on  w0, u \u000b the probability is at least 1 − 8c − 4/C1 − 4e−d. Choosing sufficiently small c, large C1, and respectively large d and n′ ≳ d with sufficiently large absolute constant, we can arbitrarily increase the (constant) probability of success. Thus the analysis of this regime is complete. Case 2. r1 < r2 < 2r1: This time we use the fact that \f\f w0, u \u000b\f\f ≤ p C1/d with probability at least 1 − 1/C1, and \f\f w0, θ \u000b\f\f ≥ c/ √ d with probability at least 1 − 4c. By an argument similar to the previous case, for sufficiently large d and n′ ≳ d,  w0, u \u000b > 0 implies  w0, θ \u000b > 0, hence by (C.9)  w0, u \u000b ≳ −√C1 + cC2C3dr2−r1 − C(1 + C′ 2C′ 3dr2−r1 ) p d/n′ √ d p 1 + C′ 2C1 q 1 + C′ 2C′ 3 2dr2−2r1 ≳ dr2−r1−1/2, (C.11) with probability at least 1 − 8c − 4/C1 − 4e−d when conditioned on  w0, u \u000b > 0. Case 3. 2 r1 < r2 < 1: Once again recall (C.9). To bound the numerator, we repeat the exact same argument as in the previous case, thus  w0, u \u000b ≥ −√C1 + cC2C3dr2−r1 − C(1 + C′ 2C′ 3dr2−1) p d/n′ q (1 + C′ 2C1)C′ 2C′ 3 2d 1+r2−2r1 2 ≳ d(r2−1)/2, (C.12) 30which finishes the proof of the lemma. D Proofs of Section 4 D.1 Proof of Theorem 7 We recall from (C.5) that dwt dt = (Id − wtwt⊤ ) ˆΣ 1/2 ∥Σ1/2w∥ dwt dt . Furthermore, the preconditioned dynamics of wt given by (4.1) reads dwt dt = η(wt) ˆΣ −1/2 (Id − wtwt⊤ ) ∥ˆΣ 1/2 w∥ ( 1 n nX i=1 ϕ′ \u0010D wt, ˜z(i) E\u0011 y(i)˜z(i) ) , where we recall ˜z(i) := ˆΣ −1/2 x(i). Plugging in η(wt) = ∥ˆΣ 1/2 w∥2 yields dwt dt = (Id − wtwt⊤ )2 ( 1 n nX i=1 ϕ′ \u0010D wt, ˜z(i) E\u0011 y(i)˜z(i) ) = (Id − wtwt⊤ ) ( 1 n nX i=1 ϕ′ \u0010D wt, ˜z(i) E\u0011 y(i)˜z(i) ) The rest of the analysis is identical to that of the proof of Theorem 5 in Appendix C.3. Specifically, by defining ut ⊥ := u −  wt, u \u000b wt, we have d  wt, u \u000b dt =  ut ⊥, Ez,y \u0002 ϕ′(  wt, z \u000b )yz \u0003\u000b + * ut ⊥, 1 n nX i=1 ϕ′ \u0010D wt, z(i) E\u0011 y(i)z(i) − Ez,y \u0002 ϕ′\u0000 wt, z \u000b\u0001 yz \u0003 + | {z } =:E1 + 1 n nX i=1 n ϕ′ \u0010D wt, ˜z(i) E\u0011 − ϕ′ \u0010D wt, z(i) E\u0011o y(i) D z(i), ut ⊥ E | {z } =:E2 + 1 n nX i=1 ϕ′ \u0010D w, ˜z(i) E\u0011 y(i) D ˜z(i) − z(i), ut ⊥ E | {z } =:E3 . As long as n ≳ d, n′ = n for the smooth case, and n′ ≳ n2 for the ReLU case, the first two steps of the proof of Theorem 5 in Appendix C.3 implies that E1 ∧ E2 ∧ E3 ≥ −∥ut ⊥∥ ˜O( p d/n) ≥ −˜O( p d/n). Once again, we apply Lemma 11 to obtain E[ϕ′(⟨w, z⟩)yz] = −ζϕ,g(⟨w, u⟩)u − ψϕ,g(  wt, u \u000b )w, 31with ζϕ,g and ψϕ,g given in (C.2) and (C.3) respectively. As a result, d  wt, u \u000b dt ≥ −ζϕ,g(  wt, u \u000b )∥ut ⊥∥2 − ˜O( p d/n) ≥ c  wt, u \u000bs−1 (1 −  wt, u \u000b2 ) − ˜O( p d/n) (By Assumption 2) . We need to ensure the noise term, i.e. the second term on the RHS remains smaller than the signal, i.e. the first term. The signal term attains its minimum at either initialization t = 0 or at the end of the trajectory t = T where  wT , u \u000b = 1 −ε, which imposes the following sufficient conditions on n. Namely, at initialization we require n ≳ Cd  w0, u \u000b2(1−s) , while at t = T we require n ≳ Cd/ε2, where C hides constant that only depend on s and polylogarithmic factors of d. Hence, we obtain d  wt, u \u000b dt ≥ c′ wt, u \u000bs−1 (1 −  wt, u \u000b2 ). for some universal constant c′ > 0. Via integration (similar to the proof of Proposition 4 in Appendix C.1), for T1 := sup{t >0 :  wt, u \u000b < 1/2} we obtain T1 ≲ τs(  w0, u \u000b ), and for T2 := sup{t >0 :  wt, u \u000b < 1 − ε} we obtain T2 − T1 ≲ ln(1/ε), which completes the proof. We conclude by remarking that the proof of Corollary 8 is immediate given Theorem 7 and Lemma 19. D.2 Preliminary Lemmas for Proving Theorem 9 We will adapt the following lemma from [ MHPG+23], which provides a non-parametric approximation of g via random biases. Lemma 20. [MHPG+23, Lemma 22] For any smooth g : R → R and ∆ > 0, let ˜g : R → R be a smooth function such that ˜g(z) = g(z) for |z| ≤∆ and ˜g(−2∆) = ˜g′(−2∆) = 0. Suppose {bj}m j=1 i.i.d. ∼ Unif(−2∆, 2∆), and let ∆∗ := ∆ sup|z|≤2∆|˜g′′(z)|. Then, there exist second layer weights {aj(bj)}m j=1 with ∥a∥ ≲ ∆∗/√m, such that for any fixed z ∈ [−∆, ∆] and any δ >0, with probability at least 1 − δ over the random biases, \f\f\f\f\f\f mX j=1 a(bj)ϕ(z + bj) − g(bj) \f\f\f\f\f\f ≲ ∆∆∗ r ln(1/δ) m , where ϕ is the ReLU activation. We use the above lemma to show the existence of a second layer with ˜O(1/√m) norm with training error of order ˜O(1/m). Lemma 21. For any ε <1, suppose ⟨w, u⟩ ≥1 − ε. Then for any q >0, sufficiently large d, n ≳ d/ε2, with probability at least 1 − O(d−q) over the random biases and the dataset, there exists a second layer a with ∥a∥ ≤˜O(1/√m) described by Lemma 20 such that 1 n nX i=1   mX j=1 ajϕ( D w, ˜z(i) E + bj) − y(i)   2 ≲ E \u0002 ϵ2\u0003 + ˜O(1/m + ε), where ϕ is the ReLU activation. 32Proof. We begin by replacing w and ˜z(i) with u and z(i). Specifically, via Jensen’s inequality, 1 n nX i=1    mX j=1 ajϕ \u0010D w, ˜z(i) E + bj \u0011 − y(i)    2 ≤ 4 n nX i=1    mX j=1 ajϕ \u0010D u, z(i) E + bj \u0011 − g \u0010D u, z(i) E\u0011    2 | {z } =:E1 ≤ 4 n nX i=1 n y(i) − g \u0010D u, z(i) E\u0011o2 | {z } =:E2 ≤ 4 n nX i=1   mX j=1 aj n ϕ \u0010D w, ˜z(i) E + bj \u0011 − ϕ \u0010D w, z(i) E + bj \u0011o   2 | {z } =:E3 ≤ 4 n nX i=1   mX j=1 aj n ϕ \u0010D w, z(i) E + bj \u0011 − ϕ \u0010D u, z(i) E + bj \u0011o   2 | {z } =:E4 . We bound each term separately. For E1, we can invoke Lemma 20 which implies that each term in the sum can be bounded by ˜O(1/m) with probability at least 1 − 1/(ndq), thus by a union bound, with probability at least 1 − d−q over the random biases, E1 ≤ ˜O(1/m). ‌ By sub-Guassianity of ϵ(i) (hence sub-exponentiality of ϵ(i)2 ), for n ≳ d (with a sufficiently large constant) we have E2 ≲ E \u0002 ϵ2\u0003 + p d/n, with probability at least 1 − e−d. For E3, via the Lipschitzness of ReLU and the Cauchy-Schwartz inequality we can write E3 ≤ ˜O(1) n nX i=1 D w, ˜z(i) − z(i) E2 ≤ ˜O(1)∥I − ˆΣ −1/2 Σ1/2∥2 ≤ ˜O(d/n′), where we used the event of Lemma 24 which happens with probability at least 1 − 2e−d, and ˜O(1) represents a constant that depends at most polylogarithmically on d. Finally, we bound the last term. Once again via the Lipschitzness of the ReLU activation and the Cauchy-Schwartz inequality E4 ≤ ˜O(1) n nX i=1 D w − u, z(i) E2 ≤ ˜O(∥w − u∥2) ≤ ˜O(ε), where once again we used the event of Lemma 24. On the intersection of all desired events, we have 1 n nX i=1   mX j=1 ajϕ( D w, ˜z(i) E + bj) − y(i)   ≲ E \u0002 ϵ2\u0003 + ˜O(1/m + p d/n + d/n′ + ε). We conclude the proof by noticing that n′ ≳ n2 and n ≳ dε−2. Additionally, we will use the following standard Lemma on the Rademacher complexity of two-layer neural networks, which in particular is a restatement of [ MHPG+23, Lemma 18] in a way suitable for our analysis. 33Lemma 22. Let F be a class of real-valued functions on (z, y). Given n samples {z(i), y}n i=1, define the empirical Rademacher complexity of F as ˆRn(F) := E(ςi)n i=1 \" sup f∈F 1 n nX i=1 ςif(z(i), y(i)) # , where (ςi) are i.i.d. Rademacher random variables (i.e. ±1 with equal probability). Suppose F is given by F :=    (z, y) 7→   mX j=1 ajϕ(⟨u, z⟩ + bj) − y   2 ∧ C : ∥a∥ ≤ra/√m, |bj| ≤rb, ∀1 ≤ j ≤ m    , for some fixed u ∈ Sd−1. Suppose {z(i)}n i=1 i.i.d. ∼ N(0, Id), and suppose |ϕ′| ≤1. Then, E(z(i),y(i))n i=1 h ˆRn(F) i ≤ 2 √ 2C(1 + rb)ra√n . Proof. See the proof of [MHPG +23, Lemma 18]. D.3 Proof of Theorem 9 Throughout the proof, we will assume ⟨w, u⟩ ≥1 − ε where we recall w := ˆΣ 1/2 w ∥ˆΣ 1/2 w∥ and u := Σ1/2u ∥Σ1/2u∥. From either Theorem 5 or Theorem 7, we can assume ⟨w, u⟩ ≥1 − ε with probability at least 1 − O(d−q) for any fixed q >0. For simplicity, let ˆy(˜z; w) = mX j=1 ajϕ(⟨w, ˜z⟩ + bj), and similarly define ˆy(z; u). We define the following quantities, R(w) := Ez,y h (ˆy(˜z; w) − y)2 i and R(u) := Ez,y h (ˆy(z; u) − y)2 i , (D.1) and similarly define their empirical counterparts, ˆR(w) := 1 n nX i=1 \u0010 ˆy(˜z(i); w) − y(i) \u00112 and ˆR(u) := 1 n nX i=1 \u0010 ˆy(z(i); u) − y(i) \u00112 . (D.2) Notice that ultimately, we are interested in bounding R(w). We break down the proof into three steps. In the first step, we show that R(w) can be upper bounded by R(u). Then, via a generalization bound, we show that the R(u) can be upper bounded by ˆR(u). Finally, we show that ˆR(u) can be upper bounded by the training error, i.e. ˆR(w), and convex optimization of the last layer can attain the near-optimal value of this training error which is bounded by Lemma 21. Step 1. Bounding R(w) via R(u). By Jensen’s inequality, Ez,y h (ˆy(˜z; w) − y)2 i ≤ 3 Ez h (ˆy(˜z; w) − ˆy(z; w))2 i + 3Ez h (ˆy(z; w) − ˆy(z; u))2 i + 3Ez,y h (ˆy(z; u) − y)2 i . 34Suppose ∥a∥ ≤ra/√m. For the first term, by Lipschitzness of ϕ and the Cauchy-Schwartz inequality Ez h (ˆy(˜z; w) − ˆy(z; w))2 i = Ez     mX j=1 aj n ϕ \u0010D w, ˜z(i) E + bj \u0011 − ϕ \u0010D w, z(i) E + bj \u0011o   2  ≤ r2 a Ez h ⟨w, ˜z − z⟩2 i ≤ r2 a∥Id − ˆΣ −1/2 Σ1/2∥2 ≲ r2 ad/n′, where the last inequality holds with probability at least 1 − 2e−d on the event of Lemma 24. For the middle term, via a similar argument, Ez h (ˆy(z; w) − ˆy(z; u))2 i ≤ r2 a Ez h ⟨w − u, z⟩2 i ≤ 2raε. In what follows, we will restrict the analysis to the case where ra = ˜O(1). Therefore, we have Ez,y h (ˆy(˜z; w) − y)2 i ≤ 3 Ez,y h (ˆy(z; u) − y)2 i + ˜O(d/n′ + ε). Step 2. Generalization: Bounding R(u) via ˆR(u). Define the event E := n |⟨u, z⟩| ∨ |ϵ| ≤ p 2 ln(ndq) o . and similarly define E(i) by replacing z and ϵ with z(i) and ϵ(i) respectively. Via the Cauchy-Schwartz and Jensen inequalities Ez,y h (ˆy(z; u) − y)2 i = Ez,y h (ˆy(z; u) − y)21(E) i + Ez,y h (ˆy(z; u) − y)21(EC) i ≤ Ez,y h (ˆy(z; u) − y)21(E) i + √ 8 \u0000 E \u0002 ˆy(z; u)4\u0003 + E \u0002 y4\u0003\u00011/2 P \u0000 EC\u00011/2 Moreover, E \u0002 y4\u0003 ≲ 1, Ez,y \u0002 ˆy(z; u)4\u0003 ≤ ˜O(1), and P \u0000 EC\u0001 ≤ 4/(ndq) (via a standard sub-Gaussian tail bound). Consequently, Ez,y h (ˆy(z; u) − y)2 i ≤ Ez,y h (ˆy(z; u) − y)21(E) i + ˜O(n−1d−q), Let ℓ(z(i), y(i); a, b) :=   mX j=1 ajϕ \u0010D u, z(i) E + bj \u0011 − y(i)   2 1(E). Notice that u is fixed. Then, by a standard symmetrization argument (see e.g. [ VH16, Lemma 7.4]) and Lemma 22 E \" sup ∥a∥≤ra/√m,|bj|≤rb Ez,y[ℓ(z, y; a, b)] − 1 n nX i=1 ℓ(z(i), y(i); a, b) # ≤ 2 E h ˆRn(F) i ≤ ˜O( p 1/n). where ˆRn(F). As the loss is bounded, we can apply McDiarmid’s inequality to turn the above bound in expectation into a bound in probability, in particular sup ∥a∥≤ra/√m,|bj|≤rb Ez,y[ℓ(z, y; a, b)] − 1 n nX i=1 ℓ(z(i), y(i); a, b) ≤ ˜O( p d/n) 35with probability at least 1 − 2e−d. Therefore, we conclude this step by noticing that Ez,y h (ˆy(z; u) − y)2 i ≤ 1 n nX i=1 \u0010 ˆy(z(i); u) − y(i) \u00112 1(E(i)) + ˜O( p d/n + n−1d−q) ≤ 1 n nX i=1 \u0010 ˆy(z(i); u) − y(i) \u00112 + ˜O( p d/n), with probability at least 1 − 2e−d. Step 3. Bounding the training error and finishing the proof. This step is similar to the proof of [MHPG+23, Theorem 4]. For conciseness, define ˆR(a) := 1 n nX i=1 \u0010 ˆy(x(i); W, a, b) − y(i) \u00112 . and ˆRλ(a) := ˆR(a) + λ∥a∥2/2. Our goal is to choose suitable λ such that the minimizer a∗ := arg min a∈Rm ˆR(a) + λ∥a∥2/2, satisfies ∥a∗∥ ≤ra/√m while the value of the above minimization problem which we denote with ˆR∗ λ does not significantly exceed min ∥a∥≤ra/√m ˆR(a). We argue that the suitable choice for λ is λ ≍ m E \u0002 ϵ2\u0003 + mε + 1 r2a = ˜Θ \u0000 m E \u0002 ϵ2\u0003 + mε + 1 \u0001 . (D.3) Let ˆR∗ denote the minimizer of the regularized problem and ˜a := arg min∥a∥≤ra/√m ˆR(a). From Lemma 21, with a proper choice of ra = ˜Θ(1), we have ˆR(˜a) ≲ E \u0002 ϵ2\u0003 + ˜O(1/m + ε). with probability at least 1 − O(d−q) over the biases and the dataset. Note that as a∗ is the minimizer of ˆRλ, we have ˆR(a∗) + λ∥a∗∥2 2 ≤ ˆR(˜a) + λ∥˜a∥2 2 , and in particular λ∥a∗∥2 2 ≤ ˆR(˜a) + λ∥˜a∥2 2 =⇒ ∥a∗∥ ≤˜O(1/√m). and ˆR(a∗) ≤ ˆR(˜a) + λ∥˜a∥2 2 ≲ E[ϵ]2 + ˜O(1/m + ε). Let {at}t≥0 be the solution to the gradient flow of a. Then, d∥at − a∗∥2 dt = −2 D at − a∗, ∇ ˆRλ(at) E , and by the first-order condition of strong convexity D at − a∗, ∇ ˆRλ(at) E ≥ λ∥at − a∗∥2, therefore ∥aT′ − a∗∥2 ≤ e−2λT′ ∥a0 − a∗∥2. 36As the training error (of the regularized problem) is λ-strongly convex in a, by applying the standard Polyak- Lojasiewicz condition, gradient flow for training a obtains ˆRλ(aT′ ) − ˆR∗ λ ≤ \u0010 ˆRλ(a0) − ˆR∗ λ \u0011 e−2λT′ . Furthermore, since ∥a∗∥2 − ∥aT′ ∥2 ≤ 2∥a∗∥∥aT′ − a∗∥ − ∥aT′ − a∗∥2 ≤ 2∥aT′ − a∗∥∥a∗∥, we have ˆR(aT′ ) − ˆR∗ ≤ 2∥a0 − a∗∥∥a∗∥e−λT′ + \u0010 ˆRλ(a0) − ˆR∗ λ \u0011 e−2λT′ . Consequently, choosing T′ ≥ ln \u0010 ∥a0−a∗∥ ∥a∗∥ \u0011 λ ∨ ln \u0010 4∥a0−a∗∥∥a∗∥ ε \u0011 λ ∨ ln \u0010 2( ˆRλ(a0)− ˆR∗ λ) ε \u0011 2λ , (D.4) implies ˆR(aT′ ) ≤ ˆR∗ + ε and ∥aT′ ∥ ≤2∥a∗∥ ≲ ˜O(1/√m). Therefore ˆR(aT′ ) ≲ E \u0002 ϵ2\u0003 + ˜O(1/m + ε). Recall that ˆR(aT′ ) = 1 n nX i=1 \u0010 ˆy(˜z(i); w) − y(i) \u00112 , is the final training error which we also denoted by ˆR(w) earlier in this section when were not focusing on the second layer. From the previous two steps, we know how to bound R(w) via ˆR(u). Thus the last step is to upper bound ˆR(u) via ˆR(w). To that end, via Jensen’s inequality 1 n nX i=1 \u0010 ˆy(z(i); u) − y(i) \u00112 ≤3 n nX i=1 \u0010 ˆy(˜z(i); w) − y(i) \u00112 + 3 n nX i=1 \u0010 ˆy(z(i); w) − ˆy(˜z(i); w) \u00112 + 3 n nX i=1 \u0010 ˆy(z(i); w) − ˆy(z(i); u) \u00112 . The first term on the RHS is ˆR(w) for which we developed a bound earlier in this step. Bounding the latter two terms can be performed similarly to the arguments in the previous sections. In particular, 1 n nX i=1 \u0010 ˆy(z(i); w) − ˆy(˜z(i); w) \u00112 ≤ r2 a∥Id − ˆΣ −1/2 Σ1/2∥2∥1 n nX i=1 z(i)z(i)⊤ ∥2 ≤ ˜O(d/n′), where the last inequality holds with probability at least 1 − 2e−d (over the event of Lemma 24). Similarly, 1 n nX i=1 \u0010 ˆy(z(i); w) − ˆy(z(i); u) \u00112 ≤ ra∥w − u∥2 ≤ ˜O(ε). Putting the bounds back together (recall n′ ≥ n ≳ dε−2), we arrive at ˆR(u) ≲ ˆR(w) + ˜O(ε + d/n′) ≲ ˆR(w) + ˜O(ε). Combining the result of this step with the two previous steps implies R(w) ≲ E \u0002 ϵ2\u0003 + ˜O(1/m + ε), with probability at least 1 − O(d−q) (when conditioned on  w0, u \u000b > 0) which completes the proof of Theorem 9. 37E Auxiliary Lemmas In this section, we recall a number of standard lemmas which we employ in various parts of our proofs. Lemma 23. [Wai19, Theorem 6.1]. Suppose {x(i)}n′ i=1 i.i.d. ∼ N(0, Σ). Let ˆΣ := 1 n′ Pn′ i=1 x(i)x(i)⊤ . Then, for n′ ≥ tr(Σ)/λmax(Σ), λmax \u0010 ˆΣ \u0011 ≤ λmax(Σ)   4 + 5 s tr(Σ) n′λmax(Σ) ! with probability at least 1 − e−n′/2. Furthermore, for n′ ≥ d, λmin \u0010 ˆΣ \u0011 ≥ λmin(Σ)   1 4 − s tr(Σ) nλmin(Σ) ! with probability at least 1 − e−n′/8. Lemma 24. Suppose \b z(i)\tn′ i=1 i.i.d. ∼ N(0, Id), let x(i) := Σ1/2z(i) for some invertible Σ, and define ˆΣ := 1 n′ n′ X i=1 x(i)x(i)⊤ . Then ∥Id − ˆΣ 1/2 Σ−1/2∥ ∨ ∥Id − ˆΣ −1/2 Σ1/2∥ ≲ r d n′ with probability at least 1 − 2e−d. Proof. We have ∥Id − ˆΣ −1/2 Σ1/2∥ = n λmax \u0010 ˆΣ −1/2 Σ1/2 \u0011 − 1 o ∨ n 1 − λmin \u0010 ˆΣ −1/2 Σ1/2 \u0011o = \u001a λmax \u0010 Σ1/2 ˆΣ −1 Σ1/2 \u00111/2 − 1 \u001b ∨ n 1 − λmin \u0010 Σ1/2 ˆΣ −1 Σ1/2 \u0011o = \u001a λmin \u0010 Σ−1/2 ˆΣΣ−1/2 \u0011−1/2 − 1 \u001b ∨ \u001a 1 − λmax \u0010 Σ−1/2 ˆΣΣ−1/2 \u0011−1/2\u001b =    λmin   1 n′ n′ X i=1 z(i)z(i)⊤   −1/2 − 1    ∨    1 − λmax   1 n′ n′ X i=1 z(i)z(i)⊤   −1/2   . Similarly, ∥Id − ˆΣ 1/2 Σ−1/2∥ =    λmax   1 n′ n′ X i=1 z(i)z(i)⊤   1/2 − 1    ∨    1 − λmin   1 n′ n′ X i=1 z(i)z(i)⊤   1/2   Moreover, by [Wai19, Example 6.2], we have with probability at least 1 − 2e−d, λmax   1 n′ n′ X i=1 z(i)z(i)⊤   ≤ 1 + ( √ 2 + 1) r d n′ and λmin   1 n′ n′ X i=1 z(i)z(i)⊤   ≥ 1 − ( √ 2 + 1) r d n′ . Thus, for n′ ≳ d (with a sufficiently large absolute constant), we have ∥Id − ˆΣ 1/2 ˆΣ −1/2 ∥ ∨ ∥Id − ˆΣ −1/2 Σ1/2∥ ≲ r d n′ with probability at least 1 − 2e−d. 38Lemma 25 (Chernoff’s Inequality). Suppose X1, . . . , Xn are i.i.d. Bernoulli random variables, and further assume that E[P i Xi] ≤ µ. Then, for any δ ≥ 1, P  nX i=1 Xi ≥ µ(1 + δ) ! ≤ e−µδ/3. (E.1) Proof. The proof follows from a standard Chernoff bound. From [Ver18, Theorem 2.3.1] P  X i Xi ≥ µ(1 + δ) ! ≤ eµ(δ−(1+δ) ln(1+δ)), (notice that the statement of [ Ver18, Theorem 2.3.1] holds true even when E[P i Xi] = µ is replaced with E[P i Xi] ≤ µ). We conclude by remarking that δ − (1 + δ) ln(1 +δ) ≤ −δ/3 for δ ≥ 1. 39",
      "meta_data": {
        "arxiv_id": "2309.03843v1",
        "authors": [
          "Alireza Mousavi-Hosseini",
          "Denny Wu",
          "Taiji Suzuki",
          "Murat A. Erdogdu"
        ],
        "published_date": "2023-09-07T16:55:50Z",
        "pdf_url": "https://arxiv.org/pdf/2309.03843v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper investigates how a spiked covariance structure in input data affects the sample complexity of gradient-based learning for single index models. It reveals that standard spherical gradient dynamics, which are covariance-agnostic, fail to recover the true direction even when the spike is aligned with the target. The authors demonstrate that an appropriate covariance-aware normalization, resembling batch normalization, resolves this issue. By exploiting the alignment between the spiked input covariance and the target, the paper achieves improved sample complexity compared to the isotropic case. Specifically, under a suitably large spike, sample complexity can be made independent of the information exponent and can outperform lower bounds for rotationally invariant kernel methods. The work identifies a three-stage phase transition for sample complexity based on spike-target alignment (r1) and spike magnitude (r2) and shows that preconditioning the training dynamics with the inverse covariance further improves sample complexity, achieving nearly linear rates in dimension.",
        "methodology": "The research focuses on learning single index models of the form y = g(⟨u, x⟩) + ϵ, where x ∼ N(0, Σ) and Σ is a (κ, θ)-spiked covariance matrix. The learning architecture is a two-layer neural network, primarily analyzed in a simplified single-neuron setting for the first layer, with extensions to multi-neuron networks. Two gradient-based training procedures are studied: 1) Spherical gradient flow (covariance-agnostic), which is shown to fail. 2) Normalized gradient flow (covariance-aware), where weights are normalized by ∥Σ^(1/2)w∥^2, resembling batch normalization. 3) Preconditioned gradient flow using the inverse empirical covariance ˆΣ^(−1). The analysis relies on the information exponent 's' of the link function 'g' (defined via Hermite expansion) as a measure of complexity. The approach involves theoretical analysis of population and empirical gradient dynamics using tools like Stein's lemma, concentration inequalities, and covering arguments to derive sample complexity bounds and alignment guarantees.",
        "experimental_setup": "The paper primarily conducts a theoretical analysis of gradient-based learning. The experimental setup involves a synthetic statistical model where input data `x` follows a Gaussian distribution `N(0, Σ)` with a (κ, θ)-spiked covariance `Σ = (Id+κθθ⊤)/(1+κ)`. The target output `y` is generated from a single index model `y = g(⟨u, x⟩) + ϵ`. The activation function `ϕ` used in the neural network is either ReLU or a general smooth activation. The validation of the contributions is entirely analytical, focusing on deriving sample complexity bounds and conditions for successful alignment of the first-layer weights `w` with the target direction `u`. No real-world datasets or empirical benchmarks are used.",
        "limitations": "The current work focuses on single index models, and extending to multi-index models would provide a more complete understanding. The information exponent and its interplay with structured covariance need further generalization for multi-index scenarios. For odd information exponents, the initial condition `⟨w0, u⟩ > 0` is required, which only holds with 0.5 probability for uniform initialization. Training networks with multiple neurons using more standard initializations (beyond the somewhat unconventional initialization for a single neuron) is identified as a challenging area for future analysis, potentially relaxing Assumption 2. The paper does not explore the limitations of learning single index models under structured input covariance through a Correlational Statistical Query (CSQ) lower bound perspective. For ReLU activation, the empirical dynamics require a more accurate covariance estimator `n' ≳ n^2`, assuming access to additional unlabeled input data points.",
        "future_research_directions": "Future research could extend this work to multi-index models, which would necessitate generalizing the concept of information exponent to more complex measures and establishing incremental learning dynamics. Another promising direction is to investigate the limitations of learning single index models under structured input covariance through the lens of Correlational Statistical Query (CSQ) lower bounds, which would complement the paper's current results. Additionally, exploring scenarios where networks with multiple neurons are trained starting from more standard initializations is an interesting challenge. This could lead to a deeper understanding of neuronal interactions and potentially relax certain assumptions made in the current analysis, such as Assumption 2."
      }
    },
    {
      "title": "Knowledge-Adaptation Priors",
      "abstract": "Humans and animals have a natural ability to quickly adapt to their\nsurroundings, but machine-learning models, when subjected to changes, often\nrequire a complete retraining from scratch. We present Knowledge-adaptation\npriors (K-priors) to reduce the cost of retraining by enabling quick and\naccurate adaptation for a wide-variety of tasks and models. This is made\npossible by a combination of weight and function-space priors to reconstruct\nthe gradients of the past, which recovers and generalizes many existing, but\nseemingly-unrelated, adaptation strategies. Training with simple first-order\ngradient methods can often recover the exact retrained model to an arbitrary\naccuracy by choosing a sufficiently large memory of the past data. Empirical\nresults show that adaptation with K-priors achieves performance similar to full\nretraining, but only requires training on a handful of past examples.",
      "full_text": "Knowledge-Adaptation Priors Mohammad Emtiyaz Khan∗ RIKEN Center for AI Project Tokyo, Japan emtiyaz.khan@riken.jp Siddharth Swaroop∗ University of Cambridge Cambridge, UK ss2163@cam.ac.uk Abstract Humans and animals have a natural ability to quickly adapt to their surroundings, but machine-learning models, when subjected to changes, often require a complete retraining from scratch. We present Knowledge-adaptation priors (K-priors) to reduce the cost of retraining by enabling quick and accurate adaptation for a wide- variety of tasks and models. This is made possible by a combination of weight and function-space priors to reconstruct the gradients of the past, which recovers and generalizes many existing, but seemingly-unrelated, adaptation strategies. Training with simple ﬁrst-order gradient methods can often recover the exact retrained model to an arbitrary accuracy by choosing a sufﬁciently large memory of the past data. Empirical results show that adaptation with K-priors achieves performance similar to full retraining, but only requires training on a handful of past examples. 1 Introduction Machine-Learning (ML) at production often requires constant model updating which can have huge ﬁnancial and environmental costs [19, 44]. The production pipeline is continuously evolving, where new data are regularly pooled and labeled and old data become irrelevant. Regular tuning of hyperparameters is required to handle drifts [19], and sometimes even the model class/architecture may need to change. Due to this, the model is frequently retrained, retested, and redeployed, which can be extremely costly, especially when the data and model sizes are large. The cost can be reduced if, instead of repeated retraining, the system can quickly adapt to incremental changes. Humans and animals can naturally use their prior knowledge to handle a wide-variety of changes in their surroundings, but such quick, wide, and accurate adaptation has been difﬁcult to achieve in ML. In theory, this should be possible within a Bayesian framework where the posterior is used as the prior for the future, but exact Bayes is computationally challenging and the design of generic Bayesian priors has its own challenges [55, 39]. In ML, simpler mechanisms are more popular, for example, in Support Vector Machines (SVMs) for adding/removing data [12, 58, 60], and in deep learning for model compression [24]. Weight-priors are used in online learning [13], and more recently for continual learning [30, 40, 47, 33, 62], but they are not suited for many other tasks, such as model compression. In some settings, they also perform worse, for example, in continual learning when compared to memory-based strategies [ 34, 46, 45, 9]. All these previous works apply to narrow, speciﬁc settings, and designing generic adaptation-mechanisms remains an open challenge. We present Knowledge-adaptation priors (K-priors) for the design of generic adaptation-mechanisms. The general principle of adaptation is to combine the weight and function-space divergences to faithfully reconstruct the gradient of the past. K-priors can handle a wide variety of adaptation tasks (Fig. 1, left) and work for a range of models, such as generalized linear models, deep networks, and their Bayesian extensions. The principle uniﬁes and generalizes many seemingly-unrelated existing works, for example, weight-priors [ 13], knowledge distillation [ 24], SVMs [ 12, 35, 58], * Authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. arXiv:2106.08769v2  [cs.LG]  27 Oct 2021Change Regularizer K-priors Past Model Past Memory Add new dataRemove  old dataChange RegularizerChange architecture Updated Model Adaptation tasks Past  data New  data Past  model Past memory Used in K-priors Add new dataRemove old data Batch  K-prior Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Validation acc (%)Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult,logisticregression (b) USPS,logisticregression (c) USPS,neuralnetwork Add new data Remove old dataChange regularizerChange model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Change architecture (b) USPS Add Data(a)           over all (c) CIFAR-10 Add data Validation acc (%) Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) (d) CIFAR-10 KnowledgeDistillationwb Figure 3: (a) When compared at the Batch solution for the ‘Add Data’ task on USPS, weight priors give incorrect values of h 0 ( f i w ) (shown with black dots, each dot correspond to a data examples). Points on the diagonal means a perfect match which is the case for K-priors (show with red dots). (b) Due to this, weight-priors (green diamonds) perform worse than K-priors (red squares). (c) For the ’Add data’ task on CIFAR-10, K-priors outperform replay (blue circles), but performance can still be improved by using a temperature parameter (dark-red triangles). (d) The same is true for knowledge distillation [ 21 ], and we see that we can reduce memory size while still performing better than the student model. The dip in the middle, we believe, is due to suboptimal hyperparameter tuning. [ 29 ] with MLPs and 10-way classiﬁcation on CIFAR-10 with CifarNet [ 57 ], trained with the Adam319 optimizer [ 26 ]. In Figure 3(c) we show one representative result for the ‘Add data’ task with CIFAR-320 10, where we add a random 10% of CIFAR-10 training data to the other 90% (mean and standard321 deviation over 3 runs). Although vanilla K-priors outperform Replay, there is now a bigger gap322 between K-prior and Batch even with 50% past data stored. However, when we use a temperature323 (similar to knowledge distillation in (14) but with the weight term included), K-priors improves.324 A similar result is shown in Figure 3(d) for knowledge distillation ( \u0000 =0 but with a temperature325 parameter) where we are distill from a CifarNet teacher to a LeNet5-style student (details in Ap-326 pendix D). Here, K-priors with 100% data is equivalent to Knowledge Distillation, but when we327 reduce the memory size using our method, we still outperform Batch (which is trained from scratch328 on all data). There is a dip in performance at memory sizes of 10% past data, which we believe is due329 to the use of a suboptimal ⌧ or \u0000 . More empirical effort is require to tune various hyperparameters to330 get a consistent behavior at all memory sizes. Overall, our initial effort here suggests that K-priors331 can do better than Replay, and have potential to give better results with more hyperparameter tuning.332 6 Discussion333 In this paper, we proposed a class of new priors, called K-prior. We show general principles of334 obtaining accurate adaptation with K-priors which are based on accurate gradient reconstructions.335 These principles have many properties that a good prior is expected to have [ 50 ]. The prior applies336 to a wide-variety of adaptation tasks for a range of models, and helps us to connect many existing,337 seemingly-unrelated adaptation strategies in ML. Based on our adaptation principles, we derived338 practical methods to enable adaptation by tweaking models’ predictions at a few past examples. This339 is analogous to adaptation in humans and animals where past experiences is used for new situations.340 In practice, the amount of required past memory seems sufﬁciently low for many tasks. Overall,341 K-priors provide an intuitive yet practical mechanisms for generic adaptation in ML.342 The ﬁnancial and environmental costs of retraining are a huge concern for ML practitioners, which343 can be reduced with quick adaptations. During this work, we realized how little work has been done344 on this topic. The current pipelines and designs are specialized for an ofﬂine, static setting. Our345 approach here pushes towards a simpler design which will support a more dynamic setting. The346 approach can eventually lead to new systems that learn quickly and ﬂexibly, and also act sensibly347 across a wide range of tasks. This opens a path towards systems that learn incrementally in a continual348 fashion, with the potential to fundamentally change the way ML is used in scientiﬁc and industrial349 applications. We hope that this work will help others to do more work towards this goal in the future.350 We ourselves will continue to push this work in that direction.351 9 (b) USPS Add Data(a)           over all (c) CIFAR-10 Add data Validation acc (%) Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) (d) CIFAR-10 KnowledgeDistillationwb Figure 3: (a) When compared at the Batch solution for the ‘Add Data’ task on USPS, weight priors give incorrect values of h 0 ( f i w ) (shown with black dots, each dot correspond to a data examples). Points on the diagonal means a perfect match which is the case for K-priors (show with red dots). (b) Due to this, weight-priors (green diamonds) perform worse than K-priors (red squares). (c) For the ’Add data’ task on CIFAR-10, K-priors outperform replay (blue circles), but performance can still be improved by using a temperature parameter (dark-red triangles). (d) The same is true for knowledge distillation [ 21 ], and we see that we can reduce memory size while still performing better than the student model. The dip in the middle, we believe, is due to suboptimal hyperparameter tuning. [ 29 ] with MLPs and 10-way classiﬁcation on CIFAR-10 with CifarNet [ 57 ], trained with the Adam319 optimizer [ 26 ]. In Figure 3(c) we show one representative result for the ‘Add data’ task with CIFAR-320 10, where we add a random 10% of CIFAR-10 training data to the other 90% (mean and standard321 deviation over 3 runs). Although vanilla K-priors outperform Replay, there is now a bigger gap322 between K-prior and Batch even with 50% past data stored. However, when we use a temperature323 (similar to knowledge distillation in (14) but with the weight term included), K-priors improves.324 A similar result is shown in Figure 3(d) for knowledge distillation ( \u0000 =0 but with a temperature325 parameter) where we are distill from a CifarNet teacher to a LeNet5-style student (details in Ap-326 pendix D). Here, K-priors with 100% data is equivalent to Knowledge Distillation, but when we327 reduce the memory size using our method, we still outperform Batch (which is trained from scratch328 on all data). There is a dip in performance at memory sizes of 10% past data, which we believe is due329 to the use of a suboptimal ⌧ or \u0000 . More empirical effort is require to tune various hyperparameters to330 get a consistent behavior at all memory sizes. Overall, our initial effort here suggests that K-priors331 can do better than Replay, and have potential to give better results with more hyperparameter tuning.332 6 Discussion333 In this paper, we proposed a class of new priors, called K-prior. We show general principles of334 obtaining accurate adaptation with K-priors which are based on accurate gradient reconstructions.335 These principles have many properties that a good prior is expected to have [ 50 ]. The prior applies336 to a wide-variety of adaptation tasks for a range of models, and helps us to connect many existing,337 seemingly-unrelated adaptation strategies in ML. Based on our adaptation principles, we derived338 practical methods to enable adaptation by tweaking models’ predictions at a few past examples. This339 is analogous to adaptation in humans and animals where past experiences is used for new situations.340 In practice, the amount of required past memory seems sufﬁciently low for many tasks. Overall,341 K-priors provide an intuitive yet practical mechanisms for generic adaptation in ML.342 The ﬁnancial and environmental costs of retraining are a huge concern for ML practitioners, which343 can be reduced with quick adaptations. During this work, we realized how little work has been done344 on this topic. The current pipelines and designs are specialized for an ofﬂine, static setting. Our345 approach here pushes towards a simpler design which will support a more dynamic setting. The346 approach can eventually lead to new systems that learn quickly and ﬂexibly, and also act sensibly347 across a wide range of tasks. This opens a path towards systems that learn incrementally in a continual348 fashion, with the potential to fundamentally change the way ML is used in scientiﬁc and industrial349 applications. We hope that this work will help others to do more work towards this goal in the future.350 We ourselves will continue to push this work in that direction.351 9 Figure 1: Left: K-priors can handle a wide-variety of adaptation tasks by using the past model and a small memory of the past data. Middle: For adaptation, the model needs tweaking at only a handful of past examples (shown with dark purple markers). The adapted model (dashed red line) is very close to the retraining on the full batch (solid black line). Right: Results on binary classiﬁcation on ‘USPS’ digits with neural networks show that K-priors (red square) obtain solutions close to the batch training (gray line) but by using only a fraction (2-5%) of the past data. It also performs better than ‘Replay’ (blue circles) where the same memory is used for replay. See Sec. 5 for more details. online Gaussian Processes [17], and continual learning [ 36, 45, 9]. It leads to natural adaptation- mechanisms where models’ predictions need to be readjusted only at a handful of past experiences (Fig. 1, middle). This is quick and easy to train with ﬁrst-order optimization methods and, by choosing a sufﬁciently large memory, we obtain results similar to retraining-from-scratch (Fig. 1, right). Code is available at https://github.com/team-approx-bayes/kpriors. 2 Adaptation in Machine Learning 2.1 Knowledge-adaptation tasks Our goal is to quickly and accurately adapt an already trained model to incremental changes in its training framework. Throughout we refer to the trained model as the base model. We denote its outputs by fw∗(x) for inputs x ∈RD, and assume that its parameters w∗∈W⊂ RP are obtained by solving the following problem on the data D, w∗= arg min w∈W ¯ℓ(w), where ¯ℓ(w) = ∑ i∈D ℓi(w) + R(w). (1) Here, ℓi(w) denotes the loss function on the i’th data example, andR(w) is a regularizer. A good adaptation method should be able to handle many types of changes. The simplest and most common change is to add/remove data examples, as shown below at the left where the data example j /∈D is added to get w+, and at the right where an example k∈D is removed to get w−, w+ = arg min w∈W ∑ i∈D∪j ℓi(w) + R(w), w−= arg min w∈W ∑ i∈D\\k ℓi(w) + R(w). (2) We refer to these problems as ‘Add/Remove Data’ tasks. Other changes are the ‘Change Regularizer’ (shown on the left) task where the regularizer is replaced by a new one G(w), and the ‘Change Model Class/Architecture’ task (shown on the right) where the model class/architecture is changed leading to a change of the parameter space from Wto Θ, wG= arg min w∈W ∑ i∈D ℓi(w) + G(w), θ∗= arg min θ∈Θ ∑ i∈D ˜ℓi(θ) + ˜R(θ). (3) Here, we assume the loss ˜ℓi(θ) has the same form as ℓi(w) but using the new model ˜fθ(x) for prediction, with θ ∈Θ as the new parameter. θ can be of a different dimension, and the new regularizer can be chosen accordingly. The change in the model class could be a simple change in 2features, for example it is common in linear models to add/remove features, or the change could be similar to model compression or knowledge distillation [24], which may not have a regularizer. Another type of change is to add privileged information, originally proposed by Vapnik and Izmailov [60]. The goal is to include different types of data to improve the performance of the model. This is combined with knowledge distillation by Lopez-Paz et al. [37] and has recently been applied to domain adaptation in deep learning [4, 50, 51]. There could be several other kinds of changes in the training framework, for example, those involving a change in the loss function, or even a combination of the changes discussed above. Our goal is to develop a method that can handle such wide-variety of changes, or ‘adaptation tasks’ as we will call them throughout. Such adaptation can be useful to reduce the cost of model updating, for example in a continuously evolving ML pipeline. Consider k-fold cross-validation, where the model is retrained from scratch for every data-fold and hyperparameter setting. Such retraining can be made cheaper and faster by reusing the model trained in the previous folds and adapting them for new folds and hyperparameters. Model reuse can also be useful during active-learning for dataset curation, where a decision to include a new example can be made by using a quick Add Data adaptation. In summary, model adaptation can reduce the cost by avoiding the need to constantly retrain. 2.2 Challenges of knowledge adaptation Knowledge adaptation in ML has proven to be challenging. Currently there are no methods that can handle many types of adaptation tasks. Most existing works apply narrowly to speciﬁc models and mainly focuses on adaptation to Add/Remove Data only. This includes many early proposals for SVMs [12, 59, 25, 20, 49, 35, 31, 58], recent ones for machine-unlearning [11, 21, 41, 22, 53, 8], and methods that use weight and functional priors, for example, for online learning [13], continual deep learning [30, 40, 47, 33, 62, 34, 46, 7, 57, 45, 9], and Gaussian-Process (GP) models [ 17, 52, 56]. The methodologies of these methods are entirely different from each other, and they do not appear to have any common principles behind their adaptation mechanisms. Our goal is to ﬁll this gap and propose a single method that can handle a wide-variety of tasks for a range of models. We also note that there is no prior work on the Change Regularizer task. Adaptation has been used to boost hyperparameter tuning [61] but only Add/Remove Data adaptation is used. Warm-starts have been employed as well [18], but it is often not sufﬁcient and can even hurt performance [5]. 2.3 Problem setting and notation Throughout, we will use a supervised problem where the loss is speciﬁed by an exponential-family, ℓ(y,h(f)) = −log p(y|f) = −⟨y,f⟩+ A(f), (4) where y ∈Y denotes the scalar observation output, f ∈F is the canonical natural parameter, A(f) is the log-partition function, and h(f) = E(y) = ∇A(f) is the expectation parameter. A typical example is the cross-entropy loss for binary outcomes y∈{0,1}where A(f) = log(1 + ef) and h(f) = σ(f) is the Sigmoid function. It is straightforward to extend our method to a vector observation and model outputs. An extension to other types of learning frameworks is discussed in the next section (see the discussion around Eq. 14). Throughout the paper, we will use a shorthand for the model outputs, where we denote fi w = fw(xi). We will repeatedly make use of the following expression for the derivative of the loss, ∇ℓ(yi,h(fi w)) = ∇fi w [h(fi w) −yi]. (5) 3 Knowledge-Adaptation Priors (K-priors) We present Knowledge-adaptation priors (K-priors) to quickly and accurately adapt a model’s knowledge to a wide variety of changes in its training framework. K-priors, denoted below by K(w; w∗,M), refer to a class of priors that use both weight and function-space regularizers, K(w; w∗,M) = Df (f(w)∥f(w∗)) + τDw(w∥w∗) , (6) where f(w) is a vector of fw(ui), deﬁned at inputs in M= (u1,u2,..., uM). The divergence Df(·∥·) measures the discrepancies in the function space F, while Dw(·∥·) measures the same in the 3weight space W. Throughout, we will use Bregman divergences Bψ(p1,p2) = ψ(p1) −ψ(p2) − ∇ψ(p2)⊤(p1 −p2), speciﬁed using a strictly-convex Bregman function ψ(·). K-priors are deﬁned using the base model w∗, the memory set M, and a trade-off parameter τ >0. We keep τ = 1 unless otherwise speciﬁed. It might also use other parameters required to deﬁne the divergence functions. We will sometimes omit the dependency on parameters and refer to K(w). Our general principle of adaptation is to use K(w) to faithfully reconstruct the gradients of the past training objective. This is possible due to the combination of weight and function-space divergences. Below, we illustrate this point for supervised learning for Generalized Linear Models (GLMs). 3.1 K-priors for GLMs GLMs include models such as logistic and Poisson regression, and have a linear model fi w = φ⊤ i w, with feature vectors φi = φ(xi). The base model is obtained as follows, w∗= arg min w∈W ∑ i∈D ℓ(yi,h(fi w)) + R(w). (7) In what follows, for simplicity, we use an L2 regularizer R(w) = 1 2 δ∥w∥2, with δ >0. We will now discuss a K-prior thatexactly recovers the gradients of this objective. For this, we choose Dw(·∥·) to be the Bregman divergence with R(w) as the Bregman function, Dw(w∥w∗) = BR(w∥w∗) = 1 2 δ∥w −w∗∥2. We set memory M= X, where Xis the set of all inputs from D. We regularize each example using separate divergences whose Bregman function is equal to the log-partition A(f) (deﬁned in Eq. 4), Df (f(w)∥f(w∗)) = ∑ i∈X BA(fi w∥fi w∗) = ∑ i∈X ℓ ( h(fi w∗),h(fi w) ) + constant. Smaller memories are discussed later in this section. Setting τ = 1, we get the following K-prior, K(w; w∗,X) = ∑ i∈X ℓ ( h(fi w∗),h(fi w) ) + 1 2 δ∥w −w∗∥2, (8) which has a similar form to Eq. 7, but the outputs yi are now replaced by the predictions h(fi w∗), and the base model w∗serves as the mean of a Gaussian weight prior. We can now show that the gradient of the above K-prior is equal to that of the objective used in Eq. 7, ∇K(w; w∗,X) = ∑ i∈X φi [ h(fi w) −h(fi w∗) ] + δ(w −w∗), (9) = ∑ i∈D φi [ h(fi w) −yi ] + δw    =∇¯ℓ(w). − \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018∑ i∈D φi [ h(fi w∗) −yi ] −δw∗    =0. , (10) where the ﬁrst line is obtained by using Eq. 5 and noting that ∇fi w = φi, and the second line is obtained by adding and subtracting outputs yi in the ﬁrst term. The second term there is equal to 0 because w∗is a minimizer and therefore ∇¯ℓ(w∗) = 0. In this case, the K-prior with M= Xexactly recovers the gradient of the past training objective. Why are we able to recover exact gradients? This is because the structure of the K-prior closely follows the structure of Eq. 7: the gradient of each term in Eq. 7 is recovered by a corresponding divergence in the K-prior. The gradient recovery is due to the property that the gradient of a Bregman divergence is the difference between the dual parameters ∇ψ(p): ∇p1 B(p1,p2) = ∇ψ(p1) −∇ψ(p2). This leads to Eq. 9. For the function-space divergence term, h(fi w) −h(fi w∗) are the differences in the (dual) expectation parameters. For the weight-space divergence term, we note that the dual space is equal to the original parameter space Wfor the L2 regularizer, leading to w −w∗. Lastly, we ﬁnd that terms cancel out by using the optimality of w∗, giving us the exact gradients. 43.2 K-priors with limited memory In practice, setting M= Xmight be as slow as full retraining, but for incremental changes, we may not need all of them (see Fig. 1, for example). Then, which inputs should we include? The answer lies in the gradient-reconstruction error e, shown below for M⊂X , e(w; M) = ∇¯ℓ(w) −∇K(w; w∗,M) = ∑ i∈X\\M φi [ h(fi w) −h(fi w∗) ] . (11) The error depends on the “leftover”φi for i∈X\\M, and their discrepancies h(fi w) −h(fi w∗). A simple idea could be to include the inputs where predictions disagree the most, but this is not feasible because the candidates w are not known beforehand. The following approximation is more practical, e(w; M) ≈G∗(X\\M)(w −w∗), where G∗(X\\M) = ∑ i∈X\\M φih′(fi w∗)φ⊤ i . (12) This is obtained by using the Taylor approximation h(fi w) −h(fi w∗) ≈h′(fi w∗)(∇fi w∗)⊤(w −w∗) in Eq. 11 (h′(fi) is the derivative). The approximation is conveniently expressed in terms of the Generalized Gauss-Newton (GGN) matrix [ 38], denoted by G∗(·). The approximation suggests that Mshould be chosen to keep the leftover GGN matrix G∗(X\\M) orthogonal to w −w∗. Since w changes during training, a reasonable approximation is to choose examples that keep the top-eigenvalues of the GGN matrix. This can be done by forming a low-rank approximation by using sketching methods, such as the leverage score [16, 1, 15, 10]. A cheaper alternative is to choose the examples with highest h′(fi w∗). The quantity is cheap to compute in general, for example, for deep networks it is obtained with just a forward pass. Such a set has been referred to as the ‘memorable past’ by Pan et al.[45], who found it to work well for classiﬁcation. Due to its simplicity, we will use this method in our experiments, and leave the application of sketching methods as future work. K-priors with limited memory can achieve low reconstruction error. This is due to an important feature of K-priors: they do not restrict inputs ui to lie within the training set X. The inputs can be arbitrary locations in the input space. This still works because a ground-truth label is not needed for ui, and only model predictions fw(ui) are used in K-priors. As long as the chosen ui represent X well, we can achieve a low gradient-reconstruction error, and sometimes even perfect reconstruction. Theoretical results regarding this point are discussed in App. A, where we present the optimal K-prior which can theoretically achieve perfect reconstruction error by using singular vectors of Φ⊤= [φ(x1),φ(x2),..., φ(xN)]. When only top-M singular vectors are chosen, the error grows according to the leftover singular values. The optimal K-prior is difﬁcult to realize in practice, but the result shows that it is theoretically possible to achieve low error with limited memory. 3.3 Adaptation using K-priors We now discuss how the K-prior of Eq. 8 can be used for the Add/Remove Data tasks. Other adaptation tasks and corresponding K-priors are discussed in App. B. Because K-priors can reconstruct the gradient of ¯ℓ(w), we can use them to adapt instead of retraining from scratch. For example, to add/remove data from the GLM solution in Eq. 7, we can use the following K-prior regularized objectives, ˆw+ = arg min w∈W ℓj(w) + K(w; w∗,M), ˆw−= arg min w∈W −ℓk(w) + K(w; w∗,M). (13) Using Eq. 10, it is easy to show that this recovers the exact solution when all the past data is used. App. B details analogous results for the Change Regularizer and Change Model Class tasks. Theorem 1. For M= X, we have w+ = ˆw+ and w−= ˆw−. For limited memory, we expect the solutions to be close when memory is large enough. This is because the error in the gradient is given by Eq. 11. The error can be reduced by choosing better M and/or by increasing its size to ultimately get perfect recovery. We stress that Eq. 13 is fundamentally different from replay methods that optimize an objective similar to Eq. 2 but use a small memory of past examples [ 48]. Unlike such methods, we use the predictions h(fi w∗), which we can think of as soft labels, with potentially more information than the true one-hot encoded labels yi. Given a ﬁxed memory budget, we expect K-prior regularization to perform better than such replay methods, and we observe this empirically in Sec. 5. 53.4 K-priors for Generic Learning Problems The main principle behind the design of K-priors is to construct it such that the gradients can faithfully be reconstructed. As discussed earlier, this is often possible by exploiting the structure of the learning problem. For example, to replace an old objective such as Eq. 1, with loss ℓold i (f) and regularizer Rold(w), with a new objective with loss ℓnew i (f) and regularizer Rnew(w), the divergences should be chosen such that they have the following gradients, ∇Dw(w∥w∗) = ∇Rnew(w) −∇Rold(w), ∇Df(f(w)∥f(w∗)) = ∇f(w)⊤B dm (14) where dm is an M-length vector with the discrepancy ∇ℓnew i (fi w) −∇ℓold i (fi w∗) as the i’th entry. The matrix B is added to counter the mismatch between Dand M. Similar constructions can be used for other learning objectives. For non-differentiable functions, a Bayesian version can be used with the Kullback-Leibler (KL) divergence (we discuss an example in the next section). We can use exponential-family distributions which implies a Bregman divergence through KL [ 6]. Since the gradient of such divergences is equal to the difference in the dual-parameters, the general principle is to use divergences with an appropriate dual space to swap the old information with new information. 4 K-priors: Extensions and Connections The general principle of adaptation used in K-priors connects many existing works in speciﬁc settings. We will now discuss these connections to show that K-priors provide a unifying and generalizing principle for these seemingly unrelated methods in ﬁelds such as online learning, deep learning, SVMs, Bayesian Learning, Gaussian Processes, and continual learning. 4.1 Weight-Priors Quadratic or Gaussian weight-priors [13, 30, 54] be seen as as specialized cases of K-priors, where restrictive approximations are used. For example, the following quadratic regularizer, Rquad(w; w∗) = (w −w∗)⊤[G∗(X) + δI] (w −w∗), which is often used in used in online and continual learning [ 13, 30], can be seen as a ﬁrst-order approximation of the K-prior in Eq. 8. This follows by approximating the K-prior gradient in Eq. 8 by using the Taylor approximation used in Eq. 12, to get ∇K(w; w∗,X) ≈ ∑ i∈X φi [ h′(φ⊤ i w∗)φ⊤ i (w −w∗) ] + δ(w −w∗) = ∇Rquad(w; w∗). K-priors can be more accurate than weight priors but may require larger storage for the memory points. However, we expect the memory requirements to grow according to the rank of the feature matrix (see App. A) which may still be manageable. If not, we can apply sketching methods. 4.2 K-priors for Deep Learning and Connections to Knowledge Distillation We now discuss the application to deep learning. It is clear that the functional term in K-priors is similar to Knowledge distillation (KD) [24], which is a popular approach for model compression in classiﬁcation problems using a softmax function (the following expression is from Lopez-Paz et al. [37], also see App. E), ℓKD(w) = λ ∑ i∈D ℓ ( yi,h(fi w) ) + (1 −λ) ∑ i∈D ℓ ( h(fi w∗/T), h(fi w) ) , (15) The base model predictions are often scaled with a temperature parameter T >0, and λ∈[0,1]. KD can be seen as a special case of K-priors without the weight-space term (τ = 0). K-priors extend KD in three ways, by (i) adding the weight-space term, (ii) allowing general link functions or divergence functions, and (iii) using a potentially small number of examples in Minstead of the whole dataset. With these extensions, K-priors can handle adaptation tasks other than compression. Due to their similarity, it is also possible to borrow tricks used in KD to improve the performance of K-priors. KD often yields solutions that are better than retraining from scratch. Theoretically the reasons behind this are not understood well, but we can view KD as a mechanism to reconstruct the past 6gradients, similarly to K-priors. As we now show, this gives a possible explanation behind KD’s success. Unlike GLMs, K-priors for deep learning do not recover the exact gradient of the past training objective, and there is an additional left-over term (a derivation is in App. C), ∇K(w) = ∑ i∈D ∇fi w [ h(fi w) −yi ] + δw    =∇¯ℓ(w) − (∑ i∈D ∇fi wri w∗+ δw∗ )    Additional term since ∇fiw̸=∇fiw∗ , (16) where ri w∗ := h(fi w∗) −yi is the residual of the base model. It turns out that the gradient of the KD objective in Eq. 15 has this exact same form when δ= 0, T = 1 (derivation in App. C), ∇ℓKD(w) = ∑ i∈D ∇fi w [ h(fi w) −yi ] −(1 −λ) ∑ i∈D ∇fi wri w∗. The additional term adds large gradients to push away from the high residual examples (the examples the teacher did not ﬁt well). This is similar to Similarity-Control for SVMs from Vapnik and Izmailov [60], where “slack”-variables are used in a dual formulation to improve the student, who could now be solving a simpler separable classiﬁcation problem. The residuals ri w∗ above play a similar role as the slack variables, but they do not require a dual formulation. Instead, they arise due to the K-prior regularization in a primal formulation. In this sense, K-priors can be seen as an easy-to-implement scheme for Similarity Control, that could potentially be useful for student-teacher learning. Lopez-Paz et al. [37] use this idea further to generalize distillation and interpret residuals from the teachers as corrections for the student (see Eq. 6 in their paper). In general, it is desirable to trust the knowledge of the base model and use it to improve the adapted model. These previous ideas are now uniﬁed in K-priors: we can provide the information about the decision boundary to the student in a more accessible form than the original data (with true labels) could. 4.3 Adding/removing data for SVMs K-prior regularized training yields equivalent solutions to the adaptation strategies used in SVM to add/remove data examples. K-priors can be shown to be equivalent to the primal formulation of such strategies [35]. The key trick to show the equivalence is to use the representer theorem which we will now illustrate for the ‘Add Data’ task in Eq. 13. LetΦ+ be the (N + 1) ×P feature matrix obtained on the dataset D∪j, then by the representer theorem we know that there exists a β ∈RN+1 such that w+ = Φ⊤ +β. Taking the gradient of Eq. 13, and multiplying by Φ+, we can write the optimality condition as, 0 = Φ⊤ +∇[ℓj(w+) + K(w+)] = ∑ i∈D∪j ( ∇fℓ(yi,h(f))|f=β⊤ i ki,+ ) ki,+ + δK+β, (17) where K+ = Φ+Φ⊤ + and its i’th column is denoted byki,+. This is exactly the gradient of the primal objective in the function-space deﬁned over the full batch D∪j; see Equation 3.6 in Chapelle [14]. The primal strategy is equivalent to the more common dual formulations [12, 59, 25, 20, 49, 31, 58]. The function-space formulations could be computationally expensive, but speed-ups can be obtained by using support vectors. This is similar to the idea of using limited memory in K-priors in Sec. 3. 4.4 K-priors for Bayesian Learning and Connections to GPs K-priors can be seamlessly used for adaptation within a Bayesian learning framework. Consider a Gaussian approximation q∗(w) trained on a variational counterpart of Eq. 1 with prior p(w) ∝ exp [−R(w)], and its adapted version where we add data, as shown below ( DKL[·∥·] is the KL divergence), q∗(w) = arg min q∈Q ∑ i∈D Eq[¯ℓi(w)] + DKL[p∥q], q+(w) = arg min q∈Q ∑ i∈D∪j Eq[ℓi(w)] + DKL[p∥q]. Assuming the same setup as Sec. 3, we can recover q+(w) by using qK(w) ∝exp [−K(w)] where we use the K-prior deﬁned in Eq. 8 (note that normalization constant of qKis not required), ˆq+(w) = arg min q∈Q Eq[ℓj(w)] + DKL[q∥qK], (18) 7This follows using Eq. 10. Details are in App. D. In fact, when this Bayesian extension is written in the function-space similarly to Eq. 17, it is related to the online updates used in GPs [17]. When qKis built with limited memory, as described in Sec. 3, the application is similar to sparse variational GPs, but now data examples are used as inducing inputs. These connections are discussed in more detail in App. D. Our K-prior formulations operates in the weight-space and can be easily trained with ﬁrst-order methods, however an equivalent formulation in the function space can also be employed, as is clear from these connections. The above extensions can be extended to handle arbitrary exponential- family approximations by appropriately deﬁning K-priors using KL divergences. We omit these details since this topic is more suitable for a Bayesian version of this paper. 4.5 Memory-Based Methods for Deep Continual Learning K-priors is closely related to recent functional regularization approaches proposed for deep continual learning [34, 46, 7, 54, 57, 45, 9]. The recent FROMP approach of Pan et al. [45] is closest to ours where the form of the functional divergence used is similar to our suggestion in Eq. 14. Speciﬁcally, comparing with Eq. 14, their functional divergence correspond to the vector dm with the i’th entry as h(fw(ui)) −h(fw∗(ui)) for ui ∈M, and the matrix B is (can be seen as Nystrom approximation), B = Λ(w) [ Λ(w∗)∇f(w∗)G(w∗)−1∇f(w∗)⊤Λ(w∗) ]−1 , where Λ(w) is a diagonal matrix with h′(fw(ui)) as the i’th diagonal entry, and G(w∗) = ∇f(w∗)⊤Λ(w∗)∇f(w∗) + δI is the GGN matrix. They also propose to use ‘memorable past’ examples obtained by sorting h′(fi w∗), which is consistent with our theory (see Eq. 11). Based on our work, we can interpret the approach of Pan et al. [45] as a mechanism to reconstruct the gradient of the past, which gives very good performance in practice. Another related approach is the gradient episodic memory (GEM) [36], where the goal is to ensure that the ∑ i∈M[ℓ(yi,fi w∗) −ℓ(yi,fi w)] <0. This is similar in spirit to the student-teacher transfer of Vapnik and Izmailov[60] where the loss of the student is regularized using the model output of the teacher (see Eq. 7 in Vapnik and Izmailov [60] for an example). Lopez-Paz and Ranzato [36] relax the optimization problem to write it in terms of the gradients, which is similar to K-priors, except that K-priors use a ﬁrst-order optimization method, which is simpler than the dual approach used in Lopez-Paz and Ranzato [36]. Most of these approaches do not employ a weight-space divergence, and sometimes even the function- space divergence is replaced by the Euclidean one [ 7, 9]. Often, the input locations are sampled randomly, or using a simple replay method [9] which could be suboptimal. Some approaches propose computationally-expensive methods for choosing examples to store in memory [2, 3], and some can be seen as related to choosing points with high leverage [3]. The approach in Titsias et al. [57] uses inducing inputs which is closely connected to the online GP update. The method we proposed does not contradict with these, but gives a more direct way to choose the points where the gradient errors are taken into consideration. 5 Experimental Results We compare the performance of K-priors to retraining with full-batch data (‘Batch’) and a retraining with replay from a small memory (‘Replay’), and useτ = 1. For fair comparisons, we use the same memory for Replay and K-priors obtained by choosing points with highest h′(fi w∗) (see Sec. 3). Memory chosen randomly often gives much worse results and we omit these results. Replay uses the true label while K-priors use model-predictions. We compare these three methods on the four adaptation tasks: ‘Add Data’, ‘Remove Data’, ‘Change Regularizer’, and ‘Change Architecture’. For the ‘Add Data’ task, we also compare to Weight-Priors with GGN. Our overall ﬁnding is that, for GLMs and deep learning on small problems, K-priors can achieve the same performance as Batch, but with a small fraction of data (often 2-10% of the full data (Fig. 1, right, and Fig. 2). Replay does much worse for small memory size, which clearly shows the advantage of using the model predictions (instead of true labels) in K-priors. Weight priors generally perform well, but they can do badly when the adaptation involves a drastic change for examples inM(see Fig. 3). Finally, for large deep-learning problems, results are promising but more investigations are required with extensive hyperparameter tuning. 8Validation acc (%)Validation acc (%) Memory size (% of past data) (a) Adult, logistic regression (b) USPS, logistic regression Add new data Remove old data Change regularizer Change model class Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Figure 2: K-priors (red squares) match Batch (grey) while mostly using 2-5% of the data (for only 3 tasks a larger fraction is required). K-priors always outperforms Replay which uses the true labels. K-priors replace the labels by the model predictions (see the discussion after Theorem 1). Several additional experiments are in App. E. In App. E.4, we study the effect of randomly initializa- tion and ﬁnd that it performs similarly to a warm start at w∗. In App. E.5, we ﬁnd that K-priors with limited memory are take much less time to reach a speciﬁed accuracy than both batch and replay. The low cost is due to a small memory size. Replay also uses small memory but performs poorly. Logistic Regression on the ‘UCI Adult’ dataset.This is a binary classiﬁcation problem consisting of 16,100 examples to predict income of individuals. We randomly sample 10% of the training data (1610 examples), and report mean and standard deviation over 10 such splits. For training, we use the L-BFGS optimizer for logistic regression with polynomial basis. Results are summarized in Fig. 2(a). For the ‘Add Data’ task, the base model uses 9% of the data and we add 1% new data. For ‘Remove Data’, we remove 100 data examples (6% of the training set) picked by sorting h′(fi w∗)). For the ‘Change Regularizer’ task, we change theL2 regularizer from δ= 50 to 5, and for ‘Change Model Class’, we reduce the polynomial degree from2 to 1. K-priors perform very well on the ﬁrst three tasks, remaining very close to Batch, even when the memory sizes are down to 2%. ‘Changing Model Class’ is slightly challenging, but K-priors still signiﬁcantly out-perform Replay. Logistic Regression on the ‘USPS odd vs even’ dataset.The USPS dataset consists of 10 classes (one for each digit), and has 7,291 training images of size 16 ×16. We split the digits into two classes: odd and even digits. Results are in Fig. 2(b). For the ‘Add Data’ task, we add all examples for the digit 9 to the rest of the dataset, and for ‘Remove Data’ we remove the digit 8 from the whole dataset. By adding/removing an entire digit, we enforce an inhomogeneous data split, making the tasks more challenging. The ‘Change Regularizer’ and ‘Change Model Class’ tasks are the same as the Adult dataset. K-priors perform very well on the ‘Add Data’ and ‘Change Regularizer’ tasks, always achieving close to Batch performance. For ‘Remove Data’, which is a challenging task due to inhomogenity, K-priors still only need to store 5% of past data to maintain close to 90% accuracy, whereas Replay requires 10% of the past data. Neural Networks on the ‘USPS odd vs even’ dataset.This is a repeat of the previous experiment but with a neural network (a 1-hidden-layer MLP with 100 units). Results are in Fig. 1 (right). The ‘Change Regularizer’ task now changesδ= 5 to 10, and the ‘Change Architecture’ task compresses the architecture from a 2-hidden-layer MLP (100 units per layer) to a 1-hidden-layer MLP with 100 units. We see that even with neural networks, K-priors perform very well, similarly out-performing Replay and remaining close to the Batch solution at small memory sizes. Weight-priors vs K-priors.As discussed in the main text, weight-priors can be seen as an approxi- mation of K-priors where h′(fi w) are replaced by ‘stale’h′(fi w∗), evaluated at the oldw∗. In Fig. 3(a), we visualize these ‘stale’h′(fi w∗) and compare them to K-priors which obtains values close to the ones found by Batch. Essentially, for the points at the diagonal the match is perfect, and we see that it is the case for K-priors but not for the weight-priors. We use logistic regression on the USPS data (the ‘Add Data’ task). This inhomogeneous data split is difﬁcult for weight-priors, and we show in Fig. 3(b) that weight-priors do not perform well. For homogeneous data splits, weight-priors do 9(b) USPS Add Data(a)           over all (c) CIFAR-10 Add data Validation acc (%) Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) (d) CIFAR-10 Knowledge Distillation Figure 3: (a) When compared at the Batch solution for the ‘Add Data’ task on USPS, weight priors give incorrect values of h′(fi w) (shown with black dots, each dot correspond to a data examples). Points on the diagonal means a perfect match which is the case for K-priors (show with red dots). (b) Due to this, weight-priors (green diamonds) perform worse than K-priors (red squares). (c) For the ’Add data’ task on CIFAR-10, K-priors outperform replay (blue circles), but performance can still be improved by using a temperature parameter (dark-red triangles). (d) The same is true for knowledge distillation [24], and we see that we can reduce memory size while still performing better than the student model. better than this, but this result goes to show that they do not have any mechanisms to ﬁx the mistakes made in the past. In K-priors, we can always change Mto improve the performance. We provide more plots and results in App. E, including results for weight-priors on all the ‘Add data’ tasks we considered in this paper. MNIST and CIFAR-10, neural networks.Finally, we discuss results on larger problems in deep learning. We show many adaptation tasks in App. E for 10-way classiﬁcation on MNIST [32] with MLPs and 10-way classiﬁcation on CIFAR-10 with CifarNet [62], trained with the Adam optimizer [29]. In Fig. 3(c) we show one representative result for the ‘Add data’ task with CIFAR-10, where we add a random 10% of CIFAR-10 training data to the other 90% (mean and standard deviation over 3 runs). Although vanilla K-priors outperform Replay, there is now a bigger gap between K-prior and Batch even with 50% past data stored. However, when we use a temperature (similar to knowledge distillation in (15) but with the weight term included), K-priors improves. A similar result is shown in Fig. 3(d) for knowledge distillation ( δ = 0 but with a temperature parameter) where we are distill from a CifarNet teacher to a LeNet5-style student (details in App. E). Here, K-priors with 100% data is equivalent to Knowledge Distillation, but when we reduce the memory size using our method, we still outperform Batch (which is trained from scratch on all data). Overall, our initial effort here suggests that K-priors can do better than Replay, and have potential to give better results with more hyperparameter tuning. 6 Discussion In this paper, we proposed a class of new priors, called K-prior. We show general principles of obtaining accurate adaptation with K-priors which are based on accurate gradient reconstructions. The prior applies to a wide-variety of adaptation tasks for a range of models, and helps us to connect many existing, seemingly-unrelated adaptation strategies in ML. Based on our adaptation principles, we derived practical methods to enable adaptation by tweaking models’ predictions at a few past examples. This is analogous to adaptation in humans and animals where past experiences is used for new situations. In practice, the amount of required past memory seems sufﬁciently low. The ﬁnancial and environmental costs of retraining are a huge concern for ML practitioners, which can be reduced with quick adaptations. The current pipelines and designs are specialized for an ofﬂine, static setting. Our approach here pushes towards a simpler design which will support a more dynamic setting. The approach can eventually lead to new systems that learn quickly and ﬂexibly, and also act sensibly across a wide range of tasks. This opens a path towards systems that learn incrementally in a continual fashion, with the potential to fundamentally change the way ML is used in scientiﬁc and industrial applications. We hope that this work will help others to do more towards this goal in the future. We ourselves will continue to push this work in that direction. 10Acknowledgements We would like to thank the members of the Approximate-Bayesian-Inference team at RIKEN- AIP. Special thanks to Dr. Thomas Möllenhoff (RIKEN-AIP), Dr. Gian Maria Marconi (RIKEN- AIP), Peter Nickl (RIKEN-AIP), and also to Prof. Richard E. Turner (University of Cambridge). Mohammad Emtiyaz Khan is partially supported by KAKENHI Grant-in-Aid for Scientiﬁc Research (B), Research Project Number 20H04247. Siddharth Swaroop is partially supported by a Microsoft Research EMEA PhD Award. Author Contributions Statement List of Authors: Mohammad Emtiyaz Khan (M.E.K.), Siddharth Swaroop (S.S.). Both the authors were involved in the idea conception. S.S. derived a version of theorem 1 with some help from M.E.K. This was then modiﬁed and generalized by M.E.K. for generic adaptation tasks. The general principle of adaptation described in the paper are due to M.E.K., who also derived connections to SVMs and GPs, and extensions to Bayesian settings (with regular feedback from S.S.). Both authors worked together on the connections to Knowledge Distillation and Deep Continual Learning. S.S. performed all the experiments (with regular feedback from M.E.K.). M.E.K. wrote the main sections with the help of S.S., and S.S. wrote the section about the experiments. Both authors proof-read and reviewed the paper. References [1] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statis- tical guarantees. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. InAdvances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [4] Shuang Ao, Xiang Li, and Charles Ling. Fast generalized distillation for semi-supervised domain adaptation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017. [5] Jordan Ash and Ryan P Adams. On warm-starting neural network training. In Advances in Neural Information Processing Systems, volume 33, pages 3884–3894, 2020. [6] Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. Journal of machine learning research, 6(Oct):1705–1749, 2005. [7] Ari Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in function space. In International Conference on Learning Representations, 2019. [8] Jonathan Brophy and Daniel Lowd. Machine unlearning for random forests. In International Conference on Machine Learning, 2021. [9] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In Advances in Neural Information Processing Systems, volume 33, pages 15920–15930, 2020. [10] Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, and Lorenzo Rosasco. Gaussian process optimization with adaptive sketching: Scalable and no regret. In Conference on Learning Theory, pages 533–557. PMLR, 2019. [11] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In2015 IEEE Symposium on Security and Privacy, page 463–480, May 2015. doi: 10.1109/SP.2015.35. [12] Gert Cauwenberghs and Tomaso Poggio. Incremental and decremental support vector machine learning. In Advances in Neural Information Processing Systems, volume 13. MIT Press, 2001. 11[13] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge university press, 2006. [14] Olivier Chapelle. Training a support vector machine in the primal. Neural Computation, 19(5): 1155–1178, May 2007. ISSN 0899-7667. doi: 10.1162/neco.2007.19.5.1155. [15] Michael B Cohen, Cameron Musco, and Christopher Musco. Ridge leverage scores for low-rank approximation. arXiv preprint arXiv:1511.07263, 6, 2015. [16] R Dennis Cook. Detection of inﬂuential observation in linear regression. Technometrics, 19(1): 15–18, 1977. [17] Lehel Csató and Manfred Opper. Sparse on-line Gaussian processes. Neural computation, 14 (3):641–668, 2002. [18] Dennis DeCoste and Kiri Wagstaff. Alpha seeding for support vector machines. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’00, page 345–349. ACM Press, 2000. ISBN 978-1-58113-233-5. doi: 10.1145/347090. 347165. [19] Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. Continual learning in practice. arXiv preprint arXiv:1903.05202, 2019. [20] Hua Duan, Hua Li, Guoping He, and Qingtian Zeng. Decremental learning algorithms for nonlinear langrangian and least squares support vector machines. In Proceedings of the First International Symposium on Optimization and Systems Biology (OSB’07) , pages 358–366. Citeseer, 2007. [21] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304–9312, 2020. [22] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certiﬁed data removal from machine learning models. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3832–3842. PMLR, 13–18 Jul 2020. [23] Ralf Herbrich, Neil Lawrence, and Matthias Seeger. Fast sparse gaussian process methods: The informative vector machine. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2003. [24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [25] Masayuki Karasuyama and Ichiro Takeuchi. Multiple incremental decremental learning of support vector machines. IEEE Transactions on Neural Networks, 21(7):1048–1059, Jul 2010. ISSN 1045-9227, 1941-0093. doi: 10.1109/TNN.2010.2048039. [26] Mohammad Emtiyaz Khan. Decoupled variational Gaussian inference. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. [27] Mohammad Emtiyaz Khan and Haavard Rue. Learning-algorithms from Bayesian principles. 2020. https://emtiyaz.github.io/papers/learning_from_bayes.pdf. [28] Mohammad Emtiyaz Khan, Aleksandr Aravkin, Michael Friedlander, and Matthias Seeger. Fast dual variational inference for non-conjugate latent Gaussian models. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 951–959, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. [29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna- tional Conference on Learning Representations, 2015. [30] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. [31] Pavel Laskov, Christian Gehl, Stefan Krüger, Klaus-Robert Müller, Kristin P Bennett, and Emilio Parrado-Hernández. Incremental support vector learning: Analysis, implementation and applications. Journal of machine learning research, 7(9), 2006. 12[32] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http: //yann.lecun.com/exdb/mnist/. [33] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [34] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. [35] Zhizheng Liang and YouFu Li. Incremental support vector machine learning in the primal and applications. Neurocomputing, 72(10):2249–2258, Jun 2009. ISSN 0925-2312. doi: 10.1016/j.neucom.2009.01.001. [36] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learn- ing. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6470–6479, Red Hook, NY , USA, 2017. Curran Associates Inc. ISBN 9781510860964. [37] David Lopez-Paz, L. Bottou, B. Schölkopf, and V . Vapnik. Unifying distillation and privileged information. arXiv preprint arXiv:1511.03643, 2016. [38] James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1–76, 2020. [39] Eric Nalisnick, Jonathan Gordon, and Jose Miguel Hernandez-Lobato. Predictive complexity priors. In Proceedings of The 24th International Conference on Artiﬁcial Intelligence and Statistics, volume 130, pages 694–702. PMLR, 13–15 Apr 2021. [40] Cuong V . Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning. In International Conference on Learning Representations, 2018. [41] Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. Variational bayesian unlearning. In Advances in Neural Information Processing Systems, volume 33, pages 16025– 16036. Curran Associates, Inc., 2020. [42] Frank Nielsen. On a variational deﬁnition for the jensen-shannon symmetrization of distances based on the information radius. Entropy, 23(4):464, 2021. [43] M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural Computation, 21(3):786–792, 2009. [44] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D. Lawrence. Challenges in deploying machine learning: a survey of case studies. arXiv:2011.09926 [cs], Jan 2021. arXiv: 2011.09926. [45] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard Turner, and Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable past. In Advances in Neural Information Processing Systems, volume 33, pages 4453–4464. Curran Associates, Inc., 2020. [46] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL: Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017. [47] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [48] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7 (2):123–146, 1995. [49] Enrique Romero, Ignacio Barrio, and Lluís Belanche. Incremental and decremental learning for linear support vector machines. In International Conference on Artiﬁcial Neural Networks, pages 209–218. Springer, 2007. [50] Sebastian Ruder, Parsa Ghaffari, and John G Breslin. Knowledge adaptation: Teaching to adapt. arXiv preprint arXiv:1702.02052, 2017. [51] Nikolaos Saraﬁanos, Michalis Vrigkas, and Ioannis A Kakadiaris. Adaptive SVM+: Learning with privileged information for domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 2637–2644, 2017. 13[52] Simo Särkkä, Arno Solin, and Jouni Hartikainen. Spatiotemporal learning via inﬁnite- dimensional Bayesian ﬁltering and smoothing: A look at Gaussian process regression through Kalman ﬁltering. IEEE Signal Processing Magazine, 30(4):51–61, 2013. [53] Sebastian Schelter. amnesia–towards machine learning models that can forget user data very fast. In 1st International Workshop on Applied AI for Database Systems and Applications (AIDB19), 2019. [54] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In International Conference on Machine Learning, pages 4528–4537. PMLR, 2018. [55] Daniel Simpson, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. Penalising model component complexity: A principled, practical approach to constructing priors. Statistical Science, 32(1):1–28, 2017. ISSN 08834237, 21688745. [56] Arno Solin, James Hensman, and Richard E Turner. Inﬁnite-horizon Gaussian processes. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [57] Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, and Yee Whye Teh. Functional regularisation for continual learning with gaussian processes. In International Conference on Machine Learning, 2020. [58] Cheng-Hao Tsai, Chieh-Yen Lin, and Chih-Jen Lin. Incremental and decremental training for linear classiﬁcation. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, page 343–352. ACM, Aug 2014. [59] Amund Tveit, Magnus Lie Hetland, and Håavard Engum. Incremental and decremental proximal support vector classiﬁcation using decay coefﬁcients. In International Conference on Data Warehousing and Knowledge Discovery, pages 422–429. Springer, 2003. [60] Vladimir Vapnik and Rauf Izmailov. Learning using privileged information: similarity control and knowledge transfer. Journal of Machine Learning Research, 16(1):2023–2049, 2015. [61] Zeyi Wen, Bin Li, Ramamohanarao Kotagiri, Jian Chen, Yawen Chen, and Rui Zhang. Improv- ing efﬁciency of SVM k-fold cross-validation by alpha seeding. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017. [62] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning , pages 3987–3995. PMLR, 2017. 14A Optimal K-priors for GLMs We present theoretical results to show that K-priors with limited memory can achieve low gradient- reconstruction error. We will discuss the optimal K-prior which can theoretically achieve perfect reconstruction error. Note that the prior is difﬁcult to realize in practice since it requires all past training-data inputs X. Our goal here is to establish a theoretical limit, not to give practical choices. Our key idea is to choose a few input locations that provide a good representation of the training-data inputs X. We will make use of the singular-value decomposition (SVD) of the feature matrix, Φ⊤= U∗ 1:KS∗ 1:K(V∗ 1:K)⊤ where K ≤min(N,P ) is the rank, U∗ 1:K is P ×K matrix of left-singular vectors u∗ i, V∗ 1:K is N ×Kmatrix of right-singular vectors v∗ i, and S∗ 1:K is a diagonal matrix with singular values si as the i’th diagonal entry. We deﬁne M∗= {u∗ 1,u∗ 2,..., u∗ K}, and the following K-prior, Kopt(w; w∗,M∗) = K∑ j=1 β∗ jℓ ( h(fw∗(u∗ j)),h(fw(u∗ j)) ) + 1 2 δ∥w −w∗∥2. (19) Here, each functional divergence is weighted by β∗ j which refers to the elements of the following, β∗= D−1 u S∗ 1:KV⊤ 1:Kdx where dx is an N-length vector with entries h(fi w) −h(fi w∗) for all i∈X, while Du is a K×K diagonal matrix with diagonal entries h(fw(uj)) −h(fw∗(uj)) for all j = 1,2,...,K . The above deﬁnition departs slightly from the original deﬁnition where only a single τ is used. The weights β∗ j depend on X, so it is difﬁcult to compute them in practice when the memory is limited. However, it might be possible to estimate them for some problems. Nevertheless, with β∗ j, the above K-prior can be achieve perfect reconstruction. The proof is very similar to the one given in Equations 9 and 10, and is shown below, ∇Kopt(w; w∗,M∗) = K∑ j=1 β∗ ju∗ i [h(fw(ui)) −h(fw∗(ui))] + δ(w −w∗), = U∗ 1:KDuβ∗+ δ(w −w∗), = U∗ 1:KS∗ 1:KV⊤ 1:Kdx + δ(w −w∗), = Φ⊤dx + δ(w −w∗), = ∑ i∈X φi [ h(fi w) −h(fi w∗) ] + δ(w −w∗), = ∑ i∈D φi [ h(fi w) −yi ] + δw    =∇¯ℓ(w). − \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018∑ i∈D φi [ h(fi w∗) −yi ] −δw∗    =0. . The ﬁrst line is simply the gradient, which is then rearranged in the matrix-vector product in the second line. The third line uses the deﬁnition of β∗, and the fourth line uses the SVD of Φ. In the ﬁfth line we expand it to show that it is the same as Eq. 9, and the rest follows as before. Due to their perfect gradient-reconstruction property, we call the prior in Eq. 19 the optimal prior. When only top-M singular vectors are chosen, the gradient reconstruction error grows according to the leftover singular values. We show this below where we have chosenM∗ M = {u∗ 1,u∗ 2,..., u∗ M} as the set of top-M singular vectors, eopt(w; w,M∗ M) = ∇¯ℓ(w) −∇Kopt(w; w∗,M∗ M) = ∇¯ℓ(w) −∇Kopt(w; w∗,M∗) + ∇Kopt(w; w∗,M∗) −∇Kopt(w; w∗,M∗ M) = ∇Kopt(w; w∗,M∗) −∇Kopt(w; w∗,M∗ M) = M∑ j=M+1 β∗ ju∗ i [h(fw(ui)) −h(fw∗(ui))] , = U∗ M+1:KS∗ M+1:KV⊤ M+1:Kdx. 15The ﬁrst line is simply the deﬁnition of the error, and in the second line we add and subtract the optimal K-prior with memory M∗. The next few lines use the deﬁnition of the optimal K-prior and rearrange terms. Using the above expression, we ﬁnd the following error, ∥eopt(w; w,M∗ M)∥= √ ΣK j=M+1s2 j(ax j)2 where ax j is the j’th entry of a vectora = V⊤ 1:Kdx. The error depends on the leftover singular values. The error is likely to be the optimal error achievable by any memory of size M, and establishes a theoretical bound on the best possible performance achievable by any K-prior. B Additional Examples of Adaptation with K-priors Here, we brieﬂy discuss the K-prior regularization for the other adaptation tasks. B.1 The Change Regularizer task For Change Regularizer task, we need to slightly modify the K-prior of Eq. 8. We replace the weight-space divergence in Eq. 8 with a Bregman divergence deﬁned using two different regularizers (see Proposition 5 in Nielsen [42]), BGR(w∥w∗) = G(w) + R∗(η∗) −w⊤η∗, (20) where η∗ = ∇R(w∗) is the dual parameter and R∗is the convex-conjugate of R. This is very similar to the standard Bregman divergence but uses two different (convex) generating functions. To get an intuition, consider hyperparameter-tuning for the L2 regularizer R(w) = 1 2 δ∥w∥2, where our new regularizer G(w) = 1 2 γ∥w∥2 uses a hyperparameter γ ̸= δ. Since the conjugate R∗(η) = 1 2 ∥η∥2/δand η∗= ∇R(w∗) = δw∗, we get BGR(w∥w∗) = 1 2 (γ∥w∥2 + δ∥w∗∥2 −2δw⊤w∗). When γ = δ, then this reduces to the divergence used in Eq. 8, but otherwise it enables us to reconstruct the gradient of the past objective but with the new regularizer. We deﬁne the following K-prior where the weight-divergence is replaced by Eq. 20, and use it to obtain ˆwG, K(w; w∗,M) = ∑ i∈M ℓ ( h(fi w∗),h(fi w) ) + BGR(w∥w∗), ˆwG= arg min w∈W K(w; w∗,M) (21) The following theorem states the recovery of the exact solution. Theorem 2. For M= Xand strictly-convex regularizers, we havewG= ˆwG. The derivation is very similar to Eq. 10, where δ(w −w∗) is replaced by ∇G(w) −∇R(w∗). B.2 The Change Model Class task We discuss the ‘Change Model Class’ task through an example. Suppose we want to remove the last feature from φi so that w ∈RP is replaced by a smaller weight-vector θ ∈RP−1. Assuming no change in the hyperparameter, we can simply use a weighting matrix to ‘kill’ the last element ofw∗. We deﬁne the matrix A = IP−1×P whose last column is 0 and the rest is the identity matrix of size P −1. With this, we can use the following training procedure over a smaller space ¯w, K(θ) = ∑ i∈M ℓ ( h(fi w∗),h(fθ(xi)) ) + BR(θ∥Aw∗), ˆθ∗= arg min θ∈Θ K(θ) (22) If the hyperparameters or regularizer are different for the new problem, then the Bregman divergence shown in Eq. 20 can be used, with an appropriate weighting matrix. Model compression is a speciﬁc instance of the ‘Change Model Class’ task, where the architecture is entirely changed. For neural networks, this also changes the meaning of the weights and the 16regularization term may not make sense. In such cases, we can simply use the functional-divergence term in K-priors, K(θ) = ∑ i∈M ℓ ( h(fi w∗),h(fθ(xi)) ) , ˆθ∗= arg min θ∈Θ K(θ) (23) This is equivalent to knowledge distillation (KD) in Eq. 15 with λ= 0 and T = 1. Since KD performs well in practice, it is possible to use a similar strategy to boost K-prior, e.g., we can deﬁne the following, ˆθ∗= arg min θ∈Θ λ ∑ i∈M ℓ(yi,h(fi θ)) + (1−λ)K(θ) (24) We could even use limited-memory in the ﬁrst term. The term λlets us trade-off teacher predictions with the actual data. We can construct K-priors to change multiple things at the same time, for example, changing the regularizer, the model class, and adding/removing data. A K-prior for such situations can be constructed using the same principles we have detailed. C Derivation of the K-priors Gradients for Deep Learning The gradient is obtained similarly to (10) where we add and subtract yi in the ﬁrst term in the ﬁrst line below, ∇K(w) = ∑ i∈X ∇fi w [ h(fi w) −h(fi w∗) ] + δ(w −w∗), = ∑ i∈D ∇fi w [ h(fi w) −yi ] + δw    =∇¯ℓ(w) − ∑ i∈D ∇fi w[h(fi w∗) −yi] −δw∗    ̸=∇¯ℓ(w∗),because ∇fiw̸=∇fiw∗ , The second term is not zero because ∇fi w ̸= ∇fi w∗ to get ∇¯ℓ(w∗) in the second term. The gradient of the KD objective can be obtained in a similar fashion, where we add and subtract yi in the second term in the ﬁrst line to get the second line, ∇ℓKD(w) = λ ∑ i∈D ∇fi w [ h(fi w) −yi ] + (1 −λ) ∑ i∈D ∇fi w [ h(fi w) −h(fi w∗) ] , = ∑ i∈D ∇fi w [ h(fi w) −yi ] −(1 −λ) ∑ i∈D ∇fi w [ h(fi w∗) −yi ] . D Proof for Adaptation for Bayesian Learning with K-priors To prove the equivalence of (18) to the full batch variational inference problem with a Gaussian q(w) = N(w|µ,Σ), we can use the following ﬁxed point of the variational objective (see Section 3 in [27] for the expression), 0 = ∇µEq[L(w)] |µ=µ+,Σ=Σ+ = Eq[∇wL(w)]|µ=µ+,Σ=Σ+ , (25) Σ−1 + = ∇ΣEq[L(w)]|µ=µ+,Σ=Σ+ = Eq[∇2 wL(w)] ⏐⏐ µ=µ+,Σ=Σ+ , (26) where L(w) = [ℓj(w) + ¯ℓ(w) + R(w)], µ+ and Σ+ are the mean and covariance of the optimal q+(w) for the ‘Add Data’ task. For GLMs, both the gradient and Hessian of¯ℓ(w) is equal to those of K(w) deﬁned in (8), which proves the equivalence. For equivalence to GPs, we ﬁrst note that, similarly to the representer theorem, the mean and covariance of q+(w) can be expressed in terms of the two N-length vectors α and λ [43, 26, 28], µ+ = Φ⊤ +α, Σ+ = (Φ⊤ +ΛΦ+ + δI)−1, 17where Λ is a diagonal matrix with λ as the diagonal. Using this, we can deﬁne a marginal q(fi) = N(fi|mi,vi), where fi = φ⊤ i w, with the mean and variance deﬁned as follows, mi = φ⊤ i µ+ = k⊤ i,+α, v i = φ⊤ i Σ+φi = kii,+ −k⊤ i,+ ( Λ−1 + δK+ )−1 ki,+, where kii,+ = φ⊤ i φi. Using these, we can now rewrite the optimality conditions in the function-space to show equivalence to GPs. We show this for the ﬁrst optimality condition (25), ∇µEq[L(w)]|µ=µ+,Σ=Σ+ = ∑ i∈D∪j EN(ϵi|0,1) [ ∇fℓ(yi,h(f))|f=φ⊤ i µ++(φ⊤ i Σ+φi) 1/2 ϵi ] φi + δµ+ Multiplying it by Φ+, we can rewrite the gradient in the function space, 0 = ∑ i∈D∪j EN(ϵi|0,1) [ ∇fℓ(yi,h(f))|f=mi+v1/2 i ϵi ] ki,+ + δK+α = ∑ i∈D∪j ∇miEq(fi) [ℓ(yi,h(fi))] ki,+ + δK+α where m is the vector of mi. Setting this to 0, gives us the ﬁrst-order condition for a GP with respect to the mean, e.g., see Equation 3.6 and 4.1 in Chapelle [14]. It is easy to check this for GP regression, where ℓ(yi,h(fi)) = (yi −fi)2, in which case, the equation becomes, 0 = ∑ i∈D∪j (mi −yi)ki,+ + δK+α ⇒α = (K+ + δI)−1y, which is the quantity which gives us the posterior mean. A similar condition condition for the covariance can be written as well. Clearly, when we use a limited memory, some of the data examples are removed and we get a sparse approximation similarly to approaches such as informative vector machine which uses a subset of data to build a sparse approximation [ 23]. Better sparse approximations can be built by carefully designing the functional divergence term. For example, we can choose the matrixB in the divergence, Df(f(w)∥f(w∗)) = 1 2 d⊤ mBdm ⇒ ∇Df(f(w)∥f(w∗)) = ∇f(w)⊤Bdm This type of divergence is used in Pan et al. [45], where the matrix B is set to correlate the examples in Mwith the examples in D. Design of such divergence function is a topic which requires more investigation in the future. E Further experimental results We provide more details on all our experiments, such as hyperparameters and more results. E.1 Adaptation tasks Logistic Regression on the ‘UCI Adult’ dataset.In Fig. 2(a) we show results for the 4 adaptation tasks on the UCI Adult dataset, and provide experimental details in Sec. 5. Note that for all but the ‘Change Model Class’ task, we used polynomial degree 1. For all but the ‘Change Regularizer’ task, we use δ= 5. We optimize using LBFGS (default PyTorch implementation) with a learning rate of 0.01 until convergence. Throughout our experiments in the paper, we used the same memorable points for Replay as for K-priors (the points with the highest h′(fi w∗)), and used τ = 1 (from Eq. 6). In Fig. 4 we provide an ablation study for Replay with different strategies: (i) we choose points byh′(fi w∗) and use τ = N/M, (ii) we choose points randomly and use τ = 1, (iii) we choose points randomly and use τ = N/M. Recall that N is the past data size (the size of D) and M is the number of datapoints stored in memory (the size of M). We see that choosing points by h′(fi w∗) and using τ = 1 performs very well, and we therefore choose this for all our experiments. 18Validation acc (%) Add new data Memory size (% of past data) Figure 4: This ﬁgure shows using τ = 1 works well for Replay, both for random selection of memory and choosing memory by sorting h′(fi w∗). We compare different methods for Replay on the Adult ‘Add Data’ task. ‘Random’ means the points in memory are chosen randomly as opposed to choosing the points with highest h′(fi w∗). We also consider using τ = N/M instead of τ = 1. Choosing randomly or by h′(fi w∗) are within standard deviations in this task, so we choose to report memory chosen by h′(fi w∗) in other experiments (this is then consistent with the memory in K-priors). Logistic Regression on the ‘USPS odd vs even’ dataset.For all but the ‘Change Model Class’ task, we used polynomial degree 1. For all but the ‘Change Regularizer’ task, we useδ= 50. We optimize using LBFGS with a learning rate of 0.1 until convergence. Neural Networks on the ‘USPS odd vs even’ dataset.For all but the ‘Change Regularizer’ task, we use δ= 5. We optimize using Adam with a learning rate of 0.005 for 1000 epochs (which is long enough to reach convergence). Neural Networks on the ‘MNIST’ dataset.We show results on 10-way classiﬁcation with MNIST in Fig. 5, which has 60,000 training images across 10 classes (handwritten digits), with each image of size 28 ×28. We use a two hidden-layer MLP with 100 units per layer, and report means and standard deviations across 3 runs. For the ‘Add Data’ task, we start with a random 90% of the dataset and add 10%. For the ‘Change Regularizer’ task, we changeδ= 1 to 5 (we use δ= 1 for all other tasks). For the ‘Change Architecture’ task, we compress to a single hidden layer with 100 hidden units. We optimize using Adam with a learning rate of 0.001 for 250 epochs, using a minibatch size of 512. Validation acc (%) Memory size (% of past data) Add new data Change regularizer Change architecture Memory size (% of past data) Memory size (% of past data) Figure 5: K-priors work well on MNIST (with an MLP), similar to other results on the USPS and UCI Adult datasets. For details on the experiments, see App. E.1. Neural Networks on the ‘CIFAR-10’ dataset.We provide results for CIFAR-10 using 10-way classiﬁcation. CIFAR-10 has 60,000 images (50,000 for training), and each image has 3 channels, each of size 32 ×32. We report mean and standard deviations over 3 runs. We use the CifarNet architecture from Zenke et al. [62].We optimize using Adam with a learning rate of 0.001 for 100 epochs, using a batch size of 128. In Fig. 6 we also provide results on the ‘Change Regularizer’ task, where we changeδ= 1 to 0.5 (we use δ= 1 for all the other tasks). We also provide results on the ‘Change Architecture’ task, where we change from the CifarNet architecture to a LeNet5-style architecture. This smaller architecture has two convolution layers followed by two fully-connected layers: the ﬁrst convolution layer has 6 output channels and kernel size 5, followed by the ReLU activation, followed by a Max Pool layer with kernel size 2 (and stride 2), followed by the second convolution layer with 16 output channels and kernel size 5, followed by the ReLU activation, followed by another Max Pool layer with kernel size 2 (and stride 2), followed by a fully-connected layer with 120 hidden units, followed 19by the last fully-connected layer with 84 hidden units. We also use ReLU activation functions in the fully-connected layers. Validation acc (%) Change regularizer Change architecture Memory size (% of past data) Memory size (% of past data) Figure 6: Results for two adaptation tasks on CIFAR-10 with CNNs. See also Fig. 3(c) for results on the ‘Add Data’ task. K-priors perform well, especially on the ‘Change Regularizer’ task. The ‘Change Architecture’ task is more difﬁcult, but we note that we do not use a temperature. Having a temperature greater than 1 is known to help in similar settings, such as knowledge distillation [24]. For the knowledge distillation task, we used K-priors with a temperature, similar to the temperature commonly used in knowledge distillation [ 24]. We note that there is some disagreement in the literature regarding how the temperature should be applied, with some works using a temperature only on the teacher’s logits (such as in Eq. 15) [37], and other works having a temperature on both the teacher and student’s logits [24]. In our experiments, we use a temperature T on both the student and teacher logits, as written in the ﬁnal term of Eq. 27. We also multiply the ﬁnal term by T2 so that the gradient has the same magnitude as the other data term (as is common in knowledge distillation). ℓKD,expt(w) = λ ∑ i∈D ℓ ( yi,h(fi w) ) + δ∥w∥2 + (1 −λ) T2 ∑ i∈D ℓ ( h(fi w∗/T), h(fi w/T) ) . (27) We used λ = 0.5 in the experiment. We performed a hyperparameter sweep for the temperature (across T = [1,5,10,20]), and used T = 5. For K-priors in this experiment, we optimize for 10 epochs instead of 100 epochs, and use τ = 1. In Fig. 3(c) we also showed initial results using a temperature on the ‘Add Data’ task on CIFAR-10. We used the same temperature from the knowledge distillation experiment (T = 5 and λ= 0.5), but did not perform an additional hyperparameter sweep. We ﬁnd that using a temperature improved results for CNNs, and we expect increased improvements if we perform further hyperparameter tuning. Note that many papers that use knowledge distillation perform more extensive hyperparameter sweeps than we have here. E.2 Weight-priors vs K-priors In Fig. 7 we provide results comparing with weight-priors for all the ‘Add Data’ tasks. We see that for homogeneous data splits (such as UCI Adult, MNIST and CIFAR), weight-priors perform relatively well. For inhomogeneous data splits (USPS with logistic regression and USPS with neural networks), weight-priors perform worse. Validation acc (%) Memory size (% of past data) (a) Adult, logistic regression (b) USPS, NN (c) MNIST, NN (d) CIFAR-10, NN Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Figure 7: Results on the ‘Add Data’ task, with a comparison to weight-priors. (a), (c), (d) For homogeneous data splits, weight-priors can perform relatively well. (b) For inhomogeneous data splits, weight-priors perform worse (see also Fig. 3(b)). 20E.3 K-priors ablation with weight-term In this section we perform an ablation study on the importance of the weight-term 1 2 δ∥w −w∗∥2 in Eq. 8. In Fig. 8 we show results on logistic regression on USPS where we do not havew∗in this term (the update equation is the same as Eq. 8 except the weight-term is1 2 δ∥w∥2 instead of 1 2 δ∥w−w∗∥2). We see that the weight-term is important: including the weight-term always improves performance. Validation acc (%) Add new data Remove old data Change regularizer Memory size (% of past data) Memory size (% of past data) Memory size (% of past data) Figure 8: Comparing K-priors with a version of K-priors without the weight-term on USPS logistic regression. We see that the weight-term is important, especially on the ‘Add Data’ task. E.4 K-priors with random initialization In all experiments so far, when we train on a new task, we initialize the parameters at the previous parameters w∗. Note that this is not possible in the “Change architecture” task, where weights were initialized randomly. Our results are independent of initialization strategy: we get the same results whether we use random initialization or initializing at previous values. The only difference is that random initialization can sometimes take longer until convergence (for all methods: Batch, Replay and K-priors). For GLMs, where we always train until convergence and there is a single optimum, it is clear that the exact same solution will always be reached. We now also provide the result for ‘USPS odd vs even’, with random initialization in Fig. 9, for the 3 tasks where we had earlier initialized at previous values (compare with Fig. 1 (right)). We use exactly the same hyperparameters and settings as in Fig. 1 (right), aside from initialization method. Validation acc (%) Memory size (% of past data) Add new data Remove old data Change regularizer Memory size (% of past data) Memory size (% of past data) Figure 9: K-priors obtain the same results when randomly initializing the weights for the ‘Add new data’, ‘Remove old data’ and ‘Change regularizer’ tasks on USPS odd vs even with neural networks. Previous results, including Fig. 1 (right), initialized parameters at previously learnt values. The ‘Change architecture’ task originally used random initialization and so is not repeated here. E.5 K-priors converge cheaply In this section, we show that K-priors with limited memory converge to the ﬁnal solution cheaply, converging in far fewer passes through data than the batch solution. This is because we use a limited memory, and only touch the more important datapoints. Table 1 shows the “number of backprops” until reaching speciﬁc accuracies (90% and 97%) on USPS with a neural network (using the same settings as in Fig. 1 (right)). This is one way of measuring the “time taken”, as backprops through the model are the time-limiting step. For K-priors and Replay, we use 10% of past memory. All methods use random initializations when starting training on a new task. 21We see that K-priors with 10% of past data stored are quicker to converge than Batch, even though both eventually converge to the same accuracy (as seen in Fig. 1 (right)). For example, to reach 97% accuracy for the Change Regularizer task, K-priors only need 54,000 backward passes, while Batch requires 2,700,000 backward passes. We also see that Replay is usually very slow to converge. This is because it does not use the same information as K-priors (as Replay uses hard labels), and therefore requires signiﬁcantly more passes through data to achieve the same accuracy. In addition, Replay with 10% of past data cannot achieve high accuracies (such as 97% accuracy), as seen in Fig. 1 (right). Table 1: Number of backpropagations required to achieve a speciﬁed accuracy on USPS with a neural network (1000s of backprops). K-priors with 10% past memory require much fewer backprops to achieve the same accuracy as Batch, while Replay with 10% memory cannot achieve high accuracies. Accuracy Method Add Remove Change Change achieved new data old data regularizer model class 90% Batch 87 94 94 86 90% Replay (10% memory) 348 108 236 75 90% K-prior (10% memory) 73 53 13 22 97% Batch 1,900 1,800 2,700 3,124 97% Replay (10% memory) – 340 – – 97% K-prior (10% memory) 330 120 54 68 E.6 Further details on Fig. 1 (middle), moons dataset. To create this dataset, we took 500 samples from the moons dataset, and split them into 5 splits of 100 datapoints each, with each split having 50 datapoints from each task. Additionally, the splits were ordered according to the x-axis, meaning the 1st split were the left-most points, and the 5th split had the right-most points. In the provided visualisations, we show transfer from ‘past data’ consisting of the ﬁrst 3 splits (so, 300 datapoints) and the ‘new data’ consisting of the 4th split (a new 100 datapoints). We store 3% of past data as past memory in K-priors, chosen as the points with the highest h′(fi w∗). F Changes in the camera-ready version compared to the submitted version This section lists the major changes we made for the camera-ready version of the paper, incorporating reviewer feedback. • Added a paragraph on the optimal K-prior after Eq. 12, as well as a detailed explanation in App. A. • Updated Fig. 3(d), following a more extensive sweep of hyperparameters. • Added App. E.4, showing K-priors with random initialization give the same results as K-priors that are initialized at the previous model parameters. • Added App. E.5, showing that K-priors with limited memory converge to the ﬁnal solution cheaply, requiring fewer passes through the data than the batch solution. 22",
      "meta_data": {
        "arxiv_id": "2106.08769v2",
        "authors": [
          "Mohammad Emtiyaz Khan",
          "Siddharth Swaroop"
        ],
        "published_date": "2021-06-16T13:27:22Z",
        "pdf_url": "https://arxiv.org/pdf/2106.08769v2.pdf",
        "github_url": "https://github.com/team-approx-bayes/kpriors"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Knowledge-adaptation priors (K-priors) to address the high financial and environmental costs of retraining machine learning models when changes occur. K-priors enable quick and accurate model adaptation for a wide variety of tasks and models by combining weight and function-space priors to faithfully reconstruct the gradients of past training objectives. This approach unifies and generalizes many existing, seemingly unrelated, adaptation strategies in machine learning. Empirically, K-priors achieve performance similar to full retraining but require training on only a handful of past examples.",
        "methodology": "K-priors are defined as a class of priors using both weight and function-space regularizers, specifically K(w; w*,M) = Df(f(w)||f(w*)) + τDw(w||w*), where Df and Dw are Bregman divergences. The core principle is to reconstruct the gradients of the past training objective. For Generalized Linear Models (GLMs), an L2 regularizer is used for Dw and a Bregman divergence with the log-partition function A(f) for Df, which exactly recovers past gradients when the full past data is used. For limited memory, a practical approximation involves choosing examples with the highest derivative h'(fi w*) (referred to as 'memorable past') to minimize gradient-reconstruction error. For deep learning, K-priors extend Knowledge Distillation by adding a weight-space term, allowing general link functions, and using a small number of memory examples. Adaptation is achieved by minimizing new objectives regularized by the K-prior (e.g., for adding/removing data, changing regularizers, or changing model classes).",
        "experimental_setup": "Experiments were conducted on four adaptation tasks: 'Add Data', 'Remove Data', 'Change Regularizer', and 'Change Architecture' (including model compression/knowledge distillation). Models included logistic regression (on UCI Adult and USPS datasets) and neural networks (1-hidden-layer MLP on USPS, 2-hidden-layer MLPs on MNIST, CifarNet and LeNet5-style student on CIFAR-10). Comparisons were made against 'Batch' (full retraining from scratch), 'Replay' (retraining with a small memory of past examples using true labels), and 'Weight-Priors' (for the 'Add Data' task). Memory points for K-priors and Replay were chosen based on the highest h'(fi w*) values. Optimizers used were L-BFGS (logistic regression) and Adam (neural networks). Validation accuracy was the primary performance metric. Hyperparameter tuning, including a temperature parameter (similar to knowledge distillation) and λ, was explored for deep learning tasks.",
        "limitations": "The theoretical 'optimal K-prior' that can achieve perfect reconstruction error using singular vectors of the feature matrix is difficult to realize in practice. For larger deep learning problems (e.g., CIFAR-10 with 50% past data), there can be a noticeable gap between K-prior and Batch performance, suggesting the need for more extensive hyperparameter tuning. A dip in performance was observed at certain memory sizes (e.g., 10% past data on CIFAR-10 for knowledge distillation), which was attributed to suboptimal hyperparameter tuning (τ or λ). The function-space formulations, while equivalent to K-priors, could be computationally expensive.",
        "future_research_directions": "Future research includes applying sketching methods (e.g., leverage scores) for more efficient memory selection in K-priors. More empirical effort is required to tune various hyperparameters to achieve consistent behavior across all memory sizes, particularly for large deep learning problems. Investigation into the design of divergence functions is needed for building better sparse approximations (e.g., choosing the matrix B to correlate examples in memory with examples in the full dataset). The authors also express a broader goal of pushing towards simpler designs that support dynamic ML settings, leading to systems that learn quickly, flexibly, and incrementally in a continual fashion.",
        "experimental_code": "def select_memory_points(dataloader, model, num_points, additional_memory_data=None, use_cuda=False, descending=True):\n    memory_points_list = {}\n    points_indices = {}\n    data, target = dataloader\n    num_points_per_class = [int(num_points/2),int(num_points/2)]\n    if torch.sum(target==0) < num_points_per_class[0]:\n        num_points_per_class[0] = torch.sum(target==0).numpy()\n        num_points_per_class[1] = num_points - num_points_per_class[0]\n    elif torch.sum(target==1) < num_points_per_class[1]:\n        num_points_per_class[1] = torch.sum(target==1).numpy()\n        num_points_per_class[0] = num_points - num_points_per_class[1]\n    if use_cuda:\n        data_in = data.cuda()\n    else:\n        data_in = data\n    preds = model.forward(data_in)\n    lamb = softmax_hessian(preds)\n    if use_cuda:\n        lamb = lamb.cpu()\n    lamb = torch.sum(lamb, dim=-1)\n    lamb = lamb.detach()\n    for cid in range(2):\n        p_c = data[target == cid]\n        indices_for_points = np.argwhere(target == cid)[0].numpy()\n        if len(p_c) > 0:\n            scores = lamb[target == cid]\n            _, indices = scores.sort(descending=descending)\n            memory_points_list[cid] = p_c[indices[:num_points_per_class[cid]]]\n            points_indices[cid] = indices_for_points[indices[:num_points_per_class[cid]]]\n    r_points = []\n    r_labels = []\n    r_indices = []\n    for cid in range(2):\n        r_points.append(memory_points_list[cid])\n        r_labels.append(cid*torch.ones(memory_points_list[cid].shape[0], dtype=torch.long,\n                                   device=memory_points_list[cid].device))\n        r_indices.append(points_indices[cid])\n    memory_points = {}\n    memory_points['inputs'] = torch.cat(r_points, dim=0)\n    memory_points['true_labels'] = torch.cat(r_labels, dim=0)\n    if np.sum(num_points_per_class) > 2:\n        memory_points['indices'] = np.concatenate(np.array(r_indices), axis=0)\n    else:\n        memory_points['indices'] = r_indices\n    if additional_memory_data is not None:\n        memory_points['inputs'] = torch.cat((memory_points['inputs'], additional_memory_data[0]))\n        memory_points['true_labels'] = torch.cat((memory_points['true_labels'], additional_memory_data[1]))\n    if use_cuda:\n        memory_points['inputs'] = memory_points['inputs'].cuda()\n    memory_points['soft_labels'] = torch.softmax(model.forward(memory_points['inputs']), dim=-1)\n    return memory_points\n\n# Code for AdamReg and LBFGSReg optimizers (logic in step method)\n# Differentiates K-priors/Replay and applies weight-space regularization\n# This logic is present in both AdamReg.step and LBFGSReg.step\n\n# ... (inside AdamReg.step or LBFGSReg.step)\n# Loss term over memory points (only if K-priors or Replay)\nif closure_memory is not None:\n    preds = closure_memory()\n    self.total_datapoints_this_iter += len(preds)\n    preds_soft = torch.softmax(preds, dim=-1)\n    delta_logits = preds_soft.detach() - self.memory_labels\n    grad_message = torch.autograd.grad(preds, self.model.parameters(), grad_outputs=delta_logits)\n    grad_vec = []\n    for i in range(len(grad_message)):\n        grad_vec.append(grad_message[i].data.view(-1))\n    grad_vec = torch.cat(grad_vec, dim=-1)\n    grad.add_(grad_vec.detach())\n    # Weight regularisation\n    if adaptation_method == \"K-priors\" and self.prior_prec_old is not None:\n        grad.add_(self.previous_weights, alpha=-self.prior_prec_old)\n\n# Code from main.py for setting up memory labels and previous weights\noptimiser.previous_weights = base_model.return_parameters()\nif adaptation_method == \"K-priors\":\n    memory_points['labels'] = memory_points['soft_labels']\nelif adaptation_method == \"Replay\":\n    memory_points['labels'] = torch.nn.functional.one_hot(memory_points['true_labels'])\noptimiser.memory_labels = memory_points['labels']",
        "experimental_info": "The experiments evaluate K-priors and Replay methods on adaptation tasks for both Generalized Linear Models (GLMs) and Deep Learning models.\n\n**Datasets:**\n*   **Adult:** UCI Adult dataset for binary classification.\n*   **USPS Binary:** USPS dataset, classifying odd vs. even digits.\n\n**Network Architectures:**\n*   **Linear Model (GLM):** Logistic Regression, used for the Adult dataset and some USPS experiments. Uses `LBFGSReg` optimizer.\n*   **MLP (Multi-Layer Perceptron):** Deep learning model with one or two hidden layers of 100 units, used for USPS experiments. Uses `AdamReg` optimizer.\n\n**Adaptation Tasks:**\n*   **Add Data:** New data (or classes, e.g., digit '9' for USPS) is added to the training set.\n*   **Remove Data:** A subset of data (e.g., digit '8' for USPS, or top `h'(f)` points for Adult) is removed.\n*   **Change Regulariser:** The L2 regularization strength (weight_decay/prior_prec) is changed.\n    *   Adult/USPS Linear: From 50 to 5.\n    *   USPS MLP: From 5 to 10.\n*   **Change Model:** The model architecture is changed.\n    *   Linear Model: Polynomial degree of features changes from 2 to 1.\n    *   MLP: Number of hidden layers changes (e.g., from two 100-unit layers to one 100-unit layer).\n\n**Memory Selection Strategy:**\n*   For limited memory, 'memorable past' examples are selected based on the highest derivative `h'(f)` (calculated as `softmax_hessian` sum over the output dimension) from the *base model* (model trained on the initial task).\n\n**Memory Sizes:**\n*   The fraction of base task data stored in memory is varied in experiments: [1.0, 0.5, 0.2, 0.1, 0.07, 0.05, 0.02].\n\n**Regularization Parameters:**\n*   **`prior_prec` (L2 weight_decay):**\n    *   Adult: 5\n    *   USPS Linear: 50\n    *   USPS MLP: 5\n*   **`prior_prec_old` (for K-priors weight-space term `Dw(w||w*)`):** Set to the `prior_prec` of the base task, or specifically adjusted for 'change_regulariser' tasks.\n\n**K-prior vs. Replay Distinction:**\n*   **K-priors:** Use soft labels (output of `softmax(model.forward(memory_inputs))` from the base model) as targets for the function-space regularizer (`Df`). They also include a weight-space L2 regularizer towards the previous weights `w*` (`Dw`).\n*   **Replay:** Uses hard (true, one-hot encoded) labels for the function-space regularizer. It does not include the explicit weight-space regularizer towards `w*` (`Dw`).\n\n**Optimization:**\n*   **Optimizers:** LBFGS (`LBFGSReg`) for Linear models, Adam (`AdamReg`) for MLPs.\n*   **Learning Rate:** 0.005 for Adult/USPS MLP, 0.1 for USPS Linear.\n*   **Epochs:** 1000 for Adult/USPS MLP, 300 for USPS Linear.\n\n**Evaluation:**\n*   Each experiment is repeated for 3 runs with different random seeds (`seed_init + random_run`).\n*   Test accuracy is reported on the adaptation task's test set."
      }
    },
    {
      "title": "Directional Smoothness and Gradient Methods: Convergence and Adaptivity",
      "abstract": "We develop new sub-optimality bounds for gradient descent (GD) that depend on\nthe conditioning of the objective along the path of optimization rather than on\nglobal, worst-case constants. Key to our proofs is directional smoothness, a\nmeasure of gradient variation that we use to develop upper-bounds on the\nobjective. Minimizing these upper-bounds requires solving implicit equations to\nobtain a sequence of strongly adapted step-sizes; we show that these equations\nare straightforward to solve for convex quadratics and lead to new guarantees\nfor two classical step-sizes. For general functions, we prove that the Polyak\nstep-size and normalized GD obtain fast, path-dependent rates despite using no\nknowledge of the directional smoothness. Experiments on logistic regression\nshow our convergence guarantees are tighter than the classical theory based on\n$L$-smoothness.",
      "full_text": "Directional Smoothness and Gradient Methods: Convergence and Adaptivity Aaron Mishkin∗ Stanford University amishkin@cs.stanford.edu Ahmed Khaled∗ Princeton University ahmed.khaled@princeton.edu Yuanhao Wang Princeton University yuanhaoa@princeton.edu Aaron Defazio FAIR, Meta AI adefazio@meta.com Robert M. Gower CCM, Flatiron Institute gowerrobert@gmail.com Abstract We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness. 1 Introduction Gradient methods for differentiable functions are typically analyzed under the assumption that f is L-smooth, meaning ∇f is L-Lipschitz continuous. This condition implies f is upper-bounded by a quadratic and guarantees that gradient descent (GD) with step-size η <2/L decreases the optimality gap at each iteration (Bertsekas, 1997). However, experience shows that GD can still decrease the objective when f is not L-smooth, particularly for deep neural networks (Bengio, 2012; Z. Li et al., 2020; J. Cohen et al., 2021). Even for functions verifying smoothness, convergence rates are often pessimistic and fail to predict optimization speed in practice (Paquette et al., 2023). One alternative to global smoothness is local Lipschitz continuity of the gradient (“local smoothness”). Local smoothness assumes different Lipschitz constants hold for different neighbourhoods, which avoids global assumptions and improves rates. However, such analyses typically rely on boundedness of the iterates and then use local smoothness to obtain L-smoothness over a compact set (Malitsky and Mishchenko, 2020). Boundedness is guaranteed in several ways: Junyu Zhang and Hong (2020) break optimization into stages, Patel and Berahas (2022) use stopping-times, and Lu and S. Mei (2023) employ a line-search. Unfortunately, these approaches modify the underlying optimization algorithm, require local smoothness oracles (Park et al., 2021), or rely on highly complex arguments. In contrast, we prove simple rates for GD without global smoothness by deriving bounds of the form, f(xk+1) ≤ f(xk) + ⟨∇f(xk), xk+1 − xk⟩ + M(xk+1, xk) 2 ∥xk+1 − xk∥2 2, (1) ∗Equal contribution. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2403.04081v2  [cs.LG]  13 Jan 202550 100 150 200 Iterations 10−2 100 102 Optimality Gap ionosphere 10 20 30 40 Iterations 10−10 10−6 10−2 mammographic 1/M(xk+1, xk) Polyak Bound (L-Smooth) Bound (1/M(xk+1, xk)) Bound (Polyak) Figure 1: Comparison of actual (solid lines) and theoretical (dashed lines) convergence rates for GD with (i) step-sizes strongly adapted to the directional smoothness (ηk = 1/M(xk+1, xk)) and (ii) the Polyak step-size. Both problems are logistic regressions on UCI repository datasets (Asuncion and Newman, 2007). Our bounds using directional smoothness are tighter than those based on global L- smoothness of f and adapt to the optimization path. For example, on mammographic our theoretical rate for the Polyak step-size concentrates rapidly exactly when the optimizer shows fast convergence. where the directional smoothness function M(xk+1, xk) depends only on properties of f along the chord between xk and xk+1. Our sub-optimality bounds provide a path-dependent perspective on GD and are tighter than conventional analyses when the step-size sequence is adapted to the directional smoothness, meaning ηk < 2/M(xk+1, xk). See Figure 1 for two real-data examples highlighting our improvement over classical rates. We summarize all our contributions as follows. Directional Smoothness. We introduce three constructive directional smoothness functions M(x, y). The first, point-wise smoothness, depends only on the end-points x, yand is easily computed, while the second, path-wise smoothness, yields a tighter bound, but depends on the chord C = {αx + (1 − α)y : α ∈ [0, 1]}. The last function, which we call the optimal point-wise smoothness, is both easy-to-evaluate and provides the tightest possible quadratic upper bound. Sub-optimality bounds. We leverage directional smoothness functions to prove new sub-optimality bounds for GD on convex functions. Our bounds are localized to the GD trajectory, hold for any step-size sequence, and are tighter than the classic analysis using L-smoothness. They are also more general since we do not need to assume that f is globally L-smooth to show progress; all we require is a sequence of step-sizes adapted to the directional smoothness function. Furthermore, our approach extends naturally to acceleration, allowing us to prove optimal rates for (strongly)-convex functions. Adaptive Step-Sizes in the Quadratic Case. In the general setting, computing step-sizes which are adapted to the directional smoothness requires solving a challenging non-linear root-finding problem. For quadratic problems, we show that the ideal step-size that satisfies ηk = 1/M(xk+1, xk) is the Rayleigh quotient and is connected to the hedging algorithm (Altschuler and Parrilo, 2023). Exponential Search. Moving beyond quadratics, we prove that the equation ηk = 1/M(xk+1, xk) admits a solution under mild conditions, meaning ideal step-sizes can be computed using Newton’s method. Since computing these step-sizes is typically impractical, we adapt exponential search (Carmon and Hinder, 2022) to obtain similar path-dependent complexities up to a log-log penalty. Polyak and Normalized GD.More importantly, we show that the Polyak step-size (Polyak, 1987) and normalized GD achieve fast, path-dependent rates without knowledge of the directional smoothness. Our analysis reveals that the Polyak step-size adapts to any directional smoothness to obtain the tightest possible convergence rate. This property is not shared by constant step-size GD and may explain the superiority of the Polyak step-size in many practical settings. 1.1 Additional Related Work Directional smoothness is a relaxation of non-uniform smoothness (J. Mei et al., 2021), which restricts the smoothness function M to depend only on x, the origin point. J. Mei et al. (2021) leverage non-uniform smoothness and a non-uniform Łojasiewicz inequality to break lower-bounds for first-order optimization. Similarly, Berahas et al. (2023) show that a weak local smoothness oracle can break lower bounds for gradient methods. A major advantage of our work over such oracle-based approaches is that we construct explicit directional smoothness functions that are easy to evaluate. 2Similar to non-uniform smoothness, Grimmer (2019) and Orabona (2023) consider Hölder-type growth conditions with constants that depend on a neighbourhood of x. Since directional smoothness is stronger than and implies these Hölder error bounds, our M functions can be leveraged to make their results fully explicit (the Hölder bounds are non-constructive). Finally, while they also analyze normalized GD, our rates are anytime and do not use online-to-batch reductions like Orabona (2023). Directional smoothness is also related to(L0, L1)-smoothness (Jingzhao Zhang et al., 2020; B. Zhang et al., 2020), which can be interpreted as a directional smoothness function with exponential depen- dence on the distance between x and y. The extension of (L0, L1)-smoothness to (r, l)-smoothness by H. Li et al. (2023) shows how to bound sequences of such directional smoothness functions, even for accelerated methods. These approaches are complementary to ours and showcase a setting where directional smoothness leads to concrete convergence rates. Our work is most closely connected to that by Malitsky and Mishchenko (2020), who use a smoothed version of M(x, y) to set the step-size. Vladarean et al. (2021) apply a similar smoothed step-size scheme to primal-dual hybrid gradient methods, while Zhao and Huang (2024) relate directional smoothness to Barzilai-Borwein updates (Barzilai and Borwein, 1988) and Vainsencher et al. (2015) use local smoothness over neighbourhoods of the global minimizer to set the step-size for SVRG. Finally, we note that adaptivity to directional smoothness is different from adaptivity to the sequence of observed gradients obtained by methods such as Adagrad (Duchi et al., 2010; Streeter and McMahan, 2010). Adagrad and its variants are most useful when the gradients are bounded, such as in Lipschitz optimization, although they can also be used to obtain rates for smooth functions (Levy, 2017). We do not address adaptivity to gradients in this work. 2 Directional Smoothness We say that a convex functionf is L-smooth if for all x, y∈ Rd, f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + L 2 ∥y − x∥2 2. (2) Minimizing this quadratic upper bound in y gives the classical GD update with step-size ηk = 1/L. However, this viewpoint leads to rates which depend on the global, worst-case growth off. This is both counter-intuitive and undesirable because the iterates of GD, xk+1 = xk − ηk∇f(xk), depend only on local properties of f. Ideally, the analysis should also depend only on the local conditioning along the path {x1, x2, . . .}. Towards this end, we generalize the smoothness upper-bound as follows. Definition 2.1. We call M : Rd,d → R+ a directional smoothness function for f if for all x, y∈ Rd, f(y) ≤ f(x)+⟨∇f(x), y−x⟩ + M(x, y) 2 ∥y−x∥2. (3) If a function is L-smooth, then M(x, y) = L is a trivial choice of directional smoothness function. In the rest of this section, we construct different M functions that provide tighter bounds on f while still being possible to evaluate. The first is the point-wise directional smoothness, D(x, y) := 2∥∇f(y) − ∇f(x)∥2 ∥y − x∥2 . (4) Point-wise smoothness is a directional estimate of L and satisfies D(x, y) ≤ 2L. Indeed, L can be equivalently defined as the supremum ofD(x, y)/2 over the domain of f (Beck, 2017). If f is convex and differentiable, then D(x, y) is a directional smoothness function according to Definition 2.1. Lemma 2.2. If f is convex and differentiable, then the point-wise directional smoothness satisfies, f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + D(x, y) 2 ∥y − x∥2 2. (5) See Appendix A (we defer all proofs to the relevant appendices). In the worst-case, the point-wise directional smoothness D is weaker than the standard upper-bound M(x, y) = L by a factor of two. This is not an artifact of the analysis and is generally unavoidable, as the next proposition shows. Proposition 2.3. There exists a convex, differentiablef and x, y∈ Rd such that if t <2, then f(y) > f(x) + ⟨∇f(x), y− x⟩ + t∥∇f(x) − ∇f(y)∥ 2∥y − x∥2 ∥y − x∥2 2. (6) 3xk xk+1 x∗ f(x) Actual Progress L-Smooth Mk-Smooth Figure 2: Illustration of GD with ηk = 1 /L. Even though this step-size exactly minimizes the upper-bound from L-smoothness, Mk directional smoothness better predicts the progress of the gradient step because Mk ≪ L. Our rates improve on L-smoothness because of this tighter bound. While the point-wise smoothness is easy to compute, this additional factor of two can make Equa- tion (5) looser than L-smoothness — on isotropic quadratics, for example. As an alternative, we define the path-wise directional smoothness, A(x, y) := sup t∈[0,1] ⟨∇f(x+t(y−x))−∇f(x), y−x⟩ t∥y−x∥2 , (7) and show it verifies the quadratic upper-bound and satisfies Definition 2.1 even without convexity. Lemma 2.4. For any differentiable functionf, the path-wise smoothness (7) satisfies f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + A(x, y) 2 ∥y − x∥2 2. (8) Path smoothness is tighter than point-wise smoothness since A(x, y) ≤ D(x, y), but hard to compute because it depends on the chord between x and y. That is, it depends on the properties off on the line {tx + (1 − t)y : t ∈ [0, 1]} rather than solely on the points x and y like the point-wise smoothness. Point-wise and path-wise smoothness are constructive, but they may not yield the tightest bounds in all situations. The tightest directional smoothness function, which we call the optimal point-wise smoothness, is the smallest number for which the quadratic upper bound holds, H(x, y) = |f(y) − f(x) − ⟨∇f(x), y− x⟩| 1 2 ∥y − x∥2 (9) By definition, H is the tightest possible directional smoothness function; it lower bounds any constant C that satisfies the quadratic bound (2). Thus, H(x, y) ≤ M(x, y) for any smoothness function M. The directional smoothness functions introduced in this section represent different trade-offs between computability and tightness. The optimal point-wise smoothness H(x, y) requires access to both the function and gradient values, whereas the point-wise directional-smoothness D(x, y) requires only access to the gradients and convexity. In contrast, the path-wise direction smoothnessA(x, y) satisfies Lemma 2.4 with or without convexity, but may be hard to evaluate. 3 Path-Dependent Sub-Optimality Bounds Using directional smoothness, we obtain a descent lemma which depends only on local geometry, f(xk+1) ≤ f(xk) − \u0012 ηk − η2 kM(xk, xk+1) 2 \u0013 ∥∇f(xk)∥2 2. (10) See Lemma A.1. If ηk < 2/M(xk, xk+1), then GD is guaranteed to decrease the function value and we call ηk adapted to M(xk, xk+1). However, computing adapted step-sizes is not always straight- forward. For instance, finding ηk = 1/M(xk, xk+1(ηk)) requires solving a non-linear equation. The rest of this section leverages directional smoothness to derive new guarantees for GD with arbi- trary step-sizes. We emphasize that these results are sub-optimality bounds, rather than convergence rates; a sequence of adapted step-sizes is required to convert our propositions into a convergence theory. As a trade-off, our bounds reflect the locality of GD, rather than treating it as a global method. 4We start with the case when f has lower curvature. Instead of using strong convexity or the PL- condition (Karimi et al., 2016), we propose the directional strong convexity constant, µ(x, y)= inf t∈[0,1] ⟨∇f(x+t(y−x))−∇f(x), y−x⟩ t∥y − x∥2 2 . (11) If f is convex, then µ(x, y) ≥ 0 and it verifies the standard lower-bound from strong convexity, f(y) ≥ f(x) + ⟨∇f(x), y− x⟩ + µ(x, y) 2 ∥y − x∥2 2. (12) Moreover, we have µ(x, y) ≥ µ when f is µ–strongly convex. We prove two bounds for convex functions using directional strong convexity. For brevity, we denote Mi := M(xi, xi+1), µi := µi(xi, x∗), δi = f(xi) − f(x∗), and ∆i = ∥xi − x∗∥2 2, where x∗ is a minimizer of f. Proposition 3.1. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, δk ≤ \"Y i∈G (1 + ηiλiµi) # δ0 + X i∈B   Y j>i,j∈G (1 + ηjλjµj)   ηiλi 2 ∥∇f(xi)∥2 2, (13) where λi =ηiMi−2, G = {i : ηi <2/Mi}, and B = [k]\\G. The analysis splits iterations into good steps G, where ηk is adapted to the directional smoothness, and bad steps B, where the step-size is too large and GD may increase the optimality gap. When f is L-smooth and µ-strongly convex, using the step-size sequence ηk = 1/L gives f(xk+1) − f(x∗) ≤ \" kY i=0 \u0012 1 − µi (2 − Mi/L) L \u0013# (f(x0) − f(x∗)) (14) where µi (2 − Mi/L) ≥ µ. Thus, Equation (13) gives at least as tight a rate as standard assumptions by localizing to the convergence path using any directional smoothness M. When Mi < L, the gap in constants yields a strictly improved rate (see Figure 2). We also prove a more elegant bound. Proposition 3.2. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, ∆k ≤ \" kY i=0 |1 − µiηi| 1 + µi+1ηi # ∆0 + kX i=0  Y j>i |1 − µjηj| 1 + µj+1ηj   \u0000 Miη3 i − η2 i \u0001 1 + µi+1ηi ∥∇f(xi)∥2 2. (15) Unlike Proposition 3.1, this analysis shows linear progress at each iteration and does not divide k into good steps and bad steps. In exchange, the second term in Equation (15) reflects how much convergence is degraded when ηk is not adapted to the directional smoothness function M. We conclude this section with a bound for when there is no lower curvature, meaning µi = 0. Proposition 3.3. Let xk =Pk i=0 ηixi+1/Pk i=0 ηi. If f is convex and differentiable, then GD satisfies, f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk i=0 ηi + Pk i=0 η2 i (ηiMi − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . (16) Eq. (16) is faster than standard analyses whenever Mi < L; it will be a key tool in the next sections. 3.1 Path-Dependent Acceleration Now we show that directional smoothness can also be used to derive path-dependent sub-optimality bounds for accelerated algorithms — that is, methods obtaining optimal rates for smooth, convex optimization. In particular, we study Nesterov’s accelerated gradient descent (AGD) (Nesterov, 1983) and prove that directional smoothness leads to tighter rates given adapted step-sizes. Throughout this section we assume that f is µ-strongly convex with µ = 0 when f is merely convex. Although our analysis uses estimating sequences (Nesterov et al., 2018), we state AGD in the following “momentum” formulation, where yk is the momentum and αk the momentum parameter, xk+1 = yk − ηk∇f(yk) α2 k+1 = (1 − αk+1)α2 k ηk+1 ηk + ηk+1αk+1µ yk+1 = xk+1 + αk(1 − αk) α2 k + αk+1 (xk+1 − xk) . (17) 5If ηk ≤ 1/M(xk, xk+1), then Equation (10) combined with 1 − ηkM(xk, xk+1)/2 ≥ 1/2 implies, f(xk+1) ≤ f(yk) − ηk 2 ∥∇f(yk)∥2 2. (18) Our analysis leverages the fact that this descent condition for xk+1 is the only connection between the smoothness of f and the convergence rate of AGD. Since Equation (18) depends only on the step-size ηk, we can replace L within the analysis of AGD with a sequence of adapted step-sizes. The following theorem controls the effect of these step-sizes to obtain path-dependent bounds. Theorem 3.4. Suppose f is differentiable, µ–strongly convex and AGD is run with adapted step-sizes ηk ≤ 1/Mk. If µ >0 and α0 = √η0µ, then AGD obtains the following accelerated rate: f(xk+1) − f(x∗) ≤ kY i=0 (1 − √µηi) h f(x0) − f(x∗) + µ 2 ∥x0 − x∗∥2 2 i . (19) Let ηmin = mini∈[k] ηi. If µ ≥ 0 and α0 ∈ (√µη0, c), where c is the maximum value of α0 for which γ0 = α2 0−η0α0µ η0(1−α0) satisfies γ0 < 3/ηmin + µ, then AGD obtains the following rate: f(xk+1) − f(x∗) ≤ 4 ηmin(γ0 − µ)(k + 1)2 h f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 i . (20) If ηk = 1/Mk > 1/L, then these rates are strictly faster than those obtained under L-smoothness and Theorem 3.4 shows that AGD provably benefits from taking the largest possible steps given the local geometry of f. However, obtaining accelerated rates when µ = 0 requires prior knowledge of the minimum step-size; while this is straightforward for L-smooth functions, it is not clear how to extend such result to non-strongly convex acceleration with locally Lipschitz gradients. For example, while H. Li et al. (2023) show that the (r, l)-smoothness (a valid directional smoothness function) is bounded over the iterate trajectory, their rate does not adapt to the optimization path. 4 Adaptive Learning Rates Converting our sub-optimality bounds into convergence rates requires adapted step-sizes satisfying ηk < 2/M(xk, xk+1). Given an adapted step-size, the directional descent lemma (Equation (10)) implies GD decreases f and we can obtain fast rates if the step-sizes are bounded below. However, xk+1 is itself a function of ηk, meaning adapted step-sizes are not straightforward to compute. For L-smooth f, the different directional smoothness functions M introduced in Section 2 satisfy M(xk, xk+1) ≤ 2L. This implies ηk < 1 L is trivially adapted. As such step-sizes don’t capture local properties of f, we introduce the notion of strongly adapted step-sizes, which satisfy ηk = 1/M(xk+1(ηk), xk). (21) Equation (10) implies GD with a strongly adapted step-size makes guaranteed progress as, f(xk+1) ≤ f(xk) − [2M(xk+1, xk)]−1 ∥∇f(xk)∥2 2. (22) This progress is greater than that guaranteed by L-smoothness when M(xk, xk+1) < Land holds even when f is not L-smooth. However, it is not clear a priori if (i) strongly adapted step-sizes exist or if (ii) any iterative method achieves the progress in Eq.(21). Surprisingly, we provide a positive answer to both questions. Strongly adapted ηk are computable and we also prove GD with the Polyak step-size adapts to any choice of directional smoothness, including the optimal point-wise smoothness. Before presenting this strong result, we consider the illustrative case of quadratic minimization. 4.1 Adaptivity in Quadratics Now we show that step-sizes adapted to both the point-wise smoothness M and the path-wise smoothness A exist when f is quadratic. Let f(x) = x⊤Bx/2 − c⊤x, where B is positive semi- definite. Assuming {ηk} is strongly adapted to the directional smoothness, Equation (16) implies f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk i=0 ηi = ∥x0 − x∗∥2 2 2 Pk i=0 1 M(xi,xi+1) ≤ ∥x0 − x∗∥2 2 2(k + 1) Pk i=0 M(xi, xi+1) k + 1 , (23) 6101 103 Iteration 104 105 106 Optimality Gap 101 103 Iteration 102 107 Point-Wise Smoothness 101 103 Iteration 10 3 2 × 10 3 3 × 10 3 4 × 10 3 Adapted Step-Sizes 1/L 1/Dk 1/Ak 2/L Figure 3: Performance of GD with different step-size rules for a synthetic quadratic problem. We run GD for 20,000 steps on 20 random quadratic problems with L = 1000 and Hessian skew. Left- to-right, the first plot shows the optimality gap f(xk) − f(x∗), the second shows the point-wise directional smoothness D(xk, xk+1), and the third shows step-sizes used by the different methods. where we used ηiMi = 1 as well as Jensen’s inequality. This guarantee depends solely on the average directional smoothness along the optimization trajectory {x0, x1, . . .}. When f is quadratic, we can exactly compute these smoothness constants. In particular, the point-wise directional smoothness is, D(xi, xi+1) = 2∥B∇f(xi)∥2/∥∇f(xi)∥2. Notably, D(xi, xi+1) has no dependence on xi+1 and the corresponding strongly adapted step-size is given by ηi = ∥∇f(xi)∥2/(2∥B∇f(xi)∥2) — see Lemma C.1. Remarkably, this expression recovers the step-size proposed by Dai and Yang (2006), who show it approximates the Cauchy step-size and converges to the “edge-of-stability” (J. Cohen et al., 2021) at 2/L as k → ∞. Combining this simple expression with Equation (23) gives a fast, non-asymptotic convergence rate for GD and new theoretical justification for their work. We can also compute the path-wise directional smoothness in closed form. As Lemma C.2 shows, A(xi, xi+1) = ∇f(xi)⊤B∇f(xi)/∇f(x)⊤∇f(x), and ηi = ∇f(xi)⊤∇f(xi)/[∇f(xi)⊤B∇f(xi)] is the well-known Cauchy step-size. Path-wise directional smoothness thus provides another interpretation (and convergence guarantee) for the Cauchy step-size, which is traditionally derived by minimizing f(x − η∇f(x)) in η. 4.2 Adaptivity for Convex Functions In the last subsection, we proved that strongly adapted step-sizes for the point-wise and path-wise directional smoothness functions have closed-form expressions when f is quadratic. Moreover, these step-sizes recover two classical schemes from the optimization literature, giving them new justification and fast convergence rates. Now we consider the existence of strongly adapted step-sizes for general convex functions. Our first result gives simple conditions for Equation (21) to have at least one solution when M is the point-wise directional smoothness. Proposition 4.1. If f is convex and continuously differentiable, then either (i) f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/D(x, x− η∇f(x)). The next proposition uses a similar argument with slightly stronger conditions to show existence of strongly adapted step-sizes for the path-wise smoothness. Proposition 4.2. If f is convex and twice continuously differentiable, then either (i)f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/A(x, x− η∇f(x)). Propositions 4.1 and 4.2 do not assume the global smoothness; although neither proof is constructive, it is possible to compute strongly adapted step-sizes for the point-wise directional smoothness using root-finding methods. We show in Section 5 that iff is twice differentiable, then strongly adapted step-sizes can be found via Newton’s method using only Hessian-vector products, ∇2f(x)∇f(x). 4.2.1 Exponential Search Now we show that the exponential search algorithm developed by Carmon and Hinder (2022) can be used to find step-sizes that adapt on average to the directional smoothness. Consider a fixed 7optimization horizon k and denote by xi(η) the sequence of iterates obtained by running GD from x0 using a fixed step-size η. Define the criterion function, ψ(η) = Pk i=0 ∥∇f(xi(η))∥2 2 Pk i=0 M(xi(η), xi+1(η))∥∇f(xi(η))∥2 2 , (24) and suppose that we have a step-size η that satisfies ψ(η)/2 ≤ η ≤ ψ(η). Using these bounds in Proposition 3.3 yields the following convergence rate, f(xk) − f∗ ≤ h k Pk i=0 M(xi,xi+1)∥∇f(xi)∥2 2Pk i=0 ∥∇f(xi)∥2 2 i ∥x0 − x∗∥2 2. (25) While η does not adapt to each directional smoothness M(xi, xi+1) along the path, it adapts to a weighted average of the directional smoothness constants, where the weights are the observed squared gradient norms. This is always smaller than the maximum directional smoothness along the trajectory and can be much smaller than the global smoothness. Furthermore, we have reduced our problem to finding η ∈ [ψ(η)/2, ψ(η)], which is similar to the problem Carmon and Hinder (2022) solve with exponential search. We adopt their approach as Algorithm 1 and give a convergence guarantee. Theorem 4.3. Assume f is convex and L-smooth. Then Algorithm 1 with η0 > 0 requires at most 2K(log log(2η0/L) ∨ 1) iterations of GD and in the last run it outputs a step-size η and point xK = 1 K PK−1 i=0 xi(η) such that exactly one of the following holds: Case 1: η = η0 and f(xK) − f(x∗) ≤ ∥x0 − x∗∥2 2 2Kη0 Case 2: η ̸= η0 and f(xK) − f∗ ≤ ∥x0 − x∗∥2 2 2K \"Pk i=0 Mi∥∇f(x′ i)∥2 2 Pk i=0 ∥∇f(x′ i)∥2 2 # , where Mi def = M(x′ i, x′ i+1) and x′ i are the iterates generated by GD with step-size η′ ∈ [η, 2η]. Theorem 4.3 requires f to be L-smooth, but has only a log log dependence on the global smoothness constant. Moreover, the rate scales with the weighted average of smoothness constants along a very close trajectory {x′ 1, x′ 2, . . .}. In the next section, we give convergence bounds that depend on the unweighted average of the directional smoothness constants along the actual optimization trajectory. 4.2.2 Polyak’s Step-Size Rule Our theory so-far suggests using strongly adapted step-sizes, but neither root-finding nor exponential search are practical methods for large-scale optimization. Thus, we now consider other step-size selection rules which may leverage directional smoothness. In particular, the Polyak step-size sets, ηk = γ (f(xk) − f(x∗)) /∥∇f(xk)∥2 2, (26) for some γ >0, which is optimal for smooth and non-smooth optimization (Hazan and Kakade, 2019) given knowledge of f(w∗). Surprisingly, we show that GD with the Polyak step-size also achieves the same guarantee as strongly adapted step-sizes without knowledge of the directional smoothness. Theorem 4.4. Suppose that f is convex and differentiable and let M be any directional smoothness function for f. Let ∆0 := ∥x0 − x∗∥2 2. Then GD with the Polyak step-size and γ ∈ (1, 2) satisfies f(xk) − f(x∗) ≤ c(γ)∆0 2 Pk−1 i=0 M(xi, xi+1)−1 , (27) where c(γ) = γ/(2 − γ)(γ − 1) and xk = Pk−1 i=0 \u0002 M(xi, xi+1)−1xi \u0003 / \u0010Pk−1 i=0 M(xi, xi+1)−1 \u0011 . Theorem 4.4 measures sub-optimality at an average iterate obtained using the directional smoothness. However, it also holds for the best iterate, ˆxk = arg mini∈[k] f(xi), meaning no knowledge of the directional smoothness is required to obtain the guarantee. We prove an alternative guarantee for the Polyak step-size in Theorem D.2, where the progress depends on the sum of step-sizes rather than on the average directional smoothness. This shows that the step-size in Equation (26) can itself be viewed as a measure of local smoothness, albeit without formal justification. 8Compared with the standard guarantee for the Polyak step-size underL-smoothness, f(xk)−f(x∗) ≤ 2L∆0/k (Hazan and Kakade, 2019), our analysis in Theorem 4.4 with the choice γ = 1.5 yields f(xk) − f(x∗) ≤ 3∆0 Pk−1 i=0 M(xi, xi+1)−1 ≤ 3∆0 k Pk−1 k=0 M(xi, xi+1) k , where the second bound follows from Jensen’s inequality and shows that the convergence depends on the average directional smoothness along the trajectory, rather than on L. If f is L-smooth, then M(xk, xk+1) ≤ L immediately recovers the classic rate for Polyak’s method up to a 3/2 constant factor. If f is not L-smooth, but M(xk, xk+1) is bounded, then Equation (27) generalizes the O(1/k) rate proved concurrently by Takezawa et al. (2024), but for any choice of directional smoothness (of which (L0, L1)-smoothness (Jingzhao Zhang et al., 2020) is but one). Comparison with strongly adapted step-sizes.As we saw for quadratics, strongly adapted step-sizes for any directional smoothness function allow us to obtain the following convergence rate, f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk−1 i=0 M(xi, xi+1)−1 . This is matches the guarantee given by Equation (27) up to constant factors. As a result, we give a positive answer to the question posed earlier in this section: GD with the Polyak step-size achieves the same convergence for any smoothness function M as GD with step-sizes strongly adapted to M. Application to the optimal directional smoothness. Theorem 4.4 holds for every directional smoothness function M. Therefore we can specialize Equation (27) with the optimal point-wise directional smoothness H (as defined in Equation (4)) and γ = 1.5 to get the guarantee, min i∈[k−1] [f(xi) − f(x∗)] ≤ 3∥x0 − x∗∥2 2Pk−1 i=0 H(xi, xi+1)−1 . (28) This rate requires computing the iterate with the minimum function value, but that is easy to track during optimization. Unlike our previous results, Equation (28) requires no access to the optimal point-wise smoothness, yet obtains a dependence on the tightest constant possible. 4.3 Normalized Gradient Descent Now we change directions slightly and study normalized GD, whose convergence also depends on the directional smoothness. Normalized GD uses step-sizes which are divided by the gradient magnitude, xk+1 = xk − ηk ∥∇f(xk)∥2 ∇f(xk). (29) Our next theorem shows that normalized GD obtains a guarantee which depends solely on the average of the point-wise directional smoothness Dk := D(xk, xk+1) despite no explicit knowledge of Dk. Theorem 4.5. Suppose that f is convex and differentiable. Let D be the point-wise directional smoothness defined by Equation (4) and ∆0 := ∥x0 − x∗∥2 2. Then normalized GD with a sequence of non-increasing step-sizes ηk satisfies f(ˆxk) − f(x∗) ≤ ∆0 + Pk−1 i=0 η2 i 2k2 \u0012f(x0) η2 0 − f(x∗) η2 k−1 \u0013 + ∆0 + Pk−1 i=0 η2 i 2k k−1X i=0 M(xi, xi+1) k , (30) where ˆxk = arg mini∈[k−1] f(xi). If maxi∈[k−1] M(xi, xi+1) is bounded for all k (i.e. f is L- smooth), then for ηi = 1/ √ i we have f(ˆxk) − f(x∗) ∈ O(1/k) and for ηi = 1/ √ i we get the anytime result f(ˆxk) − f(x∗) ∈ O(log(k)/k). Theorem 4.5 gives a rate for normalized GD which is valid for any convex f without any dependence on global smoothness. However, does not adapt to any smoothness function like the Polyak step-size. 5 Experiments We evaluate the practical improvement of our convergence rates over those using L-smoothness on two logistic regression problems taken from the UCI repository (Asuncion and Newman, 2007). 90 100 200 Iterations 10□3 10□2 10□1 Optimality Gap ionosphere 0 100 200 Iterations 10□14 10□11 10□8 10□5 10□2 horse-colic 0 100 200 Iterations 10□2 10□1 ozone GD (1/L) GD (1/Dk) Polyak Norm. GD AdGD Figure 4: Comparison of GD withηk = 1/L, step-sizes strongly adapted to the point-wise smoothness (ηk = 1/D(xk, xk+1)), and the Polyak step-size against normalized GD (Norm. GD) and the AdGD method on three logistic regression problems. AdGD uses a smoothed version of the point-wise directional smoothness from the previous iteration to set ηk. We find that GD methods with adaptive step-sizes consistently outperform GD with ηk = 1/L and even obtain a linear rate on horse-colic. Figure 1 compares GD with strongly adapted step-sizes η = 1/Mk, where Mk is the point-wise smoothness, against GD with the Polyak step-size. We also plot the exact convergence rates for each method, Equation (16) and Equation (27), respectively, and compare against the classical guarantee for both methods. Our convergence rates are an order of magnitude tighter on the ionosphere dataset and display a remarkable ability to adapt to the path of optimization on mammographic. Figure 3 compares the performance of GD with strongly adapted step-sizes and with the fixed step- size ηk = 1/L for a synthetic quadratic with Hessian skew (R. Pan et al., 2022). Results are averaged over twenty random problems. We find that strongly adapted step-sizes lead to significantly faster convergence. Since Ak, Dk ≪ L, the adapted step-sizes are larger than 2/L, especially at the start of training; they eventually converge to2/L, indicating these methods operate at the edge-of-stability (J. Cohen et al., 2021; J. M. Cohen et al., 2022). This is consistent with Ahn et al. (2022) and Y . Pan and Y . Li (2023), who show local smoothness is correlated with edge-of-stability behavior. We conclude with a comparison of empirical convergence rates on three additional logistic regression problems from the UCI repository. We compare GD with ηk = 1/L, GD with step-sizes strongly adapted to the point-wise smoothness ( ηk = 1/Dk), GD with the Polyak step-size (Polyak), and normalized GD (Norm. GD) against the AdGD method (Malitsky and Mishchenko, 2020). The Polyak step-size performs best on every dataset butozone, where GD with ηk = 1/Dk solves the problem to high accuracy in just a few iterations. Thus, although Polyak step-sizes have the optimal dependence on directional smoothness, computing strongly adapted step-sizes can still be advantageous. 6 Conclusion We present new sub-optimality bounds for GD under novel measures of local gradient variation which we call directional smoothness functions. Our results hold for any step-sizes, improve over standard analyses when ηk is adapted to the choice of directional smoothness, and depend only on properties of f local to the optimization path. For convex quadratics, we show that computing step-sizes strongly adapted to directional smoothness functions is straightforward and recovers two well-known step-size schemes, including the Cauchy step-size. In the general case, we prove that an algorithm based on exponential search gives a weighted-version of the path-dependent convergence rate with no need for adapted step-sizes. We also show that GD with the Polyak step-size and normalized GD both obtain fast rates with no dependence on the global smoothness parameter. Crucially, the Polyak step-size adapts to any choice of directional smoothness, including the tightest possible parameter. 10Acknowledgements Aaron Mishkin was supported by NSF Grant DGE-1656518, by NSERC Grant PGSD3-547242-2020, and by an internship at the Center for Computational Mathematics, Flatiron Institute. We thank Si Yi Meng for insightful discussions during the preparation of this work and Fabian Schaipp for use of the step-back code. We also thank the anonymous reviewers for comments leading to improvements in Proposition 3.2 and the addition of Theorem D.2. References Ahn, Kwangjun, Jingzhao Zhang, and Suvrit Sra (2022). “Understanding the unstable convergence of gradient descent”. In: International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA. V ol. 162. Proceedings of Machine Learning Research, pp. 247–257. Altschuler, Jason M. and Pablo A. Parrilo (2023). “Acceleration by Stepsize Hedging I: Multi-Step Descent and the Silver Stepsize Schedule”. In: CoRR abs/2309.07879. Asuncion, Arthur and David Newman (2007). UCI machine learning repository. Barzilai, Jonathan and Jonathan M Borwein (1988). “Two-point step size gradient methods”. In: IMA journal of numerical analysis 8.1, pp. 141–148. Beck, Amir (2017). First-order methods in optimization. MOS-SIAM series on optimization. Philadel- phia : Philadelphia: Society for Industrial and Applied Mathematics ; Mathematical Optimization Society. ISBN : 978-161-197-4-9-9-7. Bengio, Yoshua (2012). “Practical Recommendations for Gradient-Based Training of Deep Archi- tectures”. In: Neural Networks: Tricks of the Trade - Second Edition. V ol. 7700. Lecture Notes in Computer Science, pp. 437–478. Berahas, Albert S., Lindon Roberts, and Fred Roosta (2023). “Non-Uniform Smoothness for Gradient Descent”. In: arXiv preprint arXiv:2311.08615 abs/2311.08615. Bertsekas, Dimitri P (1997). “Nonlinear programming”. In: Journal of the Operational Research Society 48.3, pp. 334–334. Bubeck, Sébastien et al. (2015). “Convex optimization: Algorithms and complexity”. In: Foundations and Trends® in Machine Learning 8.3-4, pp. 231–357. Carmon, Yair and Oliver Hinder (2022). “Making SGD Parameter-Free”. In:Conference on Learn- ing Theory, 2-5 July 2022, London, UK. V ol. 178. Proceedings of Machine Learning Research, pp. 2360–2389. Cohen, Jeremy et al. (2021). “Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability”. In: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Cohen, Jeremy M. et al. (2022). “Adaptive Gradient Methods At the Edge of Stability”. In: arXiv preprint arXiv:2207.14484 abs/2207.14484. Dai, Y . H. and X. Q. Yang (2006). “A New Gradient Method with an Optimal Stepsize Property”. In: Computational Optimization and Applications 33.1, pp. 73–88. Duchi, John C., Elad Hazan, and Yoram Singer (2010). “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization”. In: COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pp. 257–269. Fernández-Delgado, Manuel et al. (2014). “Do we need hundreds of classifiers to solve real world classification problems?” In: The journal of machine learning research 15.1, pp. 3133–3181. Grimmer, Benjamin (2019). “Convergence Rates for Deterministic and Stochastic Subgradient Methods without Lipschitz Continuity”. In: SIAM J. Optim. 29.2, pp. 1350–1365. Hazan, Elad and Sham Kakade (2019). “Revisiting the Polyak step size”. In: arXiv preprint arXiv:1905.00313. He, Kaiming et al. (2015). “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification”. In: Proceedings of the IEEE international conference on computer vision, pp. 1026–1034. Hogan, William W (1973). “Point-to-set maps in mathematical programming”. In: SIAM review 15.3, pp. 591–603. Karimi, Hamed, Julie Nutini, and Mark Schmidt (2016). “Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition”. In: Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16. Springer, pp. 795–811. 11Levy, Kfir Y . (2017). “Online to Offline Conversions, Universality and Adaptive Minibatch Sizes”. In: Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1613–1622. Li, Haochuan et al. (2023). “Convex and Non-convex Optimization Under Generalized Smoothness”. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural In- formation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Li, Zhiyuan, Kaifeng Lyu, and Sanjeev Arora (2020). “Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate”. In: Advances in Neural Informa- tion Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Liu, Dong C and Jorge Nocedal (1989). “On the limited memory BFGS method for large scale optimization”. In: Mathematical programming 45.1-3, pp. 503–528. Lu, Zhaosong and Sanyou Mei (2023). “Accelerated first-order methods for convex optimization with locally Lipschitz continuous gradient”. In: SIAM Journal on Optimization 33.3, pp. 2275–2310. Malitsky, Yura and Konstantin Mishchenko (2020). “Adaptive Gradient Descent without Descent”. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event. V ol. 119. Proceedings of Machine Learning Research, pp. 6702–6712. Mei, Jincheng et al. (2021). “Leveraging Non-uniformity in First-order Non-convex Optimization”. In: Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. V ol. 139. Proceedings of Machine Learning Research, pp. 7555–7564. Mishkin, Aaron, Mert Pilanci, and Mark Schmidt (2024). “Faster Convergence of Stochastic Acceler- ated Gradient Descent under Interpolation”. In: arXiv preprint arXiv:2404.02378. Nesterov, Yurii (1983). “A method for solving the convex programming problem with convergence rate O (1/k2)”. In: Dokl akad nauk Sssr. V ol. 269, p. 543. Nesterov, Yurii et al. (2018).Lectures on convex optimization. V ol. 137. Springer. Orabona, Francesco (2023). “Normalized Gradients for All”. In: arXiv preprint arXiv:2308.05621 abs/2308.05621. Pan, Rui, Haishan Ye, and Tong Zhang (2022). “Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums”. In: ICLR. Pan, Yan and Yuanzhi Li (2023). “Toward understanding why adam converges faster than sgd for transformers”. In: arXiv preprint arXiv:2306.00204. Paquette, Courtney et al. (2023). “Halting time is predictable for large models: A universality property and average-case analysis”. In: Foundations of Computational Mathematics 23.2, pp. 597–673. Park, Jea-Hyun, Abner J Salgado, and Steven M Wise (2021). “Preconditioned accelerated gradient descent methods for locally Lipschitz smooth objectives with applications to the solution of nonlinear PDEs”. In: Journal of Scientific Computing 89.1, p. 17. Paszke, Adam et al. (2019). “Pytorch: An imperative style, high-performance deep learning library”. In: Advances in neural information processing systems 32. Patel, Vivak and Albert S. Berahas (2022). “Gradient descent in the absence of global Lipschitz conti- nuity of the gradients: Convergence, divergence and limitations of its continuous approximation”. In: arXiv preprint arXiv:2210.02418. Polyak, Boris T (1987). “Introduction to optimization”. In. Streeter, Matthew and H. Brendan McMahan (2010). “Less Regret Via Online Conditioning”. In: arXiv preprint arXiv:1002.4862. Takezawa, Yuki et al. (2024). “Polyak Meets Parameter-free Clipped Gradient Descent”. In:CoRR abs/2405.15010. DOI : 10 . 48550 / ARXIV . 2405 . 15010. arXiv: 2405 . 15010. URL : https : //doi.org/10.48550/arXiv.2405.15010. Vainsencher, Daniel, Han Liu, and Tong Zhang (2015). “Local Smoothness in Variance Reduced Optimization”. In: Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2179–2187. Virtanen, Pauli et al. (2020). “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python”. In: Nature Methods 17, pp. 261–272. Vladarean, Maria-Luiza, Yura Malitsky, and V olkan Cevher (2021). “A first-order primal-dual method with adaptivity to local smoothness”. In: Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 6171–6182. 12Zhang, Bohang et al. (2020). “Improved Analysis of Clipping Algorithms for Non-convex Optimiza- tion”. In: Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Zhang, Jingzhao et al. (2020). “Why Gradient Clipping Accelerates Training: A Theoretical Justifica- tion for Adaptivity”. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Zhang, Junyu and Mingyi Hong (2020). “First-order algorithms without Lipschitz gradient: A sequential local optimization approach”. In: arXiv preprint arXiv:2010.03194. Zhao, Weijing and He Huang (2024). “Adaptive stepsize estimation based accelerated gradient descent algorithm for fully complex-valued neural networks”. In:Expert Systems with Applications 236, p. 121166. 13A Proofs for Section 2 Lemma 2.2. If f is convex and differentiable, then the point-wise directional smoothness satisfies, f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + D(x, y) 2 ∥y − x∥2 2. (5) Proof. By the convexity of f we have f(x) + ⟨∇f(x), y− x⟩ ≤f(y). Rearranging and then using Cauchy-Schwarz we get f(x) ≤ f(y) + ⟨∇f(x), x− y⟩ = f(y) + ⟨∇f(y), x− y⟩ + ⟨∇f(x) − ∇f(y), x− y⟩ ≤ f(y) + ⟨∇f(y), x− y⟩ + ∥∇f(x) − ∇f(y)∥∥x − y∥ = f(y) + ⟨∇f(y), x− y⟩ + D(x, y) 2 ∥x − y∥2. Lemma 2.4. For any differentiable functionf, the path-wise smoothness (7) satisfies f(y) ≤ f(x) + ⟨∇f(x), y− x⟩ + A(x, y) 2 ∥y − x∥2 2. (8) Proof. Starting from the fundamental theorem of calculus, f(y) − f(x) − ⟨∇f(x), y− x⟩ = Z 1 0 ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩dt ≤ Z 1 0 A(x, y)t∥x − y∥2 2dt = A(x, y) 2 ∥y − x∥2 2. which completes the proof. Proposition 2.3. There exists a convex, differentiablef and x, y∈ Rd such that if t <2, then f(y) > f(x) + ⟨∇f(x), y− x⟩ + t∥∇f(x) − ∇f(y)∥ 2∥y − x∥2 ∥y − x∥2 2. (6) Proof. Let Hf denote the optimal pointwise directional smoothness associated with some convex and differentiable function f : Rd → R (as defined in Equation (4)), and Df denote the pointwise directional smoothness associated with f (as defined in Equation (4)). For any t, the statement of (6) is equivalent to saying Hf > t∥∇f(x)−∇f(y)∥ ∥x−y∥ for all x, y∈ Rd and convex, differentiablef. Observe that Lemma 2.2 already shows that for all convex and differentiable functions f : Rd → R Hf (x, y) ≤ Df (x, y) = 2∥∇f(x) − ∇f(y)∥ ∥x − y∥ for all x, y∈ Rd. In order to show that this is tight, we suppose by the way of contradiction that there exists some 2 > t≥ 0 such that for all convex and differentiable functions f : Rd → R Hf (x, y) ≤ t · ∥∇f(x) − ∇f(y)∥ ∥x − y∥ (31) for all x, y∈ R. We shall show that no sucht exists by showing for each such t there exists a function ft such that Equation (31) does not hold. Consider fϵ(x) = √ x2 + ϵ2 for ϵ ≤ 1. The function f is differentiable. Moreover f′ ϵ(x) = x√ x2 + ϵ2 , f ′′ ϵ (x) = ϵ2 (ϵ2 + x2) 3 2 ≥ 0. 14Therefore f is convex. Let g(x) = |x|. Fix x = 1 and y = 0, we have |g(x) − g(y) − sign(y) · (x − y)| ≤ |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| + |g(x) − fϵ(x)| + |g(y) − fϵ(y)| + |(f′ ϵ(y) − sign(y)) · (x − y)| = |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| + \f\f\f1 − p 1 + ϵ2 \f\f\f + \f\f\f0 − √ ϵ2 \f\f\f + |(0 − 0) · (1 − 0)| ≤ |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| + 2ϵ. Now observe that g(x) − g(y) − sign(y) · (x − y) = |1| − |0| −0 · (1 − 0) = 1. Therefore |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| ≥1 − 2ϵ. (32) By definition we have 1 2 ∥x − y∥2 = 1 2 , therefore Hf (x, y) = |fϵ(x) − fϵ(y) − f′ ϵ(y) · (x − y)| 1 2 ∥x − y∥2 ≥ 2 − 4ϵ. (33) But by our starting assumption we have that there exists some t < 2 such that Hf (x, y) ≤ t∥f′(x)−f′(y)∥ |x−y| for all differentiable and convex functions f. Applying this to f = fϵ we get Hfϵ(1, 0) ≤ t|f′ ϵ(1) − f′ ϵ(0)| |1| = t · 1√ 1 + ϵ2 ≤ t. (34) Combining Equations (33) and (34) we have 2 − 4ϵ ≤ Hfϵ(1, 0) ≤ t Rearranging we get 2 − t ≤ 4ϵ Choosing ϵ = 2−t 8 > 0 we get a contradiction. It follows that the minimal t such that H(x, y) ≤ t|f′(x)−f′(y)| |x−y| for all convex and differentiable f is t = 2. Lemma A.1. One step of gradient descent with step-size ηk > 0 makes progress as f(xk+1) ≤ f(xk) − ηk \u0012 1 − ηkM(xk, xk+1) 2 \u0013 ∥∇f(xk)∥2 2. Proof. Starting from Equation (4), we have f(xk+1) ≤ f(xk) + ⟨∇f(xk), xk+1 − xk⟩ + M(xk, xk+1) 2 ∥xk+1 − xk∥2 2 = f(xk) − ηk∥∇f(xk)∥2 2 + η2 kM(xk, xk+1) 2 ∥∇f(xk)∥2 2 = f(xk) − ηk \u00121 − ηkM(xk, xk+1) 2 \u0013 ∥∇f(xk)∥2 2. B Proofs for Section 3 Lemma B.1. If f is convex, then for any x, y∈ Rd, f(y) ≥ f(x) + ⟨∇f(x), y− x⟩ + µ(x, y) 2 ∥y − x∥2 2. (35) If f is µ strongly convex, then µ(x, y) ≥ µ. 15Proof. The fundamental theorem of calculus implies f(x) − ⟨∇f(x), y− x⟩ = Z 1 0 ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩dt ≥ Z 1 0 µ(x, y)t∥x − y∥2 2dt = µ(x, y) 2 ∥y − x∥2 2. Note that we have implicitly used convexity to verify the inequality in the second line in the case where µ(x, y) = 0. Now assume that f is µ strongly convex. As a standard consequence of strong-convexity, we obtain: ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩ t∥x − y∥2 2 = ⟨∇f(x + t(y − x)) − ∇f(x), x+ t(y − x) − x⟩ t2∥x − y∥2 2 ≥ µ∥x − t(y − x) − x∥2 2 t2∥y − x∥2 2 = µ. Proposition 3.1. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, δk ≤ \"Y i∈G (1 + ηiλiµi) # δ0 + X i∈B   Y j>i,j∈G (1 + ηjλjµj)   ηiλi 2 ∥∇f(xi)∥2 2, (13) where λi =ηiMi−2, G = {i : ηi <2/Mi}, and B = [k]\\G. Proof. First note that λi < 0 for i ∈ Gand λi ≥ 0 for i ∈ B. We start from Equation (10), f(xk+1) ≤ f(xk) + ηk \u0012ηkM(xk, xk+1) 2 − 1 \u0013 ∥∇f(xk)∥2 2 = f(xk) + 1 k∈G · \u0014ηkλk 2 ∥∇f(xk)∥2 2 \u0015 + 1 k∈B · \u0014ηkλk 2 ∥∇f(xk)∥2 2 \u0015 ≤ f(xk) + 1 k∈G · [ηkλkµk (f(xk) − f(x∗))] + 1 k∈B · \u0014ηkλk 2 ∥∇f(xk)∥2 2 \u0015 , where we used that directional strong convexity gives ∥∇f(xk)∥2 2 ≥ 2µk (f(xk) − f(x∗)) . Subtracting f(x∗) from both sides and then recursively applying the inequality gives the result. Proposition 3.2. If f is convex and differentiable, then GD with step-size sequence{ηk} satisfies, ∆k ≤ \" kY i=0 |1 − µiηi| 1 + µi+1ηi # ∆0 + kX i=0  Y j>i |1 − µjηj| 1 + µj+1ηj   \u0000 Miη3 i − η2 i \u0001 1 + µi+1ηi ∥∇f(xi)∥2 2. (15) Proof. Let ∆k = ∥xk − x∗∥2 2 and observe ∆k = ∥xk − xk+1 + xk+1 − x∗∥2 2 = ∆k+1 + ∥xk − xk+1∥2 2 + 2⟨xk − xk+1, xk+1 − x∗⟩. Using this expansion in ∆k+1 − ∆k, we obtain ∆k+1 − ∆k = −∥xk − xk+1∥2 2 − 2 ⟨xk − xk+1, xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − xk⟩ −2ηk ⟨∇f(xk), xk − x∗⟩. 16Now we control the inner-products with directional strong convexity and directional smoothness. ≤ −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − xk⟩ + 2ηk h f(x∗) − f(xk) − µk 2 ∆k i ≤ −η2 k∥∇f(xk)∥2 2 + 2ηk \u0014 f(xk) − f(xk+1) + M(xk, xk+1)η2 k 2 ∥∇f(xk)∥2 2 \u0015 + 2ηk h f(x∗) − f(xk) − µk 2 ∆k i = η2 k (M(xk, xk+1)ηk − 1) ∥∇f(xk)∥2 2 + 2ηk [f(x∗) − f(xk+1)] − µkηk∆k ≤ η2 k (M(xk, xk+1)ηk − 1) ∥∇f(xk)∥2 2 − ηkµk+1∆k+1 − µkηk∆k, where the last inequality follows from µk+1 strong convexity between xk+1 and x∗. Re-arranging this expression allows us to deduce a rate with error terms depending on the local smoothness, =⇒ (1 + µk+1ηk)∆k+1 ≤ (1 − µkηk) ∆k + η2 k (M(xk, xk+1)η − 1) ∥∇f(xk)∥2 2 ≤ |1 − µkηk|∆k + η2 k (M(xk, xk+1)η − 1) ∥∇f(xk)∥2 2 =⇒ ∆k+1 ≤ |1 − µkηk| 1 + µk+1ηk ∆k + η2 k (M(xk, xk+1)η − 1) 1 + µk+1ηk ∥∇f(xk)∥2 2 ≤ \" kY i=0 |1 − µiηi| 1 + µi+1ηi # ∆0 + kX i=0   kY j=i+1 |1 − µjηj| 1 + µj+1ηj   η2 i (M(xi, xi+1)ηi − 1) 1 + µi+1ηi ∥∇f(xi)∥2 2. Proposition 3.3. Let xk =Pk i=0 ηixi+1/Pk i=0 ηi. If f is convex and differentiable, then GD satisfies, f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2 Pk i=0 ηi + Pk i=0 η2 i (ηiMi − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . (16) Proof. Let ∆k = ∥xk − x∗∥2 2 and observe ∆k = ∥xk − xk+1 + xk+1 − x∗∥2 2 = ∆k+1 + ∥xk − xk+1∥2 2 + 2⟨xk − xk+1, xk+1 − x∗⟩. Using this expansion in ∆k+1 − ∆k, we obtain ∆k+1 − ∆k = −∥xk − xk+1∥2 2 − 2 ⟨xk − xk+1, xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − x∗⟩ = −η2 k∥∇f(xk)∥2 2 − 2ηk ⟨∇f(xk), xk+1 − xk⟩ −2ηk ⟨∇f(xk), xk − x∗⟩. Now we use convexity and directional smoothness to control the two inner-products as follows: ∆k+1 − ∆k ≤ −η2 k∥∇f(xk)∥2 2 − 2ηk (f(xk) − f(x∗)) − 2ηk ⟨∇f(xk), xk+1 − xk⟩ ≤ −η2 k∥∇f(xk)∥2 2 − 2ηk (f(xk) − f(x∗)) + 2ηk(f(xk) − f(xk+1)) + η3 kM(xk, xk+1)∥∇f(xk)∥2 2 = η2 k(ηkM(xk, xk+1) − 1)∥∇f(xk)∥2 2 − 2ηk (f(xk+1) − f(x∗)) . Re-arranging this equation and summing over iterations implies the following sub-optimality bound: kX i=0 ηi Pk i=0 ηi (f(xi+1) − f(x∗)) ≤ ∆0 + Pk i=0 η2 i (ηiM(xi, xi+1) − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . Convexity of f and Jensen’s inequality now imply the final result, =⇒ f(xk) − f(x∗) ≤ ∆0 + Pk i=0 η2 i (ηiM(xi, xi+1) − 1)∥∇f(xi)∥2 2 2 Pk i=0 ηi . 17B.1 Path-Dependent Acceleration: Proofs This section proves Theorem 3.4 using estimating sequences. Throughout this section, we assume µ ≥ 0 is the global strong convexity parameter, where µ = 0 covers the non-strongly convex case. We start from the estimating sequences version of Equation (17), which is given as follows: α2 k = ηk(1 − αk)γk + ηkαkµ γk+1 = (1 − αk)γk + αkµ yk = 1 γk + αkµ [αkγkvk + γk+1xk] xk+1 = yk − ηk∇f(yk) vk+1 = 1 γk+1 [(1 − αk)γkvk + αkµyk − αk∇f(yk)] . (36) The algorithm is initialized with x0 = v0 and some γ0 > 0. Note that y0 = x0 = v0 since yk is a convex combination of xk and vk. First we prove that this scheme is equivalent to the one given in Equation (17). Lemma B.2. Equation (36) and Equation (17) lead to equivalent updates for the yk, xk, and αk sequences. Moreover, given initializationγ0 > 0, the corresponding initialization for α0 is, α0 = η0 2 \u0010 (µ − γ0) + p (γ0 − µ)2 + 4γ0/η0 \u0011 . (37) Proof. The proof follows Nesterov et al. (2018, Theorem 2.2.3). Expanding the definition of vk+1, we obtain vk+1 = 1 γk+1 \u0014(1 − αk) αk [(γk + αkµ)yk − γk+1xk] + αkµyk − αk∇f(yk) \u0015 = 1 γk+1 \u0014(1 − αk)γk αk yk + µyk \u0015 − (1 − αk) αk xk − αk γk+1 ∇f(yk) = xk − ηk αk ∇f(yk) + 1 αk (yk − xk) = xk + 1 αk (xk+1 − xk) . Plugging this back into the expression for yk+1, yk+1 = 1 γk+1 + αk+1µ [αk+1γk+1vk+1 + γk+2xk+1] = 1 γk+1 + αk+1µ \u0014 αk+1γk+1(xk + 1 αk (xk+1 − xk)) + γk+2xk+1 \u0015 = αk+1γk1 + αk(1 − αk+1)γk+1 + αkαk+1µ αk(γk+1 + αk+1µ) xk+1 − αk+1γk+1(1 − αk) αk(γk+1 + αk+1µ)xk = xk+1 + αk+1γk+1(1 − αk) αk(γk+1 + αk+1µ)(xk+1 − xk) = xk+1 + αk+1γk+1(1 − αk) αk \u0000 γk+1 + α2 k+1/ηk − (1 − αk+1)γk+1 \u0001(xk+1 − xk) = xk+1 + αk(1 − αk) (αk+1 + α2 k)(xk+1 − xk). Note that this update is consistent with Equation (17). Since γk = α2 k/ηk, we can write, α2 k+1 = ηk+1(1 − αk+1)γk + ηk+1αk+1µ = ηk+1 ηk (1 − αk+1)α2 k + ηk+1αk+1µ, 18which is also consistent with Equation (17). Finally, the initialization for α0 is determined by γ0 in Equation (36) as, α2 0 = η0(1 − α0)γ0 + η0α0µ. The quadratic formula now implies, α0 = η0 2 \u0010 (µ − γ0) + p (γ0 − µ)2 + 4γ0/η0 \u0011 . This completes the proof. As mentioned, our proof uses the concept of estimating sequences. Definition B.3. Two sequences λk, ϕk are estimating sequences for f if λl ≥ 0 for all k ∈ N, limk→∞ λk = 0, and, ϕk(x) ≤ (1 − λk)f(x) + λkϕ0(x), (38) for all x ∈ Rd. We use the same estimating sequences as developed by Nesterov et al. (2018). Letλ0 = 1, ϕ0(x) = f(x0) + γ0 2 ∥x − x0∥2 2, and define the updates, λk+1 = (1 − αk)λk ϕk+1(x) = (1 − αk)ϕk(x) + αk \u0010 f(yk) + ⟨∇f(yk), x− yk⟩ + µ 2 ∥x − yk∥2 2 \u0011 , (39) where µ ≥ 0 is the strong convexity parameter, withµ = 0 when f is merely convex. It is straight- forward to differentiate ϕk+1 to see that vk+1 of Equation (36) is the minimizer. Indeed, Nesterov et al. (2018, Lemma 2.2.3) shows that this choice for the estimating sequences obeys the following canonical form: ϕk+1(x) = min z ϕk+1(z) + γk+1 2 ∥x − vk+1∥2 2, (40) where γk+1 and vk+1 are given by Equation (36) and the minimum value is, min z ϕk+1(z) = (1 − αk) min z ϕk(z) + αkf(yk) − α2 k 2γk+1 ∥∇f(yk)∥2 2 + αk(1 − αk)γk γk+1 \u0010µ 2 ∥yk − vk∥2 2 + ⟨∇f(yk), vk − yk⟩ \u0011 . (41) Before we can prove our main theorem, we must show that these choices for λk and ϕk yield a valid estimating sequence. The following proofs build on (Nesterov et al., 2018) and (Mishkin et al., 2024). Lemma B.4. Assume αk ∈ (0, 1] for all k ∈ N. If µ >0 and γ0 = µ, then λk = k−1Y i=0 (1 − √ηkµ). (42) If µ ≥ 0 and γ0 ∈ (µ, µ+ 3/ηmin), then, λk ≤ 4 ηmin(γ0 − µ)(k + 1)2 . (43) Proof. Assume γ0 = µ >0. Then γk = µ for all k and, α2 k = (1 − αk)ηkµ + αkηkµ = ηkµ. As a consequence, λk = k−1Y i=0 (1 − √ηkµ), as claimed. 19Now assume γ0 ∈ (µ, 3L + µ). Modifying the proof by Nesterov et al. (2018, Lemma 2.2.4), we compute as follows: γk+1 − µ = (1 − αk)γk + (αk − 1)µ = (1 − αk)(γk − µ) Recursing on this equality implies γk+1 = (γ0 − µ) kY i=0 (1 − αk) = λk+1(γ0 − µ). If αk = 1 or λk = 0, then using λk+1 = (1 − αk)λk implies λk+1 = 0 and the result trivially holds. Otherwise, recall α2 k/γk+1 = ηk to obtain, 1 − λk+1 λk = αk = (γk+1ηk)1/2 = (ηkµ + ηkλk+1(γ0 − µ))1/2 =⇒ 1 λk+1 − 1 λk = 1 λ1/2 k+1 \u0014 ηkµ λk+1 + ηk(γ0 − µ) \u00151/2 ≥ 1 λ1/2 k+1 \u0014ηminµ λk+1 + ηmin(γ0 − µ) \u00151/2 . Finally, this implies 2 λ1/2 k+1   1 λ1/2 k+1 − 1 λ1/2 k ! ≥   1 λ1/2 k+1 − 1 λ1/2 k !  1 λ1/2 k+1 + 1 λ1/2 k ! ≥ 1 λ1/2 k+1 \u0014ηminµ λk+1 + ηmin(γ0 − µ) \u00151/2 . Moreover, this bound holds uniformly for all k ∈ N. We have now exactly reached Eq. 2.2.11 of Nesterov et al. (2018, Lemma 2.2.4) with L replaced by 1/ηmin. Applying that Lemma with this modification, we obtain λk ≤ 4 ηmin(γ0 − µ)(k + 1)2 , which completes the proof. Lemma B.5. If f is strongly convex with parameter µ ≥ 0 and ηk ≤ 1/µ for all k ∈ N, then λk and ϕk are estimating sequences. Proof. Using the quadratic formula, we find αk = µ − γk ± p (µ − γk)2 + 4ˆγk/ηk 2/ηk . Thus, (µ − γk) + \u0000 (µ − γk)2 + 4ˆγk/ηk \u00011/2 > 0. is sufficient for αk > 0. This holds if µ ≥ γk. Otherwise, we require, (µ − γk)2 + 4ˆγk/ηk > (µ − γk)2, which holds if and only if ηk, γk > 0. On the other hand, we also need αk ≤ 1, which is satisfied when, 4 + 4ηk(γk − µ) + η2 k(µ − γk)2 ≤ η2 k(µ − γk)2 + 4ηkγk ⇐⇒ ηk ≤ 1 µ, as claimed. Recall λ0 = 1 and λk+1 = (1 − αk)λk. Since αk ∈ (0, 1], λk ≥ 0 holds by induction. It remains to show that λk tends to zero, which holds by Lemma B.4 since we have shown αk ∈ (0, 1] for all k. Now we establish the last piece, ϕk(x) ≤ (1 − λk)f(x) + λkϕ0(x). But this follows immediately by Nesterov et al. (2018, Lemma 2.2.2). 20Now we can prove the last major lemma before our convergence result. Lemma B.6. Suppose f is strongly convex with parameterµ ≥ 0 and ηk is a sequence of adapted step-sizes, meaning ηk ≤ 1/M(xk, xk+1). Then for every k ∈ N, min z ϕk(z) ≥ f(xk). Proof. We use an inductive proof again. The inductive assumption is min z ϕk(z) ≥ f(xk), It is easy to see this holds at k = 0 since, ϕ0(x) = f(x0) + γ0 2 ∥x − v0∥2 2, implies minz ϕ0(z) = f(x0). Using Equation (41), we obtain min z ϕk+1(z) = (1 − αk) min z ϕk(z) + αkf(yk) − α2 k 2γk+1 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 \u0010µ 2 ∥yk − vk∥2 + ⟨∇f(yk, zk), vk − yk⟩ \u0011 ≥ (1 − αk)f(xk) + αkf(yk) − α2 k 2γk+1 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 \u0010µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩ \u0011 , where the inequality holds by the inductive assumption. Using convexity off and recalling α2 k γk+1 = ηk from the definition of the update (Equation (36)), min z ϕk+1(z) ≥ (1 − αk) (f(yk) + ⟨∇f(yk), xk − yk⟩) + αkf(yk) − ηk 2 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 \u0010µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩ \u0011 = f(yk) + (1− αk) ⟨∇f(yk), xk − yk⟩ −ηk 2 ∥∇f(yk)∥2 + αk(1 − αk)γk γk+1 \u0010µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩ \u0011 Using the fact that the step-sizes are adapted and invoking the directional descent lemma (i.e. Equation (18)) now implies min z ϕk+1(z) ≥ f(xk+1) + (1− αk) \u0012 ⟨∇f(yk), xk − yk⟩ + αkγk γk+1 \u0010µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩ \u0011\u0013 . The remainder of the proof is largely unchanged from the analysis in Nesterov et al. (2018). The definition of yk gives xk − yk = αkγk γk+1 (yk − vk), which we use to obtain min z ϕk+1(z) ≥ f(xk+1) + (1− αk) \u0012αkγk γk+1 ⟨∇f(yk), yk − vk⟩ + αkγk γk+1 \u0010µ 2 ∥yk − vk∥2 + ⟨∇f(yk), vk − yk⟩ \u0011\u0013 = f(xk+1) + µαk(1 − αk)γk 2γk+1 ∥yk − vk∥2 ≥ f(xk+1), since µαk(1−αk)γk 2γk+1 ≥ 0. We conclude the desired result by induction. 21The main accelerated result now follows almost immediately. Theorem 3.4. Suppose f is differentiable, µ–strongly convex and AGD is run with adapted step-sizes ηk ≤ 1/Mk. If µ >0 and α0 = √η0µ, then AGD obtains the following accelerated rate: f(xk+1) − f(x∗) ≤ kY i=0 (1 − √µηi) h f(x0) − f(x∗) + µ 2 ∥x0 − x∗∥2 2 i . (19) Let ηmin = mini∈[k] ηi. If µ ≥ 0 and α0 ∈ (√µη0, c), where c is the maximum value of α0 for which γ0 = α2 0−η0α0µ η0(1−α0) satisfies γ0 < 3/ηmin + µ, then AGD obtains the following rate: f(xk+1) − f(x∗) ≤ 4 ηmin(γ0 − µ)(k + 1)2 h f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 i . (20) Proof. We analyze the equivalent formulation given in Equation (36). See Lemma B.2 for a formal proof that these two schemes produce the same xk, yk, and αk iterates. Note that our proof follows Nesterov et al. (2018) and Mishkin et al. (2024) closely; while their results are very similar, we are not aware of pre-existing works which adapt them to our specific setting. First, observe that M(xk, xk+1) ≥ µ for all k ∈ N. Since the step-sizes ηk are assumed to satisfy ηk ≤ 1/M(xk, xk+1), we also have that ηk ≤ 1/µ for every k. Thus, Lemma B.4 and Lemma B.5 apply. Using the definition of an estimating sequence and Lemma B.6, we obtain, f(xk) ≤ min z ϕk(z) ≤ min z (1 − λk)f(z) + λkϕ0(z) ≤ (1 − λk)f(x∗) + λkϕ0(x∗). Re-arranging this equation and expanding the definition ϕ0 (Equation (39)), we deduce the following: f(xk) − f(x∗) ≤ λk (ϕ0(x∗) − f(x∗)) = λk \u0010 f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 \u0011 . We see that the rate of convergence of AGD is entirely controlled by the convergence of the sequence λk. If µ >0 and γ0 = µ, then Lemma B.4 implies f(xk) − f(x∗) ≤ k−1Y i=0 (1 − √µηi) h f(x0) − f(x∗) + µ 2 ∥x0 − x∗∥2 2 i . By Lemma B.2, this initialization is equivalent to choosing α0 = √η0µ, which is the setting claimed in the theorem. Alternatively, if µ ≥ 0 and γ0 ∈ (µ, µ+ 3/ηmin), then, f(xk) − f(x∗) ≤ 4 ηmin(γ0 − µ)k2 h f(x0) − f(x∗) + γ0 2 ∥x0 − x∗∥2 2 i , where the equality, γ0 = α2 0 − η0α0µ η0(1 − α0) , holds by Lemma B.2. Since α0 ≤ 1 for η0 ≤ 1/µ, η0 is an increasing function of γ0. Thus, an upper-bound c on α0 can be deduced from that on γ0 using the quadratic formula: c = − 3η0 2ηmin + η0 2 \u0012 9 (ηmin)2 + 43ηmin + µ η0 \u00131/2 = 3η0 2ηmin \"\u0012 1 + 4(ηmin)2 3ηmin + µ 9η0 \u00131/2 − 1 # . 22C Proofs for Section 4.1 Lemma C.1. Let B be a positive semi-definite matrix and suppose that f(x) = 1 2x⊤Bx − c⊤x. Let xi+1 = xi − η∇f(xi). Then for any η >0, the pointwise directional smoothness between the gradient descent iterates xi, xi+1 is given by 1 2D(xi, xi+1) = ∥B∇f(xi)∥2 ∥∇f(xi)∥2 . Proof. We have by straightforward algebra, 1 2D(xi, xi+1) = ∥∇f(xi+1) − ∇f(xi)∥2 ∥xi+1 − xi∥2 = ∥[Bxi+1 − c] − [Bxi − c] ∥2 ∥xi+1 − xi∥2 = ∥B [xi+1 − xi] ∥2 ∥xi+1 − xi∥2 = ∥B [−η∇f(xi)] ∥2 ∥ −η∇f(xi)∥2 = ∥B∇f(xi)∥2 ∥∇f(xi)∥2 . Lemma C.2. Let B be a positive semi-definite matrix and suppose that f(x) = 1 2x⊤Bx − c⊤x. Let xi+1 = xi − η∇f(xi). Then for any η >0, the path-wise directional smoothness between the gradient descent iterates xi, xi+1 is given by by A(xi, xi+1) = ∇f(xi)⊤B∇f(xi) ∇f(xi)⊤∇f(xi) . Proof. Let At(x, y) = ⟨∇f(x+t(y−x))−∇f(x),y−x⟩ t∥x−y∥2 2 . We have At(x, y) = ⟨∇f(x + t(y − x)) − ∇f(x), y− x⟩ t∥x − y∥2 2 = ⟨(B(x + t(y − x))) − c − [Bx − c] , y− x⟩ t∥x − y∥2 2 = ⟨t · B(y − x), y− x⟩ t∥x − y∥2 2 = (y − x)⊤B(y − x) ∥x − y∥2 2 . The path-wise directional smoothness A is therefore A(x, y) = sup t∈[0,1] At(x, y) = sup t∈[0,1] (y − x)⊤B(y − x) ∥x − y∥2 2 = (y − x)⊤B(y − x) ∥x − y∥2 2 . 23Plugging in y = x − η∇f(x) = x − η[Bx − c] in the above gives A(x, x− η∇f(x)) = (−η [Bx − c]) B(−η) [Bx − c] ∥η[Bx − c]∥2 2 = (Bx − c)⊤B(Bx − c) ∥Bx − c∥2 2 = (Bx − c)⊤B(Bx − c) ∥Bx − c∥2 2 = ∇f(x)⊤B∇f(x) ∇f(x)⊤∇f(x) . D Proofs for Section 4.2 Proposition 4.1. If f is convex and continuously differentiable, then either (i) f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/D(x, x− η∇f(x)). Proof. Let I = {η : ∇f(x − η∇f(x)) = ∇f(x)}. For every η ∈ I, it holds that −⟨∇f(x − η∇f(x)), ∇f(x)⟩ = −∥∇f(x)∥2 2. However, since f is convex, the directional derivative −⟨∇f(x − η′∇f(x)), ∇f(x)⟩, is monotone non-decreasing in η′. We deduce that I must be an interval of form [0, η]. If η is not bounded, then f is linear along −∇f(x) and is minimized by taking η → ∞. Therefore, we may assume η is finite. Let η >η. Then we have the following: x − η∇f(x) = x − 2∥x − η∇f(x) − x∥2 ∥∇f(x − η∇f(x)) − ∇f(x)∥2 ∇f(x) ⇐⇒ ∇f(x) = 2∥∇f(x)∥2 ∥∇f(x − η∇f(x)) − ∇f(x)∥2 ∇f(x), from which we deduce ∥∇f(x − η∇f(x)) − ∇f(x)∥2 = 2∥∇f(x)∥2, is sufficient for the implicit equation to hold. Squaring both sides and multiplying by 1/2, we obtain the following alternative root-finding problem: h(η) := 1 2∥∇f(x − η∇f(x))∥2 2 − ⟨∇f(x − η∇f(x)), ∇f(x)⟩ −1 2∥∇f(x)∥2 2 = 0. (44) Because f is C1, h is a continuous function and it suffices to show that there exists an interval in which h crosses 0. From the display above, we see h(η) = −∥∇f(x)∥2 2 < 0. Continuity now implies ∃η′ > η such that h(η′) < 0. Now, supposeh(η) ≤ 0 for all η ≥ η′. Working backwards, we see that this can only occur when η ≤ 2∥x − η∇f(x) − x∥2 ∥∇f(x − η∇f(x)) − ∇f(x)∥2 = 1 D(x(η), x− η∇f(x)) for all η ≥ η′. The directional descent lemma (Equation (10)) now implies f(x − η∇f(x)) ≤ f(x) − η \u0012 1 − ηD(x, x− η∇f(x)) 2 \u0013 ∥∇f(x)∥2 2 ≤ f(x) − η 2∥∇f(x)∥2 2, Taking limits on both sides as η → ∞implies f(x − η∇f(x)) is minimized along the ray x(η) = x − η∇f(x). Thus, we deduce that either there exists η′′ > η′ such that h(η′′) > 0 exists, or f is minimized along the gradient direction as claimed. 24Proposition 4.2. If f is convex and twice continuously differentiable, then either (i)f is minimized along the ray x(η) = x − η∇f(x) or (ii) there exists η >0 satisfying η = 1/A(x, x− η∇f(x)). Proof. Let J = \b η : ⟨∇f(x − η∇f(x)), ∇f(x)⟩ = ∥∇f(x)∥2 2 \t . Since f is convex, the directional derivative −⟨∇f(x − η′∇f(x)), ∇f(x)⟩, is monotone non-decreasing in η′. We deduce that J must be an interval of form [0, η]. If η is not bounded, then convexity implies lim η→∞ f(x − η∇f(x)) ≤ lim η→∞ f(x) − η ⟨∇f(x − η∇f(x)), ∇f(x)⟩ = −∞, meaning f is minimized along −∇f(x). Therefore, we may assume η is finite. We have x − η∇f(x) = x − 1 A(x, x− η∇f(x))∇f(x) ⇐⇒ η = inf t∈[0,1] tη∥∇f(x)∥2 2 ⟨∇f(x) − ∇f(x − tη∇f(x)), ∇f(x)⟩. Thus, for η >η, the equation we must solve reduces to h(η) := η − inf t∈[0,1] tη∥∇f(x)∥2 2 ⟨∇f(x) − ∇f(x − tη∇f(x)), ∇f(x)⟩ = 0. Since f is C2, h is continuous (see, e.g. Hogan (1973, Theorem 7)) and it suffices to show that there exists an interval over which h crosses 0. Using Taylor’s theorem, we can re-write this expression as h(η) = η − inf t∈[0,1] ∥∇f(x)∥2 2 ⟨∇f(x), ∇2f(x − α(tη)∇f(x))∇f(x)⟩, where for some α(tη) ∈ [0, tη]. Examining the denominator, we find that, Z t 0 ∇f(x)⊤∇2f(x − tη∇f(x))∇f(x)dt = ⟨∇f(x − η∇f(x)) − ∇f(x), ∇f(x)⟩ = 0, which, since f is convex, implies ∇f(x)⊤∇2f(x − α∇f(x))∇f(x) = 0, for every α ∈ [0, η]. By continuity of the Hessian, for every ϵ >0, there exists δ > 0 such that η′ ∈ [η, η + δ] guarantees, ∇f(x)⊤∇2f(x − η′∇f(x))∇f(x) < ϵ. Substituting this into our expression for h, h(η′) = η′ − inf t∈[0,1] ∥∇f(x)∥2 2 ⟨∇f(x), ∇2f(x − α(tη′)∇f(x))∇f(x)⟩ < η + δ − ∥∇f(x)∥2 2 ϵ < 0, for ϵ, δsufficiently small. Thus, there exists η′ > η for which h(η′) < 0. Now let us show that h(η′′) > 0 for some η′′. For convenience, define g(η) = inf t∈[0,1] t∥∇f(x)∥2 2 ⟨∇f(x) − ∇f(x − tη∇f(x)), ∇f(x)⟩, 25Algorithm 1 Gradient Descent with Exponential Search 1: Procedure ExponentialSearch(x, η0) 2: for k = 1, 2, 3, . . .do 3: ηout ← RootFindingBisection \u0010 x, 2−2k η0, η0 \u0011 . 4: if ηout < ∞ then 5: Return ηout 6: end if 7: end for 8: End Procedure 9: Procedure RootFindingBisection(x, ηlo, ηhi) 10: Define ϕ(η) = η − ψ(η) where ψ(η) is given in (24) \\\\ One access to ϕ requires T descent steps. 11: if ϕ(ηhi) ≤ 0 then 12: Return ηhi 13: end if 14: if ϕ(ηlo) > 0 then 15: Return ∞ 16: end if 17: while ηhi > 2ηlo do 18: ηmid = √ηloηhi 19: if ϕ(ηmid) > 0 then 20: ηhi = ηmid 21: else 22: ηlo = ηmid 23: end if \\\\ Invariant: ϕ(ηhi) > 0, and ϕ(ηlo) ≤ 0. 24: end while 25: Return ηlo 26: End Procedure which is a continuous and monotone non-increasing function. Take η → ∞and let lim η→∞ g(η) = c, where the limit exists, but may be −∞. Indeed, it must hold that c <∞ since, lim η→∞ g(η) < g(η′) < ∞. If c <0, then taking η′′ large enough that g(η′′) ≤ 0 suffices. Alternatively, if c ≥ 0, then there exists ˜η such that g(η) ≤ c + ϵ for every η ≥ ˜η. Choosing η′′ > max {˜η, c} + ϵ yields h(η′′) = η′′ − g(η′′) > c+ ϵ − c − ϵ = 0. This completes the proof. Theorem 4.3. Assume f is convex and L-smooth. Then Algorithm 1 with η0 > 0 requires at most 2K(log log(2η0/L) ∨ 1) iterations of GD and in the last run it outputs a step-size η and point xK = 1 K PK−1 i=0 xi(η) such that exactly one of the following holds: Case 1: η = η0 and f(xK) − f(x∗) ≤ ∥x0 − x∗∥2 2 2Kη0 Case 2: η ̸= η0 and f(xK) − f∗ ≤ ∥x0 − x∗∥2 2 2K \"Pk i=0 Mi∥∇f(x′ i)∥2 2 Pk i=0 ∥∇f(x′ i)∥2 2 # , where Mi def = M(x′ i, x′ i+1) and x′ i are the iterates generated by GD with step-size η′ ∈ [η, 2η]. 26Proof of Theorem 4.3. This analysis follows (Carmon and Hinder, 2022). First, instantiate Equa- tion (16) from Proposition 3.3 with ηi = η for all i to obtain f(xk) − f∗ ≤ ∥x0 − x∗∥2 2ηk + η h η Pk i=0 M(xi, xi+1)∥∇f(xi)∥2 − Pk i=0 ∥∇f(xi)∥2 i 2k . (45) Now, observe that if we get a “Lucky strike” andϕ(ηhi) = ϕ(η0) ≤ 0, then specializing Equation (45) for η = η0 we get f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2η0k + η0 2k \" η0 kX i=0 M(xi, xi+1)∥∇f(xi)∥2 2 − kX i=0 ∥∇f(xi)∥2 2 # = ∥x0 − x∗∥2 2 2η0k + η0 Pk i=0 M(xi, xi+1)∥∇f(xi)∥2 2 2k · ϕ(η0) ≤ ∥x0 − x∗∥2 2 2η0k . This covers the first case of Theorem 4.3. With the first case out of the way, we may assume thatϕ(ηhi) > 0. This implies that ηhi > 1 L , since if η ≤ 1 L we have ϕ(η) ≤ 0. Now observe that when ηlo = 22−k η0 ≤ 1 L , we have that ϕ(ηlo) ≤ 0, therefore it takes at most k = ⌈log log η0 L−1 ⌉ to find such an ηlo. From here on, we suppose that ϕ(ηhi) > 0 and ϕ(ηlo) ≤ 0. Now observe that the algorithm’s main loop always maintains the invariant ϕ(ηhi) > 0 and ϕ(ηlo) ≤ 0, and every iteration of the loop halves log ηhi ηlo , therefore we make at most ⌈log logη0L⌉ loop iterations. The output step-size ηlo satisfies ηhi 2 ≤ ηlo ≤ ηhi and ϕ(ηlo) ≤ 0. Specializing Equation (45) for η = η0 and using that ϕ(ηlo) ≤ 0 we get f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 2ηlok + ηlo Pk i=0 M(xi(ηlo), xi+1(ηlo))∥∇f(xi(ηlo))∥2 2 2k · ϕ(ηlo) ≤ ∥x0 − x∗∥2 2 2ηlok . (46) By the loop invariant ϕ(ηhi) > 0 we have ϕ(ηhi) > 0 ⇔ ηhi > PK i=0 ∥∇f(xi(ηhi))∥2 2PK i=0 ∥∇f(xi(ηhi))∥2 2M(xi(ηhi), xi+1(ηhi)) By the loop termination condition we have ηlo ≥ ηhi 2 , combining this with the last equation we get ηlo ≥ ηhi 2 ≥ 1 2 PK i=0 ∥∇f(xi(ηhi))∥2 2PK i=0 ∥∇f(xi(ηhi))∥2 2M(xi(ηhi), xi+1(ηhi)) . Plugging this into Equation (46) we obtain f(xk) − f(x∗) ≤ ∥x0 − x∗∥2 2 k · PK i=0 ∥∇f(xi(ηhi))∥2 2M(xi(ηhi), xi+1(ηhi)) PK i=0 ∥∇f(xi(ηhi))∥2 2 It remains to notice that ηhi ∈ [ηlo, 2ηlo]. Theorem 4.4. Suppose that f is convex and differentiable and let M be any directional smoothness function for f. Let ∆0 := ∥x0 − x∗∥2 2. Then GD with the Polyak step-size and γ ∈ (1, 2) satisfies f(xk) − f(x∗) ≤ c(γ)∆0 2 Pk−1 i=0 M(xi, xi+1)−1 , (27) where c(γ) = γ/(2 − γ)(γ − 1) and xk = Pk−1 i=0 \u0002 M(xi, xi+1)−1xi \u0003 / \u0010Pk−1 i=0 M(xi, xi+1)−1 \u0011 . For the proof of this theorem, we will need the following proposition: 27Proposition D.1. Let x ∈ Rd. Define ηx = γ f(x)−f(x∗) ∥∇f(x)∥2 for some γ ∈ (1, 2) and let ˜x = x − ηx∇f(x). Then, f(x) − f(x∗) ≥ γ − 1 γ2 2 M(x, ˜x)∥∇f(x)∥2 2. Proof. Observe f(x) − f(x∗) = f(x) − f(˜x) + f(˜x) − f(x∗) ≥ f(x) − f(˜x). (47) By smoothness we have f(˜x) ≤ f(x) + ⟨∇f(x), ˜x − x⟩ + M(x, ˜x) 2 ∥˜x − x∥2 = f(x) − ηx∥∇f(x)∥2 2 + η2 xM(x, ˜x) 2 ∥∇f(x)∥2 2. Plugging back into Equation (47) we get f(x) − f(x∗) ≥ ηx∥∇f(x)∥2 2 − η2 xM(x, ˜x) 2 ∥∇f(x)∥2 2. Let us now use the definition of ηx = γ f(x)−f(x∗) ∥∇f(x)∥2 2 to get f(x) − f(x∗) ≥ γ(f(x) − f(x∗)) − γηxM(x, ˜x) 2 (f(x) − f(x∗)). Assuming that f(x) ̸= f(x∗) then we get by cancellation 1 ≥ γ − γηxM(x, ˜x) 2 . Using the definition of ηx again 1 − γ ≥ −γ2 M(x, ˜x) 2 f(x) − f(x∗) ∥∇f(x)∥2 2 Rearranging we get f(x) − f(x∗) ≥ γ − 1 γ2 2 M(x, ˜x)∥∇f(x)∥2 2. If f(x) = f(x∗) then ∥∇f(x)∥2 2 = 0 , both sides are identically zero and the statement holds trivially. Now we can prove our theorem on the convergence of GD with Polyak step-sizes: Proof of Theorem 4.4. We start by considering the distance to the optimum and expanding the square ∥xk+1 − x∗∥2 2 = ∥xk − x∗∥2 2 + 2⟨xk+1 − xk, xk − x∗⟩ + ∥xk+1 − xk∥2 2 = ∥xk − x∗∥2 2 − 2ηk ⟨∇f(xk), xk − x∗⟩ + η2 k∥∇f(xk)∥2 2. (48) Let δk = f(xk) − f(x∗). By convexity we have f(x∗) ≥ f(xk) + ⟨∇f(xk), x∗ − xk⟩. Therefore we can upper bound Equation (48) as ∥xk+1 − x∗∥2 2 ≤ ∥xk − x∗∥2 2 − 2ηkδk + η2 k∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − 2ηkδk + ηk   γ δk ∥∇f(xk)∥2 2 ! ∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − (2 − γ)ηkδk, (49) 28where in the second line we used the definition of ηk. By Proposition D.1 we have δk ≥ γ − 1 γ 2 M(xk, xk+1)∥∇f(xk)∥2 2. (50) Using this in Equation (49) gives ∥xk+1 − x∗∥2 2 ≤ ∥xk − x∗∥2 2 − (2 − γ)ηk γ − 1 γ2 2 M(xk, xk+1)∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − (2 − γ)γ − 1 γ2 2 M(xk, xk+1)   γ δk ∥∇f(xk)∥2 2 ! ∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − 2(2 − γ)(γ − 1) γM (xk, xk+1) δk. Rearranging we get 2(2 − γ)(γ − 1) γM (xk, xk+1) δk ≤ ∥xk − x∗∥2 2 − ∥xk+1 − x∗∥2 2. Summing up and telescoping we get k−1X i=0 2(2 − γ)(γ − 1) γM (xi, xi+1) δi ≤ ∥x0 − x∗∥2 2. Let xk = 1Pk−1 i=0 M(xi,xi+1)−1 Pk−1 i=0 M(xi, xi+1)−1xi, then by the convexity of f and Jensen’s inequality we have f(xk) − f(x∗) ≤ 1 Pk−1 i=0 M(xi, xi+1)−1 k−1X i=0 M(xi, xi+1)−1δi ≤ γ 2(2 − γ)(γ − 1) 1 Pk−1 i=0 M(xi, xi+1)−1 ∥x0 − x∗∥2 2. Theorem D.2. If f is convex and differentiable, then GD with the Polyak step-size andγ <2 satisfies, f(xk) − f(x∗) ≤ 1 (2 − γ) Pk i=0 ηi ∥x0 − x∗∥2 2, (51) where xk = Pk−1 i=0 ηixi/ \u0010Pk−1 i=0 ηi \u0011 . Proof. The proof begins in the same manner as that for Theorem 4.4, ∥xk+1 − x∗∥2 2 = ∥xk − x∗∥2 2 + 2⟨xk+1 − xk, xk − x∗⟩ + ∥xk+1 − xk∥2 2 = ∥xk − x∗∥2 2 − 2ηk ⟨∇f(xk), xk − x∗⟩ + η2 k∥∇f(xk)∥2 2 ≤ ∥xk − x∗∥2 2 − 2ηkδk + η2 k∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − 2ηkδk + ηk   γ δk ∥∇f(xk)∥2 2 ! ∥∇f(xk)∥2 2 = ∥xk − x∗∥2 2 − (2 − γ)ηkδk. Re-arranging, summing from i = 0 to k − 1, and dividing by Pk−1 i=0 ηi, =⇒ k−1X i=0 ηi Pk i=0 ηi (f(xi) − f(w∗)) ≤ 1 (2 − γ) Pk i=0 ηi \u0002 ∥x0 − x∗∥2 2 − ∥xk − x∗∥2 2 \u0003 =⇒ f(xk) − f(x∗) ≤ 1 (2 − γ) Pk i=0 ηi ∥x0 − x∗∥2 2, which completes the proof. 29Lemma D.3. Normalized GD with step-sizes ηk satisfies − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ ≤η2 kM(xk, xk+1) − ηk∥∇f(xk)∥2. (52) Proof. By convexity we have f(xk+1) ≤ f(xk) + ⟨xk+1 − xk, ∇f(xk+1)⟩ = f(xk) − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ (53) Now note that − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ = ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk) − ∇f(xk+1)⟩ (54) − ηk∥∇f(xk)∥2 ≤ ηk∥∇f(xk) − ∇f(xk+1)∥ −ηk∥∇f(xk)∥2, (55) where we used Cauchy-Schwarz. Recalling the definition of directional smoothness M(xk, xk+1) def = ∥∇f(xk) − ∇f(xk+1)∥ ∥xk − xk+1∥ = ∥∇f(xk) − ∇f(xk+1)∥ ηk in Equation (55) gives − ηk ∥∇f(xk)∥2 ⟨∇f(xk), ∇f(xk+1)⟩ ≤η2 kM(xk, xk+1) − ηk∥∇f(xk)∥2. Theorem 4.5. Suppose that f is convex and differentiable. Let D be the point-wise directional smoothness defined by Equation (4) and ∆0 := ∥x0 − x∗∥2 2. Then normalized GD with a sequence of non-increasing step-sizes ηk satisfies f(ˆxk) − f(x∗) ≤ ∆0 + Pk−1 i=0 η2 i 2k2 \u0012f(x0) η2 0 − f(x∗) η2 k−1 \u0013 + ∆0 + Pk−1 i=0 η2 i 2k k−1X i=0 M(xi, xi+1) k , (30) where ˆxk = arg mini∈[k−1] f(xi). If maxi∈[k−1] M(xi, xi+1) is bounded for all k (i.e. f is L- smooth), then for ηi = 1/ √ i we have f(ˆxk) − f(x∗) ∈ O(1/k) and for ηi = 1/ √ i we get the anytime result f(ˆxk) − f(x∗) ∈ O(log(k)/k). Proof. Here we will first establish that for any non-increasing sequence of step-sizes ηk > 0 we have that min i∈[k−1] f(xi) − f(x∗) ≤ 1 2 ∆0 + Pk−1 i=0 η2 i k   f(x0) kη2 0 − f(x∗) kη2 k−1 + k−1X i=0 M(xi, xi+1) k ! . (56) The specialized results follow by assuming that Pk−1 i=0 M(xi,xi+1) k is bounded, which it is the case of L–Lipschitz gradients. In particular the mini∈[k−1] f(xi) − f(x∗) ∈ O(1/T) result follows by plugging in ηi = 1/ √ k and using that k−1X i=0 η2 i = k−1X i=0 1 k = 1 f(x0) kη2 0 − f(x∗) kη2 k−1 = f(x0) − f(x∗). Alternatively we get mini∈[k−1] f(xi) − f(x∗) ∈ O(log(T)/T) by plugging in ηi = 1/√i + 1 and using that k−1X i=0 η2 i = k−1X i=0 1 i + 1 ≤ log(k) f(x0) kη2 0 − f(x∗) kη2 k−1 = f(x0) k − f(x∗). 30With this in mind, let us prove Equation (56). By convexity, f(xk+1) ≤ f(xk) − ηk ∥∇f(xk)∥2 ∇f(xk)⊤∇f(xk+1) ≤ f(xk) − ηk∥∇f(xk)∥2 + η2 kM(xk, xk+1). (Using (52)) Re-arranging, dividing through by η2 k, and then summing over i = 0, ··· , k− 1 gives k−1X i=0 ∥∇f(xi)∥2 ηi ≤ f(x0) η2 0 + k−2X i=1 f(xi) \u0012 1 η2 i − 1 η2 i−1 \u0013 − f(x∗) η2 k−1 + k−1X i=0 M(xi, xi+1) ≤ f(x0) η2 0 − f(x∗) η2 k−1 + k−1X i=0 M(xi, xi+1), (57) where we used that ηi−1 ≤ ηi =⇒ 1 η2 i − 1 η2 i−1 ≤ 0. Using Jensen’s inequality over the mapa 7→ 1/a, which is convex for a positive, gives k−1X i=0 ηi ∥∇f(xk)∥2 ≥ k2 Pk−1 i=0 ∥∇f(xk)∥2/ηi (57) ≥ k2 f(x0) η2 0 − f(x∗) η2 k−1 + Pk−1 i=0 M(xi, xi+1) . (58) Meanwhile, recall our notation ∆i = ∥xi − x∗∥2 2. Expanding the squares and using that f(x) is convex, we have that ∆i+1 = ∆i − 2 ηi ∥∇f(xi)∥2 ∇f(xi)⊤(xi − x∗) + η2 i ≤ ∆i − 2ηi f(xi) − f(x∗) ∥∇f(xk)∥2 + η2 i . As before, we use δi := f(xi) − f(x∗). Re-arranging, summing both sides of the above over i = 0, . . . , k− 1 and using telescopic cancellation gives k−1X i=0 ηi δi ∥∇f(xi)∥ ≤ ∆0 + Pt−1 i=0 η2 i 2 . Using the above along with (58) gives, min i∈[k−1] δi ≤ 1 Pk−1 i=0 ηi ∥∇f(xi)∥2 k−1X i=0 ηi δi ∥∇f(xi)∥2 ≤ 1 2 ∆0 + Pk−1 i=0 η2 iPk−1 i=0 ηi ∥∇f(xi)∥ ≤ 1 2 ∆0 + Pk−1 i=0 η2 i k   f(x0) kη2 0 − f(x∗) kη2 k−1 + k−1X i=0 M(xi, xi+1) k ! E Experimental Details In this section we provide additional details necessary to reproduce our experiments. We run our logistic regression experiments using PyTorch (Paszke et al., 2019). For the UCI datasets, we use the pre-processed version of the data provided by Fernández-Delgado et al. (2014), although we do not use their evaluation procedure as it is known have test-set leakage. Instead, we randomly perform an 80–20 train-test split and use the test set for validation. Unless otherwise stated, all methods are initialized using the Kaiming initialization (He et al., 2015), which is standard in PyTorch. 31In order to compute the strongly adapted step-sizes, we run the SciPy (Virtanen et al., 2020) imple- mentation of Newton method on Equation (44). In general, we find this procedure is surprisingly robust, although it can be slow. Figure 1: We pick two datasets from the UCI repository to showcase different behaviors of the upper-bounds. We compute a tight-upper bound on L as follows. Recall that for logistic regression problems the Hessian is given by ∇2f(x) = A⊤Diag \u0012 1 σ(−y · Ax) + 2 +σ(y · Ax) \u0013 A, where A is the data matrix and σ(z) = 1 1+exp(z) is the sigmoid function. A short calculation shows that the diagonal matrix Diag \u0012 1 σ(−y · Ax) + 2 +σ(y · Ax) \u0013 ⪯ 1 4I, which is tight when x = 0. As a result, L = λmax(A⊤A)/4. We compute this manually. We also compute the optimal value for the logistic regression problem using the SciPy implementation of BFGS (Liu and Nocedal, 1989). We use this value for f(x∗) to compute the Polyak step-size and when plotting sub-optimality. It turns out that the upper-bound based on L-smoothness for both GD with the Polyak step-size (Hazan and Kakade, 2019) and standard GD (Bubeck et al., 2015) is f(xk) − f(x∗) ≤ 2L∥x0 − x∗∥2 2 k . Figure 3: We run these experiments using vanilla NumPy. As mentioned in the text, we generate a quadratic optimization problem min x 1 2x⊤Ax − b⊤x, where the eigenvalues of A were generated to follow power law distribution with parameter α = 3. We scaled the eigenvalues to ensureL = 1000. The dimension of the problem we create is d = 300. We repeat the experiment for 20 random trials and plot the mean and standard deviations. Figure 4: We pick three different datasets from the UCI repository to showcase the possible con- vergence behavior of the optimization methods. We compute L and f(w∗) as described above for Figure 1. For normalized GD, we use the step-size schedule ηk = η0/ √ k as suggested by our theory. To pickη0, we run a grid search on the grid generated by np.logspace(-8, 1, 20). We implement AdGD from scratch and use a starting step-size of η0 = 10−3. We use the same procedure to compute the strongly adapted step-sizes as described above. F Computational Details The experiments in Figure 3 were run on a MacBook Pro (16 inch, 2019) with a 2.6 GHz 6-Core Intel i7 CPU and 16GB of memory. All other experiments were run on a Slurm cluster with several different node configurations. Our experiments on the cluster were run with nodes using (i) Nvidia A100 GPUs (80GB or 40GB memory) or Nvidia H100-80GB GPUs with Icelake CPUs, or (ii) Nvidia V100-32GB or V100-16GB GPUs with Skylake CPUs. All jobs were allocated a single GPU and 24GB of RAM. 32",
      "meta_data": {
        "arxiv_id": "2403.04081v2",
        "authors": [
          "Aaron Mishkin",
          "Ahmed Khaled",
          "Yuanhao Wang",
          "Aaron Defazio",
          "Robert M. Gower"
        ],
        "published_date": "2024-03-06T22:24:05Z",
        "pdf_url": "https://arxiv.org/pdf/2403.04081v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the optimization path, rather than on global, worst-case L-smoothness constants. It develops the concept of directional smoothness, a measure of gradient variation, to construct tighter upper-bounds on the objective. Key findings include: new path-dependent sub-optimality bounds for GD, demonstration that strongly adapted step-sizes recover classical schemes for quadratic problems (e.g., Cauchy step-size), and proof that the Polyak step-size and normalized GD achieve fast, path-dependent rates without explicit knowledge of directional smoothness. The research also shows that these convergence guarantees are tighter than classical L-smoothness theory in experiments.",
        "methodology": "The core methodology revolves around defining and leveraging 'directional smoothness' functions M(x, y) which quantify gradient variation along the chord between points x and y. Three constructive directional smoothness functions are introduced: point-wise smoothness D(x,y), path-wise smoothness A(x,y), and optimal point-wise smoothness H(x,y). These functions are used to derive a local descent lemma and new sub-optimality bounds for convex functions, including the case with directional strong convexity. The paper explores 'strongly adapted step-sizes' (ηk = 1/M(xk+1, xk)) which require solving non-linear root-finding problems for general functions, but are shown to be equivalent to well-known step-sizes for quadratics. For general convex functions, existence proofs for strongly adapted step-sizes are provided, and an exponential search algorithm is adapted to find approximate adaptive step-sizes. Additionally, the paper analyzes the Polyak step-size and normalized GD, demonstrating their inherent adaptivity to directional smoothness.",
        "experimental_setup": "The experimental evaluation was performed on logistic regression problems from the UCI repository (ionosphere, mammographic, horse-colic, ozone datasets) and synthetic quadratic problems. For UCI datasets, a pre-processed version was used with an 80-20 train-test split for validation. The optimal value f(x*) for logistic regression was computed using SciPy's BFGS implementation. Methods were initialized using Kaiming initialization. For synthetic quadratic problems, eigenvalues were generated to follow a power-law distribution with L=1000. Benchmarks included GD with a fixed step-size (1/L), GD with step-sizes strongly adapted to point-wise smoothness (1/Dk), Polyak step-size GD, normalized GD (with a grid-searched η0 and schedule ηk = η0/√k), and the AdGD method. Strongly adapted step-sizes were computed using SciPy's Newton method on Equation (44). Experiments were run on a MacBook Pro or a Slurm cluster with Nvidia A100/H100/V100 GPUs.",
        "limitations": "The general computation of strongly adapted step-sizes for non-quadratic functions involves solving challenging non-linear root-finding problems (e.g., using Newton's method), which can be impractical for large-scale optimization. While exponential search can find approximate adaptive step-sizes, it introduces a log-log penalty and requires the function to be L-smooth. Obtaining accelerated rates for non-strongly convex functions (µ=0) requires prior knowledge of the minimum step-size, which is not straightforward when dealing with locally Lipschitz gradients. The provided rate for normalized GD (Theorem 4.5) does not adapt to any smoothness function like the Polyak step-size does.",
        "future_research_directions": "The paper lays the groundwork for further exploration of path-dependent optimization. One implicit direction is improving the practical computation of strongly adapted step-sizes for general non-quadratic and non-convex functions. Further research could also focus on extending the analysis of directional smoothness to other optimization algorithms beyond GD and AGD, particularly those used in deep learning. Investigating how the insights from directional smoothness can be integrated into adaptive gradient methods (like Adagrad variants) is another potential area. Additionally, developing anytime rates for non-strongly convex acceleration with locally Lipschitz gradients without requiring prior knowledge of minimum step-sizes remains an open problem."
      }
    },
    {
      "title": "Optimistic Meta-Gradients",
      "abstract": "We study the connection between gradient-based meta-learning and convex\nop-timisation. We observe that gradient descent with momentum is a special case\nof meta-gradients, and building on recent results in optimisation, we prove\nconvergence rates for meta-learning in the single task setting. While a\nmeta-learned update rule can yield faster convergence up to constant factor, it\nis not sufficient for acceleration. Instead, some form of optimism is required.\nWe show that optimism in meta-learning can be captured through Bootstrapped\nMeta-Gradients (Flennerhag et al., 2022), providing deeper insight into its\nunderlying mechanics.",
      "full_text": "Optimistic Meta-Gradients Sebastian Flennerhag DeepMind flennerhag@google.com Tom Zahavy DeepMind Brendan O’Donoghue DeepMind Hado van Hasselt DeepMind András György DeepMind Satinder Singh DeepMind Abstract We study the connection between gradient-based meta-learning and convex op- timisation. We observe that gradient descent with momentum is a special case of meta-gradients, and building on recent results in optimisation, we prove con- vergence rates for meta-learning in the single task setting. While a meta-learned update rule can yield faster convergence up to constant factor, it is not sufﬁcient for acceleration. Instead, some form of optimism is required. We show that op- timism in meta-learning can be captured through Bootstrapped Meta-Gradients [Flennerhag et al., 2022], providing deeper insight into its underlying mechanics. 1 Introduction 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training Steps 1e6 50 55 60 65 70 75Top-1 Test Accuracy (%) SGD Standard meta-learning Optimistic meta-learning Figure 1: ImageNet. We compare training a 50- layer ResNet using SGD against variants that tune an element-wise learning rate online using standard meta-learning or optimistic meta-learning. Shad- ing depicts 95% conﬁdence intervals over 3 seeds. In meta-learning, a learner is using a param- eterised algorithm to adapt to a given task. The parameters of the algorithm are then meta-learned by evaluating the learner’s result- ing performance [Schmidhuber, 1987, Hinton and Plaut, 1987, Bengio et al., 1991]. This paradigm has garnered wide empirical success [Hospedales et al., 2020]. For instance, it has been used to meta-learn how to explore in re- inforcement learning (RL) [Xu et al., 2018a, Alet et al., 2020], online hyper-parameter tun- ing of non-convex loss functions [Bengio, 2000, Maclaurin et al., 2015, Xu et al., 2018b, Zahavy et al., 2020], discovering black-box loss func- tions [Chen et al., 2016, Kirsch et al., 2019, Xu et al., 2020, Oh et al., 2020], black-box learning algorithms [Hochreiter et al., 2001, Wang et al., 2016], or entire training protocols [Real et al., 2020]. Yet, very little is known in terms of the theoretical properties of meta-learning. The reason for this is the complex interac- tion between the learner and the meta-learner. learner’s problemis to minimize the expected loss f of a stochastic objective by adapting its parametersx∈Rn. The learner has an update rule ϕat its disposal that generates new parameters xt = xt−1 + ϕ(xt−1,wt); we suppress data dependence to simplify notation. A simple example is when ϕrepresents gradient descent with wt = ηits step size, that is ϕ(xt−1,η) = −η∇f(xt−1) [Mahmood et al., 2012, van Erven and Koolen, 2016]; several works have explored meta-learning other aspects of a gradient-based update rule [Finn et al., 2017, Nichol et al., 2018, Flennerhag et al., 2019, Xu et al., 2018b, Zahavy et al., 2020, Flennerhag et al., arXiv:2301.03236v1  [cs.LG]  9 Jan 20232022, Kirsch et al., 2019, Oh et al., 2020]. More generally, ϕneed not be limited to the gradient of any function, for instance, it can represent some algorithm implemented within a Recurrent Neural Network [Schmidhuber, 1987, Hochreiter et al., 2001, Andrychowicz et al., 2016, Wang et al., 2016]. The meta-learner’s problemis to optimise the meta-parameters wt to yield effective updates. In a typical (gradient-based) meta-learning setting, it does so by treating xt as a function of w. Let ht, deﬁned by ht(w) = f(xt−1 + ϕ(xt−1,w)), denote the learner’s post-update performance as a function of w. The learner and the meta-learner co-evolve according to xt = xt−1 + ϕ(xt−1,wt), and wt+1 = wt −∇ht(wt) = wt −Dϕ(xt−1,wt)T∇f(xt), where Dϕ(x,w) denotes the Jacobian of ϕwith respect to w. The nested structure between these two updates makes it challenging to analyse meta-learning, in particular it depends heavily on the properties of the Jacobian. In practice, ϕis highly complex and so Dϕis almost always intractable. For instance, in Xu et al. [2018a], the meta-parameters deﬁne the data-distribution under which a stochastic gradient is computed. In Zahavy et al. [2020], the meta-parameters deﬁne auxiliary objectives that are meant to help with representation learning; in Vinyals et al. [2016] they learn an embedding space for nearest-neighbour predictions. For this reason, the only theoretical results we are aware of specialise to the multi-task setting and assume ϕrepresents adaptation by gradient descent. In this setting, at each iteration t, the learner must adapt to a new task ft. The learner adapts by taking a (or several) gradient step(s) on ft using either a meta-learned initialisation [Flennerhag et al., 2019, Finn et al., 2019, Fallah et al., 2020, Wang et al., 2022] or using a meta-learned regulariser [Khodak et al., 2019, Denevi et al., 2019]. Because the update rule has this form, it is possible to treat the meta-optimisation problem as an online learning problem and derive convergence guarantees. Acceleration in this setup is driven by the tasks similarity. That is, if all tasks are sufﬁciently similar, a meta-learned update can accelerate convergence [Khodak et al., 2019]. However, these results do not yield acceleration in the absence of a task distribution to the best of our knowledge. This paper provides an alternative view. We study the classical convex optimisation setting of approximating the minimiser minxf(x). We observe that setting the update rule equal to the gradient, i.e. ϕ: (x,w) ↦→w∇f(x), recovers gradient descent. Similarly, we show in Section 3 that ϕcan be chosen to recover gradient descent with momentum. This offers another view of meta-learning as a non-linear transformation of classical optimisation. A direct implication of this is that a task similarity is not necessary condition for improving the rate of convergence via meta-learning. While there is ample empirical evidence to that effect [Xu et al., 2018b, Zahavy et al., 2020, Flennerhag et al., 2022, Luketina et al., 2022], we are only aware of theoretical results in the special case of meta-learned step sizes [Mahmood et al., 2012, van Erven and Koolen, 2016]. In particular, we analyse meta-learning using recent techniques developed for convex optimisation [Cutkosky, 2019, Joulani et al., 2020, Wang et al., 2021]. Given a function f that is convex with Lipschitz smooth gradients, meta-learning improves the rate of convergence by a multiplicative factor λto O(λ/T), via the smoothness of the update rule. Importantly, these works show that to achieve accelerated convergence, O(1/T2), some form of optimism is required. This optimism essentially provides a prediction of the next gradient, and hence represents a model of the geometry. We consider optimism with meta-learning in the convex setting and prove accelerated rates of convergence, O(λ/T2). Again, meta-learning affects these bounds by a multiplicative factor. We further show that optimism in meta-learning can be expressed through the recently proposed Bootstrapped Meta- Gradient method [BMG; Flennerhag et al., 2022]. Our analysis provides a ﬁrst proof of convergence for BMG and highlights the underlying mechanics that enable faster learning with BMG. Our main contributions are as follows: 1. We show that meta-learning contains gradient descent with momentum (Heavy Ball [Polyak, 1964]; Section 3) and Nesterov Acceleration [Nesterov, 1983] as special cases (Section 6). 2. We show that gradient-based meta-learning can be understood as a non-linear transformation of an underlying optimisation method (Section 3). 3. We establish rates of convergence for meta-learning in the convex setting (Sections 5 and 6). 4. We show that optimism can be expressed through [Flennerhag et al., 2022]. Our analysis (Section 6) provides a ﬁrst proof of convergence for BMG. 2Algorithm 1: Meta-learning in practice. input :Weights {βt}T t=1 input :Update rule ϕ input :Initialisation (x0,w1) for t= 1,2,...,T : xt = xt−1 + ϕ(xt−1,wt) ht(·) = f(xt−1 + ρtϕ(xt−1,·)) wt+1 = wt −βt∇ht(wt) return xT Algorithm 2: Meta-learning in the convex setting. input :Weights {αt}T t=1,{βt}T t=1 input :Update rule ϕ input :Initialisation (¯x0,w1) for t= 1,2,...,T : xt = ϕ(¯xt−1,wt) ¯xt = (1 −αt/α1:t)¯xt−1 + (αt/α1:t)xt gt = Dϕ(¯xt−1,wt)T∇f(¯xt) wt+1=arg minw∈W ∑t s=1αs⟨gs,w⟩+ 1 2βt ∥w∥2 return ¯xT 2 Meta-learning meets convex optimisation Problem deﬁnition. This section deﬁnes the problem studied in this paper and introduces our notation. Let f : X→ R be a proper and convex function. The problem of interest is to approximate the global minimum minx∈Xf(x). We assume a global minimiser exists and is unique, deﬁned by x∗= arg min x∈X f(x). (1) We assume that X⊆ Rn is a closed, convex and non-empty set. f is differentiable and has Lipschitz smooth gradients with respect to a norm ∥·∥ , meaning that there exists L ∈(0,∞) such that ∥∇f(x) −∇f(y)∥∗≤L∥x−y∥for all x,y ∈X, where ∥·∥∗is the dual norm of ∥·∥. We consider the noiseless setting for simplicity; our results carry over to the stochastic setting by replacing the key online-to-batch bound used in our analysis by its stochastic counterpart [Joulani et al., 2020]. Algorithm. Algorithm 1 describes a typical meta-learning algorithm. Unfortunately, at this level of generality, little can be said about the its convergence properties. Instead, we consider a stylized variant of meta-learning, described in Algorithm 2. This model differs in three regards: (a) it relies on moving averages (b) we use a different online learning algorithm for the meta-update, and (c) we make stricter assumptions on the update rule. We describe each component in turn. Let [T] = {1,2,...,T }. We are given weights{αt}T t=1, each αt >0, and an initialisation(¯x0,w1) ∈ X×W . At each time t ∈ [T], an update rule ϕ : X×W → Xgenerates the update xt = ϕ(¯xt−1,wt), where W⊆ Rm is closed, convex, and non-empty. We discuss ϕmomentarily. The algorithm maintains the online average ¯xt = x1:t α1:t = (1 −ρt)¯xt−1 + ρtxt, (2) where x1:t = ∑t s=1 αsxs, α1:t = ∑t s=1 αs, and ρt = αt/α1:t. Our goal is to establish conditions under which {¯xt}T t=1 converges to the minimiser x∗. While this moving average is not always used in practical applications, it is required for accelerated rates in online-to-batch conversion [Wang and Abernethy, 2018, Cutkosky, 2019, Joulani et al., 2020]. Convergence depends on how each wt is chosen. In Algorithm 1, the meta-learner faces a sequence of losses ht : W →R deﬁned by the composition ht(w) = f((1 −ρt)¯xt−1 + ρtϕ(¯xt−1,w)). This makes meta-learning a form of online optimisation [McMahan, 2017]. The meta-updates in Algorithm 1 is an instance of online gradient descent, which we can model as Follow-The-Regularized- Leader (FTRL; reviewed in Section 4). Given some norm ∥·∥ , an initialization w0 and β >0, FTRL sets each wt according to wt+1 = arg min w∈W ( t∑ s=1 αs⟨∇hs(ws),w⟩+ 1 2β∥w∥2 ) . (3) If ∥·∥ is the Euclidean norm, the interior solution to Eq. 3 is given by wt+1 = wt −αtβ∇ht(wt), the meta-update in Algorithm 1. It is straightforward to extend Eq. 3 to account for meta-updates that use AdaGrad-like [Duchi et al., 2011] acceleration by altering the norms [Joulani et al., 2017]. 30 20 40 60 80 100 Iterations 10 28 10 16 10 4 Loss 0 20 40 60 80 100 Iterations 0 20 40 60 80 100 Iterations 0 20 40 60 80 100 Iterations 0 20 40 60 80 100 Iterations Momentum Meta-Momentum AdaGrad Meta-AdaGrad 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 10 22 10 12 10 2 Loss 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate 0.1 0.3 0.5 0.7 0.9 0.990.9999 3.0 5.0 Learning rate Momentum Meta-Momentum AdaGrad Meta-AdaGrad Figure 2: Convex Quadratic. We generate convex quadratic loss functions with ill-conditioning and compare gradient descent with momentum and AdaGrad to meta-learning variants. Meta-Momentum uses ϕ: (x,w) ↦→w⊙∇f(x) while Meta-AdaGrad uses ϕ: (x,w) ↦→∇f(x)/√w, where division is element-wise. Top: loss per iteration for randomly sampled loss functions. Bottom: cumulative loss (regret) at the end of learning as a function of learning rate; details in Appendix B. Update rule. It is not possible to prove convergence outside of the convex setting, since ϕmay reach a local minimum where it cannot yield better updates, but the updates are not sufﬁcient to converge. Convexity means that each ht must be convex, which requires that ϕis afﬁne in w(but may vary non-linearly in x). We also assume that ϕis smooth with respect to ∥·∥, in the sense that it has bounded norm; for all x∈X and all w∈W we assume that there exists λ∈(0,∞) for which ∥Dϕ(x,w)T∇f(x)∥2 ∗≤λ∥∇f(x)∥2 ∗. These assumptions hold for any smooth update rule up to ﬁrst-order Taylor approximation error. 3 Meta-Gradients in the Convex Setting - An Overview In this section, we provide an informal discussion of our main results (full analysis; Sections 5 and 6). Meta-Gradients without Optimism. The main difference between classical optimisation and meta-learning is the introduction of the update rule ϕ. To see how this acts on optimisation, consider two special cases. If the update rule just return the gradient, ϕ = ∇f, Algorithm 2 is reduced to gradient descent (with averaging). The inductive bias is ﬁxed and does not change with past experience, and so acceleration is not possible—the rate of convergence is O(1/ √ T) [Wang et al., 2021]. The other extreme is an update rule that only depends on the meta-parameters, ϕ(x,w) = w. Here, the meta-learner has ultimate control and selects the next update without constraints. The only relevant inductive bias is contained in w. To see how this inductive bias is formed, suppose ∥·∥ = ∥·∥ 2 so that Eq. 3 yields wt+1 = wt −αtρtβ∇f(¯xt) (assuming an interior solution). Combining this with the moving average in Eq. 2, we may write the learner’s iterates as ¯xt = ¯xt−1 + ˜ρt(¯xt−1 −¯xt−2) −˜βt∇f(¯xt−1), where each ˜ρt = ρt 1−ρt−1 ρt−1 and ˜βt = αtρtβ; setting β = 1/(2L) and each αt = tyields ˜ρt = t−2 t+1 and ˜βt = t/(4(t+ 1)L). Hence, the canonical momentum algorithm, Polyak’s Heavy-Ball method [Polyak, 1964], is obtained as the special case of meta-learning under the update ruleϕ: (x,w) ↦→w. Because Heavy Ball carries momentum from past updates, it can encode a model of the learning dynamics that leads to faster convergence, on the order O(1/T). The implication of this is that the dynamics of meta-learning are fundamentally momentum-based and thus learns an update rule in the same cumulative manner. This manifests theoretically through its convergence guarantees. Theorem 1 (Informal). Set αt = 1 and β = 1 λL. If each xt is generated under Algorithm 2, then for any viable ϕ, f(¯xT) −f(x∗) ≤λLdiam(W) T . We refer the reader to Theorem 3 for a formal statement. Compared to Heavy Ball, meta-learning introduces a constant λthat captures the smoothness of the update rule. Hence, while meta-learning does not achieve better scaling inT through ϕ, it can improve upon classical optimisation by a constant factor if λ< 1. That meta-learning can improve upon momentum is borne out experimentally. In Figure 2, we consider the problem of minimizing a convex quadratic f : x ↦→⟨x,Qx⟩, where Q ∈Rn×n is PSD but ill-conditioned. We compare momentum to a meta-learned step-size, i.e. ϕ: (x,w) ↦→w⊙∇f(x), where ⊙is the Hadamard product. Across randomly sampled Qmatrices 4(details: Appendix B), we ﬁnd that introducing a non-linearityϕleads to a sizeable improvement in the rate of convergence. We also compare AdaGrad to a meta-learned version,ϕ: (x,w) ↦→∇f(x)/√w, where division is element-wise. While AdaGrad is a stronger baseline on account of being parameter- free, we ﬁnd that meta-learning the scale vector consistently leads to faster convergence. Meta-Gradients with Optimism. It is well known that minimizing a smooth convex function admits convergence rates of O(1/T2). Our analysis of standard meta-gradients does not achieve such acceleration. Previous work indicate that we should not expect to either; to achieve the theoretical lower-limit of O(1/T2), some form of optimism (reviewed in Section 4) is required. A typical form of optimism is to predict the next gradient. This is how Nesterov Acceleration operates [Nesterov, 1983] and is the reason for its O(1/T2) convergence guarantee. From our perspective, meta-learning is a non-linear transformation of the iterate x. Hence, we should expect optimism to play a similarly crucial role. Formally, optimism comes in the form of hint functions {˜gt}T t=1, each ˜gt ∈Rm, that are revealed to the meta-learner prior to selecting wt+1. These hints give rise to Optimistic Meta-Learning (OML) via meta-updates wt+1 = arg min w∈W ( αt+1˜gt+1 + t∑ s=1 αs⟨∇hs(ws),w⟩+ 1 2βt ∥w∥2 ) . (4) If the hints are accurate, meta-learning with optimism can achieve an accelerated rate of O(˜λ/T2), where ˜λis a constant that characterises the smoothness of ϕ, akin to λ. Again, we ﬁnd that meta- learning behaves as a non-linear transformation of classical optimism and its rate of convergence is governed by the geometry it induces. We summarise this result in the following result. Theorem 2 (Informal). Let each hint be given by ˜gt+1 = Dϕ(¯xt−1,wt)T∇f(¯xt). Assume that ϕis sufﬁciently smooth. Set αt = tand βt = t−1 2t˜λL, then f(¯xT) −f(x∗) ≤4˜λLdiam(W) T2−1 . For a formal statement, see Theorem 4. These predictions hold empirically in a non-convex setting. We train a 50-layer ResNet using either SGD with a ﬁxed learning rate, or an update rule that adapts a per-parameter learning rate online,ϕ: (x,w) ↦→w⊙∇f(x). We compare the standard meta-learning approach without optimism to optimistic meta-learning. Figure 1 shows that optimism is critical for meta-learning to achieve acceleration, as predicted by theory (experiment details in Appendix C). 4 Analysis preliminaries: Online Convex Optimisation In this section, we present analytical tools from the optimisation literature that we build upon. In a standard optimisation setting, there is no update rule ϕ; instead, the iterates xt are generated by a gradient-based algorithm, akin to Eq. 3. In particular, our setting reduces to standard optimisation if ϕis deﬁned by ϕ: (x,w) ↦→w, in which case xt = wt. A common approach to analysis is to treat the iterates x1,x2,... as generated by an online learning algorithm over online losses, obtain a regret guarantee for the sequence, and use online-to-batch conversion to obtain a rate of convergence. Online Optimisation. In online convex optimisation [Zinkevich, 2003], a learner is given a convex decision set Uand faces a sequence of convex loss functions {αtft}T t=1. At each time t ∈[T], it must make a prediction ut prior to observing αtft, after which it incurs a loss αtft(ut) and receives a signal—either αtft itself or a (sub-)gradient of αtft(ut). The learner’s goal is to minimiseregret, R(T) := ∑T t=1 αt(ft(ut) −ft(u)), against a comparator u∈U. An important property of a convex function f is f(u′) −f(u) ≤⟨∇f(u′),u′−u⟩. Hence, the regret is largest under linear losses:∑T t=1 αt(ft(ut) −ft(u)) ≤∑T t=1 αt⟨∇ft(ut),ut −u⟩. For this reason, it is sufﬁcient to consider regret under linear loss functions. An algorithm has sublinear regret if limT→∞R(T)/T = 0. FTRL & AO-FTRL. The meta-update in Eq. 3 is an instance of Follow-The-Regularised-Leader (FTRL) under linear losses. In Section 6, we show that BMG is an instance of the Adaptive-Optimistic FTRL (AO-FTRL), which is an extension due to [Rakhlin and Sridharan, 2013, Mohri and Yang, 2016, Joulani et al., 2020, Wang et al., 2021]. In AO-FTRL, we have a strongly convex regulariser ∥·∥2. FTRL and AO-FTRL sets the ﬁrst prediction u1 to minimise ∥·∥2. Given linear losses {gs}t−1 s=1 and learning rates {βt}T t=1, each βt > 0, the algorithm proceeds according to ut = arg min u∈U ( αt⟨˜gt,u⟩+ t−1∑ s=1 αs⟨gs,u⟩+ 1 2βt ∥u∥2 ) , (5) 5where each ˜gt is a “hint” that enables optimistic learning [Rakhlin and Sridharan, 2013, Mohri and Yang, 2016]; setting ˜gt = 0 recovers the original FTRL algorithm. The goal of a hint is to predict the next loss vector gt; if the predictions are accurate AO-FTRL can achieve lower regret than its non-optimistic counter-part. Since ∥·∥2 is strongly convex, FTRL is well deﬁned in the sense that the minimiser exists, is unique and ﬁnite [McMahan, 2017]. The regret of FTRL and AO-FTRL against any comparator u ∈U can be upper-bounded by R(T) = T∑ t=1 αt⟨gt,ut −u⟩≤ ∥u∥2 2βT + 1 2 T∑ t=1 α2 tβt∥gt −˜gt∥2 ∗. (6) Hence, hints that predict gt well can reduce the regret substantially. Without hints, FTRL can guarantee O( √ T) regret (for non strongly convex loss functions). However, Dekel et al. [2017] show that under linear losses, if hints are weakly positively correlated—deﬁned as ⟨gt,˜gt⟩≥ ϵ∥gt∥2 for some ϵ >0—then the regret guarantee improves to O(log T), even for non strongly-convex loss functions. We believe optimism provides an exciting opportunity for novel forms of meta-learning. Finally, we note that these regret bounds (and hence our analysis) can be extended to stochastic optimisation [Mohri and Yang, 2016, Joulani et al., 2017]. Online-to-batch conversion. The main idea behind online to batch conversion is that, forfconvex, Jensen’s inequality givesf(¯xT)−f(x∗) ≤∑T t=1 αt⟨∇f(xt),xt−x∗⟩/α1:T. Hence, one can provide a convergence rate by ﬁrst establishing the regret of the algorithm that generates xt, from which one obtains the convergence rate of the moving average of iterates. Applying this naively yields O(1/T) rate of convergence. In recent work, Cutkosky [2019] shows that one can upper-bound the sub-optimality gap by instead querying the gradient gradient at the average iterate, f(¯xT) −f(x∗) ≤∑T t=1 αt⟨∇f(¯xt),xt −x∗⟩/α1:T, which can yield faster rates of convergence. Recently, Joulani et al. [2020] tightened the analysis and proved that the sub-optimality gap can be bounded by f(¯xT) −f(x∗) ≤ 1 α1:T ( Rx(T) −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) , (7) were we deﬁne Rx(T) := ∑T t=1 αt⟨∇f(¯xt),xt −x∗⟩as the regret of the sequence {xt}T t=1 against the comparator x∗. With this machinery in place, we now turn to deriving our main results. 5 Analysis Our analytical goal is to apply the online-to-batch conversion bound in Eq. 7 to the iterates x1,x2,...,x T that Algorithm 2 generates. Our main challenge is that the update rule ϕprevents a straightforward application of this bound. Instead, we must upper bound the learner’s regret Rx by the meta-learner’s regret, which is deﬁned in terms of the iterates w1,w2,...,w T. To this end, we may decompose Rx as follows: Rx(T) = T∑ t=1 αt⟨∇f(¯xt),xt −x∗⟩= T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −x∗⟩ = T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩+ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩. The ﬁrst term in the ﬁnal expression can be understood as the regret under convex losses ℓt(·) = αt⟨∇f(¯xt),ϕ(¯xt−1,·)⟩. Since ϕ(¯xt−1,·) is afﬁne, ℓt is convex and can be upper bounded by its linearisation. The linearisation reads ⟨Dϕ(¯xt−1,wt)T∇f(¯xt),·⟩, which is identical the linear losses ⟨∇ht(wt),·⟩faced by the meta-learner in Eq. 3. Hence, we may upper bound Rx(T) by Rx(T) ≤ T∑ t=1 αt⟨Dϕ(¯xt−1,wt)T∇f(¯xt),wt −w∗⟩+ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩ = T∑ t=1 αt⟨∇ht(wt),wt −w∗⟩+ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩ = Rw(T) + T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,w∗) −x∗⟩, (8) 6where the last identity follows by deﬁnition: Rw(T) := ∑T t=1 αt⟨∇ht(wt),wt −w∗⟩. For the last term in Eq. 8 to be negative, so that Rw(T) ≥Rx(T), we need the relative power of the comparator w∗to be greater than that of the comparator x∗. Intuitively, the comparator x∗is non-adaptive. It must make one choice x∗and suffer the average loss. In contrast, the comparator w∗becomes adaptive under the update rule; it can only choose one w∗, but on each round it plays ϕ(¯xt−1,w∗). If ϕ is sufﬁciently ﬂexible, this gives the comparator w∗more power than x∗, and hence it can force the meta-learner to suffer greater regret. When this is the case, we say that regret is preserved when moving from x∗ to w∗. Deﬁnition 1. Given f, {αt}T t=1, and {xt}T t=1, an update rule ϕ: X×W→X preserves regret if there exists a comparator w∈W that satisﬁes T∑ t=1 αt⟨ϕ(¯xt−1,w),∇f(¯xt)⟩≤ T∑ t=1 αt⟨x∗,∇f(¯xt)⟩. (9) If such wexists, let w∗denote the comparator with smallest norm ∥w∥. By inspecting Eq. 9, we see that if ϕ(¯xt−1,·) can be made to negatively align with the gradient ∇f(¯xt), the update rule preserves regret. Hence, any update rule that is gradient-like in its behaviour can be made to preserve regret. However, this must not hold on every step, only sufﬁciently often; nor does it imply that the update rule must explicitly invoke∇f; for instance, update rules that are afﬁne in wpreserve regret if the diameter of Wis sufﬁciently large, provided the update rule is not degenerate. Lemma 1. Given f, {αt}T t=1, and {xt}T t=1, if ϕpreserves regret, then Rx(T) = T∑ t=1 αt⟨∇f(¯xt),xt −x∗⟩≤ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩= Rw(T). Proof: Appendix D. With Lemma 1, we can provide a convergence guarantee for meta-gradients in the convex setting. The mechanics of the proof is to use online-to-batch conversion to upper bound f(¯xT)−f(x∗) ≤Rx(T)/α1:T and then appeal to Lemma 1 to obtainf(¯xT)−f(x∗) ≤Rw(T)/α1:T, from which point we can plug in the FTRL regret bound. Theorem 3. Let ϕpreserve regret and assume Algorithm 2 satisﬁes the assumptions in Section 2. Then f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 β + T∑ t=1 λβα2 t 2 ∥∇f(¯xt)∥2 ∗ −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . Moreover, ifx∗is a global minimiser of f, setting αt = 1 and β = 1 λL yields f(¯xT) −f(x∗) ≤λLdiam(W) T . Proof: Appendix D. 6 Meta-Learning meets Optimism The reason Theorem 3 fails to achieve acceleration is because the negative terms,−∥∇f(¯xt−1) − ∇f(¯xt)∥2 ∗, do not come into play. This is because the positive term in the bound involves the norm of the gradient, rather than the norm of the difference of two gradients. The former is typically a larger quantity and hence we cannot guarantee that they vanish. To obtain acceleration, we need some form of optimism. In this section, we consider an alteration to Algorithm 2 that uses AO-FTRL for the meta-updates. Given some sequence of hints {˜gt}T t=1, each ˜gt ∈Rm, each wt+1 is given by wt+1 = arg min w∈W ( αt+1˜gt+1 + t∑ s=1 αs⟨∇hs(ws),w⟩+ 1 2βt ∥w∥2 ) . (10) 7Algorithm 3: BMG in practice. input :Weights {βt}T t=1 input :Update rule ϕ input :Target oracle input :Initialisation (x0,w1) for t= 1,2,...,T : xt = xt−1 + ϕ(xt−1,wt) Query zt from target oracle dt(·) = ∥zt −xt + ϕ(xt,·)∥2 wt+1 = wt −βt∇dt(wt) return xT Algorithm 4: Convex optimistic meta-learning. input :Weights {αt}T t=1,{βt}T t=1 input :Update rule ϕ input :Hints {˜gt}T t=1 input :Initialisation (¯x0,w1) for t= 1,2,...,T : xt = ϕ(¯xt−1,wt) ¯xt = (1 −αt/α1:t)¯xt−1 + (αt/α1:t)xt gt = Dϕ(¯xt−1,wt)T∇f(¯xt) vt = αt+1˜gt+1 + ∑t s=1 αsgs wt+1 = arg minw∈W⟨vt,w⟩+ 1 2βt ∥w∥2 return ¯xT Otherwise, we proceed as in Algorithm 2; for a complete description, see Algorithm 4. The AO-FTRL updates do not correspond to a standard meta-update. However, we show momentarily that optimism can be instantiated via the BMG method, detailed in Algorithm 3. The proof for optimistic meta- gradients proceed largely as in Theorem 3, it only differs in that we apply the AO-FTRL regret bound. Theorem 4. Let ϕpreserve regret and assume Algorithm 4 satisfy the assumptions in Section 2. Then f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 βT + T∑ t=1 α2 tβt 2 ∥Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt∥2 ∗ −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . Moreover, assume each˜gt is such that ∥Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt∥2 ∗≤q∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ for some q >0. If each αt = tand βt = t−1 2tqL, then f(¯xt) −f(x∗) ≤4qLdiam(W) T2 −1 . Proof. The proof follows the same lines as that of Theorem 3. The only difference is that the regret of the {wt}T t=1 sequence can be upper bounded by ∥w∗∥2 βT + 1 2 ∑T t=1 α2 tβt∥∇ht(wt) −˜gt∥2 ∗instead of ∥w∗∥2 βT + 1 2 ∑T t=1 α2 tβt∥∇ht(wt)∥2 ∗, as per the AO-FTRL regret bound in Eq. 6. The ﬁnal part follows immediately by replacing the norms and plugging in the values for αand β. ■ From Theorem 4, it is clear that if ˜gt is a good predictor of Dϕ(¯xt−1,wt)T∇f(¯xt), then the positive term in the summation can be cancelled by the negative term. In a classical optimisation setting, Dϕ= In, and hence it is easy to see that simply choosing ˜gt to be the previous gradient is sufﬁcient to achieve the cancellation [Joulani et al., 2020]. Indeed, this choice gives us Nesterov’s Accelerated rate [Wang et al., 2021]. The upshot of this is that we can specialise Algorithm 4 to capture Nesterov’s Accelerated method by choosing ϕ: (x,w) ↦→w—as in the reduction to Heavy Ball—and setting the hints to ˜gt = ∇f(¯xt−1). Hence, while the standard meta-update without optimism contains Heavy Ball as a special case, the optimistic meta-update contains Nesterov Acceleration as a special case. In the meta-learning setting, Dϕis not an identity matrix, and hence the best targets for meta-learning are different. Naively, choosing ˜gt = Dϕ(¯xt−1,wt)T∇f(¯xt−1) would lead to a similar cancellation, but this is not allowed. At iteration t, we have not computed wt when ˜gt is chosen, and hence Dϕ(¯xt−1,wt) is not available. The nearest term that is accessible is Dϕ(¯xt−2,wt−1). Corollary 1. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T∇f(¯xt). Assume that ϕsatisﬁes Dϕ(x′,w)T∇f(x) −Dϕ(x′′,w′)T∇f(x′) 2 ∗≤˜λ∥∇f(x′) −∇f(x)∥2 ∗ for all x′′,x′,x ∈X and w,w′ ∈W , for some ˜λ >0. If each αt = t and βt = t−1 2t˜λL, then f(¯xT) −f(x∗) ≤4˜λLdiam(W) T2−1 . Proof: Appendix D. 86.1 Bootstrapped Meta-Gradients In this section, we present a simpliﬁed version of BMG for clarity, with Appendix E providing a fuller comparison. Essentially, BMG alters the meta-update in Algorithm 1; instead of directly minimising the loss f, it introduces a sequence of targets z1,z2,... and the meta-learner’s goal is select wso that the updated parameters minimise the distance these targets. Concretely, given an update xt = xt−1 + ϕ(xt−1,wt), targets are bootstrapped from xt, meaning that a vector yt is computed to produce the target zt = xt −yt. Assuming the distance to the target is measured under 1 2 ∥·∥ 2 2, the BMG meta-update takes the form wt+1 = wt −Dϕ(xt−1,wt)Tyt. Depending on how yt is computed, it can encode optimism. For instance, the authors rely on the update rule itself to compute a tangent yt = ϕ(xt,wt) −∇f(xt + ϕ(xt,wt)). This encodes optimism via ϕbecause it encourages the meta-learner to build up momentum (i.e. to accumulate past updates). We can contrast this with the types of updates produced by AO-FTRL in Eq. 10. If we have hints ˜gt+1 = Dϕ(¯xt−1,wt)T˜yt+1 for some ˜yt+1 ∈Rn and set ∥·∥ = ∥·∥ 2; assuming an interior solution, Eq. 10 yields wt+1 = wt −Dϕ(¯xt−1,wt)T(αt+1 ˜yt+1 + αt∇f(¯xt))   BMG update + αtDϕ(¯xt−2,wt−1)T˜yt   FTRL error correction . (11) Hence, BMG encodes very similar dynamics to those of AO-FTRL in Eq. 10. Under this choice of hints, the main qualitative difference is that AO-FTRL includes a correction term. The effect of this term is to “undo” previous hints to avoid feedback loops. Notably, BMG can suffer from divergence due to feedback if the gradient in yt is not carefully scaled [Flennerhag et al., 2022]. Our theoretical analysis suggests a simple correction method that may stabilize BMG in practice. More generally, targets in BMG are isomorphic to the hint function in AO-FTRL if the measure of distance in BMG is a Bregman divergence under a strongly convex function (Appendix E). An immediate implication of this is that the hints in Corollary 1 can be expressed as targets in BMG, and hence if BMG satisﬁes the assumptions involved, it converges at a rate O(˜λ/T2). More generally, Theorem 4 provides a sufﬁcient condition for any target bootstrap in BMG to achieve acceleration. Corollary 2. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T˜yt+1, for some ˜yt+1 ∈Rn. If each ˜yt+1 is a better predictor of the next gradient than ∇f(¯xt−1), in the sense that ∥Dϕ(¯xt−2,wt−1)T˜yt −Dϕ(¯xt−1,wt)T∇f(¯xt)∥∗≤˜λ∥∇f(¯xt) −∇f(¯xt−1)∥∗, then Algorithm 4 guarantees convergence at a rate O(˜λ/T2). 7 Conclusion This paper explores a connection between convex optimisation and meta-learning. We construct an algorithm for convex optimisation that aligns as closely as possible with how meta-learning is done in practice. Meta-learning introduces a transformation and we study the effect this transformation has on the rate of convergence. We ﬁnd that, while a meta-learned update rule cannot generate a better dependence on the horizon T, it can improve upon classical optimisation up to a constant factor. An implication of our analysis is that for meta-learning to achieve acceleration, it is important to introduce some form of optimism. From a classical optimisation point of view, such optimism arises naturally by providing the meta-learner with hints. If hints are predictive of the learning dynamics these can lead to signiﬁcant acceleration. We show that the recently proposed BMG method provides a natural avenue to incorporate optimism in practical application of meta-learning. Because targets in BMG and hints in optimistic online learning commute, our results provide ﬁrst rigorous proof of convergence for BMG, while providing a general condition under which optimism in BMG yields accelerated learning. 9References F. Alet, M. F. Schneider, T. Lozano-Perez, and L. P. Kaelbling. Meta-Learning Curiosity Algorithms. In International Conference on Learning Representations, 2020. M. Andrychowicz, M. Denil, S. Gómez, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning to Learn by Gradient Descent by Gradient Descent. In Advances in Neural Information Processing Systems, 2016. Y . Bengio. Gradient-Based Optimization of Hyperparameters.Neural computation, 12(8):1889–1900, 2000. Y . Bengio, S. Bengio, and J. Cloutier.Learning a Synaptic Learning Rule. Université de Montréal, Département d’informatique et de recherche opérationnelle, 1991. Y . Chen, M. W. Hoffman, S. G. Colmenarejo, M. Denil, T. P. Lillicrap, and N. de Freitas. Learning to learn for Global Optimization of Black Box Functions. In Advances in Neural Information Processing Systems, 2016. A. Cutkosky. Anytime Online-to-Batch, Optimism and Acceleration. In International Conference on Machine Learning, 2019. O. Dekel, A. Flajolet, N. Haghtalab, and P. Jaillet. Online learning with a hint. In Advances in Neural Information Processing Systems, 2017. G. Denevi, D. Stamos, C. Ciliberto, and M. Pontil. Online-Within-Online Meta-Learning. InAdvances in Neural Information Processing Systems, 2019. J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011. A. Fallah, A. Mokhtari, and A. Ozdaglar. On the Convergence Theory of Gradient-Based Model- Agnostic Meta-Learning Algorithms. In International Conference on Artiﬁcial Intelligence and Statistics, 2020. C. Finn, P. Abbeel, and S. Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In International Conference on Machine Learning, 2017. C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online Meta-Learning. InInternational Conference on Machine Learning, 2019. S. Flennerhag, P. G. Moreno, N. D. Lawrence, and A. Damianou. Transferring Knowledge across Learning Processes. In International Conference on Learning Representations, 2019. S. Flennerhag, Y . Schroecker, T. Zahavy, H. van Hasselt, D. Silver, and S. Singh. Bootstrapped Meta-Learning. In International Conference on Learning Representations, 2022. G. E. Hinton and D. C. Plaut. Using Fast Weights to Deblur Old Memories. In Cognitive Science Society, 1987. S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning To Learn Using Gradient Descent. In International Conference on Artiﬁcial Neural Networks, 2001. T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-Learning in Neural Networks: A Survey. arXiv preprint arXiv:2004.05439, 2020. P. Joulani, A. György, and C. Szepesvári. A modular analysis of adaptive (non-) convex optimization: Optimism, composite objectives, and variational bounds. Journal of Machine Learning Research, 1:40, 2017. P. Joulani, A. Raj, A. Gyorgy, and C. Szepesvári. A Simpler Approach to Accelerated Optimization: Iterative Averaging Meets Optimism. In International Conference on Machine Learning, 2020. M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar. Adaptive Gradient-Based Meta-Learning Methods. In Advances in Neural Information Processing Systems, 2019. 10L. Kirsch, S. van Steenkiste, and J. Schmidhuber. Improving Generalization in Meta Reinforcement Learning Using Learned Objectives. arXiv preprint arXiv:1910.04098, 2019. J. Luketina, S. Flennerhag, Y . Schroecker, D. Abel, T. Zahavy, and S. Singh. Meta-gradients in non-stationary environments. In ICLR Workshop on Agent Learning in Open-Endedness, 2022. D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-Based Hyperparameter Optimization Through Reversible Learning. In International conference on machine learning, pages 2113–2122. PMLR, 2015. A. R. Mahmood, R. S. Sutton, T. Degris, and P. M. Pilarski. Tuning-Free Step-Size Adaptation. In ICASSP, 2012. H. B. McMahan. A survey of algorithms and analysis for adaptive online learning. The Journal of Machine Learning Research, 18(1):3117–3166, 2017. M. Mohri and S. Yang. Accelerating Online Convex Optimization via Adaptive Prediction. In International Conference on Artiﬁcial Intelligence and Statistics, 2016. Y . E. Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983. A. Nichol, J. Achiam, and J. Schulman. On First-Order Meta-Learning Algorithms. arXiv preprint ArXiv:1803.02999, 2018. J. Oh, M. Hessel, W. M. Czarnecki, Z. Xu, H. P. van Hasselt, S. Singh, and D. Silver. Discovering Reinforcement Learning Algorithms. In Advances in Neural Information Processing Systems , volume 33, 2020. B. T. Polyak. Some Methods of Speeding up the Convergence of Iteration Methods. USSR Computa- tional Mathematics and Mathematical Physics, 4(5):1–17, 1964. S. Rakhlin and K. Sridharan. Optimization, Learning, and Games with Predictable Sequences. In Advances in Neural Information Processing Systems, 2013. E. Real, C. Liang, D. R. So, and Q. V . Le. AutoML-Zero: Evolving Machine Learning Algorithms From Scratch. In International Conference on Machine Learning, 2020. J. Schmidhuber. Evolutionary Principles in Self-Referential Learning . PhD thesis, Technische Universität München, 1987. T. van Erven and W. M. Koolen. MetaGrad: Multiple Learning Rates in Online Learning. InAdvances in Neural Information Processing Systems, 2016. O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching Networks for One Shot Learning. In Advances in Neural Information Processing Systems, 2016. H. Wang, Y . Wang, R. Sun, and B. Li. Global convergence of maml and theory-inspired neural architecture search for few-shot learning. In Computer Vision and Pattern Recognition, 2022. J.-K. Wang and J. Abernethy. Acceleration through Optimistic No-Regret Dynamics. arXiv preprint arXiv:1807.10455, 2018. J.-K. Wang, J. Abernethy, and K. Y . Levy. No-regret dynamics in the fenchel game: A uniﬁed framework for algorithmic convex optimization. arXiv preprint arXiv:2111.11309, 2021. J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to Reinforcement Learn. In Annual Meeting of the Cognitive Science Society, 2016. T. Xu, Q. Liu, L. Zhao, and J. Peng. Learning to Explore with Meta-Policy Gradient. In International Conference on Machine Learning, 2018a. Z. Xu, H. P. van Hasselt, and D. Silver. Meta-Gradient Reinforcement Learning. In Advances in Neural Information Processing Systems, 2018b. 11Z. Xu, H. P. van Hasselt, M. Hessel, J. Oh, S. Singh, and D. Silver. Meta-gradient reinforcement learning with an objective discovered online. Advances in Neural Information Processing Systems, 33:15254–15264, 2020. T. Zahavy, Z. Xu, V . Veeriah, M. Hessel, J. Oh, H. P. van Hasselt, D. Silver, and S. Singh. A Self-Tuning Actor-Critic Algorithm. In Advances in Neural Information Processing Systems , volume 33, 2020. M. Zinkevich. Online Convex Programming and Generalized Inﬁnitesimal Gradient Ascent. In International Conference on Machine Learning, 2003. 12Appendix A Notation Table 1: Notation Indices t Iteration index: t∈{1,...,T }. T Total number of iterations. [T] The set {1,2,...,T }. i Component index: xi is the ith component of x= (x1,...,x n). αa:b Sum of weights: αa:b = ∑b s=aαs xa:b Weighted sum: xa:b = ∑b s=aαsxs ¯xa:b Weighted average: ¯xa:b = xa:b/αa:b Parameters x∗∈X Minimiser of f. xt ∈X Parameter at time t ¯xt ∈X Moving average of {xs}t s=1 under weights {αs}t s=1. ρt ∈(0,∞) Moving average coefﬁcient αt/α1:t. wt ∈W Meta parameters w∗∈X w∈W that retains regret with smallest norm ∥w∥. αt ∈(0,∞) Weight coefﬁcients βt ∈(0,∞) Meta-learning rate Maps f : X→ R Objective function ∥·∥ : X→ R Norm on X. ∥·∥∗: X∗→R Dual norm of ∥·∥. ht : W→ R Online loss faced by the meta learner Rx(T) Regret of {xt}T t=1 against x∗: Rx(T) := ∑T t=1 αt⟨∇f(¯xt),xt−x∗⟩. Rw(T) Rw(T) := ∑T t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩. ϕ: Rn×Rm →Rn Generic update rule used in practice Dϕ(x,·) : Rm→Rn×m Jacobian of ϕw.r.t. its second argument, evaluated atx∈Rn. ϕ: X×W→X Update rule in convex setting Dϕ(x,·) : W→ Rn×m Jacobian of ϕw.r.t. its second argument, evaluated atx∈X. Bµ : Rn×Rn→[0,∞) Bregman divergence under µ: Rn →R. µ: Rn →R Convex distance generating function. 13Table 2: Hyper-parameter sweep on Convex Quadratics. All algorithms are tuned for learning rate and initialisation of w. Baselines are tuned for decay rate; meta-learned variant are tuned for the meta-learning rate. Learning rate [.1, .3, .7, .9, 3., 5.] winit scale [0., 0.3, 1., 3., 10., 30.] Decay rate / Meta-learning rate [0.001, 0.003, 0.01, .03, .1, .3, 1., 3., 10., 30.] B Convex Quadratic Experiments Loss function. We consider the problem of minimising a convex quadratic loss functions f : R2 →R of the form f(x) = xTQx, where Q is randomly sampled as follows. We sample a random orthogonal matrix U from the Haar distribution scipy.stats.ortho_group. We con- struct a diagonal matrix of eigenvalues, ranked smallest to largest, with λi = i2. Hence, the ﬁrst dimension has an eigenvalue 1 and the second dimension has eigenvalue 4. The matrix Qis given by UT diag(λ1,...,λ n)U. Protocol. Given that the solution is always (0,0), this experiment revolves around understanding how different algorithms deal with curvature. Given symmetry in the solution and ill-conditioning, we ﬁx the initialisation to x0 = (4 ,4) for all sampled Qs and all algorithms and train for 100 iterations. For each Qand each algorithm, we sweep over the learning rate, decay rate, and the initialization of w see Table 2. For each method, we then report the results for the combination of hyper parameters that performed the best. Results. We report the learning curves for the best hyper-parameter choice for 5 randomly sampled problems in the top row of Figure 2 (columns correspond to different Q). We also study the sensitivity of each algorithm to the learning rate in the bottom row Figure 2. For each learning rate, we report the cumulative loss during training. While baselines are relatively insensitive to hyper-parameter choice, meta-learned improve for certain choices, but are never worse than baselines. C Imagenet Experiments Protocol. We train a 50-layer ResNet following the Haiku example, available athttps://github. com/deepmind/dm-haiku/blob/main/examples/imagenet. We modify the default setting to run with SGD. We compare default SGD to variants that meta-learn an element-wise learning rate online, i.e. (x,w) ↦→w⊙∇f(x). For each variant, we sweep over the learning rate (for SGD) or meta-learning rate. We report results for the best hyper-parameter over three independent runs. Standard meta-learning. In the standard meta-learning setting, we apply the update rule once before differentiating w.r.t. the meta-parameters. That is, the meta-update takes the form wt+1 = wt −β∇ht(wt), where ht = f(xt + wt ⊙∇f(xt)). Because the update rule is linear in w, we can compute the meta-gradient analytically: ∇ht(wt) = ∇wf(x+ ϕ(x,w)) = Dϕ(x,w)T∇f(x′) = ∇f(x) ⊙∇f(x′), where x′ = x+ ϕ(x,w). Hence, we can compute the meta-updates in Algorithm 1 manually as wt+1 = max{wt−β∇f(xt) ⊙∇f(xt+1),0.}, where we introduce the max operator on an element- wise basis to avoid negative learning rates. Empirically, this was important to stabilize training. Optimistic meta-learning. For optimistic meta-learning, we proceed much in the same way, but include a gradient prediction ˜gt+1. For our prediction, we use the previous gradient, ∇f(xt+1), as our prediction. Following Eq. 11, this yields meta-updates of the form wt+1 = max { wt −β∇f(xt+1) ⊙(∇f(xt+1) + ∇f(xt)) −∇f(xt) ⊙∇f(xt),0. } . Results. We report Top-1 accuracy on the held-out test set as a function of training steps in Figure 1. Tuning the learning rate does not yield any statistically signiﬁcant improvements under standard meta-learning. However, with optimistic meta-learning, we obtain a signiﬁcant acceleration as well as improved ﬁnal performance, increasing the mean ﬁnal top-1 accuracy from 72% to 75%. 14Table 3: Hyper-parameter sweep on Imagenet. (Meta-)learning rate [0.001, 0.01, 0.02, 0.05, 0.1] D Proofs This section provides complete proofs. We restate the results for convenience. Lemma 1. Given f, {αt}T t=1, and {xt}T t=1, if ϕpreserves regret, then Rx(T) = T∑ t=1 αt⟨∇f(¯xt),xt −x∗⟩≤ T∑ t=1 αt⟨∇f(¯xt),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩= Rw(T). Proof. Starting from Rx in Eq. 8, if the update rule preserves regret, there exists w∗∈W for which Rx(T) = T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −x∗⟩ = T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩+ T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,w∗) −x∗⟩ ≤ T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩= Rw(T), since w∗is such that ∑T t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,w∗) −x∗⟩≤ 0. ■ Theorem 3. Let ϕpreserve regret and assume Algorithm 2 satisfy the assumptions in Section 2. Then f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 β + T∑ t=1 λβα2 t 2 ∥∇f(¯xt)∥2 ∗ −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . If x∗is a global minimiser of f, setting αt = 1 and β = 1 λL yields f(¯xT) −f(x∗) ≤λLdiam(W) T . Proof. Since ϕpreserves regret, by Lemma 1, the regret term Rx(T) in Eq. 7 is upper bounded by Rw(T). We therefore have f(¯xT) −f(x∗) ≤ 1 α1:T ( Rw(T) −αt 2L∥∇f(¯xt) −∇f(x∗)∥2 ∗−α1:t−1 2L ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗ ) . (12) Next, we need to upper-bound Rw(T). Since, Rw(T) = ∑T t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) − ϕ(¯xt−1,w∗)⟩, the regret of {wt}T t=1 is deﬁned under loss functions ht : W →R given by ht = αt⟨∇f(¯xT),ϕ(¯xt−1,w))⟩. By assumption of convexity in ϕ, each ht is convex in w. Hence, the regret under {αtht}T t=1 can be upper bounded by the regret under the linear losses {αt⟨∇ht(wt),·⟩}T t=1. These linear losses correspond to the losses used in the meta-update in Eq. 3. Since the meta-update is an instance of FTRL, we may upper-bound Rw(T) by Eq. 6 with each 15˜gt = 0. Putting this together along with smoothness of ϕ, Rx(T) ≤Rw(T) = T∑ t=1 αt⟨∇f(¯xT),ϕ(¯xt−1,wt) −ϕ(¯xt−1,w∗)⟩ ≤ T∑ t=1 αt⟨∇ht(wt),wt −w∗⟩ ≤∥w∗∥2 β + β 2 T∑ t=1 α2 t∥∇ht(wt)∥2 ∗ = ∥w∗∥2 β + β 2 T∑ t=1 α2 t∥Dϕ(¯xt−1,wt)T∇f(¯xt)∥2 ∗ ≤∥w∗∥2 β + λβ 2 T∑ t=1 α2 t∥∇f(¯xt)∥2 ∗. (13) Putting Eq. 12 and Eq. 13 together gives the stated bound. Next, if x∗ is the global optimiser, ∇f(x∗) = 0 by ﬁrst-order condition. Setting β = 1/(Lλ) and αt = 1 means the ﬁrst two norm terms in the summation cancel. The ﬁnal norm term in the summation is negative and can be ignored. We are left with f(¯xT) −f(x∗) ≤λL∥w∗∥2 T ≤λLdiam(W) T . ■ Corollary 1. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T∇f(¯xt). Assume that ϕsatisﬁes Dϕ(x′,w)T∇f(x) −Dϕ(x′′,w′)T∇f(x′) 2 ∗≤˜λ∥∇f(x′) −∇f(x)∥2 ∗ for all x′′,x′,x ∈X and w,w′ ∈W , for some ˜λ >0. If each αt = t and βt = t−1 2t˜λL, then f(¯xT) −f(x∗) ≤ 4˜λLdiam(W) T2−1 . Proof. Plugging in the choice of ˜gt and using that Dϕ(¯xt−1,wt)T∇f(¯xt) −Dϕ(xt−2,wt−1)T∇f(¯xt−1) 2 ∗≤˜λ∥∇f(¯xt−1) −∇f(¯xt)∥2 ∗, the bound in Theorem 4 becomes f(¯xT) −f(x∗) ≤ 1 α1:T ( ∥w∗∥2 βT + 1 2 T∑ t=1 ( ˜λα2 tβt −α1:t−1 L ) ∥∇f(¯xt) −∇f(¯xt−1)∥2 ∗ ) , where we drop the negative terms ∥∇f(¯xt) −∇f(x∗)∥2 ∗. Setting αt = tyields α1:t−1 = (t−1)t 2 , while setting βt = t−1 2t˜λL means ˜λα2 tβt = (t−1)t 2L . Hence, ˜λα2 tβt −α1:t−1/Lcancels and we get f(¯xT) −f(x∗) ≤ ∥w∗∥2 βTα1:T = 4∥w∗∥2˜λL (T −1)(T + 1) ≤ 4˜λLdiam(W) (T −1)(T + 1) = 4˜λLdiam(W) T2 −1 . ■ Corollary 2. Let each ˜gt+1 = Dϕ(¯xt−1,wt)T˜yt+1, for some ˜yt+1 ∈Rn. If each ˜yt+1 is a better predictor of the next gradient than ∇f(¯xt−1), in the sense that ∥Dϕ(¯xt−2,wt−1)T˜yt −Dϕ(¯xt−1,wt)T∇f(¯xt)∥∗≤˜λ∥∇f(¯xt) −∇f(¯xt−1)∥∗, then Algorithm 4 guarantees convergence at a rate O(˜λ/T2). Proof. The proof follows the same argument as Corollary 1. ■ 16Algorithm 5: BMG in practice (general version). input :Weights {ρt}T t=1,{βt}T t=1 input :Update rule ϕ input :Matching function Bµ input :Target oracle input :Initialisation (x0,w1) for t= 1,2,...,T : xt = xt−1 + ϕ(xt−1,wt) Query zt from target oracle dt : w↦→Bµ zt (xt−1 + ϕ(xt−1,w)) wt+1 = wt −βt∇dt(wt) return xT E BMG Errata: this was incorrectly referred to as Appendix F in our original submission. In this section, we provide a more comprehensive reduction of BMG to AO-FTRL. First, we provide a more general deﬁnition of BMG. Let µ : X →R be a convex distance generating function and deﬁne the Bregman Divergence Bµ : Rn×Rn →R by Bµ z(x) = µ(x) −µ(z) −⟨∇µ(z),x −z⟩. Given initial condition (x0,w1), the BMG updates proceed according to xt = xt−1 + ϕ(xt−1,wt) wt+1 = wt −βt∇dt(wt), (14) where dt : Rn →R is deﬁned by dt(w) = Bµ zt (xt−1 + ϕ(xt−1,wt)), where each zt ∈Rn is referred to as a target. See Algorithm 5 for an algorithmic summary. A bootstrapped target uses the meta-learner’s most recent update, xt, to compute the target, zt = xt + yt for some tangent vector yt ∈Rn. This tangent vector represents a form of optimism, and provides a signal to the meta-learner as to what would have been a more efﬁcient update. In particular, the author’s consider using the meta-learned update rule to construct yt; yt = ϕ(xt,wt) −∇f(xtϕ(xt,w −t)). Note that xt = xt−1 + ϕ(xt−1,wt), and hence this tangent vector is obtained by applying the update rule again, but now to xt. For this tangent to represent an improvement, it must be assumed that wt is a good parameterisation. Hence, bootstrapping represents a form of optimism. To see how BMG relates to Algorithm 4, and in particular, Eq. 10, expand Eq. 14 to get wt+1 = wt −βtDϕ(xt−1,wt)T (∇µ(xt) −∇µ(zt)) . (15) In contrast, AO-FTRL reduces to a slightly different type of update. Lemma 2. Consider Algorithm 4. Given online losses ht : W → R deﬁned by {⟨Dϕ(¯xt−1,wt)T∇f(¯xt),·⟩}T t=1 and hint functions {⟨˜gt,·,}⟩T t=1, with each ˜gt ∈Rm. If ∥·∥ = (1/2)∥·∥2, an interior solution to Eq. 10 is given by wt+1 = βt βt−1 wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) . 17Proof. By direct computation: wt+1 = arg min w∈W ( αt+1⟨˜gt+1,w⟩+ t∑ s=1 αs⟨Dϕ(¯xs−1,ws)T∇f(¯xs),w⟩+ 1 2βt ∥w∥2 2 ) = −βt ( αt+1˜gt+1 + t∑ s=1 αtDϕ(¯xs−1,ws)T∇f(¯xs)) ) = −βt ( αt+1˜gt+1 + αtDϕ(¯xt−1,wt)T∇f(¯xt) + (t−1∑ s=1 αtDϕ(¯xs−1,ws)T∇f(¯xs)) )) = −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) −βt ( αt˜gt + t−1∑ s=1 αtDϕ(¯xs−1,ws)T∇f(¯xs)) ) = βt βt−1 wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) . ■ AO-FTRL includes a decay rate βt/βt−1; this decay rate can be removed by instead using optimistic online mirror descent [Rakhlin and Sridharan, 2013, Joulani et al., 2017]—to simplify the exposition we consider only FTRL-based algorithms in this paper. An immediate implication of Lemma 2 is the error-corrected version of BMG. Corollary 3. Setting ˜gt+1 = Dϕ(¯xt−1,wt)T˜gt+1 for some ˜yt+1 ∈Rn yields an error-corrected version of the BMG meta-update in Eq. 14. Speciﬁcally, the meta-updates in Lemma 2 becomes wt+1 = βt βt−1 wt −βtDϕ(¯xt−1,wt)T(αt+1 ˜yt+1 + αt∇f(¯xt))   BML update + βtαtDϕ(¯xt−2,wt−1)T˜yt   FTRL error correction . Proof. Follows immediately by substituting for each ˜gt+1 in Lemma 2. ■ To illustrate this connection, Let µ = f. In this case, the BMG update reads wt+1 = wt − βtDϕ(xt−1,wt)T(∇f(zt) −∇f(xt)). The equivalent update in the convex optimisation setting (i.e. Algorithm 4) is obtained by setting ˜yt+1 = ∇f(zt), in which case Corollary 3 yields wt+1 = βt+1 βt wt −βtDϕ(¯xt−1,wt)T(αt+1∇f(zt) −αt∇f(¯xt)) + ξt, where ξt = βtαtDϕ(¯xt−2,wt−1)T∇f(¯xt−1) denotes the error correction term we pick up through AO-FTRL. Since Algorithm 5 does not average its iterates—while Algorithm 4 does—we see that these updates (ignoring ξt) are identical up to scalar coefﬁcients (that can be controlled for by scaling each βt and each ˜gt+1 accordingly). More generally, the mapping from targets in BMG and hints in AO-FTRL takes on a more complicated pattern. Our next results show that we can always map one into the other. To show this, we need to assume a certain recursion. It is important to notice however that at each iteration introduces an unconstrained variable and hence the assumption on the recursion is without loss of generality (as the free variable can override it). Theorem 5. Targets in Algorithm 5 and hints in algorithm 4 commute in the following sense. BMG →AO-FTRL. Let BMG targets {zt}T t=1 by given. A sequence of hints {˜g}T t=1 can be constructed recursively by αt+1˜gt+1 = Dϕ(¯xt−1,wt)T(∇µ(¯xt) −∇µ(zt) −αt∇f(¯xt)) + αt˜gt, t ∈[T], (16) so that interior updates for Algorithm 4 are given by wt+1 = βt βt−1 wt −βt(∇µ(zt) −∇µ(¯xt)) . 18AO-FTRL →BMG. Conversely, assume a sequence {˜yt}T t=1 are given, each ˜yt ∈Rn. If µstrictly convex, a sequence of BMG targets {zt}T t=1 can be constructed recursively by zt = ∇µ−1 (∇µ(xt) −(αt+1 ˜yt+1 + αt∇f(xt))) t∈[T], so that BMG updates in Eq. 14 are given by wt+1 = wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) , where each ˜gt+1 is the BMG-induced hint function, given by αt+1˜gt+1 = αt+1Dϕ(xt−1,wt)T˜yt+1 + αt˜gt. Proof. First, consider BMG →AO-FTRL. First note that ˜g1 is never used and can thus be chosen arbitrarily—here, we set ˜g1 = 0. For w2, Lemma 2 therefore gives the interior update w2 = β2 β1 w1 −β1(α2˜g2 + α1Dϕ(¯x0,w1)T∇f(¯x1)). Since the formulate for ˜g2 in Eq. 16 only depends on quantities with iteration index t= 0,1, we may set α2˜gt = Dϕ(¯x0,w1)T(∇µ(¯x1) −∇µ(zt) −αt∇f(¯x1)). This gives the update w2 = β2 β1 w1 −β1Dϕ(¯x0,w1)T(∇µ(¯x1) −∇µ(z1)). Now assume the recursion holds up to time t. As before, we may choose αt+1˜gt+1 according to the formula in Eq. 16 since all quantities on the right-hand side depend on quantities computed at iteration tor t−1. Subtituting this into Lemma 2, we have wt+1 = βt βt−1 wt −βt ( αt+1˜gt+1 + αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) = βt βt−1 wt −βt ( Dϕ(¯xt−1,wt)T(∇µ(¯xt) −∇µ(zt) −αt∇f(¯xt)) + αt˜gt +αt(Dϕ(¯xt−1,wt)T∇f(¯xt) −˜gt) ) = βt βt−1 wt −βtDϕ(¯xt−1,wt)T(∇µ(¯xt) −∇µ(zt)). AO-FTRL →BMG. The proof in the other direction follows similarly. First, note that for µstrictly convex, ∇µ is invertible. Then, z1 = ∇µ−1(∇µ(x1) −(α2 ˜y2 + α1∇f(x1))). This target is permissible since x1 is already computed and {˜yt}T t=1 is given. Substituting this into the BMG meta-update in Eq. 14, we ﬁnd w2 = w1 −β1Dϕ(x0,w1)T(∇µ(x1) −∇µ(∇µ−1(∇µ(x1) −(α2 ˜y2 + α1∇f(x1))))) = w1 −β1Dϕ(x0,w1)T(α2 ˜y2 + α1∇f(x1)) = w1 −β1 ( α2˜g2 + α1(Dϕ(¯x0,w1)T∇f(¯x1) −˜g1) ) , where the last line uses that ˜g2 is deﬁned by α2˜g2 −α1˜g1 = Dϕ(¯x0,w1)T˜y2 and ˜g1 is arbitrary. Again, assume the recursion holds to time t. We then have wt+1 = wt −βtDϕ(xt−1,wt)T (∇µ(xt) −∇µ(zt)) = wt −βtDϕ(xt−1,wt)T(∇µ(xt) −∇µ(∇µ−1(∇µ(xt) −(αt+1 ˜yt+1 + αt∇f(xt))))) = wt −βtDϕ(xt−1,wt)T(αt+1 ˜yt+1 + αt∇f(xt)) = wt −βt(αt+1˜gt+1 + αt(Dϕ(xt−1,wt)T∇f(xt) −˜gt)). ■ 19",
      "meta_data": {
        "arxiv_id": "2301.03236v1",
        "authors": [
          "Sebastian Flennerhag",
          "Tom Zahavy",
          "Brendan O'Donoghue",
          "Hado van Hasselt",
          "András György",
          "Satinder Singh"
        ],
        "published_date": "2023-01-09T10:05:12Z",
        "pdf_url": "https://arxiv.org/pdf/2301.03236v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that gradient descent with momentum (Heavy Ball) and Nesterov Acceleration are special cases of meta-gradients, showing that meta-learning can be understood as a non-linear transformation of classical optimization. It establishes convergence rates for meta-learning in the single-task convex setting, proving O(λ/T) convergence. Crucially, it shows that optimism is required for meta-learning to achieve accelerated convergence rates of O(λ/T^2). Finally, the paper reveals that optimism in meta-learning can be captured through Bootstrapped Meta-Gradients (BMG), providing the first proof of convergence for BMG and insight into its underlying mechanics.",
        "methodology": "The research employs a theoretical framework that connects meta-learning to convex optimization. It introduces a stylized variant of meta-learning (Algorithm 2) for rigorous analysis, which utilizes moving averages and an online learning algorithm for meta-updates, specifically Follow-The-Regularized-Leader (FTRL) and its adaptive-optimistic variant (AO-FTRL). The update rule (ϕ) is assumed to be affine in the meta-parameters (w) and smooth with bounded norm. Optimism is formally integrated into the meta-learning process through the use of hint functions in the AO-FTRL meta-updates (Optimistic Meta-Learning, OML). The analysis relies on online-to-batch conversion techniques and regret bounds from online convex optimization, proving how the update rule affects convergence rates and how optimism enables acceleration. The paper further establishes an isomorphism between targets in BMG and hint functions in AO-FTRL.",
        "experimental_setup": "The paper conducts experiments in two settings: (1) Convex Quadratics, where ill-conditioned convex quadratic loss functions (f(x) = xTQx with Q matrices having eigenvalues λi=i^2) are used. Momentum and AdaGrad are compared against their meta-learned variants (Meta-Momentum and Meta-AdaGrad, which meta-learn element-wise step sizes). Performance is evaluated based on loss per iteration and cumulative loss sensitivity to learning rates. (2) ImageNet, where a 50-layer ResNet is trained. Standard SGD is compared against meta-learning (tuning element-wise learning rates online) and optimistic meta-learning (using previous gradients as hints). Top-1 test accuracy over training steps is reported, with shading indicating 95% confidence intervals over 3 seeds. Hyper-parameter sweeps are conducted for learning rates, winit scale, and decay/meta-learning rates in both setups.",
        "limitations": "The theoretical results are primarily derived for the single-task convex optimization setting, which simplifies the complexities of general meta-learning. The analysis relies on a \"stylized variant\" of meta-learning and specific assumptions on the update rule ϕ (e.g., affine in w, smooth with bounded norm) that may not always hold in practical, complex scenarios. Computing the Jacobian Dϕ, which is central to the theoretical derivations, is often intractable in practical meta-learning applications. Standard meta-learning without optimism only achieves a slower O(1/T) convergence rate, underscoring the necessity of optimism for acceleration. Additionally, the paper notes that Bootstrapped Meta-Gradients (BMG) can suffer from divergence if the gradient used in its target computation (yt) is not carefully scaled.",
        "future_research_directions": "The paper identifies several future research avenues, including exploring novel forms of meta-learning that effectively incorporate optimism, as it is highlighted as an \"exciting opportunity.\" The theoretical analysis provides insights into stabilizing Bootstrapped Meta-Gradients (BMG), suggesting further work on implementing and evaluating simple correction methods in practice. Moreover, the paper notes that the presented regret bounds and analysis can be extended to stochastic optimization settings, indicating this as a promising direction for broader applicability of the theoretical framework."
      }
    },
    {
      "title": "Predicting Training Time Without Training ",
      "abstract": "We tackle the problem of predicting the number of optimization steps that a\npre-trained deep network needs to converge to a given value of the loss\nfunction. To do so, we leverage the fact that the training dynamics of a deep\nnetwork during fine-tuning are well approximated by those of a linearized\nmodel. This allows us to approximate the training loss and accuracy at any\npoint during training by solving a low-dimensional Stochastic Differential\nEquation (SDE) in function space. Using this result, we are able to predict the\ntime it takes for Stochastic Gradient Descent (SGD) to fine-tune a model to a\ngiven loss without having to perform any training. In our experiments, we are\nable to predict training time of a ResNet within a 20% error margin on a\nvariety of datasets and hyper-parameters, at a 30 to 45-fold reduction in cost\ncompared to actual training. We also discuss how to further reduce the\ncomputational and memory cost of our method, and in particular we show that by\nexploiting the spectral properties of the gradients' matrix it is possible\npredict training time on a large dataset while processing only a subset of the\nsamples.",
      "full_text": "Predicting Training Time Without Training Luca Zancato1,2 Alessandro Achille2 Avinash Ravichandran2 Rahul Bhotika2 Stefano Soatto2 University of Padova1 Amazon Web Services2 luca.zancato@phd.unipd.it {aachille,ravinash,bhotikar,soattos}@amazon.com Abstract We tackle the problem of predicting the number of optimization steps that a pre- trained deep network needs to converge to a given value of the loss function. To do so, we leverage the fact that the training dynamics of a deep network during ﬁne-tuning are well approximated by those of a linearized model. This allows us to approximate the training loss and accuracy at any point during training by solving a low-dimensional Stochastic Differential Equation (SDE) in function space. Using this result, we are able to predict the time it takes for Stochastic Gradient Descent (SGD) to ﬁne-tune a model to a given loss without having to perform any training. In our experiments, we are able to predict training time of a ResNet within a 20% error margin on a variety of datasets and hyper-parameters, at a 30 to 45-fold reduction in cost compared to actual training. We also discuss how to further reduce the computational and memory cost of our method, and in particular we show that by exploiting the spectral properties of the gradients’ matrix it is possible predict training time on a large dataset while processing only a subset of the samples. 1 Introduction Say you are a researcher with many more ideas than available time and compute resources to test them. You are pondering to launch thousands of experiments but, as the deadline approaches, you wonder whether they will ﬁnish in time, and before your computational budget is exhausted. Could you predict the time it takes for a network to converge, before even starting to train it? We look to efﬁciently estimate the number of training steps a Deep Neural Network (DNN) needs to converge to a given value of the loss function, without actually having to train the network. This problem has received little attention thus far, possibly due to the fact that the initial training dynamics of a randomly initialized DNN are highly non-trivial to characterize and analyze. However, in most practical applications, it is common to not start from scratch, but from a pre-trained model. This may simplify the analysis, since the ﬁnal solution obtained by ﬁne-tuning is typically not too far from the initial solution obtained after pre-training. In fact, it is known that the dynamics of overparametrized DNNs [9, 31, 2] during ﬁne-tuning tends to be more predictable and close to convex [24]. We therefore characterize the training dynamics of a pre-trained network and provide a computation- ally efﬁcient procedure to estimate the expected proﬁle of the loss curve over time. In particular, we provide qualitative interpretation and quantitative prediction of the convergence speed of a DNN as a function of the network pre-training, the target task, and the optimization hyper-parameters. We use a linearized version of the DNN model around pre-trained weights to study its actual dynamics. In [20] a similar technique is used to describe the learning trajectories of randomly initialized wide neural networks. Such an approach is inspired by the Neural Tangent Kernel (NTK) for inﬁnitely wide networks [14]. While we note that NTK theory may not correctly predict the dynamics of real (ﬁnite size) randomly initialized networks [12], we show that our linearized approach can be extended to ﬁne-tuning of real networks in a similar vein to [24]. In order to predict ﬁne-tuning Training Time (TT) without training we introduce a Stochastic Differential Equation (SDE) (similar to [ 13]) to Preprint. Under review. arXiv:2008.12478v1  [cs.LG]  28 Aug 2020approximate the behavior of SGD: we do so for a linearized DNN and in function space rather than in weight space. That is, rather than trying to predict the evolution of the weights of the network (a D-dimensional vector), we aim to predict the evolution of the outputs of the network on the training set (a N ×C-dimensional vector, where N is the size of the dataset and Cthe number of network’s outputs). This drastically reduces the dimensionality of the problem for over-parametrized networks (that is, when NC ≪D). A possible limiting factor of our approach is that the memory requirement to predict the dynamics scales as O(DC2N2). This would rapidly become infeasible for datasets of moderate size and for real architectures ( D is in the order of millions). To mitigate this, we show that we can use random projections to restrict to a much smaller D0-dimensional subspace with only minimal loss in prediction accuracy. We also show how to estimate Training Time using a small subset of N0 samples, which reduces the total complexity to O(D0 C2N2 0 ). We do this by exploiting the spectral properties of the Gram matrix of the gradients. Under mild assumptions the same tools can be used to estimate Training Time on a larger dataset without actually seeing the data. To summarize, our main contributions are: (i) We present both a qualitative and quantitative analysis of the ﬁne-tuning Training Time as a function of the Gram-Matrix Θ of the gradients at initialization (empirical NTK matrix). (ii) We show how to reduce the cost of estimating the matrix Θ using random projections of the gradients, which makes the method efﬁcient for common architectures and large datasets. (iii) We introduce a method to estimate how much longer a network will need to train if we increase the size of the dataset without actually having to see the data (under the hypothesis that new data is sampled from the same distribution). (iv) We test the accuracy of our predictions on off-the-shelf state-of-the-art models trained on real datasets. We are able to predict the correct training time within a 20% error with 95% conﬁdence over several different datasets and hyperparameters at only a small fraction of the time it would require to actually run the training (30-45x faster in our experiments). 2 Related Work Predicting the training time of a state-of-the-art architecture on large scale datasets is a relatively understudied topic. In this direction, Justus et al. [ 15] try to estimate the wall-clock time required for a forward and backward pass on given hardware. We focus instead on a complementary aspect: estimating the number of ﬁne-tuning steps necessary for the loss to converge below a given threshold. Once this has been estimated we can combine it with the average time for the forward and backward pass to get a ﬁnal estimate of the wall clock time to ﬁne-tune a DNN model without training it. Hence, we are interested in predicting the learning dynamics of a pre-trained DNN trained with either Gradient Descent (GD) or Stochastic Gradient Descent (SGD). While different results are known to describe training dynamics under a variety of assumptions (e.g. [16, 28, 26, 6]), in the following we are mainly interested on recent developments which describe the optimization dynamics of a DNN using a linearization approach. Several works [14, 19, 10] suggest that in the over-parametrized regime wide DNNs behave similar to linear models, and in particular they are fully characterized by the Gram-Matrix of the gradients, also known as empirical Neural Tangent Kernel (NTK). Under these assumptions, [ 14, 3] derive a simple connection between training time and spectral decomposition of the NTK matrix. However, their results are limited to Gradient Descend dynamics and to simple architectures which are not directly applicable to real scenarios. In particular, their arguments hinge on the assumption of using a randomly initialized very wide two-layer or inﬁnitely wide neural network [3, 11, 22]. We take this direction a step further, providing a uniﬁed framework which allows us to describe training time for both SGD and GD on common architectures. Again, we rely on a linear approximation of the model, but while the practical validity of such linear approximation for randomly initialized state-of-the-art architectures (such as ResNets) is still discussed [12], we follow Mu et al. [24] and argue that the ﬁne-tuning dynamics of over-parametrized DNNs can be closely described by a linearization. We expect such an approximation to hold true since the network does not move much in parameters space during ﬁne-tuning and over-parametrization leads to smooth and regular loss function around the pre-trained weights [9, 31, 2, 21]. Under this 20 20 40 60 80 100 120 140 160 Real Training Time 0 25 50 75 100 125 150 175 Predicted Training Time Perfect prediction +13% error -13% error CIFAR10    slope 0.93 CIFAR100   slope 0.94 CUB200     slope 0.97 Aircrafts  slope 1.09 Mit67      slope 0.89 Surfaces   slope 0.90 Cars       slope 1.00 (a) Training with Gradient Descent. 0 20 40 60 80 100 120 140 Real Training Time 0 25 50 75 100 125 150 175 Predicted Training Time Perfect prediction +20% error -20% error CIFAR10    slope 0.87 CIFAR100   slope 0.78 CUB200     slope 0.91 Aircrafts  slope 0.78 Mit67      slope 0.87 Surfaces   slope 0.78 Cars       slope 0.90 (b) Training with SGD. Figure 1: Training time prediction (# iterations) for several ﬁne-tuning tasks. Scatter plots of the predicted time vs the actual training time when ﬁne-tuning a ResNet-18 pre-trained on ImageNet on several tasks. Each task is obtained by randomly sampling a subset of ﬁve classes with 150 images (when possible) each from one popular dataset with different hyperparameters (batch size, learning rate). The closer the scatter plots to the bisector the better the TT estimate. Our prediction is (a) within 13% of the real training time 95% of the times when using GD and (b) within 20% of the real training time when using SGD. premise, we tackle both GD and SGD in an uniﬁed framework and build on [13] to model training of a linear model using a Stochastic Differential Equation in function space. We show that, as also hypothesized by [24], linearization can provide an accurate approximation of ﬁne-tuning dynamics and therefore can be used for training time prediction. 3 Predicting training time In this section we look at how to efﬁciently approximate the training time of a DNN without actual training. By Training Time (TT) we mean the number of optimization steps – of either Gradient Descent (GD) or Stochastic Gradient Descent (SGD) – needed to bring the loss on the training set below a certain threshold. We start by introducing our main tool. Let fw(x) denote the output of the network, where wdenotes the weights of the network and x ∈Rd denotes its input (e.g., an image). Let w0 be the weight conﬁguration after pre-training. We assume that when ﬁne-tuning a pre-trained network the solution remains close to pre-trained weights w0 [24, 9, 31, 2]. Under this assumption – which we discuss further in Section 6 – we can faithfully approximate the network with its Taylor expansion around w0 [20]. Let wt be the ﬁne-tuned weights at time t. Using big-O notation and ft ≡fwt, we have: ft(x) = f0(x) + ∇wf0(x)|w=w0 (wt −w0) + O(∥wt −w0∥2) We now want to use this approximation to characterize the training dynamics of the network during ﬁne-tuning to estimate TT. In such theoretical analyses [14, 20, 3] it is common to assume that the network is trained with Gradient Descent (GD) rather than Stochastic Gradient Descent, and in the limit of a small learning rate. In this limit, the dynamics are approximated by the gradient ﬂow differential equation ˙wt = −η∇wtL[14, 20] where ηdenotes the learning rate and L(w) denotes the loss function L(w) = ∑ N i=1 ℓ(yi,fw(xi))., where ℓis the per-sample loss function (e.g. Cross- Entropy). This approach however has two main drawbacks. First, it does not properly approximate Stochastic Gradient Descent, as it ignores the effect of the gradient noise on the dynamics, which affects both training time and generalization. Second, the differential equation involves the weights of the model, which live in a very high dimensional space thus making ﬁnding numerical solutions to the equation not tractable. To address both problems, building on top of [13] in the Supplementary we prove the following result. Proposition 1 In the limit of small learning rate η, the output on the training set of a linearized network flin t trained with SGD evolves according to the following Stochastic Differential Equation (SDE): dflin t (X) = −ηΘ∇flin t (X)Ltdt    deterministic part + η√ |B| ∇wflin 0 (X)Σ 1 2 (flin t (X))dn    stochastic part , (1) 30 20 40 60 80 100 Iterations 0 20 40 60 80 Accuracy ODE vs SDE approximation SGD SDE ODE 0 500 1000 1500 2000 2500 Iterations 20 40 60 80 100 Train Err ELR effects ELR:0.001 ELR:0.005 ELR:0.010 ELR:0.050 ELR:0.100 Figure 2: (Left) ODE vs. SDE. ODE approximation may not be well suited to describe the actual non-linear SGD dynamics (high learning rates regime). (Right) Fine-tuning with the same ELR have similar curves . We ﬁne-tune an ImageNet pre-trained network on MIT-67 with different combinations of learning rates and momentum coefﬁcients. We note that as long as the effective learning rate is the same, the loss curves are also similar. where Xis the set of training images,|B|the batch-size and dnis a D-dimensional Brownian motion. We have deﬁned the Gram gradients matrix Θ [14, 27] (i.e., the empirical Neural Tangent Kernel matrix) and the covariance matrix Σ of the gradients as follows: Θ := ∇wf0(X)∇wf0(X)T, (2) Σ(flin t (X)) := E [ (gi∇flin t (xi)L) ⊗(gi∇flin t (xi)L) ] −E [ gi∇flin t (xi)L ] ⊗E [ gi∇flin t (xi)L ] . (3) where gi ≡∇wf0(xi). Note both Θ and Σ only require gradients w.r.t. parameters computed at initialization. The ﬁrst term of eq. (1) is an ordinary differential equation (ODE) describing the deterministic part of the optimization, while the second stochastic term accounts for the noise. In Figure 2 (left) we show the qualitative different behaviour of the solution to the deterministic part of eq. (1) and the complete SDE eq. (1). While several related results are known in the literature for the dynamics of the network in weight space [7], note that eq. (1) completely characterizes the training dynamics of the linearized model by looking at the evolution of the output flin t (X) of the model on the training samples – a N×C-dimensional vector – rather than looking at the evolution of the weightswt – a D-dimensional vector. When the number of data points is much smaller than the number of weights (which are in the order of millions for ResNets), this can result in a drastic dimensionality reduction, which allows easy estimation of the solution to eq. (1). Solving eq. (1) still comes with some challenges, particularly in computing Θ efﬁciently on large datasets and architectures. We tackle these in Section 4. Before that, we take a look at how different hyper-parameters and different pre-trainings affect the training time of a DNN on a given task. 3.1 Effect of hyper-parameters on training time Effective learning rate. From Proposition 1 we can gauge how hyper-parameters will affect the optimization process of the linearized model and, by proxy, of the original model it approximates. One thing that should be noted is that Proposition 1 assumes the network is trained with momentum m= 0. Using a non-zero momentum leads to a second order differential equation in weight space, that is not captured by Proposition 1. We can however, introduce heuristics to handle the effect of momentum: Smith et al. [28] note that the momentum acts on the stochastic part shrinking it by a factor √ 1/(1 −m). Meanwhile, under the assumptions we used in Proposition 1 (small learning rate), we can show (see Supplementary Material) the main effect of momentum on the deterministic part is to re-scale the learning rates by a factor1/(1 −m). Given these results, we deﬁne the effective learning rate (ELR) ˆη= η/(1 −m) and claim that, in ﬁrst approximation, we can simulate the effect of momentum by using ˆηinstead of ηin eq. (1). In particular, models with different learning rates and momentum coefﬁcients will have similar (up to noise) dynamics (and hence training time) as long as the effective learning rate ˆηremains the same. In Figure 2 we show empirically that indeed same effective learning rate implies similar loss curve. That similar effective learning rate gives similar test performance has also been observed in [21, 28]. Batch size. The batch size appears only in the stochastic part of the equation, its main effect is to decrease the scale of the SDE noise term. In particular, when the batch size goes to inﬁnity |B|→∞ 4(a) Features and Gradients clustering.  (b) Trajectory clustering. Figure 3: Are gradients good descriptors to cluster data by semantics and training time? (a) Features vs Gradients clustering. (Right) t-SNE plot of the ﬁrst ﬁve principal components of the gradients of each sample in a subset of CIFAR-10 with 3 classes. Colors correspond to the sample class. We observe that the ﬁrst 5 principal components are enough to separate the data by class. By Proposition 2 this implies faster training time. (Left) In the same setting as before, t-SNE plot of the features using the ﬁrst 5 components of PCA. We observe that gradients separate the classes better than the features. (b) t-SNE on predicted trajectories To see if gradients are good descriptors of both semantics and training time we use gradients to predict linearized trajectories: we cluster the trajectories using t-SNE and we color each point by (left) class and (right) training time. We observe that: clusters split trajectories according both to labels (left) and training time (right). Interestingly inside each class there are clusters of points that may converge at different speed. we recover the deterministic gradient ﬂow also studied by [20]. Note that we need the batch size |B| to go to inﬁnity, rather than being as large as the dataset since we assumed random batch sampling with replacement. If we assume extraction without replacement the stochasticity is annihilated as soon as |B|= N (see [7] for a more in depth discussion). 3.2 Effect of pre-training on training time We now use the SDE in eq. (1) to analyze how the combination of different pre-trainings of the model – that is, different w0’s – and different tasks affect the training time. In particular, we show that a necessary condition for fast convergence is that the gradients after pre-training cluster well with respect to the labels. We conduct this analysis for a binary classiﬁcation task with yi = ±1, but the extension is straightforward for multi-class classiﬁcation, under the simplifying assumptions that we are operating in the limit of large batch size (GD) so that only the deterministic part of eq. (1) remains. Under these assumptions, eq. (1) can be solved analitically and the loss of the linearized model at time tcan be written in closed form as (see Supplementary Material): Lt = (Y− f0(X))Te−2ηΘt(Y− f0(X)) (4) The following characterization can easily be obtained using an eigen-decomposition of the matrix Θ. Proposition 2 Let S = ∇wfw(X)T∇wfw(X) be the second moment matrix of the gradients and let S = UΣUT be the uncentered PCA of the gradients, where Σ = diag(λ1,...,λ n,0,..., 0) is a D×Ddiagonal matrix, n ≤min(N,D) is the rank of S and λi are the eigenvalues sorted in descending order. Then we have Lt = D∑ k=1 e−2ηλkt(δy ·vk)2, (5) where λkvk = (gi ·uk)N i=1 is the N-dimensional vector containing the value of the k-th principal component of gradients gi and δy := Y− f0(X). Training speed and gradient clustering.We can give the following intuitive interpretation: consider the gradient vector gi as a representation of the sample xi. If the ﬁrst principal components of gi are sufﬁcient to separate the classes (i.e., cluster them), then convergence is faster (see Figure 3). Conversely, if we need to use the higher components (associated to small λk) to separate the data, then convergence will be exponentially slower. Arora et al. [3] also use the eigen-decomposition of Θ 5100 101 Iterations Train Loss Trajectory approximations Full kernel Reduced kernel Actual dynamics 0 50 100 150 200 250 300 350 400 N 0.000 0.002 0.004 0.006 0.008 0.010 ||Θ − ˆΘ ||F ||Θ ||F 0 10 20 30 40 50 60 70 80 Computational Time (s) Approximated vs True kernel True Approximated 100 101 102 103 Eigenvalues index 10 1 100 101 102 103 Eigenvalues Power law for different dataset sizes N 30 N 60 N 150 N 300 N 600 N 1500 N 2100 Figure 4: (Left) Actual ﬁne-tuning of a DNN with GD compared to the numerical solution of eq. (1) and the solution using an approximated Θ. The approximated Θ can faithfully describe ﬁne-tuning dynamics while being twice as fast to compute and 100 times smaller to be stored. (Center) Relative difference in Frobenius norm of the real and approximated Θ as the dataset size varies (red), and their computational time (blue). Right: Eigen-spectrum of Θ computed on subsets of MIT-67 of increasing size. Note the convergence to a common power law (i.e., a line in log-log scale). to explain the slower convergence observed for a randomly initialized two-layer network trained with random labels. This is straightforward since the projection of a random vector will be uniform on all eigenvectors, rather than concentrated on the ﬁrst few, leading to slower convergence. However, we note that the exponential dynamics predicted by [3] do not hold for more general networks trained from scratch [30] (see Section 6). In particular, eq. (5) mandates that the loss curve is always convex (it is sum of convex functions), which may not be the case for deep networks trained from scratch. 4 Efﬁcient numerical estimation of training time In Proposition 2 we have shown a closed form solution to the SDE in eq. (1) in the limit of large batch size, and for the MSE loss. Unfortunately, in general eq. (1) does not have a closed form expression when using the cross-entropy loss [20]. A numerical solution is however possible, enabled by the fact that we describe the network training in function space, which is much smaller than weight space for over-parametrized models. The main computational cost is to create the matrix Θ in eq. (1) – which has cost O(DC2N2) – and to compute the noise in the stochastic term. Here we show how to reduce the cost of Θ to O(D0C2N2) for D0 ≪Dusing a random projection approximation. Then, we propose a fast approximation for the stochastic part. Finally, we describe how to reduce the cost in N by using only a subset N′<N of samples to predict training time. Random projection. To keep the notation uncluttered, here we assume w.l.o.g. C = 1. In this case the matrix Θ contains N2 pairwise dot-products of the gradients (a D-dimensional vector) for each of the N training samples (see eq. 2). Since Dcan be very large (in the order of millions) storing and multiplying all gradients can be expensive as N grows. Hence, we look at a dimensionality reduction technique. The optimal dimensionality reduction that preserves the dot-product is obtained by projecting on the ﬁrst principal components of SVD, which however are themselves expensive to obtain. A simpler technique is to project the gradients on a set ofD′standard Gaussian random vectors: it is known that such random projections preserve (in expectation) pairwise product [5, 1] between vectors, and hence allow us to reconstruct the Gram matrix while storing only D′-dimensional vector, with D′ ≪D. We further increase computational efﬁciency using multinomial random vectors {-1,0,+1} as proposed in [1] which further reduce the computational cost by avoiding ﬂoating point multiplications. In Figure 4 we show that the entries of Θ and its spectrum are well approximated using this method, while the computational time becomes much smaller. Computing the noise. The noise covariance matrix Σ is a D×D-matrix that changes over time. Both computing it at each step and storing it is prohibitive. Estimating Σ correctly is important to describe the dynamics of SGD [8], however we claim that a simple approximation may sufﬁce to describe the simpler dynamic in function space. We approximate ∇wflin 0 (X)Σ1/2 approximating Σ with its diagonal (so that the we only need to store a D-dimensional vector). Rather than computing the whole Σ at each step, we estimate the value of the diagonal at the beginning of the training. Then, by exploiting eq. (3), we see that the only change to Σ is due to ∇flin t L, whose norm decreases over time. Therefore we use the easy-to-compute ∇flin t Lto re-scale our initial estimate of Σ. Larger datasets. In the MSE case from eq. (4), knowing the eigenvalues λk and the corresponding residual projections pk = (δy ·vk)2 we can predict in closed form the whole training curve. Is it possible to predict λk and pk using only a subset of the dataset? It is known [27] that the eigenvalues 6of the Gram matrix of Gaussian data follow a power-law distribution of the form λk = ck−s. Moreover, by standard concentration argument, one can prove that the eigenvalues should converge to a given limit as the number of datapoints increases. We verify that a similar power-law and convergence result also holds for real data (see Figure 4). Exploiting this result, we can estimate c and sfrom the spectrum computed on a subset of the data, and then predict the remaining eigenvalues. A similar argument holds for the projections pk, which also follow a power-law (albeit with slower convergence). We describe the complete estimation in the Supplementary Material. 5 Results We now empirically validate the accuracy of proposition 1 in approximating the loss curve of an actual deep neural network ﬁne-tuned on a large scale dataset. We also validate the goodness of the numerical approximations described in Section 4. Due to the lack of a standard and well established benchmark to test Training Time estimation algorithms we developed one with the main goal to closely resemble ﬁne-tuning common practice for a wide spectrum of different tasks. Experimental setup. We deﬁne training time as the ﬁrst time the (smoothed) loss is below a given threshold. However, since different datasets converge at different speeds, the same threshold can be too high (it is hit immediately) for some datasets, and too low for others (it may take hundreds of epochs to be reached). To solve this, and have cleaner readings, we deﬁne a ‘normalized’ threshold as follows: we ﬁx the total number of ﬁne-tuning steps T, and measure instead the ﬁrst time the loss is within ϵfrom the ﬁnal value at time T. This measure takes into account the ‘asymptotic’ loss reached by the DNN within the computational budget (which may not be close to zero if the budget is low), and naturally adapts the threshold to the difﬁculty of the dataset. We compute both the real loss curve and the predicted training curve using Proposition 1 and compare the ϵ-training-time measured on both. We report the absolute prediction error, that is |tpredicted −treal|. For all the experiments we extract 5 random classes from each dataset (Table 1) and sample 150 images (or the maximum available for the speciﬁc dataset). Then we ﬁne-tuned ResNet18/34 using either GD or SGD. Accuracy of the prediction. In Figure 1 we show TT estimates errors (for different ϵ∈{1,..., 40}) under a plethora of different conditions ranging from different learning rates, batch sizes, datasets and optimization methods. For all the experiments we choose a multi-class classiﬁcation problem with Cross Entropy (CE) Loss unless speciﬁed otherwise, and ﬁxed computational budget of T = 150 steps both for GD and SGD. We note that our estimates are consistently within respectively a 13% and 20% relative error around the actual training time 95% of the times. In Table 1 we describe the sensitivity of our estimates to different thresholds ϵ both when our assumptions do and do not hold (high and low learning rates regimes). Note that a larger thresholdϵis hit during the initial convergence phase of the network, when a small number of iterations corresponds a large change in the loss. Correspondingly, the hitting time can be measured more accurately and our errors are lower. A smaller ϵdepends more on correct prediction of the slower asymptotic phase, for which exact hitting time is more difﬁcult to estimate. TT error (# of steps) ϵ = 1% ϵ = 10% ϵ= 40% Lr low high low high low high Cars [17] 9 18 7 8 1 0 Surfaces [4] 6 13 6 7 6 3 Mit67 [25] 8 10 6 8 3 1 Aircrafts [23] 5 21 5 4 9 7 CUB200 [29] 6 6 5 8 1 1 CIFAR100 [18] 10 15 6 7 2 3 CIFAR10 [18] 9 14 8 9 3 3 Table 1: Training Time estimation error for CE loss using GD for T = 150 epochs at different thresholds ϵ. We compare TT estimates when ODE assumptions do and do not hold: high {0.005} and small LR {0.001, 0.0001}. (S) Figure 5: Wall clock time (in seconds) to com- pute TT estimate vs ﬁne-tuning running time. We run the methods described in Section 4 both on GPU and CPU. Training is done on GPU. Wall-clock run-time. In Figure 5 we show the wall-clock runtime of our training time prediction method compared to the time to actually train the network for T steps. Our method is 30-40 times faster. Moreover, we note that it can be run completely on CPU without a drastic drop in performance. This allows to cheaply estimate TT and allocate/manage resources even without access to a GPU. 70.0001 0.001 0.005 LR 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 TT error Surfaces CIFAR100 Aircrafts Mit67 CUB200 CIFAR10 Cars 50 100 Batch Size 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 TT error Surfaces CIFAR100 Aircrafts Mit67 CUB200 CIFAR10 50 250 625 Dataset Size 0 10 20 30 40 50 TT error Surfaces CIFAR100 Aircrafts Mit67 CUB200 CIFAR10 Cars Figure 6: Average and 95% conﬁdence intervals of TT estimate error for: Left: GD using different learning rates. Center: SGD using different batch sizes. Right: SGD using different dataset sizes. The average is taken w.r.t. random classes with different number of samples: {10, 50, 125} Effect of dataset distance. We note that the average error for Surfaces (Figure 6) is uniformily higher than the other datasets. This may be due to the texture classiﬁcation task being quite different from ImageNet, on which the network is pretrained. In this case we can expect that the linearization assumption is partially violated since the features must adjust more during ﬁne-tuning. Effect of hyper-parameters on prediction accuracy. We derived Proposition 1 under several assumptions, importantly: small learning rate and wt close to w0. In Figure 6 (left) we show that indeed increasing the learning rate decreases the accuracy of our prediction, albeit the accuracy remains good even at larger learning rates. Fine-tuning on larger dataset makes the weights move farther away from the initialization w0. In Figure 6 (right) we show that this slightly increases the prediction error. Finally, we observe in Figure 6 (center) that using a smaller batch size, which makes the stochastic part of Proposition 1 larger also slightly increases the error. This can be ascribed to the approximation of the noise term (Section 4). On the other hand, in Figure 2 (right) we see that the effect of momentum on a ﬁne-tuned network is very well captured by the effective learning rate (Section 3.1), as long as the learning rate is reasonably small, which is the case for ﬁne-tuning. Hence the SDE approximation is robust to different values of the momentum. In general, we note that even when our assumptions are not fully met training time can still be approximated with only a slightly higher error. This suggest that point-wise proximity of the training trajectory of linear and real models is not necessary as long as their behavior (decay-rate) is similar (see also Supplementary Material). 6 Discussion and conclusions We have shown that we can predict with a 13-20% accuracy the time that it will take for a pre-trained network to reach a given loss, in only a small fraction of the time that it would require to actually train the model. We do this by studying the training dynamics of a linearized version of the model – using the SDE in eq. (1) – which, being in the smaller function space compared to parameters space, can be solved numerically. We have also studied the dependency of training time from pre-training and hyper-parameters (Section 3.1), and how to make the computation feasible for larger datasets and architectures (Section 4). While we do not necessarily expect a linear approximation around a random initialization to hold during training of a real (non wide) network, we exploit the fact that when using a pre-trained network the weights are more likely to remain close to initialization [ 24], improving the quality of the approximation. However, in the Supplementary Material we show that even when using a pre-trained network, the trajectories of the weights of linearized model and of the real model can differ substantially. On the other hand, we also show that the linearized model correctly predicts the outputs (not the weights) of the real model throughout the training, which is enough to compute the loss. We hypothesise that this is the reason why eq. (1) can accurately predict the training time using a linear approximation. The procedure described so far can be considered as an open loop procedure meaning that, since we are estimating training time before any ﬁne-tuning step is performed, we are not gaining any feedback from the actual training. How to perform training time prediction during the actual training, and use training feedback (e.g., gradients updates) to improve the prediction in real time, is an interesting future direction of research. 8References [1] Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary coins. J. Comput. Syst. Sci., 66(4):671–687, June 2003. [2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. arXiv preprint arXiv:1811.03962, 2018. [3] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op- timization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pages 322–332, 2019. [4] Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala. Material recognition in the wild with the materials in context database. Computer Vision and Pattern Recognition (CVPR), 2015. [5] Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245–250, 2001. [6] Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns over-parameterized networks that provably generalize on linearly separable data. CoRR, abs/1710.10174, 2017. [7] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. CoRR, abs/1710.11029, 2017. [8] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational infer- ence, converges to limit cycles for deep networks. In International Conference on Learning Representations, 2018. [9] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 1675–1685, Long Beach, California, USA, 09–15 Jun 2019. PMLR. [10] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds global minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018. [11] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018. [12] Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom Goldstein. Truth or backpropaganda? an empirical investigation of deep learning theory, 2019. [13] Souﬁane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-ﬁeld behaviour of neural tangent kernel for deep neural networks. arXiv preprint arXiv:1905.13654, 2019. [14] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pages 8571–8580, 2018. [15] Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. Predicting the computational cost of deep learning models. CoRR, abs/1811.11880, 2018. [16] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. CoRR, abs/1609.04836, 2016. [17] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for ﬁne-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. [18] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Computer Science Department, University of Toronto, 2009. [19] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017. 9[20] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent, 2019. [21] Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Rethinking the hyperparameters for ﬁne-tuning, 2020. [22] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. CoRR, abs/1705.09886, 2017. [23] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft. Technical report, 2013. [24] Fangzhou Mu, Yingyu Liang, and Yin Li. Gradients as features for deep representation learning. In International Conference on Learning Representations, 2020. [25] Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In CVPR, pages 413–420. IEEE Computer Society, 2009. [26] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. [27] J. Shawe-Taylor, C. K. I. Williams, N. Cristianini, and J. Kandola. On the eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory, 51(7):2510–2522, 2005. [28] Samuel L. Smith and Quoc V . Le. A bayesian perspective on generalization and stochastic gradient descent. ArXiv, abs/1710.06451, 2018. [29] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010. [30] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [31] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes over-parameterized deep relu networks. CoRR, abs/1811.08888, 2018. 10Predicting Training Time Without Training: Supplementary Material In the Supplementary Material we give the pseudo-code for the training time prediction algorithm (Appendix A) together with implementation details, show additional results including prediction of training time using only a subset of samples, and comparison of real and predicted loss curves in a variety of conditions (Appendix C). Finally, we give proofs of all statements. A Algorithm Algorithm 1: Estimate the Training Time on a given target dataset and hyper-parameters. 1: Data: Number of steps T to simulate, threshold ϵto determine convergence, pre-trained weights w0 of the model, a target dataset with images X= {xi}N i=1 and labels Y= {yi}N i=1, batch size B, learning rate η, momentum m∈[0,1). 2: Result: An estimate ˆTϵ of the number of steps necessary to converge within ϵto the ﬁnal value Tϵ := min{t: |Lt −LT|<ϵ}. 3: Initialization: Compute initial network predictions f0(X), estimate Θ using random projections (Section 4), compute the ELR ˜η= η/(1 −m) to use in eq. (1) instead of η; 4: if B = N then 5: Get flin t (X) solving the ODE in eq. (1) (only the deterministic part) for T steps; 6: else 7: Get flin t (X) solving the SDE in eq. (1) for T steps (see approximation in Section 4); 8: end if 9: Using flin t (X) and Ycompute linearized loss Llin t ∀t∈{1,...,T } 10: return ˆTϵ := min{t: |Llin t −Llin T |<ϵ}; We can compute the estimate on training time based also on the accuracy of the model: we straight- forwardly modify the above algorithm and use the predictions flin t (X) to compute the error instead of the loss (e.g. ﬁg. 10). We now brieﬂy describe some implementations details regarding the numerical solution of ODE and SDE. Both of them can be solved by means of standard algorithms: in the ODE case we used LSODA (which is the default integrator in scipy.integrate.odeint), in the SDE case we used Euler-Maruyama algorithm for Ito equations. We observe removing batch normalization (preventing the statistics to be updated) and removing data augmentation improve linearization approximation both in the case of GD and SGD. Interestingly data augmentation only marginally alters the spectrum of the Gram matrix Θ and has little impact on the linearization approximation w.r.t. batch normalization. [ 12] observed similar effects but, differently from us, their analysis has been carried out using randomly initialized ResNets. B Target datasets Dataset Number of images Classes Mean samples per class Imbalance factor cifar10 [18] 50000 10 5000 1 cifar100 [18] 50000 100 500 1 cub200 [29] 5994 200 29.97 1.03 fgvc-aircrafts [23] 6667 100 66.67 1.02 mit67 [25] 5360 67 80 1.08 opensurfacesminc2500 [4] 48875 23 2125 1.03 stanfordcars [17] 8144 196 41.6 2.83 Table 2: Target datasets. 11C Additional Experiments Prediction of training time using a subset of samples. In Section 4 we suggest that in the case of MSE loss, it is possible to predict the training time on a large dataset using a smaller subset of samples (we discuss the details in Appendix D). In Figure 7 we show the result of predicting the loss curve on a dataset of N = 4000 samples using a subset of N = 1000 samples. Similarly, in Figure 11 (top row) we show the more difﬁcult example of predicting the loss curve on N = 1000 samples using a very small subset of N0 = 100 samples. In both cases we correctly predict that training on a larger dataset is slower, in particular we correctly predict the asymptotic convergence phase. Note in the case N0 = 100 the prediction is less accurate, this is in part due to the eigenspectrum of Θ being still far from its limiting behaviour achieved for large number of data (see Appendix D). Comparison of predicted and real error curve. In Figure 8 we compare the error curve predicted by our method and the actual train error of the model as a function of the number of optimization steps. The model is trained on a subset of 2 classes of CIFAR-10 with 150 samples. We run the comparison for both gradient descent (left) and SGD (right), using learning rate η= 0.001, momentum m= 0 and (in the case of SGD) batch size 100. In both cases we observe that the predicted curve is reasonably close to the actual curve, more so at the beginning of the training (which is expected, since the linear approximation is more likely to hold). We also perform an ablation study to see the effect of different approximation of SGD noise in the SDE in eq. (1). In Figure 8 (center) we estimate the variance of the noise of SGD at the beginning of the training, and then assume it is constant to solve the SDE. Notice that this predicts the wrong asymptotic behavior, in particular the predicted error does not converge to zero as SGD does. In Figure 8 (right) we rescale the noise as we suggest in Section 4: once the noise is rescaled the SDE is able to predict the right asymptotic behavior of SGD. Prediction accuracy in weight space and function space. In Section 3 and Section 6 we argue that using a differential equation to predict the dynamics in function space rather than weight space is not only faster (in the over-parametrized case), but also more accurate. In Figure 9 we show empirically that solving the corresponding ODE in weight space leads to a substantially larger prediction error. Effective learning rate. In Section 3.1 we note that as long as the effective learning rate ˜η = η/(1 −m) remains constant, runs with different learning rate ηand momentum mwill have similar learning curve. We show a formal derivation in Appendix E. In Figure 12 we show additional experiments, similar to Figure 2, on several other datasets to further conﬁrm this point. Point-wise similarity of predicted and observed loss curve. In some cases, we observe that the predicted and observed loss curves can differ. This is especially the case when using cross-entropy loss (Figure 10). We hypothesize that this may be due to improper prediction of the dynamics when the softmax output saturates, as the dynamic becomes less linear [20]. However, the train error curve (which only depends on the relative order of the outputs) remains relatively correct. We should also notice that prediction of the ϵ-training-time ˆTϵ can be accurate even if the curves are not point-wise close. The ϵ-training-time seeks to ﬁnd the ﬁrst time after which the loss or the error is within an ϵ threshold. Hence, as long as the real and predicted loss curves have a similar asymptotic slope the prediction will be correct, as we indeed verify in Figure 10 (bottom). 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.1 0.2 0.3 0.4 0.5 Train Loss size 1000 size 4000 predicted 0 5 10 15 20 25 30 35 40 Threshold 0 25 50 75 100 125 150 175 TT Real TT Estimated TT Figure 7: Training-time prediction using a subset of the data. (Left) Using the method described in Appendix D, we predict (green) the loss curve on a large dataset of N = 4000 samples (orange) using a subset of N0 = 1000 samples (blue). In Figure 11 we show a similar result using a much smaller subset of N0 = 100 samples. (Right) Corresponding estimated training time on the larger dataset at different thresholds ϵcompared to the real training time on the larger dataset. 12Figure 8: (Left) Comparison of the real error curve on CIFAR10 using gradient descent and the predicted curve. (Center) Same as before, but this time we train using SGD and compare it with the prediction using the technique described in Section 4 to approximate the covariance of the SGD noise that appears in the SDE in eq. (1). (Right) Same as (center), but using constant noise instead of rescaling the noise using the value of the loss function as described in Section 4. Note that in this case we do not capture the right asymptotic behavior of SGD. D Prediction of training time on larger datasets In Section 4 we suggest that, in the case of MSE loss, it is possible to predict the training time on a large dataset using a subset of the samples. To do so we leverage the fact that the eigenvalues of Θ follows a power-law which is independent on the size of the dataset for large enough sizes (see Figure 4, right). More precisely, from Proposition 2, we know that given the eigenvalues λk of Θ and the projections pk = δy ·vk it is possible to predict the loss curve using Lt = ∑ k pke−2ηλkt. Let Θ0 be the Gram-matrix of the gradients computed on the small subset of N0 samples, and let Θ be the Gram-matrix of the whole dataset of size N. Using the fact that, as we increase the number of samples, the eigenvalues (once normalized by the dataset size) converge to a ﬁxed limit (Figure 4, right), we estimate the eigenvalues λk of Θ as follow: we ﬁt the coefﬁcients sand cof a power law λk = ck−s to the eigenvalues of Θ0, and use the same coefﬁcients to predict the eigenvalues of Θ. However, we notice that the coefﬁcient s(slope of the power law) estimated using a small subset of the data is often smaller than the slope observed on larger datase (note in Figure 4 (right) that the curves for smaller datasets are more ﬂat). We found that using the following corrected power law increases the precision of the prediction: ˆλk = ck−s+α ( N0 N −1 ) . Empirically, we determined α∈[0.1,0.2] to give a good ﬁt over different combinations ofN and N0. In Figure 11 (center) we compare the predicted eigenspectrum of Θ with the actual eigenspectrum of Θ . The projections pk follow a similar power-law – albeit more noisy (see Figure 11, right) – so directly ﬁtting the data may give an incorrect result. However, notice that in this case we can exploit an 0 25 50 75 100 125 150 175 Iterations 0.0 0.2 0.4 0.6 0.8 1.0 Relative Error Weights relative approx error Output relative approx error Figure 9: Comparison of prediction accuracy in weight space vs. function space. We compare the result of using the deterministic part of eq. (1) to predict the weights wt at time tand the outputs ft(X) of the networks under GD. The relative error in predicting the outputs is much smaller than the relative error of predicting the weights at all times. This, together with the computational advantage, motivates the decision of using eq. (1) to predict the behavior in function space. 130 20 40 60 80 100 120 140 Iterations 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Train Loss Loss 0 20 40 60 80 100 120 140 Iterations 0 10 20 30 40 50 60 70 80 Train Err Err True Linearized 0 5 10 15 20 25 30 35 Threshold 40 60 80 100 120 140 TT 0 5 10 15 20 25 30 35 Threshold 20 40 60 80 100 120 140 TT Mit67 0 20 40 60 80 100 120 140 Iterations 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Train Loss Loss 0 20 40 60 80 100 120 140 Iterations 10 20 30 40 50 60 70 80 Train Err Err True Linearized 0 5 10 15 20 25 30 35 Threshold 40 60 80 100 120 140 TT 0 5 10 15 20 25 30 35 Threshold 20 40 60 80 100 120 140 TT CIFAR10 Figure 10: Training time prediction is accurate even if loss curve prediction is not. (Top row) Loss curve and error curve prediction on MIT-67 (left) and CIFAR-10 (right).(Bottom row)Predicted time to reach a given threshold (orange) vs real training time (blue). We note that on some datasets our loss curve prediction differs from the real curve near convergence. However, since our training time deﬁnition measures the time to reach the asymptotic value (which is what is most useful in practice) rather than the time reach an absolute threshold, this does not affect the accuracy of the prediction (see Appendix C). 0 25 50 75 100 125 150 175 200 Iterations 0.0 0.1 0.2 0.3 0.4 0.5 Train Loss size 100 size 1000 predicted 100 101 102 103 Ordered eigenvalues 10 1 100 101 102 103 Spectrum Eig small dataset Eig large dataset Eig predicted 0 25 50 75 100 125 150 175 200 Ordered eigenvalues 10 6 10 5 10 4 10 3 10 2 10 1 100 101 102 y f0( ) Projections Large dataset projection Small dataset projection Prediction Figure 11: Training time prediction using a subset of the data. (Left) We predict the loss curve on a large dataset of N = 1000 samples using a subset of N0 = 100 samples on CIFAR10 (similar results hold for other datasets presented so far). (Center) Eigenspectrum of Θ computed using N0 = 100 samples (orange), N = 1000 samples (green) and predicted spectrum using our method (blue). (Right) Value of the projectionspk of δy on the eigenvectors of Θ, computed at N0 = 100 (orange) and N = 1000 (blue). Note that while they approximatively follow a power-law on average, it is much more ‘noisy’ than that of the eigenvalues. In green we show the predicted trend using our method. additional constraint, namely that ∑ kpk = ∥δy∥2 (∥δy∥2 is a known quantity: labels and initial model predictions on the large dataset). Let pk = δy ·vk and let p′ k = δy ·v′ k where vk and v′ k are the eigenvectors of Θ and Θ0 respectively. Fix a small k0 (in our experiments, k0 = 100). By convergence laws [27], we have that p′ k ≃pk when k < k0. The remaining tail of pk for k > k0 must now follow a power-law and also be such that ∑ kpk = ∥δy∥2. This uniquely identify the coefﬁcients of a power law. Hence, we use the following prediction rule for pk: ˆpk = {p′ k if k<k 0 ak−b if k≥k0 where aand bare such that ˆpk0 = p′ k0 and ∑ k ˆpk = ∥δy∥2. In Figure 11 (left), we use the approximated ˆλk and ˆpk to predict the loss curve on a dataset of N = 1000 samples using a smaller subset of N0 = 100 samples. Notice that we correctly predict that the convergence is slower on the larger dataset. Moreover, while training on the smaller dataset quickly reaches zero, we correctly estimate the much slower asymptotic phase on the larger dataset. Increasing both N and N0 increases the accuracy of the estimate, since the eigenspectrum of Θ is closer to convergence: In Figure 7 we show the same experiment as Figure 11 with N0 = 1000 and N = 4000. Note the increase in accuracy on the predicted curve. 14Figure 12: Additional experiments on the effective learning rate. We show additional plots showing the error curves obtained on different datasets using different values of the effective learning rate ˜η= η/(1 −m), where ηis the learning rate and mis the momentum. Each line is the observed error curve of a model trained with a different learning rate ηand momentum m. Lines with the same color have the same ELR ˜η, but each has a different ηand m. As we note in Section 3.1, as long as ˜η remains the same, training dynamics with different hyper-parameters will have similar error curves. E Effective learning rate We now show that having a momentum term has the effect of increasing the effective learning rate in the deterministic part of eq. (1). A similar treatment of the momentum term is also in [28, Appendix D]. Consider the update rule of SGD with momentum: at+1 = mat + gt+1, wt+1 = wt −ηat+1, If ηis small, the weights wt will change slowly and we can consider gt to be approximately constant on short time periods, that is gt+1 = g. Under these assumptions, the gradient accumulator at satisﬁes the following recursive equation: at+1 = mat + g, which is solved by (assuming a0 = 0 as common in most implementations): at = (1 −mt) g 1 −m. In particular, at converges exponentially fast to the asymptotic value a∗= g/(1 −m). Replacing this asymptotic value in the weight update equation above gives: wt+1 = wt −ηa∗= wt − η 1 −mg= wt −˜ηg, that is, once at reaches its asymptotic value, the weights are updated with an higher effective learning rate ˜η= η 1−m. Note that this approximation remains true as long as the gradient gt does not change much in the time that it takes at to reach its asymptotic value. This happens whenever the momentum mis small (since at will converge faster), or when η is small (gt will change more slowly). For larger momentum and learning rate, the effective learning rate may not properly capture the effect of momentum. F Proof of theorems F.1 Proposition 1: SDE in function space for linearized networks trained with SGD We now prove our Proposition 1 and show how we can approximate the SGD evolution in function space rather than in parameters space. We follow the standard method used in [13] to derive a general SDE for a DNN, then we speciaize it to the case of linearized deep networks. Our notation follows [20], we deﬁne fθt(X) = vec([ft(x)]x∈X) ∈RCN the stacked vector of model output logits for all examples, where Cis the number of classes and N the number of samples in the training set. 15To describe SGD dynamics in function space we start from deriving the SDE in parameter space. In order to derive the SDE required to model SGD we will start describing the discrete update of SGD as done in [13]. θt+1 = θt −η∇θLB(θt) (6) where LB(θt) = L(fθt(XB),YB) is the average loss on a mini-batch B(for simplicity, we assume that Bis a set of indexes sampled with replacement). The mini-batch gradient ∇θLB(θt) is an unbiased estimator of the full gradient, in particular the following holds: E[∇θLB(θt)] = 0 cov[ ∇θLB(θt)] = Σ(θt) |B| (7) Where we deﬁned the covariance of the gradients as: Σ(θt) := E [ (gi∇ft(xi)L) ⊗(gi∇ft(xi)L) ] −E [ gi∇ft(xi)L ] ⊗E [ gi∇ft(xi)L ] and gi := ∇wf0(xi). The ﬁrst term in the covariance is the second order moment matrix while the second term is the outer product of the average gradient. Following standard approximation arguments (see [7] and references there in) in the limit of small learning rate ηwe can approximate the discrete stochastic equation eq. (6) with the SDE: dθt = −η∇θL(θt)dt+ η√ |B| Σ(θt) 1 2 dn (8) where n(t) is a Brownian motion. Given this result, we are going now to describe how to derive the SDE for the outputft(X) of the network on the train set X. Using Ito’s lemma (see [13] and references there in), given a random variable θthat evolves according to an SDE, we can obtain a corresponding SDE that describes the evolution of a function of θ. Applying the lemma to fθ(X) we obtain: dft(X) = [−ηΘt∇ftL(ft(X),Y) + 1 2vec(A)]dt+ η√ |B| ∇θf(X)Σ(θt) 1 2 dn (9) where ∇θf(X) ∈RCN×D is the jacobian matrix and Dis the number of parameters. Note Ais a N ×Cmatrix which, denoting by f(j) θ (x) the j-th output of the model on a sample x, is given by: Aij = tr[Σ(θt)∇2 θf(j) θ (xi)]. Using the fact that in our case the model is linearized, so fθ(x) is a linear function of θ, we have that ∇2 θf(j)(x) = 0 and hence A= 0. This leaves us with the SDE: dft(X) = −ηΘt∇ftLdt+ η√ |B| ∇θf(X)Σ(θt) 1 2 dn (10) as we wanted. F.2 Proposition 2: Loss decomposition Let ∇wfw(X) = VΛU be the singular value decomposition of ∇wfw(X) where Λ is a rectangular matrix (of the same size of ∇wfw(X)) containing the singular values {σ1,...,σ N}on the diagonal. Both U and V are orthogonal matrices. Note that we have S = ∇wfw(X)T∇wfw(X) = UTΛTΛU, Θ = ∇wfw(X)∇wfw(X)T = VΛΛTVT. We now use the singular value decomposition to derive an expression for Lt in case of gradient descent and MSE loss (which we call Lt). In this case, the differential equation eq. (1) reduces to: ˙flin t (X) = −ηΘ(Y− flin t (X)), which is a linear ordinary differential equation that can be solved in closed form. In particular, we have: flin t (X) = (I−e−ηΘt)Y+ e−ηΘtf0(X). 16Replacing this in the expression for the MSE loss at time twe have: Lt = ∑ i (yi −flin t (xi))2 = (Y− flin t (X))T(Y− flin t (X)) = (Y− f0(X))Te−2ηΘt(Y− f0(X)). Now recall that, by the properties of the matrix exponential, we have: e−2ηΘt = e−2ηVΛΛT VT t = Ve−2ηΛΛT tVT, where e−2ΛΛT t = diag(e−2ηλ1t,e−2ηλ2t,... ) with λk := σ2 k. Then, deﬁning δy = Y− f0(X) and denoting with vk the k-th column of V we have: Lt = δyTVe−2ηΛΛT tVTδy = N∑ k=1 e−2ηλkt(δy ·vk). Now let uk denote the k-th column of UT and gi the i-th column of ∇wfw(X)T (that is, the gradient of the i-th sample). To conclude the proof we only need to show that λkvk = (gi ·uk)N i=1. But this follows directly from the SVD decompostion ∇wfw(X) = VΛU, since then VΛ = ∇wfw(X)UT. 17",
      "meta_data": {
        "arxiv_id": "2008.12478v1",
        "authors": [
          "Luca Zancato",
          "Alessandro Achille",
          "Avinash Ravichandran",
          "Rahul Bhotika",
          "Stefano Soatto"
        ],
        "published_date": "2020-08-28T04:29:54Z",
        "pdf_url": "https://arxiv.org/pdf/2008.12478v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper tackles the problem of predicting the number of optimization steps required for a pre-trained deep network to converge to a given loss value, without performing actual training. The main contributions include: (i) a qualitative and quantitative analysis of fine-tuning training time as a function of the Gram-Matrix (empirical Neural Tangent Kernel) of gradients at initialization; (ii) methods to reduce the computational cost of this estimation using random projections, making it efficient for common architectures and large datasets; (iii) a technique to estimate training time on larger datasets using only a subset of samples, under the assumption of data coming from the same distribution; and (iv) empirical validation showing accurate predictions (within 13-20% error margin) with a 30 to 45-fold reduction in cost compared to actual training.",
        "methodology": "The methodology relies on approximating the training dynamics of a pre-trained deep neural network during fine-tuning with those of a linearized model, obtained via a Taylor expansion around the pre-trained weights. This allows the problem to be framed as solving a low-dimensional Stochastic Differential Equation (SDE) in function space (predicting output evolution) rather than high-dimensional weight space. The SDE comprises a deterministic part and a stochastic term accounting for SGD noise, utilizing the Gram gradients matrix (empirical NTK) and the covariance matrix of gradients computed at initialization. The paper also discusses how to handle momentum via an 'effective learning rate' heuristic. To enhance efficiency, the method incorporates random projections of gradients to reduce the cost of computing the Gram matrix and approximates the noise covariance matrix with its diagonal, scaled based on the loss function's norm decrease. For larger datasets, it leverages the power-law distribution of Gram matrix eigenvalues to predict training time using a smaller subset of samples.",
        "experimental_setup": "Experiments were conducted by defining training time as the point where the smoothed loss falls within a normalized threshold (\b\t from the final loss value at a fixed total time T). ResNet18/34 models, pre-trained on ImageNet, were fine-tuned using either Gradient Descent (GD) or Stochastic Gradient Descent (SGD). The tasks involved multi-class classification with Cross-Entropy Loss on various popular datasets, including CIFAR10, CIFAR100, CUB200, FGVC-Aircrafts, MIT-67, OpenSurfaces (Minc2500), and StanfordCars. For each dataset, 5 random classes were sampled, with 150 images per class (or the maximum available). The computational budget for training was fixed at T=150 steps. Validation involved comparing predicted training times against actual training times and reporting the absolute prediction error. Wall-clock runtime comparisons were also performed to assess computational savings.",
        "limitations": "The core limitation stems from the linearization assumption, which is more robust for pre-trained networks where weights remain close to their initialization, but its validity can decrease under certain conditions. Specifically, prediction accuracy is shown to decrease with higher learning rates and larger dataset sizes, as these factors cause weights to diverge further from the initial pre-trained state. Smaller batch sizes also lead to a slight increase in prediction error, attributed to approximations made in modeling the stochastic noise term. While the linearized model accurately predicts network outputs and thus the loss, the trajectories of the actual network weights and the linearized model's weights can differ substantially. Additionally, when using cross-entropy loss, predicted and observed loss curves may differ near convergence due to potential softmax output saturation, though the \b\t-training-time prediction remains accurate if asymptotic slopes are similar.",
        "future_research_directions": "A primary future research direction involves developing a 'closed-loop' training time prediction procedure. This entails performing training time prediction not just before fine-tuning begins, but continuously during the actual training process. By leveraging real-time feedback, such as gradient updates from the ongoing training, the prediction model could adapt and improve its accuracy dynamically."
      }
    },
    {
      "title": "Identifying Policy Gradient Subspaces",
      "abstract": "Policy gradient methods hold great potential for solving complex continuous\ncontrol tasks. Still, their training efficiency can be improved by exploiting\nstructure within the optimization problem. Recent work indicates that\nsupervised learning can be accelerated by leveraging the fact that gradients\nlie in a low-dimensional and slowly-changing subspace. In this paper, we\nconduct a thorough evaluation of this phenomenon for two popular deep policy\ngradient methods on various simulated benchmark tasks. Our results demonstrate\nthe existence of such gradient subspaces despite the continuously changing data\ndistribution inherent to reinforcement learning. These findings reveal\npromising directions for future work on more efficient reinforcement learning,\ne.g., through improving parameter-space exploration or enabling second-order\noptimization.",
      "full_text": "Published as a conference paper at ICLR 2024 IDENTIFYING POLICY GRADIENT SUBSPACES Jan Schneider1∗ Pierre Schumacher1,2 Simon Guist1 Le Chen1 Daniel H¨aufle2,3 Bernhard Sch¨olkopf1 Dieter B¨uchler1 1Max Planck Institute for Intelligent Systems, T¨ubingen, Germany 2Hertie Institute for Clinical Brain Research, T¨ubingen, Germany 3Institute for Computer Engineering, University of Heidelberg, Germany ABSTRACT Policy gradient methods hold great potential for solving complex continuous con- trol tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various sim- ulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to rein- forcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization. 1 I NTRODUCTION Deep reinforcement learning (RL) has marked significant achievements in numerous challeng- ing problems, ranging from Atari games (Mnih et al., 2013) to various real robotic challenges, such as contact-rich manipulation (Gu et al., 2017; Kalashnikov et al., 2018), complex plan- ning problems (Everett et al., 2018; Ao et al., 2022), and hard-to-control dynamic tasks (Cheng et al., 2023; Kaufmann et al., 2023). Despite these notable successes, deep RL methods are often brittle due to the use of function approximators with large numbers of parameters and persistently changing data distributions – a setting notoriously hard for optimization. Deep RL, in its vanilla form, operates under limited prior knowledge and structural information about the problem, conse- quently requiring large numbers of interactions with the environment to reach good performance. For supervised learning (SL), Gur-Ari et al. (2018) demonstrated that the gradients utilized for neu- ral network optimization reside in a low-dimensional, slowly-changing subspace. Based on this insight, recent works introduce more structured optimization procedures for SL by identifying and harnessing these gradient subspaces. Exploiting this structure enables the optimization to be carried out in a reduced-dimensional subspace, yielding enhanced efficiency with minimal, if any, loss in performance (Li et al., 2018; Gressmann et al., 2020; Larsen et al., 2021; Li et al., 2022a). Despite the benefits of subspace methods in SL, their adoption in deep RL has remained limited. A straightforward way to transfer these principles is to find lower-dimensional subspaces in policy gradient approaches (Peters & Schaal, 2008). Policy gradient (PG) methods estimate the gradient of the RL objective ∇θJ(θ) to update the policy’s parameters θ using some form of stochastic gradient descent (SGD). Since most SL approaches using subspaces operate at the level of the SGD optimization, PG algorithms would be a natural choice to leverage the knowledge about subspaces from SL in the RL context. Nevertheless, in RL, such methods have been explored primarily within the realm of evolutionary strategies (Maheswaranathan et al., 2019), representation learning (Le Lan et al., 2023), and transfer learning (Gaya et al., 2022). A possible explanation is the constantly changing data distribution of RL due to continual exploration that intuitively seems to hinder the identification of gradient subspaces. The limited body of studies using subspaces in PG algorithms underlines the need for a more profound discussion in this domain. ∗Correspondence to jan.schneider@tuebingen.mpg.de 1 arXiv:2401.06604v3  [cs.LG]  18 Mar 2024Published as a conference paper at ICLR 2024 This paper conducts a comprehensive empirical evaluation of gradient subspaces in the context of PG algorithms, assessing their properties across various simulated RL benchmarks. Our experi- ments reveal several key findings: (i) there exist parameter-space directions that exhibit significantly larger curvature compared to other parameter-space directions, (ii) the gradients live in the subspace spanned by these directions, and (iii) the subspace remains relatively stable throughout the RL train- ing. Additionally, we analyze the gradients of the critic – an integral part of the PG estimation in actor-critic methods – and observe that the critic subspace often exhibits less variability and retains a larger portion of its gradient compared to the actor subspace. We also test the robustness of PG subspaces regarding mini-batch approximations of the gradient that are used in practice during train- ing and evaluate a similar mini-batch approximation of the Hessian. Lastly, we explore the extent to which the variation in the data distribution influences the aforementioned subspace analysis by conducting experiments with both an on-policy as well as an off-policy algorithm, the latter of which reuses previously collected data for training. By shedding light on gradient subspaces in deep RL, this paper provides insights that can potentially enhance RL performance by advancing parameter-space exploration or enabling second-order opti- mization. We begin by reviewing existing literature on subspace approaches in Section 2, followed by a recapitulation of the RL preliminaries in Section 3 as a foundation for the analysis of gradient subspaces in RL in Section 4. Section 5 concludes this work with a discussion of the results and implications of our work. The code for our experiments is available on the project website. 2 R ELATED WORK Numerous works have studied the use of gradient subspaces in SL. These works can be roughly divided into informed and random subspace approaches. In the following, we give an overview of these papers and highlight works that investigate related concepts in RL. Informed subspaces in supervised learning Informed subspace methods identify the subspace from some part of the training process. For instance, Gur-Ari et al. (2018) show that the gradients used for neural network optimization lie in a low-dimensional subspace of the parameter-space di- rections with the highest curvature in the loss. Furthermore, they show that this subspace changes slowly throughout the training. Recent works have highlighted numerous benefits of these insights. Li et al. (2022a) apply principal component analysis on the network parameters at previous check- points and use the top principal components as subspace. They apply SGD and BFGS algorithms in the subspace and demonstrate superior learning performance compared to training in the original parameter space. Similarly, Tuddenham et al. (2020) propose to use a BFGS method in a low- dimensional subspace defined by the full-batch gradients with respect to individual classification labels. Sohl-Dickstein et al. (2014) construct a second-order Taylor approximation of individual loss terms efficiently in a low-dimensional subspace that captures recently observed gradients. With this approximation, the overall loss is a sum of quadratic functions that can be optimized in closed form. Chen et al. (2022) utilize insights about the low-dimensional subspaces to learn an optimizer that scales to high-dimensional parameter spaces. Traditionally, learned optimizers are limited in scale by the high output dimensionality of the network. By outputting parameter updates in the low-dimensional subspace, Chen et al. circumvent this challenge and efficiently learn to optimize large neural networks. Gauch et al. (2022) apply the knowledge of low-dimensional gradient sub- space to few-shot adaptation by identifying a suitable subspace on the training task and utilizing it for efficient finetuning on the test task. Likewise, Zhou et al. (2020) tackle differential privacy by identifying a gradient subspace on a public dataset and projecting gradients calculated from the pri- vate dataset into this subspace. Adding noise in the subspace instead of the parameter space lowers the error bound of differential privacy to be linear in the number of subspace dimensions instead of the parameter dimensions. Li et al. (2022b) tackle an overfitting problem of adversarial training by identifying a gradient subspace in an early training phase before overfitting occurs. By projecting later gradients into this subspace, they enforce that the training dynamics remain similar to the early training phase, which mitigates the overfitting problem. Random subspaces in supervised learning Gressmann et al. (2020) describe different modifica- tions to improve neural network training in random subspaces, like resampling the subspace reg- ularly and splitting the network into multiple components with individual subspaces. They utilize these subspaces to reduce the communication overhead in a distributed learning setting by send- 2Published as a conference paper at ICLR 2024 ing the parameter updates as low-dimensional subspace coefficients. Li et al. (2018) quantify the difficulty of a learning task by its intrinsic dimensionality, i.e., the dimensionality of the random subspace needed to reach good performance. Aghajanyan et al. (2021) apply a similar analysis to language models and find that the effectiveness of fine-tuning language models is tied to a low in- trinsic dimensionality. Based on these findings, Hu et al. (2021) propose a method for fine-tuning large models efficiently via low-rank parameter updates. Larsen et al. (2021) compare informed and random subspaces and find that with informed subspaces, typically significantly fewer dimensions are needed for training. Furthermore, they note the benefit of first training in the original space for some time before starting subspace training, as this increases the probability of the subspace intersecting with low-loss regions in the parameter space. Subspace approaches to reinforcement learning Only a handful of works use subspace methods for RL. Tang et al. (2023) identify a parameter subspace through singular value decomposition of the policy parameters’ temporal evolution. They propose to cut off the components of the update step that lie outside this subspace and additionally elongate the update in the directions corresponding to the largest singular values. In contrast to our work, they focus solely on the policy network, consider only off-policy learning algorithms, and do not establish a connection to the curvature of the learning objective. Li et al. (2018) quantify the difficulty of RL tasks by their intrinsic dimensionality. To that end, the authors optimize the value function of Deep Q-Networks (DQN) (Mnih et al., 2013) as well as the population distribution – a distribution over the policy parameters – of evolutionary strategies (ES) (Salimans et al., 2017) in random subspaces. Maheswaranathan et al. (2019) define a search distribution for ES that is elongated along an informed subspace spanned by surrogate gradients. While the authors do not evaluate their approach for RL, they highlight the relevance to RL due to the method’s ability to use surrogate gradients generated from learned models, similar to the use of the critic in actor-critic methods. Gaya et al. (2022) enable improved generalization in online RL by learning a subspace of policies. Recently, they extended their method to scale favorably to continual learning settings (Gaya et al., 2023). Both methods explicitly learn a subspace of policy parameters, while our work investigates the natural training dynamics of PG methods. 3 P RELIMINARIES This section introduces the mathematical background and notation used throughout the paper. Fur- thermore, we briefly describe the two RL algorithms that we will analyze in Section 4. 3.1 M ATHEMATICAL BACKGROUND AND NOTATION For a given objective functionJ(θ), we useg = ∂ ∂θ J(θ) ∈ Rn to denote the gradient of a model with respect to its parameters θ and H = ∂2 ∂2θ J(θ) ∈ Rn×n to denote the corresponding Hessian matrix. We use vi to denote the ith largest eigenvector of H. Note that we use “ ith largest eigenvector” as shorthand for “eigenvector with respect to the ith largest eigenvalue”. Since H is symmetric, all eigenvectors are orthogonal to each other, and we assume ||vi|| = 1. In this work, we investigate projections of gradients into lower-dimensional subspaces, i.e., map- pings from Rn to Rk with k ≪ n. These mappings are defined by a projection matrix P ∈ Rk×n. ˆg = P g∈ Rk denotes the projection of g into the subspace and ˜g = P+ˆg ∈ Rn is the mapping of ˆg back to the original dimensionality that minimizes the projection error ||g − ˜g||. Here, P+ denotes the pseudoinverse of P. If the projection matrix is semi-orthogonal, i.e., the columns are orthogonal and norm one, the pseudoinverse simplifies to the transpose P+ = PT . The matrix of the k largest eigenvectors P = (v1, . . . , vk)T is one example of such a semi-orthogonal matrix. 3.2 R EINFORCEMENT LEARNING We consider tasks formulated as Markov decision processes (MDPs), defined by the tuple (S, A, p, r). Here, S and A are the state and action spaces, respectively. The transition dynam- ics p: S × A × S →[0, ∞) define the probability density of evolving from one state to another. At each timestep the agent receives a scalar reward r: S × A →R. A stochastic policy, πθ(at | st), defines a mapping from state st to a probability distribution over actions at. RL aims to find an optimal policy π∗, maximizing the expected cumulative return, discounted by γ ∈ [0, 1]. 3Published as a conference paper at ICLR 2024 The value function V π(s) represents the expected (discounted) cumulative reward from state s following policy π, and the action value function Qπ(s, a) denotes the expected (discounted) cu- mulative reward for taking action a in state s and then following π. The advantage function Aπ(s, a) = Qπ(s, a) − V π(s) quantifies the relative benefit of taking an action a in state s over the average action according to policy π. RL algorithms generally can be divided into two styles of learning. On-policy methods, like Prox- imal Policy Optimization (PPO) (Schulman et al., 2017), only use data generated from the current policy for updates. In contrast, off-policy algorithms, such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018) leverage data collected from different policies, such as old iterations of the policy. 3.2.1 P ROXIMAL POLICY OPTIMIZATION On-policy PG methods typically optimize the policy πθ via an objective such as J(θ) = Eπθ h πθ(at | st) ˆAt i = Eπold h rt(θ) ˆAt i with rt(θ) = πθ(at|st) πold(at|st) and ˆAt being an estimator of the advantage function at timestep t and πold denoting the policy before the update (Kakade & Lang- ford, 2002). However, optimizing this objective can result in excessively large updates, leading to instabilities and possibly divergence. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an on-policy actor-critic algorithm designed to address this issue by clipping the probability ratio rt(θ) to the interval [1−ϵ, 1 +ϵ], which removes the incentive for movingrt(θ) outside the interval, resulting in the following actor loss. JPPO actor(θ) = Eπold h min \u0010 rt(θ) ˆAt, clip (rt(θ), 1 − ϵ, 1 + ϵ) ˆAt \u0011i (1) The advantage estimation ˆAt = δt + (γλ)δt+1 +··· + (γλ)T−t+1δT−1 with δt = rt +γVϕ(st+1)− Vϕ(st) uses a learned value function Vϕ, which acts as a critic. The hyperparameter λ determines the trade-off between observed rewards and estimated values. The critic is trained to minimize the mean squared error between the predicted value and the discounted sum of future episode rewards V target t = Pt+T τ=t γτ−trτ . JPPO critic(ϕ) = E \u0002 (Vϕ(st) − V target t )2\u0003 (2) 3.2.2 S OFT ACTOR -CRITIC Soft Actor-Critic (SAC) (Haarnoja et al., 2018) is a policy gradient algorithm that integrates the maximum entropy reinforcement learning framework with the actor-critic approach. As such, it op- timizes a trade-off between the expected return and the policy’s entropy. It is an off-policy algorithm and, as such, stores transitions in a replay buffer D, which it samples from during optimization. To that end, SAC modifies the targets for the learned Q-function Qϕ to include a term that incen- tivizes policies with large entropy ˆQt = rt + γ(Qϕ(st+1, at+1) − log πθ(at+1 | st+1)), resulting in the following critic loss. JSAC critic(ϕ) = ED,πθ \u00141 2 \u0010 Qϕ(st, at) − ˆQt \u00112\u0015 (3) Note that SAC, in its original formulation, trains an additional value function and a second Q- function, but we omitted these details for brevity. The algorithm then trains the actor to minimize the KL-divergence between the policy and the exponential of the learned Q-function. JSAC actor(θ) = ED \u0014 DKL \u0012 πθ(· |st) \r\r\r\r exp(Qϕ(st, ·)) Zϕ(st) \u0013\u0015 (4) Zϕ(st) denotes the normalization to make the right side of the KL-divergence a proper distribution. Optimizing this loss increases the probability of actions with high value under the Q-function. 4 G RADIENT SUBSPACES IN POLICY GRADIENT ALGORITHMS In Section 2, we have highlighted several works from SL that utilize low-dimensional gradient sub- spaces for improving the learning performance. Naturally, we would like to transfer these benefits 4Published as a conference paper at ICLR 2024 to policy gradient algorithms. However, the training in RL is significantly less stationary than in the supervised setting (Bjorck et al., 2022). As the RL agent changes, the data distribution shifts since the data is generated by the agent’s interactions with its environment. Furthermore, the value of a state also depends on the agent’s behavior in future states. Thus, the targets for the actor and critic networks change constantly. These crucial differences between SL and RL underscore the need to analyze to which extent insights about gradient subspaces transfer between these settings. The analysis presented in this section focuses on two policy gradient algorithms: PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018), which are popular instantiations of on-policy and off-policy RL. We apply the algorithms to twelve benchmark tasks from OpenAI Gym (Brockman et al., 2016), Gym Robotics (Plappert et al., 2018a), and the DeepMind Control Suite (Tunyasuvu- nakool et al., 2020). Our code builds upon the algorithm implementations ofStable Baselines3 (Raf- fin et al., 2021). The learning curves are displayed in Appendix A. We ran each experiment for 10 random seeds and plot the mean and standard deviation for the results in Sections 4.2 and 4.3. Due to space constraints, we show the analysis results only for selected tasks and present detailed results for all twelve tasks in Appendix B. Moreover, we conduct an evaluation of the impact of the RL algorithm’s hyperparameters on the gradient subspace in Appendix C. For the following analyses, we calculate Hessian eigenvectors of the loss with respect to the net- work parameters via the Lanczos method (Lehoucq et al., 1998) since it is an efficient method for estimating the top eigenvectors that avoids explicitly constructing the Hessian matrix. Since we can only estimate the Hessian from data, we use a large number of state-action pairs to obtain precise estimates for the eigenvectors of the true Hessian, similar to how Ilyas et al. (2020) approximate the true policy gradient. For PPO, we collect 106 on-policy samples. This would, however, not be faithful to the diverse distribution of off-policy data that SAC uses for training. To match this data distribution for the analysis, we save the replay buffer during training and use the data of the complete replay buffer for estimating the Hessian. Note that the replay buffer also has a capacity of 106 samples but is not completely filled at the beginning of training. As mentioned in Section 3, SAC and PPO each train two different networks, an actor and a critic. We, therefore, conduct our analysis for each network individually. To verify that there exist high- curvature directions spanning a subspace that stays relatively stable throughout the training and that contains the gradient, we check three conditions: i) Some parameter-space directions exhibit significantly larger curvature in the actor/critic loss than other directions. ii) The actor/critic gradient mainly lies in the subspace spanned by these directions. iii) The subspaces for the actor and critic networks change slowly throughout the training. 4.1 T HE LOSS CURVATURE IS LARGE IN A FEW PARAMETER -SPACE DIRECTIONS The Hessian matrix describes the curvature of a function, with the eigenvectors being the directions of maximum and minimum curvature. The corresponding eigenvalues describe the magnitude of the curvature along these directions. Therefore, we verify condition i) by plotting the spectrum of Hessian eigenvalues for the actor and critic loss of PPO with respect to the network parameters in Figure 1. The plots show that there are a few large eigenvalues for both the actor and critic loss. All remaining eigenvalues are distributed close to zero. These plots confirm that there are a few directions with significantly larger curvature; in other words, the problem is ill-conditioned. 4.2 T HE GRADIENT LIES IN THE HIGH -CURVATURE SUBSPACE To verify condition ii) that the high-curvature subspace contains the gradients of the respective loss, we measure how well these gradients can be represented in the subspace. Let Pk ∈ Rk×n be the semi-orthogonal matrix that projects into the high-curvature subspace. Pk consists row-wise of the k largest Hessian eigenvectors. We compute the relative projection error, i.e., the relative difference between the original gradient g ∈ Rn and the projected gradient ˜g = P+ k Pkg that is the result of mapping into the high-curvature subspace and back into the original space. The fraction of the gradient that can be represented in the subspace is then given by Sfrac(Pk, g) = 1 − ||˜g − g||2 ||g||2 , (5) 5Published as a conference paper at ICLR 2024 0 2000 4000 Index 25 0 25 Eigenvalue (a) Finger-spin, actor 0 2000 4000 Index 1000 0 1000 (b) Finger-spin, critic 0 2000 4000 Index 100 0 100 (c) Walker2D, actor 0 2000 4000 Index 500 0 500 (d) Walker2D, critic Figure 1: The spectrum of the Hessian eigenvalues for PPO on the tasks Finger-spin (a, b) and Walker2D (c, d). The Hessian is estimated from 106 state-action pairs. For both the actor (a, c) and critic (b, d) loss, there is a small number of large eigenvalues, while the bulk of the eigenvalues is close to zero. This finding shows that there is a small number of high-curvature directions in the loss landscapes, which is in accordance with results from SL. which simplifies to the following gradient subspace fraction criterion of Gur-Ari et al. (2018). Sfrac(Pk, g) = ||Pk g||2 ||g||2 (6) We derive this equality in Appendix D. Note that0 ≤ Sfrac(Pk, g) ≤ 1 holds, where Sfrac(Pk, g) = 1 implies that the subspace perfectly contains the gradient, while Sfrac(Pk, g) = 0 means that the gradient lies entirely outside the subspace. Due to the normalization by||g||, this criterion is invariant to the scale of the gradient, which enables comparing gradient subspaces of different models and models at different stages of the training. To evaluate how the gradient subspace fraction evolves over time, we evaluate the criterion at check- points every 50,000 steps during the RL training. To compactly visualize this data, we split the training into three phases: initial, training, and convergence, and for each phase, we average the results of all timesteps of that phase. Since the algorithms require different numbers of timesteps for solving each of the tasks and reach different performance levels, we define the following heuristic criterion for the training phases. We first smooth the learning curves by averaging over a sliding window and compute the maximum episode return Rmax over the smoothed curve. Next, we calcu- late the improvement relative to the episode returnRinit of the initial policy at each timestept of the smoothed learning curve as ∆R(t) = R(t) − Rinit Rmax − Rinit . (7) We then define the end of the initial phase as the first timestep at which the agent reaches 10% of the total improvement, i.e., ∆R(t) ≥ 0.1. Similarly, we define the start of the convergence phase as the first timestep at which the agent reaches 90% of the total improvement, i.e., ∆R(t) ≥ 0.9. We choose k = 100 as subspace dimensionality since this subspace already largely captures the gradients, and the largest 100 eigenvectors can still be calculated reasonably efficiently with the Lanczos method. Appendix B displays results for different subspace sizes. With the tuned hyperpa- rameters from RL Baselines3 Zoo that we use for training, the PPO actor and critic usually contain around 5,000 parameters, and the SAC actor and critic around 70,000 and 140,000 parameters (2 Q-networks `a 70,000 parameters), respectively. Hence, the subspace dimensionality is around 2% the size of the parameters for PPO and around 0.14% and 0.07% for SAC. We consider a precise approximation of the true gradient computed with 106 state-action pairs for PPO and the full replay buffer for SAC. We denote this approximation as precise gradient and the low-sample gradient used during regular RL training as mini-batch gradient. In a similar manner, we denote the Hessian estimated on the large dataset as precise Hessian and the estimate from 2048 samples as mini-batch Hessian. We choose 2048 samples for the mini-batch Hessian since that is the amount of data that PPO with default hyperparameters collects for the policy updates. Hence, this is a realistic setting for estimating the subspace during training. Figure 2 shows the value of the gradient subspace fraction Sfrac(Pk, g) for PPO and SAC on four different tasks, divided into the three training phases. Note that for an uninformed random pro- jection, the gradient subspace fraction would be k/n in expectation, i.e., the ratio of the original 6Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 1.0 Gradient subspace fractionPPO SAC Ant PPO SAC Finger-spin PPO SAC LunarLanderContinuous PPO SAC Walker2D Initial phase Precise gradient, precise Hessian Training phase Mini-batch gradient, precise Hessian Convergence phase Mini-batch gradient, mini-batch Hessian 0.0 0.2 0.4 0.6 0.8 1.0 Gradient subspace fraction PPO SAC Ant PPO SAC Finger-spin PPO SAC LunarLanderContinuous PPO SAC Walker2D (a) Actor 0.0 0.2 0.4 0.6 0.8 1.0 Gradient subspace fraction PPO SAC Ant PPO SAC Finger-spin PPO SAC LunarLanderContinuous PPO SAC Walker2D (b) Critic Figure 2: The fraction Sfrac of the gradient that lies within the high-curvature subspace spanned by the 100 largest Hessian eigenvectors. Results are displayed for the actor (top) and critic (bottom) of PPO and SAC on the Ant, Finger-spin, LunarLanderContinuous, and Walker2D tasks. The results demonstrate that a significant fraction of the gradient lies within the high-curvature subspace, but the extent to which the gradient is contained in the subspace depends on the algorithm, task, and training phase. For both algorithms, the gradient subspace fraction is significantly higher for the critic than for the actor. Furthermore, the quantity is also often larger for SAC’s actor than for PPO’s, particularly in the early stages of the training. Even with mini-batch estimates for the gradient and Hessian, the gradient subspace fraction is considerable. and subspace dimensionalities (0.02 for PPO and 0.0014 for SAC’s actor and 0.0007 for its critic). The results in Figure 2 show a significantly higher gradient subspace fraction, which means that the gradients computed by PPO and SAC lie to a large extent in the high-curvature subspace. We observe that the fraction of the gradient in the subspace is considerably higher for the critic than for the actor. Furthermore, the gradient subspace fraction is also often higher for SAC’s actor than for PPO’s. This finding is particularly significant since the subspace size corresponds to a significantly lower percentage of the parameter dimensionality for SAC than for PPO. We hypothesize that the effect is caused by the off-policy nature of SAC. In the off-policy setting, the training distribution for the networks changes slowly since the optimization reuses previous data. In this regard, SAC is closer than PPO to the supervised learning setting, where the data distribution is fixed and for which Gur-Ari et al. (2018) report high gradient subspace fractions. Still, the subspace fraction for PPO is significant, considering that the dimensionality of the subspace is merely 2% of the origi- nal parameter space. Furthermore, for PPO, the subspace fraction often improves after the initial phase. Similarly, Gur-Ari et al. (2018) report for the supervised learning setting that the gradient starts evolving in the subspace only after some initial steps. However, for the SAC actor, this trend appears to be reversed, with the gradient subspace fraction being highest in the initial steps. Moreover, the precise gradient, computed with a large number of samples, tends to lie better in the subspace than the mini-batch gradient. The noise resulting from the low-sample approximation seems to perturb the gradient out of the subspace. However, since the difference is typically small, the gradient subspace is still valid for the low-sample gradient estimates used during RL training. Lastly, even the subspace identified with the mini-batch Hessian captures the gradient to a significant extent. This property is crucial since it implies that we do not need access to the precise Hessian, which is costly to compute and might require additional data. Instead, we can already obtain a reasonable gradient subspace from the mini-batch Hessian. 7Published as a conference paper at ICLR 2024 t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap PPO, actor PPO, critic SAC, actor SAC, critic t1 t1 0  1  2  3  4  5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (a) Ant t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (b) Finger-spin t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (c) LunarLanderCont. t1 0 1 2 3 4 5 t2 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (d) Walker2D Figure 3: Evolution of the overlap between the high-curvature subspace identified at an early timestep t1 = 100 ,000 and later timesteps for the actor and critic of PPO and SAC. While the overlap between the subspaces degrades as the networks are updated, it remains considerable even after 400,000 timesteps, indicating that the subspace remains similar, even under significant changes in the network parameters and the data distribution. This finding implies that information about the gradient subspace at earlier timesteps can be reused at later timesteps. 4.3 T HE HIGH -CURVATURE SUBSPACE CHANGES SLOWLY THROUGHOUT THE TRAINING So far, we have verified that the gradients of the actor and critic losses optimized by PPO and SAC lie to a large extent in the subspace spanned by the top eigenvectors of the Hessian with respect to the current parameters. However, even though there are relatively efficient methods for computing the top Hessian eigenvectors without explicitly constructing the Hessian matrix, calculating these eigenvectors at every step would be computationally expensive. Ideally, we would like to identify a subspace once that remains constant throughout the training. In practice, however, the gradient subspace will not stay exactly the same during the training, but if it changes slowly, it is possible to reuse knowledge from earlier timesteps and update the subspace at a lower frequency. To that end, we investigate condition iii) by calculating the subspace overlap, defined by Gur-Ari et al. (2018). The subspace overlap between timesteps t1 and t2 is defined as Soverlap \u0010 P(t1) k , P(t2) k \u0011 = 1 k kX i=1 \r\r\rP(t1) k v(t2) i \r\r\r 2 , (8) where v(t) i is the ith largest eigenvector at timestep t. P(t) k = \u0010 v(t) 1 , . . . , v(t) k \u0011T denotes the pro- jection matrix from the full parameter space to the high-curvature subspace, identified at timestep t. Similar to Equation (5), the criterion measures how much of the original vector is preserved during the projection into the subspace. For the subspace overlap, however, we use the projection matrix at timestep t1 not to project the gradient but rather project the Hessian eigenvectors that span the high-curvature subspace identified at a later timestep t2 of the training. This criterion, thus, mea- sures how much the gradient subspace changes between these timesteps. Note that we assume the eigenvectors v(t) i to be normalized to one and therefore do not normalize by their length. Gur-Ari et al. (2018) showed in the supervised setting that the gradient subspace stabilizes only after some initial update steps. Therefore, we choose the timestep t1 at which we initially identify the subspace as t1 = 100 ,000 since this is still relatively early in the training, but the gradient subspace should already have stabilized reasonably well. We evaluate the subspace overlap criterion every 10,000 timesteps until timestep 500,000. This interval covers a significant portion of the training and showcases the extent to which the subspace changes under significant differences in the network parameters and the data distribution. For the sake of completeness and to further highlight the influence of the data distribution on the subspace, we showcase the subspace overlap over the entire duration of the training in Appendix E. As in Section 4.2, we use k = 100 as subspace dimensionality and refer to Appendix B for the evaluation of different subspace sizes. The analysis results in Figure 3 show that the subspace overlap reduces the further apart the two timesteps t1 and t2 are, but in all cases, the subspace overlap remains significantly above zero, implying that information of previous subspaces can be reused at later timesteps. If the two timesteps are close to each other, the overlap is considerable. Similar to the gradient subspace fraction in Section 4.2, the subspace overlap is often more pronounced for the critic than the actor, particularly for SAC. 8Published as a conference paper at ICLR 2024 5 C ONCLUSION In this work, we showed that findings from the SL literature about gradient subspaces transfer to the RL setting. Despite the continuously changing data distribution inherent to RL, the gradients of the actor and critic networks of PPO and SAC lie in a low-dimensional, slowly-changing subspace of high curvature. We demonstrated that this property holds for both on-policy and off-policy learning, even though the distribution shift in the training data is particularly severe in the on-policy setting. 5.1 H IGH -CURVATURE SUBSPACES EXPLAIN CLIFFS IN REWARD LANDSCAPES Sullivan et al. (2022) investigate visualizations of the reward landscapes around policies optimized by PPO. Reward landscapes describe the resulting cumulative rewards over the space of policy pa- rameters. They observe empirically that these landscapes feature “cliffs” in policy gradient direction. When changing the parameters in this direction, the cumulative reward increases for small steps but drops sharply beyond this increase. In random directions, these cliffs do not seem to occur. The results from Section 4.2 constitute a likely explanation of this phenomenon. The cliffs that the authors describe can be interpreted as signs of large curvature in the reward landscape. Our analysis demonstrates that the policy gradient is prone to lie in a high-curvature direction of the policy loss. Sullivan et al. investigate the cumulative reward, which is different from the policy loss that we analyze in this work. However, one of the fundamental assumptions of policy gradient methods is that there is a strong link between the policy loss and the cumulative reward. Therefore, high curvature in the loss likely also manifests in the cumulative reward. There is no such influence for random directions, so the curvature in gradient direction is larger than in random directions. 5.2 P OTENTIAL OF GRADIENT SUBSPACES IN REINFORCEMENT LEARNING Leveraging properties of gradient subspaces has proven beneficial in numerous works in SL, e.g., (Li et al., 2022a; Chen et al., 2022; Gauch et al., 2022; Zhou et al., 2020; Li et al., 2022b). The analyses in this paper demonstrate that similar subspaces can be found in popular policy gradient algorithms. In the following, we outline two opportunities for harnessing the properties of gradient subspaces and bringing the discussed benefits to RL. Optimization in the subspace While the network architectures used in reinforcement learning are often small compared to the models used in other fields of machine learning, the dimensionality of the optimization problem is still considerable. Popular optimizers, like Adam (Kingma & Ba, 2014), typically rely only on gradient information, as computing the Hessian at every timestep would be computationally very demanding in high dimensions. However, in Section 4.1, we have seen that the optimization problem is ill-conditioned. Second-order methods, like Newton’s method, are known to be well-suited for ill-conditioned problems (Nocedal & Wright, 1999). With the insights of this paper, it seems feasible to reduce the dimensionality of the optimization problems in RL algorithms by optimizing in the low-dimensional subspace instead of the original parameter space. The low dimensionality of the resulting optimization problems would enable computing and inverting the Hessian matrix efficiently and make second-order optimization methods feasible. Guiding parameter-space exploration The quality of the exploration actions significantly im- pacts the performance of RL algorithms (Amin et al., 2021). Most RL algorithms explore by ap- plying uncorrelated noise to the actions produced by the policy. However, this often leads to ineffi- cient exploration, particularly in over-actuated systems, where correlated actuation is crucial (Schu- macher et al., 2022). A viable alternative is to apply exploration noise to the policy parameters instead (R ¨uckstiess et al., 2010; Plappert et al., 2018b). This approach results in a more directed exploration and can be viewed as exploring strategies similar to the current policy. In Section 4, we observed that the gradients utilized by policy gradient methods predominantly lie within a small subspace of all parameter-space directions. As typical parameter-space exploration does not consider the properties of the training gradient when inducing parameter noise, only a small fraction of it might actually push the policy parameters along directions that are relevant to the task. Considering that the optimization mostly occurs in a restricted subspace, it might be advantageous to limit exploration to these directions. Sampling parameter noise only in the high-curvature subspace constitutes one possible way of focusing exploration on informative parameter-space directions. 9Published as a conference paper at ICLR 2024 6 R EPRODUCIBILITY We applied our analyses to proven and publicly available implementations of the RL algorithms from Stable-Baselines3 (Raffin et al., 2021) on well-known, publicly available benchmark tasks from (Brockman et al., 2016; Plappert et al., 2018b; Tunyasuvunakool et al., 2020). Further exper- imental details like the learning curves of the algorithms and the fine-grained analysis results for the entire training are displayed in Appendices A and B, respectively. To facilitate reproducing our results, we make our code, as well as the raw analysis data, including hyperparmeter settings and model checkpoints, publically available on the project website. REFERENCES Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the ef- fectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Papers), pp. 7319–7328, 2021. Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A survey of exploration methods in reinforcement learning. arXiv preprint arXiv:2109.00157, 2021. Yunke Ao, Le Chen, Florian Tschopp, Michel Breyer, Roland Siegwart, and Andrei Cramariuc. Uni- fied data collection for visual-inertial calibration via deep reinforcement learning. InInternational Conference on Robotics and Automation, pp. 1646–1652. IEEE, 2022. Johan Bjorck, Carla P Gomes, and Kilian Q Weinberger. Is high variance unavoidable in RL? A case study in continuous control. In International Conference on Learning Representations, 2022. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. Xuxi Chen, Tianlong Chen, Yu Cheng, Weizhu Chen, Ahmed Awadallah, and Zhangyang Wang. Scalable learning to optimize: A learned optimizer can train big models. InEuropean Conference on Computer Vision, pp. 389–405. Springer, 2022. Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme parkour with legged robots. arXiv preprint arXiv:2309.14341, 2023. Michael Everett, Yu Fan Chen, and Jonathan P How. Motion planning among dynamic, decision- making agents with deep reinforcement learning. In International Conference on Intelligent Robots and Systems, pp. 3052–3059. IEEE, 2018. Martin Gauch, Maximilian Beck, Thomas Adler, Dmytro Kotsur, Stefan Fiel, Hamid Eghbal-zadeh, Johannes Brandstetter, Johannes Kofler, Markus Holzleitner, Werner Zellinger, et al. Few-shot learning by dimensionality reduction in gradient space. In Conference on Lifelong Learning Agents, pp. 1043–1064. PMLR, 2022. Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for online adaptation in reinforcement learning. In International Conference of Learning Representations, 2022. Jean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, and Roberta Raileanu. Building a subspace of policies for scalable continual learning. In International Con- ference of Learning Representations, 2023. Frithjof Gressmann, Zach Eaton-Rosen, and Carlo Luschi. Improving neural network training in low dimensional random bases. Advances in Neural Information Processing Systems, 33:12140– 12150, 2020. Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In International Conference on Robotics and Automation, pp. 3389–3396. IEEE, 2017. 10Published as a conference paper at ICLR 2024 Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.arXiv preprint arXiv:1812.04754, 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Confer- ence on Machine Learning, pp. 1861–1870. PMLR, 2018. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. A closer look at deep policy gradients. In International Con- ference on Learning Representations, 2020. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning, pp. 267–274, 2002. Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018. Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias M ¨uller, Vladlen Koltun, and Davide Scaramuzza. Champion-level drone racing using deep reinforcement learning. Nature, 620:982–987, 2023. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Brett W Larsen, Stanislav Fort, Nic Becker, and Surya Ganguli. How many degrees of freedom do we need to train deep networks: A loss landscape perspective. In International Conference on Learning Representations, 2021. Charline Le Lan, Joshua Greaves, Jesse Farebrother, Mark Rowland, Fabian Pedregosa, Rishabh Agarwal, and Marc G Bellemare. A novel stochastic gradient descent algorithm for learning principal subspaces. In International Conference on Artificial Intelligence and Statistics , pp. 1703–1718. PMLR, 2023. Richard B Lehoucq, Danny C Sorensen, and Chao Yang. ARPACK users’ guide: Solution of large- scale eigenvalue problems with implicitly restarted Arnoldi methods. SIAM, 1998. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018. Tao Li, Lei Tan, Zhehao Huang, Qinghua Tao, Yipeng Liu, and Xiaolin Huang. Low dimensional trajectory hypothesis is true: DNNs can be trained in tiny subspaces. Transactions on Pattern Analysis and Machine Intelligence, 45(3):3411–3420, 2022a. Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, and Xiaolin Huang. Subspace adversarial training. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 13409–13418, 2022b. Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided evolutionary strategies: Augmenting random search with surrogate gradients. In International Conference on Machine Learning, pp. 4264–4273. PMLR, 2019. V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier- stra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999. Supratik Paul, Vitaly Kurin, and Shimon Whiteson. Fast efficient hyperparameter tuning for policy gradient methods. Advances in Neural Information Processing Systems, 32, 2019. 11Published as a conference paper at ICLR 2024 Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682–697, 2008. Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow- ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce- ment learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018a. Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In International Conference on Learning Representations, 2018b. Antonin Raffin. RL Baselines3 Zoo. https://github.com/DLR-RM/ rl-baselines3-zoo, 2020. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dor- mann. Stable-Baselines3: Reliable reinforcement learning implementations. The Journal of Ma- chine Learning Research, 22(1):12348–12355, 2021. Thomas R ¨uckstiess, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi Sun, and J ¨urgen Schmidhuber. Exploring parameter space in reinforcement learning. Paladyn, Journal of Behavioral Robotics, 1:14–24, 2010. Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Pierre Schumacher, Daniel Haeufle, Dieter B ¨uchler, Syn Schmitt, and Georg Martius. DEP-RL: Embodied exploration for reinforcement learning in overactuated and musculoskeletal systems. In International Conference on Learning Representations, 2022. Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli. Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods. In International Conference on Machine Learn- ing, pp. 604–612. PMLR, 2014. Ryan Sullivan, Jordan K Terry, Benjamin Black, and John P Dickerson. Cliff diving: Exploring reward surfaces in reinforcement learning environments. InInternational Conference on Machine Learning, pp. 20744–20776. PMLR, 2022. Hongyao Tang, Min Zhang, and Jianye Hao. The ladder in chaos: A simple and effective im- provement to general DRL algorithms by policy path trimming and boosting. arXiv preprint arXiv:2303.01391, 2023. Mark Tuddenham, Adam Pr ¨ugel-Bennett, and Jonathan Hare. Quasi-Newton’s method in the class gradient defined high-curvature subspace. arXiv preprint arXiv:2012.01938, 2020. Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private SGD with gradient subspace identification. In International Conference on Learning Representations, 2020. 12Published as a conference paper at ICLR 2024 A L EARNING CURVES 0.0 0.2 0.4 0.6 0.8 1.0 Environment steps ×106 0 250 500 750 1000 Cumulative reward PPO SAC 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 2000 4000 6000 (a) Ant 0.00 0.25 0.50 0.75 1.00 0 500 1000 (b) Ball in cup 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 200 400 (c) BipedalWalker 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.1 0.2 0.3 0.4 0.5 40 20 0 (d) FetchReach 0.00 0.25 0.50 0.75 1.00 0 500 1000 (e) Finger-spin 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 5000 10000 15000 (f) HalfCheetah 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.5 1.0 1.5 0 2000 4000 (g) Hopper 0.00 0.25 0.50 0.75 1.00 250 0 250 (h) LunarLanderContinuous 0.0 0.1 0.2 0.3 1500 1000 500 0 (i) Pendulum 0.0  0.1  0.2  0.3 1500 1000 500 0 Cumulative reward 0.0 0.1 0.2 0.3 0.4 0.5 60 40 20 0 0.0  0.5  1.0  1.5  2.0  2.5  3.0 Environment steps (millions) 0 1500 3000 4500 6000 (j) Reacher 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 200 400 0.0  0.5  1.0  1.5  2.0  2.5  3.0 Environment steps (millions) 0 1500 3000 4500 6000  (k) Swimmer 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 2000 4000 6000 0.0  0.5  1.0  1.5  2.0  2.5  3.0 Environment steps (millions) 0 1500 3000 4500 6000  (l) Walker2D Figure 4: Learning curves for PPO and SAC on tasks from OpenAI Gym (Brockman et al., 2016), Gym Robotics (Plappert et al., 2018a), and the DeepMind Control Suite (Tunyasuvunakool et al., 2020). We use the algorithm implementations of Stable Baselines3 (Raffin et al., 2021) with tuned hyperparameters from RL Baselines3 Zoo (Raffin, 2020) for the Gym tasks and hyperparam- eters tuned by random search over 50 configurations for the Gym Robotics and DeepMind Control Suite tasks. Results are averaged over ten random seeds; shaded areas represent the standard devia- tion across seeds. 13Published as a conference paper at ICLR 2024 B D ETAILED ANALYSIS RESULTS FOR ALL TASKS 0.0 0.2 0.4 0.6 0.8 1.0 Environment steps ×106 0.0 0.2 0.4 0.6 0.8 1.0 Gradient fraction in subspace Precise gradient; 1 EV Mini-batch gradient; 1 EV Precise gradient; 10 EVs Mini-batch gradient; 10 EVs Precise gradient; 100 EVs Mini-batch gradient; 100 EVs 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (a) Ant, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (b) Ant, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (c) Ball in cup, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (d) Ball in cup, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (e) Bip.Walker, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (f) Bip.Walker, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (g) FetchReach, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (h) FetchReach, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (i) Finger-spin, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (j) Finger-spin, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (k) HalfCheetah, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (l) HalfCheetah, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (m) Hopper, actor 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (n) Hopper, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (o) LunarLander, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (p) LunarLander, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (q) Pendulum, actor 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (r) Pendulum, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (s) Reacher, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (t) Reacher, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0 (u) Swimmer, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (v) Swimmer, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (w) Walker2D, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (x) Walker2D, critic Figure 5: The evolution of the fraction of the gradient that lies within the high-curvature subspace throughout the training for PPO on all tasks. Evaluation of gradient subspaces with different num- bers of eigenvectors. Results for the actor and critic. 14Published as a conference paper at ICLR 2024 0.0 0.2 0.4 0.6 0.8 1.0 Environment steps ×106 0.0 0.2 0.4 0.6 0.8 1.0 Gradient fraction in subspace Precise gradient; 1 EV Mini-batch gradient; 1 EV Precise gradient; 10 EVs Mini-batch gradient; 10 EVs Precise gradient; 100 EVs Mini-batch gradient; 100 EVs 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (a) Ant, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (b) Ant, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (c) Ball in cup, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (d) Ball in cup, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (e) Bip.Walker, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (f) Bip.Walker, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (g) FetchReach, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (h) FetchReach, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (i) Finger-spin, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (j) Finger-spin, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (k) HalfCheetah, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 (l) HalfCheetah, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (m) Hopper, actor 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 (n) Hopper, critic 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (o) LunarLander, actor 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 (p) LunarLander, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (q) Pendulum, actor 0.0 0.1 0.2 0.3 0.0 0.2 0.4 0.6 0.8 1.0 (r) Pendulum, critic 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (s) Reacher, actor 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.8 1.0 (t) Reacher, critic 0.0  0.1  0.2  0.3  0.4  0.5 0.0 0.2 0.4 0.6 0.8 1.0Grad. subspace frac. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0 (u) Swimmer, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (v) Swimmer, critic 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (w) Walker2D, actor 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.5  1.0  1.5 Environment steps (millions) 0.0 0.2 0.4 0.6 0.8 1.0  (x) Walker2D, critic Figure 6: The evolution of the fraction of the gradient that lies within the high-curvature subspace throughout the training for SAC on all tasks. Evaluation of gradient subspaces with different num- bers of eigenvectors. Results for the actor and critic. 15Published as a conference paper at ICLR 2024 t1 0.0 0.2 0.4 0.6 0.8 1.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap 1 EV 10 EVs 100 EVs t1 t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (a) Ant, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (b) Ant, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (c) Ball in cup, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (d) Ball in cup, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (e) Bip.Walker, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (f) Bip.Walker, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (g) FetchReach, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (h) FetchReach, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (i) Finger-spin, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (j) Finger-spin, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (k) HalfCheetah, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (l) HalfCheetah, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (m) Hopper, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (n) Hopper, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (o) LunarLander, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (p) LunarLander, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (q) Pendulum, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (r) Pendulum, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (s) Reacher, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (t) Reacher, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (u) Swimmer, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (v) Swimmer, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (w) Walker2D, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (x) Walker2D, critic Figure 7: The evolution of the overlap between the high-curvature subspace identified at timestep t1 = 100 ,000 and later timesteps for PPO on all tasks. Evaluation of gradient subspaces with different numbers of eigenvectors. Results for the actor and critic. 16Published as a conference paper at ICLR 2024 t1 0.0 0.2 0.4 0.6 0.8 1.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap 1 EV 10 EVs 100 EVs t1 t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (a) Ant, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (b) Ant, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (c) Ball in cup, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (d) Ball in cup, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (e) Bip.Walker, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (f) Bip.Walker, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (g) FetchReach, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (h) FetchReach, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (i) Finger-spin, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (j) Finger-spin, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (k) HalfCheetah, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (l) HalfCheetah, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (m) Hopper, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (n) Hopper, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (o) LunarLander, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (p) LunarLander, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (q) Pendulum, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (r) Pendulum, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (s) Reacher, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 (t) Reacher, critic t1 0.0  0.4  0.8  1.2  1.6  2.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (u) Swimmer, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (v) Swimmer, critic t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (w) Walker2D, actor t1 0 1 2 3 4 5 ×105 0.0 0.2 0.4 0.6 0.8 1.0 t1 t1 0  1  2  3  4  5 t2 0.0 0.2 0.4 0.6 0.8 1.0 t1 (x) Walker2D, critic Figure 8: The evolution of the overlap between the high-curvature subspace identified at timestep t1 = 100 ,000 and later timesteps for SAC on all tasks. Evaluation of gradient subspaces with different numbers of eigenvectors. Results for the actor and critic. 17Published as a conference paper at ICLR 2024 C I MPACT OF SUBOPTIMAL HYPERPARAMETERS Hyperparameters typically have a significant impact on the learning performance of policy gradient algorithms (Paul et al., 2019). In Section 4.2, we analyzed the gradient subspace for training runs with tuned hyperparameter configurations. However, which hyperparameters work well for a given problem is typically not known a priori. Finding good hyperparameters often involves running nu- merous RL trainings, which might be infeasible when training on real-world tasks. For our insights to be valuable for such real-world settings, it is crucial that suitable gradient subspaces can also be identified for training runs with suboptimal hyperparameters. Therefore, in this section, we investi- gate the robustness of the gradient subspace with respect to suboptimal hyperparameters. To get a set of hyperparameters that are realistic but potentially suboptimal, we sample hyperparameter con- figurations from the ranges that are found frequently in the tuned hyperparameters ofRL Baselines3 Zoo (Raffin, 2020). Particularly, we draw the samples from the sets given in Table 1. Note that these are also the bounds we use when sampling configurations for hyperparameter tuning. Algorithm Hyperparameter Values Logscale PPO learning rate [10−5, 10−1] yes batch size {32, 64, 128, 256} no n steps {256, 512, 1024, 2048, 4096} no n epochs {5, 10, 20} no gamma [0.9, 1] no gae lambda [0.9, 1] no clip range {0.1, 0.2, 0.3} no ent coef [10−8, 10−2] yes net arch {(64, 64), (128, 128), (256, 256)} no SAC learning rate [10−5, 10−1] yes batch size {32, 64, 128, 256} no train freq {1, 4, 16, 32, 64} no gamma [0.9, 1] no tau {0.005, 0.01, 0.02} no learning starts {100, 1000, 10000} no net arch {(64, 64), (128, 128), (256, 256)} no Table 1: Sets from which we draw random hyperparameter configurations for PPO and SAC. We refer to each hyperparameter by its name in Stable Baselines3 (Raffin et al., 2021). Logscale means that we first transform the interval into logspace and draw the exponent uniformly from the trans- formed range. All other hyperparameters are drawn uniformly. These sets reflect common hyperpa- rameter choices from RL Baselines3 Zoo (Raffin, 2020). We sampled a total of 100 hyperparameter configurations per algorithm and task. Figure 9 dis- plays the distribution over the maximum cumulative reward that these hyperparameter configurations achieve. As expected, the variance in the performance is large across the hyperparameter configura- tions. While some configurations reach performance levels comparable to the tuned configuration, most sampled configurations converge to suboptimal behaviors. To display the values of the gradient subspace fraction and the subspace overlap compactly for all configurations, we compress the results for each training run to a single scalar. For the gradient subspace fraction, we compute the mean of the criterion over the timesteps. Regarding the subspace overlap, we choose the early timestept1 = 100,000 in accordance to the experiments in Section 4.3. However, taking the mean over the remaining timesteps would not be faithful to how the subspace would typically be utilized in downstream applications. Such methods would likely update the gradient subspace after a given number of gradient steps instead of identifying it once and using it for the rest of the training. The rate of this update would likely depend on the exact way that the 18Published as a conference paper at ICLR 2024 Finger-spin (PPO) Finger-spin (SAC) 0 200 400 600 800 1000Maximum reward T uned configurations (mean) Walker2D (PPO) Walker2D (SAC) 0 2000 4000 6000Maximum reward T uned configurations (mean) Figure 9: Violin plot of the maximum cumulative rewards achieved by 100 agents trained with random hyperparameters on Finger-spin (left) and Walker2D (right). The black dots mark the per- formance of the individual random configurations, and the red cross signifies the performance of the tuned hyperparameters averaged over 10 random seeds. The random hyperparameters result in agents of vastly different maximum performance. While some of the random hyperparameters achieve performance comparable to the tuned agents, the bulk of random hyperparameters stagnate at suboptimal performance levels. Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Gradient subspace fraction T uned configurations (mean) (a) Actor Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Gradient subspace fraction T uned configurations (mean) (b) Critic Figure 10: Violin plot of the mean gradient subspace fraction throughout the training of 100 agents with random hyperparameters. The red cross marks the same quantity for the high-performing hyperparameters used throughout the paper (averaged over 10 random seeds). For PPO’s actor, the variance in the gradient subspace fraction is significantly higher than for SAC’s. The critic’s gradient subspace fraction is very high across all hyperparameter configurations. 19Published as a conference paper at ICLR 2024 Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap (t2 t1 = 50,000) T uned configurations (mean) (a) Actor Finger-spin (PPO) Finger-spin (SAC) Walker2D (PPO) Walker2D (SAC) 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap (t2 t1 = 50,000) T uned configurations (mean) (b) Critic Figure 11: Violin plot of the mean subspace overlap fort1 = 100,000 and t2 = 150,000 of 50 agents with random hyperparameters. The red cross marks the same quantity for the high-performing hyperparameters used throughout the paper (averaged over 10 random seeds). For both tasks and networks, the variance of the subspace overlap is large across the hyperparameter configurations. subspace is utilized. For this analysis, we choose t2 = 150,000, so that the timestep difference of t2 −t1 = 50,000 steps is significantly shorter than the total length of the training, but the number of gradient steps is large enough that the subspace could change. The results for the gradient subspace fraction and the subspace overlap for random hyperparam- eters are visualized in Figure 10 and Figure 11, respectively. Figure 10a shows for SAC’s actor that the gradient is well contained in the subspace for most random hyperparameter configurations. For PPO’s actor, the spread in the gradient subspace fraction is significantly higher. As shown in Figure 10b, the gradient subspace fraction for the critic is very high for all hyperparameter config- urations. These results suggest that gradient subspaces can be utilized in SAC independently of the hyperparameter configuration, while PPO’s actor might require more considerations. Figure 11 shows that there is a significant spread in the subspace overlap for the random hyperpa- rameter configurations, which indicates that potential downstream applications might need to update the gradient subspace more frequently, depending on the hyperparameters. 20Published as a conference paper at ICLR 2024 D D ERIVATION OF THE GRADIENT SUBSPACE FRACTION OBJECTIVE In this section, we derive Equation (6). Recall that Pk = (v1, . . . , vk)T ∈ Rk×n is the matrix of the k largest Hessian eigenvectors, which is semi-orthogonal. We use ˜g = P+ k Pk g ∈ Rn to denote the projection of the gradient g into the subspace and back to its original dimensionality. In the following derivation, we drop the subscript k of matrix Pk for ease of notation. Sfrac(P, g) = 1 − ||˜g − g||2 ||g||2 (9) = 1 − ||P+Pg − g||2 ||g||2 (10) = 1 − ||PT Pg − g||2 ||g||2 (11) = 1 − (PT Pg − g)T (PT Pg − g) gT g (12) = 1 − (gT PT P − gT )(PT Pg − g) gT g (13) = 1 − gT PT PP T Pg − 2gT PT Pg + gT g gT g (14) = 1 − gT PT Pg − 2gT PT Pg + gT g gT g (15) = 1 − gT g − gT PT Pg gT g (16) = gT g − gT g + gT PT Pg gT g (17) = gT PT Pg gT g (18) = (Pg)T Pg gT g (19) = ||Pg||2 ||g||2 (20) Note that we used the fact that the pseudo-inverse of a semi-orthogonal matrix P is equal to its transpose P+ = PT in step (11). Furthermore, we used the property PP T = I of semi-orthogonal matrices in step (15). E S UBSPACE OVERLAP FOR THE ENTIRE TRAINING In Section 4.3, we showed the subspace overlap for a range of 400,000 timesteps. Figure 12 visual- izes the subspace overlap criterion with t1 = 100,000 for all future timesteps on tasks that require training for 3 million steps. While in practical applications of gradient subspaces, the subspace would likely be updated multiple times during training, this visualization highlights the influence of the data distribution on the subspace. The plots show a small drop in the subspace overlap for SAC at 1.1 million steps. Since the replay buffer has a size of 1 million samples, this marks the point at which the original data from timestep t1 is completely replaced by new data collected by updated policies. Since the networks’ training data is sampled from the replay buffer, this drop indicates that this change in the data distribution has a negative effect on the subspace overlap. The effect is gen- erally more pronounced for the critic than the actor because the actor’s subspace overlap degrades faster and is already at a relatively low level at the mark of 1.1 million timesteps. For PPO, there is no such drop in the subspace overlap since the algorithm does not use experience replay and instead collects new data for every update. 21Published as a conference paper at ICLR 2024 t1 0.0 0.2 0.4 0.6 0.8 1.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 Subspace overlap PPO, actor PPO, critic SAC, actor SAC, critic t1 t1 0.0  0.5  1.0  1.5  2.0  2.5  3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 0.0 0.5 1.0 1.5 2.0 2.5 3.0 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (a) Ant 0.0 0.5 1.0 1.5 2.0 2.5 3.0 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (b) HalfCheetah t1 0.0  0.5  1.0  1.5  2.0  2.5  3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0Subspace overlap t1 0.0 0.5 1.0 1.5 2.0 2.5 3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (c) Swimmer 0.0 0.5 1.0 1.5 2.0 2.5 3.0 t2 ×106 0.0 0.2 0.4 0.6 0.8 1.0 (d) Walker2D Figure 12: Evolution of the subspace overlap between the early timestep t1 = 100,000 (marked by the dashed gray line) and all future timesteps of the training. Results for the actor and critic of PPO and SAC. For SAC, a small drop in the subspace overlap is visible in all plots at around 1.1 million timesteps. This marks the timestep at which the data in the replay buffer is replaced completely by new data, indicating that the data distribution affects the subspace overlap. 22",
      "meta_data": {
        "arxiv_id": "2401.06604v3",
        "authors": [
          "Jan Schneider",
          "Pierre Schumacher",
          "Simon Guist",
          "Le Chen",
          "Daniel Häufle",
          "Bernhard Schölkopf",
          "Dieter Büchler"
        ],
        "published_date": "2024-01-12T14:40:55Z",
        "pdf_url": "https://arxiv.org/pdf/2401.06604v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the inefficiency of deep reinforcement learning (RL) due to persistently changing data distributions and a large number of parameters. Drawing inspiration from supervised learning (SL) where gradients reside in low-dimensional, slowly-changing subspaces, this work empirically evaluates if similar gradient subspaces exist in deep policy gradient (PG) methods. The main contributions are demonstrating that: (i) policy gradients for actor and critic networks in PPO and SAC indeed live in low-dimensional, high-curvature subspaces; (ii) these subspaces remain relatively stable throughout RL training despite continuous data distribution shifts; and (iii) the critic's subspace generally exhibits less variability and retains a larger fraction of its gradient. The findings highlight the potential for enhanced RL performance through improved parameter-space exploration or second-order optimization.",
        "methodology": "The study conducts a comprehensive empirical evaluation of gradient subspaces using two popular PG algorithms: Proximal Policy Optimization (PPO) (on-policy) and Soft Actor-Critic (SAC) (off-policy). It assesses three conditions: (i) the existence of high-curvature directions by analyzing the spectrum of Hessian eigenvalues (estimated via the Lanczos method); (ii) how well the gradients lie in the high-curvature subspace using the gradient subspace fraction Sfrac(Pk, g) = ||Pk g||^2 / ||g||^2; and (iii) the stability of these subspaces over time using the subspace overlap Soverlap(P(t1)k, P(t2)k). Hessian and gradient estimates were computed using 10^6 on-policy samples for PPO and the full replay buffer for SAC, with further robustness tests employing mini-batch estimates (2048 samples).",
        "experimental_setup": "Experiments were performed using PPO and SAC on twelve benchmark tasks from OpenAI Gym, Gym Robotics, and the DeepMind Control Suite. The algorithm implementations from Stable Baselines3 were utilized, with hyperparameters sourced from RL Baselines3 Zoo for Gym tasks and tuned via random search for others. Each experiment was run for 10 random seeds. Analysis of gradient subspace fraction and subspace overlap was conducted at checkpoints every 50,000 steps. Training progress was divided into initial, training, and convergence phases based on performance improvement (10% and 90% of total improvement). A subspace dimensionality of k=100 was chosen. Additionally, robustness to suboptimal hyperparameters was assessed by evaluating 100 randomly sampled configurations.",
        "limitations": "While the paper demonstrates the existence and stability of gradient subspaces, a direct limitation for immediate practical application is the computational expense of calculating Hessian eigenvectors at every training step, even with efficient methods like Lanczos. The study notes that low-sample mini-batch gradient approximations slightly perturb the gradient out of the precise subspace. Furthermore, the effectiveness and stability of gradient subspaces, as measured by gradient subspace fraction and subspace overlap, can vary significantly with suboptimal hyperparameter configurations, particularly for PPO's actor, suggesting that subspace updates might need to be more frequent depending on hyperparameters.",
        "future_research_directions": "The findings suggest two main avenues for future research: (1) Optimization in the subspace: Develop new RL optimization methods that operate within the identified low-dimensional, high-curvature subspace. This could enable efficient computation and inversion of the Hessian, making second-order optimization feasible for deep RL and addressing ill-conditioned problems. (2) Guiding parameter-space exploration: Utilize the knowledge of high-curvature subspaces to guide parameter-space exploration by sampling exploration noise predominantly in these relevant directions, potentially leading to more efficient and directed learning in RL. Additionally, further analysis could explore the implications of the critic's greater subspace stability for specialized optimization strategies."
      }
    },
    {
      "title": "Evaluation of Test-Time Adaptation Under Computational Time Constraints",
      "abstract": "This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.",
      "full_text": "Evaluation of Test-Time Adaptation Under Computational Time Constraints Motasem Alfarra 1 2 Hani Itani 1 Alejandro Pardo 1 Shyma Alhuwaider 1 Merey Ramazanova 1 Juan C. P´erez 1 Zhipeng Cai 2 Matthias M¨uller 2 Bernard Ghanem 1 Abstract This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) meth- ods, which penalizes slower methods by provid- ing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Although many effec- tive methods have been proposed, their impressive performance usually comes at the cost of signif- icantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA meth- ods, where data is received in an online fashion from a constant-speed data stream, thereby ac- counting for the method’s adaptation speed. We apply our proposed protocol to benchmark sev- eral TTA methods on multiple datasets and sce- narios. Extensive experiments show that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower methods. For example, SHOT from 2020, outperforms the state-of-the-art method SAR from 2023 in this setting. Our results re- veal the importance of developing practical TTA methods that are both accurate and efficient1. 1. Introduction In recent years, Deep Neural Networks (DNNs) have demon- strated remarkable success in various tasks (He et al., 2016) thanks to their ability to learn from large datasets (Deng et al., 2009). However, a significant limitation of DNNs is their poor performance when tested on out-of-distribution 1King Abdullah University of Science and Technol- ogy (KAUST), Thuwal, Saudi Arabia 2Intel Labs, Munich, Germany. Correspondence to: Motasem Alfarra <mo- tasem.alfarra@kaust.edu.sa>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). 1Code: github/MotasemAlfarra/Online-Test-Time-Adaptation Current Evaluation Realistic Evaluation40 45 50 55 60 65 70 75Error Rate (%)  AdaBN 17  AdaBN 17  SHOT 20  SHOT 20  TENT 21  TENT 21  SAR 23  SAR 23 Figure 1: The trend of average error rate using offline evaluation vs our proposed online evaluation. In the offline setup, TTA methods demonstrate progress across time with a decreasing average error rate, e.g. from 68.5% using AdaBN to 56.2% using SAR. We propose a realistic evaluation protocol that accounts for the adaptation speed of TTA methods. Under this protocol, fast methods ( e.g. AdaBN) are unaffected, while slower (but more recent and sophisticated) methods (e.g. SAR) are penalized. data, which violates the i.i.d. assumption that the training and testing data are from the same distribution (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Kar et al., 2022). Such failure cases are concerning, since distribu- tion shifts are common in real-world applications, e.g., im- age corruptions (Hendrycks & Dietterich, 2019), chang- ing weather conditions (Sakaridis et al., 2021), or security breaches (Goodfellow et al., 2014). Test Time Adaptation (TTA) (Saenko et al., 2010; Sun et al., 2020; Liu et al., 2021) has demonstrated promising results for solving the above problem. TTA leverages the unlabeled data that arrives at test time by adapting the forward pass of pre-trained DNNs according to some proxy task (Liang et al., 2020; Lee et al., 2013). Though recent methods have made significant progress at improving accuracy under dis- tribution shifts (Wang et al., 2020; Niu et al., 2022; Gao et al., 2022), many of them incur high computational over- head. For instance, some methods require self-supervised fine-tuning on the data (Chen et al., 2022), while others perform a diffusion process per input (Gao et al., 2022). The computational overhead of TTA methods decreases 1 arXiv:2304.04795v2  [cs.LG]  23 May 2024Evaluation of Test-Time Adaptation Under Computational Time Constraints their inference speed, which is a critical property in many real-world applications that require the TTA method to pro- duce predictions at the speed of the stream itself. This property, however, is overlooked in the current evaluation protocols for TTA methods. In particular, these protocols assume a setting, which neglects how events constantly un- fold regardless of the model’s speed, causing the model to miss incoming samples when it is busy processing previous ones. For TTA methods that adapt using test data, missing samples has a direct effect on the method’s accuracy, as it will have fewer samples for adaptation. That is, the slower the TTA method, the fewer samples it can leverage for adapt- ing to the distribution shift. Thus, the current protocol for evaluating TTA methods is not suitable for assessing their efficacy in real-world deployment. In this work, we propose a novel realistic evaluation proto- col that factors in inference speed to assess the real-world applicability of TTA methods. Our evaluation protocol is in- spired by Online Learning (Cai et al., 2021; Shalev-Shwartz et al., 2012) and mimics real-world scenarios by exposing all TTA methods to a constant-speed stream of data. In this setting, the performance of slow TTA methods is in- trinsically penalized, as the time spent adapting to a sample may lead to dropped samples that could have been useful for adaptation. Specifically, our protocol dictates that if a method gslow is k times slower than the stream, then it may only use every kth sample for adaptation. In contrast, a method gfast that is as fast as the stream is allowed to adapt to every sample. Figure 1 shows the effect of evaluating several methods under our proposed protocol, where slower methods (e.g., SAR (Niu14 et al., 2023)) are penalized and faster but simpler methods become better alternatives (e.g., SHOT (Liang et al., 2020) and AdaBN (Li et al., 2016)). We apply our proposed evaluation protocol to benchmark several TTA methods on multiple datasets, and provide a fair assessment of their performance subject to the realistic consequences of slower inference speeds. Our experimental results highlight the importance of developing TTA methods that adapt to distribution shifts with minimal impact on inference speed. Our contributions are two-fold: 1. We propose a realistic evaluation protocol for TTA methods that penalizes slower methods by providing them with fewer samples for adaptation. Our approach is effective at assessing TTA methods’ efficacy in sce- narios where data arrives as a constant-speed stream. 2. Following our proposed protocol, we provide a com- prehensive experimental analysis of 15 TTA methods evaluated on 3 large-scale datasets under 3 different evaluation scenarios. These scenarios consider adap- tation to a single domain and continual adaptation to several domains. Our analysis shows that, when in- ference speed is accounted for, simple (but faster) ap- proaches can benefit from adapting to more data, and thus outperform more sophisticated (but slower) meth- ods. Figure 1 demonstrates this for four TTA methods. We hope our evaluation scheme inspires future TTA methods to consider inference speed as a critical di- mension that affects their real-world performance. 2. Related Work Test Time Adaptation. The Test Time Adaptation (TTA) setup relaxes the “i.i.d” assumption between the training and testing distributions (Sun et al., 2020; Boudiaf et al., 2022). This relaxation is usually attained through a lifelong learning scheme on all received unlabeled data (Chen et al., 2022; Gong et al.). Earlier approaches such as TTT (Sun et al., 2020) and TTT++ (Liu et al., 2021), among others (Torralba & Efros, 2011; Tzeng et al., 2017), include a self-supervised loss (Gidaris et al., 2018) during training, which can then provide an error signal during adaptation. Despite their effectiveness, such approaches assume having control over how the model is trained. Fully Test Time Adaptation. Fully TTA methods are a subtype of TTA method that adapts at test time by modify- ing the model’s parameters (Liang et al., 2020; Lee et al., 2013; Mirza et al., 2022b; Mancini et al., 2018; Kojima et al., 2022) or its input (Gao et al., 2022) by using the incoming unlabeled data. Fully TTA methods are practi- cal, as they avoid assumptions on the training phase of a given model (Wang et al., 2020; Gao et al., 2022; Iwasawa & Matsuo, 2021). The first of these approaches adjusts the statistics of the Batch Normalization (BN) layers (Mirza et al., 2022a; Schneider et al., 2020; Li et al., 2016). For example, BN-adaptation (Schneider et al., 2020) leverages the statistics of the source data as a prior and infers the statis- tics for every received sample. On the other hand, AdaBN (Li et al., 2016) discards the statistics of the source domain and uses the statistics computed on the target domain. In line with light TTA methods, LAME (Boudiaf et al., 2022) proposes to only adapt the model’s output by finding the latent assignments that optimize a manifold-regularized like- lihood of the data. In this work, we found that such efficient methods preserve their accuracy under our proposed eval- uation. While fully TTA methods have been studied in the context of adversarial domain shifts (Alfarra et al., 2022; Croce et al., 2022; P´erez et al., 2021), in this work we focus on the context of natural shifts such as realistic image cor- ruptions (Hendrycks & Dietterich, 2019; Kar et al., 2022). Another line of work aims at adapting to distribution shifts by minimizing entropy. For instance, SHOT (Liang et al., 2020) adapts the feature extractor to minimize the entropy of individual predictions; while maximizing the entropy of the predicted classes. TENT (Wang et al., 2020) updates the learnable parameters of the BN layers to minimize the 2Evaluation of Test-Time Adaptation Under Computational Time Constraints Adapted SampleNon-AdaptedSampleTTA method Current evaluation . . . . . . Realistic evaluation . . . . . . Model Figure 2: Inference under the current and realistic evaluation protocols. The current evaluation setting (left) assumes that the incoming batches of stream S can wait until the adaptation process of a TTA method g finishes. This assumption is untenable in a real-time deployment scenario. Our proposed realistic evaluation (right) simulates a more realistic scenario where S reveals data at a constant speed. In this setup, slower TTA methods will adapt to a smaller portion of the stream. The remaining part of the stream will be predicted without adaptation by employing the most recent adapted model. We refer to the most recent adapted model as fθt+1 , with t denoting the time when the last sample was adapted to by g. When g is still adapting to a sample, the incoming sample is fed to fθt+1 to produce predictions. entropy of predictions. EATA (Niu et al., 2022) combines TENT with an active selection of reliable and non-redundant samples from the target domain and an anti-forgetting loss (Kirkpatrick et al., 2017). Further, SAR (Niu14 et al., 2023) equips TENT with an active sampling scheme that filters samples with noisy gradients. Other works use data-augmentation at test time (Ashukha et al., 2020). For example, MEMO (Zhang et al., 2021) adapts model parameters to minimize the entropy over a sample and multiple augmentations of it. CoTTA (Wang et al., 2022) uses augmentations to generate reliable pseudo- labels and then peform distillation. Finally, DDA (Gao et al., 2022) proposes to leverage a diffusion model (Ho et al., 2020) to restore corrupted inputs back to the source data distribution. These methods require multiple forward passes through the network or a diffusion model, leading to slower inference speeds. 3. Methodology In this section, we present our proposed Realistic TTA evalu- ation protocol. We first describe the current TTA evaluation protocol and its limitations Then, we introduce our Realistic TTA evaluation protocol, which addresses the shortcomings of the offline protocol. 3.1. Current Protocol TTA considers the practical setup, in which trained models are deployed in a target domain that exhibits distribution shifts to which they must adapt. Let fθ : X → Ybe a clas- sifier, parameterized by θ, that predicts the label y ∈ Yfor a given input x ∈ X. Before test time, fθ is assumed to have been trained on the dataset Dtrain ⊂ X × Y. At test time, i.e. when executing TTA,fθ is presented with a stream of data S, sampled from X, with potentially multiple distribution shifts w.r.t. Dtrain. Under this setup, a TTA method is a function g(θ, x) that sequentially adapts the model’s param- eters θ and/or the input x to enhance the performance under distributions shifts. Currently, TTA methods are evaluated in an offline setting. Formally, the Current TTA evaluation protocol simulates the interaction between the stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: Curr.1 S reveals a sample xt. Curr.2 g adapts xt to ˆxt, θt to ˆθt, generates prediction ˆyt, and updates parameters θt+1 = αθt + (1 − α)ˆθt.2 Note that all existing TTA methods can be modeled using this framework. For example, TENT (Wang et al., 2020) adapts network parameters to minimize entropy with α = 0, while leaving inputs unchanged, i.e. ˆxt = xt and θt+1 = ˆθt. DDA (Gao et al., 2022) adapts inputs via a diffusion process while preserving network parameters with α = 1, i.e. ˆxt = ˆxt and θt+1 = θt. CoTTA (Wang et al., 2022) applies knowledge distillation, and updates network parameters with an exponential moving average, i.e. setting 0 < α <1. Shortcomings of the Current TTA protocol.In the current protocol, the performance of a TTA method g is measured by comparing the ground truth labels yt with the predic- tions after adaptation ˆyt. An evaluation based only on this measure implicitly assumes that the stream is not constant 2Note that some methods abstain from adapting either xt or θt. 3Evaluation of Test-Time Adaptation Under Computational Time Constraints speed, but rather waits for g to adapt to xt (Curr.2) before revealing the next batch xt+1 (Curr.1). Figure 2 provides an illustration of this situation. This assumption results in the offline protocol favoring slower TTA methods, as the method’s performance is agnostic to its inference speed. However, in practical applications where the test data ar- rives at a constant speed, the offline protocol is not suitable for assessing a method’s performance. Next, we propose a remedy for this shortcoming. 3.2. Realistic Online Evaluation Protocol We propose a realistic evaluation of TTA methods that explicitly considers the relation between the speed of the method and the speed at which the stream reveals new data. This setup is more realistic, as it intrinsically penalizes the performance of slower TTA methods: long times spent in adaptation result in fewer samples to adapt to. A crucial aspect of our realistic TTA protocol is accounting for the implications of simulating a constant speed data stream S. For instance, consider a stream S that reveals data at a constant rate r samples per second. If a method gfast adapts to samples at speed r, then gfast will be able to adapt to every sample. On the other hand, if gslow adapts to samples at a speed r/2, then gslow will skip every other sample. We formalize the notion of the relation between the speed of the stream and the speed of a method g as the “relative adaptation speed of g”. This quantity, denoted by C(g) ∈ N, is simply the integer ratio of the speed of S to the speed of g. For instance, in the previous example, C(gfast) = 1, meaning gfast adjusts as fast as S reveals data, while C(gslow) = 2 , indicating S reveals its second batch while gslow is still adapting to the first one. Without loss of generality, we assume that fθ runs in real- time, i.e. that its speed is equal to r, and thus C(fθ) = 1 . This assumption allows us to suppose that the samples that are not processed by g can be processed by fθ. Under this setup, we define our realistic protocol by introducing the relative adaptation speed C(g) into the offline protocol. In particular, we simulate g’s availability by conditionally performing the adaptation step (Curr.2), depending on C(g). In this manner,g is only permitted to adapt when its previous adaptation step has finished. Formally, the realistic TTA evaluation protocol simulates the interaction between the constant speed stream S and the TTA method g, at each time step t ∈ {0, 1, . . . ,∞}, as follows: RTTA 1 S reveals a sample xt. RTTA 2 If (t mod C(g)) = 0, then g adapts xt to ˆxt, θt to ˆθt, generates a prediction ˆyt, and updates pa- rameters via θt+1 ← αθt + (1 − α)ˆθt. Otherwise, fθt generates a prediction ˆyt. Table 1: Average C(g(xt)). We report the average relative adaptation speed C(g) for 5 TTA methods. The higher C(g) is, the smaller the portion of data to which g adapts is. Method AdaBN TENT TTAC-NQ MEMO DDA C(g) 1 3 12 54 810 Here, “mod” represents the modulo operation. The above protocol assesses the performance of TTA methods by fac- toring in their speed. As such, faster methods are granted more adaptation steps and, conversely, slower methods are granted fewer (see Figure 2). Note that explicitly modeling the relative adaptation speeds allows us to evaluate TTA methods under different adaptation speeds by setting C(g) to arbitrary values. For instance, note that our realistic proto- col recovers the original offline protocol by settingC(g) = 1 for all methods. Next, we explain the calculation of C(g) for our realistic protocol. Online computation of C(g). In practice, estimating the relative adaptation speed C(g) can be a noisy process. The noise in this estimation essentially comes from two factors: hardware and input dependence. Hardware-induced noise applies to all methods, while input dependence applies to methods like ETA (Niu et al., 2022) which, upon receiving an input, may optionally abstain from adapting to it. This noise means that C(g) potentially varies across iterations. Our protocol accounts for this variability by conducting an online computation of C(g) on each revealed input. That is, instead of using a fixed value of C(g) at each itera- tion t, our protocol rather uses C (g(xt)). Formally, if we let R (g(x)) denote the speed at which g processes x, then the relative adaptation speed of g at x is defined as C (g(xt)) = ⌈r/R(g(x))⌉, where the ceiling function ac- counts for the stream’s discrete-time nature. Note that since we assumed C(fθ) = 1, then R (fθ(x)) = r. We report the empirical behavior of this online computation of C (g(xt)) for various TTA methods in Table 1, and leave the rest of the methods and the computation details to the Appendix. Next, we leverage our Realistic TTA protocol to conduct a comprehensive empirical study of several TTA methods. 4. Experiments We follow prior art (Wang et al., 2020; Niu14 et al., 2023; Gao et al., 2022) and focus on the task of image classifica- tion. In all our experiments, we assume that fθ is a ResNet- 50-BN3 (He et al., 2016) trained on ImageNet (Deng et al., 2009) (pretrained weights obtained from torchvision). We further assume that the stream S reveals batches of size 3SAR demonstrated the superiority of using batch independent normalization layers under batch size of 1. We leave this ablation to the Appendix along with experiments on other architectures. 4Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 2: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on ImageNet-C benchmark under both the realistic and the current setup. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The current setup is merely the reproduction of every method. The first sub-table corresponds to methods that do not incur any or few extra computations, i.e. C(g) = 1. We show that methods generally perform worse in the realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Noise Blur Weather DigitalMethod Realisticgauss. shot impul.defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 97.8 97.1 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.4 82.0 - AdaBN ✓ 84.9 84.3 84.3 85.0 84.7 73.6 61.1 65.8 66.9 52.1 34.8 83.3 56.1 51.1 60.3 68.5 - LAME ✓ 98.3 97.6 98.6 82.4 90.9 86.1 78.1 84.5 77.5 77.3 41.4 94.8 84.8 80.0 68.9 82.7 - BN ✓ 84.6 83.9 83.8 80.1 80.2 71.7 60.4 65.4 65.2 51.6 34.6 76.3 54.4 49.7 59.2 66.7 - ✗ 73.4 70.2 73.0 76.6 75.5 59.8 53.8 54.2 63.4 44.7 35.5 79.3 46.9 43.2 49.7 59.9SHOT ✓ 73.6 69.0 71.1 74.6 74.8 60.0 52.9 54.1 61.3 44.1 34.1 77.8 46.8 43.1 49.2 59.1 (-0.8) ✗ 71.3 69.4 70.2 72.0 72.9 58.7 50.7 52.8 58.8 42.7 32.7 73.3 45.5 41.5 47.7 57.3TENT ✓ 75.7 78.3 75.2 76.3 77.3 64.6 55.6 57.3 61.4 45.9 33.5 77.1 50.1 44.2 51.4 61.6 (+4.3) ✗ 69.5 69.7 69.0 71.2 71.7 58.1 50.5 52.9 57.9 42.7 32.7 62.9 45.5 41.6 47.8 56.2SAR ✓ 79.4 78.5 78.1 79.9 79.3 67.5 56.1 60.5 63.1 47.4 34.0 75.3 51.7 46.6 53.8 63.4 (+7.2) ✗ 78.4 77.8 77.2 80.5 79.1 64.0 53.3 57.8 60.7 44.1 32.9 73.1 48.6 42.3 52.6 61.5CoTTA ✓ 82.9 81.6 81.9 87.4 85.6 75.6 61.1 63.1 64.9 49.9 34.8 91.2 54.0 48.8 56.6 68.0 (+6.5) ✗ 71.3 70.3 70.8 82.1 77.4 63.9 53.9 49.9 55.5 43.9 32.8 81.4 43.7 41.1 46.7 59.0TTAC-NQ ✓ 79.4 75.7 78.9 86.6 86.2 77.1 61.8 58.8 62.4 51.5 34.4 88.5 52.1 49.1 55.5 66.5 (+7.5) ✗ 65.5 62.4 63.5 66.6 67.2 52.0 47.3 48.2 54.1 39.9 32.1 55.0 42.3 39.2 44.8 52.0EATA ✓ 69.3 67.1 69.2 71.1 71.7 57.5 49.9 51.9 57.4 42.4 32.6 60.7 45.1 41.4 47.4 55.6 (+3.6) ✗ 92.5 91.3 91.0 84.0 87.0 79.3 72.4 74.6 71.3 67.9 39.0 89.0 76.2 67.0 62.4 76.3MEMO ✓ 97.7 97.0 98.0 82.1 90.1 85.1 77.4 83.0 76.6 75.4 41.0 94.5 82.9 79.2 68.2 81.9 (+5.6) ✗ 58.6 57.8 59.0 87.0 81.6 76.6 65.9 67.9 66.7 64.0 40.0 92.2 52.2 46.6 49.9 64.4DDA ✓ 97.8 97.0 98.1 82.1 90.2 85.2 77.5 83.1 76.7 75.6 41.1 94.6 83.0 79.4 68.3 82.0 (+17.6) 644, except for MEMO (Zhang et al., 2021), which pre- dicts on single images to incentivize prediction consistency over an input and its augmentations. Regarding datasets, we follow earlier works (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Gao et al., 2022; Zhang et al., 2021), and thus evaluate on the ImageNet-C dataset (Hendrycks & Dietterich, 2019) with a corruption level of 5 for all 15 corruptions. We further extend our evaluation and consider CIFAR10-C, ImageNet-R (Hendrycks et al., 2021), and the more recent ImageNet-3DCC (Kar et al., 2022), which lever- ages depth estimates to construct more spatially-consistent corruptions. Our experiments compare the performance of the base- line model fθ (without test time adaptation) against 15 state-of-the-art TTA methods published in top-tier venues (e.g., CVPR, NeurIPS, and ICLR) between 2017 and 2023. In particular, we consider: BN (Schneider et al., 2020) and AdaBN (Li et al., 2016), which adjust the statistics of the batch normalization layers; SHOT (Liang et al., 2020) and SHOT-IM (Liang et al., 2020), which fine-tune the feature extractor to maximize mutual information; entropy mini- mization approaches such as TENT (Wang et al., 2020), 4This batch size is recommended by most baselines (Wang et al., 2020; Niu et al., 2022) ETA (Niu et al., 2022) (a more efficient version of TENT), and SAR (Niu14 et al., 2023), which trains the learnable parameters of the batch normalization layers; distillation approaches, such as CoTTA (Wang et al., 2022), Pseudo Labeling (PL) (Lee et al., 2013), and the very recent and efficient LAME (Boudiaf et al., 2022); EATA (Niu et al., 2022) and TTAC (Su et al., 2022) that assume access to the source training data; data-dependent approaches such as MEMO (Zhang et al., 2021) and the diffusion-based method DDA (Gao et al., 2022). For all methods, we use their official implementation with their recommended hyper- parameters. We report our experimental results on a subset of 12 baselines, while leaving ETA, SHOT-IM, and PL to the appendix due to space constraints and their similarity to SHOT and EATA. As mentioned in Section 3.2 , our protocol performs an online computation of the relative adaptation speed of g. In particular, for each batch revealed by the stream, we compute C (g(x)). Then, if C(g(xi)) = k, all the samples {xi+1, xi+2, . . . , xi+k} are processed by fθi without adap- tation. Otherwise, if C(g(xi)) = 1, then these samples are processed by g. For methods that accumulate parameter updates such as TENT (Wang et al., 2020), fθi is the most recent updated model g(fθi−1 ). We report all our main re- sults as the average across three seeds, and leave the detailed 5Evaluation of Test-Time Adaptation Under Computational Time Constraints SHOT TENT TTAC-NQ SAR EATA COTTA brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%) (a) Current Continual TTA. brigh.pixel.gauss.motionzoomglassimpul.jpegdefoc.elast.shotfrostsnowfog contr.clean 30 40 50 60 70 80 90 100Error Rate (%)  (b) Realistic Continual TTA. Figure 3: Continual Error Rate on ImageNet-C. We report the continual error rate of several TTA methods on ImageNet-C benchmark under both realistic and current setups. A lower error rate indicates a better TTA method. Continual evaluation means the corruptions are presented in a sequence without resetting the model in between. We choose the same order as presented along the x-axis; starting with brightness and ending with clean validation set. In the current setup, we observe an increasing trend for SHOT, TENT, and TTAC-NQ. This is hypothesized to be due to overfitting on the early distribution shifts. This behavior is mitigated in the realistic setup due to adapting to fewer batches. EATA and SAR perform equally well in both realistic and current continual setups due to sample rejection. We report the standard deviation across 3 seeds. analysis to the Appendix. Throughout the experiments, we refer to our realistic evaluation protocol as “realistic/on- line”, and refer to the current protocol as “current/offline”. Next, we evaluate all methods on four different scenarios: (i) when domain shifts happen in an episodic manner, (ii) when domain shifts happen continually, i.e. one after the other, (iii) when the stream speed varies, (iii) when domain shifts happen continually with label correlation; practical evaluation (Yuan et al., 2023) ,and (v) when the baseline fθ is unavailable for evaluating the samples skipped by the TTA method g (left for the appendix). 4.1. Episodic Evaluation of TTA First, we consider an episodic evaluation of domain shifts, whereby S contains a single domain (e.g. one corruption) from ImageNet-C. We analyze this simple and most com- mon setup to assess the performance of TTA methods under real-time evaluation. We report the error rates on all corrup- tions in Table 2 and the average error rate across corruptions. We summarize the insights as follows: (i) The performance of TTA methods often degrades significantly under the realistic setup. Most methods induce a significant computational overhead, which prevents them from adapting to every sample from the test stream. For example, the error rate increases by 7.5% for TTAC- NQ and 4.3% for TENT, where C(gTTAC-NQ) = 12 and C(gTENT) = 3 (see Table 1). That is, TENT adapts to one- third of the batches revealed by the stream, while TTAC-NQ adapts to one every twelve batches. (ii) Very efficient methods, withC(g) = 1, such as LAME and BN, do not lose in performance. Evaluating such methods in offline or realistic setups is inconsequential, as their adaptation incurs negligible additional computation (since they adapt during the forward pass (Li et al., 2016; Schneider et al., 2020) or by adjusting the logits (Boudiaf et al., 2022) at a speed that pales in comparison to that of the stream). Interestingly, in our realistic evaluation, the simple BN (published in 2020) with an average error rate of 66.7% outperforms more recent and advanced methods such as SAR (published in 2023) by 1.7%. Furthermore, AdaBN (published in 2017) significantly outperforms the very recent diffusion-based DDA by a notable 13%. (iii) Data-dependent approaches, such as MEMO and DDA, are extremely inefficient. Despite the independence of MEMO and DDA on batch size, they incur a massive computational burden. For instance, C(gMEMO) = 54 and C(gDDA) = 810. Thus, both methods will be busy adapting for considerable portions of the stream, leaving most predic- tions to the non-adapted classifier. This phenomenon is the reason behind the reported performance of these methods being so close to that of fθ (i.e. around 82%). This result calls for future research to focus on increasing the efficiency of data-dependent adaptation methods. (iv) Sample rejection-oriented methods can perform well under the realistic protocol. EATA adapts efficiently due to its fast sample rejection algorithm, which relies solely on 6Evaluation of Test-Time Adaptation Under Computational Time Constraints the forward pass to admit samples for adaptation. EATA’s low error rate of 55.6%, combined with a small performance drop of less than 4%, positions it as the top performer under the realistic evaluation protocol on ImageNet-C. On the other hand, SAR does not benefit from sample rejection. SAR’s performance drop of 7.5% is due to its dependence on gradients for sample rejection, which reduces its speed. (v) SHOT benefits from the realistic protocol. Interest- ingly, we found that SHOT (and SHOT-IM in the Appendix), a fine-tuning-based approach, benefits from our realistic evaluation. In particular, we found that SHOT’s error rate decreases by 2% on fog corruption and by 0.8% on average. This observation could suggest that SHOT could potentially improve performance by disposing of fine-tuning on every batch. It is also worth mentioning that, under our realis- tic evaluation, SHOT (introduced in 2020) outperforms all methods except EATA. (vi) Performance changes are consistent across corrup- tions. Note that all methods that are somewhat efficient can improve the source model across all corruptions, in both the offline and realistic setups. Furthermore, the performance changes when comparing the offline and realistic setups are consistent across all corruptions. This finding suggests that the performance of these methods is independent of the do- main shift being considered. We further test this hypothesis by benchmarking these methods on two other datasets with other types of domain shifts in Section 4.4. 4.2. Continual Evaluation of TTA Next, we analyze the more challenging continual setup, fol- lowing (Wang et al., 2022; Niu et al., 2022). In particular, we construct the stream S by concatenating all corruptions from ImageNet-C. That is, we adapt TTA methods continu- ally on all corruptions followed by the clean validation set, without ever resetting the network weights. We introduce the notion of realistic adaptation to the continual setup to study the effects of a constant stream speed on the bench- mark. We report results in Figure 3 for both the offline and realistic protocols, where the horizontal-axis shows how cor- ruptions are ordered in the stream. We limit the experiments in this section to six TTA methods (SHOT, TENT, TTAC- NQ, COTTA, EATA, and SAR), and leave the remaining details for the Appendix. We observe: (i) Methods that do not perform sample rejection (SHOT, TENT, TTAC) scale poorly in the offline-continual setup. This phenomenon can be attributed to these methods over- fitting to early distributions. However, methods that do perform sample rejection (SAR and EATA) do not overfit as easily to corruptions, and can thus adapt to the rest of the stream. Even worse, such methods tend to even significantly degrade the performance on clean data. 1/16 1/8 1/4 1/2 1 η 52 55 58 61 64 67Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 4: Average Error Rate on ImageNet-C Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-C under slower stream speeds. In our proposed realistic model evaluation, the stream speed r is normalized by the time needed for a for- ward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 different random seeds. Different TTA methods degrade differently when varying η. (ii) In the realistic-continual setup, methods that do not perform sample rejection benefit from skipping adapta- tion on some batches, and become competitive with the methods that perform sample rejection. That is, while skipping parts of the stream deteriorated the performance of such methods in the episodic evaluation , this skipping actu- ally helped in preventing these methods from over-fitting in the continual setup. 4.3. Stream Speed Analysis In the previous experiments, we normalized the stream speed to be the same as that of fθ’s forward pass. That is, we assumed that the rate r at which S reveals new batches is equal to R (fθ(x)). However, some applications may enjoy a slower stream, giving TTA methods more time to adapt to samples. To explore this scenario, we vary the speed at which the stream reveals new data. In particular, let the new stream rate be η rwith η ∈ (0, 1]. Hence, as η → 0, the stream slows down and allows methods to adapt to all samples. Conversely, as η → 1, the stream speeds up, and at η = 1 we recover our realistic evaluation protocol. We experiment with the stream speed by setting η ∈ {1/16, 1/8, 1/4, 1/2, 1}, and evaluate five representative TTA methods (SHOT, TENT, TTAC-NQ, SAR, and EATA) in the episodic setup . Figure 4 summarizes our results by reporting the average error rate across all corruptions. We next list our observations: (i) The performance of TTA methods varies widely.For 7Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 3: Episodic Error Rate on ImageNet-C with ViT. We report the error rate of three baselines (Source, Tent, SAR) on the 15 different corruptions on ImageNet-C when the backbone is ViT architecture pretrained on ImageNet. We observe that while generally better backbones yield smaller error rate, expensive methods perform worse under our realistic evaluation. The more expensive the method is (e.g. SAR compared to Tent), the more performance reduction it suffers. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 90.5 93.3 91.8 71.0 76.6 66.1 72.9 84.1 73.5 52.8 45.3 55.9 69.5 55.5 52.2 70.1 - ✗ 69.9 95.9 68.9 55.8 62.0 52.3 57.9 57.2 53.6 41.8 28.9 40.7 59.1 39.7 42.0 55.0Tent ✓ 80.7 88.9 81.0 63.0 69.5 58.3 64.9 65.8 59.7 47.7 33.2 47.3 64.6 45.1 46.4 61.1 (-6.1) ✗ 55.5 56.9 55.1 47.5 50.4 44.3 48.7 42.4 47.3 33.6 25.4 35.6 44.8 33.5 36.4 43.8SAR ✓ 70.0 72.5 69.4 56.6 63.4 54.0 60.0 56.4 53.5 43.0 30.5 43.3 58.7 41.5 43.8 54.5 (-10.7) example, TTAC-NQ starts degrading faster (at η = 1/16) due to its slow adaptation speed. For other methods, the η at which they degrade varies. For instance, while TENT has a higher error rate than SAR in slow streams (η ≤ 1/8), TENT outperforms SAR in the regime of faster streams η ≤ 1/4. Interestingly, SHOT (Liang et al., 2020) ranks the worst at η ≤ 1/8, then ranks second when η ≥ 1/2, becoming a viable alternative. At last, the order of different methods significantly changes depending on the speed of the stream. For example, SAR changes from being second best at η ≤ 1/8 to third at η = 1/4 and then to fifth ( i.e. second worst) at η ≥ 1/2. (ii) EATA provides a good trade-off between speed and performance. In fact, EATA gives the best overall perfor- mance (lowest error rate) independent of the stream’s speed. This virtue is attributable to EATA’s combination of good performance and adaptation speed based on efficient sample rejection. Results on other datasets are in the Appendix. 4.4. Results on Other Benchmarks and Architectures We extend our evaluation protocol to cover ImageNet- 3DCC (Kar et al., 2022) and ImageNet-R (Hendrycks et al., 2021) datasets and ResNet-18 (results in the ap- pendix) and ViT (Kolesnikov et al., 2021) architectures. ImageNet-R contains rendition versions of ImageNet span- ning 200 classes. ImageNet-3DCC constructs more spatially-consistent corruptions than ImageNet-C by lever- aging depth estimates. For ViT, we conduct episodic evalu- ation on ImageNet-C in a similar setup to Section 4.1 and report the results in Table 3 for the non-adapted model, Tent, and SAR. For ImageNet-R and ImageNet-3DCC, we fix the architecture to ResNet-50 and experiment on the entire datasets and set the severity level to 5 in ImageNet-3DCC. Due to the space constraint, we limit our experiments to the episodic evaluation, and leave other results and analyses to the Appendix. We evaluate the effectiveness of 10 TTA methods in Table 4, where we report the average error rate across all corruptions. We observe that our results are consistent across all con- Table 4: Average Error Rate on ImageNet-R and ImageNet-3DCC. We report the average error rate of dif- ferent TTA methods on ImageNet-R and ImageNet-3DCC under both the realistic and current setups. A lower error rate indicates a better TTA method. The highlighted num- bers indicate a better performance per method across setups. We observe that methods generally perform worse in the more realistic realistic setup. The conclusions are consistent with what we observed on ImageNet-C (Table 2). Method ImageNet-R ImageNet-3DCC Current Realistic ∆ Current Realistic ∆ Source 63.8 63.8 - 73.9 73.9 - AdaBN 60.6 60.6 0 72.1 72.1 0 BN 60.0 60.0 0 70.5 70.5 0 LAME 60.5 60.5 0 72.1 72.1 0 SHOT 70.3 62.6 (+7.7) 69.2 67.0 (+2.2) TENT 58.1 59.1 (-1.0) 64.5 66.8 (-2.3) SAR 57.5 59.6 (-2.1) 63.5 71.4 (-7.9) CoTTA 57.3 61.5 (-4.5) 66.4 75.6 (-9.2) EATA 55.7 57.1 (-1.4) 60.9 63.1 (-2.2) TTAC-NQ 59.2 60.8 (-1.6) 65.7 73.6 (-7.9) sidered datasets and architectures. Similar to our results in Table 2, the more computationally involved SAR de- grades more than Tent when leveraging ViT architecture. Regarding other datasets, we find that simple methods that adapt during the forward pass are unaffected by the realis- tic setup. All the other methods, except SHOT, experience degradation in their results on both datasets. We observe again that, on these two datasets, while SHOT actually ben- efits from the realistic evaluation, EATA remains the best alternative on both ImageNet-R and ImageNet-3DCC. 4.5. Evaluation under Practical TTA Recently, (Yuan et al., 2023) extended the continual test- time adaptation evaluation to include label-imbalances; known as Practical Test-Time Adaptation (PTTA) setup. In this setting, the stream not only reveals a continual se- quence of distribution shifts, but also the revealed batches 8Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 5: Episodic Error Rate on CIFAR10-C under Practical Evaluation (Yuan et al., 2023).We report the error rate of two baselines (Source, RoTTA (Yuan et al., 2023)) on the 15 different corruptions on CIFAR10-C when the backbone is ResNet-18. We observe that under our computational constrained evaluation, the only method tailored to this setting; RoTTA, performs worse than the non-adapted baseline. Noise Blur Weather DigitalMethodRealisticgauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ Source ✓ 72.3 65.7 72.9 46.9 54.3 34.8 42.0 25.1 41.3 26.0 9.3 46.7 26.6 58.5 30.3 43.5 - ✗ 36.9 34.9 45.8 16.6 44.2 19.9 16.53 21.6 22.4 18.8 9.8 20.6 28.4 27.1 34.5 26.5RoTTA ✓ 55.0 54.4 63.2 43.3 62.3 43.7 43.5 44.8 47.7 43.4 35.3 41.8 54.0 47.7 54.6 49.0 (-22.5) have significant label imbalances. To combat this combined challenge, the work of (Yuan et al., 2023) proposed to lever- age a balanced memory bank for adaptation. In this section, we extend our computational constrained evaluation to the PTTA setup and compare RoTTA (Yuan et al., 2023) with a non-adapted model on CIFAR10-C benchmark. Table 5 summarizes the results. We observe that while RoTTA indeed reduces the error rate under the PTTA setup on CIFAR10-C (17% below the non-adapted model), our realistic evaluation uncovers its computational limitation. We found that RoTTA’s error rate increases by over 22% surpassing the error rate of the non-adapted model. Note that RoTTA stores samples from the stream in a memory bank then adapts the model on sampled samples from the memory bank. Thus, the slower the adaptation of RoTTA, the less diverse the samples in the memory bank, hindering its adaptation. 4.6. Effect of Hyper-parameter Tuning The performance of different TTA methods heavily depends on their hyper-parameter settings (Zhao et al., 2023). Here, we assess the impact of our proposed evaluation on TTA methods when tuning their hyperparameters. For that regard, we conduct hyper parameter search for Tent (as a fundamen- tal baseline) and experiment with different learning rates (the only hyper-parameter for Tent). Table 6 summarizes the results under episodic evaluation for 4 different corruptions on ImageNet-C. We observe that while conducting hyper-parameter search indeed improves the performance of TENT, its error rate increases under our realistic evaluation across all hyperparameters. That is, while conducting hyper-parameter search might indeed result in a better performance for TTA methods, the insights obtained through our proposed evaluation scheme remains consistent: more efficient TTA methods will have a smaller performance drop under the realistic evaluation. 5. Conclusions In this work, we find that the performance of Test Time Adaptation (TTA) methods can vary depending on the con- Table 6: Effect of our evaluation under hyperparameter tuning. We report the error rate for Tent under different learning rates under both the current and our proposed real- istic evaluation. While carefully tuning the learning rate for Tent results in a better performance, our realistic evaluation causes a performance drop under all learning rates. lr Realisticgauss. motion fog pixel. Avg. ∆ ✗ 74.1 63.3 44.7 43.5 56.41×10−4 ✓ 79.7 69.0 47.8 46.8 60.8 (-4.4) ✗ 71.1 59.7 43.1 41.9 53.92×10−4 ✓ 77.6 66.1 46.0 45.0 58.7 (-4.7) ✗ 69.6 58.1 42.4 41.1 52.83×10−4 ✓ 74.9 64.0 45.0 44.0 57.0 (-4.2) ✗ 68.8 57.1 42.0 40.8 52.24×10−4 ✓ 73.7 62.3 44.5 43.2 55.9 (-3.7) text in which they are used. In the episodic evaluation, the efficiency of the method is the most important factor, with more efficient methods like AdaBN and BN showing consistent performance, while data-dependent approaches suffer. Sample rejection methods generally perform well, and fine-tuning approaches such as SHOT can even improve when adapting to fewer samples. In the continual evalua- tion, methods that do not perform sample rejection scale poorly in the offline-continual setup but benefit from skip- ping adaptation on some batches in the realistic-continual setup. Furthermore, our stream speed analysis shows that the performance of TTA methods can vary widely at differ- ent speeds. Our findings are consistent across corruptions and multiple datasets. They can help researchers and practi- tioners to better understand the strengths and weaknesses of different TTA methods, and to choose the most appropriate method for their specific use case. Acknowledgements This work was partially done during a research internship of the first author at Intel Labs. This work was supported by the King Abdullah University of Science and Technol- ogy (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2021-4648. We would like to thank Yasir Ghunaim and Mattia Soldan for the helpful discussion. 9Evaluation of Test-Time Adaptation Under Computational Time Constraints Impact Statement Our work advances Machine Learning by proposing a re- alistic evaluation protocol for Test Time Adaptation meth- ods, prioritizing computational efficiency. This approach promotes the development of AI systems that are both ac- cessible in resource-limited settings and environmentally sustainable, by favoring simpler, faster methods. Such ad- vancements contribute to more inclusive and responsible AI deployment, aligning with ethical goals of broadening access and reducing environmental impacts References Alfarra, M., P´erez, J. C., Thabet, A., Bibi, A., Torr, P. H., and Ghanem, B. Combating adversaries with anti-adversaries. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 36, pp. 5992–6000, 2022. Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensem- bling in deep learning. arXiv preprint arXiv:2002.06470, 2020. Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L. Parameter-free online test-time adaptation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8344–8353, 2022. Cai, Z., Sener, O., and Koltun, V . Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pp. 8281–8290, 2021. Chen, D., Wang, D., Darrell, T., and Ebrahimi, S. Con- trastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 295–305, 2022. Croce, F., Gowal, S., Brunner, T., Shelhamer, E., Hein, M., and Cemgil, T. Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421–4435. PMLR, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and Wang, D. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Gong, T., Jeong, J., Kim, T., Kim, Y ., Shin, J., and Lee, S.-J. Note: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain- ing and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and pertur- bations. Proceedings of the International Conference on Learning Representations, 2019. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba- bilistic models. Advances in Neural Information Process- ing Systems, 33:6840–6851, 2020. Iwasawa, Y . and Matsuo, Y . Test-time classifier adjustment module for model-agnostic domain generalization. Ad- vances in Neural Information Processing Systems , 34: 2427–2440, 2021. Kar, O. F., Yeo, T., Atanov, A., and Zamir, A. 3d common corruptions and data augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18963–18974, 2022. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des- jardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017. Kojima, T., Matsuo, Y ., and Iwasawa, Y . Robustifying vision transformer without retraining from scratch by test- time class-conditional feature alignment. arXiv preprint arXiv:2206.13951, 2022. Kolesnikov, A., Dosovitskiy, A., Weissenborn, D., Heigold, G., Uszkoreit, J., Beyer, L., Minderer, M., Dehghani, M., Houlsby, N., Gelly, S., Unterthiner, T., and Zhai, X. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 10Evaluation of Test-Time Adaptation Under Computational Time Constraints Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896, 2013. Li, Y ., Wang, N., Shi, J., Liu, J., and Hou, X. Revisit- ing batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016. Liang, J., Hu, D., and Feng, J. Do we really need to access the source data? source hypothesis transfer for unsuper- vised domain adaptation. In International Conference on Machine Learning, pp. 6028–6039. PMLR, 2020. Liu, Y ., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan, T., and Alahi, A. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:21808–21820, 2021. Mancini, M., Karaoguz, H., Ricci, E., Jensfelt, P., and Ca- puto, B. Kitting in the wild through online domain adap- tation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1103–1109. IEEE, 2018. Mirza, M. J., Micorek, J., Possegger, H., and Bischof, H. The norm must go on: dynamic unsupervised do- main adaptation by normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14765–14775, 2022a. Mirza, M. J., Soneira, P. J., Lin, W., Kozinski, M., Possegger, H., and Bischof, H. Actmad: Activation matching to align distributions for test-time-training, 2022b. URL https://arxiv.org/abs/2211.12870. Niu, S., Wu, J., Zhang, Y ., Chen, Y ., Zheng, S., Zhao, P., and Tan, M. Efficient test-time model adaptation with- out forgetting. In International conference on machine learning, pp. 16888–16905. PMLR, 2022. Niu14, S., Wu, J., Zhang, Y ., Wen, Z., Chen, Y ., Zhao, P., and Tan15, M. Towards stable test-time adaptation in dynamic wild world. International Conference on Learning Representations, 2023. P´erez, J. C., Alfarra, M., Jeanneret, G., Rueda, L., Thabet, A., Ghanem, B., and Arbel´aez, P. Enhancing adversarial robustness via test-time transformation ensembling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81–91, 2021. Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new domains. In Computer Vision–ECCV 2010: 11th European Conference on Com- puter Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11 , pp. 213–226. Springer, 2010. Sakaridis, C., Dai, D., and Van Gool, L. Acdc: The ad- verse conditions dataset with correspondences for seman- tic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10765–10775, 2021. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M. Improving robustness against com- mon corruptions by covariate shift adaptation. Advances in Neural Information Processing Systems, 2020. Shalev-Shwartz, S. et al. Online learning and online con- vex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. Su, Y ., Xu, X., and Jia, K. Revisiting realistic test-time training: Sequential inference and adaptation by anchored clustering. arXiv preprint arXiv:2206.02721, 2022. Sun, Y ., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generaliza- tion under distribution shifts. In International conference on machine learning, pp. 9229–9248. PMLR, 2020. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adver- sarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7167–7176, 2017. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimiza- tion. arXiv preprint arXiv:2006.10726, 2020. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual test- time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7201–7211, 2022. Yuan, L., Xie, B., and Li, S. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15922–15932, 2023. Zhang, M., Levine, S., and Finn, C. Memo: Test time ro- bustness via adaptation and augmentation. arXiv preprint arXiv:2110.09506, 2021. Zhao, H., Liu, Y ., Alahi, A., and Lin, T. On pitfalls of test- time adaptation. International Conference on MAchine Learning, 2023. 11Evaluation of Test-Time Adaptation Under Computational Time Constraints A. Methodology A.1. Online Computation of C(g) Section 3.2 discussed the online evaluation protocol of TTA methods. Here, we give more details on the calcu- lation of C(g), the relative adaptation speed of g, during our online evaluation. First, we set R (g(x)) as the time recording function for g to perform a forward pass for a single batch. To ensure a reliable time calculation, we exe- cute torch.cuda.synchronize() before starting the timer and before ending it. This ensures all GPU operations are finished for the moment time is computed. To alleviate hardware dependence, we also calculate R(fθ(x)) for each evaluation step computing the relative adaptation complex- ity. It is worth mentioning that C(g) for SHOT, EATA, SAR, and COTTA are[3, 3, 8, 103] on average, respectively. B. Experiments B.1. Episodic Evaluation of TTA SHOT, PL, and ETA For completeness, we report the results on 3 baselines: Pseudo Label (Lee et al., 2013), SHOT-IM (Liang et al., 2020), and ETA (Niu et al., 2022) in Table 7. We follow the same setup as in the main paper. Our results are consistent with the findings of Section 4.1 and Table 2. In particular, SHOT-IM improves its perfor- mance under the online evaluation, similar to SHOT. Further, the performance of ETA and PL degrades under the online evaluation due to the additional computational burden. Nev- ertheless, ETA is similar to EATA in providing the best tradeoff between additional computational requirements and performance improvements. SAR with GN We equip our results to include ResNet50 with Group Normalization (GN) layers, following (Niu14 Figure 5: C(g) computation across iterations. We report our online calculations for the relative adaptation speed ofg, C(g), for SAR, SHOT, EATA, and TENT throughout a full evaluation episode. We observe that, overall, C(g) has a stable behavior throughout evaluation iterations. et al., 2023). We report the results in Table 7, where we observe that: (i) Under a relatively large batch size (64), ResNet50 with GN underperforms ResNet50 with Batch Normalization. In fact, the average error rate for SAR in- creases from 56.2% to 65.8%. (ii) The online evaluation penalizes SAR in both architecture choices with a perfor- mance degradation of 3.6% under the GN-based ResNet. Finally, it is worth mentioning that SAR with GN layers attains a similar performance under a batch size of 1. Ablating Batch Sizes In the experiments section, we fixed the batch size to 64 following the recommendations of ear- lier works (Wang et al., 2020; Niu et al., 2022). Here, we investigate the effect of our proposed online evaluation un- der different choices of batch sizes. To that end, we vary the batch size in {1, 16, 32, 128}, and report the results in Figure 6. We draw the following observations: Table 7: Episodic Error Rate on ImageNet-C. We report the error rate of different TTA methods on the ImageNet-C benchmark under both the online and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup is merely the reproduction of every method. We show that methods generally perform worse in the more realistic online setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse its performance. SAR-GN represents SAR when deployed on ResNet50 with Group Normalization (GN) layers, following (Niu14 et al., 2023). Noise Blur Weather DigitalMethod Online gauss. shot impul. defoc. glass motionzoom snow frost fog brigh. contr. elast. pixel. jpeg Avg. ∆ ✗ 73.1 69.8 72.0 76.9 75.9 58.5 52.7 53.3 62.2 43.8 34.6 82.6 46.0 42.3 48.9 59.5SHOT-IM ✓ 71.1 68.6 70.7 73.2 73.6 59.1 51.9 52.8 60.5 43.7 33.6 77.3 45.7 42.1 48.6 58.2 (-0.3) ✗ 92.2 92.2 92.8 97.0 89.8 57.7 49.6 50.7 57.1 41.5 32.6 91.1 44.3 40.3 46.6 65.0PL ✓ 90.6 86.3 83.6 93.2 89.7 63.0 51.7 55.0 59.3 43.8 32.9 92.3 47.3 42.4 49.3 65.3 (+0.3) ✗ 64.9 62.7 63.6 66.4 66.3 52.4 47.3 48.2 54.1 40.2 32.2 54.8 42.3 39.2 44.7 52.0ETA ✓ 70.2 67.0 69.6 71.5 71.5 56.9 50.2 51.9 57.0 42.0 32.5 60.5 44.6 40.8 47.1 55.6 (+3.6) ✗ 71.8 69.0 70.3 81.5 81.0 69.6 69.5 57.1 56.6 94.3 29.2 56.0 84.8 51.4 44.7 65.8SAR-GN ✓ 82.0 80.2 82.1 80.2 88.6 78.5 75.1 59.6 53.9 66.9 30.7 63.3 81.3 71.3 47.5 69.4 (+3.6) 12Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) ADABN OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  BN-ADAPTATION OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 COTTA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) EATA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ETA OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100  LAME OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) PL OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SAR OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 SHOT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) SHOTIM OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TENT OFFLINE ONLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 TTAC-NQ OFFLINE ONLINE Figure 6: Batch Size Analysis current vs. realistic setups for every method. We assess the performance variation of 12 different TTA methods under varying batch sizes. We experiment with batch sizes in{1, 16, 32, 128}. We do not include the baseline, MEMO, and DDA, since they are data-dependent approaches and are unaffected by batch size. All TTA methods, except LAME, are severely affected by smaller batch sizes. Nonetheless, the realistic evaluation degrades the performance of all methods, except SHOT and SHOT-IM. (i) Online evaluation improves the performance of SHOT and SHOT-IM. This result is consistent with the earlier observations in Table 2. Note that PL shares a similar trend as well. (ii) The performance of TTA methods degrades when switching from offline to online evaluation, regardless of the batch size. This result is highlighted in COTTA, ETA, EATA, SAR, TENT, and TTAC-NQ. (iii) Performance of TTA methods vastly varies when varying the batch size. This result is consistent with earlier findings in the literature (Gao et al., 2022; Niu14 et al., 2023), where most TTA methods fail with small batch sizes. At last, and to ease comparison across methods, we summa- rize all the plots for all methods in Figure 7. Consistency with 3 random seeds. For all of our exper- iments, we run each experiment with 3 random seeds. In most of our results, we found out that the standard deviation of performance across runs is very small. Our results in Figures 3 and 4 demonstrate this variation in the shaded area for 5 different TTA methods. B.2. Continual Evaluation of TTA We further explore another setup for the continual evalua- tion of TTA. In particular, we follow (Wang et al., 2022) in concatenating all corruptions in ImageNet-C with 11 differ- ent orders. We then report the average performance of each method across all runs and corruptions in Table 8. We run each experiment with 3 random seeds, and report our results with standard deviations. For the remaining implementation 13Evaluation of Test-Time Adaptation Under Computational Time Constraints 1 16 32 128 Batch Size 50 60 70 80 90 100Avg. Error Rate (%) OFFLINE 1 16 32 128 Batch Size 50 60 70 80 90 100 ONLINE ADABN BN-ADAPTATION COTTA EATA ETA LAME PL SAR SHOT SHOTIM TENT TTAC-NQ Figure 7: Summary of batch size analysis: current vs. realistic setups. Left: Current evaluation, i.e.,Section 3.1. Right: Realistic evaluation,i.e.,Section 3.2. While EATA achieves the lowest error rate under batch sizes≥ 32, SHOT becomes a very competitive baseline, outperforming EATA, at a batch size of 128. Table 8: Continual Error Rate on ImageNet-C. We report the average continual error rate for 11 different corruption orders, with 3 different seeds, under both the offline and online setups with a corruption severity level of 5. Continual refers to continually adapting after each corruption without resetting. This metric indicates the model’s capability to learn from previous corruptions. The offline setup refers to the performance of the model in a continual learning scheme, whereas the online setup refers to the performance of the model in a continual learning scheme, under our more realistic online setup. We show that the more complex a method is, the fewer samples it adapts to, achieving better performance in a continual learning scheme. Avg. Error (%) COTTA ETA TENT SAR EATA SHOT TTAC-NQ Offline 65.3 ± 5.9 56 .4 ± 2.3 84 .6 ± 16.0 59 .8 ± 3.0 56 .4 ± 2.3 88 .4 ± 11.4 81 .8 ± 11.4 Online 69.3 ± 2.8 57 .7 ± 2.0 65 .6 ± 5.0 60 .4 ± 1.8 57 .7 ± 1.9 78 .2 ± 7.7 65 .1 ± 3.8 details, we follow our setup in main paper. We observe that, similar to our conclusions in Section 4.2, online eval- uation helps methods that do not perform sample rejection (e.g.,TENT). Nonetheless, both ETA and EATA provide the best trade-off between performance and additional compu- tational burden. B.3. Stream Speed Analysis For completeness, we extend our stream speed analysis in Section 4.3 to cover the ImageNet-3DCC dataset. We preserve our experimental setup by varying the stream speed according to ηr, with η ∈ {1/16, 1/8, 1/4, 1/2, 1. Figure 8 summarizes our results for SHOT, TENT, TTAC-NQ, EATA, and SAR. We observe similar trends to the ones in Figure 4, where the performance of different TTA methods varies widely under different stream speeds. The large relative adaptation speed of TTAC-NQ degrades its performance under even slow streams (e.g.,η = 1/8), while SHOT reduces its error rate under faster streams. Furthermore, EATA is consistently outperforming all other considered approaches under different stream speeds. B.4. Evaluation on Other Benchmarks We report the error rates on all corruptions of ImageNet- 3DCC (Kar et al., 2022), along with the overall average error rate, in Table 9. The conclusions we draw for ImageNet- 3DCC (Kar et al., 2022) are very similar to the ones ob- served on ImageNet-C (Hendrycks & Dietterich, 2019) (in Section 4.1). We observe that efficient methods, with C(g) = 1, such as LAME and BN, maintain performance. Furthermore, the performance of some TTA methods (Wang et al., 2020; Niu14 et al., 2023; Niu et al., 2022; Wang et al., 2022) degrades in the online setup, while others that use pseudo labeling (Lee et al., 2013; Liang et al., 2020) actually improve. This degradation seems to be directly proportional to the amount of data a method misses according to its C(g). 14Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 9: Episodic Error Rate on ImageNet-3DCommonCorruptions. We report the error rate of different TTA methods on ImageNet-3DCC (Kar et al., 2022) benchmark under both the realistic and offline setups. A lower error rate indicates a better TTA method. The highlighted numbers indicate a better performance per method across setups. Episodic means the model will adapt to one corruption at a time. The model is reset back to the base model when moving to the next corruption. The offline setup corresponds to reproducing the reported performance of every method. The first sub-table corresponds to methods that incur none or few additional computations, i.e.,C(g) = 1. We show that methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the fewer data it will adapt to, and the worse its performance. Depth of field Noise LightingWeather Video Camera motionMethod RealisticNear focus Far focusColor quant. ISO noise Low lightFlash Fog 3DBit error H.265 ABR H.265 CRFXY-mot. blur Z-mot. blurAvg. ∆ Source ✓ 46.9 55.6 82.5 94.0 71.7 78.7 75.3 88.6 70.6 65.4 82.0 75.3 73.9 -AdaBN ✓ 45.2 55.0 71.8 76.8 64.1 80.8 75.0 91.8 80.9 76.7 79.1 67.5 72.1 -LAME ✓ 45.3 55.0 71.9 76.9 64.1 80.8 75.1 91.8 80.9 76.8 79.2 67.6 72.1 -BN ✓ 43.9 54.3 72.3 76.6 60.9 80.1 72.4 90.9 78.7 73.8 76.9 65.6 70.5 - PL ✗ 39.8 49.8 65.5 72.6 48.9 79.0 66.1 97.5 92.1 86.2 88.7 57.6 70.3(-1.6)✓ 41.0 51.3 66.5 71.5 52.8 77.4 68.1 95.6 86.0 78.7 77.0 59.2 68.7 SHOT ✗ 43.0 53.6 67.1 64.2 51.9 81.1 73.2 97.2 83.5 77.8 77.3 60.1 69.2(-2.2)✓ 41.7 51.4 64.4 63.8 51.6 77.5 71.6 95.1 79.9 74.6 73.7 58.5 67.0 SHOT-IM✗ 42.2 52.7 66.6 63.7 51.0 81.0 72.1 97.0 83.3 77.6 75.6 59.2 68.5(-1.9)✓ 41.2 51.2 64.4 63.3 51.3 77.5 70.9 94.9 79.4 74.1 72.3 58.3 66.6 TENT ✗ 39.9 49.6 62.4 62.2 50.7 75.6 68.5 91.6 75.7 70.2 70.4 57.0 64.5(+2.3)✓ 41.7 51.4 65.5 67.2 54.7 77.4 70.1 90.7 76.8 71.9 74.0 60.8 66.8 SAR ✗ 40.3 50.0 62.0 61.2 50.6 73.8 65.8 90.1 73.9 68.8 69.1 56.8 63.5(+6.9)✓ 44.9 54.7 71.1 75.4 62.6 80.3 73.8 91.7 80.5 76.1 78.6 66.9 71.4 ETA ✗ 38.7 47.9 59.1 56.7 46.8 71.0 62.1 90.6 72.8 67.3 64.7 52.9 60.9(+2.3)✓ 39.7 49.3 61.6 60.7 50.0 73.5 65.2 90.3 74.4 69.1 68.8 55.9 63.2 CoTTA ✗ 40.8 50.9 66.3 68.3 54.6 77.2 68.0 90.2 76.4 71.1 73.1 60.4 66.4(+9.2)✓ 55.4 63.1 74.1 77.0 64.7 83.4 78.1 93.7 84.0 80.3 81.7 71.9 75.6 TTAC-NQ✗ 40.7 50.5 61.0 61.1 51.5 72.8 66.6 93.8 81.1 74.7 75.7 59.1 65.7(+7.9)✓ 49.9 57.0 69.3 72.3 58.9 79.8 76.3 95.8 86.5 83.0 84.6 69.8 73.6 EATA ✗ 38.6 47.8 59.2 56.6 46.9 71.2 62.2 90.9 72.5 67.4 64.6 52.9 60.9(+2.2)✓ 39.8 49.3 61.6 60.5 49.9 73.5 64.8 90.6 73.7 69.1 68.6 55.7 63.1 C. Single Model Evaluation Scheme In Section 3.2, we assume fθt can generate predictions whenever g is occupied with adapting to a batch. This setup assumes the capacity to concurrently deploy two models. However, this assumption might be unfair to methods with C(g) = 1, since it allows expensive methods to skip batches without large penalties. We thus also study the case where only one model can be deployed. Studying this setup requires establishing a policy on how samples missed by the TTA method g are treated. That is, when g is busy adapting, all skipped samples still must be predicted without access to fθt . Depending on the applica- tion, this prediction could leverage prior knowledge about the problem e.g. temporal correlation across samples, or the bias of the distribution. In our setup, we consider the most strict scenario in which, whenever g is busy, a ran- dom classifier generates predictions for the incoming sam- ples. This naive design choice results from our evaluation on ImageNet-based datasets, which contain images whose classes display no bias nor temporal correlation. We conduct episodic evaluation, similar to Section 4.1, on ImageNet-C dataset. We average the error rates per corruption category (e.g. averaging error rates for gaussian, shot, and impulse noises) and present the results of this study in Table 10. We draw the following observation. Single model evaluation strongly favors methods with C(g) = 1. We observe that all models that are slower than the stream are heavily penalized to the point that using the original pre-trained model becomes a better alternative. However, methods that can be as fast as the stream, like AdaBN or BN, become the best alternative due to their speed. This result encourages more research toward devel- oping efficient TTA methods that have negligible additional computational overhead. D. Results on ResNet18 In our experiments in the main paper, we focused on the stan- dard ResNet18-architecture, following the common practice in the literature. Here, and for completeness, we extend our results to cover the smaller and more efficient ResNet18 architecture. Teble 11 summarizes the episodic evaluation of 6 TTA methods on ImageNet-C dataset. Similar to our conclusions in the episodic evaluation section in the main paper, more expensive adaptation methods degrade more under our realistic evaluation scheme. 15Evaluation of Test-Time Adaptation Under Computational Time Constraints Table 10: Per Corruption Category Average Error Rate Using Single Model Evaluation on ImageNet-C. We re- port the average error rate per corruption category of dif- ferent TTA methods under single model realistic evaluation mode on ImageNet-C. Single model mode assumes the de- ployment of a single modelg instead of two under a constant speed stream S. We assume the most extreme scenario, that is if a model g is occupied adapting to a batch, the incoming batch is fed to a random classifier. We observe that the best TTA methods to use in this scenario are AdaBN (Li et al., 2016) and BN (Schneider et al., 2020), which simply adapt the BN statistics. Method Realistic Noise Blur Weather Digital Avg. Source ✓ 97.7 83.8 69.1 81.4 82.0 AdaBN ✓ 84.5 76.1 54.9 62.7 68.5 BN ✓ 84.1 73.1 54.2 59.9 66.7 SHOT ✓ 92.6 91.3 87.0 88.5 89.7 TENT ✓ 91.9 89.4 83.0 85.0 87.0 SAR ✓ 95.6 94.0 90.1 91.3 92.6 EATA ✓ 89.4 87.6 82.0 83.2 85.3 TTAC-NQ ✓ 96.6 96.9 96.3 96.4 96.5 Table 11: Evaluating different TTA methods with ResNet- 18 architecture on ImageNet-C. We report the average error rate across all different types of corruptions (lower is bet- ter). TTA methods generally perform worse in the more realistic setup. The more computationally complex the TTA method is, the less data it will adapt to, and the worse is its performance. Method Basic BN SHOT Tent EATA SAR Current 85.4 70.1 64.4 64.9 59.7 63.8 Realistic 85.4 70.1 64.5 68.3 63.2 69.5 Diff - - 0.1 3.4 3.5 5.7 1/16 1/8 1/4 1/2 1 η 62 64 66 68 70 72 74Error Rate (%) SHOT TENT TTAC-NQ SAR EATA Figure 8: Average Error Rate on ImageNet-3DCC Under Slower Stream Speeds. We report the average error rate for several TTA methods on ImageNet-3DCC under slower stream speeds. In our proposed online model evaluation, the stream speed r is normalized by the time needed for a forward pass using the base model. We evaluate different TTA methods under a stream with speed ηr with η ∈ (0, 1]. An η = 1/16 means the stream is 16 times slower than the forward pass of the base model. We report the standard deviation across 3 random seeds. Different TTA methods degrade differently when varying η. 16",
      "meta_data": {
        "arxiv_id": "2304.04795v2",
        "authors": [
          "Motasem Alfarra",
          "Hani Itani",
          "Alejandro Pardo",
          "Shyma Alhuwaider",
          "Merey Ramazanova",
          "Juan C. Pérez",
          "Zhipeng Cai",
          "Matthias Müller",
          "Bernard Ghanem"
        ],
        "published_date": "2023-04-10T18:01:47Z",
        "pdf_url": "https://arxiv.org/pdf/2304.04795v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods that explicitly accounts for computational time constraints. It penalizes slower methods by providing them with fewer samples for adaptation, simulating a constant-speed data stream. The study demonstrates that when inference speed is considered, simpler and faster TTA approaches can often outperform more sophisticated but slower state-of-the-art methods. The research highlights the critical importance of developing TTA methods that are both accurate and computationally efficient for real-world applicability.",
        "methodology": "The proposed 'Realistic Online Evaluation Protocol' simulates an online data stream with a constant speed, unlike current offline protocols that assume the stream waits for adaptation. It introduces a 'relative adaptation speed' C(g) for each TTA method g, which is the integer ratio of the stream's speed to the method's adaptation speed. If C(g) = k, the method adapts to every k-th sample, while skipped samples are processed by the most recent adapted model (or the base model). C(g) is computed online for each input, allowing for variability. This intrinsically penalizes slower methods by reducing their opportunities for adaptation.",
        "experimental_setup": "The evaluation was conducted on image classification tasks using a ResNet-50-BN backbone (pretrained on ImageNet), with additional experiments on ViT and ResNet-18. A typical stream batch size of 64 was used, with ablations for {1, 16, 32, 128}. Datasets included ImageNet-C (with 15 corruptions at level 5), CIFAR10-C, ImageNet-R, and ImageNet-3DCC (severity level 5). Fifteen state-of-the-art TTA methods (e.g., AdaBN, SHOT, TENT, SAR, EATA, DDA, MEMO) published between 2017 and 2023 were benchmarked. Experiments covered episodic evaluation (single domain shift, model reset), continual evaluation (concatenated shifts, no reset), stream speed analysis (varying ηr), and practical TTA with label imbalances. Validation involved error rates, average error rates, and standard deviations across 3 random seeds, using official implementations and recommended hyperparameters.",
        "limitations": "The primary limitation identified is the high computational overhead of many existing TTA methods, which significantly increases adaptation time and renders them impractical in real-world scenarios with constant-speed data streams. The current offline evaluation protocols fail to capture this critical aspect, leading to an overestimation of the real-world performance of slower, more complex methods. Data-dependent approaches like MEMO and DDA were found to be extremely inefficient, performing close to non-adapted models under realistic conditions. Slower sample rejection methods, such as SAR, also experienced significant performance degradation. Additionally, some methods without sample rejection in continual adaptation can overfit to early distributions when not constrained by speed.",
        "future_research_directions": "The paper strongly encourages future research to focus on developing TTA methods that are both accurate and computationally efficient. A key direction is to increase the efficiency of data-dependent adaptation methods. The proposed evaluation scheme aims to inspire TTA methods to prioritize inference speed as a critical dimension for real-world performance. Further exploration into efficient TTA methods with negligible additional computational overhead is also suggested, especially in scenarios where only a single model can be deployed."
      }
    },
    {
      "title": "Persistent Test-time Adaptation in Recurring Testing Scenarios",
      "abstract": "Current test-time adaptation (TTA) approaches aim to adapt a machine learning\nmodel to environments that change continuously. Yet, it is unclear whether TTA\nmethods can maintain their adaptability over prolonged periods. To answer this\nquestion, we introduce a diagnostic setting - recurring TTA where environments\nnot only change but also recur over time, creating an extensive data stream.\nThis setting allows us to examine the error accumulation of TTA models, in the\nmost basic scenario, when they are regularly exposed to previous testing\nenvironments. Furthermore, we simulate a TTA process on a simple yet\nrepresentative $\\epsilon$-perturbed Gaussian Mixture Model Classifier, deriving\ntheoretical insights into the dataset- and algorithm-dependent factors\ncontributing to gradual performance degradation. Our investigation leads us to\npropose persistent TTA (PeTTA), which senses when the model is diverging\ntowards collapse and adjusts the adaptation strategy, striking a balance\nbetween the dual objectives of adaptation and model collapse prevention. The\nsupreme stability of PeTTA over existing approaches, in the face of lifelong\nTTA scenarios, has been demonstrated over comprehensive experiments on various\nbenchmarks. Our project page is available at https://hthieu166.github.io/petta.",
      "full_text": "Persistent Test-time Adaptation in Recurring Testing Scenarios Trung-Hieu Hoang1 Duc Minh Vo2 Minh N. Do1,3 1Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign 2The University of Tokyo 3VinUni-Illinois Smart Health Center, VinUniversity {hthieu, minhdo}@illinois.edu vmduc@nlab.ci.i.u-tokyo.ac.jp Abstract Current test-time adaptation (TTA) approaches aim to adapt a machine learn- ing model to environments that change continuously. Yet, it is unclear whether TTA methods can maintain their adaptability over prolonged periods. To answer this question, we introduce a diagnostic setting - recurring TTA where envi- ronments not only change but also recur over time, creating an extensive data stream. This setting allows us to examine the error accumulation of TTA models, in the most basic scenario, when they are regularly exposed to previous testing environments. Furthermore, we simulate a TTA process on a simple yet repre- sentative ϵ-perturbed Gaussian Mixture Model Classifier, deriving theoretical insights into the dataset- and algorithm-dependent factors contributing to gradual performance degradation. Our investigation leads us to propose persistent TTA (PeTTA), which senses when the model is diverging towards collapse and adjusts the adaptation strategy, striking a balance between the dual objectives of adaptation and model collapse prevention. The supreme stability of PeTTA over existing approaches, in the face of lifelong TTA scenarios, has been demonstrated over comprehensive experiments on various benchmarks. Our project page is available at https://hthieu166.github.io/petta. 1 Introduction Machine learning (ML) models have demonstrated significant achievements in various areas [18, 38, 47, 23]. Still, they are inherently susceptible to distribution-shift [46, 13, 48, 21, 6] (also known as the divergence between the training and testing environments), leading to a significant degradation in model performance. The ability to deviate from the conventional testing setting appears as a crucial aspect in boosting ML models’ adaptability when confronted with a new testing environment that has been investigated [ 30, 53, 14]. Among common domain generalization methods [ 58, 24, 1], test-time adaptation (TTA) takes the most challenging yet rewarding path that leverages unlabeled data available at test time for self-supervised adaptation prior to the final inference [57, 39, 8, 41, 59]. Early TTA studies have concentrated on a simply ideal adaptation scenario where the test samples come from a fixed single domain [57, 39, 41]. As a result, such an assumption is far from the ever- changing and complex testing environments. To confront continually changing environments [59, 12], Yuan et al. [61] proposed a practical TTA scenario where distribution changing and correlative sampling occur [15] simultaneously. Though practical TTA is more realistic than what the previous assumptions have made, it still assumes that any environment only appears once in the data stream, a condition which does not hold true. Taking a surveillance camera as an example, it might accom- modate varying lighting conditions recurringly day after day (Fig. 1-left). Based on this reality, we hypothesize that the recurring of those conditions may reveal the error accumulation phenomenon in TTA, resulting in performance degradation over a long period. To verify our hypothesis, we simulate a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2311.18193v4  [cs.CV]  2 Nov 2024Testing Error Time Day 1 Illumination Condition Day 2 Day 3 0 50 100 150 200 250 300 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 201 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Test-time adaptation step Testing Error No TTA RoTTA PeTTA (ours) Figure 1: Recurring Test-time Adaption (TTA). (left) Testing environments may change recurringly and preserving adaptability when visiting the same testing condition is not guaranteed. (right) The testing error of RoTTA [61] progressively raises (performance degradation) and exceeds the error of the source model (no TTA) while our PeTTA demonstrates its stability when adapting to the test set of CIFAR-10-C [19] 20 times. The bold lines denote the running mean and the shaded lines in the background represent the testing error on each domain (excluding the source model, for clarity). recurring testing environment and observe the increasing error rate by recurringly adapting to the test set of CIFAR-10-C [19] multiple times. We showcase the testing error of RoTTA [61] after 20 cycles of adaptation in Fig. 1-right. As expected, RoTTA can successfully adapt and deliver encouraging outcomes within the first few passes. However, this advantage is short-lived as our study uncovers a significant issue: TTA approaches in this setting may experience severe and persistent degradation in performance. Consequently, the testing error of RoTTA gradually escalates over time and quickly surpasses the model without adaptation. This result confirms the risk of TTA deployment in our illustrative scenario, as an algorithm might work well in the first place and gradually degenerate. Therefore, ensuring sustainable quality is crucial for real-world applications, especially given the recurring nature of testing environments. This study examines whether the adaptability of a TTA algorithm persists over an extended testing stream. Specifically, in the most basic scenario, where the model returns to a previously encountered testing environment after undergoing various adjustments. We thus propose a more general testing scenario than the practical TTA [61], namely recurring TTA, where the environments not only change gradually but also recur in a correlated manner over time. We first analyze a simulation using the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) on a synthesized dataset and derive a theoretical analysis to confirm our findings, offering insights to tackle similar issues in deep neural networks. The analysis provides hints for reasoning the success of many recent robust continual TTA approaches [61, 12, 59, 15] and leading us to propose a simple yet effective baseline to avoid performance degradation, namely Persistent TTA (PeTTA). PeTTA continuously monitors the chance of collapsing and adjusts the adaptation strategy on the fly, striking a balance between the two objectives: adaptation and collapse prevention. Our contributions can be summarized as follows: • First, this work proposes a testing scenario - recurring TTA, a simple yet sufficient setup for diagnosing the overlooked gradual performance degradation phenomenon of TTA. • Second, we formally define the phenomenon of TTA collapsing and undertake a theoretical analysis on an ϵ-GMMC, shedding light on dataset-dependent and algorithm-dependent factors that contribute to the error accumulation during TTA processes. • Third, we introduce persistent TTA (PeTTA)- a simple yet effective adaptation scheme that surpasses all baseline models and demonstrates a persisting performance. For more context on related work, readers are directed to visit our discussions in Appdx. A. 2 Background Test-time Adaptation (TTA). A TTA algorithm operates on an ML classifier ft : X → Ywith parameter θt ∈ Θ (parameter space) gradually changing over time (t ∈ T) that maps an input image x ∈ Xto a category (label) y ∈ Y. Let the capital letters (Xt, Yt) ∈ X × Ydenote a pair of random variables with the joint distribution Pt(x, y) ∈ Pd, t∈ T. Here, Pd belongs to collection of D sets of testing scenarios (domains) {Pd}D d=1. The covariate shift [46] is assumed: Pt(x) and Pt′(x) 2could be different but Pt(y|x) = Pt′(y|x) holds ∀t ̸= t′. At t = 0, θ0 is initialized by a supervised model trained on P0 ∈ P0 (source dataset). The model then explores an online stream of testing data. For each t >0, it receives Xt (typically in form of a batch of Nt testing samples) for adapting itself ft−1 → ft before making the final prediction ft (Xt). TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model ft are updated indirectly through a student model with parameters θ′ t [57, 61, 12, 15, 55]. At first, the teacher model in the previous step introduces a pseudo label [28] ˆYt for each Xt: ˆYt = ft−1(Xt). (1) With a classification loss LCLS (e.g., cross-entropy [16]), and a model parameters regularizer R, the student model is first updated with a generic optimization operatorOptim, followed by an exponential moving average (EMA) update of the teacher model parameter θt−1: θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011i + λR(θ′), (2) θt = (1 − α)θt−1 + αθ′ t, (3) with α ∈ (0, 1) - the update rate of EMA, andλ ∈ R+ - the weighting coefficient of the regularization term, are the two hyper-parameters. Practical TTA. In practical TTA [61], two characteristics of the aforementioned distribution of data stream are noticeable. Firstly, Pt’s can be partitioned by td’s in which {Pt}td t=td−1 ⊂ Pd. Here, each partition of consecutive steps follows the same underlying distribution which will change continually through D domains [59] (P1 → P2 ··· → PD). Secondly, the category distribution in each testing batch is temporally correlated [15]. This means within a batch, a small subset of categories is dominant over others, making the marginal distribution Pt(y) = 0, ∀y ̸∈ Yt ⊂ Yeven though the category distribution over all batches are balanced. Optimizing under this low intra-batch diversity (|Yt| ≪ |Y|) situation can slowly degenerate the model [7]. 3 Recurring TTA and Theoretical Analysis This section conducts a theoretical analysis on a concrete failure case of a simple TTA model. The results presented at the end of Sec. 3.2 will elucidate the factors contributing to the collapse (Sec. 3.1), explaining existing good practices (Sec. 3.3) and give insights into potential solutions (Sec. 4). 3.1 Recurring TTA and Model Collapse Recurring TTA.To study the gradual performance degradation (or model collapse), we propose anew testing scenario based on practical TTA [61]. Conducting a single pass through D distributions, as done in earlier studies [61, 59], may not effectively identify the degradation. To promote consistency, our recurring TTA performs revisiting the previous distributions K times to compare the incremental error versus the previous visits. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. Appdx. D extends our justifications on constructing recurring TTA. Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Here, upon collapsing, a model tends to ignore almost categories in ˜Y. As it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to θ0. 3.2 Simulation of Failure and Theoretical Analysis Collapsing behavior varies across datasets and the adaptation processes. Formally studying this phenomenon on a particular real dataset and a TTA algorithm is challenging. Therefore, we propose a theoretical analysis on ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) that shares the typical characteristics by construction and demonstrates the same collapsing pattern in action (Sec. 5.1) as observed on real continual TTA processes (Sec. 5.3). 3Pseudo-label Predictor ˆYt = argmax y∈Y Pr(Xt|y;θt−1) Xt Mean-teacher Update θ′ t = Optim θ′∈Θ EPt h LCLS \u0010ˆYt, Xt;θ′\u0011i θt = (1−α)θt−1 +αθ′ t ϵt ··· θt−1 θt ··· Figure 2: ϵ-perturbed binary Gaussian Mix- ture Model Classifier, imitating a continual TTA algorithm for theoretical analysis. Two main components include a pseudo-label predictor (Eq. 1), and a mean teacher up- date (Eqs. 2, 3). The predictor is perturbed for retaining a false negative rate of ϵt to simulate an undesirable TTA testing stream. Simulated Testing Stream. Observing a testing stream with (Xt, Yt) ∈ X × Y= R × {0, 1} and the underlying joint distribution Pt(x, y) = py,t · N(x; µy, σ2 y). The main task is predicting Xt was sampled from cluster 0 or 1 (negative or positive). Conveniently, let py,t ∆ = Pt(y) = Pr(Yt = y) and ˆpy,t ∆ = Pr( ˆYt = y) be the marginal distribution of the true label Yt and pseudo label ˆYt. GMMC and TTA. GMMC first implies an equal prior distribution by construction which is desirable for the actual TTA algorithms (e.g., category-balanced sampling strategies in [ 61, 15]). Thus, it simplifies ft into a maximum likelihood estimation ft(x) = argmaxy∈Y Pr(x|y; θt) with Pr(x|y; θt) = N(x; ˆµy,t, ˆσ2 y,t). The goal is estimating a set of parameters θt = {ˆµy,t, ˆσ2 y,t}y∈Y. A perfect classifier θ0 = {µy, σ2 y}y∈Y is initialized at t = 0. For the consecutive steps, the simplicity of GMMC allows solving the Optim (for finding θ′ t, Eq. 2) perfectly by computing the empirical mean and variance of new samples, approximating EPt. The mean teacher update (Eq. 3) for GMMC is: ˆµy,t = ( (1 − α)ˆµy,t−1 + αEPt h Xt|ˆYt i if ˆYt = y ˆµy,t−1 otherwise . (4) The update of ˆσ2 y,t is similar. ˆYt = ft−1(Xt) can be interpreted as a pseudo label (Eq. 1). ϵ-GMMC. Severe distribution shifts or low intra-batch category diversity of recurring TTA/practical TTA both result in an increase in the error rate of the predictor . Instead of directly modeling the dynamic changes of py,t (which can be complicated depending on the dataset), we study an ϵ−pertubed GMMC (ϵ−GMMC), where py,t is assumed to be static (defined below) and the pseudo- label predictor of this model is perturbed to simulate undesirable effects of the testing stream on the predictor. Two kinds of errors appear in a binary classifier [4]. Let ϵt = Pr{Yt = 1|ˆYt = 0} (5) be the false negative rate (FNR) of the model at step t. Without loss of generality, we study the increasing type II collapse of ϵ-GMMC. By intentionally flipping the true positive pseudo labels in simulation, an FNR of ϵt is maintained (Fig. 2). Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Lemma 1 states the negative correlation between ˆp1,t and ϵt. Unsurprisingly, towards the collapsing point where all predictions are zeros, the FNR also increases at every step and eventually reaches the highest possible FNR of p1. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Lemma 2 states the resulting ϵ−GMMC after collapsing. Cluster 0 now covers the whole data distribution (and assigning label 0 for all samples). Furthermore, collapsing happens when ˆµ0,t moves toward µ1. We next investigate the factors and conditions for this undesirable convergence. 4Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . From Thm. 1, we observe that the distance d0→1 t ’s converges (also indicating the convergence to the distribution in Lemma 2) if d0→1 t < d0→1 t−1 . The model collapse happens when this condition holds for a sufficiently long period. Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Corollary 1 introduces a condition ϵ-GMMC collapse. Here, ϵt’s are non-decreasing, lim t→τ ϵt = p1. Remarks. Thm. 1 concludes two sets of factors contributing to collapse: (i) data-dependent factors: the prior data distribution (p0), the nature difference between two categories (|µ0 − µ1|); and (ii) algorithm-dependent factors: the update rate (α), the FNR at each step (ϵt). ϵ-GMMC analysis sheds light on explaining model collapse on real datasets (Sec. 5.3), reasons the existing approaches (Sec. 3.3) and motivates the development of our baseline (Sec. 4). 3.3 Connection to Existing Solutions Prior TTA algorithms have already incorporated implicit mechanisms to mitigate model collapse. The theoretical results in the previous section explain the rationale behind these effective strategies. Regularization Term for θt. Knowing that f0 is always well-behaved, an attempt is restricting the divergence of θt from θ0, e.g. using R(θt) ∆ = ∥θ0 − θt∥2 2 regularization [40]. The key idea is introducing a penalty term to avoid an extreme divergence as happening in Thm. 1. Memory Bank for Harmonizing Pt(x). Upon receiving Xt, samples in this batch are selectively updated to a memory bank M (which already contains a subset of some instances ofXt′, t′ < tin the previous steps). By keeping a balanced number of samples from each category, distribution PM t (y) of samples in M is expected to have less zero entries than Pt(y), making the optimization step over PM t more desirable. From Thm. 1, M moderates the extreme value of the category distribution (p0 term) which typically appears on batches with low intra-batch category diversity. 4 Persistent Test-time Adaptation (PeTTA) Now we introduce our Persistent TTA (PeTTA) approach. Further inspecting Thm. 1, while ϵt (Eq. 5) is not computable without knowing the true labels, the measure of divergence from the initial distribution (analogously to d0→1 t−1 term) can provide hints to fine-tune the adaptation process. Key Idea. A proper adjustment toward the TTA algorithm can break the chain of monotonically increasing ϵt’s in Corollary 1 to prevent the model collapse. In the mean teacher update, the larger value of λ (Eq. 2) prioritizes the task of preventing collapse on one hand but also limits its adaptability to the new testing environment. Meanwhile, α (Eq. 3) controls the weight on preserving versus changing the model from the previous step. Drawing inspiration from the exploration-exploitation tradeoff [49, 25] encountered in reinforcement learning [54], we introduce a mechanism for adjusting λ and α on the fly, balancing between the two primary objectives: adaptation and preventing model collapse. Our strategy is prioritizing collapse prevention (increasing λ) and preserving the model from previous steps (decreasing α) when there is a significant deviation from θ0. In [40, 61, 59], λ and α were fixed through hyper-parameter tuning. This is suboptimal due to varying TTA environments and the lack of validation set [62]. Furthermore, Thm. 1 suggests the convergence rate quickly escalates when ϵt increases, making constant λ, αinsufficient to prevent collapse. Sensing the Divergence of θt. We first equip PeTTA with a mechanism for measuring its divergence from θ0. Since ft(x) = argmax y∈Y Pr(y|x; θt), we can decompose Pr(y|x; θt) = [h (ϕθt(x))]y, with ϕθt(·) is a θt-parameterized deep feature extractor followed by a fixed classification head (a linear and softmax layer) h(·). The operator [·]y extracts the yth component of a vector. 5Since h(·) remains unchanged, instead of comparing the divergence in the parameter space (Θ) or between the output probability Pr(y|x; θt) and Pr(y|x; θ0), we suggest an inspection over the feature embedding space that preserves a maximum amount of information in our case (data processing inequality [9]). Inspired by [31] and under Gaussian assumption, the Mahalanobis distance of the first moment of the feature embedding vectors is compared. Let z = ϕθt(x), we keep track of a collection of the running mean of feature vector z: {ˆµy t }y∈Y in which ˆµy t is EMA updated with vector z if ft(x) = y. The divergence of θt at step t, evaluated on class y is defined as: γy t = 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 , (6) where µy 0 and Σy 0 are the pre-computed empirical mean and covariant matrix of feature vectors in the source dataset (P0). The covariant matrix here is diagonal for simplicity. In practice, without directly accessing the training set, we assume a small set of unlabeled samples can be drawn from the source distribution for empirically computing these values (visit Appdx. E.4 for further details). Here, we implicitly expect the independence of each entry in z and TTA approaches learn to align feature vectors of new domains back to the source domain (P0). Therefore, the accumulated statistics of these feature vectors at each step should be concentrated near the vectors of the initial model. The value of γy t ∈ [0, 1] is close to 0 when θt = θ0 and increases exponentially as ˆµy t diverging from µy 0. Adaptive Regularization and Model Update. With α0, λ0 are initial values, utilizing γy t derived in Eq. 6, a pair of (λt, αt) is adaptively chosen at each step: ¯γt = 1 | ˆYt| X y∈ ˆYt γy t , ˆYt = n ˆY (i) t |i = 1, ··· , Nt o ; λt = ¯γt · λ0, α t = (1 − ¯γt) · α0, (7) ˆYt is a set of unique pseudo labels in a testing batch ( ˆY (i) t is the ith realization of ˆYt). Anchor Loss. Penalizing the divergence with regular vector norms in high-dimensional space (Θ) is insufficient (curse of dimensionality [5, 51]), especially with a large model and limited samples. Anchor loss LAL can nail down the similarity between ft and f0 in the probability space [32, 12]: LAL(Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ), (8) which is equivalent to minimizing the KL divergence DKL (Pr(y|Xt; θ0)∥Pr(y|Xt; θ)). Persistent TTA.Having all the ingredients, we design our approach, PeTTA, following the convention setup of the mean teacher update, with the category-balanced memory bank and the robust batch normalization layer from [61]. Appdx. E.1 introduces the pseudo code of PeTTA. ForLCLS, either the self-training scheme [12] or the regular cross-entropy [16] is adopted. With R(θ), cosine similarity or L2 distance are both valid metrics for measuring the distance between θ and θ0 in the parameter space. Fisher regularizer coefficient [ 40, 27] can also be used, optionally. To sum up, the teacher model update of PeTTA is an elaborated version of EMA with λt, αt (Eq. 7) and LAL (Eq. 8): θ′ t = Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′), θt = (1 − αt)θt−1 + αtθ′ t. 5 Experimental Results 5.1 ϵ−MMC Simulation Result Simulation Setup. A total of 6000 samples from two Gaussian distributions: N(µ0 = 0, σ2 0 = 1) and N(µ1 = 2, σ2 1 = 1) with p0 = p1 = 1 2 are synthesized and gradually released in a batch of B = 10 samples. For evaluation, an independent set of 2000 samples following the same distribution is used for computing the prediction frequency, and the false negative rate (FNR). ϵ−GMMC update follows Eq. 4 with α = 5e−2. To simulate model collapse, the predictor is intercepted and 10% of the true-postive pseudo labels at each testing step are randomly flipped (Corollary 1). Simulation Result. In action, both the likelihood of predicting class 0 (Fig. 3a-left) and theϵt (Eq. 5) (Fig. 3c-right, solid line) gradually increases over time as expected (Lemma 1). After collapsing, 60 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) 0 120 240 360 480 6000 0.2 0.4 0.6 0.8 1 Testing Step (t) −4 −2 0 2 4x−4 −2 0 2 40 0.2 0.4 0.6 0.8 1 x Probability density N(µ0, σ0) N(µ1, σ1) N(ˆµ0, ˆσ0) N(ˆµ1, ˆσ1) 0 100 200 300 400 500 600 0.8 1.2 1.6 2.0 Testing step (t) |ˆµ0,t −µ1| Numerical Simulation Theoretical Result 0 100 200 300 400 500 600 0.1 0.2 0.3 0.4 0.5 Testing step (t) ϵt Prediction Frequency GMMCϵ-GMMC ϵ-GMMC GMMC (a) (b) (c) Figure 3: Simulation result on ϵ-perturbed Gaussian Mixture Model Classifier ( ϵ-GMMC) and GMMC (perturbed-free). (a) Histogram of model predictions through time. A similar prediction frequency pattern is observed on CIFAR-10-C (Fig. 5a-left). (b) The probability density function of the two clusters after convergence versus the true data distribution. The initial two clusters of ϵ-GMMC collapsed into a single cluster with parameters stated in Lemma 2. In the perturbed-free, GMMC converges to the true data distribution. (c) Distance toward µ1 (|EPt [ˆµ0,t] − µ1|) and false- negative rate (ϵt) in simulation coincides with the result in Thm. 1 (with ϵt following Corollary 1). ϵ-GMMC merges the two initial clusters, resulting in a single one (Fig. 3b-left) with parameters that match Lemma 2. The distance from ˆµ0,t (initialized at µ0) towards µ1 converges (Fig. 3c-left, solid line), coincided with the analysis in Thm. 1 when ϵt is chosen following Corollary 1 (Fig. 3c, dashed line). GMMC (perturbed-free) stably produces accurate predictions (Fig. 3a-right) and approximates the true data distribution (Fig. 3b-right). The simulation empirically validates our analysis (Sec. 3.2), confirming the vulnerability of TTA models when the pseudo labels are inaccurately estimated. 5.2 Setup - Benchmark Datasets Datasets. We benchmark the performance on four TTA classification tasks. Specifically, CIFAR10 → CIFAR10-C, CIFAR100→ CIFAR100-C, and ImageNet → ImageNet-C [19] are three corrupted images classification tasks (corruption level 5, the most severe). Additionally, we incorporate DomainNet [44] with 126 categories from four domains for the task real → clipart, painting, sketch. Compared Methods. Besides PeTTA, the following algorithms are investigated: CoTTA [ 59], EATA [40], RMT [12], MECTA [22], RoTTA [61], ROID [37] and TRIBE [52]. Noteworthy, only RoTTA is specifically designed for the practical TTA setting while others fit the continual TTA setting in general. A parameter-free approach: LAME [ 7] and a reset-based approach (i.e., reverting the model to the source model after adapting to every 1, 000 images): RDumb [45] are also included. Recurring TTA. Following the practical TTA setup, multiple testing scenarios from each testing set will gradually change from one to another while the Dirichlet distribution (Dir(0.1) for CIFAR10- C, DomainNet, and ImageNet-C, and Dir(0.01) for CIFAR100-C) generates category temporally correlated batches of data. For all experiments, we set the number of revisits K = 20 (times) as this number is sufficient to fully observe the gradual degradation on existing TTA baselines. Implementation Details. We use PyTorch [43] for implementation. RobustBench [10] and torchvision [35] provide pre-trained source models. Hyper-parameter choices are kept as close as possible to the original selections of authors. Visit Sec. G for more implementation details. Unless otherwise noted, for all PeTTA experiments, the EMA update rate for robust batch normalization [61] and feature embedding statistics is set to 5e−2; α0 = 1e−3 and cosine similarity regularizer is used. On CIFAR10/100-C and ImageNet-C we use the self-training loss in [ 12] for LCLS and λ0 = 10 while the regular cross-entropy loss [ 13] and λ0 = 1 (severe domain shift requires prioritizing 7Table 1: Average classification error of the task CIFAR-10→ CIFAR-10-C in recurring TTA. The lowest error is in bold,(∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 43.5 43.5 LAME [7] 31.1 31.1 CoTTA [59]82.2 85.6 87.2 87.8 88.2 88.5 88.7 88.7 88.9 88.9 88.9 89.2 89.2 89.2 89.1 89.2 89.2 89.1 89.3 89.388.3EATA [40]81.6 87.0 88.7 88.7 88.9 88.7 88.6 89.0 89.3 89.6 89.5 89.6 89.7 89.7 89.3 89.6 89.6 89.8 89.9 89.488.8RMT [12]77.5 76.9 76.5 75.8 75.5 75.5 75.4 75.4 75.5 75.3 75.5 75.6 75.5 75.5 75.7 75.6 75.7 75.6 75.7 75.875.8MECTA [22]72.2 82.0 85.2 86.3 87.0 87.3 87.3 87.5 88.1 88.8 88.9 88.9 88.6 89.1 88.7 88.8 88.5 88.6 88.3 88.886.9RoTTA [61]24.6 25.5 29.6 33.6 38.2 42.8 46.2 50.6 52.2 54.1 56.5 57.5 59.4 60.2 61.7 63.0 64.8 66.1 68.2 70.351.3RDumb [45]31.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9ROID [37]72.7 72.6 73.1 72.4 72.7 72.8 72.7 72.7 72.9 72.8 72.9 72.9 72.8 72.5 73.0 72.8 72.5 72.5 72.7 72.772.7TRIBE [52]15.3 16.6 16.6 16.3 16.7 17.0 17.3 17.4 17.4 18.0 17.9 18.0 17.9 18.6 18.2 18.8 18.0 18.2 18.4 18.017.5PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 adaptability) are applied in DomainNet experiments. In Appdx. F.5, we provide a sensitivity analysis on the choice of hyper-parameter λ0 in PeTTA. 5.3 Result - Benchmark Datasets Recurring TTA Performance. Fig. 1-right presents the testing error on CIFAR-10-C in recurring TTA setting. RoTTA [61] exhibits promising performance in the first several visits but soon raises and eventually exceeds the source model (no TTA). The classification error of compared methods on CIFAR-10→CIFAR-10-C, and ImageNet → ImageNet-C [19] tasks are shown in Tab. 1, and Tab. 2. Appdx. F.1 provides the results on the other two datasets. The observed performance degradation of CoTTA [59], EATA [40], RoTTA [61], and TRIBE [52] confirms the risk of error accumulation for an extensive period. While RMT [12], MECTA [22], and ROID [37] remain stable, they failed to adapt to the temporally correlated test stream at the beginning, with a higher error rate than the source model. LAME [7] (parameter-free TTA) and RDumb [45] (reset-based TTA) do not suffer from collapsing. However, their performance is lagging behind, and knowledge accumulation is limited in these approaches that could potentially favor a higher performance as achieved by PeTTA. Furthermore, LAME [7] is highly constrained by the source model, and selecting a precise reset frequency in RDumb [45] is challenging in practice (see Appdx. F.3 for a further discussion). 0 10 20 30 40 16 18 20 22 24 Recurring TTA Visit Classification Error PeTTA (ours) TRIBE [52] Figure 4: Classification error of TRIBE [ 52] and PeTTA (ours) of the task CIFAR-10→CIFAR10-C task in recurring TTA with 40 visits. In average, PeTTA outperforms almost every baseline approaches and persists across 20 vis- its over the three datasets. The only exception is at the case of TRIBE [ 52] on CIFAR-10- C. While this state-of-the-art model provides stronger adaptability, outweighing the PeTTA, and baseline RoTTA [61] in several recurrences, the risk of the model collapsing still presents in TRIBE [52]. This can be clearly observed when we increase the observation period to 40 recur- ring visits in Fig. 4. As the degree of freedom for adaptation in PeTTA is more constrained, it takes a bit longer for adaptation but remains sta- ble afterward. Fig. 5b-bottom exhibits the con- fusion matrix at the last visit with satisfactory accuracy. The same results are also observed when shuffling the order of domain shifts within each recurrence (Appdx. D.3), or extending the number of recurrences to 40 visits (Appdx. F.4). Continuously Changing Corruption (CCC) [45] Performance. Under CCC [45], Tab. 3 reveals the supreme performance of PeTTA over RoTTA [61] and RDumb [45]. Here, we report the average classification error between two consecutive adaptation step intervals. An adaptation step in this table corresponds to a mini-batch of data with 64 images. The model is adapted to 80, 000 steps in total with more than 5.1M images, significantly longer than 20 recurring TTA visits. Undoubtedly, PeTTA still achieves good performance where the corruptions are algorithmically generated, non-cyclic with two or more corruption types can happen simultaneously. This experiment also empirically justifies the construction of our recurring TTA as a diagnostic tool (Appdx. D.2) where similar observations are concluded on the two settings. Obviously, our recurring TTA is notably simpler than CCC [45]. 8Table 2: Average classification error of the task ImageNet → ImageNet-C in recurring TTA scenario. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 82.0 82.0 LAME [7] 80.9 80.9 CoTTA [59]98.6 99.1 99.4 99.4 99.5 99.5 99.5 99.5 99.6 99.7 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.6 99.7 99.799.5EATA [40]60.4 59.3 65.4 72.6 79.1 84.2 88.7 92.7 95.2 96.9 97.7 98.1 98.4 98.6 98.7 98.8 98.8 98.9 98.9 99.089.0RMT [12]72.3 71.0 69.9 69.1 68.8 68.5 68.4 68.3 70.0 70.2 70.1 70.2 72.8 76.8 75.6 75.1 75.1 75.2 74.8 74.771.8MECTA [22]77.2 82.8 86.1 87.9 88.9 89.4 89.8 89.9 90.0 90.4 90.6 90.7 90.7 90.8 90.8 90.9 90.8 90.8 90.7 90.889.0RoTTA [61]68.3 62.1 61.8 64.5 68.4 75.4 82.7 95.1 95.8 96.6 97.1 97.9 98.3 98.7 99.0 99.1 99.3 99.4 99.5 99.687.9RDumb [45]72.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8ROID [37]62.7 62.3 62.3 62.3 62.5 62.3 62.4 62.4 62.3 62.6 62.5 62.3 62.5 62.4 62.5 62.4 62.4 62.5 62.4 62.562.4TRIBE [52]63.664.0 64.9 67.8 69.6 71.7 73.5 75.5 77.4 79.8 85.0 96.5 99.4 99.8 99.9 99.8 99.8 99.9 99.9 99.984.4PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Table 3: Average classification error on CCC [45] setting. Each column presents the average error within an adaptation interval (e.g., the second column provides the average error between the 6701 and 13400 adaptation steps). Each adaptation step here is performed on a mini-batch of 64 images. CCC [45] Adaptation Step− − − − − − − − − − − − − − − − − − − − − − − − − → Method6700 13400 20100 26800 33500 40200 46900 53600 60200 66800 73400 80000Avg Source 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.83 0.84 0.83 0.83 0.83 0.83 RoTTA [61]0.70 0.85 0.92 0.96 0.98 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.95 RDumb [45]0.78 0.74 0.75 0.77 0.75 0.72 0.75 0.77 0.75 0.74 0.75 0.75 0.75 PeTTA(ours) 0.67 0.63 0.62 0.65 0.65 0.64 0.64 0.68 0.63 0.63 0.65 0.65 0.64 0.46 0.44 0.4 0.43 0.46 0.47 0.44 0.43 0.48 0.4 0.43 0.43 0.41 airplane bird cat dog frog ship auto deer horse truck 0.13 0.34 0.44 0.32 0.14 0.44 0.51 0.46 0.46 0.34 airplane bird catdog frog ship auto deer horse truck Inter-category cosine similarity (source model)Misclassification rate of collapsed RoTTA 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits 1 5 10 15 200 0.2 0.4 0.6 0.8 1 Visits RoTTA [61] PeTTA (ours) PeTTA (ours) - 20th visit RoTTA [61] - 20th visit Predicted label (a) (b)(c) True labelTrue label Prediction Frequency Figure 5: Recurring TTA (20 visits) on CIFAR-10 →CIFAR10-C task. (a) Histogram of model predictions (10 labels are color-coded). PeTTA achieves a persisting performance while RoTTA [61] degrades. (b) Confusion matrix at the last visit, RoTTA classifies all samples into a few categories (e.g., 0: airplane, 4: deer). (c) Force-directed graphs showing (left) the most prone to misclassification pairs (arrows indicating the portion and pointing from the true to the misclassified category); (right) similar categories tend to be easily collapsed. Edges denote the average cosine similarity of feature vectors (source model), only the highest similar pairs are shown. Best viewed in color. Collapsing Pattern. The rise in classification error (Fig. 1-right) can be reasoned by the prediction frequency of RoTTA [ 61] in an recurring TTA setting (Fig. 5a-left). Similar to ϵ-GMMC, the likelihood of receiving predictions on certain categories gradually increases and dominates the others. Further inspecting the confusion matrix of a collapsed model (Fig. 5b-top) reveals two major groups of categories are formed and a single category within each group represents all members, thereby becoming dominant. To see this, Fig. 5c-left simplifies the confusion matrix by only visualizing the 9Table 4: Average (across 20 visits) error of multiple variations of PeTTA: without (w/o) R(θ), LAL; LAL only; fixed regularization coefficient λ; adaptive coef- ficient λt, update rate αt; using anchor loss LAL. Method CF-10-CCF-100-CDN IN-C Baseline w/oR(θ),LAL 42.6 63.0 77.9 93.4 R(θ)fixedλ= 0.1λ0 43.3 65.0 80.0 92.5R(θ)fixedλ=λ0 42.0 64.6 66.6 92.9 LALonly 25.4 56.5 47.5 68.1 PeTTA -λt 27.1 55.0 59.7 92.7PeTTA -λt +αt 23.9 41.4 44.5 75.7PeTTA -λt +LAL 26.2 36.3 43.2 62.0 PeTTA -λt +αt +LAL 22.8 35.1 42.9 60.5 Table 5: Average (across 20 visits) error of PeTTA. PeTTA favors various choices of reg- ularizers R(θ): L2 and cosine similarity in conjunction with Fisher [27, 40] coefficient. Method CF-10-CCF-100-CDN IN-CR(θ) Fisher L2 ✗ 23.0 35.6 43.1 70.8✓ 22.7 36.0 43.9 70.0 Cosine ✗ 22.8 35.1 42.9 60.5✓ 22.6 35.9 43.3 63.8 CF: CIFAR, DN: DomainNet, IN: ImageNet top prone-to-misclassified pair of categories. Here, label deer is used for almost every living animal while airplane represents transport vehicles. The similarity between categories in the feature space of the source model (Fig. 5c-right) is correlated with the likelihood of being merged upon collapsing. As distance in feature space is analogous to |µ0 − µ1| (Thm. 1), closer clusters are at a higher risk of collapsing. This explains and showcases that the collapsing behavior is predictable up to some extent. 5.4 Ablation Study Effect of Each Component. Tab. 4 gives an ablation study on PeTTA, highlighting the use of a regularization term (R(θ)) with a fixed choice of λ, αnot only fails to mitigate model collapse but may also introduce a negative effect (rows 2-3). Trivially applying the anchor loss (LAL) alone is also incapable of eliminating the lifelong performance degradation in continual TTA (row 4). Within PeTTA, adopting the adaptiveλt scheme alone (row 5) or in conjunction with either αt or anchor loss LAL (rows 6-7) partially stabilizes the performance. Under the drastic domain shifts with a larger size of categories or model parameters (e.g., on CIFAR-100-C, DomainNet, ImageNet-C), restricting αt adjustment limits the ability of PeTTA to stop undesirable updates while a common regularization term without LAL is insufficient to guide the adaptation. Thus, leveraging all elements secures the persistence of PeTTA (row 8). Various Choices of Regularizers. The design of PeTTA is not coupled with any specific regu- larization term. Demonstrated in Tab. 5, PeTTA works well for the two common choices: L2 and cosine similarity. The conjunction use of Fisher coefficent [27, 40] for weighting the model parameter importance is also studied. While the benefit (in terms of improving accuracy) varies across datasets, PeTTA accommodates all choices, as the model collapse is not observed in any of the options. 6 Discussions and Conclusion On a Potential Risk of TTA in Practice. We provide empirical and theoretical evidence on the risk of deploying continual TTA algorithms. Existing studies fail to detect this issue with a single pass per test set. The recurring TTA could be conveniently adopted as astraightforward evaluation, where its challenging test stream magnifies the error accumulation that a model might encounter in practice. Limitations. PeTTA takes one step toward mitigating the gradual performance degradation of TTA. Nevertheless, a complete elimination of error accumulation cannot be guaranteed rigorously through regularization. Future research could delve deeper into expanding our efforts to develop an algorithm that achieves error accumulation-free by construction. Furthermore, as tackling the challenge of the temporally correlated testing stream is not the focus of PeTTA, using a small memory bank as in [61, 15] is necessary. It also assumes the features statistics from the source distribution are available (Appdx. E.3, E.4). These constraints potentially limit its scalability in real-world scenarios. Conclusion. Towards trustworthy and reliable TTA applications, we rigorously study theperformance degradation problem of TTA. The proposed recurring TTAsetting highlights the limitations of modern TTA methods, which struggle to prevent the error accumulation when continuously adapting to demanding test streams. Theoretically inspecting a failure case of ϵ−GMMC paves the road for designing PeTTA- a simple yet efficient solution that continuously assesses the model divergence for harmonizing the TTA process, balancing adaptation, and collapse prevention. 10Acknowledgements This work was supported by the Jump ARCHES Endowment through the Health Care Engineering Systems Center, JSPS/MEXT KAKENHI JP24K20830, ROIS NII Open Collaborative Research 2024-24S1201, in part by the National Institute of Health (NIH) under Grant R01 AI139401, and in part by the Vingroup Innovation Foundation under Grant VINIF.2021.DA00128. References [1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution gener- alization. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan, editors,Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=jlchsFOLfeF. [2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/15825aee15eb335cc13f9b559f166ee8-Paper.pdf. [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample se- lection for online continual learning. In Advances in Neural Information Processing Systems , volume 32, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf. [4] Amitav Banerjee, U. B. Chitnis, S. L. Jadhav, J. S. Bhawalkar, and S. Chaudhury. Hypothesis testing, type I and type II errors. Industrial Psychiatry Journal, 18(2):127–131, 2009. ISSN 0972-6748. doi: 10.4103/0972-6748.62274. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996198/. [5] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1957. [6] Arno Blaas, Andrew Miller, Luca Zappella, Joern-Henrik Jacobsen, and Christina Heinze-Deml. Con- siderations for distribution shift robustness in health. In ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare, 2023. URL https://openreview.net/forum?id=y7XveyWYzIB. [7] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8334–8343, 2022. doi: 10.1109/CVPR52688.2022.00816. [8] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2022. [9] Thomas M. Cover and Joy A. Thomas.Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954. [10] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum?id=SSKZPJCt7B. [11] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3366–3385, 2022. doi: 10.1109/ TPAMI.2021.3057446. [12] Mario Döbler, Robert A. Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7704–7714, June 2022. [13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the 32nd International Conference on Machine Learning , volume 37 of Proceedings of Machine Learning Research , pages 1180–1189, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html. [14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks, pages 189– 209. Springer International Publishing, 2017. doi: 10.1007/978-3-319-58347-1_10. URL https: //doi.org/10.1007/978-3-319-58347-1_10 . [15] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. NOTE: Robust continual test-time adaptation against temporal correlation. In Advances in Neural Information Processing Systems, 2022. 11[16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In L. Saul, Y . Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems , volume 17, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1026–1034, 2015. [19] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019. [20] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. Proceedings of the International Conference on Learning Representations (ICLR), 2020. [21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8320–8329, 2021. doi: 10.1109/ICCV48922.2021.00823. [22] Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, and Michael Spranger. MECTA: Memory-economic continual test-time model adaptation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=N92hjSf5NNh. [23] Fabian Isensee, Paul F. Jaeger, Simon A. A. Kohl, Jens Petersen, and Klaus H. Maier-Hein. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203–211, February 2021. ISSN 1548-7105. doi: 10.1038/s41592-020-01008-z. URL https: //www.nature.com/articles/s41592-020-01008-z . [24] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wort- man Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 2427–2440, 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 1415fe9fea0fa1e45dddcff5682239a0-Paper.pdf. [25] Michael N. Katehakis and Arthur F. Veinott. The multi-armed bandit problem: Decomposition and compu- tation. Mathematics Operations Research, 12:262–268, 1987. URL https://api.semanticscholar. org/CorpusID:656323. [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412. 6980. [27] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.Pro- ceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/10.1073/pnas.1611835114. [28] Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop : Challenges in Representation Learning (WREPL), 07 2013. [29] T. Lee, S. Chottananurak, T. Gong, and S. Lee. Aetta: Label-free accuracy estimation for test-time adaptation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28643–28652, Los Alamitos, CA, USA, jun 2024. IEEE Computer Society. doi: 10.1109/CVPR52733. 2024.02706. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.02706. [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [31] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normalization for practical domain adaptation. In International Conference on Learning Representations Workshop, 2017. URL https://openreview.net/forum?id=BJuysoFeg. [32] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2935–2947, 2018. doi: 10.1109/TPAMI.2017.2773081. [33] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning (ICML), pages 6028–6039, 2020. 12[34] Sen Lin, Peizhong Ju, Yingbin Liang, and Ness Shroff. Theory on forgetting and generalization of continual learning. In Proceedings of the 40th International Conference on Machine Learning, ICML’23, 2023. [35] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library. https: //github.com/pytorch/vision, 2016. [36] Robert A Marsden, Mario Döbler, and Bin Yang. Gradual test-time adaptation by self-training and style transfer. arXiv preprint arXiv:2208.07736, 2022. [37] Robert A Marsden, Mario Döbler, and Bin Yang. Universal test-time adaptation through weight ensem- bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2555–2565, 2024. [38] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. [39] A. Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip Torr. TIPI: Test time adaptation with transformation invariance. In Conference on Computer Vision and Pattern Recognition 2023, 2023. URL https://openreview.net/forum?id=NVh1cy37Ge. [40] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In The Internetional Conference on Machine Learning, 2022. [41] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=g2YraF75Tj. [42] K. R. Parthasarathy. Introduction to Probability and Measure , volume 33 of Texts and Readings in Mathematics. Hindustan Book Agency, Gurgaon, 2005. ISBN 978-81-85931-55-5 978-93-86279-27-9. doi: 10.1007/978-93-86279-27-9. URL http://link.springer.com/10.1007/978-93-86279-27-9 . [43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [44] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1406–1415, 2019. [45] Ori Press, Steffen Schneider, Matthias Kuemmerer, and Matthias Bethge. RDumb: A simple approach that questions our progress in continual test-time adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=VfP6VTVsHc. [46] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 18–24 Jul 2021. URL https://proceedings. mlr.press/v139/radford21a.html. [48] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pages 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. [49] Mooweon Rhee and Tohyun Kim. Exploration and Exploitation, pages 543–546. Palgrave Macmillan UK, London, 2018. ISBN 978-1-137-00772-8. doi: 10.1057/978-1-137-00772-8_388. URL https: //doi.org/10.1057/978-1-137-00772-8_388 . [50] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [51] Tanin Sirimongkolkasem and Reza Drikvandi. On Regularisation Methods for Analysis of High Di- mensional Data. Annals of Data Science , 6(4):737–763, December 2019. ISSN 2198-5812. doi: 10.1007/s40745-019-00209-4. URL https://doi.org/10.1007/s40745-019-00209-4 . 13[52] Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balanced normalization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(13):15126– 15135, 2024. [53] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9229–9248. PMLR, 13–18 Jul 2020. URL https://proceedings. mlr.press/v119/sun20b.html. [54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018. [55] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 1195–1204, 2017. ISBN 9781510860964. [56] Daniel Vela, Andrew Sharp, Richard Zhang, Trang Nguyen, An Hoang, and Oleg S. Pianykh. Temporal quality degradation in AI models. Scientific Reports, 12(1):11654, July 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-15245-z. URL https://www.nature.com/articles/s41598-022-15245-z . [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=uXl3bZLkr3c. [58] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen domains: A survey on domain generalization. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4627–4635. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/628. URL https://doi.org/10. 24963/ijcai.2021/628. Survey Track. [59] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7201–7211, June 2022. [60] Zachary Young and Robert Steele. Empirical evaluation of performance degradation of machine learning-based predictive models – a case study in healthcare information systems. International Journal of Information Management Data Insights , 2(1):100070, 2022. ISSN 2667-0968. doi: https: //doi.org/10.1016/j.jjimei.2022.100070. URL https://www.sciencedirect.com/science/article/ pii/S2667096822000143. [61] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922– 15932, 2023. [62] Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023. URL https: //openreview.net/forum?id=0Go_RsG_dYn. 14Persistent Test-time Adaptation in Recurring Testing Scenarios Technical Appendices Table of Contents A Related Work 16 B Proof of Lemmas and Theorems 16 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Proof of Lemma 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.3 Proof of Theorem 1 and Corollary 1. . . . . . . . . . . . . . . . . . . . . . . . 18 C Further Justifications on Gaussian Mixture Model Classifier 19 D Further Justifications on the Recurring Testing Scenario 20 D.1 Recurring TTA Follows the Design of a Practical TTA Stream . . . . . . . . . . 20 D.2 Recurring TTA as a Diagnostic Tool . . . . . . . . . . . . . . . . . . . . . . . . 20 D.3 Recurring TTA with Random Orders . . . . . . . . . . . . . . . . . . . . . . . 20 E Further Justifications on Persistent TTA (PeTTA) 21 E.1 Pseudo Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 E.2 Anchor Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 E.3 The Use of the Memory Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset . 23 E.5 Novelty of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Additional Experimental Results of PeTTA 24 F.1 Performance of PeTTA Versus Compared Methods . . . . . . . . . . . . . . . . 24 F.2 An Inspection of PeTTA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.3 Does Model Reset Help? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 F.4 PeTTA with 40 Recurring Visits . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.5 The Sensitivity of Hyper-parameter Choices in PeTTA . . . . . . . . . . . . . . 27 F.6 More Details on the Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . 27 F.7 More Confusion Matrices in Recurring TTA Setting . . . . . . . . . . . . . . . 29 G Experimental Details 29 G.1 Computing Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.2 Experiments on CCC Testing Stream . . . . . . . . . . . . . . . . . . . . . . . 29 G.3 Test-time Adaptation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4 The Use of Existing Assets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 15A Related Work Towards Robust and Practical TTA. While forming the basis, early single-target TTA ap- proaches [53, 57, 39, 41, 33] is far from practice. Observing the dynamic of many testing envi- ronments, a continual TTA setting is proposed where an ML model continuously adapts to a sequence of multiple shifts [36, 59]. Meanwhile, recent studies [15, 7] point out that the category distribution realistic streams is highly temporally correlated. Towards real-world TTA setting, Yuanet al. [61] launch the practical TTA which considers the simultaneous occurrence of the two aforementioned challenges. For a robust and gradual adaptation, an update via the mean teacher [55] mechanism is exploited in many continual TTA algorithms [59, 61, 12, 22]. To moderate the temporally correlated test stream, common approaches utilize a small memory bank for saving a category-balanced subset of testing samples [15, 61], inspired by the replay methods [50, 2] to avoid forgetting in the task of continual learning [34, 3, 11]. Our study emphasizes another perspective: beyond a supreme performance, a desirable TTA should also sustain it for an extended duration. Temporal Performance Degradation.By studying the quality of various ML models across multiple industry applications [56, 60] the issue of AI “aging\" with the temporal model degradation progress, even with data coming from a stable process has been confirmed. In TTA, the continuous changes of model parameters through gradient descent aggravate the situation, as also recently noticed in [45]. Apart from observation, we attempt to investigate and provide theoretical insights towards the mechanism of this phenomenon. Accumulated Errors in TTA. In TTA, the issue of accumulated error has been briefly acknowledged. Previous works strive to avoid drastic changes to model parameters as a good practice. Up to some degree, it helps to avoid performance degradation. Nevertheless, it is still unclear whether their effectiveness truly eliminates the risk. To preserve in-distribution performance, regularization [27, 40] or replaying of training samples at test-time [ 12] have been used. Other studies explore reset (recovering the initial model parameters) strategies [59, 45], periodically or upon the running entropy loss approaches a threshold [ 41]. Unfortunately, knowledge accumulated in the preceding steps will vanish, and a bad heuristic choice of threshold or period leads to highly frequent model resets. Noteworthy, tuning those hyper-parameters is exceedingly difficult due to the unavailability of the validation set [62]. LAME [ 7] suggests a post-processing step for adaptation (without updating the parameters). This approach, however, still limits the knowledge accumulation. Our PeTTA is reset-free by achieving an adaptable continual test-time training. B Proof of Lemmas and Theorems In this section, we prove the theoretical results regarding the ϵ−perturbed Gaussian Mixture Model Classifier (ϵ−GMMC) introduced in Sec. 3.2. We first briefly summarize the definition of model collapse and the static data stream assumption: Definition 1 (Model Collapse). A model is said to be collapsed from step τ ∈ T, τ <∞ if there exists a non-empty subset of categories ˜Y ⊂ Ysuch that Pr{Yt ∈ ˜Y} > 0 but the marginal Pr{ˆYt ∈ ˜Y} converges to zero in probability: lim t→τ Pr{ˆYt ∈ ˜Y} = 0. Assumption 1 (Static Data Stream). The marginal distribution of the true label follows the same Bernoulli distribution Ber(p0): p0,t = p0, (p1,t = p1 = 1 − p0), ∀t ∈ T. Preliminary. Following the same set of notations introduced in the main text, recall that we denoted py,t ∆ = Pr{Yt = y}, ˆpy,t ∆ = Pr{ˆYt = y} (marginal distribution of the true label Yt and pseudo label ˆYt receiving label y, respectively) and ϵt = Pr{Yt = 1|ˆYt = 0} (the false negative rate (FNR) of 16ϵ−GMMC). At testing step t, we obtain the following relations: EPt h Xt|ˆYt = 0 i = (1 − ϵt)µ0 + ϵtµ1, (9) EPt h Xt|ˆYt = 1 i = µ1, (10) VarPt \u0010 Xt|ˆYt = 0 \u0011 = (1 − ϵt)σ2 0 + ϵtσ2 1 + ϵt(1 − ϵt)(µ0 − µ1)2, (11) VarPt \u0010 Xt|ˆYt = 1 \u0011 = σ2 1. (12) In addition, under Assumption 1, the marginal distribution Pt(x) (also referred as data distribution in our setup) is: Pt(x) = N(x; p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2) ∀t ∈ T. (13) B.1 Proof of Lemma 1 Lemma 1 (Increasing FNR). Under Assumption 1, a binary ϵ-GMMC would collapsed (Def. 1) with lim t→τ ˆp1,t = 0 (or lim t→τ ˆp0,t = 1, equivalently) if and only if lim t→τ ϵt = p1. Proof. Under Assumption 1, we have EPt [Xt] = p0µ0 + (1 − p0)µ1. Also note that: EPt [Xt] = EPt h EPt h Xt|ˆYt ii = EPt h Xt|ˆYt = 0 i ˆp0,t + EPt h Xt|ˆYt = 1 i ˆp1,t (14) = [(1 − ϵt)µ0 + ϵtµ1] ˆp0,t + µ1(1 − ˆp0,t) = [(1 − ϵt)ˆp0,t] µ0 + [1 − ˆp0,t(1 − ϵt)] µ1 = p0µ0 + (1 − p0)µ1, where the second equality follows Eqs. 9-10. Therefore: ˆp0,t = p0 1 − ϵt . (15) Eq. 15 shows positive correlation between ˆp0,t and ϵt. Given lim t→τ ϵt = p1, taking the limit introduces: lim t→τ ˆp0,t = lim t→τ p0 1 − ϵt = p0 1 − p1 = 1. Similarly, having lim t→τ ˆp0,t = 1, the false negative rate ϵt when t → τ is: lim t→τ ϵt = 1 − p0 = p1. Since ˆp0,t + ˆp1,t = 1, lim t→τ ˆp1,t = 0, equivalently. Towards the collapsing point, the model tends to predict a single label (class 0 in the current setup). In addition, the FNR of the model ϵt also raises correspondingly. B.2 Proof of Lemma 2. Lemma 2 (ϵ-GMMC After Collapsing ). For a binary ϵ-GMMC model, with Assumption 1, if lim t→τ ˆp1,t = 0 (collapsing), the cluster 0 in GMMC converges in distribution to a single-cluster GMMC with parameters: N(ˆµ0,t, ˆσ2 0,t) d. → N(p0µ0 + p1µ1, p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2). Proof. From Eqs. 9-10, under the increasing type II collapse of ϵ−GMMC setting, the perturbation does not affect the approximation of µ1. Meanwhile, when ϵt increases, one can expect that ˆµ0,t 17moves further away from µ0 toward µ1. Frist, the mean teacher model of GMMC (Eq. 4, main text) gives: EPt h ˆµ0,t|ˆYt = 1 i = EPt−1 [ˆµ0,t−1] , EPt h ˆµ0,t|ˆYt = 0 i = (1 − α)EPt−1 h ˆµ0,t−1|ˆYt = 0 i + αEPt h Xt|ˆYt = 0 i = (1 − α)EPt−1 [ˆµ0,t−1] + α \u0010 EPt h Xi|ˆYt = 0 i\u0011 , EPt h ˆµ1,t|ˆYt = 1 i = (1 − α)EPt−1 h ˆµ1,t−1|ˆYt = 1 i + αEPt h Xt|ˆYt = 1 i = (1 − α)EPt−1 [ˆµ1,t−1] + α \u0010 EPt h Xi|ˆYt = 1 i\u0011 , EPt h ˆµ1,t|ˆYt = 0 i = EPt−1 [ˆµ1,t−1] . By defining uy,t = EPt [ˆµy,t], we obtain the following recurrence relation between u0,t and u0,t−1: u0,t = EPt h ˆµ0,t|ˆYt = 0 i ˆp0,t + EPt h ˆµ0,t|ˆYt = 1 i ˆp1,t = \u0010 (1 − α)u0,t−1 + αEPt h Xt|ˆYt = 0 i\u0011 ˆp0,t + u0,t−1 ˆp1,t = [(1 − α)ˆp0,t + ˆp1,t] u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,tEPt h Xt|ˆYt = 0 i = (1 − αˆp0,t)u0,t−1 + αˆp0,t [(1 − ϵt)µ0 + ϵtµ1] . (16) Given lim t→τ ˆp0,t = 1, it follows that lim t→τ ϵ0,t = p1 by Lemma 1. From this point: u0,t = (1 − α)u0,t−1 + α (p0µ0 + p1µ1) ∀t > τ. Taking the limit t → ∞: lim t→∞ u0,t = lim t→∞ (1 − α)u0,t−1 + α (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + α tX i=1 (1 − α)i−1 (p0µ0 + p1µ1) = lim t→∞ (1 − α)t ˆµ0,0 + (1 − (1 − α)t)(p0µ0 + p1µ1) = p0µ0 + p1µ1. The second equation is obtained by solving the recurrence relation. When lim t→τ ˆp0,t = 1, {ˆµy,t}y∈{0,1} becomes a deterministic values. Hence, giving uy,t = EPt [ˆµy,t] = ˆµ0,t(∀t > τ) and lim t→∞ ˆµ0,t = lim t→∞ u0,t = p0µ0 + p1µ1. (17) Repeating the steps above with Eqs. 11-12 in place of Eqs. 9-10, we obtain a similar result for σ2 0,t: lim t→∞ ˆσ2 0,t = p0σ2 0 + p1σ2 1 + p0p1(µ0 − µ1)2. (18) By Lévy’s continuity theorem (p. 302, [ 42]), from Eqs. 17-18, when t → ∞, the estimated distribution of the first cluster N(x; ˆµ0,tˆσ2 0,t) converges to the whole data distribution Pt(x) (Eq. 13) when collapsing. B.3 Proof of Theorem 1 and Corollary 1. Theorem 1 (Convergence of ϵ−GMMC). For a binary ϵ-GMMC model, with Assumption 1, let the distance from ˆµ0,t toward µ1 is d0→1 t = |EPt [ˆµ0,t] − µ1|, then: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . 18Proof. Substituting Eq. 15 into ˆp0,t of Eq. 16 gives: u0,t = \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0 1 − ϵt [(1 − ϵt)µ0 + ϵtµ1] . Hence, we have the distance from u0,t toward µ1: |u0,t − µ1| = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 u0,t−1 + αp0µ0 + αp0ϵtµ1 1 − ϵt − µ1 \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 + αp0ϵtµ1 1 − ϵt − αp0µ1 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0µ0 − αp0µ1(1 − ϵt) 1 − ϵt \f\f\f\f = \f\f\f\f \u0012 1 − αp0 1 − ϵt \u0013 (u0,t−1 − µ1) + αp0(µ0 − µ1) \f\f\f\f ≤ \u0012 1 − αp0 1 − ϵt \u0013 |u0,t−1 − µ1| + αp0|µ0 − µ1|. The last inequality holds due to the triangle inequality. Equivalently, |u0,t − µ1| − |u0,t−1 − µ1| ≤α · p0 · \u0012 |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt \u0013 . Let d0→1 t = |EPt [ˆµ0,t] − µ1|, we conclude that: d0→1 t − d0→1 t−1 ≤ α · p0 · \u0012 |µ0 − µ1| −d0→1 t−1 1 − ϵt \u0013 . Corollary 1 (A Condition forϵ−GMMC Collapse). With fixedp0, α, µ0, µ1, ϵ−GMMC is collapsed if there exists a sequence of {ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) such that: p1 ≥ ϵt > 1 − d0→1 t−1 |µ0 − µ1|, t ∈ [τ − ∆τ , τ]. Proof. Initialized at µ0, ϵ-GMMC is collapsing when ˆµ0,t converges to the mid-point p0µ0 + p1µ1 (Lemma 2), i.e., moving closer to µ1. From Thm. 1, the distance towards µ1 d0→1 t < d0→1 t−1 if |µ0 − µ1| −|u0,t−1 − µ1| 1 − ϵt < 0 ⇔ |µ0 − µ1| < |u0,t−1 − µ1| 1 − ϵt ⇔ ϵt > 1 − |u0,t−1 − µ1| |µ0 − µ1| . When there exists this sequence{ϵt}τ τ−∆τ (τ ≥ ∆τ > 0) it follows that d0→1 t < d0→1 t−1 and ϵt > ϵt−1 is guaranteed ∀t ∈ [τ − ∆τ , τ]. Hence, lim t→τ ϵt = p1 (model collapsed, by Lemma 1). C Further Justifications on Gaussian Mixture Model Classifier One may notice that in ϵ-GMMC (Sec. 4.2), the classifier is defined ft(x) = argmaxy∈Y Pr(x|y; θt) (maximum likelihood estimation) while in general, ft(x) = argmaxy∈Y Pr(y|x; θt) (maximum a posterior estimation), parameterized by a neural network. In this case, since the equal prior (i.e., Pr(y; θt) = Pr(y′; θt), ∀y, y′ ∈ C) is enforced in ϵ-GMMC, the two definitions are equivalent. Proof. Having: argmaxy∈Y Pr(y|x; θt) = argmaxy∈Y Pr(x|y; θt) Pr(y; θt)P y′∈Y Pr(x|y′; θt) Pr(y′; θt) = argmaxy∈Y Pr(x|y; θt). We conclude that the two definitions are equivalent. In fact, it is well-known that maximum likelihood estimation is a special case of maximum a posterior estimation when the prior is uniform. 19D Further Justifications on the Recurring Testing Scenario D.1 Recurring TTA Follows the Design of a Practical TTA Stream Note that in recurring TTA, besides the recurrence of environments (or corruptions) as in [59, 40], the distribution of class labels is also temporally correlated (non-i.i.d.) as suggested by [15, 61] to reflect the practical testing stream better. In short, recurring TTA is formed by recurring the environments of practical TTA scenario introduced in [61] multiple times (readers are encouraged to visit the original paper for additional motivations on this scenario). D.2 Recurring TTA as a Diagnostic Tool Noticeably, CoTTA [59] also performed 10-round repetition across multiple domain shifts to simulate a lifelong TTA testing stream just like our recurring TTA. However, the key difference is CoTTA assumes the distribution of class labels is i.i.d., which does not hold in many real-life testing scenarios as argued in [ 15, 61]. Our recurring TTA lifts this assumption and allows temporally correlated (non-i.i.d.) label distribution (more challenging, more practical). This extension allows recurring TTA to spot the risk of model collapse on CoTTA [59] and other methods. The over-simplicity of the repeating scheme in CoTTA for spotting performance degradation is also suggested in [45]. Clearly, it seems not to be a problem at first glance in Tab. 5 of [59] (CoTTA’s 10-round repetition), but in fact, the risk in CoTTA remains, as explored in our scenario and also on CCC [45]. The construction of our recurring TTA is notably simple - a technical effort to extend the testing stream. However, this simplicity is on purpose, serving as a diagnostic tool for lifelong continual TTA. Counterintuitively, our experiments on four different tasks with the latest methods verify that even if the model is exposed to the same environment(the most basic case), their adaptability and performance are still consistently reduced (demonstrated visually in Fig. 1, quantitatively in Sec. 5.3). We believe that the extensive testing stream by recurrence in our setup is a simple yet sufficient scenario to demonstrate the vulnerability of existing continual TTA methods when facing the issue of model collapse (compared to CCC [45], a notably more complicated scenario than our recurring TTA). Indeed, recurring shifts are sufficient to show this failure mode and any lifelong TTA method should necessarily be able to handle recurring conditions. D.3 Recurring TTA with Random Orders Recall that in Sec. 3.1,recurring TTAis constructed by repeatingthe same sequence of D distributions K times. For example, a sequence with K = 2 could be P1 → P2 → ··· → PD → P1 → P2 → ··· → PD. For simplicity and consistency that promote reproducibility, the same order of image corruptions (following [61]) is used for all recurrences. This section presents supplementary experimental findings indicating that the order of image corruptions within each recurrence, indeed, does not affect the demonstration of TTA model collapse and the performance of our PeTTA. Experiment Setup. We refer to the setting same-order as using one order of image corruptions in [61] for all recurrences (specifically, on CIFAR-10/100-C and ImageNet-C:motion → snow → fog → shot → defocus → contrast → zoom → brightness → frost → elastic → glass → gaussian → pixelated → jpeg → impulse). Conversely, in random-order, the order of image corruptions is randomly shuffled at the beginning of each recurrence. Hence, the corruption orders across K recurrences are now entirely different. We redo the experiment of the second setting three times (with different random seeds = 0, 1, 2). Nevertheless, different TTA methods are ensured to be evaluated on the same testing stream, since it is fixed after generation. Without updating its parameters, the performance of the source model is trivially independent of the order of corruptions. Experimental Result. The experimental results are visualized in Fig. 6. The first column plots the experiments under the same-order, while the remaining three columns plot the experiments in the random-order setting, with varying random seeds. Note that the message conveyed by each sub-figure entirely matches that of Fig. 1-right. Discussions. Clearly, a similar collapsing pattern is observed in all three TTA tasks, with three combinations of 20 image corruption orders. This pattern also matches the easiest setting using the same order of image corruptions we promoted in recurring TTA. 201 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (a) CIFAR-10 → CIFAR-10-C task. 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (b) CIFAR-100 → CIFAR-100-C task. 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 1 5 10 15 20 0.5 0.6 0.7 0.8 0.9 1.0 Same-order Random-order (seed=0) Random-order (seed=1) Random-order (seed=2) Testing Error Recurring TTA visit Recurring TTA visit Recurring TTA visit Recurring TTA visit (c) ImageNet → ImageNet-C task. Figure 6: Recurring TTA with different order of corruptions. This figure plots the testing error of two TTA approaches: RoTTA - - [61], and, PeTTA- - (ours), and source model-×- as a reference performance under our recurring TTA (with 20 visits) across three TTA tasks. On the same-order experiments (column 1), the same order of image corruptions is applied for all 20 visits. Meanwhile, in random-order, this order is reshuffled at the beginning of each visit (columns 2-4). Random-order experiments are redone three times with different random seeds. Here, we empirically validate that using the same order of domain shifts (image corruptions) in our recurring TTA is sufficient to showcase the model collapse and evaluate the persistence of our PeTTA. Best viewed in color. E Further Justifications on Persistent TTA (PeTTA) E.1 Pseudo Code We summarize the key steps of our proposed PeTTA in Alg. 1, with the key part (lines 4-13) highlighted in blue. Our approach fits well in the general workflow of a TTA algorithm, enhancing the regular mean-teacher update step. Appdx. E.5 elaborates more on our contributions in PeTTA, distinguishing them from other components proposed in previous work. The notations and definitions of all components follow the main text (described in detail in Sec. 4). On line 8 of Alg. 1, as a 21Algorithm 1 Persistent TTA (PeTTA) Input: Classification model ft and its deep feature extractor ϕθt, both parameterized by θt ∈ Θ. Testing stream {Xt}T t=0, initial model parameter (θ0), initial update rate (α0), regularization term coefficient (λ0), empirical mean ({µy 0}y∈Y) and covariant matrix ({Σy 0}y∈Y) of feature vectors in the training set, ˆµy t EMA update rate (ν). 1 ˆµy 0 ← µy 0, ∀y ∈ Y; // Initialization 2 for t ∈ [1, ··· , T] do 3 ˆYt ← ft−1(Xt) ; // Obtaining pseudo-labels for all samples in Xt 4 // Persistent TTA (PeTTA) 5 ˆYt ← n ˆY (i) t |i = 1, ··· , Nt o ; // Set of (unique) pseudo-labels in Xt 6 ¯γt ← 0 ; 7 for y ∈ ˆYt do 8 γy t ← 1 − exp \u0010 −(ˆµy t − µy 0)T (Σy 0)−1 (ˆµy t − µy 0) \u0011 ; // Divergence sensing term on category y 9 ¯γt ← ¯γt + γy t | ˆYt| ; // Average divergence sensing term for step t 10 ˆµy t ← (1 − ν)ˆµy t−1 + νϕθt−1 (Xt|ˆYt = y) ; // EMA update of ˆµy t for samples with ˆYt = y 11 end 12 λt ← ¯γt · λ0 ; // Computing adaptive regularization term coefficient 13 αt ← (1 − ¯γt) · α0 ; // Computing adaptive update rate 14 // Regular Mean-teacher Update 15 θ′ t ← Optim θ′∈Θ EPt h LCLS \u0010 ˆYt, Xt; θ′ \u0011 + LAL (Xt; θ′) i + λtR(θ′) ; // Student model update 16 θt ← (1 − αt)θt−1 + αtθ′ t. ; // Teacher model update 17 // Final prediction 18 yeild ft(Xt) ; // Returning the final inference with updated model ft 19 end shorthand notation, ϕθt−1 (Xt|ˆYt = y) denotes the empirical mean of all feature vectors of X(i) t (extracted by ϕθt−1 \u0010 X(i) t \u0011 ) if ˆY (i) t = y, i= 1, ··· , Nt in the current testing batch. E.2 Anchor Loss KL Divergence Minimization-based Interpretation of Anchor Loss. In Sec. 4, we claimed that minimizing the anchor loss LAL is equivalent to minimizing the relative entropy (or KL divergence) between the output probability of two models parameterized by θ0 and θ. Proof. Having: DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) = X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ0) Pr(y|Xt; θ) = − X y∈Y Pr(y|Xt; θ0) log Pr(y|Xt; θ) | {z } LAL(Xt;θ) −H(Pr(y|Xt; θ0))| {z } constant . Hence, argmin θ∈Θ LAL(Xt; θ) = argmin θ∈Θ DKL (Pr(y|Xt; θ0)||Pr(y|Xt; θ)) . 22Intuitively, a desirable TTA solution should be able to adapt to novel testing distributions on the one hand, but it should not significantly diverge from the initial model. LAL fits this purpose, constraining the KL divergence between two models at each step. Connections between Anchor Loss and Regularizer Term. While supporting the same objective (collapse prevention by avoiding the model significantly diverging from the source model), the major difference between Anchor loss ( LAL) and the Regularizer term ( R(θ)) is that the anchor loss operates on the probability space of model prediction while the regularizer term works on the model parameter spaces. Tab. 4 (lines 1 and 5) summarizes the ablation study when each of them is eliminated. We see the role of the regularization term is crucial for avoiding model collapse, while the anchor loss guides the adaptation under the drastic domain shift. Nevertheless, fully utilizing all components is suggested for maintaining TTA persistence. E.3 The Use of the Memory Bank The size of Memory Bank. The size of the memory bank in PeTTA is relatively small, equal to the size of one mini-batch for update (64 images, specifically). The Use of the Memory Bank in PeTTA is Fair with Respect To the Compared Methods.Our directly comparable method - RoTTA [61] also takes this advantage (referred to as category-balanced sampling, Sec. 3.2 of [ 61]). Hence, the comparison between PeTTA and RoTTA is fair in terms of additional memory usage. Noteworthy, the use of a memory bank is a common practice in TTA literature (e.g., [15, 8, 61]), especially in situations where the class labels are temporally correlated or non-i.i.d. distributed (as we briefly summarized in Appdx. A - Related Work section). CoTTA [59], EATA [40] and MECTA [ 22] (compared method) assume labels are i.i.d. distributed. Hence, a memory bank is unnecessary, but their performance under temporally correlated label distribution has dropped significantly as a trade-off. The RMT [12] (compared method) does not require a memory bank but it needs to cache a portion of the source training set for replaying (Sec. 3.3 in [12]) which even requires more resources than the memory bank. Eliminating the Need for a Memory Bank. As addressing the challenge of temporally correlated label distribution on the testing stream is not the focus of PeTTA, we have conveniently adopted the use of the memory bank proposed in [61]. Since this small additional memory requirement is not universally applied in every real-world scenario, we believe that this is a reasonable assumption, and commonly adopted in TTA practices. Nevertheless, exploring alternative ways for reducing the memory size (e.g., storing the embedded features instead of the original image) would be an interesting future direction. E.4 Empirical Mean and Covariant Matrix of Feature Vectors on the Source Dataset Two Ways of Computing µy 0 and Σy 0 in Practice. One may notice that in PeTTA, computing γy t requires the pre-computed empirical mean (µy 0) and covariance (Σy 0) of the source dataset . This requirement may not be met in real-world situations where the source data is unavailable. In practice, the empirical mean and covariance matrix computed on the source distribution can be provided in the following two ways: 1. Most ideally, these values are computed directly by inference on the entire training set once the model is fully trained. They will be provided alongside the source-distribution pre-trained model as a pair for running TTA. 2. With only the source pre-trained model available, assume we can sample a set of unlabeled data from the source distribution. The (pseudo) labels for them are obtained by inferring from the source model. Since the source model is well-performed in this case, using pseudo is approximately as good as the true label. Accessing the Source Distribution Assumption in TTA. In fact, the second way is typically assumed to be possible in previous TTA methods such as EATA [40], and MECTA [22] (a compared method) to estimate a Fisher matrix (for anti-forgetting regularization purposes). Our work - PeTTA follows the same second setup as the previous approaches mentioned above. A variation of RMT [12] (a compared method) approach even requires having the fully labeled source data available at test-time for source replaying (Sec. 3.3 of [12]). This variation is used for comparison in our experiments. 23We believe that having the empirical mean and covariant matrix pre-computed on a portion of the source distribution in PeTTA is a reasonable assumption . Even in the ideal way, revealing the statistics might not severely violate the risk of data privacy leakage or require notable additional computing resources. Number of Samples Needed for Computation. To elaborate more on the feasibility of setting (2) mentioned above, we perform a small additional experiment on the performance of PeTTA while varying the number of samples used for computing the empirical mean and covariant matrix on the source distribution. In this setting, we use the test set of CIFAR-10, CIFAR-100, DomainNet validation set of ImageNet (original images, without corruption, or the real domain test set of DomainNet), representing samples from the source distribution. The total number of images is 10, 000 in CIFAR-10/A00, 50, 000 in ImageNet, and 69, 622 in DomainNet. We randomly sample 25%, 50%, 75%, and 100% of the images in this set to run PeTTA for 20 rounds of recurring. The result is provided in Tab. 6 below. Table 6: Average classification error of PeTTA (across 20 visits) with varying sizes of source samples used for computing feature empirical mean (µy 0) and covariant matrix (Σy 0). TTA Task 25% 50% 75% 100% CIFAR-10→CIFAR-10-C 22.96 22.99 23.03 22.75 CIFAR-100→CIFAR-100-C 35.01 35.11 35.09 35.15 DomainNet:real→clip→paint→sketch 43.18 43.12 43.15 42.89 ImageNet→ImageNet-C 61.37 59.68 61.05 60.46 The default choice of PeTTA is using 100% samples of the validation set of the source dataset. However, we showcase that it is possible to reduce the number of unlabeled samples from the source distribution to compute the empirical mean and covariant matrix for PeTTA, without significantly impacting its performance. E.5 Novelty of PeTTA PeTTA is composed of multiple components. Among them, the anchor loss is an existing idea (examples of previous work utilizing this idea are [ 32, 12]). Similarly, the mean-teacher update; and regularization are well-established techniques and very useful for the continual or gradual TTA scenario. Hence, we do not aim to improve or alternate these components. Nevertheless, the novelty of our contribution is the sensing of the divergence and adaptive model update, in which the importance of minimizing the loss (adaptation) and regularization (collapse prevention) is changed adaptively. In short, we propose a harmonic way of combining those elements adaptively to achieve a persistent TTA process. The design of PeTTA draws inspiration from a theoretical analysis (Sec. 3.2), empirically surpassing both the conventional reset-based approach [45] (Appdx. F.3) and other continual TTA approaches [61, 12, 59, 22, 7] on our proposed recurring TTA (Sec. 3.1, Appdx. F.1), as well as the previously established CCC [45] benchmark. F Additional Experimental Results of PeTTA F.1 Performance of PeTTA Versus Compared Methods Performance on CIFAR-100-C and Domainnet Datasets. Due to the length constraint, the classification errors on the tasks CIFAR-100→CIFAR-100-C, and real → clipart, painting, sketch of DomainNet are provided in Tab. 7 and Tab. 8. To prevent model collapse, the adaptability of PeTTA is more constrained. As a result, it requires more time for adaptation initially (e.g., in the first visit) but remains stable thereafter. Generally, consistent trends and observations are identified across all four TTA tasks. Standard Deviation of PeTTA Performance Across Multiple Runs. For PeTTA experiments marked with (*) in Tab. 1, Tab. 2, Tab. 7, and Tab. 8, the average performance across five independent runs with different random seeds is reported. Due to the space constraint, the corresponding standard deviation values are now reported in Tab. 9. Generally, the average standard deviation across runs 24Table 7: Average classification error of the task CIFAR-100 → CIFAR-100-C in recurring TTA scenario. The lowest error is highlighted in bold, (∗)average value across 5 runs (different random seeds) is reported for PeTTA. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 46.5 46.5 LAME [7] 40.5 40.5 CoTTA [59]53.4 58.4 63.4 67.6 71.4 74.9 78.2 81.1 84.0 86.7 88.8 90.7 92.3 93.5 94.7 95.6 96.3 97.0 97.3 97.683.1EATA [40]88.5 95.0 96.8 97.3 97.4 97.2 97.2 97.3 97.4 97.5 97.5 97.5 97.6 97.7 97.7 97.7 97.8 97.8 97.7 97.796.9RMT [12]50.5 48.6 47.9 47.4 47.3 47.1 46.9 46.9 46.6 46.8 46.7 46.5 46.5 46.6 46.5 46.5 46.5 46.5 46.5 46.547.1MECTA [22]44.8 44.3 44.6 43.1 44.8 44.2 44.4 43.8 43.8 43.9 44.6 43.8 44.4 44.6 43.9 44.2 43.8 44.4 44.9 44.244.2RoTTA [61]35.5 35.2 38.5 41.9 45.3 49.2 52.0 55.2 58.1 61.5 64.6 67.5 70.7 73.2 75.4 77.1 79.2 81.5 82.8 84.561.4RDumb [45]36.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6ROID [37]76.4 76.4 76.2 76.2 76.3 76.1 75.9 76.1 76.3 76.3 76.6 76.3 76.8 76.7 76.6 76.3 76.2 76.0 75.9 76.076.3TRIBE [52]33.8 33.335.334.935.335.137.1 37.2 37.2 39.1 39.2 41.1 41.0 43.1 45.1 45.1 45.0 44.9 44.9 44.939.6PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 8: Average classification error of the task real → clipart → painting → sketch on DomainNet dataset in recurring TTA scenario. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Source 45.3 45.3 LAME [7] 45.6 45.6 CoTTA [59]96.2 97.1 97.4 97.8 98.1 98.2 98.4 98.4 98.4 98.5 98.6 98.6 98.6 98.6 98.6 98.7 98.7 98.7 98.7 98.798.3RMT [12]76.2 77.1 77.3 77.3 77.2 77.1 76.8 76.9 76.5 76.4 76.4 76.3 76.4 76.2 76.2 76.1 76.4 76.1 76.0 75.876.5MECTA [22]94.6 98.4 98.6 98.8 99.1 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.0 99.098.7RoTTA [61]44.3 43.8 44.7 46.7 48.7 50.8 52.7 55.0 57.1 59.7 62.7 65.1 68.0 70.3 72.7 75.2 77.2 79.6 82.6 85.362.1RDumb [45]44.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 stays within ±0.1% for small datasets (CIFAR-10-C, CIFAR-100-C) and±0.5% for larger datasets (ImageNet-C, DomainNet). Table 9: Mean and standard deviation classification error of PeTTA on the four datasets: CIFAR-10-C (CF-10-C), CIFAR-100-C (CF-100-C), DomainNet (DN), and ImageNet-C (IN-C) with recurring TTA scenario. Each experiment is run 5 times with different random seeds. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Dataset1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg CF-10-C24.3 23.0 22.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8±0.4±0.3±0.4±0.3±0.3±0.3±0.4±0.2±0.3±0.4±0.4±0.2±0.1±0.3±0.5±0.2±0.2±0.3±0.4±0.5 ±0.1 CF-100-C35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1±0.4±0.4±0.2±0.2±0.1±0.1±0.2±0.2±0.1±0.2±0.1±0.2±0.2±0.1±0.1±0.1±0.1±0.1±0.2±0.2 ±0.1 DN43.8 42.6 42.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9±0.1±0.1±0.2±0.2±0.3±0.3±0.3±0.4±0.4±0.4±0.4±0.4±0.4±0.3±0.3±0.2±0.4±0.3±0.3±0.3 ±0.3 IN-C65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5±0.6±0.5±0.5±0.5±1.4±1.1±1.0±0.5±0.8±0.9±0.4±0.8±0.9±0.8±0.9±0.8±1.0±0.6±0.6±0.7 ±0.5 F.2 An Inspection of PeTTA In Fig. 7, we showcase an inspection of our PeTTA on the task CIFAR-10→ CIFAR-10-C [19] in a typical recurring TTA with 20 visits. Specifically, the visualizations of PeTTA parameters ( ¯γt, λt, and αt), adaptation losses (LCLS, LAL) and regularization term (R(θ)) are provided. Here, we observe the values of adaptive parameters λt and αt continuously changing through time, as the testing scenarios evolve during recurring TTA. This proposed mechanismstabilizes the value of the loss functions, and regularization term, balancing between the two primary objectives: adaptation and preventing model collapse. Thus, the error rate persists as a result. A similar pattern is observed on other datasets (CIFAR-100-C [19] and DomainNet [44]). F.3 Does Model Reset Help? Experiment Setup. We use the term “model reset” to represent the action of “reverting the current TTA model to the source model” . This straightforward approach is named RDumb [ 45]. We thoroughly conducted experiments to compare the performance of RDumb with PeTTA. The implementation of RDumb in this setting is as follows. We employ RoTTA [61] as the base test-time adaptor due to the characteristics of the practical TTA [ 61] stream. The model (including model 25parameters, the optimizer state, and the memory bank) is reset after adapting itself to T images.1 For each dataset, three values of this hyper-parameter T are selected: • T = 1, 000: This is the value selected by the RDumb’s authors [ 45]. Unless specifically stated, we use this value when reporting the performance of RDumb [45] in all other tables. • T = 10, 000 (CIFAR-10/100-C), T = 5, 000 (ImageNet-C) and T = 24, 237 (Domain- Net).2 This value is equal to the number of samples in the test set of a single corruption type, i.e., the model is reset exactly after visiting each Pi’s (see Sec. 3.1 for notations). For DomainNet [44], since the number of images within each domain is unequal, the average number of images is used instead. • T = 150, 000 (CIFAR-10/100-C), T = 75, 000 (ImageNet-C) and T = 72, 712 (Domain- Net). This number is equal to the number of samples in one recurrence of our recurring TTA, i.e., the model is reset exactly after visitingP1 → ··· → PD. Here, D = 15 - types of corruptions [19] for CIFAR-10/100-C and ImageNet-C and D = 3 for DomainNet (clipart, painting, sketch). For example, the model is reset 20 times within a recurring TTA setting with 20 recurrences under this choice of T. The second and the last reset scheme could be interpreted as assuming the model has access to an oracle model with a capability of signaling the transitions between domains, or recurrences. Typically, this is an unrealistic capability in real-world scenarios, and a desirable continual TTA algorithm should be able to operate independently without knowing when the domain shift happening. Experimental Results. An empirical comparison between RDumb [45] and our PeTTA are reported in Tab. 10, Tab. 11, Tab. 12 and Tab. 13 for all four tasks. Table 10: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-10→ CIFAR-10-C task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100031.1 32.1 32.3 31.6 31.9 31.8 31.8 31.9 31.9 32.1 31.7 32.0 32.5 32.0 31.9 31.6 31.9 31.4 32.3 32.431.9T= 1000025.8 25.9 26.5 26.1 26.4 25.4 25.8 25.8 26.1 26.2 26.1 26.1 26.1 26.1 26.1 25.9 25.5 25.5 25.7 26.226.0T= 15000024.8 25.3 24.3 24.1 25.3 25.4 25.4 24.5 25.0 24.9 25.0 24.8 25.0 24.5 24.9 24.1 24.0 24.7 24.9 24.424.8 PeTTA(ours)(∗) 24.323.022.622.422.422.522.322.522.822.822.622.722.722.922.622.722.622.822.923.022.8 Table 11: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on CIFAR-100-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100036.7 36.7 36.6 36.6 36.7 36.8 36.7 36.5 36.6 36.5 36.7 36.6 36.5 36.7 36.5 36.6 36.6 36.7 36.6 36.536.6T= 1000043.5 43.6 43.7 43.7 43.4 43.5 43.6 43.4 43.5 43.6 43.8 43.5 43.5 43.6 43.4 43.6 43.5 43.8 43.7 43.643.6T= 15000035.435.4 35.4 35.3 35.4 35.4 35.5 35.6 35.4 35.4 35.535.3 35.235.435.135.835.135.6 35.3 35.835.4 PeTTA(ours)(∗) 35.834.434.735.035.135.135.235.335.335.335.235.335.235.235.135.235.235.235.235.235.1 Table 12: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on DomainNet dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100044.3 44.4 44.3 44.5 44.2 44.2 44.3 44.5 44.4 44.2 44.3 44.3 44.3 44.3 44.5 44.3 44.2 44.3 44.4 44.344.3T= 2423744.1 44.3 43.9 44.2 44.1 44.3 44.2 44.4 44.1 44.1 44.0 44.3 44.1 44.0 44.0 44.2 44.1 44.1 44.1 44.444.1T= 7271244.3 44.3 44.0 44.3 44.1 44.3 44.2 44.4 44.2 44.1 44.0 44.1 44.2 44.1 44.1 44.1 44.1 44.0 44.0 44.344.2 PeTTA(ours)(∗) 43.842.642.342.342.642.842.843.042.942.943.143.042.943.043.043.143.042.842.942.942.9 Discussions. Across datasets and reset frequencies, our PeTTA approach is always better than RDumb [45]. The supreme performance holds even when RDumb has access to the oracle information that can reset the model exactly at the transition between each domain shift or recurrence. Importantly, this oracle information is typically unavailable in practice. 1A slight abuse of notation. T here is the number of images between two consecutive resets, following the notation on Sec. 3 of [45], not the sample indices in our notations. 2A subset of 5, 000 samples from ImageNet-C are selected following RobustBench [10] for a consistent evaluation with other benchmarks. 26Table 13: Average classification error comparison between RDumb [45] (a reset-based approach) with different reset frequencies and our PeTTA on ImageNet-C dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Reset Every1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg T= 100072.2 73.0 73.2 72.8 72.2 72.8 73.3 72.7 71.9 73.0 73.2 73.1 72.0 72.7 73.3 73.1 72.1 72.6 73.3 73.172.8T= 500070.2 70.8 71.6 72.1 72.4 72.6 72.9 73.1 73.2 73.6 73.7 73.9 74.0 74.0 74.3 74.1 74.1 73.8 73.5 71.973.0T= 7500067.0 67.1 67.2 67.5 67.5 67.6 67.8 67.6 67.6 67.6 67.5 67.7 67.6 67.9 68.1 67.9 67.4 67.5 67.7 67.567.6 PeTTA(ours)(∗) 65.361.759.859.159.459.659.859.359.460.060.361.060.760.460.660.760.860.760.460.260.5 Noteworthy, it is clear that the performance of RDumb varies when changing the choice of the reset frequency. For a given choice of T, the better performance on one dataset does not guarantee the same performance on other datasets. For example, T = 1, 000 - the best empirical value found by RDumb authors [45] on CCC, does not give the best performance on our recurring TTA scenario; the second choice of T negatively impact the performance on many tasks; the third choice gives the best results, but knowing this exact recurrence frequency of the testing stream is unrealistic. The result highlights the challenge in practice when tuning this parameter (too slow/frequent), especially in the TTA setting where a validation set is unavailable. Our PeTTA, in contrast, is reset-free. F.4 PeTTA with 40 Recurring Visits To demonstrate the persistence of PeTTA over an even longer testing stream, in Tab. 14 and Fig. 8, we provide the evaluation results of PeTTA on recurring with 40 recurrences. F.5 The Sensitivity of Hyper-parameter Choices in PeTTA Table 15: Sensitivity of PeTTA with different choices ofλ0. Dataset λ0 = 1e0 λ0 = 5e0 λ0 = 1e1 λ0 = 5e1 λ0 = 1e2 CIFAR-10-C 22.9 22.7 22.8 23.2 24.1 CIFAR-100-C 35.7 35.3 35.1 35.6 36.1 ImageNet-C 61.2 61.0 60.5 61.3 62.4 There are two hyper-parameters in PeTTA: α0 and λ0. The initial learning rate of α0 = 1e−3 is used for all experiments. We do not tune this hyper-parameter, and the choice of α0 is universal across all datasets, following the previous works/compared methods (e.g., RoTTA [61], CoTTA [59]). Since λ0 is more specific to PeTTA, we included a sensitive analysis with different choices of λ0 on PeTTA, evaluated with images from CIFAR-10/100-C and ImageNet-C in Tab. 15. Overall, the choice of λ0 is not extremely sensitive, and while the best value is1e1 on most datasets, other choices such as 5e0 or 5e1 also produce roughly similar performance. Selecting λ0 is intuitive, the larger value of λ0 stronger prevents the model from collapsing but also limits its adaptability as a trade-off. In action, λ0 is an initial value and will be adaptively scaled with the sensing model divergence mechanism in PeTTA, meaning it does not require careful tuning. More generally, this hyper- parameter can be tuned similarly to the hyper-parameters of other TTA approaches, via an additional validation set, or some accuracy prediction algorithm [29] when labeled data is unavailable. F.6 More Details on the Ablation Study We provide the detailed classification error for each visit in the recurring TTA setting of each row entry in Tab. 4 (PeTTA Ablation Study): Tab. 16, Tab. 17, Tab. 18, Tab. 19; and Tab. 5 (PeTTA with various choices of regularizers): Tab. 20, Tab. 21, Tab. 22, Tab. 23. Fig. 9 presents an additional examination of the ablation study conducted on the task CIFAR-100 → CIFAR-100-C [19] for our PeTTA approach. We plot the classification error (top) and the value of ¯γt (bottom) for various PeTTA variations. As the model diverges from the initial state, the value of ¯γt increases. Unable to adjust αt or constraint the probability space via LAL limits the ability of PeTTA to prevent model collapse. In all variations with the model collapse in ablation studies, the rapid saturation of ¯γt is all observed. Therefore, incorporating all components in PeTTA is necessary. 27Table 16: Average classification error of multiple variations of PeTTA. Experiments on CIFAR10→ CIFAR10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 23.5 24.0 27.4 29.9 33.4 35.6 38.0 40.7 43.1 45.0 46.0 48.6 50.0 49.7 50.8 51.5 52.3 53.3 54.3 55.542.6 R(θ)fixedλ= 0.1λ0 23.5 24.0 27.2 29.8 33.4 35.3 37.9 40.5 43.3 45.3 46.8 49.3 50.9 51.0 52.1 53.2 54.0 54.8 56.0 57.643.3R(θ)fixedλ=λ0 23.5 23.6 26.2 28.4 31.6 33.5 36.4 38.7 41.1 43.1 44.8 47.6 49.3 49.5 50.9 52.1 53.1 54.2 55.6 57.042.0 PeTTA-λt 24.9 25.3 26.0 26.4 27.2 26.5 27.2 27.1 27.4 27.7 27.8 28.0 27.5 28.0 27.7 27.4 27.0 27.6 27.8 27.827.1PeTTA-λt+αt 25.5 24.5 23.7 23.1 23.222.423.3 23.2 23.7 24.1 23.9 24.5 24.3 24.0 23.8 23.9 23.8 24.1 24.6 24.723.9PeTTA-λt+LAL 23.323.9 24.6 25.3 26.2 25.9 26.4 26.6 26.9 26.6 26.7 26.7 26.7 26.8 26.8 27.2 26.9 26.9 26.8 27.026.2 PeTTAαt+LAL 24.323.0 22.6 22.4 22.422.522.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8 Table 17: Average classification error of multiple variations of PeTTA. Experiments on CIFAR-100 → CIFAR100-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 40.2 46.3 51.2 54.4 57.3 59.4 61.3 62.6 63.9 65.1 66.3 67.1 68.1 68.9 69.6 70.3 71.1 71.6 72.4 72.963.0 R(θ)fixedλ= 0.1λ0 40.5 46.1 51.5 55.1 58.2 60.5 62.6 64.2 65.7 67.3 68.6 69.5 70.6 71.6 72.5 73.4 74.2 74.9 75.8 76.565.0R(θ)fixedλ=λ0 41.8 47.6 52.6 56.1 58.9 60.7 62.5 63.9 65.0 66.2 67.1 68.3 69.5 70.3 71.4 72.4 73.4 74.1 75.0 75.664.6 PeTTA-λt 39.4 43.4 46.6 49.1 51.0 52.6 53.8 54.7 55.7 56.5 57.1 57.7 58.3 58.8 59.3 59.9 60.6 61.0 61.6 62.155.0PeTTA-λt+αt 39.4 40.1 40.8 40.7 41.2 41.5 41.4 41.6 41.5 41.5 41.7 41.6 41.8 41.7 41.8 42.0 41.9 41.9 42.0 41.841.4PeTTA-λt+LAL 36.2 35.6 35.7 36.1 36.2 36.4 36.4 36.5 36.2 36.2 36.6 36.5 36.5 36.6 36.5 36.6 36.5 36.5 36.3 36.536.3 PeTTAλt+αt+LAL 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1 Table 18: Average classification error of multiple variations of PeTTA. Experiments onreal → clipart, painting, sketch task from DomainNet [44] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 52.3 69.0 68.6 68.6 69.4 70.5 71.8 73.4 75.6 77.6 78.8 81.0 82.8 84.3 85.9 87.4 88.5 89.9 90.8 92.177.9 R(θ)fixedλ= 0.1λ0 52.5 70.0 69.8 70.0 71.1 72.5 74.6 76.1 77.8 80.4 81.9 83.5 85.2 87.2 89.1 90.2 91.5 93.2 94.1 94.980.0R(θ)fixedλ=λ0 54.6 69.8 63.7 56.0 61.7 76.4 70.4 62.5 58.2 76.0 73.6 66.8 58.6 62.3 80.8 75.5 67.0 59.9 59.3 78.366.6 PeTTA-λt 49.2 64.5 62.4 60.9 59.6 58.6 57.7 57.8 57.6 57.7 58.0 58.5 59.0 59.5 59.8 61.1 62.0 62.6 63.6 64.959.7PeTTA-λt+αt 43.942.5 42.3 42.3 42.6 42.843.1 43.7 43.9 44.3 44.6 45.1 45.4 45.7 45.7 46.1 46.1 46.2 46.5 46.444.5PeTTA-λt+LAL 43.6 42.542.6 42.6 42.9 43.0 43.3 43.4 43.1 43.243.143.3 43.3 43.2 43.2 43.9 43.7 43.0 43.2 43.543.2 PeTTAλt+αt+LAL 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9 Table 19: Average classification error of multiple variations of PeTTA. Experiments on ImageNet→ ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg Baseline w/oR(θ) 66.9 61.9 72.7 93.6 97.4 97.8 98.0 98.2 98.3 98.3 98.4 98.4 98.5 98.5 98.6 98.6 98.6 98.6 98.7 98.793.4 R(θ)fixedλ= 0.1λ0 65.5 70.9 79.1 85.2 90.3 92.6 95.8 95.8 95.4 97.3 96.9 97.7 97.9 98.2 98.0 98.7 98.6 98.4 98.4 98.792.5R(θ)fixedλ=λ0 66.5 62.1 73.0 93.5 97.0 97.2 97.5 97.5 97.6 97.5 97.7 97.7 97.7 97.8 97.9 97.9 98.0 98.0 98.0 97.992.9 PeTTA-λt 65.9 62.1 76.3 96.7 97.0 96.9 96.9 96.9 97.0 97.1 97.0 97.2 97.0 97.1 97.1 97.0 97.0 97.0 97.0 97.092.7PeTTA-λt+αt 64.870.5 74.6 75.8 75.5 75.8 76.1 76.2 76.2 76.5 76.7 77.0 76.9 77.4 77.1 77.3 77.2 77.4 77.6 77.475.7PeTTA-λt+LAL 64.8 61.160.0 59.8 60.4 60.4 61.2 61.2 61.8 61.9 62.1 62.2 62.1 62.9 62.1 62.8 62.7 62.1 62.8 66.662.0 PeTTA(ours)(∗) 65.3 61.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5 Table 20: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-10 → CIFAR-10-C [19] task. Episodic TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 25.6 24.8 23.8 23.1 23.2 22.7 23.0 22.7 22.7 22.7 22.8 22.7 22.8 22.7 22.522.3 22.2 22.4 22.7 22.823.0L2+Fisher25.2 23.7 22.5 21.8 22.3 21.5 22.3 22.1 22.5 22.8 22.6 22.622.622.8 22.6 22.9 22.6 22.9 23.0 23.322.7 Cosine 24.3 23.022.6 22.4 22.4 22.5 22.3 22.5 22.8 22.8 22.6 22.7 22.7 22.9 22.6 22.7 22.6 22.8 22.9 23.022.8Cosine+Fisher25.1 23.822.2 21.6 22.0 21.4 22.0 21.8 22.1 22.3 22.5 22.4 22.6 22.6 22.422.7 22.6 22.8 22.8 23.322.6 Table 21: Average classification error of PeTTA with various choices of regularizers. Experiments on CIFAR-100 → CIFAR-100-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 36.9 35.5 35.5 35.5 35.7 35.6 35.6 35.5 35.5 35.4 35.6 35.5 35.7 35.7 35.7 35.7 35.8 35.5 35.4 35.535.6L2+Fisher36.8 35.4 35.4 35.8 35.9 36.0 35.9 35.9 35.9 35.8 36.1 36.1 36.1 36.1 36.1 36.1 36.2 36.0 36.0 35.936.0 Cosine 35.8 34.4 34.7 35.0 35.1 35.1 35.2 35.3 35.3 35.3 35.2 35.3 35.2 35.2 35.1 35.2 35.2 35.2 35.2 35.235.1Cosine+Fisher36.7 35.2 35.5 35.6 35.9 35.9 36.1 36.0 36.0 35.9 36.0 36.0 36.0 36.1 36.0 36.0 35.9 35.9 35.9 36.035.9 28Table 22: Average classification error of PeTTA with various choices of regularizers. Experiments on real → clipart, painting, sketch task from DomainNet [44] dataset. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 43.8 42.7 42.5 42.4 42.8 42.9 43.0 43.1 43.1 43.2 43.4 43.3 43.2 43.3 43.2 43.2 43.4 43.0 43.1 43.143.1L2+Fisher43.9 42.8 42.7 43.0 43.2 43.4 43.6 43.8 43.9 44.1 44.0 44.2 44.2 44.2 44.4 44.4 44.5 44.5 44.5 44.543.9 Cosine 43.8 42.642.3 42.3 42.6 42.8 42.8 43.0 42.9 42.9 43.1 43.0 42.9 43.0 43.0 43.1 43.0 42.8 42.9 42.942.9Cosine+Fisher43.7 42.542.5 42.6 42.9 43.2 43.2 43.5 43.4 43.5 43.4 43.5 43.4 43.6 43.5 43.5 43.4 43.5 43.3 43.443.3 Table 23: Average classification error of PeTTA with various choices of regularizers. Experiments on ImageNet → ImageNet-C [19] task. Recurring TTA visit− − − − − − − − − − − − − − − − − − − − − − − − − →Method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Avg L2 70.8 72.2 71.5 69.8 72.3 69.3 70.3 70.5 70.0 70.8 70.2 72.1 71.4 70.8 70.9 70.9 69.7 71.0 71.1 70.470.8L2+Fisher70.5 70.0 69.5 69.4 69.6 69.9 69.2 69.3 72.2 70.4 71.0 70.5 71.7 71.5 71.3 68.4 68.6 68.8 68.7 68.770.0 Cosine 65.361.7 59.8 59.1 59.4 59.6 59.8 59.3 59.4 60.0 60.3 61.0 60.7 60.4 60.6 60.7 60.8 60.7 60.4 60.260.5Cosine+Fisher65.1 61.760.9 61.2 61.9 62.6 62.8 63.2 64.2 63.4 64.3 64.4 63.9 64.3 65.8 65.5 64.9 65.0 65.2 65.263.8 F.7 More Confusion Matrices in Recurring TTA Setting For the task CIFAR-10→ CIFAR-10-C [19] in recurring TTA setting (with 20 visits), we additionally showcase the confusion matrix of RoTTA [61] (Fig. 10) and our proposed PeTTA (Fig. 11) at each visit. Our PeTTA persistently achieves competitive performance across 20 visits while RoTTA [61] gradually degrades. G Experimental Details G.1 Computing Resources A computer cluster equipped with an Intel(R) Core(TM) 3.80GHz i7-10700K CPU, 64 GB RAM, and one NVIDIA GeForce RTX 3090 GPU (24 GB VRAM) is used for our experiments. G.2 Experiments on CCC Testing Stream In this section, we further evaluate the performance of our PeTTA on the testing data stream of Continuous Changing Corruption (CCC) [ 45] setting. Here we use the baseline accuracy 20%, transition speed 1000, and random seed 44.3 The compared methods are source model (ResNet 50), PeTTA, RoTTA [61], and RDumb [45]. Noteworthy, different from recurring TTA, the class labels here are i.i.d. distributed. The adaptation configuration of PeTTA follows the same settings as used on ImageNet-C, while the same setting introduced in Sec. F.3, with T = 1000 is used for RDumb [45]. G.3 Test-time Adaptation Methods Pre-trained Model on Source Distribution. Following previous studies [57, 61, 12, 59], only the batch norm layers are updated. As stated in Sec. 5.2, RobustBench [10] and torchvision [35] provide pre-trained models trained on source distributions. Specifically, for ImageNet-C and Do- mainNet experiments, a ResNet50 model [17] pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNeXt [20] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. Lastly, experiments on DomainNet dataset utilize the checkpoint (best_real_2020) provided in AdaContrast [8] study.4 Optimizer. Without specifically stated, Adam [26] optimizer with learning rate equal 1e−3, and β = (0.9, 0.999) is selected as a universal choice for all experiments. More Details on PeTTA. Since designing the batch normalization layers, and the memory bank is not the key focus of PeTTA, we conveniently adopt the implementation of the Robust Batch Norm layer and the Category-balanced Sampling strategy using a memory bank introduced in RoTTA [61]. 3https://github.com/oripress/CCC 4https://github.com/DianCh/AdaContrast 29G.4 The Use of Existing Assets Many components of PeTTA is utilized from the official repository of RoTTA [61] 5 and RMT [12]. 6 These two assets are released under MIT license. All the datasets, including CIFAR-10-C, CIFAR- 100-C and ImageNet-C [ 19] are publicly available online, released under Apache-2.0 license. 7 DomainNet dataset [44] (cleaned version) is also released for research purposes.8 5https://github.com/BIT-DA/RoTTA 6https://github.com/mariodoebler/test-time-adaptation 7https://github.com/hendrycks/robustness 8https://ai.bu.edu/M3SDA/ 300 10000 20000 30000 40000 Test-time adaptation step (t) 0.40 0.60 0.80 1.00 ¯γt 0 10000 20000 30000 40000 Test-time adaptation step (t) 4.00 6.00 8.00 10.00λt 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00αt 1e 3 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 1.00 2.00 3.00 4.00LCLS 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 2.50 5.00 7.50 10.00LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.50 1.00 1.50 2.00 R(θ) 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.00 0.25 0.50 0.75 1.00Testing error Figure 7: An inspection of PeTTA on the task CIFAR-10 → CIFAR-10-C [19] in a recurring with 20 visits (visits are separated by the vertical dashed lines). Here, we visualize (rows 1-3) the dynamic of PeTTA adaptive parameters (¯γt, λt, αt), (rows 4-5) the value of the loss functions (LCLS, LAL) and (row 6) the value of the regularization term (R(θ)) and (row 7) the classification error rate at each step. The solid line in the foreground of each plot denotes the running mean. The plots show an adaptive change of λt, αt through time in PeTTA, which stabilizes TTA performance, making PeTTA achieve a persisting adaptation process in all observed values across 20 visits. 31Figure 8: Testing error of PeTTA with 40 recurring TTA visits. Total Visits CF-10-C CF-100-C IN-C 20 visits 22.8 35.1 60.5 40 visits 22.9 35.1 61.0 Table 14: Average testing error of PeTTA in recurring TTA with 20 and 40 visits. PeTTA demonstrates its persistence over an extended testing time horizon beyond the 20 th visit milestone (Fig. 8’s horizontal dashed line). 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.30 0.40 0.50 0.60 0.70 0.80Testing Error PeTTA - λt Baseline w/o R(θ) PeTTA - λt + αt R(θ) fixed λ= 0.1λ0 PeTTA - λt + LAL R(θ) fixed λ= λ0 PeTTA - λt  + αt  + LAL 0 10000 20000 30000 40000 Test-time adaptation step (t) 0.20 0.40 0.60 0.80 1.00 ¯γt PeTTA - λt PeTTA - λt + αt PeTTA - λt + LAL PeTTA - λt  + αt  + LAL Figure 9: An inspection on the ablation study of multiple variations of PeTTA on the task CIFAR-100 → CIFAR-100-C [19] in an episodic TTA with 20 visits (visits are separated by the vertical dashed lines). (top): testing error of multiple variations of PeTTA. The performance of PeTTA without (w/o) R(θ), or fixed regularization coefficient ( λ = λ0/0.1λ0) degrades through time (the top 3 lines). The degradation of PeTTA -λt is still happening but at a slower rate (justification below). The performance of the other three variations persists through time with PeTTA -λt + αt + LAL achieves the best performance. (bottom): changes of ¯γt in multiple variations of PeTTA. When limiting the degree of freedom in adjusting αt or lacking of supervision from LAL (e.g., PeTTA -λt + αt, PeTTA -λt + LAL, and especially PeTTA -λt), the value of γt, unfortunately, escalates and eventually saturated. After this point, PeTTA has the same effect as using a fixed regularization coefficient. Therefore, fully utilizing all components is necessary to preserve the persistence of PeTTA. Best viewed in color. 320: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.79 0.01 0.04 0.03 0.02 0.01 0.01 0.02 0.05 0.02 0.02 0.82 0.01 0.01 0 0.01 0.01 0.01 0.01 0.09 0.06 0 0.68 0.07 0.04 0.03 0.06 0.03 0.01 0.01 0.02 0.01 0.04 0.66 0.04 0.08 0.07 0.05 0.01 0.02 0.03 0 0.04 0.06 0.68 0.02 0.06 0.09 0.01 0.01 0.03 0 0.05 0.15 0.03 0.61 0.03 0.07 0.01 0.01 0.02 0.01 0.03 0.07 0.02 0.02 0.8 0.02 0 0.01 0.01 0 0.02 0.03 0.03 0.02 0.01 0.87 0 0.01 0.09 0.02 0.02 0.02 0.01 0 0.02 0.01 0.77 0.04 0.03 0.03 0.01 0.01 0 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.03 0.03 0.01 0 0.03 0.02 0.07 0.03 0.02 0.76 0 0.01 0 0 0.03 0.01 0.02 0.16 0.07 0 0.63 0.08 0.06 0.02 0.08 0.04 0.01 0.01 0.02 0 0.04 0.7 0.04 0.04 0.09 0.05 0.01 0.02 0.03 0 0.03 0.05 0.73 0.01 0.06 0.08 0.01 0.01 0.01 0 0.03 0.23 0.04 0.53 0.06 0.08 0.01 0.01 0.02 0 0.02 0.1 0.02 0.01 0.81 0.01 0 0.01 0.01 0 0.01 0.05 0.03 0.01 0.01 0.87 0 0.01 0.08 0.01 0.01 0.02 0.01 0 0.02 0.01 0.8 0.04 0.03 0.02 0.01 0.02 0 0 0.02 0.01 0.02 0.87 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.7 0.01 0.03 0.04 0.02 0 0.03 0.03 0.09 0.06 0.01 0.72 0 0.01 0 0 0.04 0 0.01 0.2 0.07 0 0.56 0.1 0.08 0.02 0.09 0.04 0.01 0.02 0.01 0 0.03 0.7 0.05 0.02 0.13 0.04 0 0.02 0.04 0 0.03 0.07 0.69 0 0.08 0.07 0.01 0.01 0.01 0 0.04 0.26 0.05 0.42 0.13 0.07 0 0.01 0.01 0 0.02 0.11 0.03 0 0.8 0.01 0 0.01 0.01 0 0.02 0.06 0.05 0.01 0.04 0.8 0 0.01 0.07 0.01 0.01 0.03 0.01 0 0.03 0.01 0.78 0.05 0.02 0.01 0.01 0.02 0.01 0 0.04 0.01 0.02 0.86 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0.01 0.03 0.06 0.03 0 0.05 0.04 0.09 0.08 0.01 0.66 0 0.02 0.01 0 0.04 0 0.02 0.25 0.07 0 0.48 0.13 0.1 0.02 0.13 0.03 0.01 0.02 0.01 0 0.02 0.68 0.05 0.02 0.17 0.03 0 0.02 0.03 0 0.02 0.07 0.67 0 0.12 0.07 0.01 0.01 0.01 0 0.02 0.29 0.07 0.39 0.14 0.06 0 0.01 0.01 0 0.01 0.11 0.04 0 0.8 0.01 0 0.01 0.01 0 0.02 0.08 0.06 0.01 0.06 0.75 0 0.01 0.05 0.01 0.01 0.04 0.02 0 0.05 0.01 0.74 0.07 0.01 0.01 0 0.03 0.01 0 0.05 0.01 0.02 0.86 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0.03 0.07 0.04 0 0.07 0.04 0.1 0.1 0.01 0.61 0 0.01 0.01 0 0.07 0 0.02 0.26 0.08 0 0.42 0.13 0.13 0.02 0.15 0.03 0.01 0.02 0.02 0 0.01 0.62 0.06 0.02 0.21 0.03 0 0.02 0.03 0 0.02 0.06 0.66 0 0.16 0.06 0.01 0.01 0.01 0 0.02 0.3 0.08 0.34 0.17 0.06 0 0.02 0.01 0 0.01 0.12 0.07 0 0.76 0.01 0 0.02 0.01 0 0.02 0.1 0.08 0.01 0.08 0.69 0 0.02 0.05 0.01 0.01 0.05 0.02 0 0.09 0.01 0.68 0.09 0.01 0.01 0 0.03 0.02 0 0.09 0.01 0.02 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.51 0 0.02 0.07 0.04 0 0.09 0.03 0.1 0.14 0.01 0.56 0 0.01 0.02 0 0.09 0 0.02 0.29 0.08 0 0.35 0.15 0.16 0.02 0.18 0.03 0.01 0.03 0.02 0 0.01 0.57 0.07 0.02 0.27 0.02 0 0.03 0.04 0 0.01 0.08 0.62 0 0.18 0.05 0.01 0.01 0.01 0 0.01 0.29 0.09 0.3 0.21 0.05 0 0.02 0.01 0 0.01 0.12 0.09 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.12 0.01 0.1 0.6 0 0.03 0.06 0.01 0 0.04 0.02 0 0.09 0 0.66 0.11 0.01 0.01 0 0.02 0.03 0 0.11 0 0.02 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.48 0 0.02 0.08 0.04 0 0.11 0.03 0.11 0.13 0.01 0.54 0 0.01 0.02 0 0.11 0 0.02 0.28 0.09 0 0.3 0.16 0.16 0.02 0.21 0.02 0.01 0.03 0.02 0 0.01 0.51 0.08 0.01 0.33 0.01 0.01 0.02 0.03 0 0.01 0.05 0.65 0 0.21 0.03 0.01 0.01 0.02 0 0.01 0.27 0.11 0.25 0.28 0.03 0 0.02 0.01 0 0.01 0.12 0.1 0 0.75 0 0 0.01 0.02 0 0.01 0.11 0.13 0.01 0.13 0.56 0 0.03 0.06 0 0 0.06 0.03 0 0.13 0 0.6 0.11 0.02 0.01 0 0.03 0.04 0 0.15 0 0.02 0.73 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.07 0.06 0 0.13 0.01 0.09 0.15 0.01 0.48 0 0.01 0.04 0 0.16 0 0.01 0.28 0.09 0 0.27 0.15 0.19 0.01 0.23 0.01 0.01 0.03 0.02 0 0.01 0.44 0.12 0.01 0.37 0.01 0.01 0.02 0.04 0 0.01 0.05 0.63 0 0.23 0.02 0.01 0.01 0.02 0 0.01 0.25 0.13 0.22 0.33 0.02 0 0.01 0.01 0 0 0.11 0.15 0 0.71 0 0 0.01 0.02 0 0.01 0.09 0.22 0 0.15 0.47 0 0.02 0.08 0 0 0.06 0.05 0 0.15 0 0.55 0.1 0.02 0.01 0 0.04 0.05 0 0.16 0 0.02 0.7 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.06 0.06 0 0.13 0.01 0.1 0.16 0.02 0.47 0 0.01 0.04 0 0.13 0 0.03 0.29 0.1 0 0.24 0.12 0.22 0.01 0.24 0.01 0.01 0.03 0.03 0 0 0.4 0.12 0.01 0.39 0 0.01 0.02 0.05 0 0.01 0.06 0.61 0 0.23 0.02 0.01 0.01 0.03 0 0.01 0.22 0.15 0.2 0.35 0.02 0 0.02 0.01 0 0 0.11 0.15 0 0.7 0 0.01 0.01 0.03 0 0.01 0.08 0.25 0 0.15 0.44 0.01 0.03 0.09 0 0 0.04 0.07 0 0.14 0 0.55 0.1 0.02 0.01 0 0.03 0.05 0 0.16 0 0.02 0.7 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.46 0 0.01 0.05 0.07 0 0.14 0.01 0.1 0.16 0.04 0.43 0 0.02 0.07 0 0.14 0 0.03 0.27 0.11 0 0.22 0.11 0.23 0.01 0.26 0.01 0.02 0.03 0.04 0 0 0.33 0.16 0.01 0.43 0 0.01 0.02 0.05 0 0 0.03 0.66 0 0.23 0.01 0.01 0.01 0.04 0 0.01 0.22 0.15 0.18 0.37 0.01 0.01 0.02 0.01 0 0 0.1 0.19 0 0.68 0 0.01 0.01 0.03 0 0.01 0.08 0.28 0.01 0.16 0.41 0.01 0.03 0.11 0 0 0.04 0.05 0 0.14 0 0.56 0.09 0.04 0.01 0 0.02 0.08 0 0.18 0 0.02 0.65 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.47 0 0.01 0.04 0.07 0 0.14 0 0.1 0.16 0.04 0.42 0 0.01 0.07 0 0.15 0 0.05 0.26 0.11 0 0.21 0.1 0.26 0.01 0.26 0 0.02 0.03 0.05 0 0 0.31 0.18 0.01 0.42 0 0.01 0.02 0.06 0 0 0.04 0.65 0 0.21 0.01 0.01 0.02 0.04 0 0.01 0.17 0.21 0.15 0.39 0.01 0.01 0.02 0.01 0 0 0.1 0.24 0 0.64 0 0.01 0.01 0.04 0 0.01 0.09 0.28 0 0.16 0.39 0 0.03 0.14 0 0 0.03 0.07 0 0.14 0 0.52 0.09 0.05 0.01 0 0.03 0.1 0 0.18 0 0.03 0.61 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.49 0 0.01 0.03 0.06 0 0.14 0 0.11 0.17 0.07 0.4 0 0.01 0.07 0 0.12 0 0.07 0.27 0.13 0 0.19 0.08 0.27 0.01 0.25 0 0.02 0.03 0.07 0 0 0.27 0.19 0 0.43 0 0.02 0.03 0.07 0 0 0.02 0.64 0 0.23 0.01 0.01 0.01 0.06 0 0.01 0.19 0.18 0.13 0.39 0.01 0.01 0.02 0.02 0 0 0.09 0.22 0 0.65 0 0.01 0.01 0.05 0 0 0.07 0.32 0 0.15 0.36 0.01 0.04 0.17 0 0 0.03 0.07 0 0.12 0 0.53 0.08 0.06 0.01 0 0.01 0.13 0 0.17 0 0.03 0.59 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.5 0 0 0.02 0.08 0 0.13 0 0.1 0.15 0.09 0.37 0 0.01 0.11 0 0.11 0 0.08 0.24 0.15 0 0.18 0.07 0.31 0.01 0.24 0 0.03 0.02 0.09 0 0 0.24 0.17 0 0.44 0 0.02 0.03 0.08 0 0 0.02 0.66 0 0.19 0.01 0.02 0.02 0.08 0 0.01 0.15 0.23 0.11 0.38 0.01 0.01 0.02 0.02 0 0 0.08 0.31 0 0.55 0 0.02 0.01 0.05 0 0 0.05 0.37 0 0.14 0.34 0.01 0.04 0.2 0 0 0.03 0.06 0 0.12 0 0.52 0.08 0.08 0.01 0 0.01 0.11 0 0.15 0 0.04 0.59 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.54 0 0 0.02 0.06 0 0.11 0 0.12 0.15 0.13 0.35 0 0.01 0.1 0 0.09 0 0.12 0.21 0.16 0 0.18 0.07 0.29 0.01 0.24 0 0.03 0.02 0.11 0 0 0.22 0.19 0 0.42 0 0.03 0.03 0.08 0 0 0.03 0.65 0 0.2 0.01 0.02 0.01 0.09 0 0.01 0.12 0.29 0.08 0.37 0 0.02 0.02 0.02 0 0 0.09 0.29 0 0.56 0 0.02 0.01 0.06 0 0 0.05 0.39 0 0.13 0.32 0.01 0.04 0.23 0 0 0.02 0.07 0 0.1 0 0.51 0.07 0.12 0.01 0 0.01 0.11 0 0.13 0 0.05 0.57 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.56 0 0 0.02 0.08 0 0.1 0 0.12 0.12 0.18 0.32 0 0 0.11 0 0.08 0 0.13 0.19 0.18 0 0.15 0.05 0.34 0 0.2 0 0.04 0.02 0.12 0 0 0.19 0.27 0 0.36 0 0.04 0.02 0.09 0 0 0.02 0.69 0 0.15 0.01 0.02 0.02 0.11 0 0 0.1 0.33 0.07 0.33 0 0.03 0.01 0.03 0 0 0.09 0.35 0 0.5 0 0.02 0.01 0.08 0 0 0.04 0.43 0 0.1 0.29 0.01 0.04 0.26 0 0 0.02 0.08 0 0.08 0 0.51 0.06 0.15 0.01 0 0.01 0.12 0 0.1 0 0.07 0.55 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.58 0 0 0.01 0.07 0 0.09 0 0.13 0.11 0.16 0.32 0 0 0.11 0 0.07 0 0.16 0.18 0.18 0 0.15 0.05 0.36 0 0.19 0 0.04 0.02 0.14 0 0 0.18 0.26 0 0.35 0 0.05 0.02 0.1 0 0 0.01 0.69 0 0.15 0.01 0.03 0.01 0.11 0 0 0.1 0.36 0.05 0.32 0 0.04 0.01 0.03 0 0 0.08 0.38 0 0.46 0 0.03 0.01 0.09 0 0 0.04 0.43 0 0.09 0.29 0.02 0.04 0.29 0 0 0.02 0.09 0 0.08 0 0.47 0.06 0.18 0.01 0 0.01 0.11 0 0.08 0 0.1 0.5 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.6 0 0 0.01 0.08 0 0.08 0 0.13 0.1 0.2 0.28 0 0 0.1 0 0.06 0 0.19 0.17 0.2 0 0.14 0.05 0.36 0 0.18 0 0.05 0.02 0.17 0 0 0.16 0.28 0 0.29 0 0.08 0.02 0.1 0 0 0.01 0.71 0 0.11 0.01 0.04 0.02 0.13 0 0 0.1 0.4 0.04 0.27 0 0.05 0.01 0.04 0 0 0.09 0.4 0 0.41 0 0.04 0.01 0.1 0 0 0.04 0.45 0 0.07 0.27 0.03 0.04 0.34 0 0 0.01 0.08 0 0.05 0 0.47 0.05 0.22 0.01 0 0.01 0.13 0 0.06 0 0.12 0.44 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.62 0 0 0.01 0.09 0 0.08 0 0.13 0.08 0.24 0.26 0 0 0.1 0 0.05 0 0.19 0.15 0.2 0 0.13 0.04 0.41 0 0.16 0 0.05 0.02 0.16 0 0 0.14 0.3 0 0.29 0 0.09 0.02 0.11 0 0 0.01 0.7 0 0.1 0.01 0.05 0.02 0.14 0 0 0.09 0.42 0.02 0.27 0 0.05 0.01 0.03 0 0 0.09 0.44 0 0.39 0 0.04 0.01 0.12 0 0 0.04 0.43 0 0.06 0.28 0.03 0.04 0.35 0 0 0.01 0.07 0 0.06 0 0.46 0.04 0.26 0.01 0 0.01 0.13 0 0.06 0 0.13 0.41 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.67 0 0 0 0.1 0 0.05 0 0.11 0.06 0.3 0.21 0 0 0.1 0 0.03 0 0.21 0.13 0.26 0 0.11 0.04 0.4 0 0.11 0 0.06 0.02 0.2 0 0 0.13 0.32 0 0.21 0 0.12 0.02 0.13 0 0 0.01 0.72 0 0.07 0.01 0.05 0.01 0.2 0 0 0.09 0.42 0.01 0.19 0 0.08 0.01 0.04 0 0 0.08 0.49 0 0.3 0 0.07 0.01 0.16 0 0 0.03 0.45 0 0.04 0.24 0.05 0.03 0.42 0 0 0.01 0.06 0 0.04 0 0.43 0.04 0.33 0.01 0 0 0.11 0 0.03 0 0.15 0.36 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.69 0 0 0 0.08 0 0.04 0 0.13 0.05 0.34 0.17 0 0 0.1 0 0.03 0 0.23 0.13 0.24 0 0.1 0.03 0.44 0 0.11 0 0.07 0.01 0.21 0 0 0.11 0.32 0 0.21 0 0.14 0.01 0.14 0 0 0.01 0.71 0 0.06 0.01 0.06 0.01 0.21 0 0 0.07 0.44 0 0.16 0 0.1 0.01 0.05 0 0 0.08 0.51 0 0.25 0 0.1 0.01 0.17 0 0 0.03 0.46 0 0.04 0.21 0.06 0.04 0.46 0 0 0.01 0.06 0 0.03 0 0.41 0.03 0.34 0 0 0 0.12 0 0.03 0 0.18 0.32 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 10: The dynamic of the confusion matrix of RoTTA [61] in episodic TTA with 20 visits. 330: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.03 0.01 0.02 0.02 0.05 0.02 0.02 0.84 0.01 0.02 0 0.01 0.02 0.01 0.02 0.06 0.04 0 0.69 0.07 0.05 0.05 0.05 0.02 0.01 0.01 0.04 0.01 0.05 0.62 0.05 0.1 0.06 0.04 0.01 0.02 0.03 0 0.06 0.07 0.68 0.05 0.04 0.05 0.01 0.01 0.01 0 0.04 0.14 0.03 0.7 0.03 0.04 0.01 0.01 0.01 0.01 0.04 0.06 0.03 0.03 0.78 0.01 0.01 0.01 0.03 0 0.03 0.04 0.04 0.04 0.01 0.79 0.01 0.01 0.08 0.02 0.02 0.02 0.01 0.01 0.02 0.01 0.8 0.03 0.03 0.05 0.02 0.02 0.01 0.01 0.01 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.77 0.01 0.04 0.03 0.02 0.01 0.03 0.01 0.06 0.02 0.01 0.87 0.01 0.01 0 0.01 0.01 0 0.02 0.05 0.04 0 0.7 0.09 0.05 0.03 0.06 0.02 0.01 0.01 0.03 0.01 0.06 0.64 0.05 0.08 0.06 0.04 0.01 0.02 0.02 0 0.05 0.06 0.74 0.03 0.05 0.04 0.01 0.01 0.01 0 0.05 0.15 0.04 0.66 0.04 0.04 0.01 0.01 0.02 0.01 0.04 0.06 0.02 0.02 0.78 0.01 0.01 0.03 0.02 0 0.03 0.05 0.05 0.03 0.01 0.81 0 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0.01 0.83 0.03 0.02 0.05 0.01 0.02 0.01 0.01 0.01 0.01 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.03 0.02 0 0.03 0.01 0.06 0.02 0.02 0.87 0.01 0.02 0 0 0.01 0 0.02 0.05 0.05 0 0.7 0.07 0.05 0.03 0.06 0.02 0.01 0.01 0.02 0.01 0.05 0.68 0.05 0.07 0.07 0.03 0.01 0.02 0.02 0 0.05 0.06 0.77 0.02 0.04 0.03 0 0 0.01 0 0.07 0.15 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.03 0.02 0.83 0.01 0 0.01 0.01 0 0.03 0.04 0.04 0.02 0.01 0.82 0 0.01 0.06 0.02 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.01 0.02 0.01 0 0.01 0.01 0.03 0.85 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.05 0.04 0.02 0 0.02 0.01 0.07 0.03 0.01 0.87 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.03 0.06 0.01 0.01 0.01 0.01 0.01 0.05 0.71 0.05 0.06 0.06 0.03 0.01 0.01 0.02 0 0.04 0.05 0.78 0.02 0.04 0.03 0.01 0 0.01 0 0.06 0.17 0.04 0.64 0.04 0.03 0.01 0.01 0.01 0 0.03 0.06 0.03 0.01 0.85 0.01 0 0.01 0.01 0 0.04 0.04 0.05 0.02 0.01 0.81 0.01 0.01 0.05 0.02 0.01 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.02 0.01 0 0.02 0.01 0.04 0.83 True label 1st visit 2nd visit 3rd visit 4th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.02 0.04 0.04 0.02 0 0.02 0.01 0.08 0.02 0.02 0.86 0.01 0.02 0 0 0.01 0 0.02 0.05 0.04 0 0.73 0.07 0.05 0.03 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.69 0.05 0.06 0.07 0.02 0.01 0.01 0.02 0 0.05 0.07 0.76 0.02 0.04 0.03 0.01 0 0.01 0 0.07 0.17 0.04 0.64 0.03 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.84 0.01 0.01 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.81 0.01 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.03 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.05 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.05 0.7 0.06 0.06 0.07 0.02 0.01 0.01 0.01 0 0.04 0.06 0.79 0.02 0.04 0.02 0.01 0 0.01 0 0.06 0.17 0.04 0.65 0.04 0.03 0.01 0.01 0.01 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0.01 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.87 0.02 0.02 0.05 0.02 0.02 0 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.76 0.01 0.04 0.04 0.02 0 0.03 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.01 0 0.02 0.05 0.04 0 0.74 0.06 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.06 0.68 0.05 0.06 0.08 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.04 0.02 0 0 0.01 0 0.07 0.18 0.05 0.61 0.04 0.03 0.01 0.01 0 0 0.03 0.06 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.04 0.06 0.02 0.01 0.8 0 0.01 0.06 0.02 0.02 0.02 0.01 0 0.02 0 0.84 0.02 0.02 0.05 0.01 0.03 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.04 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.05 0.73 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.06 0.18 0.04 0.63 0.04 0.02 0.01 0.01 0 0 0.03 0.08 0.02 0.01 0.83 0 0 0 0.01 0 0.04 0.05 0.07 0.02 0.01 0.79 0 0.01 0.04 0.02 0.01 0.02 0.01 0 0.02 0 0.86 0.02 0.02 0.04 0.01 0.03 0.01 0 0.03 0.01 0.04 0.81 True label 5th visit 6th visit 7th visit 8th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.71 0.05 0.05 0.07 0.02 0.01 0.01 0.01 0 0.04 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.62 0.04 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.05 0.05 0.08 0.02 0.02 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.74 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0.01 0.05 0.7 0.06 0.05 0.08 0.02 0.02 0.02 0.02 0 0.05 0.07 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.03 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.84 0 0 0.01 0.01 0 0.04 0.05 0.08 0.02 0.01 0.78 0 0 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0.01 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.02 0.06 0.05 0.02 0 0.04 0.01 0.07 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.74 0.08 0.05 0.02 0.05 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.04 0.02 0.01 0 0 0 0.06 0.19 0.05 0.61 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.04 0.05 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.04 0.02 0.03 0.01 0 0.02 0 0.03 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.04 0.01 0.08 0.02 0.01 0.87 0.01 0.01 0 0 0.02 0 0.02 0.04 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.05 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.06 0.19 0.05 0.61 0.04 0.03 0.01 0 0 0 0.03 0.09 0.02 0.01 0.83 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.78 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.01 0.05 0.01 0.03 0.01 0 0.02 0 0.04 0.83 True label 9th visit 10th visit 11th visit 12th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.05 0.04 0.02 0 0.03 0.01 0.09 0.02 0.01 0.86 0.01 0.01 0 0 0.02 0 0.02 0.06 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.02 0 0.06 0.73 0.05 0.04 0.06 0.02 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.07 0.02 0.01 0.77 0 0 0.03 0.02 0.02 0.02 0.01 0 0.04 0 0.83 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.87 0.01 0.02 0 0 0.02 0 0.02 0.05 0.05 0 0.72 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0.01 0.05 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.57 0.05 0.02 0.01 0 0 0 0.03 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.02 0 0.04 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.05 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.08 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.04 0.06 0.79 0.01 0.04 0.02 0.01 0 0.01 0 0.07 0.2 0.05 0.6 0.04 0.02 0.01 0.01 0 0 0.04 0.07 0.02 0.01 0.85 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.78 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.82 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.75 0.01 0.05 0.04 0.02 0 0.02 0.01 0.09 0.02 0.01 0.86 0.01 0.02 0 0 0.02 0 0.02 0.05 0.04 0 0.74 0.07 0.05 0.02 0.05 0.01 0.01 0.01 0.02 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.02 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.19 0.05 0.6 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0 0.01 0.01 0 0.04 0.06 0.08 0.02 0.01 0.77 0 0.01 0.04 0.02 0.03 0.03 0.01 0 0.04 0 0.8 0.02 0.01 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 True label 13th visit 14th visit 15th visit 16th visit 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.04 0.01 0.07 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.74 0.05 0.05 0.06 0.01 0.01 0.01 0.01 0 0.05 0.06 0.8 0.01 0.04 0.02 0 0 0.01 0 0.07 0.2 0.05 0.59 0.06 0.02 0.01 0.01 0 0 0.04 0.08 0.02 0.01 0.84 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.02 0.76 0 0.01 0.05 0.01 0.01 0.02 0.01 0 0.02 0 0.85 0.02 0.02 0.05 0.02 0.03 0.01 0 0.03 0 0.03 0.81 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.72 0.01 0.05 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.04 0.04 0 0.73 0.07 0.06 0.02 0.06 0.01 0.01 0 0.01 0 0.06 0.73 0.05 0.04 0.07 0.01 0.01 0.01 0.01 0 0.06 0.06 0.79 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.59 0.04 0.02 0.01 0 0 0 0.04 0.07 0.02 0.01 0.86 0 0 0 0.01 0 0.05 0.05 0.08 0.02 0.01 0.76 0.01 0.01 0.05 0.02 0.02 0.03 0.01 0 0.02 0 0.85 0.01 0.02 0.05 0.02 0.03 0.01 0 0.03 0.01 0.04 0.8 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.73 0.06 0.05 0.02 0.06 0.01 0.01 0.01 0.01 0 0.06 0.73 0.05 0.05 0.07 0.01 0.01 0.01 0.01 0 0.05 0.06 0.78 0.01 0.05 0.02 0.01 0 0.01 0 0.07 0.21 0.05 0.58 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.85 0 0.01 0.01 0.01 0 0.06 0.05 0.08 0.02 0.02 0.75 0 0.01 0.03 0.02 0.02 0.02 0.01 0 0.03 0 0.85 0.02 0.02 0.05 0.02 0.02 0.01 0 0.02 0 0.04 0.83 0: airplane 1: auto2: bird3: cat4: deer5: dog6: frog7: horse8: ship9: truck 0 1 2 3 4 5 6 7 8 9 0.73 0.01 0.06 0.04 0.02 0 0.03 0.01 0.08 0.02 0.01 0.88 0.01 0.01 0 0 0.02 0 0.02 0.05 0.04 0 0.75 0.07 0.05 0.02 0.05 0.01 0.01 0 0.01 0 0.06 0.72 0.05 0.04 0.06 0.02 0.01 0.01 0.02 0 0.06 0.07 0.76 0.01 0.05 0.02 0.01 0 0 0 0.07 0.19 0.05 0.59 0.05 0.02 0.01 0.01 0 0 0.03 0.07 0.02 0.01 0.84 0 0.01 0.01 0.01 0 0.06 0.06 0.08 0.02 0.02 0.74 0 0.01 0.04 0.02 0.02 0.02 0.01 0 0.03 0 0.84 0.02 0.01 0.05 0.02 0.03 0.01 0 0.02 0.01 0.04 0.82 Predicted label Predicted label Predicted label Predicted label True label 17th visit 18th visit 19th visit 20th visit Figure 11: The dynamic of the confusion matrix of PeTTA (ours) in episodic TTA with 20 visits. 34NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: We have highlighted the three main claims and contributions of our work in both the abstract (highlighted in bold font) and the introduction section (listed as bullet points). Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the limitations and potential future work of our study in Sec. 6. Specifically, three main limitations are included: (1) Collapse prevention can not be guaranteed through regularization, PeTTA requires (2) the use of a relatively small memory bank is available and (3) the empirical mean and covariant matrix of feature vectors on the source dataset is computable. We also include discussions in Appdx. E.3 and Appdx. E.4 to further elaborate (2), and (3) respectively. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 353. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided the full proof of all lemmas and theorem in Appdx. B. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This study propose a new TTA approach - PeTTA. A full description of this approach is given in Sec. 4 with its pseudo-code provided in Appdx. E.1. The implementation of PeTTA in Python is also attached as supplemental material. Additionally, Sec. 5.2 and Appdx. G are dedicated to providing further implementation details for reproducing the main experimental results. Lastly, the construction of recurring TTA is notably simple, and can be easily extended to other TTA streams. Its configuration on each tasks is described in the Recurring TTA paragraph of Sec. 5.2. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 36(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: This study does not involve any private datasets. All datasets used in our exper- iments are publicly available online from previous works (more information in Appdx. G.4). The source code of PeTTA is also attached as supplemental material. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings of the key results in the paper have been provided in Sec. 5.1 (Simulation Setup) and Sec. 5.2 (Setup - Benchmark Datasets). In the supplementary material, any additional experimental results beyond the main paper, such as those in Appdx. D.3, and Appdx. F.3, are consistently preceded by a subsection titledExperiment Setup summarizing the experimental details before presenting the results. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 37Answer: [Yes] Justification: Due to the limited computing resources, we only extensively evaluate the performance of our proposed method (PeTTA) across 5 independent runs, with different random seeds. Specifically, the mean values in 5 runs are reported in Tab. 1, Tab. 2, Tab. 7, and Tab. 8. The corresponding standard deviation values are provided in Appdx. F.1. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer \"Yes\" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information on the computing resources used in our experiments in Appdx. G.1. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed and to the best of our judgment, this study has conformed to the NeurIPS Code of Ethics. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. 38• The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This study advances the research in test-time adaptation area in general, and not tied to particular applications. Hence, there are no significant potential societal consequences of our work which we feel must be specifically highlighted here. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: To the best of our judgment, this study poses no risks for misuse. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? 39Answer: [Yes] Justification: The original papers that produced the code package or dataset have been properly cited throughout the paper. Further information on the licenses of used assets are provided in Appdx. G.4. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This study does not release new assets. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects 40Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This study does not involve crowdsourcing nor research with human subjects. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 41",
      "meta_data": {
        "arxiv_id": "2311.18193v4",
        "authors": [
          "Trung-Hieu Hoang",
          "Duc Minh Vo",
          "Minh N. Do"
        ],
        "published_date": "2023-11-30T02:24:44Z",
        "pdf_url": "https://arxiv.org/pdf/2311.18193v4.pdf",
        "github_url": "https://github.com/mariodoebler/test-time-adaptation"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces the \"recurring test-time adaptation (TTA)\" diagnostic setting to study and reveal the overlooked gradual performance degradation (model collapse) of TTA methods over prolonged periods. It formally defines TTA collapsing and provides a theoretical analysis using an ϵ-perturbed Gaussian Mixture Model Classifier (ϵ-GMMC), identifying dataset- and algorithm-dependent factors that contribute to error accumulation. Furthermore, the paper proposes \"Persistent TTA (PeTTA)\", a novel adaptation scheme that senses when the model is diverging towards collapse and adaptively adjusts its strategy to balance adaptation and collapse prevention, demonstrating superior stability and performance over existing approaches in lifelong TTA scenarios.",
        "methodology": "The study's methodology involves: (1) **Introducing Recurring TTA:** A diagnostic testing scenario where environments not only change but also recur over time, based on practical TTA but with repeated cycles through distributions. (2) **Theoretical Analysis with ϵ-GMMC:** A simplified yet representative ϵ-perturbed binary Gaussian Mixture Model Classifier (ϵ-GMMC) is used on a synthesized dataset to model and analyze the collapsing phenomenon. This analysis helps derive theoretical insights into data-dependent factors (prior data distribution, category difference) and algorithm-dependent factors (update rate, false negative rate) that cause performance degradation. (3) **Persistent TTA (PeTTA) Algorithm:** PeTTA extends the mean teacher update framework by continuously monitoring model divergence. It employs a mechanism to sense divergence from the initial model in the feature embedding space using Mahalanobis distance of running mean feature vectors (γ_y^t). Based on an average divergence measure (¯γ_t), PeTTA adaptively adjusts the regularization coefficient (λ_t) and the EMA update rate (α_t) on the fly. It also incorporates an anchor loss (LAL) that minimizes the KL divergence between the current and source model's output probabilities to prevent significant deviation, along with a category-balanced memory bank and robust batch normalization layers adopted from prior work.",
        "experimental_setup": "The experimental setup includes: (1) **ϵ-GMMC Simulation:** A synthesized dataset of 6000 samples from two Gaussian distributions (N(0,1), N(2,1)) with equal priors (p0=p1=0.5), released in batches of 10. Model updates followed Eq. 4 with α=5e-2, and model collapse was simulated by randomly flipping 10% of true-positive pseudo-labels. (2) **Benchmark Datasets:** CIFAR-10-C, CIFAR-100-C, and ImageNet-C (all with corruption level 5), and DomainNet (real → clipart, painting, sketch). (3) **Testing Scenarios:** The proposed recurring TTA setting, where testing environments gradually change and recur 20 times (K=20). Category distributions within batches are temporally correlated, generated by Dirichlet distributions (Dir(0.1) for CIFAR-10-C, DomainNet, ImageNet-C; Dir(0.01) for CIFAR-100-C). Performance was also evaluated on the Continuously Changing Corruption (CCC) benchmark, involving 80,000 adaptation steps across over 5.1 million images. (4) **Compared Methods:** CoTTA, EATA, RMT, MECTA, RoTTA, ROID, TRIBE, LAME (parameter-free), and RDumb (reset-based). (5) **Implementation Details:** Experiments used PyTorch with pre-trained source models from RobustBench and torchvision. Hyper-parameters like α0 (1e-3) and λ0 (10 for CIFAR-C/ImageNet-C, 1 for DomainNet) were set, and PeTTA's performance was averaged across 5 runs with different random seeds. EMA update rate for robust batch normalization and feature embedding statistics was 5e-2.",
        "limitations": "The study acknowledges several limitations: (1) A complete elimination of error accumulation through regularization cannot be rigorously guaranteed by PeTTA. (2) The approach implicitly assumes the availability of a small memory bank to handle the challenge of temporally correlated testing streams, as this aspect was not the primary focus of PeTTA. (3) PeTTA requires pre-computed feature statistics (empirical mean and covariant matrix of feature vectors) from the source distribution, which might not always be available in certain real-world scenarios and could potentially limit its scalability.",
        "future_research_directions": "Future research could delve deeper into developing TTA algorithms that are inherently error accumulation-free by construction, moving beyond regularization-based mitigation. Additionally, further exploration is needed to find alternative ways for reducing memory size (e.g., storing embedded features instead of original images) to enhance the scalability of TTA methods in real-world scenarios where memory banks might be a constraint.",
        "experimental_code": "import math\nimport torch\nimport torch.nn as nn\n\nfrom copy import deepcopy\nfrom methods.base import TTAMethod\nfrom augmentations.transforms_cotta import get_tta_transforms\nfrom utils.registry import ADAPTATION_REGISTRY\nfrom utils.misc import ema_update_model\n\n\n@ADAPTATION_REGISTRY.register()\nclass RoTTA(TTAMethod):\n    def __init__(self, cfg, model, num_classes):\n        super().__init__(cfg, model, num_classes)\n\n        self.memory_size = cfg.ROTTA.MEMORY_SIZE\n        self.lambda_t = cfg.ROTTA.LAMBDA_T\n        self.lambda_u = cfg.ROTTA.LAMBDA_U\n        self.nu = 1 - cfg.ROTTA.NU  # we do the ema update consistently the other way round: (param * ema_model + (1-param) * model)\n        self.update_frequency = cfg.ROTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n        self.mem = CSTU(capacity=self.memory_size, num_class=self.num_classes, lambda_t=self.lambda_t, lambda_u=self.lambda_u)\n\n        # setup the ema model\n        self.model_ema = self.copy_model(self.model)\n        for param in self.model_ema.parameters():\n            param.detach_()\n\n        # note: if the self.model is never reset, like for continual adaptation,\n        # then skipping the state copy would save memory\n        self.models = [self.model, self.model_ema]\n        self.model_states, self.optimizer_state = self.copy_model_and_optimizer()\n\n        # create the test-time transformations\n        self.transform = get_tta_transforms(self.img_size)\n\n    def loss_calculation(self):\n        self.model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        loss_sup = torch.tensor([(float('nan'))])\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = self.model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages, device=self.device)\n            loss_sup = (softmax_cross_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n        return loss_sup\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, x):\n        imgs_test = x[0]\n\n        with torch.no_grad():\n            self.model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(imgs_test)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(imgs_test):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                if self.mixed_precision and self.device == \"cuda\":\n                    with torch.cuda.amp.autocast():\n                        loss = self.loss_calculation()\n                    self.scaler.scale(loss).backward()\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                    self.optimizer.zero_grad()\n                else:\n                    loss = self.loss_calculation()\n                    loss.backward()\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n\n                self.model_ema = ema_update_model(\n                    model_to_update=self.model_ema,\n                    model_to_merge=self.model,\n                    momentum=self.nu,\n                    device=self.device,\n                    update_all=True\n                )\n\n        return ema_out\n\n    def reset(self):\n        if self.model_states is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved self.model/optimizer state\")\n        self.load_model_and_optimizer()\n        self.current_instance = 0\n        self.mem = CSTU(capacity=self.memory_size,\n                        num_class=self.num_classes,\n                        lambda_t=self.lambda_t,\n                        lambda_u=self.lambda_u)\n\n    def configure_model(self):\n        self.model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in self.model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n            elif isinstance(sub_module, (nn.LayerNorm, nn.GroupNorm)):\n                sub_module.requires_grad_(True)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(self.model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer, self.cfg.ROTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(self.model, name, momentum_bn)\n\n\n@torch.jit.script\ndef softmax_cross_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\n\n\ndef timeliness_reweighting(ages, device):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().to(device)\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\n\n\ndef get_named_submodule(model, sub_name: str):\n    names = sub_name.split(\".\")\n    module = model\n    for name in names:\n        module = getattr(module, name)\n\n    return module\n\n\ndef set_named_submodule(model, sub_name, value):\n    names = sub_name.split(\".\")\n    module = model\n    for i in range(len(names)):\n        if i != len(names) - 1:\n            module = getattr(module, names[i])\n\n        else:\n            setattr(module, names[i], value)\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\n",
        "experimental_info": "Method: RoTTA (Recurrent Test-Time Adaptation), a method in the repository that shares several components with the described Persistent TTA (PeTTA) algorithm.\n\nCore Idea: RoTTA extends the mean teacher framework for test-time adaptation by incorporating a category-balanced memory bank (CSTU) and robust batch normalization (RobustBN) layers to handle recurring shifts.\n\nKey Components and Settings:\n- **Mean Teacher Update Framework**: The `model_ema` (EMA teacher model) is updated using an exponential moving average (EMA) with a momentum derived from `cfg.ROTTA.NU` (specifically, `1 - cfg.ROTTA.NU`). This helps stabilize learning by averaging model weights over time.\n- **Robust Batch Normalization Layers**: Batch normalization layers (`nn.BatchNorm1d`, `nn.BatchNorm2d`) are replaced with `RobustBN1d` and `RobustBN2d`. These custom BN layers update their running mean and variance using a momentum specified by `cfg.ROTTA.ALPHA`. This allows the batch statistics to adapt to new domains while retaining some information from previous statistics.\n- **Category-balanced Memory Bank (CSTU)**: A memory bank (`self.mem`) with a `capacity` of `cfg.ROTTA.MEMORY_SIZE` is used. It stores instances including the input data, its pseudo-label, and its uncertainty (entropy). Instances are added to the memory, and a `heuristic_score` is used to decide which instances to remove when the memory is full. This score combines an `age` factor (influenced by `cfg.ROTTA.LAMBDA_T`) and an `uncertainty` factor (influenced by `cfg.ROTTA.LAMBDA_U`). This mechanism helps maintain a diverse and relevant set of samples in memory.\n- **Update Frequency**: The model's parameters and the EMA teacher model are updated every `cfg.ROTTA.UPDATE_FREQUENCY` incoming test instances.\n- **Loss Function**: The primary loss `loss_sup` is a consistency loss, specifically `softmax_cross_entropy`, calculated between the student model's output on augmented memory data (`stu_sup_out`) and the EMA teacher's output on original memory data (`ema_sup_out`). This loss is reweighted by `timeliness_reweighting(ages)`, which gives more importance to newer samples in the memory.\n\nDiscrepancies from \"Method\" description:\n- **Adaptive Adjustment of Parameters**: The \"Method\" describes PeTTA as adaptively adjusting the regularization coefficient (`λ_t`) and the EMA update rate (`α_t`) on the fly based on an average divergence measure (`¯γ_t`). In the provided RoTTA implementation, `cfg.ROTTA.LAMBDA_T`, `cfg.ROTTA.LAMBDA_U`, `cfg.ROTTA.ALPHA`, and `cfg.ROTTA.NU` are fixed hyperparameters and do not adapt dynamically.\n- **Divergence Monitoring**: The \"Method\" mentions monitoring model divergence using Mahalanobis distance of running mean feature vectors (`γ_y^t`). This specific divergence monitoring mechanism is not explicitly present in the RoTTA implementation.\n- **Anchor Loss (LAL)**: The \"Method\" specifies an anchor loss (LAL) that minimizes the KL divergence between the current and source model's output probabilities. The consistency loss used in RoTTA is between the student model and the EMA teacher model on memory data, not explicitly between the current model and the initial source model's output probabilities."
      }
    },
    {
      "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
      "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test\ndistributions with only unlabeled test data streams. Most of the previous TTA\nmethods have achieved great success on simple test data streams such as\nindependently sampled data from single or multiple distributions. However,\nthese attempts may fail in dynamic scenarios of real-world applications like\nautonomous driving, where the environments gradually change and the test data\nis sampled correlatively over time. In this work, we explore such practical\ntest data streams to deploy the model on the fly, namely practical test-time\nadaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA)\nmethod against the complex data stream in PTTA. More specifically, we present a\nrobust batch normalization scheme to estimate the normalization statistics.\nMeanwhile, a memory bank is utilized to sample category-balanced data with\nconsideration of timeliness and uncertainty. Further, to stabilize the training\nprocedure, we develop a time-aware reweighting strategy with a teacher-student\nmodel. Extensive experiments prove that RoTTA enables continual testtime\nadaptation on the correlatively sampled data streams. Our method is easy to\nimplement, making it a good choice for rapid deployment. The code is publicly\navailable at https://github.com/BIT-DA/RoTTA",
      "full_text": "Robust Test-Time Adaptation in Dynamic Scenarios Longhui Yuan Binhui Xie Shuang Li \f School of Computer Science and Technology, Beijing Institute of Technology {longhuiyuan,binhuixie,shuangli}@bit.edu.cn Abstract Test-time adaptation (TTA) intends to adapt the pre- trained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distri- butions. However, these attempts may fail in dynamic sce- narios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we ex- plore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Exten- sive experiments prove that RoTTA enables continual test- time adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA 1. Introduction In recent years, many machine learning problems have made considerable headway with the success of deep neu- ral networks [13, 22, 33, 38]. Unfortunately, the perfor- mance of deep models drops significantly when training data and testing data come from different distributions [59], which limits their utility in real-world applications. To re- duce the distribution shift, a handful of works focus on transfer learning field [56], in particular, domain adapta- tion (DA) [17, 42, 45, 48, 69, 72] or domain generalization (DG) [40, 41, 52, 71, 83], in which one or more different but \fCorresponding author Test data stream Continual TTANon-i.i.d.TTAPractical  TTACategoryDistribution Fully TTA Correlation samplingDistributionchanging Figure 1. We consider the practical test-time adaptation (TTA) setup and compare it with related ones. First, Fully TTA [70] adapts models on a fixed test distribution with an independently sampled test stream. Then, on this basis, Continual TTA [73] takes the continually changing distributions into consideration. Next, Non-i.i.d. TTA [19] tries to tackle the correlatively sampled test streams on a single test distribution, where the label distribution among a batch of data deviates from that of the test distribution. To be more practical, Practical TTA strives to connect both worlds: distribution changing and correlation sampling. related labeled datasets (a.k.a. source domain) are collected to help the model generalize well to unlabeled or unseen samples in new datasets (a.k.a. target domain). While both DA and DG have extensively studied the problem of distribution shifts, they typically assume acces- sibility to the raw source data. However, in many practical scenarios like personal consumption records, the raw data should not be publicly available due to data protection reg- ulations. Further, existing methods have to perform heavy backward computation, resulting in unbearable training costs. Test-time adaptation (TTA) [3,11,16,24,26,54,65,81] attempts to address the distribution shift online at test time with only unlabeled test data streams. Unequivocally, TTA has drawn widespread attention in a variety of applications, e.g., 2D/3D visual recognition [2, 29, 49, 65, 82], multi- modality [63, 64] and document understanding [15]. Prior TTA studies [7, 20, 70, 73] mostly concentrate on a simple adaptation scenario, where test samples are inde- pendently sampled from a fixed target domain. To name a few, Sun et al. [65] adapt to online test samples drawn from a constant or smoothly changing distribution with an auxil- iary self-supervised task. Wang et al. [70] adapt to a fixed arXiv:2303.13899v1  [cs.CV]  24 Mar 2023Table 1. Comparison between our proposed practical test-time adaptation (PTTA) and related adaptation settings. Setting Adaptation StageAvailable Data Test Data Stream Train Test Source Target Distribution Sampling Protocol Domain Adaptation ! % ! ! - - Domain Generalization ! % ! % - - Test-Time Training [65] ! ! ! ! stationary independently Fully Test-Time Adaptation [70] % ! % ! stationary independently Continual Test-Time Adaptation [73]% ! % ! continually changing independently Non-i.i.d. Test-Time Adaptation [5, 19]% ! % ! stationary correlatively Practical Test-Time Adaptation (Ours)% ! % ! continually changing correlatively target distribution by performing entropy minimization on- line. However, such an assumption is violated when the test environments change frequently [73]. Later on, Boudiaf et al. [5] and Gonget al. [19] consider the temporal correlation ship within test samples. For example, in autonomous driv- ing, test samples are highly correlated over time as the car will follow more vehicles on the highway or will encounter more pedestrians in the streets. More realistically, the data distribution changes as the surrounding environment alerts in weather, location, or other factors. In a word, distribution change and data correlation occur simultaneously in reality. Confronting continually changing distributions, tradi- tional algorithms like pseudo labeling or entropy minimiza- tion become more unreliable as the error gradients cumu- late. Moreover, the high correlation among test samples re- sults in the erroneous estimation of statistics for batch nor- malization and collapse of the model. Driven by this analy- sis, adapting to such data streams will encounter two major obstacles: 1) incorrect estimation in the batch normaliza- tion statistics leads to erroneous predictions of test samples, consequently resulting in invalid adaptation; 2) the model will easily or quickly overfit to the distribution caused by the correlative sampling. Thus, such dynamic scenarios are pressing for a new TTA paradigm to realize robust adapta- tion. In this work, we launch a more realistic TTA setting, where distribution changing and correlative sampling oc- cur simultaneously at the test phase. We call this Practical Test-Time Adaptation, or briefly,PTTA. To understand more clearly the similarities and differences between PTTA and the previous setups, we visualize them in Figure 1 and sum- marize them in Table 1. To conquer this challenging prob- lem, we propose a Robust Test-Time Adaptation (RoTTA) method, which consists of three parts: 1) robust statistics es- timation, 2) category-balanced sampling considering time- liness and uncertainty and 3) time-aware robust training. More concretely, we first replace the erroneous statistics of the current batch with global ones maintained by the expo- nential moving average. It is a more stable manner to esti- mate the statistics in BatchNorm layers. Then, we simulate a batch of independent-like data in memory with category- balanced sampling while considering the timeliness and un- certainty of the buffered samples. That is, samples that are newer and less uncertain are kept in memory with higher priority. With this batch of category-balanced, timely and confident samples, we can obtain a snapshot of the current distribution. Finally, we introduce a time-aware reweight- ing strategy that considers the timeliness of the samples in the memory bank, with a teacher-student model to perform robust adaptation. With extensive experiments, we demon- strate that RoTTA can robustly adapt in the practical setup, i.e., PTTA. In a nutshell, our contributions can be summarized as: • We propose a new test-time adaptation setup that is more suitable for real-world applications, namely practical test-time adaptation (PTTA). PTTA considers both distribution changing and correlation sampling. • We benchmark the performance of prior methods in PTTA and uncover that they only consider one aspect of the problem, resulting in ineffective adaptation. • We propose a robust test-time adaptation method (RoTTA), which has a more comprehensive considera- tion of PTTA challenges. Ease of implementation and effectiveness make it a practical deployment option. • We extensively demonstrate the practicality of PTTA and the effectiveness of RoTTA on common TTA benchmarks [23], i.e., CIFAR-10-C and CIFAR-100- C and a large-scale DomainNet [58] dataset. RoTTA obtains state-of-the-art results, outperforming the best baseline by a large margin (reducing the averaged classification error by over 5.9%, 5.5% and 2.2% on CIFAR-10-C, CIFAR-100-C and DomainNet, respec- tively). 2. Related Work Domain adaptation (DA) studies the problem of transfer- ring the knowledge learned from a labeled source dataset to an unlabeled target dataset [8, 17, 43, 51, 67, 68]. Represen- tative techniques include latent distribution alignment [48, 77], adversarial training [17, 62], or self-training [75, 85]. The limitation of this setting, however, is that an unlabeled test dataset (target domain) is needed at training time, in addition to a labeled training dataset (source domain). Ac- cordingly, it might fail to handle more practical scenariosFeature 𝐹Robust batch normalization (RBN)Update𝜇௚, 𝜎௚ଶNormalizeFeature𝐹′Update bank with current sample  Training lossℒ௥in Eq. (7) Teacher StudentAdaptation with RBNMemorybankEMA 𝑡A stream of online dataUpdateTest timeCorrelationsamplingStrong & weakaugmentation flowDistributionsCategoryTeacherMajor classhas highest ℋin majorRemoveAddWhen ℋ>ℋSamples to beadded& removed Figure 2. Framework overview. Firstly, we replace the batch normalization layer with RBN which robustly normalizes the feature map. During the inference of the online test stream of PTTA, we utilize the predictions of samples to maintain a memory bank by category- balanced sampling with timeliness and uncertainty. Finally, we use the category-balanced, timely and confident data in the memory bank combined with a robust loss to adapt the model at test time. like test-time adaptation. Our practical test-time adaptation setting can be viewed as performing correlatively sample adaptation on the fly. It is worth noting that standard domain adaptation techniques might collapse when only continual data streams from multiple target domains are accessible. Domain generalization (DG) assumes that multiple source domains are available for model training and tries to learn models that can generalize well to any unseen domains [4, 26,40,41,52,84]. A broad spectrum of methodologies based on data augmentation [78, 84], meta-learning [14, 40], or domain alignment [50,52] has made great progress. In con- trast, this work instead aims to improve the performance of source pre-trained models at the test time by using unla- beled online data streams from multiple continually chang- ing target domains. Continual learning (CL) (also known as incremental learning, life-long learning) addresses the problem of learn- ing a model for many tasks sequentially without forgetting knowledge obtained from the preceding tasks. [1, 6, 31, 37, 60]. CL methods can often be categorized into replay- based [60, 66] and regularization-based [31, 44] methods. Ideas from continual learning are also adopted for continu- ous domain adaptation approaches [34, 74] In our work, we share the same motivation as CL and point out that prac- tical test-time adaptation (PTTA) also suffers catastrophic forgetting (i.e., performance degradation on new test sam- ples due to correlation sampling), which makes test-time adaptation approaches are unstable to deploy. Test-time adaptation (TTA) focus on more challenging settings where only source model and unlabeled target data are available [9, 18, 27, 28, 35, 46, 61]. A similar paradigm is source-free domain adaptation (SFDA) [10, 36, 47, 79], which also requires no access to the training (source) data. To name a few, Liang et al . [45] fit the source hypoth- esis by exploiting the information maximization and self- supervised pseudo-labeling. Kundu et al. [35] formalize a unified solution that explores SFDA without any category- gap knowledge. To fully utilize any arbitrary pre-trained model, Sun et al. [65] propose conducting adaptation on the fly with an auxiliary self-supervised task. Later on, Wanget al. [70] take a source pre-trained model and adapt it to the test data by updating a few trainable parameters in Batch- Norm layers [25] using entropy minimization [21]. While standard TTA has been widely studied in many tasks [2, 20, 63, 64, 70, 82], the fact remains that both dis- tribution changing [73] and data correlation sampling [19] has only been considered in isolation. For example, Gong et al. [19] propose instance-aware batch normalization and prediction-balanced reservoir sampling to address the chal- lenges of correlatively sampled test streams, however, it does not consider unstable adaptation resulting from long- term adaptation on continually changing distributions. On the other hand, Wang et al. [73] assume that the target test data is streamed from a continually changing environment and continually adapt an off-the-shelf source pre-trained model to the current test data. In this work, we launch PTTA, a more practical TTA setting to connect both worlds: distribution changing and correlation sampling. 3. Method 3.1. Problem Definition and Motivation Given a model fθ0 with parameter θ0 pre-trained on source domain DS = {(xS, yS)}, the proposed practical test-time adaptation (PTTA) aims to adapt fθ0 to a stream of online unlabeled samples X0, X1, ...,XT , where Xt is a batch of highly correlated samples from the distribution Ptest that changes with time t continually. More specifi- cally, at test time, with time going on, the test distribution Ptest changes continually as P0, P1, ...,P∞. At time step t, we will receive a batch of unlabeled and correlated samplesmotion distribution changing snow time  Distributions and Labels of PTTA T est Stream uniform 10 1 0.1 0.01 0.001 Dirichlet Parameter  Figure 3. Illustration of the labels and distributions of the test stream of CIFAR10-C under the setup PTTA. And we adopt Dirichlet distribution to simulate the process of correlative sam- pling. It is clear that as the concentration parameter δ decreases, the correlation among sampled data increases, which is reflected in the increasing aggregation of categories. Xt from Ptest. Next, Xt is fed into the model fθt and the model needs to adapt itself to the current test data streams and make predictions fθt (Xt) on the fly. As a matter of fact, this setup is largely driven the prac- tical demands of deploying models in dynamic scenarios. Taking for example the case of autonomous driving men- tioned in § 1, test samples are highly correlated and the data distribution changes continually with the weather or loca- tion. Another example is the situation of intelligent moni- toring, the camera will continuously capture more people at certain times, such as after work, but fewer of them during work time. Meanwhile, the light condition changes con- tinually from day to night. The deployed model should be robustly adapted in such dynamic scenarios. In a word, dis- tribution change and data correlation often happen simul- taneously in the real world. For this reason, existing TTA methods [7,9,19,28,70,73,81] might become unstable when the test stream is sampled from such dynamic scenarios. To obtain the test stream of PTTA, we adopt Dirich- let Distribution with parameter δ to simulate the correla- tion among test samples. We present the test data streams corresponding to different values of δ on the CIFAR10-C dataset in Figure 3. We can observe that the smaller δ is, the higher the correlation will be. For the sake of unity, we set δ = 0.1 as the default for all experiments. In the follow- ing, we present a robust test-time adaptation framework for the practical test-time adaptation setup defined above. An overview of our RoTTA is illustrated in Figure 2. 3.2. Robust Test-Time Adaptation Motivated by the fact that the statistics of current batch data, which are commonly used in previous TTA meth- ods [7, 20, 65, 70, 73], become unreliable when they en- counter correlative test data streams, we first turn to the global robust statistics for normalization. Then, to effec- tively adapt to the current distribution, we maintain a mem- ory bank by category-balanced sampling with considering timeliness and uncertainty, which captures a more stable snapshot of the distribution. Finally, we utilize the teacher- student model and design a timeliness-based reweighting strategy to train the model robustly. Robust batch normalization (RBN). Batch Normaliza- tion (BN) [25] is a widely-used training technique as it can accelerate the training and convergence speed of networks and stabilize the training process by reducing the risk of gradient explosion and vanishing. Given the feature map F ∈ RB×C×H×W as the input for a BN layer when train- ing, the channel-wise mean µ ∈ RC and variance σ2 ∈ RC are calculated as follows: µc = 1 BHW BX b=1 HX h=1 WX w=1 F(b,c,h,w) , (1) σ2 c = 1 BHW BX b=1 HX h=1 WX w=1 (F(b,c,h,w) − µc)2 . (2) Then the feature map is normalized and refined in a channel-wise manner as BN (F(b,c,h,w); µ, σ2) =γc F(b,c,h,w) − µc √σ2c + ϵ + βc , (3) where γ, β∈ RC are learnable parameters in the layer and ϵ > 0 is a constant for numerical stability. Meanwhile, during training, the BN layer maintains a group of global running mean and running variance (µs, σ2 s) for inference. Due to the domain shift at test time, the global statis- tics (µs, σ2 s) normalize test features inaccurately, causing significant performance degradation. To tackle the prob- lem above, some methods [55, 70, 73] use the statistics of the current batch to perform normalization. Unfortunately, when the test samples have a high correlation under PTTA setup, the statistics of the current batch also fail to correctly normalize the feature map, as demonstrated in Figure 4c. Specifically, the performance of BN [53] decreases rapidly as the data correlation increases. Based on the analysis above, we propose a robust batch normalization (RBN) module, which maintains a group of global statistics (µg, σ2 g) to normalize the feature map ro- bustly. Before the whole test-time adaptation, (µg, σ2 g) is initialized as the running mean and variance (µs, σ2 s) of the pre-trained model. When adapting the model, we update the global statistics first by exponential moving average as µg = (1− α)µg + αµ , (4) σ2 g = (1− α)σ2 g + ασ2 , (5) where (µ, σ2) is the statistics of the buffered samples in the memory bank. Then we normalize and affine the feature as Eq. (3) with (µg, σ2 g). When inferring for test samples, we directly utilize (µg, σ2 g) to calculate the output as Eq (3). Al- though simple, RBN is effective enough to tackle the prob- lem of normalization on test streams of PTTA.Category-balanced sampling with timeliness and uncer- tainty (CSTU). In the PTTA setup, the correlation among test samples Xt at time t leads to a deviation between the observed distribution bPtest and the test distribution Ptest. Specifically, the marginal label distribution p(y|t) tends to differ from p(y). Continuously learning with Xt over time t can lead to model adaptation to an unreliable distribution bPtest, resulting in ineffective adaptation and an increased risk of model collapse. To address this issue, we propose a category-balanced memory bank M with a capacity of N, which takes into account the timeliness and uncertainty of samples when up- dating. In particular, we adopt the predictions of test sam- ples as pseudo labels to guide the update ofM. Meanwhile, to guarantee the balance among categories, we distribute the capacity of M equally to each category, and samples of the major categories will be replaced first (refer to lines 5-9 in Algorithm 1). Furthermore, due to the continually changing test distribution, old samples in M are limited in value, and could even impair the ability of the model to adapt to the current distribution. Additionally, samples of high uncer- tainty always produce erroneous gradient information that can hinder model adaptation, as suggested by [55]. With this in mind, we attach each sample in M with a group of heuristics (A, U), where A, initialized as 0 and in- creasing with time t, is the age of the sample, and U the un- certainty calculated as the entropy of the prediction. Next, we combine the timeliness and uncertainty to calculate a heuristic score, i.e., category-balanced sampling with time- liness and uncertainty (CSTU), as follows: H = λt 1 1 + exp(−A/N) + λu U log C , (6) where λt and λu make the trade-off between timeliness and uncertainty, and for simplicity, λt and λu are set to 1.0 for all experiments, andC is the number of categories. We sum- marize our sampling algorithm in Algorithm 1. With CSTU, we can obtain a robust snapshot of the current test distribu- tion Ptest, and effectively adapt the model to it. Robust training with timeliness. Actually, after replacing BN layers with our RBN and obtaining the memory bank selected via CSTU, we can directly adopt the widely used techniques like pseudo labeling or entropy minimization to perform test-time adaptation. However, we notice that too old or unreliable instances still have the opportunity to stay in M since keeping the category balance is assigned the top priority. In addition, too aggressive updates of the model will make the category balance ofM unreliable, resulting in unstable adaptation. Meanwhile, error accumulation caused by the distribution change also makes the aforementioned approaches unworkable. To further reduce the risk of error gradients information from old and unreliable instances and stabilize the adapta- tion, we turn to the robust unsupervised learning method Algorithm 1: CSTU for one test sample. 1 Input: a test sample x and the teacher model fθT . 2 Define: memory bank M and its capacity N, number of classes C, per class occupation O ∈RC, total occupation Ω, classes to pop instance D. 3 Infer as p(y|x) =Softmax(fθT (x)). 4 Calculate the predicted category of x as ˆy = arg maxc p(c|x), the uncertainty as Ux = −PC c=1 p(c|x) log(p(c|x)), the age as Ax = 0, and the heuristic score Hx of x with Eq (6) 5 if Oˆy < N C then 6 if Ω <N: Search range D = ∅. 7 else: Search range D = {j|j = arg maxc Oc} 8 else 9 Search range D = {ˆy} 10 if D is ∅ then 11 Add (x, ˆy, Hx, Ux) into M. 12 else 13 Find the instance (ˆx, yˆx, Aˆx, Uˆx) with the highest value in Eq (6) Hˆx among D. 14 if Hx < Hˆx then 15 Remove (ˆx, yˆx, Aˆx, Uˆx) from M. 16 Add (x, ˆy, Hx, Ux) into M. 17 else 18 Discard x. 19 Increase the age of all instances in M. teacher-student model and propose a timeliness reweight- ing strategy. In addition, for the sake of time efficiency and stability, only affine parameters in RBN are trained during adaptation. At time step t, after inferring for the correlated data Xt with the teacher model fθT t and updating the memory bank M with Xt, we begin updating the student model fθS t and the teacher model fθT t . Firstly, we update parameters of stu- dent model θS t → θS t+1 by minimizing the following loss: Lr = 1 Ω ΩX i=1 L(xM i , Ai; θT t , θS t ) , (7) where Ω = |M| is the total occupation of the memory bank, and xM i and Ai(i = 1, ..., Ω) are instances in the memory bank and their age respectively. Subsequently, the teacher model is updated by exponential moving average as θT t+1 = (1− ν)θT t + νθS t+1 . (8) To calculate the loss value of an instancexM i from the mem- ory bank, the timeliness reweighting term is computed as E(Ai) = exp(−Ai/N) 1 + exp(−Ai/N) , (9)where Ai is the age of xM i , and N is the capacity of the bank. And then we calculate the cross entropy between the soft-max prediction pS(y|x′′ i ) of the strong-augmented view x′′ i from the student model and that pT (y|x′ i) of the weak- augmented view 1 x′ i from the teacher model as follows: ℓ(x′ i, x′′ i ) =−1 C CX c=1 pT (c|x′ i) logpS(c|x′′ i ) . (10) Finally, equipped with Eq. (9) and Eq. (10), the right-hand side of Eq. (7) reduces to L(xM i , Ai; θT t , θS t ) =E(Ai)ℓ(x′ i, x′′ i ) . (11) To sum up, equipped with RBN, CSTU, and robust training with timeliness, our RoTTA is capable of effectively adapt- ing any pre-trained models in dynamic scenarios. 4. Experiments 4.1. Setup Datasets. CIFAR10-C and CIFAR100-C [23] are the com- monly used TTA benchmarks to testify the robustness un- der corruptions. Both of them are obtained by applying 15 kinds of corruption with 5 different degrees of severity on their clean test images of original datasets CIFAR10 and CIFAR100 respectively. CIFAR10/CIFAR100 [32] have 50,000/10,000 training/test images, all of which fall into 10/100 categories. DomainNet [58] is the largest and hard- est dataset to date for domain adaptation and consists of about 0.6 million images with 345 classes. It consists of six different domains including Clipart (clp), Infograph (inf), Painting (pnt), Quickdraw (qdr), Real (rel), and Sketch (skt). We first pre-train a source model on the train set in one of six domains and testify all baseline methods on the test set of the remaining five domains. Implementation details. All experiments are conducted with PyTorch [57] framework. In the case of robustness to corruption, following the previous methods [55, 70, 73], we obtain the pre-trained model from RobustBench bench- mark [12], including the WildResNet-28 [80] for CIFAR10 → CIFAR10-C, and the ResNeXt-29 [76] for CIFAR100 → CIFAR100-C. Then, we change the test corruption at the highest severity 5 one by one to simulate that the test distri- bution continually changes with time in PTTA. And in the case of generalization under the huge domain gap, we train a ResNet-101 [22] by standard classification loss for each domain in DomainNet and adapt them continually to differ- ent domains except the source domain. Meanwhile, we uti- lize the Dirichlet distribution to simulate the correlatively sampled test stream for all datasets. For optimization, we adopt Adam [30] optimizer with learning rate 1.0 × 10−3, 1Weak augmentation is ReSize+CenterCrop. Strong augmentation is a combination nine operations like Clip, ColorJitter, and RandomAffine. β = 0.9. For a fair comparison, we set the batch size for all methods as 64 and the capacity of the memory bank of RoTTA as N = 64. Concerning the hyperparameters, we adopt a unified set of values for RoTTA across all experi- ments including α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. More details are provided in the appendix. 4.2. Comparisons with the State-of-the-arts Robustness under corruptions. The classification error on CIFAR10→CIFAR10-C and CIFAR100→CIFAR100-C are shown in Table 2 and Table 3 respectively. We change the type of the current corruption at the highest severity 5 as time goes on, and sample data correlatively for infer- ence and adaptation simultaneously. The same test stream is shared across all compared methods. From Table 2 and Table 3, we can see that RoTTA achieves the best performance compared to previous meth- ods. Moreover, RoTTA has a significant performance gain to the second-best method that 5.9% improvement on CIFAR10 →CIFAR10-C and 5.5% improvement on CIFAR100→CIFAR100-C respectively, verifying the effec- tiveness of RoTTA to adapt the model under PTTA. In more detail, we can observe that BN [53], PL [39], TENT [70] and CoTTA [73] negatively adapt the model to the test streams of both datasets compared to Source (−6.5 ∼ −46.4%). This is attributed to the fact that these methods overlook the issues posed by correlation sampling, which can result in highly correlated data within a batch. As a consequence, traditional normalization statistics may be ineffective in appropriately normalizing the feature maps. Equipped with RBN and CSTU, RoTTA no longer suffers from this issue. Meanwhile, in Table 3, if focus on the adaptation procedure, we can see that the performance of PL [39], TENT [70] and NOTE [19] becomes worse and worse, and eventually, the model even collapses (error rate > 97%). This reveals that the impact of error accumula- tion on long-term adaptation can be catastrophic. To tackle this problem, RoTTA turns to robustly adapt the model with timeliness reweighting and confident samples in the mem- ory bank, and superior performance throughout the adapta- tion process demonstrates its effectiveness. In addition, we find that although LAME [5] never tunes the parameters of the model, it is still a competi- tive baseline for example it achieves the second-best result on CIFAR100→CIFAR100-C. However, its performance is very dependent on the performance of the pre-trained model e.g. negligible improvement on difficult corruptions (shot, gaussian, pixelate). On the contrary, our RoTTA is more flexible and achieves better and more robust results. Generalization under domain shift. We also evalu- ate RoTTA under a more challenging dataset DomainNet, where we continually adapt a source pre-trained model to correlatively sampled test streams of the rest domains. AsTable 2. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 3. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 4. Average classification error of DomainNet while continually adapting to different domains with correlatively sampled test stream. Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Sourceclp inf pnt qdr rel sktAvg. BN clp inf pnt qdr rel sktAvg. PL clp inf pnt qdr rel sktAvg.TENTclp inf pnt qdr rel sktAvg. clp N/A 83.9 65.4 88.6 48.0 59.1 69.0clp N/A 88.6 70.7 90.5 65.4 67.0 76.5clp N/A 94.5 98.9 99.5 99.7 99.7 98.5clp N/A 87.5 71.9 94.2 96.2 98.9 89.7inf 61.8 N/A 66.9 96.0 50.0 70.6 69.1inf 68.6 N/A 74.2 96.2 69.9 76.8 77.1inf 82.6 N/A 99.2 99.6 99.7 99.3 96.1inf 68.6 N/A 75.0 97.3 95.9 98.7 87.1pnt 56.5 83.7 N/A 94.2 42.6 63.4 68.1pnt 60.8 87.9 N/A 94.3 62.3 68.7 74.8pnt 78.6 99.4 N/A 99.7 99.6 99.7 95.4pnt 61.7 87.1 N/A 96.4 95.3 98.8 87.8qdr 89.2 99.0 98.6 N/A 95.0 92.3 94.8qdr 80.3 97.7 92.6 N/A 88.7 88.1 89.5qdr 81.7 99.5 99.6 N/A 99.7 99.8 96.1qdr 78.9 97.1 91.6 N/A 89.2 88.7 89.1rel 49.4 80.4 51.5 93.4 N/A 63.3 67.6rel 57.9 87.1 63.1 94.3 N/A 70.8 74.6rel 73.5 99.4 99.2 99.6 N/A 99.7 94.3rel 57.8 86.4 68.1 96.9 N/A 96.7 81.2skt 47.5 88.2 62.9 87.1 51.8 N/A 67.5skt 50.4 87.6 64.6 89.6 63.1 N/A 71.1skt 64.8 99.2 99.4 99.7 99.7 N/A 92.6skt 51.9 87.2 69.1 95.3 97.3 N/A 80.1Avg.60.9 87.0 69.1 91.9 57.5 69.7 72.7Avg.63.6 89.8 73.0 93.0 69.9 74.3 77.3Avg.76.2 98.4 99.3 99.6 99.7 99.6 95.5Avg.63.8 89.0 75.1 96.0 94.8 96.4 85.8 Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →Timet− − − − − − − − − − − − − − − − − − →LAMEclp inf pnt qdr rel sktAvg.COTTAclp inf pnt qdr rel sktAvg.NOTEclp inf pnt qdr rel sktAvg.RoTTAclp inf pnt qdr rel sktAvg. clp N/A 82.2 64.5 87.7 46.9 58.9 68.0clp N/A 90.6 77.9 89.3 76.3 72.7 81.4clp N/A 89.2 73.0 94.8 98.4 99.4 91.0clp N/A 85.5 62.0 82.0 49.3 59.8 67.7inf 60.1 N/A 65.7 95.4 48.5 69.4 67.8inf 74.5 N/A 82.0 95.7 80.2 81.5 82.8inf 75.4 N/A 78.7 98.7 98.1 99.5 90.1inf 61.8 N/A 63.7 91.5 52.5 67.6 67.4pnt 55.8 81.5 N/A 93.3 41.3 62.1 66.8pnt 66.3 89.8 N/A 93.4 74.0 75.4 79.8pnt 64.7 89.8 N/A 97.8 98.4 99.2 90.0pnt 53.3 84.1 N/A 89.1 47.3 61.4 67.0qdr 88.3 99.1 99.0 N/A 94.9 92.2 94.7qdr 82.3 98.2 94.6 N/A 92.5 90.1 91.5qdr 74.7 97.2 92.2 N/A 93.5 99.6 91.4qdr 77.5 97.0 89.8 N/A 80.3 82.2 85.3rel 48.0 79.3 50.1 91.6 N/A 60.2 65.8rel 64.0 90.3 73.2 93.5 N/A 77.6 79.7rel 61.3 89.2 68.9 98.8 N/A 99.2 83.5rel 49.1 82.3 50.3 88.0 N/A 61.1 66.2skt 45.6 87.1 59.5 83.9 49.9 N/A 65.2skt 56.1 89.2 71.9 89.2 73.5 N/A 76.0skt 55.2 89.7 70.1 96.9 98.3 N/A 82.0skt 42.6 83.7 54.4 80.9 47.5 N/A 61.8Avg.59.6 85.8 67.8 90.4 56.3 68.6 71.4Avg.68.6 91.6 79.9 92.2 79.3 79.5 81.9Avg.66.3 91.0 76.6 97.4 97.3 99.4 88.0Avg.56.8 86.5 64.0 86.3 55.4 66.469.2(+2.2) shown in Table 4, consistent with the previous analysis, most of the methods include BN [53], PL [39], TENT [70], CoTTA [73] and NOTE [19] even perform worse than the Source model ( −4.6 ∼ −22.8%). RoTTA consistently achieves the best performance and has 2.2% gain than the second method LAME [5], demonstrating RoTTA’s effec- tiveness again. 4.3. Ablation Study Effect of each component. To further investigate the effi- cacy of each component, we replace each part with the nor- mally used solutions to obtain three variants: (1) RoTTA w/o RBN, replace RBN with test-time BN in TENT [70]; (2) RoTTA w/o CSTU, directly adapt the model on test stream; (3) RoTTA w/o robust training (RT), directly adapt the model only with entropy minimization. As shown in Table 5, we can observe that significant performance degra- dation occurs for all variants, proving that every part of our proposed method is valid for PTTA. Take one com- ponent for a detailed example, without RBN robustly nor- malizing feature maps, the performance of RoTTA drops 50.2% and 16.3% on CIFAR10-C and CIFAR100-C respec- tively, proving that RBN is robust enough to tackle the prob- lem of normalization of correlatively sampled data streams. CSTU enables RoTTA to adapt to a more stable distribu- tion by maintaining a timely and confident snapshot of the test distribution. Meanwhile, robust training with timeliness greatly reduces the accumulation of errors. Every compo- nent behaves significantly to enable effective adaptation un- der PTTA. Effect of the distribution changing order. To exclude the effect of a fixed order of distribution changing, we con- ducted experiments on ten different sequences of changes on CIFAR10-C and CIFAR100-C with independently andBN PL TENT LAME CoTTA NOTE RoTTA0 10 20 30 40 50 60 70 80Classification error (%) Source CIFAR-10  CIFAR-10-C Independent Correlative (a) CIFAR10-C. BN PL TENT LAME CoTTA NOTE RoTTA0 20 40 60 80Classification error (%) Source CIFAR-100  CIFAR-100-C Independent Correlative (b) CIFAR100-C. uniform 10 1 0.1 0.01 0.001 30 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (c) δ. 16 32 64 128 256 512 40 50 60 70 80 90 100Classification error (%) Source BN PL TENT LAME CoTTA NOTE RoTTA (d) Batch size. Figure 4. (a) & (b) we adapt the model continually to different corruptions of 10 different orders with independently and correlatively sampled test streams on CIFAR10-C and CFAR100-C respectively and report their average classification error. (c) & (d) we verify the effect of δ and batch size to different methods on CIFAR100-C respectively. Table 5. Classification error of different variants of our RoTTA. Variant CIFAR10-C CIFAR100-C Avg. RoTTA w/o RBN 75.4 51.3 63.4 RoTTA w/o CSTU 47.1 46.3 46.7 RoTTA w/o RT 78.2 95.0 81.6 RoTTA 25.2 35.0 30.1 correlatively sampled test streams respectively. As shown in Figure 4a and 4b, no matter what kind of setup, RoTTA can achieve excellent results. The detailed results on the correlatively sampled test streams are shown in Table 6, RoTTA achieves 4.3% and 4.7% progress on CIFAR10- C and CIFAR100-C respectively. This shows that RoTTA can adapt the model robustly and effectively in long-term scenarios where distribution continually changes and test streams are sampled either independently or correlatively, making it a good choice for model deployment. Effect of Dirichlet concentration parameter δ. We vary the value of δ on CIFAR100-C and compare RoTTA with other approaches in Figure 4c. As the value of δ increases, the performance of BN [53], PL [39], TENT [70] and CoTTA [73] drops quickly, because they never consider the increasing correlation among test samples. NOTE [19] is stable to correlatively sampled test streams but does not consider the distribution changing, causing ineffective adaptation. Meanwhile, the higher correlation between test samples will make the propagation of labels more accurate, which is why the result of LAME [5] slightly improves. Fi- nally, excellent and stable results once again prove the sta- bility and effectiveness of RoTTA. Effect of batch size. In real scenarios, considering deploy- ment environments may use different test batch sizes, we conduct experiments with different values of test batch sizes and results are shown in Figure 4d. For a fair comparison, we control the frequency of updating the model of RoTTA so that the number of samples involved in back-propagation is the same. As the batch size increases, we can see that all of the compared methods have a significant improvement except for lame which has a slight decrease. This is be- cause the number of categories in a batch increases with the Table 6. Average classification error of tasks CIFAR10 → CIFAR10-C and CIFAR100 → CIFAR100-C while continually adapting to different corruptions of 10 different orders at the high- est severity 5 with correlatively sampled test stream. Method CIFAR10-C CIFAR100-C Avg. Source 43.5 46.4 46.9 BN [53] 75.2 52.9 64.1 PL [39] 75.2 52.9 60.1 TENT [70] 82.3 93.2 87.8 LAME [5] 39.5 40.6 40.1 NOTE [19] 30.5 76.1 53.3 CoTTA [73] 83.1 52.8 67.9 RoTTA 26.2(+4.3) 35.9(+4.7) 31.1(+9.0) increasing batch size, causing the overall correlation to be- come lower but the propagation of labels to become more difficult. Most significantly, RoTTA achieves the best re- sults across different batch sizes, demonstrating its robust- ness in dynamic scenarios once again. 5. Conclusion This work proposes a more realistic TTA setting where distribution changing and correlative sampling occur si- multaneously at the test phase, namely Practical Test-Time Adaptation (PTTA). To tackle the problems of PTTA, we propose Robust Test-Time Adaptation (RoTTA) method against the complex data stream. More specifically, a group of robust statistics for the normalization of feature maps is estimated by robust batch normalization. Meanwhile, a memory bank is adopted to capture a snapshot of the test distribution by category-balanced sampling with consider- ing timeliness and uncertainty. Further, we develop a time- aware reweighting strategy with a teacher-student model to stabilize the adaptation process. Extensive experiments and ablation studies are conducted to verify the robustness and effectiveness of the proposed method. We believe this work will pave the way for thinking about adapting models into real-world applications by test-time adaptation algorithm. Acknowledgements. This paper was supported by National Key R&D Program of China (No. 2021YFB3301503), and also supported by the National Natural Science Foundation of China under Grant No. 61902028.References [1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. In NeurIPS, pages 11816–11825, 2019. 3 [2] Fatemeh Azimi, Sebastian Palacio, Federico Raue, J ¨orn Hees, Luca Bertinetto, and Andreas Dengel. Self-supervised test-time adaptation on video data. In WACV, pages 2603– 2612, 2022. 1, 3 [3] Mathilde Bateson, Herve Lombaert, and Ismail Ben Ayed. Test-time adaptation with shape moments for image segmen- tation. In MICCAI, pages 736–745, 2022. 1 [4] Gilles Blanchard, Gyemin Lee, and Clayton Scott. General- izing from several related classification tasks to a new unla- beled sample. In NeurIPS, pages 2178–2186, 2011. 3 [5] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In CVPR, pages 8344–8353, 2022. 2, 6, 7, 8, 13, 14, 15, 16, 17 [6] Francisco M Castro, Manuel J Mar ´ın-Jim´enez, Nicol´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV, pages 233–248, 2018. 3 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In CVPR, pages 295–305, 2022. 1, 4 [8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In CVPR, pages 3339–3348, 2018. 2 [9] Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang. Test- time fast adaptation for dynamic scene deblurring via meta- auxiliary learning. In CVPR, pages 9137–9146, 2021. 3, 4 [10] Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In KDD, pages 451–460, 2016. 3 [11] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sun- grack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440–458, 2022. 1 [12] Francesco Croce, Maksym Andriushchenko, Vikash Se- hwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In Neurips, 2021. 6 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1 [14] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain general- ization. In ECCV, pages 200–216, 2020. 3 [15] Sayna Ebrahimi, Sercan ¨O. Arik, and Tomas Pfister. Test- time adaptation for visual document understanding. CoRR, abs/2206.07240, 2022. 1 [16] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A Efros. Test-time training with masked autoencoders. In NeurIPS, 2022. 1 [17] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1– 59:35, 2016. 1, 2 [18] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N. Metaxas. Vi- sual prompt tuning for test-time domain adaptation. CoRR, abs/2210.04831, 2022. 3 [19] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Robust continual test- time adaptation: Instance-aware BN and prediction-balanced memory. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [20] Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate pseudo-labels. In NeurIPS, 2022. 1, 3, 4 [21] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In NeurIPS, pages 529– 536, 2004. 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016. 1, 6 [23] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and per- turbations. In ICLR, 2019. 2, 6 [24] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative continuous-time bayesian neural network for fast training-free test-time adap- tation. In NeurIPS, 2022. 1 [25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448–456, 2015. 3, 4 [26] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier ad- justment module for model-agnostic domain generalization. In NeurIPS, pages 2427–2440, 2021. 1, 3 [27] Vidit Jain and Erik Learned-Miller. Online domain adapta- tion of a pre-trained cascade of classifiers. In CVPR, pages 577–584, 2011. 3 [28] Minguk Jang and Sae-Young Chung. Test-time adaptation via self-training with nearest neighbor information. CoRR, abs/2207.10792, 2022. 3, 4 [29] Junho Kim, Inwoo Hwang, and Young Min Kim. Ev-tta: Test-time adaptation for event-based object recognition. In CVPR, pages 17724–17733, 2022. 1 [30] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6 [31] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska- Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku- maran, and Raia Hadsell. Overcoming catastrophic forget- ting in neural networks. CoRR, abs/1612.00796, 2016. 3 [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural net- works. In NeurIPS, pages 1097–1105, 2012. 1 [34] Ananya Kumar, Tengyu Ma, and Percy Liang. Understand- ing self-training for gradual domain adaptation. In ICML, pages 5468–5479, 2020. 3 [35] Jogendra Nath Kundu, Naveen Venkat, Rahul M. V ., and R. Venkatesh Babu. Universal source-free domain adapta- tion. In CVPR, pages 4543–4552, 2020. 3 [36] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free do- main adaptation method. In WACV, pages 615–625, 2021. 3 [37] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, 2022. 3 [38] Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nat., 521(7553):436–444, 2015. 1 [39] Dong-Hyun Lee et al. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, page 896, 2013. 6, 7, 8, 12, 14, 15, 16, 17 [40] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for do- main generalization. In AAAI, pages 3490–3497, 2018. 1, 3 [41] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In CVPR, pages 5400–5409, 2018. 1, 3 [42] Shuang Li, Binhui Xie, Qiuxia Lin, Chi Harold Liu, Gao Huang, and Guoren Wang. Generalized domain conditioned adaptation network. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4093–4109, 2022. 1 [43] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Yulin Wang, and Wei Li. Transferable semantic augmen- tation for domain adaptation. In CVPR, pages 11516–11525, 2021. 2 [44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2935–2947, 2018. 3 [45] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for un- supervised domain adaptation. In ICML, pages 6028–6039, 2020. 1, 3 [46] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: when does self-supervised test-time training fail or thrive? In NeurIPS, pages 21808–21820, 2021. 3 [47] Yuang Liu, Wei Zhang, and Jun Wang. Source-free do- main adaptation for semantic segmentation. In CVPR, pages 1215–1224, 2021. 3 [48] Mingsheng Long, Yue Cao, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Transferable representation learning with deep adaptation networks. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):3071–3085, 2019. 1, 2 [49] Wenao Ma, Cheng Chen, Shuang Zheng, Jing Qin, Huimao Zhang, and Qi Dou. Test-time adaptation with calibration of medical image classification nets for label distribution shift. In MICCAI, pages 313–323, 2022. 1 [50] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In ICML, pages 7313– 7324, 2021. 3 [51] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009. 2 [52] Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant fea- ture representation. In ICML, pages 10–18, 2013. 1, 3 [53] Zachary Nado, Shreyas Padhy, D. Sculley, Alexander D’Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robust- ness under covariate shift. CoRR, abs/2006.10963, 2020. 4, 6, 7, 8, 12, 14, 15, 16, 17 [54] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, pages 16888–16905, 2022. 1 [55] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test- time model adaptation without forgetting. In ICML, volume 162, pages 16888–16905, 2022. 4, 5, 6 [56] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359, 2010. 1 [57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 6 [58] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In ICCV, pages 1406–1415, 2019. 2, 6 [59] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in ma- chine learning. 2008. 1 [60] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classi- fier and representation learning. InCVPR, pages 5533–5542, 2017. 3 [61] Amelie Royer and Christoph H Lampert. Classifier adapta- tion at prediction time. In CVPR, pages 1401–1409, 2015. 3 [62] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat- suya Harada. Maximum classifier discrepancy for unsuper- vised domain adaptation. In CVPR, pages 3723–3732, 2018. 2 [63] Inkyu Shin, Yi-Hsuan Tsai, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Sparsh Garg, In So Kweon, and Kuk- Jin Yoon. MM-TTA: multi-modal test-time adaptation for 3d semantic segmentation. In CVPR, pages 16907–16916, 2022. 1, 3[64] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test- time prompt tuning for zero-shot generalization in vision- language models. In NeurIPS, 2022. 1, 3 [65] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self- supervision for generalization under distribution shifts. In ICML, pages 9229–9248, 2020. 1, 2, 3, 4 [66] Rishabh Tiwari, KrishnaTeja Killamsetty, Rishabh K. Iyer, and Pradeep Shenoy. GCR: gradient coreset based replay buffer selection for continual learning. In CVPR, pages 99– 108, 2022. 3 [67] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR, pages 7472–7481, 2018. 2 [68] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, pages 4068–4076, 2015. 2 [69] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971, 2017. 1 [70] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Ol- shausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In ICLR, 2021. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 15, 16, 17 [71] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng., 2022. 1 [72] Mei Wang and Weihong Deng. Deep visual domain adapta- tion: A survey. Neurocomputing, 312:135–153, 2018. 1 [73] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Con- tinual test-time domain adaptation. In CVPR, pages 7191– 7201, 2022. 1, 2, 3, 4, 6, 7, 8, 13, 14, 15, 16, 17 [74] Markus Wulfmeier, Alex Bewley, and Ingmar Posner. Incre- mental adversarial domain adaptation for continually chang- ing environments. In ICRA, pages 4489–4495, 2018. 3 [75] Binhui Xie, Shuang Li, Mingjia Li, Chi Harold Liu, Gao Huang, and Guoren Wang. Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., pages 1–17, 2023. 2 [76] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987–5995, 2017. 6 [77] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In ICCV, pages 1426– 1435, 2019. 2 [78] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In ICLR, 2021. 3 [79] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adapta- tion. In ICCV, pages 8978–8987, 2021. 3 [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 6 [81] Marvin Mengxin Zhang, Sergey Levine, and Chelsea Finn. MEMO: Test time robustness via adaptation and augmenta- tion. In NeurIPS, 2022. 1, 4 [82] Yizhe Zhang, Shubhankar Borse, Hong Cai, and Fatih Porikli. Auxadapt: Stable and efficient test-time adaptation for temporally consistent video semantic segmentation. In WACV, pages 2633–2642, 2022. 1, 3 [83] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 2022. 1 [84] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do- main generalization with mixstyle. In ICLR, 2021. 3 [85] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic seg- mentation via class-balanced self-training. In ECCV, pages 289–305, 2018. 26. Appendix 6.1. Discussion Societal impact. RoTTA enables adapting pre-trained models on continually changing distributions with correl- atively sampled test streams without any more raw data or label requirements. Thus, our work may have a positive im- pact on communities to effectively deploy and adapt models in various real-world scenarios, which is economically and environmentally friendly. And since no training data is re- quired, this protects data privacy and has potential commer- cial value. We carry out experiments on benchmark datasets and do not notice any societal issues. It does not involve sensitive attributes. Future work. Our work suggests a few promising direc- tions for future work. Firstly, the proposed RoTTA is a preliminary attempt to perform test-time adaptation for the more realistic test stream under the setup PTTA. One could experiment to improve the algorithm by replacing some parts of RoTTA. More importantly, we hope that with this work, we can open a path to the original goal of test-time adaptation, which is performing test-time adaptation in real- world scenarios. Thus, one could improve PTTA to make it more realistic. Limitations. RoTTA achieves excellent performance on various tasks under the setup PTTA as demonstrated in Sec- tion 4 in the main paper, but we still find some limitations of it. Firstly, the adopted robust batch normalization (RBN) is a naive solution to the normalization of the correlatively sampled batch of data. This requires careful design of the value of α in RBN. Secondly, we observe that during the adaptation procedure of some methods like PL [39] and TENT [70], the model collapse finally. Although we de- sign many strategies to stabilize the adaptation and model collapse never occurs in the experiments of RoTTA, we are still missing a way to recover the model from the collapse state as a remedy. Thirdly, category similarity is only one kind of correlation. Although we conduct experiments on different datasets with Dirichlet distribution to simulate cor- relatively sampled test streams, we still need to validate our approach in some real-world scenarios. 6.2. Sensitivity to different hyper-parameters In this section, we conduct a detailed sensitivity analy- sis of the hyperparameters involved in RoTTA. All experi- ments are conducted on CIFAR100→CIFAR100-C, and the corruptions changes as motion, snow, fog, shot, defocus, contrast, zoom, brightness, frost, elastic, glass, gaussian, pixelate, jpeg, and impulse, and test streams are sampled correlatively with the Dirichlet parameter δ = 0.1. When we investigate the sensitivity to a specific hyperparameter, other hyperparameters are fixed to the default values, i.e., λt = 1.0, λu = 1.0, α = 0.05, and ν = 0.001, for all experiments. Table 7. Classification error with different value of λt/λu. λt/λu 0.0/2.0 0.5/1.5 1.0/1.0 1.5/ 0.5 2.0/ 0.0 CIFAR100-C 57.5 36.9 35.0 35.9 38.9 Trade-off between timeliness and uncertainty. When updating the memory bank, we take the timeliness and uncertainty of samples into account simultaneously, and λt and λu will make a trade-off between them. In Table 7, we show the results of RoTTA with varying λt/λu, i.e., λt/λu ∈ {0.0/2.0, 0.5/1.5, 1.0/1.0, 1.5/0.5, 2.0/0.0}. When we consider both of them, the results are relatively stable (35.0-36.9%). When we only think about one side, the performance drops significantly. For example, when we set λt/λu = 0.0/2.0 which means only considering uncer- tainty, the performance drops 22.5%. That’s because some confident samples get stuck in the memory bank, making it not work the way we design it. Table 8. Classification error with varying α α 0.5 0.1 0.05 0.01 0.005 0.001 CIFAR100-C 39.0 36.0 35.0 36.0 38.1 41.5 Sensitivity to α. We show the results of RoTTA with vary- ing α, i.e., α ∈ {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} in Ta- ble 8. A larger value of α means updating the global statis- tics faster and vice versa. We can see that RoTTA achieves competitive results (35.0 − 36.0%) at appropriate values of α, i.e., α ∈ {0.1, 0.05, 0.01}. Updating too aggressively or too gently can lead to unreliable estimates of statistics. Table 9. Classification error with varying ν ν 0.05 0.01 0.005 0.001 0.0005 0.0001 CIFAR100-C 44.8 39.1 37.1 35.0 37.6 43.6 Sensitivity to ν. We show the results of RoTTA with vary- ing ν, i.e., ν ∈ {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in Table 9. As we can see, the best performance is achieved at ν = 0.001. Updating the teacher model too quickly or too slowly can cause performance degradation. 6.3. Additional experiment details and results 6.3.1 Compared methods BN [53] utilizes statistics of the current batch of data to nor- malize their feature maps without tuning any parameters. PL [39] is based on BN [53], and adopts pseudo labels to train the affine parameters in BN layers.TENT [70] is the first to propose fully test-time adaptation. It adopts test-time batch normalization and utilizes entropy minimization to train the affine parameters of BN layers. We reimplement it following the released code https:// github.com/DequanWang/tent. LAME [5] adapts the output of the pre-trained model by optimizing a group of latent variables without tuning any in- ner parts of the model. We reimplement it following the re- leased code https://github.com/fiveai/LAME. CoTTA [73] considers performing test-time adapta- tion on continually changing distributions and pro- pose augmentation-averaged pseudo-labels and stochastic restoration to address error accumulation and catastrophic forgetting. We reimplement it following the released code https://github.com/qinenergy/cotta. NOTE [19] proposes instance-aware normalization and prediction-balanced reservoir sampling to stable the adapta- tion on temporally correlated test streams. We reimplement it following the released code https://github.com/ TaesikGong/NOTE. 6.3.2 Simulate correlatively sampling As we described in the scenarios of autonomous driving that the car will follow more vehicles on the highway or will en- counter more pedestrians on the sidewalk, so we use the same category to simulate correlation. From a macro point of view, the test distribution Ptest changes continually as P0, P1, ...,P∞. During the period when Ptest = Pt, we adopt Dirichlet distribution to simulate correlatively sam- pled test stream. More specifically, we consider dividing samples of C classes into T slots. Firstly, we utilize Dirich- let distribution with parameter γ to generate the partition criterion q ∈ RC×T . Then for each class c, we split samples into T parts according to qc and assign each part to each slot respectively. Finally, we concatenate all slots to sim- ulate the correlatively sampled test stream for Ptest = Pt. And as Ptest changes, we use the above method again to generate the test stream. 6.3.3 Detailed results of different orders We report the average classification error of ten different distribution changing orders in Table 6 of the main pa- per. And then we present the specific results here, includ- ing Table 10, 11, 12, 13, 14, 15, 16, 17, 18, and 19 for CIFAR10→CIFAR10-C and Table 20, 21, 22, 23, 24, 25, 26, 27, 28, and 29 for CIFAR100 →CIFAR100-C. We can see consistently superior performance of RoTTA. One thing to mention is that on DomainNet we use alphabetical order to determine the order of domain changes.Table 10. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 9.3 58.5 72.3 34.8 42.0 54.3 72.9 30.3 46.9 26.6 65.7 41.3 25.1 26.0 46.7 43.5BN [53] 71.1 75.2 76.8 74.2 73.7 80.1 79.3 77.5 73.8 77.7 77.2 73.3 73.8 72.7 71.7 75.2PL [39] 71.7 75.9 80.2 78.4 80.2 85.2 85.3 85.4 85.1 86.7 87.9 87.9 88.1 88.3 87.9 83.6TENT [70] 71.6 75.9 81.3 80.5 82.3 85.6 87.1 87.0 87.1 88.1 88.2 87.8 87.9 88.3 88.2 84.4LAME [5] 5.4 56.8 73.1 29.1 37.0 50.5 71.4 22.3 42.8 18.6 65.5 37.3 18.8 20.4 43.6 39.5CoTTA [73] 75.0 79.8 83.1 83.4 83.2 84.0 84.5 83.2 83.5 83.3 83.6 83.0 83.0 83.4 83.7 82.6NOTE [19] 10.1 29.9 47.1 23.4 28.4 48.4 46.1 41.8 26.9 36.1 37.5 25.0 25.0 23.2 14.2 30.9 RoTTA 10.4 26.6 37.5 23.9 17.0 40.9 39.7 30.1 18.0 29.9 30.1 23.6 21.7 17.6 19.0 25.7(+5.2) Table 11. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 30.3 65.7 42.0 41.3 46.7 26.0 46.9 26.6 72.3 9.3 54.3 72.9 58.5 25.1 34.8 43.5BN [53] 77.6 75.8 73.4 74.1 73.1 72.5 72.9 77.1 77.2 72.2 79.9 79.9 75.5 74.6 72.9 75.2PL [39] 77.6 77.1 76.6 78.3 77.5 79.8 82.0 84.8 86.1 83.5 87.8 87.1 86.5 85.6 85.7 82.4TENT [70] 78.5 78.2 79.2 81.8 84.8 84.8 86.4 87.3 87.9 86.7 87.3 87.8 87.2 87.5 87.1 84.8LAME [5] 22.5 65.2 37.0 37.1 44.0 20.3 41.7 18.7 72.8 5.2 51.2 71.5 57.0 19.0 29.4 39.5CoTTA [73]78.5 81.0 82.8 84.1 84.9 83.4 83.5 83.5 84.5 83.3 84.7 84.6 83.0 84.4 83.4 83.3NOTE [19]35.4 36.1 22.1 21.3 11.6 24.8 24.5 36.0 37.7 18.4 49.0 47.4 43.9 30.4 29.2 31.2 RoTTA 33.2 33.3 19.8 24.1 24.9 20.5 16.2 31.7 28.4 11.8 43.1 36.9 32.5 20.7 20.6 26.5(+4.7) Table 12. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 46.7 46.9 72.3 65.7 25.1 41.3 54.3 42.0 26.6 30.3 58.5 9.3 72.9 34.8 26.0 43.5BN [53] 72.3 72.6 76.9 77.1 74.8 73.5 80.0 73.2 77.4 78.6 76.4 71.0 79.1 73.9 71.5 75.2PL [39] 72.4 75.3 80.7 82.6 83.3 83.5 86.6 85.7 86.6 88.4 87.5 86.6 88.3 88.2 86.8 84.1TENT [70] 73.5 77.9 85.5 86.9 87.6 87.8 88.3 87.7 88.6 89.2 88.5 88.5 89.3 88.6 88.6 86.4LAME [5] 43.5 42.3 73.1 65.3 19.2 37.3 51.1 36.8 18.5 22.5 56.9 5.5 71.1 29.1 20.5 39.5CoTTA [73]79.4 80.3 83.8 83.9 83.9 83.4 85.0 83.2 85.1 84.3 83.9 83.3 84.7 83.9 82.5 83.4NOTE [19] 9.6 21.8 40.1 31.0 25.5 22.6 44.8 22.8 33.2 39.4 33.2 18.1 50.0 28.3 29.8 30.0 RoTTA 18.4 17.9 38.4 31.9 23.3 19.8 40.7 17.4 31.4 29.8 27.8 11.3 43.8 19.7 18.8 26.0(+4.0) Table 13. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 65.7 26.0 54.3 58.5 25.1 26.6 9.3 72.9 46.9 41.3 46.7 72.3 34.8 30.3 42.0 43.5BN [53] 76.4 72.0 80.4 76.2 74.8 77.0 71.1 79.6 73.8 74.4 73.0 77.0 72.5 78.3 72.5 75.3PL [39] 77.0 73.3 82.4 79.8 81.0 82.3 79.5 84.4 82.7 83.5 83.5 85.5 84.8 87.0 84.5 82.1TENT [70]76.9 74.6 82.3 81.7 82.0 84.9 84.8 87.3 86.6 87.3 87.6 89.2 88.3 88.9 87.3 84.6LAME [5] 65.3 20.6 50.9 56.7 19.2 18.8 5.4 71.8 42.8 37.2 43.3 73.2 29.4 22.6 36.9 39.6CoTTA [73]77.4 77.6 83.8 81.9 82.2 82.6 80.4 83.3 82.3 81.5 82.7 82.6 81.1 82.9 81.0 81.6NOTE [19]34.0 20.9 43.1 36.6 24.0 36.4 12.1 48.0 25.9 23.9 13.4 38.1 25.0 43.2 24.2 29.9 RoTTA 35.0 21.1 43.9 29.2 22.1 29.7 10.8 44.6 25.3 22.7 24.6 29.4 26.9 34.4 16.1 27.7(+2.2) Table 14. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 58.5 54.3 42.0 25.1 26.0 72.9 9.3 34.8 41.3 30.3 72.3 65.7 46.7 46.9 26.6 43.5BN [53] 76.0 79.6 73.3 75.2 72.9 79.8 71.1 73.5 74.1 78.6 77.4 76.1 72.0 73.8 76.4 75.3PL [39] 76.7 81.3 77.4 80.3 81.2 86.3 83.3 85.9 86.2 87.7 88.1 88.4 87.4 87.6 87.7 84.4TENT [70] 76.4 80.2 77.8 81.2 83.0 87.1 85.6 87.2 87.6 88.7 88.6 88.9 88.5 88.6 88.2 85.2LAME [5] 56.9 50.7 37.0 19.0 20.3 71.5 5.4 29.2 37.2 22.5 73.0 65.3 43.8 42.4 18.7 39.5CoTTA [73]77.1 83.6 84.1 84.8 84.4 85.2 84.0 84.3 84.9 84.9 85.0 84.7 85.3 84.4 84.3 84.1NOTE [19] 27.8 52.2 24.5 22.3 21.6 44.5 14.5 21.3 25.9 42.5 38.8 36.0 16.7 28.1 40.6 30.5 RoTTA 25.9 43.3 17.7 22.1 20.2 41.5 12.2 22.9 22.5 31.2 33.8 26.0 31.4 17.7 27.6 26.4(+4.1)Table 15. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 34.8 25.1 26.0 65.7 46.9 46.7 42.0 9.3 41.3 26.6 54.3 72.3 58.5 30.3 72.9 43.5BN [53] 73.2 73.4 72.7 77.2 73.7 72.5 72.9 71.0 74.1 77.7 80.0 76.9 75.5 78.3 79.0 75.2PL [39] 73.9 75.0 75.6 81.0 79.9 80.6 82.0 83.2 85.3 87.3 88.3 87.5 87.5 87.5 88.2 82.9TENT [70] 74.3 77.4 80.1 86.2 86.7 87.3 87.9 87.4 88.2 89.0 89.2 89.0 88.3 89.7 89.2 86.0LAME [5] 29.5 19.0 20.3 65.3 42.4 43.4 36.8 5.4 37.2 18.6 51.2 73.2 57.0 22.6 71.3 39.5CoTTA [73]77.1 80.6 83.1 84.4 83.9 84.2 83.1 82.6 84.4 84.2 84.5 84.6 82.7 83.8 84.9 83.2NOTE [19] 18.0 22.1 20.6 35.6 26.9 13.6 26.5 17.3 27.2 37.0 48.3 38.8 42.6 41.9 49.7 31.1 RoTTA 18.1 21.3 18.8 33.6 23.6 16.5 15.1 11.2 21.9 30.7 39.6 26.8 33.7 27.8 39.5 25.2(+5.9) Table 16. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 41.3 72.9 30.3 46.7 42.0 54.3 58.5 25.1 46.9 34.8 9.3 26.6 65.7 26.0 72.3 43.5BN [53] 73.8 79.1 77.9 73.0 73.7 80.1 75.7 74.4 73.7 74.0 71.7 77.0 75.9 72.8 76.2 75.3PL [39] 74.2 80.9 80.4 79.5 81.8 85.9 83.9 85.1 84.7 85.9 85.9 86.7 87.2 87.0 87.8 83.8TENT [70]73.9 80.3 81.8 81.6 83.6 86.3 85.6 85.7 86.4 87.7 87.4 88.8 88.8 88.5 88.4 85.0LAME [5] 37.4 71.8 22.4 43.5 37.0 50.5 57.0 19.0 42.8 29.1 5.4 18.7 65.2 20.4 72.9 39.5CoTTA [73]76.5 82.2 82.8 85.0 82.9 85.0 83.0 82.9 83.5 83.4 82.6 83.7 83.2 83.3 83.6 82.9NOTE [19]21.1 41.4 36.3 10.2 21.7 46.7 37.5 26.4 26.1 21.4 14.3 37.9 38.5 24.4 40.7 29.6 RoTTA 22.2 44.9 35.2 18.8 19.7 41.5 28.5 23.2 21.2 18.6 12.4 30.0 27.4 20.0 31.2 26.3(+3.3) Table 17. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 46.9 34.8 42.0 65.7 72.3 54.3 30.3 26.0 46.7 58.5 41.3 25.1 9.3 26.6 72.9 43.5BN [53] 72.8 72.7 73.3 77.2 77.3 80.0 77.6 72.6 73.3 76.6 73.8 74.1 70.3 77.5 79.0 75.2PL [39] 73.2 74.6 76.5 81.7 82.8 84.6 85.1 84.6 86.2 86.4 86.1 87.1 86.8 88.4 88.1 83.5TENT [70] 73.7 74.3 77.1 82.5 84.3 86.9 87.4 86.6 88.0 88.5 88.1 88.5 88.4 89.4 88.9 84.8LAME [5] 42.5 29.3 37.0 65.3 73.2 50.5 22.5 20.5 43.5 56.9 37.1 18.9 5.4 18.5 71.3 39.5CoTTA [73]76.3 79.8 82.4 83.3 83.8 84.5 83.1 82.7 84.7 82.9 83.0 83.3 81.4 83.8 83.8 82.6NOTE [19] 18.5 18.8 23.6 36.5 33.7 47.8 38.6 22.8 13.0 40.0 29.2 26.3 17.5 44.0 52.9 30.9 RoTTA 17.0 17.5 16.5 33.8 33.3 42.7 29.4 18.0 19.6 29.5 20.7 22.1 11.5 29.5 38.1 25.3(+5.6) Table 18. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.3 42.0 72.9 26.0 25.1 30.3 72.3 41.3 65.7 9.3 46.7 34.8 58.5 46.9 26.6 43.5BN [53] 79.7 72.3 79.8 73.2 74.7 77.7 76.6 73.2 77.1 72.2 73.0 73.3 75.5 73.8 76.4 75.2PL [39] 79.6 73.2 81.3 77.3 79.1 83.0 83.2 83.0 85.5 84.3 87.0 86.9 86.4 86.5 87.6 82.9TENT [70] 79.5 74.1 84.2 82.2 84.5 86.5 86.7 85.9 87.2 86.6 86.8 87.3 86.9 87.4 87.3 84.9LAME [5] 50.8 36.9 71.3 20.6 19.2 22.4 72.5 37.2 65.4 5.2 43.3 29.1 57.0 42.4 18.7 39.5CoTTA [73]81.5 79.4 85.2 84.1 84.5 84.2 84.8 84.0 84.8 83.2 85.2 83.8 83.2 84.6 83.6 83.7NOTE [19]45.0 21.2 42.3 21.0 21.6 38.4 36.4 21.4 33.1 16.7 14.6 25.4 43.5 29.1 38.5 29.9 RoTTA 42.6 17.6 48.1 23.9 21.9 32.6 32.1 20.7 30.2 12.0 21.9 20.0 33.7 16.4 28.1 26.8(+3.1) Table 19. Average classification error of the task CIFAR10→ CIFAR10-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 46.7 72.3 46.9 42.0 41.3 54.3 30.3 26.0 58.5 26.6 65.7 72.9 25.1 34.8 9.3 43.5BN [53] 72.4 76.2 73.2 73.7 73.6 80.0 77.6 72.6 76.4 77.7 77.2 79.9 73.8 73.9 70.0 75.2PL [39] 73.0 78.2 76.7 79.7 81.6 85.6 86.0 85.3 87.2 88.2 88.3 88.9 88.5 89.2 88.2 84.3TENT [70] 73.6 80.9 83.1 85.6 87.1 88.5 88.8 88.4 89.2 89.3 89.0 89.0 89.3 89.9 89.1 86.7LAME [5] 43.5 73.2 42.3 37.0 37.2 50.5 22.5 20.5 57.0 18.6 65.5 71.5 18.8 29.1 5.6 39.5CoTTA [73]79.5 81.4 83.4 83.6 83.9 85.0 84.0 82.8 84.8 84.8 84.5 84.7 84.1 84.4 82.8 83.6NOTE [19] 9.6 43.6 26.5 24.8 23.9 46.9 38.0 23.4 34.0 41.2 41.5 45.0 27.6 25.8 19.0 31.4 RoTTA 18.4 36.0 21.1 15.6 23.0 41.7 30.8 19.1 34.1 31.1 31.3 39.9 26.0 18.8 12.8 26.6(+4.8)Table 20. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method brightnesspixelategaussianmotionzoom glass impulsejpeg defocuselasticshot frost snow fog contrast Avg. Source 29.5 74.7 73.0 30.8 28.8 54.1 39.4 41.2 29.3 37.2 68.0 45.8 39.5 50.3 55.1 46.4BN [53] 46.5 52.0 58.6 47.4 47.4 57.6 58.2 56.9 47.0 53.4 56.0 52.5 53.1 57.7 49.1 52.9PL [39] 48.5 60.7 77.1 85.9 91.5 95.5 95.8 96.6 96.8 96.9 97.3 97.5 97.6 97.7 97.9 88.9TENT [70] 49.8 69.4 92.2 96.0 96.7 97.3 97.5 97.9 97.5 97.9 98.0 98.2 98.2 98.2 98.2 92.2LAME [5] 21.7 75.1 72.7 22.9 20.6 49.0 32.1 33.3 21.2 28.0 66.8 40.0 30.6 43.9 51.3 40.6CoTTA [73] 46.8 48.4 54.7 48.7 48.6 53.5 55.4 52.8 49.8 51.8 53.5 52.9 54.1 56.7 53.6 52.1NOTE [19] 42.6 53.0 69.9 52.1 53.3 70.4 73.1 76.7 80.8 96.0 97.7 97.1 96.6 97.2 95.8 76.8 RoTTA 28.4 37.3 44.6 31.9 28.3 41.8 43.6 39.9 28.0 35.2 38.2 33.7 33.0 39.5 31.0 35.6(+5.0) Table 21. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method jpeg shot zoom frost contrastfog defocuselasticgaussianbrightnessglass impulsepixelatesnow motion Avg. Source 41.2 68.0 28.8 45.8 55.1 50.3 29.3 37.2 73.0 29.5 54.1 39.4 74.7 39.5 30.8 46.4BN [53] 58.3 56.8 47.8 51.8 48.9 57.3 46.8 53.5 57.8 45.5 57.1 58.5 51.7 53.3 48.8 52.9PL [39] 59.4 66.3 74.9 87.5 94.2 95.5 96.2 97.1 97.4 97.2 97.5 97.7 98.0 98.2 98.2 90.4TENT [70] 62.0 79.3 91.7 95.8 96.9 97.0 97.4 97.7 97.6 97.7 97.9 97.9 98.0 97.9 97.9 93.5LAME [5] 33.6 66.7 21.1 39.9 50.6 43.9 21.0 28.6 72.5 21.6 48.6 32.5 74.5 30.6 22.5 40.6CoTTA [73]54.6 54.1 49.6 52.1 52.7 58.0 50.3 53.3 55.0 49.1 55.4 55.7 51.0 54.6 52.1 53.2NOTE [19]60.4 63.0 49.9 55.7 47.0 65.2 59.4 76.6 90.9 87.2 96.8 97.0 97.3 96.7 96.8 76.0 RoTTA 43.9 45.3 31.0 37.3 35.7 41.2 27.7 34.8 39.7 26.6 39.5 41.9 32.0 33.0 30.5 36.0(+4.6) Table 22. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastdefocusgaussianshot snow frost glass zoom elasticjpeg pixelatebrightnessimpulsemotion fog Avg. Source 55.1 29.3 73.0 68.0 39.5 45.8 54.1 28.8 37.2 41.2 74.7 29.5 39.4 30.8 50.3 46.4BN [53] 49.4 47.2 58.6 56.2 52.7 52.0 57.9 46.1 54.4 57.7 50.5 46.2 58.2 47.6 58.5 52.9PL [39] 54.8 64.2 83.3 92.4 95.5 96.5 96.9 96.4 97.2 97.4 97.8 97.8 97.9 97.7 98.0 90.9TENT [70] 60.2 83.1 95.2 96.5 96.9 97.3 97.0 97.3 97.8 97.8 97.6 97.9 97.8 97.9 98.1 93.9LAME [5] 51.3 21.3 72.7 66.3 30.2 40.0 48.6 20.9 27.7 33.3 75.0 21.5 32.2 22.5 43.8 40.5CoTTA [73]52.1 48.6 55.1 52.7 53.4 51.9 55.9 49.2 53.2 52.8 49.2 49.7 56.2 50.7 58.1 52.6NOTE [19] 39.5 45.9 68.8 61.8 57.4 58.5 71.4 66.5 80.8 90.9 94.2 94.9 97.0 95.5 96.6 74.6 RoTTA 41.7 30.5 44.9 40.5 35.4 34.1 40.5 28.2 34.5 39.5 31.1 26.7 43.3 31.4 38.8 36.1(+4.4) Table 23. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method shot fog glass pixelatesnow elasticbrightnessimpulsedefocusfrost contrastgaussianmotionjpeg zoom Avg. Source 68.0 50.3 54.1 74.7 39.5 37.2 29.5 39.4 29.3 45.8 55.1 73.0 30.8 41.2 28.8 46.4BN [53] 57.5 58.6 58.5 50.5 52.7 53.1 45.9 57.9 47.0 51.5 47.8 58.2 48.2 57.1 47.7 52.8PL [39] 59.5 72.9 85.1 89.6 94.5 96.8 97.1 97.9 97.8 98.0 98.3 98.2 98.0 98.0 98.2 92.0TENT [70]60.3 81.4 95.0 96.6 97.0 97.3 97.3 97.7 97.7 97.7 97.8 97.7 97.6 97.6 97.9 93.8LAME [5] 66.4 43.2 49.0 75.2 30.2 28.5 21.6 32.5 21.2 39.5 52.0 72.8 22.3 33.1 20.5 40.5CoTTA [73]54.5 58.4 55.6 50.0 53.9 53.4 50.3 56.7 51.3 53.2 53.7 56.1 52.0 54.5 51.5 53.7NOTE [19]61.8 60.2 63.4 55.6 59.8 65.9 58.6 75.1 77.8 93.8 94.2 97.0 95.0 95.5 94.4 76.5 RoTTA 45.5 44.5 43.5 35.6 35.1 35.7 26.2 44.0 29.7 34.2 32.0 40.7 31.4 39.4 27.7 36.3(+4.2) Table 24. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method pixelateglass zoomsnow fog impulsebrightnessmotionfrost jpeg gaussianshot contrastdefocus elastic Avg. Source 74.7 54.1 28.8 39.5 50.3 39.4 29.5 30.8 45.8 41.2 73.0 68.0 55.1 29.3 37.2 46.4BN [53] 51.7 58.6 47.8 52.9 57.1 58.2 45.9 47.6 52.9 57.8 57.5 56.7 49.5 46.1 54.0 52.9PL [39] 52.4 68.0 73.4 87.9 93.7 96.1 95.7 96.0 96.5 96.7 97.5 97.7 97.7 97.3 97.7 89.6TENT [70] 53.5 77.8 91.1 96.0 97.0 97.6 97.4 97.6 97.9 98.1 98.1 98.0 98.1 97.9 98.1 92.9LAME [5] 74.8 48.2 21.1 30.6 43.4 32.5 21.6 23.0 39.6 33.3 72.7 66.5 51.5 20.7 27.5 40.5CoTTA [73]49.3 55.1 49.1 52.9 56.8 55.7 49.5 50.0 53.6 53.4 54.9 53.9 53.8 50.1 53.5 52.8NOTE [19] 52.2 64.9 47.5 57.0 61.9 67.3 60.4 67.8 77.4 90.6 97.1 96.8 92.8 95.9 96.6 75.1 RoTTA 36.4 44.4 29.7 36.5 41.0 44.1 26.8 29.5 33.0 40.3 40.3 38.2 33.9 28.5 34.9 35.8(+4.7)Table 25. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method motionsnow fog shot defocuscontrastzoom brightnessfrost elasticglass gaussianpixelatejpeg impulse Avg. Source 30.8 39.5 50.3 68.0 29.3 55.1 28.8 29.5 45.8 37.2 54.1 73.0 74.7 41.2 39.4 46.4BN [53] 48.5 54.0 58.9 56.2 46.4 48.0 47.0 45.4 52.9 53.4 57.1 58.2 51.7 57.1 58.8 52.9PL [39] 50.6 62.1 73.9 87.8 90.8 96.0 94.8 96.4 97.4 97.2 97.4 97.4 97.3 97.4 97.4 88.9TENT [70] 53.3 77.6 93.0 96.5 96.7 97.5 97.1 97.5 97.3 97.2 97.1 97.7 97.6 98.0 98.3 92.8LAME [5] 22.4 30.4 43.9 66.3 21.3 51.7 20.6 21.8 39.6 28.0 48.7 72.8 74.6 33.1 32.3 40.5CoTTA [73]49.2 52.7 56.8 53.0 48.7 51.7 49.4 48.7 52.5 52.2 54.3 54.9 49.6 53.4 56.2 52.2NOTE [19] 45.7 53.0 58.2 65.6 54.2 52.0 59.8 63.5 74.8 91.8 98.1 98.3 96.8 97.0 98.2 73.8 RoTTA 31.8 36.7 40.9 42.1 30.0 33.6 27.9 25.4 32.3 34.0 38.8 38.7 31.3 38.0 42.9 35.0(+5.5) Table 26. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method frost impulsejpeg contrastzoom glass pixelatesnow defocusmotionbrightnesselasticshot fog gaussian Avg. Source 45.8 39.4 41.2 55.1 28.8 54.1 74.7 39.5 29.3 30.8 29.5 37.2 68.0 50.3 73.0 46.4BN [53] 52.9 58.8 57.6 48.2 47.4 57.6 50.9 52.4 47.0 47.2 45.1 54.0 56.4 57.7 58.2 52.8PL [39] 56.9 73.3 86.7 94.4 95.8 97.3 97.2 97.4 97.6 97.4 97.7 97.6 97.8 98.3 98.1 92.2TENT [70]60.1 84.2 95.7 97.2 97.4 97.9 97.8 98.0 98.1 98.2 98.3 98.4 98.4 98.4 98.4 94.4LAME [5] 39.9 32.4 33.4 51.4 20.6 49.0 74.4 31.3 21.2 22.6 21.9 28.1 66.9 43.9 72.5 40.6CoTTA [73]51.5 55.3 54.3 51.8 49.4 55.3 50.7 54.2 51.4 50.6 49.5 53.6 55.0 57.1 55.8 53.0NOTE [19]51.6 60.9 60.3 45.4 54.3 70.8 68.8 75.0 75.7 87.1 94.7 95.6 96.7 96.4 97.2 75.4 RoTTA 40.0 46.3 42.8 36.4 29.2 42.3 33.2 34.4 28.4 29.2 26.4 34.5 38.5 39.8 39.3 36.0(+4.6) Table 27. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method defocusmotionzoom shot gaussianglass jpeg fog contrastpixelatefrost snow brightnesselastic impulse Avg. Source 29.3 30.8 28.8 68.0 73.0 54.1 41.2 50.3 55.1 74.7 45.8 39.5 29.5 37.2 39.4 46.4BN [53] 47.1 48.6 47.8 56.2 57.6 57.6 57.6 57.5 48.7 50.6 51.8 53.2 46.9 53.5 58.8 52.9PL [39] 48.8 58.7 69.9 88.0 95.1 96.6 96.7 96.9 97.4 97.4 98.2 98.2 98.2 98.3 98.5 89.1TENT [70] 51.0 67.6 85.8 95.9 97.2 97.5 97.2 97.7 98.1 97.9 97.7 97.7 98.0 98.0 98.2 91.7LAME [5] 21.2 22.8 21.1 66.3 72.8 49.0 33.3 44.8 51.7 74.9 39.8 31.2 21.3 27.3 32.3 40.6CoTTA [73]48.4 48.8 48.2 52.9 54.0 53.8 52.7 57.2 52.6 48.6 51.8 53.9 49.4 52.3 56.0 52.0NOTE [19] 45.1 46.7 49.1 67.3 65.5 69.4 75.5 80.3 83.8 96.0 97.6 97.1 96.1 97.9 98.7 77.7 RoTTA 29.6 31.3 28.8 43.9 41.5 41.3 40.9 39.8 32.1 32.6 33.1 33.0 26.5 34.5 42.9 35.4(+5.2) Table 28. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method glass zoom impulsefog snow jpeg gaussianfrost shot brightnesscontrastmotionpixelatedefocus elastic Avg. Source 54.1 28.8 39.4 50.3 39.5 41.2 73.0 45.8 68.0 29.5 55.1 30.8 74.7 29.3 37.2 46.4BN [53] 58.8 47.7 59.2 57.6 52.7 56.9 58.2 52.0 56.7 45.5 47.8 48.2 51.7 46.1 54.0 52.9PL [39] 60.1 59.5 75.1 85.7 91.5 94.6 96.5 97.1 97.4 97.3 98.0 97.7 97.9 97.8 97.7 89.6TENT [70] 61.6 71.5 91.0 95.9 96.6 97.1 96.9 97.3 97.4 97.2 97.9 98.0 98.1 97.9 97.8 92.8LAME [5] 48.6 20.6 32.3 44.4 30.2 33.6 72.4 40.0 66.3 21.6 52.0 22.8 74.6 20.7 27.5 40.5CoTTA [73]56.4 48.9 56.1 57.8 54.1 54.2 56.2 53.6 55.4 50.0 53.6 51.6 51.2 50.7 54.4 53.6NOTE [19]62.5 46.3 61.5 61.1 58.6 68.4 76.1 78.3 92.0 93.4 96.1 95.4 96.2 95.8 96.4 78.5 RoTTA 45.5 30.0 45.9 42.6 35.3 41.8 42.2 34.5 40.2 27.3 31.3 30.2 32.7 28.1 34.9 36.2(+4.3) Table 29. Average classification error of the task CIFAR100 → CIFAR100-C while continually adapting to different corruptions at the highest severity 5 with correlatively sampled test stream under the proposed setup PTTA. Time t− − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − → Method contrastgaussiandefocuszoom frost glass jpeg fog pixelateelasticshot impulsesnow motion brightness Avg. Source 55.1 73.0 29.3 28.8 45.8 54.1 41.2 50.3 74.7 37.2 68.0 39.4 39.5 30.8 29.5 46.4BN [53] 49.5 58.8 47.0 46.5 52.2 57.6 57.6 57.6 51.7 53.5 56.0 58.5 53.1 47.6 46.3 52.9PL [39] 53.6 70.4 76.0 85.1 91.2 95.2 96.0 97.0 96.9 97.3 97.3 97.6 97.5 97.6 97.7 89.8TENT [70] 60.2 89.1 95.0 96.2 96.9 97.0 96.5 97.0 97.0 97.2 97.6 97.8 97.5 97.9 97.7 94.0LAME [5] 51.3 72.5 21.5 21.0 39.6 49.0 33.3 44.8 74.8 28.0 66.8 32.5 30.6 22.5 21.4 40.6CoTTA [73]52.3 55.3 49.5 48.1 52.1 54.8 52.7 56.9 50.6 52.6 53.7 55.8 54.6 50.6 50.5 52.7NOTE [19] 39.1 64.7 48.9 50.6 59.1 70.1 71.7 75.0 85.2 95.7 96.9 98.4 96.0 95.9 94.9 76.1 RoTTA 41.4 46.2 30.5 28.5 36.0 40.9 40.5 39.6 33.0 35.0 38.2 43.1 33.9 30.7 27.1 36.3(+4.3)",
      "meta_data": {
        "arxiv_id": "2303.13899v1",
        "authors": [
          "Longhui Yuan",
          "Binhui Xie",
          "Shuang Li"
        ],
        "published_date": "2023-03-24T10:19:14Z",
        "pdf_url": "https://arxiv.org/pdf/2303.13899v1.pdf",
        "github_url": "https://github.com/BIT-DA/RoTTA"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the limitations of prior Test-Time Adaptation (TTA) methods in dynamic real-world scenarios, which are characterized by continually changing data distributions and correlative test data sampling over time. The authors propose a new, more realistic TTA setup called Practical Test-Time Adaptation (PTTA). Their main technical contribution is Robust Test-Time Adaptation (RoTTA), a method designed to effectively adapt pre-trained models in PTTA settings. RoTTA achieves state-of-the-art performance, outperforming baselines significantly on common TTA benchmarks (CIFAR-10-C, CIFAR-100-C, and DomainNet) by reducing average classification error.",
        "methodology": "The proposed Robust Test-Time Adaptation (RoTTA) method comprises three key components: 1) Robust Batch Normalization (RBN): It replaces traditional batch normalization statistics with global running mean and variance, initialized from the pre-trained model and updated via exponential moving average (EMA) using statistics from a memory bank, to handle correlatively sampled test data streams. 2) Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) for the Memory Bank: A memory bank is maintained using pseudo-labels from model predictions. Its capacity is equally distributed among categories. Samples are prioritized based on a heuristic score that combines their age (timeliness, increasing over time) and uncertainty (entropy of prediction), ensuring that newer and more confident samples are kept while maintaining category balance. 3) Time-Aware Robust Training with a Teacher-Student Model: A teacher-student architecture is used, where the student model is updated by minimizing a robust loss on memory bank samples, and the teacher model is updated via EMA of the student's parameters. A timeliness reweighting strategy is applied to the loss, giving less weight to older samples to stabilize adaptation and reduce error accumulation. For efficiency and stability, only affine parameters in RBN are trained during adaptation.",
        "experimental_setup": "The method was evaluated on: 1) CIFAR10-C and CIFAR100-C datasets, commonly used TTA benchmarks, under 15 corruption types at severity 5 to simulate continually changing distributions. 2) DomainNet, a large-scale domain adaptation dataset (0.6 million images, 345 classes, 6 domains) to test generalization under a huge domain gap. Correlative sampling for all datasets was simulated using a Dirichlet distribution (default parameter δ = 0.1). Pre-trained models included WildResNet-28 for CIFAR10-C and ResNeXt-29 for CIFAR100-C (from RobustBench), and ResNet-101 for DomainNet. For optimization, the Adam optimizer was used with a learning rate of 1.0 × 10⁻³ and batch size 64. The memory bank capacity was set to N=64. Hyperparameters for RoTTA were unified across experiments: α = 0.05, ν = 0.001, λt = 1.0, λu = 1.0, and δ = 0.1. The approach was validated by comparing average classification error against state-of-the-art baselines (BN, PL, TENT, LAME, CoTTA, NOTE) and through ablation studies on RoTTA's components, as well as sensitivity analyses to hyperparameters, distribution changing order, and batch size.",
        "limitations": "The Robust Batch Normalization (RBN) component is considered a naive solution for normalizing correlatively sampled data and requires careful tuning of its 'α' parameter. The current method lacks a mechanism to explicitly recover the model from a collapsed state, a phenomenon observed in some baseline adaptation procedures. Furthermore, while the experiments use Dirichlet distribution to simulate correlative sampling based on category similarity, it is noted that category similarity is only one type of correlation, and the approach needs further validation in more diverse, real-world scenarios.",
        "future_research_directions": "Future work could focus on improving the RoTTA algorithm by replacing or refining some of its current components. More broadly, the authors hope this work encourages further exploration into making the Practical Test-Time Adaptation (PTTA) setup even more realistic, ultimately paving the way for deploying and adapting models in complex, dynamic real-world applications through test-time adaptation algorithms.",
        "experimental_code": "import torch\nimport torch.nn as nn\nfrom ..utils import memory\nfrom .base_adapter import BaseAdapter\nfrom copy import deepcopy\nfrom .base_adapter import softmax_entropy\nfrom ..utils.bn_layers import RobustBN1d, RobustBN2d\nfrom ..utils.utils import set_named_submodule, get_named_submodule\nfrom ..utils.custom_transforms import get_tta_transforms\n\n\nclass RoTTA(BaseAdapter):\n    def __init__(self, cfg, model, optimizer):\n        super(RoTTA, self).__init__(cfg, model, optimizer)\n        self.mem = memory.CSTU(capacity=self.cfg.ADAPTER.RoTTA.MEMORY_SIZE, num_class=cfg.CORRUPTION.NUM_CLASS, lambda_t=cfg.ADAPTER.RoTTA.LAMBDA_T, lambda_u=cfg.ADAPTER.RoTTA.LAMBDA_U)\n        self.model_ema = self.build_ema(self.model)\n        self.transform = get_tta_transforms(cfg)\n        self.nu = cfg.ADAPTER.RoTTA.NU\n        self.update_frequency = cfg.ADAPTER.RoTTA.UPDATE_FREQUENCY  # actually the same as the size of memory bank\n        self.current_instance = 0\n\n    @torch.enable_grad()\n    def forward_and_adapt(self, batch_data, model, optimizer):\n        # batch data\n        with torch.no_grad():\n            model.eval()\n            self.model_ema.eval()\n            ema_out = self.model_ema(batch_data)\n            predict = torch.softmax(ema_out, dim=1)\n            pseudo_label = torch.argmax(predict, dim=1)\n            entropy = torch.sum(- predict * torch.log(predict + 1e-6), dim=1)\n\n        # add into memory\n        for i, data in enumerate(batch_data):\n            p_l = pseudo_label[i].item()\n            uncertainty = entropy[i].item()\n            current_instance = (data, p_l, uncertainty)\n            self.mem.add_instance(current_instance)\n            self.current_instance += 1\n\n            if self.current_instance % self.update_frequency == 0:\n                self.update_model(model, optimizer)\n\n        return ema_out\n\n    def update_model(self, model, optimizer):\n        model.train()\n        self.model_ema.train()\n        # get memory data\n        sup_data, ages = self.mem.get_memory()\n        l_sup = None\n        if len(sup_data) > 0:\n            sup_data = torch.stack(sup_data)\n            strong_sup_aug = self.transform(sup_data)\n            ema_sup_out = self.model_ema(sup_data)\n            stu_sup_out = model(strong_sup_aug)\n            instance_weight = timeliness_reweighting(ages)\n            l_sup = (softmax_entropy(stu_sup_out, ema_sup_out) * instance_weight).mean()\n\n        l = l_sup\n        if l is not None:\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n\n        self.update_ema_variables(self.model_ema, self.model, self.nu)\n\n    @staticmethod\n    def update_ema_variables(ema_model, model, nu):\n        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n            ema_param.data[:] = (1 - nu) * ema_param[:].data[:] + nu * param[:].data[:]\n        return ema_model\n\n    def configure_model(self, model: nn.Module):\n\n        model.requires_grad_(False)\n        normlayer_names = []\n\n        for name, sub_module in model.named_modules():\n            if isinstance(sub_module, nn.BatchNorm1d) or isinstance(sub_module, nn.BatchNorm2d):\n                normlayer_names.append(name)\n\n        for name in normlayer_names:\n            bn_layer = get_named_submodule(model, name)\n            if isinstance(bn_layer, nn.BatchNorm1d):\n                NewBN = RobustBN1d\n            elif isinstance(bn_layer, nn.BatchNorm2d):\n                NewBN = RobustBN2d\n            else:\n                raise RuntimeError()\n\n            momentum_bn = NewBN(bn_layer,\n                                self.cfg.ADAPTER.RoTTA.ALPHA)\n            momentum_bn.requires_grad_(True)\n            set_named_submodule(model, name, momentum_bn)\n        return model\n\n\ndef timeliness_reweighting(ages):\n    if isinstance(ages, list):\n        ages = torch.tensor(ages).float().cuda()\n    return torch.exp(-ages) / (1 + torch.exp(-ages))\nimport random\nimport copy\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\nclass MemoryItem:\n    def __init__(self, data=None, uncertainty=0, age=0):\n        self.data = data\n        self.uncertainty = uncertainty\n        self.age = age\n\n    def increase_age(self):\n        if not self.empty():\n            self.age += 1\n\n    def get_data(self):\n        return self.data, self.uncertainty, self.age\n\n    def empty(self):\n        return self.data == \"empty\"\n\n\nclass CSTU:\n    def __init__(self, capacity, num_class, lambda_t=1.0, lambda_u=1.0):\n        self.capacity = capacity\n        self.num_class = num_class\n        self.per_class = self.capacity / self.num_class\n        self.lambda_t = lambda_t\n        self.lambda_u = lambda_u\n\n        self.data: list[list[MemoryItem]] = [[] for _ in range(self.num_class)]\n\n    def get_occupancy(self):\n        occupancy = 0\n        for data_per_cls in self.data:\n            occupancy += len(data_per_cls)\n        return occupancy\n\n    def per_class_dist(self):\n        per_class_occupied = [0] * self.num_class\n        for cls, class_list in enumerate(self.data):\n            per_class_occupied[cls] = len(class_list)\n\n        return per_class_occupied\n\n    def add_instance(self, instance):\n        assert (len(instance) == 3)\n        x, prediction, uncertainty = instance\n        new_item = MemoryItem(data=x, uncertainty=uncertainty, age=0)\n        new_score = self.heuristic_score(0, uncertainty)\n        if self.remove_instance(prediction, new_score):\n            self.data[prediction].append(new_item)\n        self.add_age()\n\n    def remove_instance(self, cls, score):\n        class_list = self.data[cls]\n        class_occupied = len(class_list)\n        all_occupancy = self.get_occupancy()\n        if class_occupied < self.per_class:\n            if all_occupancy < self.capacity:\n                return True\n            else:\n                majority_classes = self.get_majority_classes()\n                return self.remove_from_classes(majority_classes, score)\n        else:\n            return self.remove_from_classes([cls], score)\n\n    def remove_from_classes(self, classes: list[int], score_base):\n        max_class = None\n        max_index = None\n        max_score = None\n        for cls in classes:\n            for idx, item in enumerate(self.data[cls]):\n                uncertainty = item.uncertainty\n                age = item.age\n                score = self.heuristic_score(age=age, uncertainty=uncertainty)\n                if max_score is None or score >= max_score:\n                    max_score = score\n                    max_index = idx\n                    max_class = cls\n\n        if max_class is not None:\n            if max_score > score_base:\n                self.data[max_class].pop(max_index)\n                return True\n            else:\n                return False\n        else:\n            return True\n\n    def get_majority_classes(self):\n        per_class_dist = self.per_class_dist()\n        max_occupied = max(per_class_dist)\n        classes = []\n        for i, occupied in enumerate(per_class_dist):\n            if occupied == max_occupied:\n                classes.append(i)\n\n        return classes\n\n    def heuristic_score(self, age, uncertainty):\n        return self.lambda_t * 1 / (1 + math.exp(-age / self.capacity)) + self.lambda_u * uncertainty / math.log(self.num_class)\n\n    def add_age(self):\n        for class_list in self.data:\n            for item in class_list:\n                item.increase_age()\n        return\n\n    def get_memory(self):\n        tmp_data = []\n        tmp_age = []\n\n        for class_list in self.data:\n            for item in class_list:\n                tmp_data.append(item.data)\n                tmp_age.append(item.age)\n\n        tmp_age = [x / self.capacity for x in tmp_age]\n\n        return tmp_data, tmp_age\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\n\n\nclass MomentumBN(nn.Module):\n    def __init__(self, bn_layer: nn.BatchNorm2d, momentum):\n        super().__init__()\n        self.num_features = bn_layer.num_features\n        self.momentum = momentum\n        if bn_layer.track_running_stats and bn_layer.running_var is not None and bn_layer.running_mean is not None:\n            self.register_buffer(\"source_mean\", deepcopy(bn_layer.running_mean))\n            self.register_buffer(\"source_var\", deepcopy(bn_layer.running_var))\n            self.source_num = bn_layer.num_batches_tracked\n        self.weight = deepcopy(bn_layer.weight)\n        self.bias = deepcopy(bn_layer.bias)\n\n        self.register_buffer(\"target_mean\", torch.zeros_like(self.source_mean))\n        self.register_buffer(\"target_var\", torch.ones_like(self.source_var))\n        self.eps = bn_layer.eps\n\n        self.current_mu = None\n        self.current_sigma = None\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass RobustBN1d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=0, unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1), var.view(1, -1)\n        else:\n            mean, var = self.source_mean.view(1, -1), self.source_var.view(1, -1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1)\n        bias = self.bias.view(1, -1)\n\n        return x * weight + bias\n\n\nclass RobustBN2d(MomentumBN):\n    def forward(self, x):\n        if self.training:\n            b_var, b_mean = torch.var_mean(x, dim=[0, 2, 3], unbiased=False, keepdim=False)  # (C,)\n            mean = (1 - self.momentum) * self.source_mean + self.momentum * b_mean\n            var = (1 - self.momentum) * self.source_var + self.momentum * b_var\n            self.source_mean, self.source_var = deepcopy(mean.detach()), deepcopy(var.detach())\n            mean, var = mean.view(1, -1, 1, 1), var.view(1, -1, 1, 1)\n        else:\n            mean, var = self.source_mean.view(1, -1, 1, 1), self.source_var.view(1, -1, 1, 1)\n\n        x = (x - mean) / torch.sqrt(var + self.eps)\n        weight = self.weight.view(1, -1, 1, 1)\n        bias = self.bias.view(1, -1, 1, 1)\n\n        return x * weight + bias\n@torch.jit.script\ndef softmax_entropy(x, x_ema):\n    return -(x_ema.softmax(1) * x.log_softmax(1)).sum(1)\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import ColorJitter, Compose, Lambda\nfrom numpy import random\nimport PIL\nimport torchvision.transforms as transforms\n\n\ndef get_tta_transforms(cfg, gaussian_std: float=0.005, soft=False):\n    img_shape = (*cfg.INPUT.SIZE, 3)\n    n_pixels = img_shape[0]\n\n    clip_min, clip_max = 0.0, 1.0\n\n    p_hflip = 0.5\n\n    tta_transforms = transforms.Compose([\n        Clip(0.0, 1.0),\n        ColorJitterPro(\n            brightness=[0.8, 1.2] if soft else [0.6, 1.4],\n            contrast=[0.85, 1.15] if soft else [0.7, 1.3],\n            saturation=[0.75, 1.25] if soft else [0.5, 1.5],\n            hue=[-0.03, 0.03] if soft else [-0.06, 0.06],\n            gamma=[0.85, 1.15] if soft else [0.7, 1.3]\n        ),\n        transforms.Pad(padding=int(n_pixels / 2), padding_mode='edge'),\n        transforms.RandomAffine(\n            degrees=[-8, 8] if soft else [-15, 15],\n            translate=(1/16, 1/16),\n            scale=(0.95, 1.05) if soft else (0.9, 1.1),\n            shear=None,\n            resample=PIL.Image.BILINEAR,\n            fillcolor=None\n        ),\n        transforms.GaussianBlur(kernel_size=5, sigma=[0.001, 0.25] if soft else [0.001, 0.5]),\n        transforms.CenterCrop(size=n_pixels),\n        transforms.RandomHorizontalFlip(p=p_hflip),\n        GaussianNoise(0, gaussian_std),\n        Clip(clip_min, clip_max)\n    ])\n    return tta_transforms\n\n\nclass GaussianNoise(torch.nn.Module):\n    def __init__(self, mean=0., std=1.):\n        super().__init__()\n        self.std = std\n        self.mean = mean\n\n    def forward(self, img):\n        noise = torch.randn(img.size()) * self.std + self.mean\n        noise = noise.to(img.device)\n        return img + noise\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Clip(torch.nn.Module):\n    def __init__(self, min_val=0., max_val=1.):\n        super().__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, img):\n        return torch.clip(img, self.min_val, self.max_val)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(min_val={0}, max_val={1})'.format(self.min_val, self.max_val)\n\n\nclass ColorJitterPro(ColorJitter):\n    \"\"\"Randomly change the brightness, contrast, saturation, and gamma correction of an image.\"\"\"\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, gamma=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.gamma = self._check_input(gamma, 'gamma')\n\n    @staticmethod\n    @torch.jit.unused\n    def get_params(brightness, contrast, saturation, hue, gamma):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        if gamma is not None:\n            gamma_factor = random.uniform(gamma[0], gamma[1])\n            transforms.append(Lambda(lambda img: F.adjust_gamma(img, gamma_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def forward(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image or Tensor): Input image.\n\n        Returns:\n            PIL Image or Tensor: Color jittered image.\n        \"\"\"\n        fn_idx = torch.randperm(5)\n        for fn_id in fn_idx:\n            if fn_id == 0 and self.brightness is not None:\n                brightness = self.brightness\n                brightness_factor = torch.tensor(1.0).uniform_(brightness[0], brightness[1]).item()\n                img = F.adjust_brightness(img, brightness_factor)\n\n            if fn_id == 1 and self.contrast is not None:\n                contrast = self.contrast\n                contrast_factor = torch.tensor(1.0).uniform_(contrast[0], contrast[1]).item()\n                img = F.adjust_contrast(img, contrast_factor)\n\n            if fn_id == 2 and self.saturation is not None:\n                saturation = self.saturation\n                saturation_factor = torch.tensor(1.0).uniform_(saturation[0], saturation[1]).item()\n                img = F.adjust_saturation(img, saturation_factor)\n\n            if fn_id == 3 and self.hue is not None:\n                hue = self.hue\n                hue_factor = torch.tensor(1.0).uniform_(hue[0], hue[1]).item()\n                img = F.adjust_hue(img, hue_factor)\n\n            if fn_id == 4 and self.gamma is not None:\n                gamma = self.gamma\n                gamma_factor = torch.tensor(1.0).uniform_(gamma[0], gamma[1]).item()\n                img = img.clamp(1e-8, 1.0)  # to fix Nan values in gradients, which happens when applying gamma\n                                            # after contrast\n                img = F.adjust_gamma(img, gamma_factor)\n\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        format_string += ', gamma={0})'.format(self.gamma)\n        return format_string",
        "experimental_info": "The RoTTA method involves three main components with the following experimental settings:\n\n1.  **Robust Batch Normalization (RBN):**\n    *   Traditional Batch Normalization layers are replaced with `RobustBN1d` or `RobustBN2d`. These layers use global running mean and variance (initialized from the pre-trained model) and update them via exponential moving average (EMA) with a momentum (`ALPHA`) of 0.05. Only the affine parameters of these RBN layers are trained during adaptation.\n\n2.  **Category-Balanced Sampling with Timeliness and Uncertainty (CSTU) for the Memory Bank:**\n    *   A memory bank (`CSTU`) is maintained with a capacity (`MEMORY_SIZE`) of 64 samples. It aims to distribute this capacity equally among categories based on the number of classes (`NUM_CLASS`) in the dataset (e.g., 10 for CIFAR-10, 100 for CIFAR-100).\n    *   Samples are prioritized for retention/removal based on a heuristic score combining their age (timeliness) and uncertainty (entropy of prediction). The weighting factors for timeliness (`LAMBDA_T`) and uncertainty (`LAMBDA_U`) are both set to 1.0.\n    *   The heuristic score is calculated as `lambda_t * 1 / (1 + math.exp(-age / capacity)) + lambda_u * uncertainty / math.log(num_class)`.\n\n3.  **Time-Aware Robust Training with a Teacher-Student Model:**\n    *   A teacher-student architecture is employed. The student model is updated using samples from the memory bank.\n    *   The teacher model's parameters are updated via EMA of the student's parameters with a decay rate (`NU`) of 0.001.\n    *   The student model minimizes a robust loss (`softmax_entropy`) which measures the difference between the student's prediction on strongly augmented memory samples and the teacher's prediction on original memory samples.\n    *   A timeliness reweighting strategy is applied to this loss, where `instance_weight = torch.exp(-ages) / (1 + torch.exp(-ages))`, giving less weight to older samples from the memory bank. `ages` are normalized by the memory capacity.\n    *   The model (student) is updated using the collected memory bank data every `UPDATE_FREQUENCY` instances, which is set to 64 (equal to `MEMORY_SIZE`).\n\n**General Optimization and Data Settings:**\n*   **Optimization:** The optimizer used is Adam with a learning rate (`LR`) of 1e-3, betas of (0.9, 0.999), and weight decay (`WD`) of 0.0. One optimization step (`STEPS`=1) is performed per batch of incoming data. During each step, an update to the model might occur if the `update_frequency` condition is met.\n*   **Data Augmentation (for memory bank samples):** Strong augmentations are applied to memory bank samples before they are passed to the student model. These transformations include clipping (0.0, 1.0), `ColorJitterPro` (brightness [0.6, 1.4], contrast [0.7, 1.3], saturation [0.5, 1.5], hue [-0.06, 0.06], gamma [0.7, 1.3]), padding (half image size), `RandomAffine` (degrees [-15, 15], translate (1/16, 1/16), scale (0.9, 1.1)), `GaussianBlur` (kernel size 5, sigma [0.001, 0.5]), `CenterCrop` to original size (e.g., 32x32), `RandomHorizontalFlip` (p=0.5), and `GaussianNoise` (std=0.005).\n*   **Input Data:** The method is evaluated on datasets like CIFAR-10/CIFAR-100 under various corruption types and severities. Input images are resized to (32, 32), normalized using standard ImageNet mean ([0.485, 0.456, 0.406]) and std ([0.229, 0.224, 0.225]). The testing batch size (`TEST.BATCH_SIZE`) is 64."
      }
    },
    {
      "title": "Bayesian Meta Sampling for Fast Uncertainty Adaptation"
    },
    {
      "title": "Test Time Adaptation via Conjugate Pseudo-labels",
      "abstract": "Test-time adaptation (TTA) refers to adapting neural networks to distribution\nshifts, with access to only the unlabeled test samples from the new domain at\ntest-time. Prior TTA methods optimize over unsupervised objectives such as the\nentropy of model predictions in TENT [Wang et al., 2021], but it is unclear\nwhat exactly makes a good TTA loss. In this paper, we start by presenting a\nsurprising phenomenon: if we attempt to meta-learn the best possible TTA loss\nover a wide class of functions, then we recover a function that is remarkably\nsimilar to (a temperature-scaled version of) the softmax-entropy employed by\nTENT. This only holds, however, if the classifier we are adapting is trained\nvia cross-entropy; if trained via squared loss, a different best TTA loss\nemerges. To explain this phenomenon, we analyze TTA through the lens of the\ntraining losses's convex conjugate. We show that under natural conditions, this\n(unsupervised) conjugate function can be viewed as a good local approximation\nto the original supervised loss and indeed, it recovers the best losses found\nby meta-learning. This leads to a generic recipe that can be used to find a\ngood TTA loss for any given supervised training loss function of a general\nclass. Empirically, our approach consistently dominates other baselines over a\nwide range of benchmarks. Our approach is particularly of interest when applied\nto classifiers trained with novel loss functions, e.g., the recently-proposed\nPolyLoss, where it differs substantially from (and outperforms) an\nentropy-based loss. Further, we show that our approach can also be interpreted\nas a kind of self-training using a very specific soft label, which we refer to\nas the conjugate pseudolabel. Overall, our method provides a broad framework\nfor better understanding and improving test-time adaptation. Code is available\nat https://github.com/locuslab/tta_conjugate.",
      "full_text": "Test-Time Adaptation via Conjugate Pseudo-labels Sachin Goyal⋆1 Mingjie Sun⋆1 Aditi Raghunathan1 Zico Kolter1,2 1Carnegie Mellon University, 2Bosch Center for AI {sachingo, mingjies, raditi, zkolter}@cs.cmu.edu Abstract Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [50], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the “best” possible TTA loss over a wide class of functions, then we recover a function that isremarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classiﬁer we are adapting is trained via cross-entropy loss; if the classiﬁer is trained via squared loss, a different “best” TTA loss emerges. To explain this phenomenon, we analyze test-time adaptation through the lens of the training losses’sconvex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the “best” losses found by meta-learning. This leads to a generic recipe that can be used to ﬁnd a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other TTA alternatives over a wide range of domain adaptation benchmarks. Our approach is particularly of interest when applied to classiﬁers trained with novel loss functions, e.g., the recently-proposed PolyLoss [25] function, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our conjugate based approach can also be interpreted as a kind of self-training using a very speciﬁc soft label, which we refer to as the conjugate pseudo-label. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/ tta_conjugate. 1 Introduction Modern deep networks perform exceeding well on new test inputs that are close to the training distribution. However, this performance dramatically decreases on test inputs drawn from a different distribution. While there is a large body of work on improving the robustness of models, most robust training methods are highly specialized to the setting they cater to. For e.g., they assume pre-speciﬁed perturbations, subpopulations, and spurious correlations, or access to unlabeled data from the target distribution, and most methods offer close to no improvement on general distribution shifts beyond what they were trained for [12, 21]. In practice, it is often cumbersome (or even impossible) to precisely characterize all possible distri- bution shifts a model could encounter and then train accordingly. Instead, a model already trained on some source data must be able to adapt at test-time to new inputs from a different domain. This setting of test-time adaptation (TTA) has gained interest in recent years [ 6, 47, 50, 54]. TTA is typically accomplished by updating the source model parameters via a few steps of optimization on an unsupervised objective involving the new test sample from the target distribution. The choice ⋆ Equal Contribution 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2207.09640v2  [cs.LG]  23 Nov 2022of this unsupervised objective, which we call the TTA loss, dictates the success of the adaptation procedure. [47] uses a self-supervised objective on the test sample, [50] uses the entropy of model predictions, and several follow-ups have proposed variants or alternatives [ 40, 54]. However, it remains unclear as to how to choose or guide the selection of this TTA loss, and thus far the choice of these losses has remained largely heuristic in nature. In this work, we begin by presenting a set of intriguing experiments where we attempt to learn the “best” TTA loss for a given source classiﬁer and distribution shift. We parameterize the TTA loss by another neural network whose parameters are learnt via meta-learning [ 3, 9] where we differentiate through the adaptation process to ﬁnd the TTA loss that achieves the best adaptation on distribution shifts. Surprisingly, we ultimately learn a TTA loss that looksremarkably similar to (a temperature-scaled version of) the softmax-entropy loss, which was already proposed by [50]. Why did we recover the commonly used softmax-entropy loss despite the fact that the procedure is capable of learning a very general class of losses and the meta-learning process could potentially specialize to both the source classiﬁer and the distribution shift of interest? Furthermore, we ﬁnd that this pattern only holds when the loss used to train the source classiﬁer is cross-entropy loss; when a different loss such as squared loss is used instead, the meta-learning procedure recovers a TTA loss that itself looks more like a negative squared error, and is very different from the softmax-entropy loss (Section 3). In order to explain this phenomenon, we propose to consider TTA through the lens of the convex conjugate function. Speciﬁcally, given a hypothesis function h(x) and label y, several common losses (cross-entropy and the squared loss amongst them, but not limited to these) can be written in the form L(h(x),y) = f(h(x)) −yTh(x) for some function f. In these cases, we show that “natural” TTA loss for such classiﬁers is precisely the (negation of) the convex conjugate evaluated at the gradient of h, LTTA(x) = −f∗(∇f(h(x)), where f∗is the convex conjugate of f. This framework not only recovers the results of our meta-learning experiments, but also justiﬁes why some speciﬁc choices of TTA loss in the previous literature work well (e.g., this framework recovers TENT’s choice of softmax-entropy for cross-entropy-trained classiﬁer). Moreover, it also provides a broad framework for what the TTA loss should be when the source model is trained using various different loss functions (for example the recently-proposed PolyLoss [25, 29]) as is becoming increasingly common in machine learning. Further, we show that our proposed conjugate adaptation loss is in fact a kind of self-training with pseudo-labels [42], a classic approach in machine learning. Various formulations of the pseudo-label have been proposed in the literature, and our conjugate analysis provides a general recipe for the “correct” choice of soft pseudo-labels given byˆy(x) = ∇f(h(x)). We thus refer to these as conjugate pseudo-labels (Conjugate PL’s), and believe our work provides a broad framework for understanding adaptation with unlabeled data in general. Finally, we empirically verify the effectiveness of our proposed conjugate adaptation loss across several datasets and training losses, such as cross-entropy and squared loss, along with the recently- proposed PolyLoss [ 25] (which itself has shown higher standard test accuracy on a wide range of vision tasks). Over all models, datasets and training losses, we ﬁnd our proposed conjugate pseudo-labeling consistently outperforms prior TTA losses and improves TTA performance over the current state of the art. 2 Background and preliminaries. Test-time adaptation. We are interested in mapping an input x∈Rd to a label y∈Y. We learn a model hθ : Rd ↦→R|Y|parameterized by θthat maps an input xto predictions hθ(x). We assume access to a trained source model and adapt at test-time over the test input, before making the ﬁnal prediction. This is the standard test-time adaptation (TTA) setting [47, 50]. During TTA, we update the model parameters on an unsupervised objective L(x,hθ). For example, in TENT [50], this loss is the entropy of the softmax-normalized predictions of the model. At each time step of adaptation, we observe a batch of test inputs and we take a gradient step towards optimizing the TTA loss on this test batch. As is standard, we measure the average online performance of models across all steps (number of test batch inputs seen) in the adaptation process. Meta learning the loss function. In order to explore the existence of different TTA losses, we employ the meta-learning procedure where we attempt to learn the TTA loss. We use a similar procedure as prior work on meta-learning loss functions [3, 37] and parameterize the loss function via a neural network mφ : R|Y| ↦→R that takes in the model predictions/logits and outputs a loss value. We want to learn parameter φsuch that when we update θvia the loss function mφ, our ﬁnal 2performance is optimal. In order to do so, let xbe the unlabeled test samples to adapt to, and ybe the corresponding labels. We update θand φalternatively as follows. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt , φt+1 ←φt −β∂L(hθt+1 (x′),y′) ∂φt , (1) where Lis some supervised surrogate loss function such as cross-entropy. Please refer to Appendix A3 for further details regarding meta-learning setup. Note that the meta-learning process above assumes access to labels yof test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We discuss our ﬁndings from this exploration in the next section. 3 Test-time Adaptation via Meta-Learnt Losses The objective used in TENT is the softmax-entropy of the model predictions which essentially makes the classiﬁer more conﬁdent in its current predictions. The same can be achieved by various other loss formulations such as those mentioned in [40]. With so many possible choices for the loss function, what should we use for TTA? In this section, we attempt to answer this empirically and present some intriguing observations. (a)  (b) Figure 1: Visualization of meta loss (blue) by varying one input prediction score. (a) For cross-entropy loss trained model, the learnt meta loss can be approximated with a scaled softmax-entropy function (dashed red). (b) When the source model is trained with a squared loss for classiﬁcation, the learnt meta loss (blue) can be ﬁtted closely with a quadratic function (dashed red), shown in Figure 1b. The range (max/min) of the prediction score (logit) in x-axis is chosen to cover the empirical range of the predicted logits. Experiment 1. We learn the TTA loss parameterized by a neural network via meta-learning as described in Section 2. Our source classiﬁer is a ResNet-26 trained on CIFAR-10 and we adapt to distribution shifts in CIFAR-10-C. We use the 4 labeled validation noises in CIFAR-10-C to learn the meta-loss network parameters and we denote the resulting learnt loss function by meta-TTA loss. We then adapt the source classiﬁer to the test set of 15 corruptions by optimizing the meta-TTA loss. Observations. First, we ﬁnd that TTA using meta-TTA loss performs better than TENT (12.35% vs 13.14%), suggesting that there are better TTA losses than previous losses based on softmax-entropy. However, on examining this meta-TTA loss, we ﬁnd a surprising observation. Figure 1a (blue curve) visualizes the learnt meta-loss over model predictions as we vary a single class prediction with the rest ﬁxed. Qualitatively, the learnt meta-loss looks very similar to softmax-entropy in one dimension. In fact, we can ﬁt it closely with a scaled softmax-entropy function (dashed red curve): α·H(softmax(hθ(x)/T)), where αis a magnitude parameter and T is a temperature scaler. We want to test if the meta-loss is basically learning the softmax-entropy function. Hence, we perform test-time adaptation with the ﬁtted softmax-entropy function instead (dashed red curve) and achieve an error of 12.32%, essentially recovering the performance of meta-TTA. 3Despite the ability to represent many different loss functions and potentially specialize to the CIFAR- 10-C setting, the meta-loss procedure gave back the standard entropy objective.Do we always recover a loss that looks like softmax-entropy? Experiment 2. In an attempt to isolate when we get back the entropy objective, we vary several things. We tried different architectures for the source classiﬁer, different lossesLduring the meta- learning process (1) and different training losses for the source classiﬁer. Results. We observed that we consistently recovered the temperature scaled softmax-entropy function in all cases except when we varied the training loss for the source classiﬁer (Appendix A.10). On using the squared loss function [18], a strikingly different meta-TTA loss emerges. Figure 1b (blue curve) shows the learnt meta-loss (13.48% error) for this network. Here again, the meta-TTA loss outperforms entropy (14.57%) but it is not simply due to a scaling factor. The loss now looks like the negative squared error (red curve). Like previously, we tried ﬁtting a quadratic loss directly to the meta loss in Figure 1b, and this time we even slightly outperformed the meta-TTA loss. To summarize, we used a meta-learning procedure to search for the “best” TTA loss, where the loss itself was parameterized by a neural network that could potentially represent arbitrarily complex loss functions. However, we ended up with loss functions displaying remarkable structure: across different architectures and different variants of meta-learning, for a classiﬁer trained with cross-entropy, the meta-TTA loss was temperature scaled softmax-entropy and for a classiﬁer trained with squared loss, the meta-TTA loss was a negative squared loss. This is interesting from both a practical and conceptual standpoint where the “best” TTA loss depends on the loss used to train the source classiﬁer in a clean fashion. We attempt to understand and explain this phenomenon in the next section. 4 Conjugate Pseudo Labels Results in the previous section raise an obvious question: why does softmax-entropy as used in TENT seem to be the “best” possible test time adaptation loss for classiﬁers trained via cross-entropy (at least, best in the sense that meta-learning consistently recovers something which essentially mimics softmax-entropy, even though meta-loss is parameterized by a neural network and hence could learn much more complex functions speciﬁc to the model and the particular shift)? And why, alternatively, does a quadratic TTA loss seem to perform best when the classiﬁer is trained via squared loss? In this section, we offer an explanation of this phenomenon via the construct of the convex conjugate function [1]. As we will see, our method recovers softmax-entropy and quadratic loss as the “natural” objectives for classiﬁers trained via cross-entropy and squared loss respectively. Furthermore, for classiﬁers trained via other loss functions, as is becoming increasingly common in deep learning, our approach naturally suggests corresponding test-time adaptation losses, which we show in the next section to comparatively outperform alternatives. Thus, we argue that our framework overall provides a compelling recipe for specifying the “correct” method for TTA for a large class of possible losses. 4.1 Losses and the convex conjugate We begin by formally considering loss functions between a hypothesis outputhθ(x) (e.g., the logit outputs of a classiﬁer, or the direct prediction of a regressor) and targetythat take the following form L(hθ(x),y) = f(hθ(x)) −yThθ(x) (2) for some function f; when there is no risk of confusion, we will use hin place of hθ(x) for simplicity of notation. While not every loss can be expressed in such a form, this captures a wide variety of common losses (possibly scaled by a constant value). For example, cross-entropy loss corresponds to the choice f(h) = log ∑ iexp(hi) and where y denotes a one-hot encoding of the class label; similarly, squared loss corresponds to the choice f(h) = 1 2 ∥h∥2 2. When training an over-parameterized classiﬁer, we can roughly view the training process as (approxi- mately) attaining the minimum over hypotheses hfor each training example min θ 1 t t∑ i=1 L(hθ(xi),yi) ≈1 t t∑ i=1 min h L(h,yi) (3) 4where t is the number of training samples. However, in the case of losses in the form (2), the minimization over hin this form represents a very speciﬁc and well-known optimization problem: it is known as the convex conjugate [1] of the function f min h L(h,y) = min h {f(h) −yTh}= −f⋆(y) (4) where f⋆ denotes the convex conjugate of f. f⋆ is a convex function in y(and indeed, is convex regardless of whether or not f is convex). Furthermore, for the case that f is convex differentiable, the optimality condition of this minimization problem is given by ∇f(hopt) = y, so we also have that f⋆(y) = f⋆(∇f(hopt)) (5) where hopt refers to the optimal classiﬁer (used interchangeably with hθopt ). Putting this all together, we can state (admittedly, in a rather informal manner) that under the assumption that θopt is chosen so as to approximately minimize the empirical loss on the source data in the over-parameterized setting, we have that for tinputs 1 t t∑ i=1 L(hθopt (xi),yi) ≈1 t t∑ i=1 −f⋆(∇f(hθopt (xi))) (6) i.e., the empirical loss can be approximated by the (negative) conjugate applied to the gradient of the f, at least in a region close to the optimal θopt that minimizes the empirical loss. But the later expression has the notable beneﬁt that it does not require any label yi in order to compute the loss, and thus can be used as a basis for TTA on target domain of the hypothesis function hθopt . Deﬁnition 1 (conjugate adaptation loss) Consider a loss function that takes the form given in 2, used for training a hypothesis hθ in the over-parameterized regime. We deﬁne the conjugate adaptation loss Lconj(hθ(x)) : R|Y|↦→R as follows. Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))⊤hθ(x). (7) 4.2 Recovery of existing test-time adaptation strategies Cross-entropy The interesting aspect to this formalism is that when applied to classiﬁers trained with cross-entropy, it recovers exactly the TENT approach to TTA : minimizing the softmax-entropy of hθ(x). And indeed, this loss was also recovered when using meta-learning to learn the “optimal” test-time adaptation loss. To see this, note that for cross-entropy, we have thatf(h) = log ∑ iexp(hi), giving the optimality condition y= ∇f(hopt) = exp(hopt)∑ iexp(hopt i ) and the conjugate function f⋆(y) = { ∑ iyilog yi if ∑ iyi = 1 ∞ otherwise . (8) In other words, Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (9) i.e. softmax-entropy of the model prediction, which is exactly the TTA loss that TENT uses. Squared loss For the squared loss, we have thatf(h) = 1 2 ∥h∥2 2, leading to the optimality condition y = hand conjugate function f⋆(y) = 1 2 ∥y∥2 2. Hence, the adaptation loss in this case would be simply given by Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = −1 2 ∥h∥2 2 which is also what we observed in the meta-learning experiments discussed in Section 3. 4.3 Conjugate pseudo-labels We now emphasize that by the nature of our approximations, there is an additional simple interpre- tation of the conjugate loss: it is also equal to the original loss (2) applied to the “psuedo-labels” ˜yCPL θ (x) = ∇f(hθ(x)), where CPL refers to conjugate pseudo-labels, i.e., Lconj(hθ(x)) = −f⋆(∇f(hθ(x))) = f(hθ(x)) −∇f(hθ(x))Thθ(x) = L(hθ(x),∇f(hθ(x))). (10) 5This property is known as the Fenchel-Young inequality, that isf(x) + f⋆(u) ≥xTuholding with equality when u = ∇f(x). In other words, our conjugate adaptation loss is precisely equivalent to self-training under the speciﬁc soft pseudo-labels given by ˜yCPL = ∇f(hθ(x)). And indeed, for many cases, this may be a more convenient form to compute than explicitly computing the conjugate function at all. For this reason, we refer to our method as that of conjugate pseudo-labels. In the case of cross-entropy loss, this approach then corresponds exactly to self-training using labels given by the softmax applied to the current hypothesis. We must emphasize, however, that while our conjugate formulation indeed has this “simple” form for the case of cross-entropy loss, the real advantage comes in that it provides the “correct”pseudo-label for use with other losses, which may result in pseudo-labels different from the “common” softmax operation. Example: conjugate pseudo-labels for PolyLoss. PolyLoss [25] is a recently-proposed simple alternative to cross-entropy loss than has been shown to improve performance across a wide variety of compute tasks. This loss is given by the form Lpoly(hθ(x),y) = Lce(hθ(x),y) + ϵ·yT(1 −softmax(hθ(x))) (11) We note that this can be put exactly into our conjugate form (equation 2) by writing the loss in a slightly more involved fashion, which we refer to as the expanded conjugate form Lpoly(hθ(x),y) = f(hθ(x)) −yTg(hθ(x)). (12) where f is the log-sum-exp function as before, and g(h) = h−ϵ(1 −softmax(h)). In order to formally put this into the form of the previous loss function (equation 2), we can simply deﬁne an alternative hypothesis as the function h′ θ(x) = g(hθ(x)), and then deﬁne PolyLoss in the conjugate form as Lpoly(h′ θ(x),y) = f(g−1(h′ θ(x))) −yTh′ θ(x). (13) Typically, however, it is easier to simply operate on the expanded conjugate form, which yields the optimality condition for the pseudo-label ∇f(hopt) = Dg(hopt)˜yCPL θ (x), where D is the Jacobian operator. For the case of PolyLoss, this leads to the conjugate pseudo-label of the following form: ˜yCPL θ (x) = (I+ ϵdiag(z) −ϵzzT)−1z, z ≡softmax(hθ(x)). Test-time adaptation. Finally, we note that the above discussion doesn’t actually address any topics related to test-time adaptation to OOD data, but merely provides a generic characterization of a self- training procedure for generic loss functions of the form(2). However, the application toTTA on OOD data is fairly straightforward: as long as the learnt source parameters θis a reasonable approximation to the true optimal θopt on the shifted domain, self-training with the conjugate pseudo-labels provides a reasonable proxy for ﬁne-tuning the network on the true OOD loss. We emphasize that, common to most approaches for TTA , there are still some amount of design decisions that must be put in place; these are detailed in Section 5.1. In practice, we observe OOD generalization typically beneﬁts (across all baselines) from an additional “temperature” scaling, i.e., applying the TTA loss to hθ(x)/T for some ﬁxed temperature T, although it requires a held-out validation dataset for tuningT. However, we should emphasize that truly unsupervisedTTA would require making an informed guess for the value of these hyper-parameters. The full procedure for test time adaptation via conjugate pseudo-labels is shown in Algorithm 1. Algorithm 1 Conjugate pseudo-labeling (Conjugate PL) Input: Source classiﬁer θ0 trained using loss L(hθ(x),y) = f(hθ(x)) −hθ(x)⊤y. N batches of test data Dtest = [x1,x2,...,x N] Hyperparams: learning rate ηand temperature T. Let ¯hθ(x) def = hθ(x)/T be the temperature scaled predictor. Let ˜yCPL θ (x) denote the conjugate pseudo-label function ˜yCPL θ (x) = ∇(f(¯hθ(x))). for n= 0,1,...N −1 do θn+1 = θn −η∇L ( ¯hθ(xn),˜yCPL θ (xn) ) [Self-training with conjugate pseudo-labels] 65 Experiments In this section, we empirically evaluate the effectiveness and generality of the proposed conjugate pseudo-labeling procedure (Algorithm 1) for test-time adaptation on a variety of datasets. 5.1 Setup Datasets. We evaluate on the three common corruption benchmarks: adapting a classiﬁer trained on CIFAR-10 to CIFAR-10-C, CIFAR-100 to CIFAR-100-C and ImageNet to ImageNet-C [ 15]. Following the previous works [47, 50], we report the error averaged across corruptions at the highest severity for CIFAR-10/100-C and averaged across corruptions and severity level for ImageNet-C. We also evaluate on three domain adaptation datasets: adapting a classiﬁer trained on SVHN to MNIST, an ImageNet classiﬁer to ImageNet-R [16] and adapting from synthetic to real data in VISDA-C [38]. Models and Training losses. Following previous works on TTA[47, 50], we use ResNet-26 [14] as the source classiﬁer architecture for CIFAR-10/100 experiments, ResNet-18 for SVHN to MNIST and a ResNet-50 for ImageNet and source synthetic data on VisDA-C. We consider source classiﬁers trained via the following loss functions: the de-facto cross-entropy, recently proposed polyloss [25] and squared loss [18]. Baselines. Our proposed conjugate pseudo-label is the classic approach of self-training with a speciﬁc form of pseudo-labels. In self-training, we replace the label ywith a pseudo-label ˜y(x) and adapt by optimizing the loss function L(hθ(x),˜y(x)). Note that we could either instantaneously update the pseudo-labels using the current classiﬁer, or generate pseudo-labels once with just the source classiﬁer. Instantaneous updates have been shown to work better for domain adaptation [7, 40], and we perform instantaneous updates for all methods. While we propose using ˜yCPL(x) = ∇f(hθ(x)) (See Section 4.3), we compare to the standard pseudo-labels used in the literature: • (i) the “hard” pseudo-label (hard PL) where ˜y(x) = arg maxi ( hθ(x) ) i is the most likely class as predicted by hθ. As is common in the self-training literature, we perform conﬁdence thresholding. • (ii) The “soft” pseudo-label (soft PL) where ˜y(x) is obtained by applying a softmax function to the model predictions hθ(x). We also compare with the following recently proposed test-time adaptation methods. • Entropy Minimization (ENT) [50] minimizes the entropy of model predictions. • Robust Pseudo-Label [40] where we minimize a robust classiﬁcation loss, Lrpl = q−1(1 −p(i|x)q) where i= argmaxjp(j|x) and q∈[0,1]. • MEMO [54] minimizes entropy of a model’s outputs across different augmentations of a test input. We implement a batch version, where we see multiple test points at once, for fair comparisons. TTA methodology. Following [ 50] and [40], we ﬁne-tune by updating the learnable scale and shift parameters of the batch normalization layers across all adaptation losses. For each batch, batch normalization statistics is also updated, as suggested in [41]. We report performance at the end of one round of test-time adaptation over the entire test set. We tune the learning rate (LR) and temperature (T) on the validation noises in the corruption benchmark by grid-search. LR is selected from {1e−1,1e−2,... 1e−4}and T from {1,2 ... 5}. All the experiments have been performed on A6000 GPU’s. On domain adaptation benchmarks, where there is no held-out target domain, we set T to be 1 and use the LR suggested by [ 6, 50]. We use the same hyperparameter tuning protocol across all methods. We single out temperature as a very important hyperparameter, as we discuss in the results below. 5.2 Results on classiﬁers trained with cross-entropy We study the effectiveness of our proposed conjugate pseudo-labels when the source classiﬁer is trained via cross-entropy loss. In this case, baselines Softmax PL and ENT are the same as Conjugate PL. Thus we omit them in our results. Table 1, reports the performance of various TTA methods. When the source classiﬁer is trained via cross-entropy, our conjugate pseudo-label algorithm exactly corresponds to entropy minimization with an additional temperature scaling. Entropy minimization as 7Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 13.95 (±0.06) 13.97 ( ±0.04) 12.60(±0.04) 13.07 (±0.05) \u0013 13.95 (±0.06) 12.85 ( ±0.04) 12.51(±0.01) 12.51(±0.03) CIFAR-100-C \u0017 45.22 (±0.4) 39.80 ( ±0.18) 38.52(±0.16) 41.15 (±0.25) \u0013 45.22 (±0.4) 36.37 ( ±0.10) 37.38 ( ±0.06) 36.10(±0.07) ImageNet-C \u0017 45.43(±0.05) 45.68 ( ±0.01) 48.91( ±0.03) 45.82(±0.01) \u0013 45.43 (±0.05) 45.61 ( ±0.01) 48.91( ±0.04) 45.36(±0.01) Table 1: Mean errors when adapting to corruptions using a source classiﬁer trained via cross- entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. With the right temperature scaling, softmax-entropy minimization matches or outperforms other approaches. Prior reported gains of other methods over softmax-entropy minimization disappear when we use temperature scaling. For additional context, the source classiﬁer errors without adaptation are: CIFAR-10-C (29.54%), CIFAR-100-C (62.26%), ImageNet-C (61.89%) proposed in prior work [50] does not tune the temperature parameter, and some newer objectives such as robust PL or MEMO outperform vanilla entropy minimization. For example, on CIFAR-100-C, vanilla ENT obtaines 41.15% average error, while robust PL improves this to39.80% and MEMO to 38.52%. However, with the right temperature scaling, entropy minimization obtains 36.10% error which outperforms the newer objectives (with and without temperature scaling). A similar observation holds for CIFAR-10-C and ImageNet-C as well. Essentially, the gains over vanilla entropy minimization vanish when we do temperature scaling, and entropy minimization (i.e. conjugate pseudo-labeling corresponding to cross-entropy) turns out to be the best objective after all. 5.3 Results on classiﬁers trained with polyloss and squared loss In the case of cross-entropy, conjugate pseudo-labeling reduces to the familiar notion of entropy minimization. We now explore the performance of our method on different loss functions where the conjugate pseudo-labels differ substantially from entropy minimization (section 4.3). Table 2 presents the results on the corruption benchmarks and Table 3 presents the results on the other domain adaptation datasets for source classiﬁers trained with PolyLoss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C \u0017 13.81(±0.12) 14.23(±0.02) 13.46(±0.06) 13.23(±0.07) 14.64(±0.11) 13.02(±0.09) \u0013 13.81(±0.12) 12.45(±0.05) 12.23(±0.06) 12.33(±0.04) 12.26(±0.04) 12.08(±0.05) CIFAR-100-C\u0017 40.47(±0.05) 42.86(±0.11) 40.12(±0.08) 39.90(±0.05) 41.00(±0.11) 38.17(±0.17) \u0013 40.47(±0.05) 39.80(±0.08) 38.23(±0.05) 39.23(±0.04) 37.04(±0.06) 36.83(±0.08) ImageNet-C \u0017 45.44(±0.21) 46.27(±0.03) 46.10(±0.03) 48.21(±0.05) 44.63(±0.03) 44.01(±0.01) \u0013 45.44(±0.21) 46.27(±0.03) 45.50(±0.02) 48.21(±0.04) 44.45(±0.03) 44.01(±0.01) Table 2: Mean errors when adapting to corruptions using a source classiﬁer trained via recently proposed Poly-1 Loss [ 25]. Conjugate pseudo-labeling consistently outperforms all previous ap- proaches. For additional context, source classiﬁer errors without adaptation : CIFAR-10-C (30.22%), CIFAR-100-C (63.91%) and ImageNet-C (62.18%). First, we note that, across all datasets in Table 2 and Table 3, our conjugate PL approach outperforms all other TTA losses. With polyloss classiﬁers, entropy minimization is no longer the best method—on CIFAR-100-C, entropy minimization achieves38.23% error while our conjugate PL achieves36.83%. We see similar consistent gains on CIFAR-10-C, ImageNet-C, ImageNet-R and VisDA-C. On digit adaptation tasks from SVHN to MNIST/USPS/MNISTM, where there is a larger shift between source and target, the gains are especially pronounced. Figure 2 compares how the task loss (polyloss ϵ= 6) on the test data decreases as we adapt the model through conjugate PL and other baselines. We use CIFAR-10-C as an example. Observe that our proposed conjugate PL indeed reduces the task loss the most among other baselines. 8Dataset Source Error Hard PL Robust PL EntropySoftmax PL Conjugate PL Ours SVHN→MNIST 28.33 20.21 19.73 14.28 16.54 10.73 SVHN→USPS 31.58 23.32 26.12 23.12 24.07 21.62 SVHN→MNISTM61.69 50.73 51.35 49.33 50.47 47.59 ImageNet-R 64.19 58.52 59.46 58.25 56.62 55.63 VisDA-C 58.13 40.43 45.44 44.11 39.63 38.42 Table 3: Target error when adapting models trained via polyloss on source domains across different domain adaptation bench- marks. Conjugate pseudo-labeling offers consistent and substan- tial gains over previous approaches across three datasets. Figure 2: Task Loss (PolyLoss ϵ= 6) evaluated on CIFAR-10-C test data during test-time adaptation. Furthermore, on CIFAR-10-C and ImageNet-C, we ﬁnd that adapting polyloss classiﬁers via conjugate PL improves the performance over all methods applied to cross-entropy trained source classiﬁers. For e.g., on ImageNet-C, the performance improves from 45.34% to 44.01%. However, this is only true when using the proposed conjugate PL. If we just did softmax-entropy minimization (even with temperature scaling), the ﬁnal adapted performance of a polyloss classiﬁer (45.5%) is in fact worse than that of a cross-entropy classiﬁer (45.34%). Our results suggest that as we develop new training losses that improve the source classiﬁers, it is important to adapt via conjugate pseudo-labeling to reap the maximum gains. Similarly, we experiment with the case when the source classiﬁer is trained using squared loss on the CIFAR-10 and CIFAR-100 datasets, and observe consistent gains using the proposed conjugate pseudo-labels over the baselines. For example, on CIFAR-10-C, TTA using conjugate PL gives and error of 12.87%, outperforming baselines like ENT (13.24%) and Softmax PL (31.81%). Table 5 in Appendix A.7 shows the detailed results. Comparing Table 1 and Table 2, we see that the relative ordering between the various baselines differs. This is further evidence that the adaptation loss has to depend on the training loss, and we believe our conjugate pseudo-label approach captures this appropriately by offering consistent gains across the various settings we experimented with. 6 Related Works Test-time adaptation methods. In recent years, the setting of test-time adaptation has gained a lot of interest with a host of different approaches proposed in the literature. One family of TTA approaches update the source classiﬁer by minimizing an unsupervised loss on the target distribution [4, 6, 20, 22, 35, 36, 40, 43, 44, 50, 51, 54]. TENT [ 50] proposes to minimize the entropy of model predictions at test time. Several follow ups like [ 6, 35, 40, 44, 54] propose alternative TTA objectives, e.g. robust pseudo-labelling [40], likelihood ratio loss [35], entropy of marginal probability averaged across augmentations [54] and self-supervised contrastive losses [6, 49]. However, most of these objectives are heuristically designed or chosen. In this paper, we provide a principled approach of designing unsupervised objectives for TTA . Another family of approaches for test-time adaptation such as [ 2, 8, 13, 31, 34, 47] leverage an auxiliary self-supervised task (e.g. rotation prediction [ 47], masked autoencoders [10]) to update model parameters on each test sample. Crucially, these methods require modifying the source model training by augmenting the supervised training objective with an auxiliary self-supervised loss. Hence it cannot be applied to typical standard classiﬁers that are trained by minimizing a supervised loss on the source data. Source-free domain adaptation. A very related setting to test-time adaptation is source-free domain adaptation, where a trained source classiﬁer must be adapted to a target distribution of interest, although the entire target unlabeled data is available at once. SHOT [28] proposes to optimize the source hypothesis (i.e. feature extractor) with a combination of entropy minimization, diversity and self-training on pseudo-labels on the unlabeled target data. [53] promotes feature clustering on features from target distributions. [24, 26] use generative modeling to estimate the underlying source distributions for enforcing feature invariance. Such approaches typically require multiple epochs over the target data and cannot be easily adopted to work in an online fashion. 9Unsupervised domain adaptation. The most canonical setting of domain adaptation involves access to labeled source data and unlabeled target data, all during training. The availability of source and target data during training lends itself to approaches that “align” the source and target representations in some way: [ 32, 33, 45, 48] match distribution statistics, [ 11] uses a discriminator, [ 46] uses self-supervised learning. However, such approaches require access to source data which might not always be feasible due to data privacy and efﬁciency issues. Pseudo-labels and self-training. Self-training is a classic idea for leveraging unlabeled data, devel- oped ﬁrst for the semi-supervised setting. Self-training generates pseudo-labels on the unlabeled data, allowing us to use any “supervised” loss on this pseudo-labeled data. Self-training has shown promising results in various settings like semi-supervised learning [ 19] and improving adversarial robustness [ 5]. Self-training has also been gaining attention in the setting of unsupervised domain adaptation [28, 39], where pseudo-labels generated on the unlabeled data from target domain is used to supervise the adaptation process. [ 7, 23, 52] provide theoretical insights into how self-training with pseudo-labels can help under distribution shift. TENT [50] (i.e entropy minimization) can be viewed as a form of self-training with instantaneous softmax pseudo-labels. Our work provides a general framework for the choice of soft pseudo-labels based on the conjugate analysis of the source training objective. Some prior works like [7, 17, 27, 30, 55, 56] have documented the improvement in performance when using instantaneous pseudo-labels over pre-computed pseudo-labels, and thus lend further support to the beneﬁts of our proposed conjugate pseudo-labeling approach. The ex- periment results presented in this work supporting conjugate pseudo-labels suggest that conjugate pseudo-labels is a promising direction of pseudo-labeling in a broader context. 7 Conclusion, Limitations and Future Directions In this work, we proposed a general test-time adaptation loss, based on the convex conjugate formulation which in turn was motivated by the intriguing meta learning experiments. The fact that meta-learning recovers the proposed loss hints at some kind of optimality of the loss. In Section 4, we prove that for a broad set of loss functions, the proposed (unsupervised) conjugate loss is close to the oracle supervised loss. However, this still does not completely answer what the optimal test-time adaptation loss is and why. The meta-learning framework in this work was constrained to learn functions over the logits of each individual input. It can be expanded to more involved setups, where we consider functions over the intermediate representations too and also consider learning functions over a batch of input while accounting for their interactions. Beyond the choice of the adaptation loss itself, achieving good test-time adaptation generally involves several heuristics like updating only the batch norm parameters [50]. While our work was motivated by the loss function, via the meta-learning experiments, we discovered that temperature scaling is another important hyper-parameter that improves the performance of all previous baselines as well. At a high level, test-time adaptation has to be appropriately regularized to prevent the updates over batches from taking the model too far: updating only a few batch norm parameters is one way to do that, and perhaps temperature scaling provides a similar beneﬁcial regularization effect by making the network predictions on unlabeled inputs less conﬁdent. Understanding the role of these heuristics more concretely is an interesting direction for future work. It also remains an open problem to understand under what sort of real-world distribution shifts would self-training based approaches would help. Finally, it is also worth extending and applying the conjugate pseudo-labeling to other settings like semi-supervised learning. 8 Acknowledgments We thank Shubhang Bhatnagar and Asher Trockman for helping with running the ImageNet experi- ments. We thank Zhili Feng for useful feedback. Sachin Goyal and Mingjie Sun were supported by funding from the Bosch Center for Artiﬁcial Intelligence. Aditi Raghunathan was supported by an Open Philanthropy AI Fellowship. 10References [1] https://en.wikipedia.org/wiki/Convex_conjugate. [2] Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. Self-supervised test-time learning for reading comprehension. In Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2021. [3] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Edward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, and Franziska Meier. Meta-learning via learned loss. arXiv preprint arXiv:1906.05374, 2019. [4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [5] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un- labeled data improves adversarial robustness. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf. [6] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious features under domain shift. In Advances in Neural Information Processing Systems, 2020. [8] Mohammad Zalbagi Darestani, Jiayu Liu, and Reinhard Heckel. Test-time training can close the natural distribution shift performance gap in deep learning based compressed sensing. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. [10] Yossi Gandelsaman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems, 2022. [11] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. [12] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. InInternational Conference on Learning Representations, 2021. [13] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenya, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on Learning Representations, 2021. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. [16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [17] Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Advancing momentum pseudo-labeling with conformer and initialization strategy. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. 11[18] Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. [19] Dong hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. [20] Yusuke Iwasawa and Yutaka Matsuo. Test-time classiﬁer adjustment module for model-agnostic domain generalization. In Advances in Neural Information Processing Systems, 2021. [21] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [22] Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa. Robustifying vision transformer without retraining from scratch by test-time class-conditional feature alignment. In International Joint Conference on Artiﬁcial Intelligence, 2022. [23] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Proceedings of the 37 th International Conference on Machine Learning (ICML), 2020. [24] Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. Domain impression: A source data free domain adaptation method. In IEEE Winter Conference on Applications of Computer Vision (WACV), 2021. [25] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classiﬁcation loss functions. In International Conference on Learning Representations, 2022. [26] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsuper- vised domain adaptation without source data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [27] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train for semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, 2019. [28] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. InProceedings of the 37th International Conference on Machine Learning (ICML), 2020. [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [30] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021. [31] Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Advances in Neural Information Processing Systems, 2021. [32] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning with joint distribution adaptation. In IEEE/CVF International Conference on Computer Vision (ICCV), 2013. [33] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep adaptation networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015. [34] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. In SIGGRAPH, 2020. 12[35] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-Time Adaptation to Distribution Shift by Conﬁdence Maximization and Input Transformation. arXiv preprint arXiv: 2106.14999, 2021. [36] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efﬁcient test-time model adaptation without forgetting. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. [37] Junhyuk Oh, Matteo Hessel, Wojciech M. Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, and David Silver. Discovering reinforcement learning algorithms. In Advances in Neural Information Processing Systems, 2020. [38] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge, 2017. [39] Viraj Prabhu, Shivam Khare, Deeksha Kartik, and Judy Hoffman. Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [40] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use self- learning, 2022. URL https://openreview.net/forum?id=1oEvY1a67c1. [41] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In Advances in Neural Information Processing Systems, 2020. [42] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac- tions on Information Theory, 1965. [43] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In Advances in Neural Information Processing Systems, 2022. [44] Prabhu Teja Sivaprasad and François Fleuret. Test time adaptation through perturbation robust- ness. arXiv preprint arXiv: 2110.10232, 2021. [45] Baochen Sun, Jiashi Feng, and Kate Saenko. Correlation alignment for unsupervised domain adaptation. arXiv preprint arXiv: 1612.01939, 2016. [46] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. Unsupervised domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825, 2019. [47] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Proceedings of the 36th International Conference on Machine Learning (ICML), 2019. [48] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014. [49] Dequan Wang, Shaoteng Liu, Sayna Ebrahimi, Evan Shelhamer, and Trevor Darrell. On-target adaptation. arXiv preprint arXiv: 2109.01087, 2021. [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations, 2021. 13[53] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [54] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. In Advances in Neural Information Processing Systems, 2022. [55] Yang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and Jinsong Wang. Domain adaptation for semantic segmentation via class-balanced self-training. European Conference on Computer Vision, 2018. [56] Yang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya Kumar, and Jinsong Wang. Conﬁdence regularized self-training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 14A Appendix A.1 Conjugate Derivations Cross-Entropy Loss : L(h,y) = − c∑ i=1 yilog exp(hi)∑c j=1 exp(hj) = − c∑ i=1 yi ∗hi + log c∑ j=1 exp(hj) = f(h) −y⊤h, (14) where f(h) is log ∑c j=1 exp(hj) and the constraint that ∑c i=1 yi = 1. Now, the conjugate f⋆(y) is given by : f⋆(y) = −min h {f(h) −yTh}= −min h {log c∑ j=1 exp(hj) −yTh} (15) with the constraint ∑c i=1 yi = 1. At the optimality, yi = (∇f(h))i = exp(hi)∑ jexp(hj) (16) Then, f⋆(y) = −log c∑ j=1 exp(hj) + c∑ i=1 hi exp(hi)∑ jexp(hj) = ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj), (17) if the constraint ∑c i=1 yi = 1 is satisﬁed, otherwise f⋆(y) = ∞by duality. This in turn gives, the conjugate loss for cross-entropy (when the constraint is satisﬁed) : Lconj(h) = −f⋆(y) = −f⋆(∇f(h)) = − ∑ i exp(hi)∑ jexp(hj) log exp(hi)∑ jexp(hj) (18) Squared Loss : L(h,y) = 1 2||h−y||2 2 ≈1 2||h||2 2 −y⊤h [ignoring the constant term] = f(h) −y⊤h, (19) Now, the conjugate f⋆(y) is given by: f⋆(y) = −min h {f(h) −yTh}= −min h {1 2||h||2 2 −yTh} = −1 2||h||2 2 (20) A.2 Experiments on Binary Classiﬁcation with Exponential Loss Here we present the results on a binary classiﬁcation task over a synthetic dataset of 100 dimensional gaussian clusters. 15Dataset Creation For the binary classiﬁcation task, we create a synthetic dataset similar to [23]. Speciﬁcally, let the data X ∼ N(µ,Σ) ∈ R100 and labels Y ∈ {−1,+1}. We sample µ ∼ N(k,I100). For Σ, similar to [ 23], we sample a diagonal matrix D, where each entry is sampled uniformly from a speciﬁed range, and a rotation matrix U from a HAAR distribution, giving Σ = UDUT. For the source data, we sample µ−1 s ,µ+1 s ,Σ−1 s ,Σ+1 s as speciﬁed above with k= 0. Now to create a distribution shifted data of various severity, we sampleµ−1 t ,µ+1 t ,Σ−1 t ,Σ+1 t as speciﬁed above with k= 1, which are then used to sample the shifted data as follows : µ1 λ = λµ1 t + (1 −λ)µ1 s µ−1 λ = λµ−1 t + (1 −λ)µ−1 s Σ1 λ = λΣ1 t + (1 −λ)Σ1 s Σ−1 λ = λΣ−1 t + (1 −λ)Σ−1 s Xλ ∼N(µλ,Σλ) In the following experiments, easy shift refers to λ= 0.6, moderate shift to λ= 0.65 and hard shift to λ= 0.7. Exponential Loss for Binary Classiﬁcation Let zbe the classiﬁcation score hθ(x). For logistic training loss, conjugate adaptation loss would default to entropy with sigmoid probability. Thus, here we experiment with a different but also commonly used surrogate loss to 0/1 loss: exponential loss, which is deﬁned as: Lexp(z,y) = exp(−yz) (21) where y∈{−1,+1}. It can be rewritten in the expanded conjugate form of: Lexp(z,y) = 1 2 · ( ez + e−z) −1 2 ·y· ( ez −e−z) (22) For exponential loss, the conjugate pseudo-label function and the conjugate pseudo-label loss are: yCPL exp (z) = ez −e−z ez + e−z, LCPL exp (z) = 2 ez + e−z (23) The model is adapted on shifted gaussian clusters and we compare the conjugate loss with two baseline approaches: 1) Hard pseudo-labelling exp(−yhard pl ·z); 2) Entropy applied to sigmoid probability P(y= +1) = σ(z). The losses are compared on three degrees of shift (easy, moderate and hard), which is controlled by the drifted distance of Gaussian clusters. The results are shown in Figure 3, where we plot the accuracy curve with respect to adaptation iterations. With easy and moderate shift, conjugate loss (green) generalizes faster to shifted test data; with hard shift, only conjugate loss improves model accuracy on shifted test data while entropy (blue) deteriorates model performance. Figure 3: Test-time adaptation result on synthetic data with three shift levels ranging from easy, moderate and hard (detailed in section A.2). The source model is a linear classiﬁer trained with exponential loss Lexp = e−yhθ(x). Adaptation with the conjugate loss generalizes better compared to baseline losses. 16A.3 Meta Learning Experiment Details In section 3 we talked about learning the meta-loss function parameterized by a neural network mφ : R|Y|↦→R, that takes in the model predictions/logits and outputs a loss value. Here we discuss the architecture chosen and the implementation details. Further, in Appendix A.4 we empirically show that the learnt meta-loss is not affected by the choice of task loss / surrogate loss used in meta learning (Lin Equation 1). Note that the task loss / surrogate loss function is used to update the meta-loss mφ during meta-learning. The surrogate loss is calculated on updated source model’s predictions on labeled samples from test domain. The surrogate loss tries to update the meta-loss in the outer loop such that when meta-loss is later used to update the source model in the inner loop, the source model generalizes better to the test domain. Architecture and Implementation Details Figure 4 gives an overall schema for meta-learning the loss function and algorithm 2 gives the pseudo-code for meta-learning the loss function. Below we describe this in further detail. We use a transformer (denoted by T) with a MLP (denoted by P) over the output of transformer as the architecture for mφ, i.e. mφ(x) = P(T(x)). Speciﬁcally, for a given source trained model hθ and input x∼Dtest : 1. Let hθ(x) ∈R|Y|be the model predictions/logits, where |Y|denotes the number of classes. 2. Let hj θ(x) ∈R,∀j ∈|Y| be the prediction corresponding to class j. 3. The input to transformer is then given by z ∈R|Y|×(1+e), where zj ∈R1+e,∀j ∈|Y| is the concatenation of hj θ(x) and the learnable positional embedding pej ∈Re. 4. The transformer output is given by w= T(z) ∈Rd, where ddenotes the feed-forward dimension of the transformer. 5. The transformer output wis ﬁnally passed through a MLP to get the meta-loss valuemφ(hθ(x)) = P(w) ∈R 6. The source model is updated by optimizing over the meta-loss. θt+1 ←θt −α∂mφt(hθt(x)) ∂θt (24) 7. The updated source model is then used to update the meta-loss by optimizing over some supervised loss function Ltask. φt+1 ←φt −β∂Ltask(hθt+1 (x′),y′) ∂φt , where (x′,y′) ∼Dtest (25) Note that the last step assumes access to labels of test inputs. In this paper, we do not propose meta-learning the TTA loss as an approach. Rather, we use meta-learning to explore what the “best” TTA losses look like. We select the trasformer input embedding dimension (1 + e) from {16,32,64}and transformer feed-forward dimension dfrom {32,64,128}. The number of transformer layers and the hidden layers in MLP are selected from {1,2}. We use Adam optimizer with a learning rate of 1e−3 for learning the meta-loss (i.e. the transformer + MLP). We train the meta-loss for 100 epochs with a batch size of 200. A.4 Effect of Task Loss in Meta Learning In section 3, we show that the meta losses learned on different source classiﬁers differ substantially if the source classiﬁers are trained using different source loss functions. Here we further empirically verify that the learnt meta loss is not affected by the task loss used in meta learning (Lin Equation 1). Thus the learnt meta loss is determined by the source model. In Figure 5, we show the meta loss learnt on a ResNet-26 trained with Cross Entropy loss for two meta task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. We plot the meta loss as a function over one of its input prediction scores, while keeping other ﬁxed. We can see that the task loss barely affects the learnt meta loss. Similar observations can be made for the classiﬁer trained with squared loss Figure 6. 17Meta-Loss  Backpropogate  Figure 4: Meta-Loss learning procedure : The model predictions hθt(x) are passed through the parameterized loss function mφt, which outputs a loss value. We optimize φ such that when optimizing the source model over the loss mφt(hθt(x)), the updated θt+1 has a better performance on the test domain. To do this, we take one gradient step over the meta-loss to get the update source model parameters θt+1, and then update φby evaluating θt+1 on the labeled validation data using some task loss Ltask. Algorithm 2 Learning the Meta-Loss Input: Source trained classiﬁer hθ0 . Randomly initialized meta-loss mφ0 . Task loss / Surrogate loss Ltask like cross-entropy or squared loss for meta learning N batches of test data Dtest = [(x1,y1),..., (xN,yN)] Hyperparams: learning rates αand β. for epoch= 0,1,2,... do for n= 0,1,...N −1 do θt+1 ←θt −α ∂mφt(hθt(xn)) ∂θt Sample (xr,yr) ∼Dtest. φt+1 ←φt −β∂Ltask(hθt+1 (xr),yr) ∂φt A.5 Test-Time Adaptation Detail For completeness, we also give the test-time adaptation setup in Algorithm 3. A.6 ImageNet results on each severity level In continuation with results shown in Table 2 in Section 5.3, Table 4 shows the mean errors averaged across the 15 corruption types for each of the severity level on ImageNet-C, for a source classiﬁer trained with PolyLoss (ϵ= 8). A.7 Square Loss Trained Source Classiﬁer In Section 5.3, we brieﬂy discussed that similar to the other source training losses like cross-entropy and polyloss, our proposed conjugate loss outperforms the baselines when the source classiﬁer is 18(a)  (b) Figure 5: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Cross Entropy. Here we show meta loss trained by two different task losses: Cross Entropy Figure 5a and Squared Loss Figure 5b. (a)  (b) Figure 6: Visualizations of meta loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with Squared Loss. Here we show meta loss trained by two different task losses: Cross Entropy Figure 6a and Squared Loss Figure 6b. Algorithm 3 Test-Time Adaptation Input: Source classiﬁer θ0 trained using loss L(hθ(x),y), An unsupervised loss function for test-time adaptation Ltta(x), N batches of test data Dtest = [x1,...,x N] Hyperparams: learning rate η. for n= 0,1,...N −1 do θn+1 = θn −η∇Ltta(xn) ˆyn = hθn+1 (xn) [Predictions for the nth batch] 19Corrution Severity Temperature Robust PL Entropy MEMO Softmax PL Conjugate 1 \u0017 34.27 33.17 34.39 32.49 32.26 \u0013 34.27 32.84 34.39 32.70 32.26 2 \u0017 41.25 39.04 40.38 37.78 37.40 \u0013 41.25 38.50 40.38 37.75 37.40 3 \u0017 47.37 44.04 45.67 42.30 41.72 \u0013 47.37 43.33 45.67 42.14 41.72 4 \u0017 56.63 51.88 54.49 49.61 48.84 \u0013 56.63 51.03 54.49 49.39 48.84 5 \u0017 67.11 62.53 66.13 60.94 59.90 \u0013 67.11 61.80 66.13 60.30 59.90 Mean \u0017 49.32 46.13 48.21 44.62 44.02 \u0013 49.32 45.50 48.21 44.45 44.02 Table 4: Mean Errors across the 15 noises for various severity level on the ImageNet-C dataset, with source model trained using Poly-1 Loss. Note that Temperature scaling helped only in the case of Entropy and Softmax PL. trained using a squared loss. Table 5 shows a detailed comparison with the baselines. We note that for the conjugate of squared loss, the temperature scaling can be wrapped into the learning rate as shown in Section 4.2. Further, on the CIFAR-10-C dataset we observe temperature scaling doesn’t help any of the other baselines too, hence we do not include the temperature row in CIFAR-10-C. Dataset Temperature Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL CIFAR-10-C \u0017 13.71 (±0.07) 13.06 (±0.05) 13.24 (±0.02) 13.22 (±0.04) 14.85 (±0.08)12.99(±0.04) CIFAR-100-C \u0017 50.82 (±0.31) 44.53 (±0.13) 43.55 (±0.12) 51.35 (±0.04) 51.99 (±0.03)43.39(±0.11) \u0013 50.82 (±0.31) 43.99 (±0.15)43.21(±0.08) 51.35 (±0.04) 51.99 (±0.03) 43.39 (±0.11) Table 5: Mean Errors on the common corruptions datasets for source classiﬁer trained using squared loss. We note that temperature scaling didn’t help on the CIFAR-10-C dataset. Source Classiﬁer Errors without adaptation : CIFAR-10-C (28.34%), CIFAR-100-C (68.79%) Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) CIFAR-10-C \u0017 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,1 e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1 e−2, 2 SGD,5 e−3, 3 Adam,1e−3, 2 CIFAR-100-C \u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,5 e−3, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD,1e−2, 2 ImageNet-C \u0017 SGD,1e−2, 1 SGD,2.5 e−3, 1 SGD,1 e−3, 1 SGD,2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1.5 SGD,1e−3, 1 SGD,2.5e−3, 1.5 Table 6: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 1, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using cross-entropy loss. A.8 Hyper-Parameters We share the exact hyper-parameters found using gridsearch over the 4 validation noises for the common corruptions dataset. 20Cross Entropy Classiﬁer Experiments In Section 5.2, Table 1 shows the results when adapting a cross entropy trained classiﬁer on various common corruptions dataset. Table 6 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. PolyLoss Classiﬁer Experiments In Section 5.3, Table 2 shows the results when adapting a polyloss trained classiﬁer on various common corruptions dataset. Table 7 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss. Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−3, 1 SGD,1e−3, 1 SGD,1 e−3, 1 SGD,5 e−3, 1 SGD, 1e−3, 1 SGD, 1e−3, 1 \u0013 SGD,1e−3, 1 SGD,1e−2, 3 SGD,1 e−2, 3 SGD,5 e−3, 3 SGD, 1e−3, 2 SGD, 1e−3, 1.5 CIFAR-100-C\u0017 SGD,1e−2, 1 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD, 1e−2, 1 SGD, 1e−2, 1 \u0013 SGD,1e−2, 1 Adam,1e−3, 3 SGD,1 e−2, 2 SGD,1 e−2, 2 SGD, 1e−2, 2.5 SGD, 1e−2, 1.5 ImageNet-C\u0017 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1 SGD,5e−3, 1 SGD, 2.5e−3, 1 SGD, 2.5e−3, 1 \u0013 SGD,1e−2, 1 SGD,2.5e−3, 1 SGD,2.5e−3, 1.5 SGD,5e−3, 1 SGD, 2.5e−3, 2 SGD, 2.5e−3, 1 Table 7: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 2, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using poly-loss. Squared Loss Classiﬁer Experiments In Section 5.3, we brieﬂy discussed the results when adapt- ing a squared loss trained classiﬁer on various common corruptions dataset. Table 8 gives the optimizer, learning rate and optimal temperature for each of the baseline and our proposed conjugate loss for the results in Table 5. Digit Adaptation Datasets For the experiments on digits adaptation tasks, we do not have any validation set. Hence, we don’t use temperature scaling here (T = 1) and ﬁx the optimizer and LR as Adam and 1e−2 respectively for all the baselines. A.9 Additional Experiments on Digit Adaptation Datasets Similar to the setting of Table 1, we perform additional experiments on digit adaptation datasets when the source classiﬁer is trained using the cross-entropy loss. Note that when the source classiﬁer is trained using cross-entropy loss, the conjugate loss is equal to the softmax-entropy. In the absence of validation dataset in digit adaptation benchmarks, we used a ﬁxed learning rate of 0.01 for all the baselines, optimizer as Adam and an informed temperature scaling guess of T=2. Table 9 compares softmax-entropy minimization with various baselines. Here, again we observe that on SVHN →MNIST benchmark, without temperature scaling, MEMO (10.67% error) outperforms softmax-entropy (14.41% error). However, similar to the observations in Table 1, with temperature scaling, softmax-entropy minimization (9.26% error) is able to match the performance of MEMO (9.36% error). Further, on the SVHN →USPS benchmark, softmax-entropy (conjugate) and MEMO perform similar even without temperature scaling. A.10 Additional Meta Learning the TTA Loss Experiments In Section 3, we tried to learn a test-time adaptation (TTA) loss via meta-learning for adapting a CIFAR10 trained ResNet26 to distribution shifts on CIFAR10 corruptions. Figure 1 showed that the learnt meta-loss looks like a temperature scaled softmax-entropy. In this section, we show the learnt meta loss across a range of settings as described below : 1. Digit Adaptation: Figure 7a and 7b show the learnt meta-loss when adapting a SVHN trained ResNet26 to MNIST dataset and USPS dataset respectively. We observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 2. Various Noise Types: In Figure 8, we show the learnt meta-loss when adapting a ResNet26 trained on CIFAR10 dataset using cross-entropy loss, to various noise types like speckle, gaussian, saturate and spatter. The severity level is kept ﬁxed at the maximum i.e. 5. 21Dataset T Hard PL Robust PL ENT MEMO Softmax PL Conjugate PL (Ours) CIFAR-10-C\u0017 SGD,1e−2, 1 SGD,1 e−2, 1 SGD,1 e−2, 1 SGD,1e−2, 1 SGD,1 e−4, 1 SGD,1e−2, 1 CIFAR-100-C\u0017 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam,1e−3, 1 Adam, 1e−4, 1 Adam, 1e−3, 1 \u0013 Adam,1e−3, 1 Adam,1e−3, 0.5 Adam,1e−3, 2 Adam,1e−3, 2 Adam, 1e−4, 2.5 Adam, 1e−3, 1 Table 8: Hyper-parameters (Optimizer, Learning Rate, Temperature) for the results in Table 5, where we showed the mean errors on the common corruptions dataset for a source classiﬁer trained using squared loss. Dataset Temperature (T) Hard PL Robust PL MEMO Conjugate PL (ENT) SVHN→MNIST \u0017 21.54 27.44 10.67 14.41 \u0013 21.54 13.26 9.36 9.26 SVHN→USPS \u0017 26.06 26.81 22.72 22.57 \u0013 26.06 22.32 22.42 22.27 Table 9: Mean errors when adapting to digit adaptation benchmarks using a source classiﬁer trained via cross-entropy loss. Here, conjugate pseudo-labeling becomes softmax-entropy minimization. Again we observe that with the right temperature scaling, softmax-entropy minimization matches other approaches. For additional context, the source classiﬁer errors without adaptation are: SVHN →MNIST (34.17%), SVHN →USPS (31.84%). 20  10  0 10 20 prediction score 5 0 5 10loss value meta loss (error 10.44%) softmax entropy (error 14.41) fitted entropy (error 9.26) Meta Loss for SVHN -> MNIST (a) 20  10  0 10 20 prediction score 6 4 2 0 2 4 6 8 loss value meta loss (error 20.13%) softmax entropy (error 22.57) fitted entropy (error 22.22) Meta Loss for SVHN -> USPS adpatation (b) Figure 7: Visualizations of the learnt meta-loss by varying one input dimension (prediction score). The source model is a ResNet-26 trained with cross-entropy on the SVHN dataset. (a) The learnt meta-loss when adapting to the MNIST test dataset. (b) The learnt meta-loss when adapting to the USPS test dataset. 3. Various Severity Levels: In Figure 9, we vary the severity level of the noise, keeping the noise type ﬁxed. 4. Dataset and Architecture: In Figure 10, we compare the learnt meta-loss when adapting to speckle noise, for different source classiﬁer architectures (ResNet26 and ResNet50) and different source training dataset (CIFAR10 and CIFAR100). In all the cases, we again observe that the learnt meta-loss can be well approximated by a temperature scaled softmax-entropy. 5. Squared Loss : Finally, in Figure 11 we show the learnt meta-loss for classiﬁers trained with squared loss function instead of cross-entropy. We observe that in this case, the learnt meta loss mimics a quadratic function as expected from the conjugate formulation. 22For each of the learnt meta losses, we also show the values (α,T,C ) we use to ﬁt the meta loss with softmax entropy function: α·H(softmax(x/T)) −C. Note that although the learnt meta-loss can be approximated by the conjugate, the parameters α,T,C differ across the settings. In the case of classiﬁers trained with squared loss, we ﬁt the meta loss with a quadratic function∑K i=1(A·x2 i + C), where Kis the number of classes and xis the logit vector. Again, we also show the ﬁtted parameter value A,C. The meta loss follows the trend of a quadratic function. The ﬁtted quadratic function performs better or similar as the meta loss, while the parameters of the ﬁtted quadratic function remain different across the meta learning setup (base classiﬁer architectures and noise types). (a)  (b) (c)  (d) Figure 8: Visualization of meta loss (blue) learnt from various noise types in CIFAR-10-C validation set, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 23(a)  (b) (c)  (d) Figure 9: Visualization of meta loss (blue) learnt on speckle noise with different severity level for CIFAR-10-C, where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters (α,T,C ) in the ﬁtted entropy. 24(a)  (b) (c)  (d) Figure 10: Visualization of meta loss (blue) learnt across datasets (CIFAR-10-C/CIFAR-100-C) and base classiﬁer architectures (ResNet-26/ResNet-50), where base classiﬁers are trained with cross-entropy loss. We show the error of meta loss, softmax entropy and ﬁtted entropy for test-time adaptation on the corresponding noise types. We also show the parameters ( α,T,C ) in the ﬁtted entropy. (a)  (b) Figure 11: Visualization of meta loss (blue), where base classiﬁer is trained with quadratic loss. We show the error of meta loss, softmax entropy and ﬁtted quadratic function for test-time adaptation on the corresponding noise types. We also show the parameters ( A,B,C ) in the ﬁtted quadratic function. 25",
      "meta_data": {
        "arxiv_id": "2207.09640v2",
        "authors": [
          "Sachin Goyal",
          "Mingjie Sun",
          "Aditi Raghunathan",
          "Zico Kolter"
        ],
        "published_date": "2022-07-20T04:02:19Z",
        "pdf_url": "https://arxiv.org/pdf/2207.09640v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of selecting effective unsupervised objectives for Test-Time Adaptation (TTA) by proposing a principled framework based on the convex conjugate function. It reveals through meta-learning experiments that the 'best' TTA loss is dependent on the source classifier's training loss (e.g., softmax-entropy for cross-entropy, negative squared error for squared loss). The core contribution is a generic recipe for deriving TTA losses for a broad class of supervised training losses, showing that the conjugate adaptation loss is equivalent to self-training with specific 'conjugate pseudo-labels'. Empirically, the proposed conjugate pseudo-labeling method consistently outperforms other TTA alternatives across various domain adaptation benchmarks, especially for models trained with novel loss functions like PolyLoss.",
        "methodology": "The methodology involves two main components: 1) An exploratory meta-learning procedure where the TTA loss function is parameterized by a neural network and learned by differentiating through the adaptation process to discover optimal TTA losses. 2) The proposed Conjugate Pseudo-labels (CPL) framework, which defines the TTA loss Lconj(hθ(x)) as the negative convex conjugate of a function f from the supervised training loss L(hθ(x),y) = f(hθ(x)) - yᵀhθ(x). This approach interprets TTA as self-training using conjugate pseudo-labels ˜yCPLθ(x) = ∇f(hθ(x)). The adaptation process (Algorithm 1) involves iteratively updating model parameters by taking gradient steps on this conjugate pseudo-labeling loss, typically applying a temperature scaling T to the predictions hθ(x)/T, and often updating only the batch normalization layers.",
        "experimental_setup": "The evaluation uses three common corruption benchmarks (CIFAR-10-C, CIFAR-100-C, ImageNet-C) with errors averaged across corruptions and severity, and three domain adaptation datasets (SVHN to MNIST, ImageNet to ImageNet-R, VisDA-C). Source classifiers include ResNet-26 for CIFAR, ResNet-18 for SVHN, and ResNet-50 for ImageNet/VisDA-C. Models are trained with cross-entropy, PolyLoss, and squared loss. Baselines include Hard Pseudo-Labels (PL), Soft Pseudo-Labels (PL), Entropy Minimization (ENT), Robust Pseudo-Label (RPL), and MEMO. TTA involves fine-tuning batch normalization layers, with LR and temperature (T) tuned via grid-search on validation noises for corruption benchmarks, or fixed (T=1, LR from literature) for domain adaptation tasks. All experiments were performed on A6000 GPUs.",
        "limitations": "The work does not fully explain the fundamental reasons for the optimality of the discovered conjugate loss, beyond showing its proximity to the oracle supervised loss. The meta-learning framework used was constrained to learn functions only over the logits of individual inputs, suggesting potential for more complex loss functions. Achieving good TTA still relies on heuristics like updating only batch norm parameters and temperature scaling, whose precise roles are not concretely understood. Furthermore, it remains an open problem to identify the types of real-world distribution shifts where self-training-based approaches are most beneficial.",
        "future_research_directions": "Future research directions include expanding the meta-learning framework to learn more complex loss functions, such as those operating on intermediate representations or accounting for interactions within a batch of inputs. A deeper understanding of the role of practical heuristics like batch normalization parameter updates and temperature scaling in TTA is also suggested. Investigating the specific real-world distribution shifts where self-training-based methods offer significant advantages is another open problem. Finally, extending and applying the conjugate pseudo-labeling framework to other settings like semi-supervised learning is a promising avenue for further exploration."
      }
    },
    {
      "title": "Look-ahead Meta Learning for Continual Learning",
      "abstract": "The continual learning problem involves training models with limited capacity\nto perform well on a set of an unknown number of sequentially arriving tasks.\nWhile meta-learning shows great potential for reducing interference between old\nand new tasks, the current training procedures tend to be either slow or\noffline, and sensitive to many hyper-parameters. In this work, we propose\nLook-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm\nfor online-continual learning, aided by a small episodic memory. Our proposed\nmodulation of per-parameter learning rates in our meta-learning update allows\nus to draw connections to prior work on hypergradients and meta-descent. This\nprovides a more flexible and efficient way to mitigate catastrophic forgetting\ncompared to conventional prior-based methods. La-MAML achieves performance\nsuperior to other replay-based, prior-based and meta-learning based approaches\nfor continual learning on real-world visual classification benchmarks. Source\ncode can be found here: https://github.com/montrealrobotics/La-MAML",
      "full_text": "La-MAML: Look-ahead Meta Learning for Continual Learning Gunshi Gupta ∗ Mila, UdeM guptagun@mila.quebec Karmesh Yadav * Carnegie Mellon University karmeshy@andrew.cmu.edu Liam Paull Mila, UdeM paulll@iro.umontreal.ca Abstract The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or ofﬂine, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online- continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more ﬂexible and efﬁcient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classiﬁcation benchmarks. 1 Introduction Embodied or interactive agents that accumulate knowledge and skills over time must possess the ability to continually learn. Catastrophic forgetting [11, 18], one of the biggest challenges in this setup, can occur when the i.i.d. sampling conditions required by stochastic gradient descent (SGD) are violated as the data belonging to different tasks to be learnt arrives sequentially. Algorithms for continual learning (CL) must also use their limited model capacity efﬁciently since the number of future tasks is unknown. Ensuring gradient-alignment across tasks is therefore essential, to make shared progress on their objectives. Gradient Episodic Memory (GEM) [17] investigated the connection between weight sharing and forgetting in CL and developed an algorithm that explicitly tried to minimise gradient interference. This is an objective that meta-learning algorithms implicitly optimise for (refer to [20] for derivations of the effective parameter update made in ﬁrst and second order meta learning algorithms). Meta Experience Replay (MER) [22] formalized the transfer- interference trade-off and showed that the gradient alignment objective of GEM coincide with the objective optimised by the ﬁrst order meta-learning algorithm Reptile [20]. Besides aligning gradients, meta-learning algorithms show promise for CL since they can directly use the meta-objective to inﬂuence model optimisation and improve on auxiliary objectives like generalisation or transfer. This avoids having to deﬁne heuristic incentives like sparsity [ 15] for better CL. The downside is that they are usually slow and hard to tune, effectively rendering them more suitable for ofﬂine continual learning [12, 22]. In this work, we overcome these difﬁculties and develop a gradient-based meta-learning algorithm for efﬁcient, online continual learning. We ﬁrst propose a base algorithm for continual meta-learning referred to as Continual-MAML (C-MAML) that utilizes a replay-buffer and optimizes a meta-objective that mitigates forgetting. Subsequently, ∗equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2007.13904v2  [cs.LG]  12 Nov 2020we propose a modiﬁcation to C-MAML, named La-MAML, which incorporates modulation of per- parameter learning rates (LRs) to pace the learning of a model across tasks and time. Finally, we show that the algorithm is scalable, robust and achieves favourable performance on several benchmarks of varying complexity. 2 Related work Relevant CL approaches can be roughly categorized intoreplay-based, regularisation (or prior-based) and meta-learning-based approaches. In order to circumvent the issue of catastrophic forgetting,replay-based methods maintain a collection of samples from previous tasks in memory. Approaches utilising an episodic-buffer [5, 21] uniformly sample old data points to mimic the i.i.d. setup within continual learning. Generative-replay [27] trains generative models to be able to replay past samples, with scalability concerns arising from the difﬁculty of modeling complex non-stationary distributions. GEM [17] and A-GEM [6] take memory samples into account to determine altered low-interference gradients for updating parameters. Regularisation-based methods avoid using replay at all by constraining the network weights according to heuristics intended to ensure that performance on previous tasks is preserved. This involves penal- ising changes to weights deemed important for old tasks [14] or enforcing weight or representational sparsity [3] to ensure that only a subset of neurons remain active at any point of time. The latter method has been shown to reduce the possibility of catastrophic interference across tasks [15, 26]. Meta-Learning-based approaches are fairly recent and have shown impressive results on small benchmarks like Omniglot and MNIST. MER [22], inspired by GEM[17], utilises replay to incentivise alignment of gradients between old and new tasks. Online-aware Meta Learning (OML) [ 12] introduces a meta-objective for a pre-training algorithm to learn an optimal representation ofﬂine, which is subsequently frozen and used for CL. [2, 10, 19] investigate orthogonal setups in which a learning agent uses all previously seen data to adapt quickly to an incoming stream of data, thereby ignoring the problem of catastrophic forgetting. Our motivation lies in developing a scalable, online algorithm capable of learning from limited cycles through streaming data with reduced interference on old samples. In the following sections, we review background concepts and outline our proposed algorithm. We also note interesting connections to prior work not directly pertaining to CL. 3 Preliminaries We consider a setting where a sequence of T tasks [τ1,τ2,..τT] is learnt by observing their training data [D1,D2,..DT] sequentially. We deﬁne Xi,Y i = {(xi n,yi n)}Ni n=0 as the set of Ni input-label pairs randomly drawn from Di. An any time-step jduring online learning, we aim to minimize the empirical risk of the model on all the ttasks seen so far (τ1:t), given limited access to data (Xi,Y i) from previous tasks τi (i<t ). We refer to this objective as the cumulative risk, given by: t∑ i=1 E(Xi,Yi) [ ℓi ( fi ( Xi; θ ) ,Y i)] = E(X1:t,Y1:t) [ Lt ( f ( X1:t; θ ) ,Y 1:t)] (1) where ℓi is the loss on τi and fi is a learnt, possibly task-speciﬁc mapping from inputs to outputs using parameters θj 0. Lt = ∑t i=1 ℓi is the sum of all task-wise losses for tasks τ1:t where tgoes from 1 to T. Let ℓdenote some loss objective to be minimised. Then the SGD operator acting on parameters θj 0, denoted by U(θj 0) is deﬁned as: U ( θj 0 ) = θj 1 = θj 0 −α∇θj 0 ℓ(θj 0) = θj 0 −αgj 0 (2) where gj 0 = ∇θj 0 ℓ(θj 0). U can be composed for kupdates as Uk ( θj 0 ) = U...◦U ◦U(θj 0) = θj k. α is a scalar or a vector LR. U(·,x) implies gradient updates are made on data sample x. We now introduce the MAML [9] and OML [12] algorithms, that we build upon in Section 4. Model-Agnostic Meta-Learning (MAML) : Meta-learning [ 24], or learning-to-learn [29] has emerged as a popular approach for training models amenable to fast adaptation on limited data. 2MAML [9] proposed optimising model parameters to learn a set of tasks while improving on auxil- iary objectives like few-shot generalisation within the task distributions. We review some common terminology used in gradient-based meta-learning: 1) at a given time-step jduring training, model parameters θj 0 (or θ0 for simplicity), are often referred to as aninitialisation, since the aim is to ﬁnd an ideal starting point for few-shot gradient-based adaptation on unseen data. 2) Fast or inner-updates, refer to gradient-based updates made to a copy of θ0, optimising some inner objective (in this case, ℓi for some τi). 3) A meta-update involves the trajectory of fast updates from θ0 to θk, followed by making a permanent gradient update (or slow-update) to θ0. This slow-update is computed by evalu- ating an auxiliary objective (or meta-loss Lmeta) on θk, and differentiating through the trajectory to obtain ∇θ0 Lmeta(θk). MAML thus optimises θj 0 at time j, to perform optimally on tasks in {τ1:t} after undergoing a few gradient updates on their samples. It optimises in every meta-update, the objective: min θj 0 Eτ1:t [ Lmeta ( Uk(θj 0) )] = min θj 0 Eτ1:t [ Lmeta(θj k) ] . (3) Equivalence of Meta-Learning and CL Objectives : The approximate equivalence of ﬁrst and second-order meta-learning algorithms like Reptile and MAML was shown in [20]. MER [22] then showed that their CL objective of minimising loss on and aligning gradients between a set of tasks τ1:t seen till any time j(on the left), can be optimised by the Reptile objective (on the right), ie. : min θj 0   t∑ i=1 ( ℓi(θj 0) ) −α ∑ p,q≤t   ∂ℓp ( θj 0 ) ∂θj 0 · ∂ℓq ( θj 0 ) ∂θj 0    = min θj 0 Eτ1:t [ Lt ( Uk(θj 0) )] (4) where the meta-loss Lt = ∑t i=1 ℓi is evaluated on samples from tasks τ1:t. This implies that the procedure to meta-learn an initialisation coincides with learning optimal parameters for CL. Online-aware Meta-Learning (OML) : [12] proposed to meta-learn a Representation-Learning Network (RLN) to provide a representation suitable for CL to a Task-Learning Network (TLN). The RLN’s representation is learnt in anofﬂine phase, where it is trained using catastrophic forgetting as the learning signal. Data from a ﬁxed set of tasks (τval), is repeatedly used to evaluate the RLN and TLN as the TLN undergoes temporally correlated updates. In every meta-update’s inner loop, the TLN undergoes fast updates on streaming task data with a frozen RLN. The RLN and updated TLN are then evaluated through a meta-loss computed on data from τval along with the current task. This tests how the performance of the model has changed on τval in the process of trying to learn the streaming task. The meta-loss is then differentiated to get gradients for slow updates to the TLN and RLN. This composition of two losses to simulate CL in the inner loop and test forgetting in the outer loop, is referred to as the OML objective. The RLN learns to eventually provide a better representation to the TLN for CL, one which is shown to have emergent sparsity. 4 Proposed approach In the previous section, we saw that the OML objective can directly regulate CL behaviour, and that MER exploits the approximate equivalence of meta-learning and CL objectives. We noted that OML trains a static representation ofﬂine and that MER’s algorithm is prohibitively slow. We show that optimising the OML objective online through a multi-step MAML procedure is equivalent to a more sample-efﬁcient CL objective. In this section, we describe Continual-MAML (C-MAML), the base algorithm that we propose for online continual learning. We then detail an extension to C-MAML, referred to as Look-Ahead MAML (La-MAML), outlined in Algorithm 1. 4.1 C-MAML C-MAML aims to optimise the OML objective online, so that learning on the current task doesn’t lead to forgetting on previously seen tasks. We deﬁne this objective, adapted to optimise a model’s parameters θinstead of a representation at time-step j, as: min θj 0 OML(θj 0,t) = min θj 0 ∑ Sj k∼Dt [ Lt ( Uk(θj 0,Sj k) )] (5) 3where Sj k is a stream of kdata tuples ( Xt j+l,Y t j+l )k l=1 from the current task τt that is seen by the model at time j. The meta-loss Lt = ∑t i=1 ℓi is evaluated on θj k = Uk(θj 0,Sj k). It evaluates the ﬁtness of θj k for the continual learning prediction task deﬁned in Eq. 1 until τt. We omit the implied data argument (xi,yi) ∼(Xi,Y i) that is the input to each loss ℓi in Lt for any task τi. We will show in Appendix B that optimising our objective in Eq. 5 through the k-step MAML update in C-MAML also coincides with optimising the CL objective of AGEM [6]: min θj 0 Eτ1:t [ Lt ( Uk(θj 0) )] = min θj 0 t∑ i=1  ℓi(θj 0) −α ∂ℓi ( θj 0 ) ∂θj 0 · ∂ℓt ( θj 0 ) ∂θj 0  . (6) This differs from Eq. 4’s objective by beingasymmetric: it focuses on aligning the gradients of τt and the average gradient of τ1:t instead of aligning all the pair-wise gradients between tasks τ1:t. In Appendix D, we show empirically that gradient alignment amongst old tasks doesn’t degrade while a new task is learnt, avoiding the need to repeatedly optimise the inter-task alignment between them. This results in a drastic speedup over MER’s objective (Eq. 4) which tries to align allτ1:t equally, thus resampling incoming samples s∼τt to form a uniformly distributed batch over τ1:t. Since each sthen has 1 t-th the contribution in gradient updates, it becomes necessary for MER to take multiple passes over many such uniform batches including s. Figure 1: The proposed La-MAML algorithm: For every batch of data, the initial weights undergo a series of k fast updates to obtain θj k (here j = 0), which is evaluated against a meta-loss to backpropagate gradients with respect to the weights θ0 0 and LRs α0. First α0 is updated to α1 which is then used to update θ0 0 to θ1 0 The blue boxes indicate fast weights while the green boxes indicate gradients for the slow updates. LRs and weights are updated in an asynchronous manner. During training, a replay-buffer Ris populated through reservoir samplingon the incoming data stream as in [ 22]. At the start of every meta- update, a batch bis sampled from the current task. bis also combined with a batch sampled from Rto form themeta-batch, bm, representing samples from both old and new tasks. θj 0 is updated through kSGD-based inner-updates by seeing the current task’s samples frombone at a time. The outer-loss or meta-loss Lt(θj k) is evaluated on bm. It indicates the performance of parameters θj k on all the tasks τ1:t seen till time j. The complete training procedure is described in Appendix C. 4.2 La-MAML Despite the fact that meta-learning incentivises the alignment of within-task and across-task gra- dients, there can still be some interference be- tween the gradients of old and new tasks, τ1:t−1 and τt respectively. This would lead to forget- ting on τ1:t−1, since its data is no longer fully available to us. This is especially true at the beginning of training a new task, when its gradients aren’t necessarily aligned with the old ones. A mechanism is thus needed to ensure that meta-updates are conservative with respect to τ1:t−1, so as to avoid negative transfer on them. The magnitude and direction of the meta-update needs to be regulated, guided by how the loss on τ1:t−1 would be affected by the update. We propose Lookahead-MAML (La-MAML), where we include a set of learnable per-parameter learning rates (LRs) to be used in the inner updates, as depicted in Figure 1. This is motivated by our observation that the expression for the gradient of Eq. 5 with respect to the inner loop’s LRs directly reﬂects the alignment between the old and new tasks. The augmented learning objective is deﬁned as min θj 0,αj ∑ Sj k∼Dt [ Lt ( Uk ( αj,θj 0,Sj k ))] , (7) 4and the gradient of this objective at timej, with respect to the LR vectorαj (denoted as gMAML(αj)) is then given as: gMAML(αj) = ∂ ∂αjLt ( θj k ) = ∂ ∂θj k Lt ( θj k ) · ( − k−1∑ k′=0 ∂ ∂θj k′ ℓt ( θj k′ )) . (8) We provide the full derivation in the Appendix A, and simply state the expression for a ﬁrst-order approximation [9] of gMAML(α) here. The ﬁrst term in gMAML(α) corresponds to the gradient of the meta-loss on batch bm: gmeta. The second term indicates the cumulative gradient from the inner-updates: gtraj. This expression indicates that the gradient of the LRs will be negative when the inner product between gmeta and gtraj is high, ie. the two are aligned; zero when the two are orthogonal (not interfering) and positive when there is interference between the two. Negative (positive) LR gradients would pull up (down) the LR magnitude. We depict this visually in Figure 2. Algorithm 1 La-MAML : Look-ahead MAML Input: Network weights θ, LRs α, inner objective ℓ, meta objective L, learning rate for α: η j ←0, R←{} ⊿Initialise Replay Buffer for t:= 1 to T do for ep:= 1 to numepochs do for batch bin (Xt,Y t) ∼Dt do k←sizeof(b) bm ←Sample(R) ∪b for n= 0 to k−1 do Push b[k′] to R with reservoir sampling θj k′+1 ←θj k′ −αj ·∇θj k′ end for αj+1 ←αj −η∇αj Lt(θj k,bm) (a) θj+1 0 ←θj 0 −max(0,αj+1) ·∇θj 0 Lt(θj k,bm) (b) j ←j+ 1 end for end for end for Figure 2: Different scenarios for the alignment of gtraj (blue dashed line) and gmeta, going from interference (left) to alignment (right). Yellow ar- rows denote the inner updates. The LR αincreases (decreases) when gradients align (interfere). We propose updating the network weights and LRs asynchronously in the meta-update. Let αj+1 be the updated LR vector obtained by taking an SGD step with the LR gradient from Eq. 8 at time j. We then update the weights as: θj+1 0 ←θj 0 −max(0,αj+1) ·∇θj 0 Lt(θj k) (9) where kis the number of steps taken in the inner-loop. The LRs αj+1 are clipped to positive values to avoid ascending the gradient, and also to avoid making interfering parameter-updates, thus mitigating catastrophic forgetting. The meta-objective thus conservatively modulates the pace and direction of learning to achieve quicker learning progress on a new task while facilitating transfer on old tasks. Algorithm 1 2 illustrates this procedure. Lines (a), (b) are the only difference between C-MAML and La-MAML, with C-MAML using a ﬁxed scalar LR αfor the meta-update to θj 0 instead of αj+1. Our meta-learning based algorithm incorporates concepts from both prior-based and replay-based approaches. The LRs modulate the parameter updates in an data-driven manner, guided by the interplay between gradients on the replay samples and the streaming task. However, since LRs evolve with every meta-update, their decay is temporary. This is unlike many prior-based approaches, where penalties on the change in parameters gradually become so high that the network capacity saturates [14]. Learnable LRs can be modulated to high and low values as tasks arrive, thus being a simpler, ﬂexible and elegant way to constrain weights. This asynchronous update resembles trust-region optimisation [31] or look-ahead search since the step-sizes for each parameter are adjusted based on 2The code for our algorithm can be found at: https://github.com/montrealrobotics/La-MAML 5the loss incurred after applying hypothetical updates to them. Our LR update is also analogous to the heuristic uncertainty-based LR update in UCB [8], BGD [32], which we compare to in Section 5.3. 4.3 Connections to Other Work Stochastic Meta-Descent (SMD) : When learning over a non-stationary data distribution, using decaying LR schedules is not common. Strictly diminishing LR schedules aim for closer convergence to a ﬁxed mimima of a stationary distribution, which is at odds with the goal of online learning. It is also not possible to manually tune these schedules since the extent of the data distribution is unknown. However, adaptivity in LRs is still highly desired to adapt to the optimisation landscape, accelerate learning and modulate the degree of adaptation to reduce catastrophic forgetting. Our adaptive LRs can be connected to work on meta-descent [4, 25] in ofﬂine supervised learning (OSL). While several variations of meta-descent exist, the core idea behind them and our approach is gain adaptation. While we adapt the gain based on the correlation between old and new task gradients to make shared progress on all tasks, [4, 25] use the correlation between two successive stochastic gradients to converge faster. We rely on the meta-objective’s differentiability with respect to the LRs, to obtain LR hypergradients automatically. Learning LRs in meta-learning: Meta-SGD [16] proposed learning the LRs in MAML for few-shot learning. Some notable differences between their update and ours exist. They synchronously update the weights and LRs while ourasynchronous update to the LRs serves to carry out a more conservative update to the weights. The intuition for our update stems from the need to mitigate gradient interference and its connection to the transfer-interference trade-off ubiquitous in continual learning. α-MAML [28] analytically updates the two scalar LRs in the MAML update for more adaptive few-shot learning. Our per-parameter LRs are modulated implicitly through back-propagation, to regulate change in parameters based on their alignment across tasks, providing our model with a more powerful degree of adaptability in the CL domain. 5 Experiments In this section, we evaluate La-MAML in settings where the model has to learn a set of sequentially streaming classiﬁcation tasks. Task-agnostic experiments, where the task identity is unknown at training and test-time, are performed on the MNIST benchmarks with a single-headed model. Task- aware experiments with known task identity, are performed on the CIFAR and TinyImagenet [ 1] datasets with a multi-headed model. Similar to [ 22], we use the retained accuracy (RA) metric to compare various approaches. RA is the average accuracy of the model across tasks at the end of training. We also report the backward-transfer and interference (BTI) values which measure the average change in the accuracy of each task from when it was learnt to the end of the last task. A smaller BTI implies lesser forgetting during training. Efﬁcient Lifelong Learning (LLL): Formalized in [6], the setup of efﬁcient lifelong learning assumes that incoming data for every task has to be processed in only one single pass: once processed, data samples are not accessible anymore unless they were added to a replay memory. We evaluate our algorithm on this challenging (Single-Pass) setup as well as the standard (Multiple-Pass) setup, where Table 1: RA, BTI and their standard deviation on MNIST benchmarks. Each experiment is run with 5 seeds. METHOD ROTATIONS PERMUTATIONS MANY RA BTI RA BTI RA BTI ONLINE 53.38 ± 1.53 -5.44 ± 1.70 55.42 ± 0.65 -13.76 ± 1.19 32.62 ± 0.43 -19.06 ± 0.86 EWC 57.96 ± 1.33 -20.42 ± 1.60 62.32 ± 1.34 -13.32 ± 2.24 33.46 ± 0.46 -17.84 ± 1.15 GEM 67.38 ± 1.75 -18.02 ± 1.99 55.42 ± 1.10 -24.42 ± 1.10 32.14 ± 0.50 -23.52 ± 0.87 MER 77.42 ± 0.78 -5.60 ±0.70 73.46 ± 0.45 -9.96 ± 0.45 47.40 ± 0.35 -17.78 ± 0.39 C-MAML 77.33 ± 0.29 -7.88 ± 0.05 74.54 ± 0.54 -10.36 ± 0.14 47.29 ± 1.21 -20.86 ± 0.95 SYNC 74.07 ± 0.58 -6.66 ± 0.44 70.54 ± 1.54 -14.02 ± 2.14 44.48 ± 0.76 -24.18 ± 0.65 LA-MAML 77.42 ± 0.65 -8.64 ± 0.403 74.34 ± 0.67 -7.60 ± 0.51 48.46 ± 0.45 -12.96 ± 0.073 6ideally ofﬂine training-until-convergence is performed for every task, once we have access to the data. 5.1 Continual learning benchmarks Table 2: Running times for MER and La-MAML on MNIST benchmarks for one epoch METHOD ROTATIONS PERMUTATIONS LA-MAML 45.95 ± 0.38 46.13 ± 0.42 MER 218.03 ± 6.44 227.11 ± 12.12 First, we carry out experiments on the toy con- tinual learning benchmarks proposed in prior CL works. MNIST Rotations, introduced in [17], comprises tasks to classify MNIST digits rotated by a different common angle in [0, 180] degrees in each task. In MNIST Permutations, tasks are generated by shufﬂing the image pixels by a ﬁxed random permutation. Unlike Rotations, the input distribution of each task is unrelated here, leading to less positive transfer between tasks. Both MNIST Permutation and MNIST Rotation have 20 tasks with 1000 samples per task. Many Permutations, a more complex version of Per- mutations, has ﬁve times more tasks (100 tasks) and ﬁve times less training data (200 images per task). Experiments are conducted in the low data regime with only 200 samples for Rotation and Permutation and 500 samples for Many, which allows the differences between the various algorithm to become prominent (detailed in Appendix G). We use the same architecture and experimental settings as in MER [22], allowing us to compare directly with their results. We use the cross-entropy loss as the inner and outer objectives during meta-training. Similar to [20], we see improved performance when evaluating and summing the meta-loss at all steps of the inner updates as opposed to just the last one. We compare our method in the Single-Pass setup against multiple baselines including Online, In- dependent, EWC [14], GEM [17] and MER [22] (detailed in Appendix H), as well as different ablations (discussed in Section 5.3). In Table 1, we see that La-MAML achieves comparable or better performance than the baselines on all benchmarks. Table 2 shows that La-MAML matches the performance of MER in less than 20% of the training time, owing to its sample-efﬁcient objective which allows it to make make more learning progress per iteration. This also allows us to scale it to real-world visual recognition problems as described next. 5.2 Real-world classiﬁcation While La-MAML fares well on the MNIST benchmarks, we are interested in understanding its capabilities on more complex visual classiﬁcation benchmarks. We conduct experiments on the CIFAR-100 dataset in a task-incremental manner [ 17] where, 20 tasks comprising of disjoint 5- way classiﬁcation problems are streamed. We also evaluate on the TinyImagenet-200 dataset by partitioning its 200 classes into 40 5-way classiﬁcation tasks. Experiments are carried out in both the Single-Pass and Multiple-Pass settings, where in the latter we allow all CL approaches to train up to a maximum of 10 epochs. Each method is allowed a replay-buffer, containing upto 200 and 400 samples for CIFAR-100 and TinyImagenet respectively. We provide further details about the baselines in Appendix H and about the architectures, evaluation setup and hyper-parameters in Appendix G. Table 3 reports the results of these experiments. We consistently observe superior performance of La-MAML as compared to other CL baselines on both datasets across setups. While the iCARL baseline attains lower BTI in some setups, it achieves that at the cost of much lower performance throughout learning. Among the high-performing approaches, La-MAML has the lowest BTI. Recent work [7, 22] noted that Experience Replay (ER) is often a very strong baseline that closely matches the performance of the proposed algorithms. We highlight the fact that meta-learning and LR modulation combined show an improvement of more than 10 and 18% (as the number of tasks increase from CIFAR to TinyImagenet) over the ER baseline in our case, with limited replay. Overall, we see that our method is robust and better-performing under both the standard and LLL setups of CL which come with different kinds of challenges. Many CL methods [8, 26] are suitable for only one of the two setups. Further, as explained in Figure 3, our model evolves to become resistant to forgetting as training progresses. This means that beyond a point, it can keep making gradient updates on a small window of incoming samples without needing to do meta-updates. 75.3 Evaluation of La-MAML’s learning rate modulation To capture the gains from learning the LRs, we compare La-MAML with our base algorithm, C-MAML. We ablate our choice of updating LRs asynchronously by constructing a version of C-MAML where per-parameter learnable LRs are used in the inner updates while the meta-update still uses a constant scalar LR during training. We refer to it as Sync-La-MAML or Sync since it has synchronously updated LRs that don’t modulate the meta-update. We also construct an ablation referred to as La-ER, where the parameter updates are carried out as in ER but the LRs are modulated using the La-MAML objective’s ﬁrst-order version. This tells us what the gains of LR modulation are over ER, since there is no meta-learning to encourage gradient alignment of the model parameters. While only minor gains are seen on the MNIST benchmarks from asynchronous LR modulation, the performance gap increases as the tasks get harder. On CIFAR-100 and TinyImagenet, we see a trend in the RA of our variants with La-MAML performing best followed by Sync. This shows that optimising the LRs aids learning and our asynchronous update helps in knowledge consolidation by enforcing conservative updates to mitigate interference. To test our LR modulation against an alternative bayesian modulation scheme proposed in BGD [32], we deﬁne a baseline called Meta-BGD where per-parameter variances are modulated instead of LRs. This is described in further detail in Appendix H. Meta-BGD emerges as a strong baseline and matches the performance of C-MAML given enough Monte Carlo iterations m, implying m times more computation than C-MAML. Additionally, Meta-BGD was found to be sensitive to hyperparameters and required extensive tuning. We present a discussion of the robustness of our approach in Appendix E, as well as a discussion of the setups adopted in prior work, in Appendix I. We also compare the gradient alignment of our three variants along with ER in Table 4 by calculating the cosine similarity between the gradients of the replay samples and newly arriving data samples. As previously stated, the aim of many CL algorithms is to achieve high gradient alignment across tasks to allow parameter-sharing between them. We see that our variants achieve an order of magnitude higher cosine similarity compared to ER, verifying that our objective promotes gradient alignment. 6 Conclusion We introduced La-MAML, an efﬁcient meta-learning algorithm that leverages replay to avoid for- getting and favor positive backward transfer by learning the weights and LRs in an asynchronous manner. It is capable of learning online on a non-stationary stream of data and scales to vision tasks. We presented results that showed better performance against the state-of-the-art in the setup of efﬁcient lifelong learning (LLL) [ 6], as well as the standard continual learning setting. In the future, more work on analysing and producing good optimizers for CL is needed, since many of our standard go-to optimizers like Adam [13] are primarily aimed at ensuring faster convergence in stationary supervised learning setups. Another interesting direction is to explore how the connections to meta-descent can lead to more stable training procedures for meta-learning that can automatically adjust hyper-parameters on-the-ﬂy based on training dynamics. Table 3: Results on the standard continual (Multiple) and LLL (Single) setups with CIFAR-100 and TinyImagenet-200. Experiments are run with 3 seeds. * indicates result omitted due to high instability. METHOD CIFAR-100 T INY IMAGENET MULTIPLE SINGLE MULTIPLE SINGLE RA BTI RA BTI RA BTI RA BTI IID 85.60 ± 0.40 - - - 77.1 ± 1.06 - - - ER 59.70 ± 0.75 -16.50 ± 1.05 47.88 ± 0.73 -12.46 ± 0.83 48.23 ± 1.51 -19.86 ± 0.70 39.38 ± 0.38 -14.33 ± 0.89 ICARL 60.47 ± 1.09 -15.10 ± 1.04 53.55 ± 1.69 -8.03 ± 1.16 54.77 ± 0.32 -3.93 ± 0.55 45.79 ± 1.49 -2.73 ± 0.45 GEM 62.80 ± 0.55 -17.00 ± 0.26 48.27 ± 1.10 -13.7 ± 0.70 50.57 ± 0.61 -20.50 ± 0.10 40.56 ± 0.79 -13.53 ± 0.65 AGEM 58.37 ± 0.13 -17.03 ± 0.72 46.93 ± 0.31 -13.4 ± 1.44 46.38 ± 1.34 -19.96 ± 0.61 38.96 ± 0.47 -13.66 ± 1.73 MER - - 51.38 ± 1.05 -12.83 ± 1.44 - - 44.87 ± 1.43 -12.53 ± 0.58 META-BGD 65.09 ± 0.77 -14.83 ± 0.40 57.44 ± 0.95 -10.6 ± 0.45 * * 50.64 ± 1.98 -6.60 ± 1.73 C-MAML 65.44 ± 0.99 -13.96 ± 0.86 55.57 ± 0.94 -9.49 ± 0.45 61.93 ± 1.55 -11.53 ± 1.11 48.77 ± 1.26 -7.6 ± 0.52 LA-ER 67.17 ± 1.14 -12.63 ± 0.60 56.12 ± 0.61 -7.63 ± 0.90 54.76 ± 1.94 -15.43 ± 1.36 44.75 ± 1.96 -10.93 ± 1.32 SYNC 67.06 ± 0.62 -13.66 ± 0.50 58.99 ± 1.40 -8.76 ± 0.95 65.40 ± 1.40 -11.93 ± 0.55 52.84 ± 2.55 -7.3 ± 1.93 LA-MAML 70.08 ± 0.66 -9.36 ± 0.47 61.18 ± 1.44 -9.00 ± 0.2 66.99 ± 1.65 -9.13 ± 0.90 52.59 ± 1.35 -3.7 ± 1.22 8Figure 3: Retained Accuracy (RA) for La-MAML plotted every 25 meta-updates up to Task 5 on CIFAR-100. RA at iteration j (with j increasing along the x-axis) denotes accuracy on all tasks seen uptil then. Red denotes the RA computed during the inner updates (at θj k). Blue denotes RA computed at θj+1 0 right after a meta-update. We see that in the beginning, inner updates lead to catastrophic forgetting (CF) since the weights are not suitable for CL yet, but eventually become resistant when trained to retain old knowledge while learning on a stream of correlated data. We also see that RA maintains its value even as more tasks are added indicating that the model is successful at learning new tasks without sacriﬁcing performance on old ones. Table 4: Gradient Alignment on CIFAR-100 and TinyImagenet dataset (values lie in [-1,1], higher is better) DATASET ER C-MAML SYNC L A-MAML CIFAR-100 0.22 ×10−2 ± 0.0017 1.84 ×10−2 ± 0.0003 2.28 ×10−2 ± 0.0004 1.86 ×10−2 ± 0.0027 TINY IMAGENET 0.27 ×10−2 ± 0.0005 1.74 ×10−2 ± 0.0005 2.17 ×10−2 ± 0.0020 2.14 ×10−2 ± 0.0023 Broader Impact This work takes a step towards enabling deployed models to operate while learning online. This would be very relevant for online, interactive services like recommender systems or home robotics, among others. By tackling the problem of catastrophic forgetting, the proposed approach goes some way in allowing models to add knowledge incrementally without needing to be re-trained from scratch. Training from scratch is a compute intensive process, and even requires access to data that might not be available anymore. This might entail having to navigate a privacy-performance trade-off since many techniques like federated learning actually rely on not having to share data across servers, in order to protect user-privacy. The proposed algorithm stores and replays random samples of prior data, and even with the higher alignment of the samples within a task under the proposed approach, there will eventually be some concept drift. While the proposed algorithm itself does not rely on or introduce any biases, any bias in the sampling strategy itself might inﬂuence the distribution of data that the algorithm remembers and performs well on. Acknowledgments and Disclosure of Funding The authors are grateful to Matt Riemer, Sharath Chandra Raparthy, Alexander Zimin, Heethesh Vhavle and the anonymous reviewers for proof-reading the paper and suggesting improvements. This research was enabled in part by support provided by Compute Canada (www.computecanada.ca). References [1] URL https://tiny-imagenet.herokuapp.com/. [2] Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Contin- uous adaptation via meta-learning in nonstationary and competitive environments. In International Confer- ence on Learning Representations, 2018. URL https://openreview.net/forum?id=Sk2u1g-0-. [3] Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selﬂess sequential learning. In International Con- ference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkxbrn0cYX. 9[4] Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representa- tions, 2018. URL https://openreview.net/forum?id=BkrsAzWAb. [5] Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 233–248, 2018. [6] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-GEM. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=Hkf2_sC5FX. [7] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K. Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato. On Tiny Episodic Memories in Continual Learning. arXiv e-prints, art. arXiv:1902.10486, February 2019. [8] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklUCCVKDB. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126–1135. JMLR. org, 2017. [10] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research , pages 1920–1930, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/finn19a. html. [11] Robert French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3:128–135, 05 1999. doi: 10.1016/S1364-6613(99)01294-2. [12] Khurram Javed and Martha White. Meta-learning representations for continual learning. In Advances in Neural Information Processing Systems, pages 1818–1828, 2019. [13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 12 2014. [14] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/content/114/13/3521. [15] Lei Le, Raksha Kumaraswamy, and Martha White. Learning sparse representations in reinforcement learning with sparse coding. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, IJCAI’17, page 2067–2073. AAAI Press, 2017. ISBN 9780999241103. [16] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017. [17] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pages 6467–6476, 2017. [18] James Mcclelland, Bruce Mcnaughton, and Randall O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102:419–57, 08 1995. doi: 10.1037/0033-295X.102.3.419. [19] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual adaptation for model-based RL. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyxAfnA5tm. [20] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. [21] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017. 10[22] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [23] Levent Sagun, Utku Evci, V . Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical Analysis of the Hessian of Over-Parametrized Neural Networks. arXiv e-prints, art. arXiv:1706.04454, June 2017. [24] Jürgen Schmidhuber. Evolutionary principles in self-referential learning. 1987. [25] Nicol Schraudolph. Local gain adaptation in stochastic gradient descent. 06 1999. doi: 10.1049/cp: 19991170. [26] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4548–4557, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL http:// proceedings.mlr.press/v80/serra18a.html. [27] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2990–2999. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 6892-continual-learning-with-deep-generative-replay.pdf . [28] Harkirat Singh Behl, Atılım Günes, Baydin, and Philip H. S. Torr. Alpha MAML: Adaptive Model-Agnostic Meta-Learning. arXiv e-prints, art. arXiv:1905.07435, May 2019. [29] Sebastian Thrun and Lorien Pratt, editors. Learning to Learn. Kluwer Academic Publishers, USA, 1998. ISBN 0792380479. [30] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. [31] Ya-xiang Yuan. A review of trust region algorithms for optimization. ICM99: Proceedings of the Fourth International Congress on Industrial and Applied Mathematics, 09 1999. [32] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task Agnostic Continual Learning Using Online Variational Bayes. arXiv e-prints, art. arXiv:1803.10123, Mar 2018. 11A Hypergradient Derivation for La-MAML We derive the gradient of the weights θj 0 and LRs αj at time-step j under the k-step MAML objective, with Lt = ∑t i=0 ℓi as the meta-loss and ℓt as the inner-objective: gMAML(αj) = ∂ ∂αjLt ( θj k ) = ∂ ∂θj k Lt ( θj k ) · ∂ ∂αj ( θj k ) = ∂ ∂θj k Lt ( θj k ) · ∂ ∂αj ( U ( θj k−1 )) = ∂ ∂θj k Lt ( θj k ) · ∂ ∂αj ( θj k−1 −αj∂ℓt(θj k−1) ∂θj k−1 ) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjθj k−1 − ∂ ∂αj ( αj∂ℓt(θj k−1) ∂θj k−1 )) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjθj k−1 −∂ℓt(θj k−1) ∂θj k−1 ) (Taking ∂ℓt ( θj k−1 ) ∂θj k−1 as a constant w.r.t αj to get the ﬁrst-order MAML approximation) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjU ( θj k−2 ) − ( ∂ℓt(θj k−1) ∂θj k−1 )) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjθj 0 − k−1∑ k′=0 ∂ℓt(θj n) ∂θj n ) (a) = ∂ ∂θj k Lt ( θj k ) · ( − k−1∑ k′=0 ∂ℓt(θj k′ ) ∂θj k′ ) (b) Where (a) is obtained by recursively expanding and differentiating the update function U() as done in the step before it. (b) is obtained by assuming that the initial weight in the meta-update at time j : θj 0, is constant with respect to αj. Similarly we can derive the MAML gradient for the weights θj 0, denoted as gMAML(θj 0) as: gMAML(θj 0) = ∂ ∂θj 0 Lt(θj k) = ∂ ∂θj k Lt(θj k)∂θj k ∂θj 0 = ∂ ∂θj k Lt(θj k)∂Uk(θj k−1) ∂θj 0 = ∂ ∂θj k Lt(θj k) ∂ ∂θj k−1 U(θj k−1) ··· ∂ ∂θj 0 U(θj 1) (repeatedly applying chain rule and using θj k = U(θj k−1) ) = L′ t(θj k) ( I−αℓ′′ t(θj k−1) ) ··· ( I−αℓ′′ t(θj 0) ) ( using U′(θj k′ ) = I−αℓ′′ t(θj k′ ) ) (′implies derivative with respect to argument) = (k−1∏ k′=0 ( I−αℓ′′ t(θj k′ ) )) L′ t(θj k) Setting all ﬁrst-order gradient terms as constants to ignore second-order derivatives, we get the ﬁrst order approximation as: gFOMAML(θj 0) = (∏k−1 k′=0 ( I−αℓ′′ t ( θj k′ ))) L′ t(θj k) = L′ t(θj k) In Appendix B, we show the equivalence of the C-MAML and CL objectives in Eq. 6 by showing that the gradient of the former (gMAML(θj 0)) is equivalent to the gradient of the latter. 12B Equivalence of Objectives It is straightforward to show that when we optimise the OML objective through the k-step MAML update, as proposed in C-MAML in Eq. 5: min θj 0 Eτ1:t [ Lt ( Uk(θj 0) )] (10) where the inner-updates are taken using data from the streaming task τt, and the meta-loss Lt(θ) =∑t i=1 ℓi(θ) is computed on the data from all tasks seen so far, it will correspond to minimising the following surrogate loss used in CL : min θj 0 t∑ i=1  ℓi(θj 0) −α ∂ℓi ( θj 0 ) ∂θj 0 · ∂ℓt ( θj 0 ) ∂θj 0   (11) We show the equivalence for the case whenk= 1, for higher kthe form gets more complicated but essentially has a similar set of terms. Reptile [ 20] showed that the k-step MAML gradient for the weights θj 0 at time j, denoted as gMAML(θj 0) is of the form: ∂Lmeta(θj k) ∂θj 0 = ¯gk −α¯Hk k−1∑ k′=0 ¯gk′ −α k−1∑ k′=0 ¯Hk′ ¯gk + O ( α2) (αis the inner-loop learning rate) = ¯g1 −α¯H1¯g0 −α¯H0¯g1 + O ( α2) (using k= 1) Expressing the terms as derivatives, and using ∂ ∂θj 0 (¯g0 ·¯g1) = ¯H1¯g0 + ¯H0¯g1, we get : = ∂Lmeta(θj 0) ∂θj 0 − ∂ ∂θj 0 (¯g0 ·¯g1) = ∂ (∑t i=1 ℓi(θj 0) −α¯g1 ·¯g0 ) ∂θj 0 (substituting Lmeta = Lt = t∑ i=1 ℓi) = ∂ (∑t i=1 ℓi(θj 0) −α∂Lmeta(θj 0) ∂θj 0 ∂ℓt(θj 0) ∂θj 0 ) ∂θj 0 = ∂ (∑t i=1 ℓi(θj 0) −α∂∑t i=1 ℓi(θj 0) ∂θj 0 ∂ℓt(θj 0) ∂θj 0 ) ∂θj 0 (expanding Lmeta) = ∂ (∑t i=1 ℓi(θj 0) −α∑t i=1 ∂ℓi(θj 0) ∂θj 0 ∂ℓt(θj 0) ∂θj 0 ) ∂θj 0 which is the same as the gradient of Eq. 11. where: ¯gk = ∂Lmeta ( θj 0 ) ∂θj 0 (gradient of the meta-loss evaluated at the initial point ) ¯gk′ = ∂ ∂θj 0 Linner(θj 0) ( for k′<k) ( gradients of the inner-updates evaluated at the initial point) θj k′+1 = θj k′ −αgk′ (sequence of parameter vectors) 13¯Hk = L′′ meta ( θj 0 ) (Hessian of the meta-loss evaluated at the initial point) ¯Hk′ = L′′ inner ( θj 0 ) (for k′<k) (Hessian of the inner-objective evaluated at the initial point) And, in our case: Lmeta = Lt = t∑ i=1 ℓi Linner = ℓt Bias in the objective : We can see in Eq. 11 that the gradient alignment term introduces some bias, which means that the parameters don’t exactly converge to the minimiser of the losses on all tasks. This has been acceptable in the CL regime since we don’t aim to reach the minimiser of some stationary distribution anyway (as also mentioned in Section 4.3). If we did converge to the minimiser of say ttasks at some time j, this minimiser would no longer be optimal as soon as we see the new task τt+1. Therefore, in the limit of inﬁnite tasks and time, ensuring low-interference between tasks will pay off much more as opposed to being able to converge to the exact minima, by allowing us to make shared progress on both previous and incoming tasks. C C-MAML Algorithm Algorithm 2 outlines the training procedure for the C-MAML algorithm we propose 3. Algorithm 2 C-MAML Input: Network weights θ0 0, inner objective ℓ, meta objective L, Inner learning rate α, Outer learning rate β j ←0 R←{} ⊿Initialise replay-buffer for t:= 1 to T do (Xt,Y t) ∼Dt for ep:= 1 to numepochs do for batch bin (Xt,Y t) do k←sizeof(b) bm ←Sample(R) ∪b ⊿ batch of samples from τ1:t for meta-loss for k′= 0 to k−1 do Push b[k′] to R with some probability based on reservoir sampling θj k′+1 ←θj k′ −α·∇θj k′ ℓt(θj k′ ,b[k′]) ⊿inner-update on each incoming sample end for θj+1 0 ←θj 0 −β·∇θj 0 Lt(θj k,bm) ⊿outer-update by differentiating meta-loss j ←j+ 1 end for end for end for D Inter-Task Alignment We assume that at time j during training, we are seeing samples from the streaming task τt. It is intuitive to realise that incentivising the alignment of all τ1:t with the current τt indirectly also incentivises the alignment amongst τ1:t−1 as well. To demonstrate this, we compute the mean dot product of the gradients amongst the old tasks τ1:t−1 as the new task τt is added, for tvarying from 2 to 11. We do this for C-MAML and La-MAML on CIFAR-100. 3Our algorithm, Continual-MAML is different from a concurrent work https://arxiv.org/abs/2003. 05856 which proposes an algorithm with the same name 14As can be seen in Figures 4a and 4b, the alignment stays positive and roughly constant even as more tasks are added. (a) C-MAML  (b) La-MAML Figure 4: Average dot product amongst gradients of τ1:t−1 as new tasks are added, for the C-MAML and La-MAML algorithms calculated over 5 runs. x-axis shows the streaming task ID, tand y-axis shows the cosine similarity. E Robustness Learning rate is one of the most crucial hyper-parameters during training and it often has to be tuned extensively for each experiment. In this section we analyse the robustness of our proposed variants to their LR-related hyper-parameters on the CIFAR-100 dataset. Our three variants have different sets of these hyper-parameters which are speciﬁed as follows: • C-MAML: Inner and outer update LR (scalar) for the weights (αand β) • Sync La-MAML: Inner loop initialization value for the vector LRs ( α0), scalar learning rate of LRs (η) and scalar learning rate for the weights in the outer update (β) • La-MAML: Scalar initialization value for the vector LRs (α0) and a scalar learning rate of LRs (η) La-MAML is considerably more robust to tuning compared to its variants, as can be seen in Figure 5c. We empirically observe that it only requires tuning of the initial value of the LR, while being relatively insensitive to the learning rate of the LR (η). We see a consistent trend where the increase in η leads to an increase in the ﬁnal accuracy of the model. This increase is very gradual, since across a wide range of LRs varying over 2 orders of magnitude (from 0.003 to 0.3), the difference in RA is only 6%. This means that even without tuning this parameter (η), La-MAML would have outperformed most baselines at their optimally tuned values. As seen in Figure 5a, C-MAML sees considerable performance variation with the tweaking of both the inner and outer LR. We also see that the effects of the variations of the inner and outer LR follow very similar trends and their optimal values ﬁnally selected are also identical. This means that we could potentially tune them by doing just a 1D search over them together instead of varying both independently through a 2D grid search. The Sync version of La-MAML (Figure 5b), while being relatively insensitive to the scalar initial value α0 and the η, sees considerable performance variation as the outer learning rate for the weights: βis varied. This variant has the most hyper-parameters and only exists for the purpose of ablation. Fig. 6 shows the result of 2D grid-searches over sets of the above-mentioned hyper-parameters for C-MAML and La-MAML for a better overview. F Timing Comparisons In this section, we compare the wall-clock running times (Retained Accuracy (RA) versus Time) of La-MAML against other baselines on the CIFAR100 dataset in the multi-pass setting. For ER, iCarl and La-MAML we see an increasing tread in the RA vs Time plot with La-MAML having the best 15(a) C-MAML: Modulation of αand β  (b) Sync: Modulation of α0, ηand β (c) La-MAML: Modulation of α0 and η Figure 5: Retained Accuracy vs Learning Rates plot for La-MAML and its variants. Figures are plotted by varying one of the learning rate hyperparameter while keeping the others ﬁxed at their optimal value. The hyperparameter is varied between [0.001, 0.3]. RA at the expense of the increase in time. In contrast, both AGEM and GEM perform worse than La-MAML while also taking much more running time. G Experimental We carry out hyperparameter tuning for all the approaches by performing a grid-search over the range [0.0001 - 0.3] for hyper-parameters related to the learning-rate. For the multi-pass setup we use 10 epochs for all the CL approaches. In the single pass setup, all compared approaches have a hyper-parameter called glances which indicates the number of gradient updates or meta-updates made on each incoming sample of data. In the Single-Pass (LLL) setup, it becomes essential to take multiple gradient steps on each sample (or see each sample for multipleglances), since once we move on to later samples, we can’t revisit old data samples. The performance of the algorithms naturally increases with the increase in glances up to a certain point. To ﬁnd the optimal number of glances to take over each sample, we search over the values [1,2,3,5,10]. Tables 5 and 6 lists the optimal hyperparameters for all the compared approaches. All setups used the SGD optimiser since it was found to preform better than Adam [13] (possibly due to reasons stated in Section 4.3 regarding the CL setup). To avoid exploding gradients, we clip the gradient values of all approaches at a norm of 2.0. Class di- visions across different tasks vary with the random seeds with which the experiments were conducted. Overall, we did not see much variability across different class splits, with the variation being within 0.5-2% of the mean reported result as can be seen from Table 3 For all our baselines, we use a constant batch-size of 10 samples from the streaming task. This batch is augmented with 10 samples from the replay buffer for the replay-based approaches. La-MAML and its variants split the batch from the streaming task into a sequence of smaller disjoint sets to take multiple (k = 10 for MNIST and k = 5 for CIFAR100/TinyImagenet) gradient steps in the inner-loop. In MER, each sample from the incoming task is augmented with a batch of 10 replay 16(a) C-MAML: Modulation of αand β  (b) La-MAML: Modulation of α0 and η Figure 6: Plots of Retained Accuracy (RA) across hyper-parameter variation for C-MAML and La-MAML. We show results of the grid search over the learning rate hyperparameters. RA decreases from red to blue. All the hyperparameters are varied between [0.001, 0.3], with the axes being in log-scale. Figure 7: Retained Accuracy vs Running time (seconds) for La-MAML vs other baselines on the CIFAR100 dataset. samples to form the batch used for the meta-update. We found very small performance gaps between the ﬁrst and second-order versions of our proposed variants with performance differences in the range of 1-2% for RA. This is in line with the observation that deep neural networks have near-zero hessians since the ReLU non-linearity is linear almost everywhere [23]. MNIST Benchmarks: On the MNIST continual learning benchmarks, images of size 28x28 are ﬂattened to create a 1x784 array. This array is passed on to a fully-connected neural network having two layers with 100 nodes each. Each layer uses ReLU non-linearity. The output layer uses a single head with 10 nodes corresponding to the 10 classes. In all our experiments, we use a modest replay buffer of size 200 for MNIST Rotations and Permutation and size 500 for Many Permutations. Real-world visual classiﬁcation: For CIFAR and TinyImageNet we used a CNN having 3 and 4 conv layers respectively with 160 3x3 ﬁlters. The output from the ﬁnal convolution layer is ﬂattened and is passed through 2 fully connected layers having 320 and 640 units respectively. All the layers are succeeded by ReLU nonlinearity. Finally, a multi-headed output layer is used for performing 5-way classiﬁcation for every task. This architecture is used in prior meta-learning work [30]. For CIFAR and TinyImagenet, we allow a replay buffer of size 200 and 400 respectively which implies that each class in these dataset gets roughly about 1-2 samples in the buffer. For TinyImagenet, we split the validation set into val and test splits, since the labels in the actual test set are not released. 17Table 5: Final hyperparameters for all compared approaches on the CIFAR and TinyImagenet benchmarks METHOD PARAMETER CIFAR-100 T INY IMAGENET SINGLE MULTIPLE SINGLE MULTIPLE ER LR 0.03 0.03 0.1 0.1 Epochs/Glances 10 10 10 10 IID LR - 0.03 - 0.01 Epochs/Glances - 50 - 50 ICARL LR 0.03 0.03 0.01 0.01 Epochs/Glances 2 10 2 10 GEM LR 0.03 0.03 0.03 0.03 Epochs/Glances 2 10 2 10 AGEM LR 0.03 0.03 0.01 0.01 Epochs/Glances 2 10 2 10 MER LR α 0.1 - 0.1 - LR β 0.1 - 0.1 - LR γ 1 - 1 - Epochs/Glances 10 - 10 - META-BGD η 50 50 50 - std-init 0.02 0.02 0.02 - βinner 0.1 0.1 0.1 - mc-iters 2 2 2 - Epochs/Glances 3 10 3 - C-MAML α 0.03 0.03 0.03 0.03 β 0.03 0.03 0.03 0.03 Epochs/Glances 5 10 2 10 LA-ER α0 0.1 0.1 0.03 0.03 η 0.1 0.1 0.1 0.1 Epochs/Glances 1 10 2 10 SYNC LA-MAML α0 0.1 0.1 0.075 0.075 β 0.1 0.1 0.075 0.075 η 0.3 0.3 0.25 0.25 Epochs/Glances 5 10 2 10 LA-MAML α0 0.1 0.1 0.1 0.1 η 0.3 0.3 0.3 0.3 Epochs/Glances 10 10 2 10 H Baselines On the MNIST benchmarks, we compare our algorithm against the baselines used in [22], which are as follows: • Online: A baseline for the LLL setup, where a single network is trained one example at a time with SGD. • EWC [14]: Elastic Weight Consolidation is a regularisation based method which constraints the weights important for the previous tasks to avoid catastrophic forgetting. • GEM [17]: Gradient Episodic Memory does constrained optimisation by solving a quadratic program on the gradients of new and replay samples, trying to make sure that these gradients do not alter the past tasks’ knowledge. • MER [22]: Meta Experience Replay samples i.i.d data from a replay memory to meta-learn model parameters that show increased gradient alignment between old and current samples. We evaluate against this baseline only in the LLL setups. On the real-world visual classiﬁcation dataset, we carry out experiments on GEM, MER along with:- 18Table 6: Final hyperparameters used for our variants on the MNIST benchmarks METHOD PARAMETER PERMUTATIONS ROTATIONS MANY C-MAML α 0.03 0.1 0.03 β 0.1 0.1 0.15 Glances 5 5 5 SYNC LA-MAML α0 0.15 0.15 0.03 β 0.1 0.3 0.03 η 0.1 0.1 0.1 Glances 5 5 10 LA-MAML α0 0.3 0.3 0.1 η 0.15 0.15 0.1 Glances 5 5 10 • IID: Network gets the data from all tasks in an independent and identically distributed manner, thus bypassing the issue of catastrophic forgetting completely. Therefore, IID acts as an upper bound for the RA achievable with this network. • ER: Experience Replay uses a small replay buffer to store old data using reservoir sampling. This stored data is then replayed again along with the new data samples. • iCARL [21]: iCARL is originally from the family of class incremental learners, which learns to classify images in the metric space. It prevents catastrophic forgetting by using a memory of exemplar samples to perform distillation from the old network weights. Since we perform experiments in a task incremental setting, we use the modiﬁed version of iCARL (as used by GEM [17]), where distillation loss is calculated only over the logits of the particular task. • A-GEM [6]: Averaged Gradient Episodic Memory proposed to project gradients of the new task to a direction such as to avoid interference with respect to the average gradient of the old samples in the buffer. • Meta-BGD: Bayesian Gradient Descent [32] proposes training a bayesian neural network for CL where the learning rate for the parameters (the means) are derived from their variances. We construct this baseline by equipping C-MAML with bayesian training, where each parameter in θis now sampled from a gaussian distribution with a certain mean and variance. The inner-loop stays same as C-MAML(constant LR), but the magnitude of the meta-update to the parameters in θis now inﬂuenced by their associated variances. The variance updates themselves have a closed form expression which depends on mmonte-carlo samples of the meta-loss, thus implying mforward passes of the inner-and-outer loops (each time with a newly sampled θ) to get mmeta-gradients. I Discussion on Prior Work In Table 7, we provide a comparative overview of various continual learning methods to situate our work better in the context of prior work. Prior-focused methods face model capacity saturation as the number of tasks increase. These methods freeze weights to defy forgetting, and so penalise changes to the weights, even if those changes could potentially improve model performance on old tasks. They are also not suitable for the LLL setup (section 5), since it requires many passes through the data for every task to learn weights that are optimal enough to be frozen. Additionally, the success of weight freezing schemes can be attributed to over-parameterisation in neural networks, leading to sub-networks with sufﬁcient capacity to learn separate tasks. However continual-learning setups are often motivated in resource-constrained settings requiring efﬁciency and scalability. Therefore solutions that allow light-weight continual learners are desirable. Meta-learning algorithms are able to exploit even small models to learn a good initialization where gradients are aligned across tasks, enabling shared progress on optimisation of task-wise objectives. Our method additionally allows meta-learning to also achieve a prior-focusing affect through the async-meta-update, without necessarily needing over-parameterised models. 19In terms of resources, meta-learning based methods require smaller replay memories than traditional methods because they learn to generalise better across and within tasks, thus being sample-efﬁcient. Our learnable learning rates incur a memory overhead equal to the parameters of the network. This is comparable to or less than many prior-based methods that store between 1 to T scalars per parameter depending on the approach (T is the number of tasks). It should be noted that our learning rate modulation involves clipping updates for parameters with non-aligning gradients. In this aspect, it is related to methods like GEM and AGEM mentioned before. Where the distinction lies, is that our method takes some of the burden off of the clipping, by ensuring that gradients are more aligned in the ﬁrst place. This means that there should be less interference and therefore less clipping of updates deemed essential for learning new tasks, on the whole. Table 7: Setups in prior work: We describe the setups and assumptions adopted by prior work, focusing on approaches relevant to our method. FWT and BWT refer to forward and backward transfer as deﬁned in [17]. ’-’ refers to no inductive bias for or against the speciﬁc property. Saturation of capacity refers to reduced network plasticity due to weight change penalties gradually making further learning impossible. The LLL setup is deﬁned in Section 5. <and >with replay indicate that a method’s replay requirements are lesser or more compared to other methods in the table. Fishers refers to the Fisher Information Matrix (FIM) computed per task. Each FIM has storage equal to that of the model parameters. Approaches using Bayesian Neural Networks require twice as many parameters (as does La-MAML) to store the mean and variance estimates per parameter. APPROACH TRANSFER CAPACITY RESOURCES ALGORITHM FWT BWT SATURATES LLL S TORAGE PRIOR-FOCUSED - × √ × T FISHERS EWC [14] PRIOR FOCUSED - × √ × T MASKS HAT [26] PRIOR FOCUSED - × √ √ 2X PARAMS BGD/UCB [32] [8] REPLAY - - × √ >REPLAY ICARL [21] REPLAY - - × √ >REPLAY GEM [17] META + REPLAY √ √ × √ REPLAY MER [22] META + REPLAY √ √ × √ REPLAY OURS 20",
      "meta_data": {
        "arxiv_id": "2007.13904v2",
        "authors": [
          "Gunshi Gupta",
          "Karmesh Yadav",
          "Liam Paull"
        ],
        "published_date": "2020-07-27T23:07:01Z",
        "pdf_url": "https://arxiv.org/pdf/2007.13904v2.pdf",
        "github_url": "https://github.com/montrealrobotics/La-MAML"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of catastrophic forgetting in continual learning (CL) for models with limited capacity, especially in online, sequential task environments. It proposes Look-ahead MAML (La-MAML), a fast optimization-based meta-learning algorithm for online-continual learning, enhanced by a small episodic memory. La-MAML introduces modulation of per-parameter learning rates (LRs) in its meta-learning update, drawing connections to hypergradients and meta-descent, which provides a flexible and efficient way to mitigate catastrophic forgetting compared to prior-based methods. The base algorithm, Continual-MAML (C-MAML), utilizes a replay-buffer and an online meta-objective that aligns gradients between current and past tasks. La-MAML demonstrates superior performance over other replay-based, prior-based, and meta-learning based CL approaches on various visual classification benchmarks, achieving comparable results to state-of-the-art methods like MER in significantly less training time (under 20%).",
        "methodology": "The core methodology revolves around optimization-based meta-learning, building upon the Model-Agnostic Meta-Learning (MAML) and Online-aware Meta-Learning (OML) objectives. The base algorithm, C-MAML, optimizes an online OML objective (Eq. 5) to minimize cumulative risk across tasks. This objective's k-step MAML update is shown to be equivalent to an asymmetric CL objective (Eq. 6) that aligns the current task's gradients with the average gradient of previous tasks, offering a speedup over MER. La-MAML extends C-MAML by incorporating learnable, per-parameter LRs into the inner updates (Algorithm 1). These LRs are updated asynchronously in the meta-update, with their gradients (gMAML(αj)) reflecting the alignment between old and new tasks (Eq. 8). The LRs are clipped to positive values to prevent ascending gradients and mitigate catastrophic forgetting. This asynchronous update mechanism allows the model to conservatively modulate the pace and direction of learning, thereby accelerating progress on new tasks while facilitating knowledge transfer from old ones. The approach integrates concepts from replay-based and prior-based methods by using a replay buffer and data-driven LR modulation.",
        "experimental_setup": "Experiments were conducted on both toy and real-world visual classification benchmarks. Toy benchmarks included MNIST Rotations, MNIST Permutations (20 tasks each, 1000 samples/task), and Many Permutations (100 tasks, 200 images/task). Real-world benchmarks involved CIFAR-100 (20 tasks, disjoint 5-way classification) and TinyImagenet-200 (40 tasks, 5-way classification). Experiments were performed in both task-agnostic (MNIST, single-headed model) and task-aware (CIFAR, TinyImagenet, multi-headed model) settings. Two primary setups were evaluated: 'Single-Pass' (Efficient Lifelong Learning - LLL), where data is processed once, and 'Multiple-Pass' (standard CL), allowing up to 10 epochs. Performance was measured using Retained Accuracy (RA) and Backward-Transfer and Interference (BTI). The architectures included a fully-connected network for MNIST (2 layers, 100 nodes) and CNNs for CIFAR/TinyImagenet (3-4 conv layers, 2 FC layers). A modest replay buffer was used (200-500 samples for MNIST, 200-400 for CIFAR/TinyImagenet). SGD optimizer with gradient clipping (norm 2.0) was used, with hyperparameters tuned via grid-search. Baselines included Online, EWC, GEM, MER, IID, ER, iCARL, A-GEM, and Meta-BGD, along with ablations C-MAML, LA-ER, and Sync-La-MAML.",
        "limitations": "The algorithm inherently faces the potential for concept drift, as it stores and replays random samples, even with improved gradient alignment. Any bias in the sampling strategy itself could influence the remembered data distribution and model performance. The equivalent CL objective (Eq. 11) is noted to introduce some bias, meaning parameters do not precisely converge to the minimizer of losses across all tasks, though this is deemed acceptable in the continual learning context. In comparison to baselines, Meta-BGD was found to be highly sensitive to hyperparameters and computationally expensive. Additionally, the learnable per-parameter learning rates introduce a memory overhead equivalent to the network's parameters.",
        "future_research_directions": "Future research could focus on developing and analyzing improved optimizers specifically tailored for continual learning, as current standard optimizers (e.g., Adam) are primarily designed for stationary supervised learning. Another promising direction involves further exploring the connections to meta-descent. This could lead to more stable meta-learning training procedures that can automatically adjust hyperparameters dynamically based on ongoing training dynamics, reducing the need for extensive manual tuning.",
        "experimental_code": "# From model/meta/learner.py\nclass Learner(nn.Module):\n    def define_task_lr_params(self, alpha_init=1e-3): \n        self.alpha_lr = nn.ParameterList([])\n        for p in self.parameters():\n            self.alpha_lr.append(nn.Parameter(alpha_init * torch.ones(p.shape, requires_grad=True)))\n\n# From model/lamaml_base.py\nclass BaseNet(torch.nn.Module):\n    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n        super(BaseNet, self).__init__()\n        self.args = args\n        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [args.n_hiddens] * args.n_layers + [n_outputs],\n                                                dataset = args.dataset, args=args)\n        self.net = Learner.Learner(config, args)\n        self.net.define_task_lr_params(alpha_init = args.alpha_init)\n        self.opt_wt = torch.optim.SGD(list(self.net.parameters()), lr=args.opt_wt)     \n        self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr) \n        self.loss = torch.nn.CrossEntropyLoss()\n        self.memories = args.memories\n        self.M = [] # Main replay buffer\n        self.M_new = [] # Temporary buffer for current task\n        self.age = 0 # Counter for reservoir sampling\n        self.current_task = 0\n        # ... (other initializations)\n\n    def push_to_mem(self, batch_x, batch_y, t):\n        if self.real_epoch == 0 and self.pass_itr == 0: # Only push once per task's initial stream\n            batch_x, batch_y, t = batch_x.cpu(), batch_y.cpu(), t.cpu()\n            for i in range(batch_x.shape[0]):\n                self.age += 1\n                if len(self.M_new) < self.memories:\n                    self.M_new.append([batch_x[i], batch_y[i], t])\n                else:\n                    p = random.randint(0, self.age)  \n                    if p < self.memories: self.M_new[p] = [batch_x[i], batch_y[i], t]\n\n    def getBatch(self, x_current, y_current, t_current, batch_size=None):\n        if x_current is not None:\n            mxi = x_current.cpu().numpy()\n            myi = y_current.cpu().numpy()\n            mti = np.full(x_current.shape[0], t_current, dtype=int)\n        else:\n            # Placeholder for correct dimension handling if x_current can be None and needed for empty array shape\n            mxi = np.empty(shape=(0,))\n            myi = np.empty(shape=(0,))\n            mti = np.empty(shape=(0,))\n\n        bxs, bys, bts = [], [], []\n\n        MEM = self.M if self.args.use_old_task_memory and t_current > 0 else self.M_new\n        batch_size = self.batchSize if batch_size is None else self.batchSize\n\n        if len(MEM) > 0:\n            order = list(range(len(MEM)))\n            random.shuffle(order)\n            osize = min(batch_size, len(MEM))\n            for j in range(osize):\n                k = order[j]\n                x_mem, y_mem, t_mem = MEM[k]\n                bxs.append(x_mem.cpu().numpy() if isinstance(x_mem, torch.Tensor) else x_mem)\n                bys.append(y_mem.cpu().numpy() if isinstance(y_mem, torch.Tensor) else y_mem)\n                bts.append(t_mem.cpu().numpy() if isinstance(t_mem, torch.Tensor) else t_mem)\n\n        for j in range(len(myi)):\n            bxs.append(mxi[j])\n            bys.append(myi[j])\n            bts.append(mti[j])\n\n        bxs = Variable(torch.from_numpy(np.array(bxs))).float() \n        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)\n        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)\n        \n        if self.cuda:\n            bxs = bxs.cuda()\n            bys = bys.cuda()\n            bts = bts.cuda()\n        return bxs, bys, bts\n\n    def zero_grads(self):\n        if self.args.learn_lr: self.opt_lr.zero_grad()\n        self.opt_wt.zero_grad()\n        self.net.zero_grad()\n        self.net.alpha_lr.zero_grad()\n\n# From model/lamaml_cifar.py (adapts BaseNet for class-incremental tasks)\nclass Net(BaseNet):\n    def __init__(self, n_inputs, n_outputs, n_tasks, args):\n        super(Net, self).__init__(n_inputs, n_outputs, n_tasks, args)\n        self.nc_per_task = n_outputs / n_tasks # Classes per task for offset calculation\n\n    def compute_offsets(self, task):\n        offset1 = task * self.nc_per_task\n        offset2 = (task + 1) * self.nc_per_task\n        return int(offset1), int(offset2)\n\n    def take_loss(self, t, logits, y):\n        offset1, offset2 = self.compute_offsets(t)\n        loss = self.loss(logits[:, offset1:offset2], y - offset1)\n        return loss\n\n    def take_multitask_loss(self, bt, t_current, logits, y):\n        loss = 0.0\n        for i, ti in enumerate(bt):\n            offset1, offset2 = self.compute_offsets(ti)\n            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0) - offset1)\n        return loss / len(bt)\n\n    def inner_update(self, x, fast_weights, y, t):\n        offset1, offset2 = self.compute_offsets(t)            \n        logits = self.net.forward(x, fast_weights)[:, :offset2] # Forward pass with current/fast weights\n        loss = self.take_loss(t, logits, y) # Calculate inner loss\n\n        if fast_weights is None: fast_weights = self.net.parameters()\n\n        graph_required = self.args.second_order\n        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))\n        \n        for i in range(len(grads)):\n            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)\n        \n        fast_weights = list(map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))\n        return fast_weights\n\n    def meta_loss(self, x, fast_weights, y, bt, t_current):\n        offset1, offset2 = self.compute_offsets(t_current)\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss_q = self.take_multitask_loss(bt, t_current, logits, y)\n        return loss_q, logits\n\n    def observe(self, x, y, t):\n        self.net.train() \n        for pass_itr in range(self.glances): # 'glances' for single-pass setting\n            self.pass_itr = pass_itr\n            perm = torch.randperm(x.size(0)); x = x[perm]; y = y[perm]\n            \n            self.epoch += 1; self.zero_grads()\n            if t != self.current_task: # If new task, update main memory from new task's samples\n                self.M = self.M_new.copy()\n                self.current_task = t\n\n            batch_sz = x.shape[0]; n_batches = self.args.cifar_batches # n_batches for inner trajectory steps\n            rough_sz = math.ceil(batch_sz/n_batches); fast_weights = None; meta_losses = [0 for _ in range(n_batches)]\n\n            bx, by, bt = self.getBatch(x, y, t)             \n\n            # Inner loop trajectory\n            for i in range(n_batches):\n                batch_x = x[i*rough_sz : (i+1)*rough_sz]; batch_y = y[i*rough_sz : (i+1)*rough_sz]\n                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)   \n                \n                if self.real_epoch == 0: self.push_to_mem(batch_x, batch_y, torch.tensor(t))\n                \n                meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) \n                meta_losses[i] += meta_loss\n\n            self.zero_grads() # Zero all gradients before meta-update\n            meta_loss = sum(meta_losses)/len(meta_losses); meta_loss.backward() # Backpropagate meta-loss\n            \n            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)\n            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)\n            \n            if self.args.learn_lr: self.opt_lr.step()\n\n            if self.args.sync_update:\n                self.opt_wt.step() # Synchronous update using opt_wt\n            else:            \n                # Asynchronous update: apply gradients using clipped (ReLU) learnable LRs as step sizes\n                for i,p in enumerate(self.net.parameters()):          \n                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])            \n            self.net.zero_grad(); self.net.alpha_lr.zero_grad() # Clear gradients\n        return meta_loss.item()",
        "experimental_info": "The La-MAML method is evaluated on diverse datasets including 'mnist_rotations', 'cifar100', and 'tinyimagenet'. Data loading is handled by 'task_incremental_loader' or 'class_incremental_loader'. For image datasets, specific transformations are applied, such as `RandomCrop`, `RandomHorizontalFlip`, `ColorJitter` (for CIFAR-100), `ToTensor`, and `Normalize` (with dataset-specific mean/std values).\n\nKey experimental settings are configured via command-line arguments:\n- **Model Architecture**: Defined by `--arch` (e.g., 'linear', 'pc_cnn'), `--n_hiddens` (default 100), and `--n_layers` (default 2). Xavier initialization (`--xav_init`) can be enabled.\n- **Optimization Parameters**:\n    - `glances`: Number of training iterations over a batch in the single-pass setting (default 1).\n    - `n_epochs`: Training epochs per task (default 1).\n    - `batch_size`: Size of the incoming data batch (default 1).\n    - `replay_batch_size`: Size of the batch sampled from the experience replay buffer (default 20).\n    - `memories`: Maximum capacity of the reservoir sampling-based replay buffer (default 5120 samples).\n    - `opt_lr`: Learning rate for optimizing the per-parameter learnable learning rates (default 1e-1).\n    - `opt_wt`: Learning rate for updating the network's weights, applicable when `--sync_update` is active (default 1e-1).\n    - `alpha_init`: Initial value for the learnable per-parameter learning rates (default 1e-3).\n    - `learn_lr`: A boolean flag (default False) to enable the optimization of per-parameter learning rates. It is essential for La-MAML.\n    - `sync_update`: A boolean flag (default False) to choose between synchronous (optimizer updates both LRs and weights) or asynchronous (weights are updated manually using ReLU-clipped learnable LRs) update mechanisms.\n    - `grad_clip_norm`: The maximum L2 norm for gradient clipping (default 2.0).\n    - `second_order`: A boolean flag (default False) to enable second-order MAML updates.\n    - `cifar_batches`: The number of mini-batches within the inner update trajectory for CIFAR-like datasets (default 3).\n    - `use_old_task_memory`: A boolean flag (default False) to determine if the replay buffer should store samples from all previously encountered tasks.\n- **Data Handling**:\n    - `data_path`: Specifies the base directory where datasets are stored.\n    - `samples_per_task`: The number of training samples considered per task (default -1, implying all available samples).\n    - `class_order`: Strategy for presenting classes within tasks ('random', 'chrono', 'old', 'super').\n    - `increment`: Number of classes added in each incremental task step for class-incremental learning (default 5).\n    - `validation`: Proportion of the training data allocated for validation (default 0.0).\n- **Evaluation**:\n    - `log_every`: Frequency (in minibatches) for logging and evaluating validation accuracy (default 1000).\n    - `calc_test_accuracy`: A boolean flag (default False) to enable calculation of test accuracy alongside validation accuracy.\n    - `test_batch_size`: Batch size used during model evaluation (default 100000)."
      }
    },
    {
      "title": "Look-ahead Meta Learning for Continual Learning",
      "abstract": "The continual learning problem involves training models with limited capacity\nto perform well on a set of an unknown number of sequentially arriving tasks.\nWhile meta-learning shows great potential for reducing interference between old\nand new tasks, the current training procedures tend to be either slow or\noffline, and sensitive to many hyper-parameters. In this work, we propose\nLook-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm\nfor online-continual learning, aided by a small episodic memory. Our proposed\nmodulation of per-parameter learning rates in our meta-learning update allows\nus to draw connections to prior work on hypergradients and meta-descent. This\nprovides a more flexible and efficient way to mitigate catastrophic forgetting\ncompared to conventional prior-based methods. La-MAML achieves performance\nsuperior to other replay-based, prior-based and meta-learning based approaches\nfor continual learning on real-world visual classification benchmarks. Source\ncode can be found here: https://github.com/montrealrobotics/La-MAML",
      "full_text": "La-MAML: Look-ahead Meta Learning for Continual Learning Gunshi Gupta ∗ Mila, UdeM guptagun@mila.quebec Karmesh Yadav * Carnegie Mellon University karmeshy@andrew.cmu.edu Liam Paull Mila, UdeM paulll@iro.umontreal.ca Abstract The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or ofﬂine, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online- continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more ﬂexible and efﬁcient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classiﬁcation benchmarks. 1 Introduction Embodied or interactive agents that accumulate knowledge and skills over time must possess the ability to continually learn. Catastrophic forgetting [11, 18], one of the biggest challenges in this setup, can occur when the i.i.d. sampling conditions required by stochastic gradient descent (SGD) are violated as the data belonging to different tasks to be learnt arrives sequentially. Algorithms for continual learning (CL) must also use their limited model capacity efﬁciently since the number of future tasks is unknown. Ensuring gradient-alignment across tasks is therefore essential, to make shared progress on their objectives. Gradient Episodic Memory (GEM) [17] investigated the connection between weight sharing and forgetting in CL and developed an algorithm that explicitly tried to minimise gradient interference. This is an objective that meta-learning algorithms implicitly optimise for (refer to [20] for derivations of the effective parameter update made in ﬁrst and second order meta learning algorithms). Meta Experience Replay (MER) [22] formalized the transfer- interference trade-off and showed that the gradient alignment objective of GEM coincide with the objective optimised by the ﬁrst order meta-learning algorithm Reptile [20]. Besides aligning gradients, meta-learning algorithms show promise for CL since they can directly use the meta-objective to inﬂuence model optimisation and improve on auxiliary objectives like generalisation or transfer. This avoids having to deﬁne heuristic incentives like sparsity [ 15] for better CL. The downside is that they are usually slow and hard to tune, effectively rendering them more suitable for ofﬂine continual learning [12, 22]. In this work, we overcome these difﬁculties and develop a gradient-based meta-learning algorithm for efﬁcient, online continual learning. We ﬁrst propose a base algorithm for continual meta-learning referred to as Continual-MAML (C-MAML) that utilizes a replay-buffer and optimizes a meta-objective that mitigates forgetting. Subsequently, ∗equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2007.13904v2  [cs.LG]  12 Nov 2020we propose a modiﬁcation to C-MAML, named La-MAML, which incorporates modulation of per- parameter learning rates (LRs) to pace the learning of a model across tasks and time. Finally, we show that the algorithm is scalable, robust and achieves favourable performance on several benchmarks of varying complexity. 2 Related work Relevant CL approaches can be roughly categorized intoreplay-based, regularisation (or prior-based) and meta-learning-based approaches. In order to circumvent the issue of catastrophic forgetting,replay-based methods maintain a collection of samples from previous tasks in memory. Approaches utilising an episodic-buffer [5, 21] uniformly sample old data points to mimic the i.i.d. setup within continual learning. Generative-replay [27] trains generative models to be able to replay past samples, with scalability concerns arising from the difﬁculty of modeling complex non-stationary distributions. GEM [17] and A-GEM [6] take memory samples into account to determine altered low-interference gradients for updating parameters. Regularisation-based methods avoid using replay at all by constraining the network weights according to heuristics intended to ensure that performance on previous tasks is preserved. This involves penal- ising changes to weights deemed important for old tasks [14] or enforcing weight or representational sparsity [3] to ensure that only a subset of neurons remain active at any point of time. The latter method has been shown to reduce the possibility of catastrophic interference across tasks [15, 26]. Meta-Learning-based approaches are fairly recent and have shown impressive results on small benchmarks like Omniglot and MNIST. MER [22], inspired by GEM[17], utilises replay to incentivise alignment of gradients between old and new tasks. Online-aware Meta Learning (OML) [ 12] introduces a meta-objective for a pre-training algorithm to learn an optimal representation ofﬂine, which is subsequently frozen and used for CL. [2, 10, 19] investigate orthogonal setups in which a learning agent uses all previously seen data to adapt quickly to an incoming stream of data, thereby ignoring the problem of catastrophic forgetting. Our motivation lies in developing a scalable, online algorithm capable of learning from limited cycles through streaming data with reduced interference on old samples. In the following sections, we review background concepts and outline our proposed algorithm. We also note interesting connections to prior work not directly pertaining to CL. 3 Preliminaries We consider a setting where a sequence of T tasks [τ1,τ2,..τT] is learnt by observing their training data [D1,D2,..DT] sequentially. We deﬁne Xi,Y i = {(xi n,yi n)}Ni n=0 as the set of Ni input-label pairs randomly drawn from Di. An any time-step jduring online learning, we aim to minimize the empirical risk of the model on all the ttasks seen so far (τ1:t), given limited access to data (Xi,Y i) from previous tasks τi (i<t ). We refer to this objective as the cumulative risk, given by: t∑ i=1 E(Xi,Yi) [ ℓi ( fi ( Xi; θ ) ,Y i)] = E(X1:t,Y1:t) [ Lt ( f ( X1:t; θ ) ,Y 1:t)] (1) where ℓi is the loss on τi and fi is a learnt, possibly task-speciﬁc mapping from inputs to outputs using parameters θj 0. Lt = ∑t i=1 ℓi is the sum of all task-wise losses for tasks τ1:t where tgoes from 1 to T. Let ℓdenote some loss objective to be minimised. Then the SGD operator acting on parameters θj 0, denoted by U(θj 0) is deﬁned as: U ( θj 0 ) = θj 1 = θj 0 −α∇θj 0 ℓ(θj 0) = θj 0 −αgj 0 (2) where gj 0 = ∇θj 0 ℓ(θj 0). U can be composed for kupdates as Uk ( θj 0 ) = U...◦U ◦U(θj 0) = θj k. α is a scalar or a vector LR. U(·,x) implies gradient updates are made on data sample x. We now introduce the MAML [9] and OML [12] algorithms, that we build upon in Section 4. Model-Agnostic Meta-Learning (MAML) : Meta-learning [ 24], or learning-to-learn [29] has emerged as a popular approach for training models amenable to fast adaptation on limited data. 2MAML [9] proposed optimising model parameters to learn a set of tasks while improving on auxil- iary objectives like few-shot generalisation within the task distributions. We review some common terminology used in gradient-based meta-learning: 1) at a given time-step jduring training, model parameters θj 0 (or θ0 for simplicity), are often referred to as aninitialisation, since the aim is to ﬁnd an ideal starting point for few-shot gradient-based adaptation on unseen data. 2) Fast or inner-updates, refer to gradient-based updates made to a copy of θ0, optimising some inner objective (in this case, ℓi for some τi). 3) A meta-update involves the trajectory of fast updates from θ0 to θk, followed by making a permanent gradient update (or slow-update) to θ0. This slow-update is computed by evalu- ating an auxiliary objective (or meta-loss Lmeta) on θk, and differentiating through the trajectory to obtain ∇θ0 Lmeta(θk). MAML thus optimises θj 0 at time j, to perform optimally on tasks in {τ1:t} after undergoing a few gradient updates on their samples. It optimises in every meta-update, the objective: min θj 0 Eτ1:t [ Lmeta ( Uk(θj 0) )] = min θj 0 Eτ1:t [ Lmeta(θj k) ] . (3) Equivalence of Meta-Learning and CL Objectives : The approximate equivalence of ﬁrst and second-order meta-learning algorithms like Reptile and MAML was shown in [20]. MER [22] then showed that their CL objective of minimising loss on and aligning gradients between a set of tasks τ1:t seen till any time j(on the left), can be optimised by the Reptile objective (on the right), ie. : min θj 0   t∑ i=1 ( ℓi(θj 0) ) −α ∑ p,q≤t   ∂ℓp ( θj 0 ) ∂θj 0 · ∂ℓq ( θj 0 ) ∂θj 0    = min θj 0 Eτ1:t [ Lt ( Uk(θj 0) )] (4) where the meta-loss Lt = ∑t i=1 ℓi is evaluated on samples from tasks τ1:t. This implies that the procedure to meta-learn an initialisation coincides with learning optimal parameters for CL. Online-aware Meta-Learning (OML) : [12] proposed to meta-learn a Representation-Learning Network (RLN) to provide a representation suitable for CL to a Task-Learning Network (TLN). The RLN’s representation is learnt in anofﬂine phase, where it is trained using catastrophic forgetting as the learning signal. Data from a ﬁxed set of tasks (τval), is repeatedly used to evaluate the RLN and TLN as the TLN undergoes temporally correlated updates. In every meta-update’s inner loop, the TLN undergoes fast updates on streaming task data with a frozen RLN. The RLN and updated TLN are then evaluated through a meta-loss computed on data from τval along with the current task. This tests how the performance of the model has changed on τval in the process of trying to learn the streaming task. The meta-loss is then differentiated to get gradients for slow updates to the TLN and RLN. This composition of two losses to simulate CL in the inner loop and test forgetting in the outer loop, is referred to as the OML objective. The RLN learns to eventually provide a better representation to the TLN for CL, one which is shown to have emergent sparsity. 4 Proposed approach In the previous section, we saw that the OML objective can directly regulate CL behaviour, and that MER exploits the approximate equivalence of meta-learning and CL objectives. We noted that OML trains a static representation ofﬂine and that MER’s algorithm is prohibitively slow. We show that optimising the OML objective online through a multi-step MAML procedure is equivalent to a more sample-efﬁcient CL objective. In this section, we describe Continual-MAML (C-MAML), the base algorithm that we propose for online continual learning. We then detail an extension to C-MAML, referred to as Look-Ahead MAML (La-MAML), outlined in Algorithm 1. 4.1 C-MAML C-MAML aims to optimise the OML objective online, so that learning on the current task doesn’t lead to forgetting on previously seen tasks. We deﬁne this objective, adapted to optimise a model’s parameters θinstead of a representation at time-step j, as: min θj 0 OML(θj 0,t) = min θj 0 ∑ Sj k∼Dt [ Lt ( Uk(θj 0,Sj k) )] (5) 3where Sj k is a stream of kdata tuples ( Xt j+l,Y t j+l )k l=1 from the current task τt that is seen by the model at time j. The meta-loss Lt = ∑t i=1 ℓi is evaluated on θj k = Uk(θj 0,Sj k). It evaluates the ﬁtness of θj k for the continual learning prediction task deﬁned in Eq. 1 until τt. We omit the implied data argument (xi,yi) ∼(Xi,Y i) that is the input to each loss ℓi in Lt for any task τi. We will show in Appendix B that optimising our objective in Eq. 5 through the k-step MAML update in C-MAML also coincides with optimising the CL objective of AGEM [6]: min θj 0 Eτ1:t [ Lt ( Uk(θj 0) )] = min θj 0 t∑ i=1  ℓi(θj 0) −α ∂ℓi ( θj 0 ) ∂θj 0 · ∂ℓt ( θj 0 ) ∂θj 0  . (6) This differs from Eq. 4’s objective by beingasymmetric: it focuses on aligning the gradients of τt and the average gradient of τ1:t instead of aligning all the pair-wise gradients between tasks τ1:t. In Appendix D, we show empirically that gradient alignment amongst old tasks doesn’t degrade while a new task is learnt, avoiding the need to repeatedly optimise the inter-task alignment between them. This results in a drastic speedup over MER’s objective (Eq. 4) which tries to align allτ1:t equally, thus resampling incoming samples s∼τt to form a uniformly distributed batch over τ1:t. Since each sthen has 1 t-th the contribution in gradient updates, it becomes necessary for MER to take multiple passes over many such uniform batches including s. Figure 1: The proposed La-MAML algorithm: For every batch of data, the initial weights undergo a series of k fast updates to obtain θj k (here j = 0), which is evaluated against a meta-loss to backpropagate gradients with respect to the weights θ0 0 and LRs α0. First α0 is updated to α1 which is then used to update θ0 0 to θ1 0 The blue boxes indicate fast weights while the green boxes indicate gradients for the slow updates. LRs and weights are updated in an asynchronous manner. During training, a replay-buffer Ris populated through reservoir samplingon the incoming data stream as in [ 22]. At the start of every meta- update, a batch bis sampled from the current task. bis also combined with a batch sampled from Rto form themeta-batch, bm, representing samples from both old and new tasks. θj 0 is updated through kSGD-based inner-updates by seeing the current task’s samples frombone at a time. The outer-loss or meta-loss Lt(θj k) is evaluated on bm. It indicates the performance of parameters θj k on all the tasks τ1:t seen till time j. The complete training procedure is described in Appendix C. 4.2 La-MAML Despite the fact that meta-learning incentivises the alignment of within-task and across-task gra- dients, there can still be some interference be- tween the gradients of old and new tasks, τ1:t−1 and τt respectively. This would lead to forget- ting on τ1:t−1, since its data is no longer fully available to us. This is especially true at the beginning of training a new task, when its gradients aren’t necessarily aligned with the old ones. A mechanism is thus needed to ensure that meta-updates are conservative with respect to τ1:t−1, so as to avoid negative transfer on them. The magnitude and direction of the meta-update needs to be regulated, guided by how the loss on τ1:t−1 would be affected by the update. We propose Lookahead-MAML (La-MAML), where we include a set of learnable per-parameter learning rates (LRs) to be used in the inner updates, as depicted in Figure 1. This is motivated by our observation that the expression for the gradient of Eq. 5 with respect to the inner loop’s LRs directly reﬂects the alignment between the old and new tasks. The augmented learning objective is deﬁned as min θj 0,αj ∑ Sj k∼Dt [ Lt ( Uk ( αj,θj 0,Sj k ))] , (7) 4and the gradient of this objective at timej, with respect to the LR vectorαj (denoted as gMAML(αj)) is then given as: gMAML(αj) = ∂ ∂αjLt ( θj k ) = ∂ ∂θj k Lt ( θj k ) · ( − k−1∑ k′=0 ∂ ∂θj k′ ℓt ( θj k′ )) . (8) We provide the full derivation in the Appendix A, and simply state the expression for a ﬁrst-order approximation [9] of gMAML(α) here. The ﬁrst term in gMAML(α) corresponds to the gradient of the meta-loss on batch bm: gmeta. The second term indicates the cumulative gradient from the inner-updates: gtraj. This expression indicates that the gradient of the LRs will be negative when the inner product between gmeta and gtraj is high, ie. the two are aligned; zero when the two are orthogonal (not interfering) and positive when there is interference between the two. Negative (positive) LR gradients would pull up (down) the LR magnitude. We depict this visually in Figure 2. Algorithm 1 La-MAML : Look-ahead MAML Input: Network weights θ, LRs α, inner objective ℓ, meta objective L, learning rate for α: η j ←0, R←{} ⊿Initialise Replay Buffer for t:= 1 to T do for ep:= 1 to numepochs do for batch bin (Xt,Y t) ∼Dt do k←sizeof(b) bm ←Sample(R) ∪b for n= 0 to k−1 do Push b[k′] to R with reservoir sampling θj k′+1 ←θj k′ −αj ·∇θj k′ end for αj+1 ←αj −η∇αj Lt(θj k,bm) (a) θj+1 0 ←θj 0 −max(0,αj+1) ·∇θj 0 Lt(θj k,bm) (b) j ←j+ 1 end for end for end for Figure 2: Different scenarios for the alignment of gtraj (blue dashed line) and gmeta, going from interference (left) to alignment (right). Yellow ar- rows denote the inner updates. The LR αincreases (decreases) when gradients align (interfere). We propose updating the network weights and LRs asynchronously in the meta-update. Let αj+1 be the updated LR vector obtained by taking an SGD step with the LR gradient from Eq. 8 at time j. We then update the weights as: θj+1 0 ←θj 0 −max(0,αj+1) ·∇θj 0 Lt(θj k) (9) where kis the number of steps taken in the inner-loop. The LRs αj+1 are clipped to positive values to avoid ascending the gradient, and also to avoid making interfering parameter-updates, thus mitigating catastrophic forgetting. The meta-objective thus conservatively modulates the pace and direction of learning to achieve quicker learning progress on a new task while facilitating transfer on old tasks. Algorithm 1 2 illustrates this procedure. Lines (a), (b) are the only difference between C-MAML and La-MAML, with C-MAML using a ﬁxed scalar LR αfor the meta-update to θj 0 instead of αj+1. Our meta-learning based algorithm incorporates concepts from both prior-based and replay-based approaches. The LRs modulate the parameter updates in an data-driven manner, guided by the interplay between gradients on the replay samples and the streaming task. However, since LRs evolve with every meta-update, their decay is temporary. This is unlike many prior-based approaches, where penalties on the change in parameters gradually become so high that the network capacity saturates [14]. Learnable LRs can be modulated to high and low values as tasks arrive, thus being a simpler, ﬂexible and elegant way to constrain weights. This asynchronous update resembles trust-region optimisation [31] or look-ahead search since the step-sizes for each parameter are adjusted based on 2The code for our algorithm can be found at: https://github.com/montrealrobotics/La-MAML 5the loss incurred after applying hypothetical updates to them. Our LR update is also analogous to the heuristic uncertainty-based LR update in UCB [8], BGD [32], which we compare to in Section 5.3. 4.3 Connections to Other Work Stochastic Meta-Descent (SMD) : When learning over a non-stationary data distribution, using decaying LR schedules is not common. Strictly diminishing LR schedules aim for closer convergence to a ﬁxed mimima of a stationary distribution, which is at odds with the goal of online learning. It is also not possible to manually tune these schedules since the extent of the data distribution is unknown. However, adaptivity in LRs is still highly desired to adapt to the optimisation landscape, accelerate learning and modulate the degree of adaptation to reduce catastrophic forgetting. Our adaptive LRs can be connected to work on meta-descent [4, 25] in ofﬂine supervised learning (OSL). While several variations of meta-descent exist, the core idea behind them and our approach is gain adaptation. While we adapt the gain based on the correlation between old and new task gradients to make shared progress on all tasks, [4, 25] use the correlation between two successive stochastic gradients to converge faster. We rely on the meta-objective’s differentiability with respect to the LRs, to obtain LR hypergradients automatically. Learning LRs in meta-learning: Meta-SGD [16] proposed learning the LRs in MAML for few-shot learning. Some notable differences between their update and ours exist. They synchronously update the weights and LRs while ourasynchronous update to the LRs serves to carry out a more conservative update to the weights. The intuition for our update stems from the need to mitigate gradient interference and its connection to the transfer-interference trade-off ubiquitous in continual learning. α-MAML [28] analytically updates the two scalar LRs in the MAML update for more adaptive few-shot learning. Our per-parameter LRs are modulated implicitly through back-propagation, to regulate change in parameters based on their alignment across tasks, providing our model with a more powerful degree of adaptability in the CL domain. 5 Experiments In this section, we evaluate La-MAML in settings where the model has to learn a set of sequentially streaming classiﬁcation tasks. Task-agnostic experiments, where the task identity is unknown at training and test-time, are performed on the MNIST benchmarks with a single-headed model. Task- aware experiments with known task identity, are performed on the CIFAR and TinyImagenet [ 1] datasets with a multi-headed model. Similar to [ 22], we use the retained accuracy (RA) metric to compare various approaches. RA is the average accuracy of the model across tasks at the end of training. We also report the backward-transfer and interference (BTI) values which measure the average change in the accuracy of each task from when it was learnt to the end of the last task. A smaller BTI implies lesser forgetting during training. Efﬁcient Lifelong Learning (LLL): Formalized in [6], the setup of efﬁcient lifelong learning assumes that incoming data for every task has to be processed in only one single pass: once processed, data samples are not accessible anymore unless they were added to a replay memory. We evaluate our algorithm on this challenging (Single-Pass) setup as well as the standard (Multiple-Pass) setup, where Table 1: RA, BTI and their standard deviation on MNIST benchmarks. Each experiment is run with 5 seeds. METHOD ROTATIONS PERMUTATIONS MANY RA BTI RA BTI RA BTI ONLINE 53.38 ± 1.53 -5.44 ± 1.70 55.42 ± 0.65 -13.76 ± 1.19 32.62 ± 0.43 -19.06 ± 0.86 EWC 57.96 ± 1.33 -20.42 ± 1.60 62.32 ± 1.34 -13.32 ± 2.24 33.46 ± 0.46 -17.84 ± 1.15 GEM 67.38 ± 1.75 -18.02 ± 1.99 55.42 ± 1.10 -24.42 ± 1.10 32.14 ± 0.50 -23.52 ± 0.87 MER 77.42 ± 0.78 -5.60 ±0.70 73.46 ± 0.45 -9.96 ± 0.45 47.40 ± 0.35 -17.78 ± 0.39 C-MAML 77.33 ± 0.29 -7.88 ± 0.05 74.54 ± 0.54 -10.36 ± 0.14 47.29 ± 1.21 -20.86 ± 0.95 SYNC 74.07 ± 0.58 -6.66 ± 0.44 70.54 ± 1.54 -14.02 ± 2.14 44.48 ± 0.76 -24.18 ± 0.65 LA-MAML 77.42 ± 0.65 -8.64 ± 0.403 74.34 ± 0.67 -7.60 ± 0.51 48.46 ± 0.45 -12.96 ± 0.073 6ideally ofﬂine training-until-convergence is performed for every task, once we have access to the data. 5.1 Continual learning benchmarks Table 2: Running times for MER and La-MAML on MNIST benchmarks for one epoch METHOD ROTATIONS PERMUTATIONS LA-MAML 45.95 ± 0.38 46.13 ± 0.42 MER 218.03 ± 6.44 227.11 ± 12.12 First, we carry out experiments on the toy con- tinual learning benchmarks proposed in prior CL works. MNIST Rotations, introduced in [17], comprises tasks to classify MNIST digits rotated by a different common angle in [0, 180] degrees in each task. In MNIST Permutations, tasks are generated by shufﬂing the image pixels by a ﬁxed random permutation. Unlike Rotations, the input distribution of each task is unrelated here, leading to less positive transfer between tasks. Both MNIST Permutation and MNIST Rotation have 20 tasks with 1000 samples per task. Many Permutations, a more complex version of Per- mutations, has ﬁve times more tasks (100 tasks) and ﬁve times less training data (200 images per task). Experiments are conducted in the low data regime with only 200 samples for Rotation and Permutation and 500 samples for Many, which allows the differences between the various algorithm to become prominent (detailed in Appendix G). We use the same architecture and experimental settings as in MER [22], allowing us to compare directly with their results. We use the cross-entropy loss as the inner and outer objectives during meta-training. Similar to [20], we see improved performance when evaluating and summing the meta-loss at all steps of the inner updates as opposed to just the last one. We compare our method in the Single-Pass setup against multiple baselines including Online, In- dependent, EWC [14], GEM [17] and MER [22] (detailed in Appendix H), as well as different ablations (discussed in Section 5.3). In Table 1, we see that La-MAML achieves comparable or better performance than the baselines on all benchmarks. Table 2 shows that La-MAML matches the performance of MER in less than 20% of the training time, owing to its sample-efﬁcient objective which allows it to make make more learning progress per iteration. This also allows us to scale it to real-world visual recognition problems as described next. 5.2 Real-world classiﬁcation While La-MAML fares well on the MNIST benchmarks, we are interested in understanding its capabilities on more complex visual classiﬁcation benchmarks. We conduct experiments on the CIFAR-100 dataset in a task-incremental manner [ 17] where, 20 tasks comprising of disjoint 5- way classiﬁcation problems are streamed. We also evaluate on the TinyImagenet-200 dataset by partitioning its 200 classes into 40 5-way classiﬁcation tasks. Experiments are carried out in both the Single-Pass and Multiple-Pass settings, where in the latter we allow all CL approaches to train up to a maximum of 10 epochs. Each method is allowed a replay-buffer, containing upto 200 and 400 samples for CIFAR-100 and TinyImagenet respectively. We provide further details about the baselines in Appendix H and about the architectures, evaluation setup and hyper-parameters in Appendix G. Table 3 reports the results of these experiments. We consistently observe superior performance of La-MAML as compared to other CL baselines on both datasets across setups. While the iCARL baseline attains lower BTI in some setups, it achieves that at the cost of much lower performance throughout learning. Among the high-performing approaches, La-MAML has the lowest BTI. Recent work [7, 22] noted that Experience Replay (ER) is often a very strong baseline that closely matches the performance of the proposed algorithms. We highlight the fact that meta-learning and LR modulation combined show an improvement of more than 10 and 18% (as the number of tasks increase from CIFAR to TinyImagenet) over the ER baseline in our case, with limited replay. Overall, we see that our method is robust and better-performing under both the standard and LLL setups of CL which come with different kinds of challenges. Many CL methods [8, 26] are suitable for only one of the two setups. Further, as explained in Figure 3, our model evolves to become resistant to forgetting as training progresses. This means that beyond a point, it can keep making gradient updates on a small window of incoming samples without needing to do meta-updates. 75.3 Evaluation of La-MAML’s learning rate modulation To capture the gains from learning the LRs, we compare La-MAML with our base algorithm, C-MAML. We ablate our choice of updating LRs asynchronously by constructing a version of C-MAML where per-parameter learnable LRs are used in the inner updates while the meta-update still uses a constant scalar LR during training. We refer to it as Sync-La-MAML or Sync since it has synchronously updated LRs that don’t modulate the meta-update. We also construct an ablation referred to as La-ER, where the parameter updates are carried out as in ER but the LRs are modulated using the La-MAML objective’s ﬁrst-order version. This tells us what the gains of LR modulation are over ER, since there is no meta-learning to encourage gradient alignment of the model parameters. While only minor gains are seen on the MNIST benchmarks from asynchronous LR modulation, the performance gap increases as the tasks get harder. On CIFAR-100 and TinyImagenet, we see a trend in the RA of our variants with La-MAML performing best followed by Sync. This shows that optimising the LRs aids learning and our asynchronous update helps in knowledge consolidation by enforcing conservative updates to mitigate interference. To test our LR modulation against an alternative bayesian modulation scheme proposed in BGD [32], we deﬁne a baseline called Meta-BGD where per-parameter variances are modulated instead of LRs. This is described in further detail in Appendix H. Meta-BGD emerges as a strong baseline and matches the performance of C-MAML given enough Monte Carlo iterations m, implying m times more computation than C-MAML. Additionally, Meta-BGD was found to be sensitive to hyperparameters and required extensive tuning. We present a discussion of the robustness of our approach in Appendix E, as well as a discussion of the setups adopted in prior work, in Appendix I. We also compare the gradient alignment of our three variants along with ER in Table 4 by calculating the cosine similarity between the gradients of the replay samples and newly arriving data samples. As previously stated, the aim of many CL algorithms is to achieve high gradient alignment across tasks to allow parameter-sharing between them. We see that our variants achieve an order of magnitude higher cosine similarity compared to ER, verifying that our objective promotes gradient alignment. 6 Conclusion We introduced La-MAML, an efﬁcient meta-learning algorithm that leverages replay to avoid for- getting and favor positive backward transfer by learning the weights and LRs in an asynchronous manner. It is capable of learning online on a non-stationary stream of data and scales to vision tasks. We presented results that showed better performance against the state-of-the-art in the setup of efﬁcient lifelong learning (LLL) [ 6], as well as the standard continual learning setting. In the future, more work on analysing and producing good optimizers for CL is needed, since many of our standard go-to optimizers like Adam [13] are primarily aimed at ensuring faster convergence in stationary supervised learning setups. Another interesting direction is to explore how the connections to meta-descent can lead to more stable training procedures for meta-learning that can automatically adjust hyper-parameters on-the-ﬂy based on training dynamics. Table 3: Results on the standard continual (Multiple) and LLL (Single) setups with CIFAR-100 and TinyImagenet-200. Experiments are run with 3 seeds. * indicates result omitted due to high instability. METHOD CIFAR-100 T INY IMAGENET MULTIPLE SINGLE MULTIPLE SINGLE RA BTI RA BTI RA BTI RA BTI IID 85.60 ± 0.40 - - - 77.1 ± 1.06 - - - ER 59.70 ± 0.75 -16.50 ± 1.05 47.88 ± 0.73 -12.46 ± 0.83 48.23 ± 1.51 -19.86 ± 0.70 39.38 ± 0.38 -14.33 ± 0.89 ICARL 60.47 ± 1.09 -15.10 ± 1.04 53.55 ± 1.69 -8.03 ± 1.16 54.77 ± 0.32 -3.93 ± 0.55 45.79 ± 1.49 -2.73 ± 0.45 GEM 62.80 ± 0.55 -17.00 ± 0.26 48.27 ± 1.10 -13.7 ± 0.70 50.57 ± 0.61 -20.50 ± 0.10 40.56 ± 0.79 -13.53 ± 0.65 AGEM 58.37 ± 0.13 -17.03 ± 0.72 46.93 ± 0.31 -13.4 ± 1.44 46.38 ± 1.34 -19.96 ± 0.61 38.96 ± 0.47 -13.66 ± 1.73 MER - - 51.38 ± 1.05 -12.83 ± 1.44 - - 44.87 ± 1.43 -12.53 ± 0.58 META-BGD 65.09 ± 0.77 -14.83 ± 0.40 57.44 ± 0.95 -10.6 ± 0.45 * * 50.64 ± 1.98 -6.60 ± 1.73 C-MAML 65.44 ± 0.99 -13.96 ± 0.86 55.57 ± 0.94 -9.49 ± 0.45 61.93 ± 1.55 -11.53 ± 1.11 48.77 ± 1.26 -7.6 ± 0.52 LA-ER 67.17 ± 1.14 -12.63 ± 0.60 56.12 ± 0.61 -7.63 ± 0.90 54.76 ± 1.94 -15.43 ± 1.36 44.75 ± 1.96 -10.93 ± 1.32 SYNC 67.06 ± 0.62 -13.66 ± 0.50 58.99 ± 1.40 -8.76 ± 0.95 65.40 ± 1.40 -11.93 ± 0.55 52.84 ± 2.55 -7.3 ± 1.93 LA-MAML 70.08 ± 0.66 -9.36 ± 0.47 61.18 ± 1.44 -9.00 ± 0.2 66.99 ± 1.65 -9.13 ± 0.90 52.59 ± 1.35 -3.7 ± 1.22 8Figure 3: Retained Accuracy (RA) for La-MAML plotted every 25 meta-updates up to Task 5 on CIFAR-100. RA at iteration j (with j increasing along the x-axis) denotes accuracy on all tasks seen uptil then. Red denotes the RA computed during the inner updates (at θj k). Blue denotes RA computed at θj+1 0 right after a meta-update. We see that in the beginning, inner updates lead to catastrophic forgetting (CF) since the weights are not suitable for CL yet, but eventually become resistant when trained to retain old knowledge while learning on a stream of correlated data. We also see that RA maintains its value even as more tasks are added indicating that the model is successful at learning new tasks without sacriﬁcing performance on old ones. Table 4: Gradient Alignment on CIFAR-100 and TinyImagenet dataset (values lie in [-1,1], higher is better) DATASET ER C-MAML SYNC L A-MAML CIFAR-100 0.22 ×10−2 ± 0.0017 1.84 ×10−2 ± 0.0003 2.28 ×10−2 ± 0.0004 1.86 ×10−2 ± 0.0027 TINY IMAGENET 0.27 ×10−2 ± 0.0005 1.74 ×10−2 ± 0.0005 2.17 ×10−2 ± 0.0020 2.14 ×10−2 ± 0.0023 Broader Impact This work takes a step towards enabling deployed models to operate while learning online. This would be very relevant for online, interactive services like recommender systems or home robotics, among others. By tackling the problem of catastrophic forgetting, the proposed approach goes some way in allowing models to add knowledge incrementally without needing to be re-trained from scratch. Training from scratch is a compute intensive process, and even requires access to data that might not be available anymore. This might entail having to navigate a privacy-performance trade-off since many techniques like federated learning actually rely on not having to share data across servers, in order to protect user-privacy. The proposed algorithm stores and replays random samples of prior data, and even with the higher alignment of the samples within a task under the proposed approach, there will eventually be some concept drift. While the proposed algorithm itself does not rely on or introduce any biases, any bias in the sampling strategy itself might inﬂuence the distribution of data that the algorithm remembers and performs well on. Acknowledgments and Disclosure of Funding The authors are grateful to Matt Riemer, Sharath Chandra Raparthy, Alexander Zimin, Heethesh Vhavle and the anonymous reviewers for proof-reading the paper and suggesting improvements. This research was enabled in part by support provided by Compute Canada (www.computecanada.ca). References [1] URL https://tiny-imagenet.herokuapp.com/. [2] Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Contin- uous adaptation via meta-learning in nonstationary and competitive environments. In International Confer- ence on Learning Representations, 2018. URL https://openreview.net/forum?id=Sk2u1g-0-. [3] Rahaf Aljundi, Marcus Rohrbach, and Tinne Tuytelaars. Selﬂess sequential learning. In International Con- ference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkxbrn0cYX. 9[4] Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, and Frank Wood. Online learning rate adaptation with hypergradient descent. In International Conference on Learning Representa- tions, 2018. URL https://openreview.net/forum?id=BkrsAzWAb. [5] Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 233–248, 2018. [6] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-GEM. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=Hkf2_sC5FX. [7] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K. Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato. On Tiny Episodic Memories in Continual Learning. arXiv e-prints, art. arXiv:1902.10486, February 2019. [8] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklUCCVKDB. [9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1126–1135. JMLR. org, 2017. [10] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research , pages 1920–1930, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/finn19a. html. [11] Robert French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3:128–135, 05 1999. doi: 10.1016/S1364-6613(99)01294-2. [12] Khurram Javed and Martha White. Meta-learning representations for continual learning. In Advances in Neural Information Processing Systems, pages 1818–1828, 2019. [13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 12 2014. [14] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences , 114(13):3521–3526, 2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/content/114/13/3521. [15] Lei Le, Raksha Kumaraswamy, and Martha White. Learning sparse representations in reinforcement learning with sparse coding. In Proceedings of the 26th International Joint Conference on Artiﬁcial Intelligence, IJCAI’17, page 2067–2073. AAAI Press, 2017. ISBN 9780999241103. [16] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017. [17] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems, pages 6467–6476, 2017. [18] James Mcclelland, Bruce Mcnaughton, and Randall O’Reilly. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102:419–57, 08 1995. doi: 10.1037/0033-295X.102.3.419. [19] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual adaptation for model-based RL. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyxAfnA5tm. [20] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. [21] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017. 10[22] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. In Interna- tional Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= B1gTShAct7. [23] Levent Sagun, Utku Evci, V . Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical Analysis of the Hessian of Over-Parametrized Neural Networks. arXiv e-prints, art. arXiv:1706.04454, June 2017. [24] Jürgen Schmidhuber. Evolutionary principles in self-referential learning. 1987. [25] Nicol Schraudolph. Local gain adaptation in stochastic gradient descent. 06 1999. doi: 10.1049/cp: 19991170. [26] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4548–4557, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL http:// proceedings.mlr.press/v80/serra18a.html. [27] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2990–2999. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 6892-continual-learning-with-deep-generative-replay.pdf . [28] Harkirat Singh Behl, Atılım Günes, Baydin, and Philip H. S. Torr. Alpha MAML: Adaptive Model-Agnostic Meta-Learning. arXiv e-prints, art. arXiv:1905.07435, May 2019. [29] Sebastian Thrun and Lorien Pratt, editors. Learning to Learn. Kluwer Academic Publishers, USA, 1998. ISBN 0792380479. [30] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. [31] Ya-xiang Yuan. A review of trust region algorithms for optimization. ICM99: Proceedings of the Fourth International Congress on Industrial and Applied Mathematics, 09 1999. [32] Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task Agnostic Continual Learning Using Online Variational Bayes. arXiv e-prints, art. arXiv:1803.10123, Mar 2018. 11A Hypergradient Derivation for La-MAML We derive the gradient of the weights θj 0 and LRs αj at time-step j under the k-step MAML objective, with Lt = ∑t i=0 ℓi as the meta-loss and ℓt as the inner-objective: gMAML(αj) = ∂ ∂αjLt ( θj k ) = ∂ ∂θj k Lt ( θj k ) · ∂ ∂αj ( θj k ) = ∂ ∂θj k Lt ( θj k ) · ∂ ∂αj ( U ( θj k−1 )) = ∂ ∂θj k Lt ( θj k ) · ∂ ∂αj ( θj k−1 −αj∂ℓt(θj k−1) ∂θj k−1 ) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjθj k−1 − ∂ ∂αj ( αj∂ℓt(θj k−1) ∂θj k−1 )) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjθj k−1 −∂ℓt(θj k−1) ∂θj k−1 ) (Taking ∂ℓt ( θj k−1 ) ∂θj k−1 as a constant w.r.t αj to get the ﬁrst-order MAML approximation) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjU ( θj k−2 ) − ( ∂ℓt(θj k−1) ∂θj k−1 )) = ∂ ∂θj k Lt ( θj k ) · ( ∂ ∂αjθj 0 − k−1∑ k′=0 ∂ℓt(θj n) ∂θj n ) (a) = ∂ ∂θj k Lt ( θj k ) · ( − k−1∑ k′=0 ∂ℓt(θj k′ ) ∂θj k′ ) (b) Where (a) is obtained by recursively expanding and differentiating the update function U() as done in the step before it. (b) is obtained by assuming that the initial weight in the meta-update at time j : θj 0, is constant with respect to αj. Similarly we can derive the MAML gradient for the weights θj 0, denoted as gMAML(θj 0) as: gMAML(θj 0) = ∂ ∂θj 0 Lt(θj k) = ∂ ∂θj k Lt(θj k)∂θj k ∂θj 0 = ∂ ∂θj k Lt(θj k)∂Uk(θj k−1) ∂θj 0 = ∂ ∂θj k Lt(θj k) ∂ ∂θj k−1 U(θj k−1) ··· ∂ ∂θj 0 U(θj 1) (repeatedly applying chain rule and using θj k = U(θj k−1) ) = L′ t(θj k) ( I−αℓ′′ t(θj k−1) ) ··· ( I−αℓ′′ t(θj 0) ) ( using U′(θj k′ ) = I−αℓ′′ t(θj k′ ) ) (′implies derivative with respect to argument) = (k−1∏ k′=0 ( I−αℓ′′ t(θj k′ ) )) L′ t(θj k) Setting all ﬁrst-order gradient terms as constants to ignore second-order derivatives, we get the ﬁrst order approximation as: gFOMAML(θj 0) = (∏k−1 k′=0 ( I−αℓ′′ t ( θj k′ ))) L′ t(θj k) = L′ t(θj k) In Appendix B, we show the equivalence of the C-MAML and CL objectives in Eq. 6 by showing that the gradient of the former (gMAML(θj 0)) is equivalent to the gradient of the latter. 12B Equivalence of Objectives It is straightforward to show that when we optimise the OML objective through the k-step MAML update, as proposed in C-MAML in Eq. 5: min θj 0 Eτ1:t [ Lt ( Uk(θj 0) )] (10) where the inner-updates are taken using data from the streaming task τt, and the meta-loss Lt(θ) =∑t i=1 ℓi(θ) is computed on the data from all tasks seen so far, it will correspond to minimising the following surrogate loss used in CL : min θj 0 t∑ i=1  ℓi(θj 0) −α ∂ℓi ( θj 0 ) ∂θj 0 · ∂ℓt ( θj 0 ) ∂θj 0   (11) We show the equivalence for the case whenk= 1, for higher kthe form gets more complicated but essentially has a similar set of terms. Reptile [ 20] showed that the k-step MAML gradient for the weights θj 0 at time j, denoted as gMAML(θj 0) is of the form: ∂Lmeta(θj k) ∂θj 0 = ¯gk −α¯Hk k−1∑ k′=0 ¯gk′ −α k−1∑ k′=0 ¯Hk′ ¯gk + O ( α2) (αis the inner-loop learning rate) = ¯g1 −α¯H1¯g0 −α¯H0¯g1 + O ( α2) (using k= 1) Expressing the terms as derivatives, and using ∂ ∂θj 0 (¯g0 ·¯g1) = ¯H1¯g0 + ¯H0¯g1, we get : = ∂Lmeta(θj 0) ∂θj 0 − ∂ ∂θj 0 (¯g0 ·¯g1) = ∂ (∑t i=1 ℓi(θj 0) −α¯g1 ·¯g0 ) ∂θj 0 (substituting Lmeta = Lt = t∑ i=1 ℓi) = ∂ (∑t i=1 ℓi(θj 0) −α∂Lmeta(θj 0) ∂θj 0 ∂ℓt(θj 0) ∂θj 0 ) ∂θj 0 = ∂ (∑t i=1 ℓi(θj 0) −α∂∑t i=1 ℓi(θj 0) ∂θj 0 ∂ℓt(θj 0) ∂θj 0 ) ∂θj 0 (expanding Lmeta) = ∂ (∑t i=1 ℓi(θj 0) −α∑t i=1 ∂ℓi(θj 0) ∂θj 0 ∂ℓt(θj 0) ∂θj 0 ) ∂θj 0 which is the same as the gradient of Eq. 11. where: ¯gk = ∂Lmeta ( θj 0 ) ∂θj 0 (gradient of the meta-loss evaluated at the initial point ) ¯gk′ = ∂ ∂θj 0 Linner(θj 0) ( for k′<k) ( gradients of the inner-updates evaluated at the initial point) θj k′+1 = θj k′ −αgk′ (sequence of parameter vectors) 13¯Hk = L′′ meta ( θj 0 ) (Hessian of the meta-loss evaluated at the initial point) ¯Hk′ = L′′ inner ( θj 0 ) (for k′<k) (Hessian of the inner-objective evaluated at the initial point) And, in our case: Lmeta = Lt = t∑ i=1 ℓi Linner = ℓt Bias in the objective : We can see in Eq. 11 that the gradient alignment term introduces some bias, which means that the parameters don’t exactly converge to the minimiser of the losses on all tasks. This has been acceptable in the CL regime since we don’t aim to reach the minimiser of some stationary distribution anyway (as also mentioned in Section 4.3). If we did converge to the minimiser of say ttasks at some time j, this minimiser would no longer be optimal as soon as we see the new task τt+1. Therefore, in the limit of inﬁnite tasks and time, ensuring low-interference between tasks will pay off much more as opposed to being able to converge to the exact minima, by allowing us to make shared progress on both previous and incoming tasks. C C-MAML Algorithm Algorithm 2 outlines the training procedure for the C-MAML algorithm we propose 3. Algorithm 2 C-MAML Input: Network weights θ0 0, inner objective ℓ, meta objective L, Inner learning rate α, Outer learning rate β j ←0 R←{} ⊿Initialise replay-buffer for t:= 1 to T do (Xt,Y t) ∼Dt for ep:= 1 to numepochs do for batch bin (Xt,Y t) do k←sizeof(b) bm ←Sample(R) ∪b ⊿ batch of samples from τ1:t for meta-loss for k′= 0 to k−1 do Push b[k′] to R with some probability based on reservoir sampling θj k′+1 ←θj k′ −α·∇θj k′ ℓt(θj k′ ,b[k′]) ⊿inner-update on each incoming sample end for θj+1 0 ←θj 0 −β·∇θj 0 Lt(θj k,bm) ⊿outer-update by differentiating meta-loss j ←j+ 1 end for end for end for D Inter-Task Alignment We assume that at time j during training, we are seeing samples from the streaming task τt. It is intuitive to realise that incentivising the alignment of all τ1:t with the current τt indirectly also incentivises the alignment amongst τ1:t−1 as well. To demonstrate this, we compute the mean dot product of the gradients amongst the old tasks τ1:t−1 as the new task τt is added, for tvarying from 2 to 11. We do this for C-MAML and La-MAML on CIFAR-100. 3Our algorithm, Continual-MAML is different from a concurrent work https://arxiv.org/abs/2003. 05856 which proposes an algorithm with the same name 14As can be seen in Figures 4a and 4b, the alignment stays positive and roughly constant even as more tasks are added. (a) C-MAML  (b) La-MAML Figure 4: Average dot product amongst gradients of τ1:t−1 as new tasks are added, for the C-MAML and La-MAML algorithms calculated over 5 runs. x-axis shows the streaming task ID, tand y-axis shows the cosine similarity. E Robustness Learning rate is one of the most crucial hyper-parameters during training and it often has to be tuned extensively for each experiment. In this section we analyse the robustness of our proposed variants to their LR-related hyper-parameters on the CIFAR-100 dataset. Our three variants have different sets of these hyper-parameters which are speciﬁed as follows: • C-MAML: Inner and outer update LR (scalar) for the weights (αand β) • Sync La-MAML: Inner loop initialization value for the vector LRs ( α0), scalar learning rate of LRs (η) and scalar learning rate for the weights in the outer update (β) • La-MAML: Scalar initialization value for the vector LRs (α0) and a scalar learning rate of LRs (η) La-MAML is considerably more robust to tuning compared to its variants, as can be seen in Figure 5c. We empirically observe that it only requires tuning of the initial value of the LR, while being relatively insensitive to the learning rate of the LR (η). We see a consistent trend where the increase in η leads to an increase in the ﬁnal accuracy of the model. This increase is very gradual, since across a wide range of LRs varying over 2 orders of magnitude (from 0.003 to 0.3), the difference in RA is only 6%. This means that even without tuning this parameter (η), La-MAML would have outperformed most baselines at their optimally tuned values. As seen in Figure 5a, C-MAML sees considerable performance variation with the tweaking of both the inner and outer LR. We also see that the effects of the variations of the inner and outer LR follow very similar trends and their optimal values ﬁnally selected are also identical. This means that we could potentially tune them by doing just a 1D search over them together instead of varying both independently through a 2D grid search. The Sync version of La-MAML (Figure 5b), while being relatively insensitive to the scalar initial value α0 and the η, sees considerable performance variation as the outer learning rate for the weights: βis varied. This variant has the most hyper-parameters and only exists for the purpose of ablation. Fig. 6 shows the result of 2D grid-searches over sets of the above-mentioned hyper-parameters for C-MAML and La-MAML for a better overview. F Timing Comparisons In this section, we compare the wall-clock running times (Retained Accuracy (RA) versus Time) of La-MAML against other baselines on the CIFAR100 dataset in the multi-pass setting. For ER, iCarl and La-MAML we see an increasing tread in the RA vs Time plot with La-MAML having the best 15(a) C-MAML: Modulation of αand β  (b) Sync: Modulation of α0, ηand β (c) La-MAML: Modulation of α0 and η Figure 5: Retained Accuracy vs Learning Rates plot for La-MAML and its variants. Figures are plotted by varying one of the learning rate hyperparameter while keeping the others ﬁxed at their optimal value. The hyperparameter is varied between [0.001, 0.3]. RA at the expense of the increase in time. In contrast, both AGEM and GEM perform worse than La-MAML while also taking much more running time. G Experimental We carry out hyperparameter tuning for all the approaches by performing a grid-search over the range [0.0001 - 0.3] for hyper-parameters related to the learning-rate. For the multi-pass setup we use 10 epochs for all the CL approaches. In the single pass setup, all compared approaches have a hyper-parameter called glances which indicates the number of gradient updates or meta-updates made on each incoming sample of data. In the Single-Pass (LLL) setup, it becomes essential to take multiple gradient steps on each sample (or see each sample for multipleglances), since once we move on to later samples, we can’t revisit old data samples. The performance of the algorithms naturally increases with the increase in glances up to a certain point. To ﬁnd the optimal number of glances to take over each sample, we search over the values [1,2,3,5,10]. Tables 5 and 6 lists the optimal hyperparameters for all the compared approaches. All setups used the SGD optimiser since it was found to preform better than Adam [13] (possibly due to reasons stated in Section 4.3 regarding the CL setup). To avoid exploding gradients, we clip the gradient values of all approaches at a norm of 2.0. Class di- visions across different tasks vary with the random seeds with which the experiments were conducted. Overall, we did not see much variability across different class splits, with the variation being within 0.5-2% of the mean reported result as can be seen from Table 3 For all our baselines, we use a constant batch-size of 10 samples from the streaming task. This batch is augmented with 10 samples from the replay buffer for the replay-based approaches. La-MAML and its variants split the batch from the streaming task into a sequence of smaller disjoint sets to take multiple (k = 10 for MNIST and k = 5 for CIFAR100/TinyImagenet) gradient steps in the inner-loop. In MER, each sample from the incoming task is augmented with a batch of 10 replay 16(a) C-MAML: Modulation of αand β  (b) La-MAML: Modulation of α0 and η Figure 6: Plots of Retained Accuracy (RA) across hyper-parameter variation for C-MAML and La-MAML. We show results of the grid search over the learning rate hyperparameters. RA decreases from red to blue. All the hyperparameters are varied between [0.001, 0.3], with the axes being in log-scale. Figure 7: Retained Accuracy vs Running time (seconds) for La-MAML vs other baselines on the CIFAR100 dataset. samples to form the batch used for the meta-update. We found very small performance gaps between the ﬁrst and second-order versions of our proposed variants with performance differences in the range of 1-2% for RA. This is in line with the observation that deep neural networks have near-zero hessians since the ReLU non-linearity is linear almost everywhere [23]. MNIST Benchmarks: On the MNIST continual learning benchmarks, images of size 28x28 are ﬂattened to create a 1x784 array. This array is passed on to a fully-connected neural network having two layers with 100 nodes each. Each layer uses ReLU non-linearity. The output layer uses a single head with 10 nodes corresponding to the 10 classes. In all our experiments, we use a modest replay buffer of size 200 for MNIST Rotations and Permutation and size 500 for Many Permutations. Real-world visual classiﬁcation: For CIFAR and TinyImageNet we used a CNN having 3 and 4 conv layers respectively with 160 3x3 ﬁlters. The output from the ﬁnal convolution layer is ﬂattened and is passed through 2 fully connected layers having 320 and 640 units respectively. All the layers are succeeded by ReLU nonlinearity. Finally, a multi-headed output layer is used for performing 5-way classiﬁcation for every task. This architecture is used in prior meta-learning work [30]. For CIFAR and TinyImagenet, we allow a replay buffer of size 200 and 400 respectively which implies that each class in these dataset gets roughly about 1-2 samples in the buffer. For TinyImagenet, we split the validation set into val and test splits, since the labels in the actual test set are not released. 17Table 5: Final hyperparameters for all compared approaches on the CIFAR and TinyImagenet benchmarks METHOD PARAMETER CIFAR-100 T INY IMAGENET SINGLE MULTIPLE SINGLE MULTIPLE ER LR 0.03 0.03 0.1 0.1 Epochs/Glances 10 10 10 10 IID LR - 0.03 - 0.01 Epochs/Glances - 50 - 50 ICARL LR 0.03 0.03 0.01 0.01 Epochs/Glances 2 10 2 10 GEM LR 0.03 0.03 0.03 0.03 Epochs/Glances 2 10 2 10 AGEM LR 0.03 0.03 0.01 0.01 Epochs/Glances 2 10 2 10 MER LR α 0.1 - 0.1 - LR β 0.1 - 0.1 - LR γ 1 - 1 - Epochs/Glances 10 - 10 - META-BGD η 50 50 50 - std-init 0.02 0.02 0.02 - βinner 0.1 0.1 0.1 - mc-iters 2 2 2 - Epochs/Glances 3 10 3 - C-MAML α 0.03 0.03 0.03 0.03 β 0.03 0.03 0.03 0.03 Epochs/Glances 5 10 2 10 LA-ER α0 0.1 0.1 0.03 0.03 η 0.1 0.1 0.1 0.1 Epochs/Glances 1 10 2 10 SYNC LA-MAML α0 0.1 0.1 0.075 0.075 β 0.1 0.1 0.075 0.075 η 0.3 0.3 0.25 0.25 Epochs/Glances 5 10 2 10 LA-MAML α0 0.1 0.1 0.1 0.1 η 0.3 0.3 0.3 0.3 Epochs/Glances 10 10 2 10 H Baselines On the MNIST benchmarks, we compare our algorithm against the baselines used in [22], which are as follows: • Online: A baseline for the LLL setup, where a single network is trained one example at a time with SGD. • EWC [14]: Elastic Weight Consolidation is a regularisation based method which constraints the weights important for the previous tasks to avoid catastrophic forgetting. • GEM [17]: Gradient Episodic Memory does constrained optimisation by solving a quadratic program on the gradients of new and replay samples, trying to make sure that these gradients do not alter the past tasks’ knowledge. • MER [22]: Meta Experience Replay samples i.i.d data from a replay memory to meta-learn model parameters that show increased gradient alignment between old and current samples. We evaluate against this baseline only in the LLL setups. On the real-world visual classiﬁcation dataset, we carry out experiments on GEM, MER along with:- 18Table 6: Final hyperparameters used for our variants on the MNIST benchmarks METHOD PARAMETER PERMUTATIONS ROTATIONS MANY C-MAML α 0.03 0.1 0.03 β 0.1 0.1 0.15 Glances 5 5 5 SYNC LA-MAML α0 0.15 0.15 0.03 β 0.1 0.3 0.03 η 0.1 0.1 0.1 Glances 5 5 10 LA-MAML α0 0.3 0.3 0.1 η 0.15 0.15 0.1 Glances 5 5 10 • IID: Network gets the data from all tasks in an independent and identically distributed manner, thus bypassing the issue of catastrophic forgetting completely. Therefore, IID acts as an upper bound for the RA achievable with this network. • ER: Experience Replay uses a small replay buffer to store old data using reservoir sampling. This stored data is then replayed again along with the new data samples. • iCARL [21]: iCARL is originally from the family of class incremental learners, which learns to classify images in the metric space. It prevents catastrophic forgetting by using a memory of exemplar samples to perform distillation from the old network weights. Since we perform experiments in a task incremental setting, we use the modiﬁed version of iCARL (as used by GEM [17]), where distillation loss is calculated only over the logits of the particular task. • A-GEM [6]: Averaged Gradient Episodic Memory proposed to project gradients of the new task to a direction such as to avoid interference with respect to the average gradient of the old samples in the buffer. • Meta-BGD: Bayesian Gradient Descent [32] proposes training a bayesian neural network for CL where the learning rate for the parameters (the means) are derived from their variances. We construct this baseline by equipping C-MAML with bayesian training, where each parameter in θis now sampled from a gaussian distribution with a certain mean and variance. The inner-loop stays same as C-MAML(constant LR), but the magnitude of the meta-update to the parameters in θis now inﬂuenced by their associated variances. The variance updates themselves have a closed form expression which depends on mmonte-carlo samples of the meta-loss, thus implying mforward passes of the inner-and-outer loops (each time with a newly sampled θ) to get mmeta-gradients. I Discussion on Prior Work In Table 7, we provide a comparative overview of various continual learning methods to situate our work better in the context of prior work. Prior-focused methods face model capacity saturation as the number of tasks increase. These methods freeze weights to defy forgetting, and so penalise changes to the weights, even if those changes could potentially improve model performance on old tasks. They are also not suitable for the LLL setup (section 5), since it requires many passes through the data for every task to learn weights that are optimal enough to be frozen. Additionally, the success of weight freezing schemes can be attributed to over-parameterisation in neural networks, leading to sub-networks with sufﬁcient capacity to learn separate tasks. However continual-learning setups are often motivated in resource-constrained settings requiring efﬁciency and scalability. Therefore solutions that allow light-weight continual learners are desirable. Meta-learning algorithms are able to exploit even small models to learn a good initialization where gradients are aligned across tasks, enabling shared progress on optimisation of task-wise objectives. Our method additionally allows meta-learning to also achieve a prior-focusing affect through the async-meta-update, without necessarily needing over-parameterised models. 19In terms of resources, meta-learning based methods require smaller replay memories than traditional methods because they learn to generalise better across and within tasks, thus being sample-efﬁcient. Our learnable learning rates incur a memory overhead equal to the parameters of the network. This is comparable to or less than many prior-based methods that store between 1 to T scalars per parameter depending on the approach (T is the number of tasks). It should be noted that our learning rate modulation involves clipping updates for parameters with non-aligning gradients. In this aspect, it is related to methods like GEM and AGEM mentioned before. Where the distinction lies, is that our method takes some of the burden off of the clipping, by ensuring that gradients are more aligned in the ﬁrst place. This means that there should be less interference and therefore less clipping of updates deemed essential for learning new tasks, on the whole. Table 7: Setups in prior work: We describe the setups and assumptions adopted by prior work, focusing on approaches relevant to our method. FWT and BWT refer to forward and backward transfer as deﬁned in [17]. ’-’ refers to no inductive bias for or against the speciﬁc property. Saturation of capacity refers to reduced network plasticity due to weight change penalties gradually making further learning impossible. The LLL setup is deﬁned in Section 5. <and >with replay indicate that a method’s replay requirements are lesser or more compared to other methods in the table. Fishers refers to the Fisher Information Matrix (FIM) computed per task. Each FIM has storage equal to that of the model parameters. Approaches using Bayesian Neural Networks require twice as many parameters (as does La-MAML) to store the mean and variance estimates per parameter. APPROACH TRANSFER CAPACITY RESOURCES ALGORITHM FWT BWT SATURATES LLL S TORAGE PRIOR-FOCUSED - × √ × T FISHERS EWC [14] PRIOR FOCUSED - × √ × T MASKS HAT [26] PRIOR FOCUSED - × √ √ 2X PARAMS BGD/UCB [32] [8] REPLAY - - × √ >REPLAY ICARL [21] REPLAY - - × √ >REPLAY GEM [17] META + REPLAY √ √ × √ REPLAY MER [22] META + REPLAY √ √ × √ REPLAY OURS 20",
      "meta_data": {
        "arxiv_id": "2007.13904v2",
        "authors": [
          "Gunshi Gupta",
          "Karmesh Yadav",
          "Liam Paull"
        ],
        "published_date": "2020-07-27T23:07:01Z",
        "pdf_url": "https://arxiv.org/pdf/2007.13904v2.pdf",
        "github_url": "https://github.com/montrealrobotics/La-MAML"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of catastrophic forgetting in continual learning for models with limited capacity learning from sequentially arriving tasks. It proposes Look-ahead MAML (La-MAML), a fast optimization-based meta-learning algorithm for online-continual learning, enhanced by a small episodic memory. La-MAML introduces modulation of per-parameter learning rates in the meta-learning update, drawing connections to hypergradients and meta-descent. This mechanism offers a more flexible and efficient approach to mitigate catastrophic forgetting compared to conventional prior-based methods. The key finding is that La-MAML achieves superior or comparable performance against state-of-the-art replay-based, prior-based, and meta-learning based continual learning approaches on real-world visual classification benchmarks (CIFAR-100, TinyImagenet) and MNIST benchmarks, while being significantly more sample-efficient and faster than prior meta-learning methods like MER.",
        "methodology": "The proposed approach builds upon a base algorithm called Continual-MAML (C-MAML). C-MAML aims to optimize an Online-aware Meta-Learning (OML) objective online, adapted to optimize model parameters, and utilizes a replay buffer. The meta-objective used is shown to be equivalent to the A-GEM objective, which is asymmetric and focuses on aligning the gradients of the current task with the average gradient of previously seen tasks, leading to faster learning compared to objectives that align all pairwise task gradients. La-MAML extends C-MAML by incorporating a set of learnable per-parameter learning rates (LRs) used in the inner updates. These LRs are modulated based on the alignment (or interference) between gradients of old and new tasks, with negative LR gradients increasing LR magnitude (for alignment) and positive gradients decreasing it (for interference). The network weights and LRs are updated asynchronously: LRs are updated first based on their hypergradients, and then weights are updated using these new, clipped-to-positive LRs to ensure conservative updates and mitigate catastrophic forgetting. A replay buffer is maintained using reservoir sampling to store past samples, which are used to form meta-batches alongside current task data for meta-loss evaluation.",
        "experimental_setup": "Experiments were conducted on both toy and real-world visual classification benchmarks. Toy benchmarks include MNIST Rotations, MNIST Permutations (20 tasks each with 1000 samples), and a more complex Many Permutations (100 tasks, 200 samples/task), all tested in a low-data regime (200-500 samples). Real-world benchmarks are CIFAR-100 (20 disjoint 5-way classification tasks) and TinyImagenet-200 (40 5-way classification tasks). Experiments were run in two settings: Single-Pass (Efficient Lifelong Learning - LLL), where data is processed once, and Multiple-Pass (standard CL), allowing up to 10 epochs of training. A modest replay buffer size (200 for MNIST Rotations/Permutations, 500 for Many Permutations, 200 for CIFAR-100, and 400 for TinyImagenet) was used. Performance metrics included Retained Accuracy (RA) and Backward-Transfer and Interference (BTI). Gradient alignment (cosine similarity) was also measured. Baselines for comparison included Online, EWC, GEM, MER, IID (upper bound), ER, iCARL, A-GEM, Meta-BGD, C-MAML, Sync-La-MAML, and La-ER. Architectures used were a fully-connected neural network for MNIST and CNNs for CIFAR/TinyImagenet. The SGD optimizer was used across all experiments, with gradient clipping at a norm of 2.0. Hyperparameters were tuned via grid-search.",
        "limitations": "The objective's gradient alignment term introduces some bias, preventing exact convergence to the minimizer of losses across all tasks, though this is deemed acceptable in the continual learning regime. The reliance on a replay buffer for prior data, even with improved gradient alignment, may lead to concept drift over extended periods. The algorithm's performance can be influenced by biases in the sampling strategy used to populate the replay buffer. While La-MAML addresses the slowness of general meta-learning, learnable per-parameter LRs still incur a memory overhead proportional to the network parameters. A strong comparison baseline, Meta-BGD, was found to be highly sensitive to hyperparameters and required significant computational resources due to Monte Carlo iterations.",
        "future_research_directions": "Future work should focus on analyzing and developing improved optimizers specifically tailored for continual learning. Standard optimizers like Adam are primarily designed for faster convergence in stationary supervised learning settings, which may not be optimal for non-stationary CL environments. Another promising direction is to further explore the connections between La-MAML's adaptive LRs and meta-descent. This could lead to more stable training procedures for meta-learning algorithms that are capable of automatically adjusting hyper-parameters adaptively based on real-time training dynamics.",
        "experimental_code": "File Path: model/meta/learner.py\nContent:\nimport math\nimport os\nimport sys\nimport traceback\nimport numpy as np\nimport ipdb\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass Learner(nn.Module):\n\n    def __init__(self, config, args = None):\n        \"\"\"\n\n        :param config: network config file, type:list of (string, list)\n        :param imgc: 1 or 3\n        :param imgsz:  28 or 84\n        \"\"\"\n        super(Learner, self).__init__()\n\n        self.config = config\n        self.tf_counter = 0\n        self.args = args\n\n        # this dict contains all tensors needed to be optimized\n        self.vars = nn.ParameterList()\n        # running_mean and running_var\n        self.vars_bn = nn.ParameterList()\n\n        self.names = []\n\n        for i, (name, param, extra_name) in enumerate(self.config):\n            if name is 'conv2d':\n                # [ch_out, ch_in, kernelsz, kernelsz]                \n                if(self.args.xav_init):\n                    w = nn.Parameter(torch.ones(*param[:4]))\n                    b = nn.Parameter(torch.zeros(param[0]))\n                    torch.nn.init.xavier_normal_(w.data)\n                    b.data.normal_(0, math.sqrt(2)/math.sqrt(1+9*b.data.shape[0]))\n                    self.vars.append(w)\n                    self.vars.append(b)\n                else:\n                    w = nn.Parameter(torch.ones(*param[:4]))\n                    # gain=1 according to cbfin's implementation\n                    torch.nn.init.kaiming_normal_(w)\n                    self.vars.append(w)\n                    # [ch_out]\n                    self.vars.append(nn.Parameter(torch.zeros(param[0])))\n\n            elif name is 'convt2d':\n                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n                w = nn.Parameter(torch.ones(*param[:4]))\n                # gain=1 according to cbfin's implementation\n                torch.nn.init.kaiming_normal_(w)\n                self.vars.append(w)\n                # [ch_in, ch_out]\n                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n\n            elif name is 'linear':\n                # layer += 1\n                if(self.args.xav_init):\n                    w = nn.Parameter(torch.ones(*param))\n                    # b = nn.Parameter(torch.zeros(param[0]))\n                    torch.nn.init.xavier_normal_(w.data)\n                    # b.data.normal_(0, math.sqrt(2)/math.sqrt(1+9*b.data.shape[0]))\n                    self.vars.append(w)\n                    # self.vars.append(b)\n                else:     \n                    # [ch_out, ch_in]\n                    w = nn.Parameter(torch.ones(*param))\n                    # gain=1 according to cbfinn's implementation\n                    torch.nn.init.kaiming_normal_(w)\n                    self.vars.append(w)\n                # [ch_out]\n                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n\n            elif name is 'cat':\n                pass\n            elif name is 'cat_start':\n                pass\n            elif name is \"rep\":\n                pass\n            elif name in [\"residual3\", \"residual5\", \"in\"]:\n                pass\n            elif name is 'bn':\n                # [ch_out]\n                w = nn.Parameter(torch.ones(param[0]))\n                self.vars.append(w)\n                # [ch_out]\n                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n\n                # must set requires_grad=False\n                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n                self.vars_bn.extend([running_mean, running_var])\n\n\n            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n                continue\n            else:\n                raise NotImplementedError\n\n    def extra_repr(self):\n\n        info = ''\n\n        for name, param, extra_name in self.config:\n            if name is 'conv2d':\n                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)' \\\n                      % (param[1], param[0], param[2], param[3], param[4], param[5],)\n                info += tmp + '\\n'\n\n            elif name is 'convt2d':\n                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)' \\\n                      % (param[0], param[1], param[2], param[3], param[4], param[5],)\n                info += tmp + '\\n'\n\n            elif name is 'linear':\n                tmp = 'linear:(in:%d, out:%d)' % (param[1], param[0])\n                info += tmp + '\\n'\n\n            elif name is 'leakyrelu':\n                tmp = 'leakyrelu:(slope:%f)' % (param[0])\n                info += tmp + '\\n'\n\n            elif name is 'cat':\n                tmp = 'cat'\n                info += tmp + \"\\n\"\n            elif name is 'cat_start':\n                tmp = 'cat_start'\n                info += tmp + \"\\n\"\n\n            elif name is 'rep':\n                tmp = 'rep'\n                info += tmp + \"\\n\"\n\n\n            elif name is 'avg_pool2d':\n                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)' % (param[0], param[1], param[2])\n                info += tmp + '\\n'\n            elif name is 'max_pool2d':\n                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)' % (param[0], param[1], param[2])\n                info += tmp + '\\n'\n            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n                tmp = name + ':' + str(tuple(param))\n                info += tmp + '\\n'\n            else:\n                raise NotImplementedError\n\n        return info\n\n    def forward(self, x, vars=None, bn_training=False, feature=False):\n        \"\"\"\n        This function can be called by finetunning, however, in finetunning, we dont wish to update\n        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n        :param x: [b, 1, 28, 28]\n        :param vars:\n        :param bn_training: set False to not update\n        :return: x, loss, likelihood, kld\n        \"\"\"\n\n        cat_var = False\n        cat_list = []\n\n        if vars is None:\n            vars = self.vars\n\n        idx = 0\n        bn_idx = 0\n\n        try:\n\n            for (name, param, extra_name) in self.config:\n                # assert(name == \"conv2d\")\n                if name == 'conv2d':\n                    w, b = vars[idx], vars[idx + 1]\n                    x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n                    idx += 2\n\n                    # print(name, param, '\\tout:', x.shape)\n                elif name == 'convt2d':\n                    w, b = vars[idx], vars[idx + 1]\n                    x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n                    idx += 2\n\n\n                elif name == 'linear':\n\n                    # ipdb.set_trace()\n                    if extra_name == 'cosine':\n                        w = F.normalize(vars[idx])\n                        x = F.normalize(x)\n                        x = F.linear(x, w)\n                        idx += 1\n                    else:\n                        w, b = vars[idx], vars[idx + 1]\n                        x = F.linear(x, w, b)\n                        idx += 2\n\n                    if cat_var:\n                        cat_list.append(x)\n\n                elif name == 'rep':\n                    # print('rep')\n                    # print(x.shape)\n                    if feature:\n                        return x\n\n                elif name == \"cat_start\":\n                    cat_var = True\n                    cat_list = []\n\n                elif name == \"cat\":\n                    cat_var = False\n                    x = torch.cat(cat_list, dim=1)\n\n                elif name == 'bn':\n                    w, b = vars[idx], vars[idx + 1]\n                    running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx + 1]\n                    x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n                    idx += 2\n                    bn_idx += 2\n                elif name == 'flatten':\n                    # print('flatten')\n                    # print(x.shape)\n\n                    x = x.view(x.size(0), -1)\n\n                elif name == 'reshape':\n                    # [b, 8] => [b, 2, 2, 2]\n                    x = x.view(x.size(0), *param)\n                elif name == 'relu':\n                    x = F.relu(x, inplace=param[0])\n                elif name == 'leakyrelu':\n                    x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n                elif name == 'tanh':\n                    x = F.tanh(x)\n                elif name == 'sigmoid':\n                    x = torch.sigmoid(x)\n                elif name == 'upsample':\n                    x = F.upsample_nearest(x, scale_factor=param[0])\n                elif name == 'max_pool2d':\n                    x = F.max_pool2d(x, param[0], param[1], param[2])\n                elif name == 'avg_pool2d':\n                    x = F.avg_pool2d(x, param[0], param[1], param[2])\n\n                else:\n                    print(name)\n                    raise NotImplementedError\n\n        except:\n            traceback.print_exc(file=sys.stdout)\n            ipdb.set_trace()\n\n        # make sure variable is used properly\n        assert idx == len(vars)\n        assert bn_idx == len(self.vars_bn)\n\n        return x\n\n\n    def zero_grad(self, vars=None):\n        \"\"\"\n\n        :param vars:\n        :return:\n        \"\"\"\n        with torch.no_grad():\n            if vars is None:\n                for p in self.vars:\n                    if p.grad is not None:\n                        p.grad.zero_()\n            else:\n                for p in vars:\n                    if p.grad is not None:\n                        p.grad.zero_()\n\n    def define_task_lr_params(self, alpha_init=1e-3): \n        # Setup learning parameters\n        self.alpha_lr = nn.ParameterList([])\n\n        self.lr_name = []\n        for n, p in self.named_parameters():\n            self.lr_name.append(n)\n\n        for p in self.parameters():\n            self.alpha_lr.append(nn.Parameter(alpha_init * torch.ones(p.shape, requires_grad=True)))                                           \n\n    def parameters(self):\n        \"\"\"\n        override this function since initial parameters will return with a generator.\n        :return:\n        \"\"\"\n        return self.vars\n\nFile Path: model/meta/modelfactory.py\nContent:\nimport ipdb\n\nclass ModelFactory():\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_model(model_type, sizes, dataset='mnist', args=None):\n\n        net_list = []\n        if \"mnist\" in dataset:\n            if model_type==\"linear\":\n                for i in range(0, len(sizes) - 1):\n                    net_list.append(('linear', [sizes[i+1], sizes[i]], ''))\n                    if i < (len(sizes) - 2):\n                        net_list.append(('relu', [True], ''))\n                    if i == (len(sizes) - 2):\n                        net_list.append(('rep', [], ''))\n                return net_list\n\n        elif dataset == \"tinyimagenet\":\n\n            if model_type == 'pc_cnn':\n                channels = 160\n                return [\n                    ('conv2d', [channels, 3, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('flatten', [], ''),\n                    ('rep', [], ''),\n\n                    ('linear', [640, 16 * channels], ''),\n                    ('relu', [True], ''),\n\n                    ('linear', [640, 640], ''),\n                    ('relu', [True], ''),\n                    ('linear', [sizes[-1], 640], '')\n                ]\n\n        elif dataset == \"cifar100\":\n\n\n            if model_type == 'pc_cnn':\n                channels = 160\n                return [\n                    ('conv2d', [channels, 3, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n                    \n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('conv2d', [channels, channels, 3, 3, 2, 1], ''),\n                    ('relu', [True], ''),\n\n                    ('flatten', [], ''),\n                    ('rep', [], ''),\n\n                    ('linear', [320, 16 * channels], ''),\n                    ('relu', [True], ''),\n\n                    ('linear', [320, 320], ''),\n                    ('relu', [True], ''),\n                    ('linear', [sizes[-1], 320], '')\n                ]\n\n        else:\n            print(\"Unsupported model; either implement the model in model/ModelFactory or choose a different model\")\n            assert (False)\n\nFile Path: model/lamaml_base.py\nContent:\nimport random\nfrom random import shuffle\nimport numpy as np\nimport ipdb\nimport math\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport model.meta.learner as Learner\nimport model.meta.modelfactory as mf\nfrom scipy.stats import pearsonr\nimport datetime\n\nclass BaseNet(torch.nn.Module):\n\n    def __init__(self,\n                 n_inputs,\n                 n_outputs,\n                 n_tasks,           \n                 args):\n        super(BaseNet, self).__init__()\n\n        self.args = args\n        nl, nh = args.n_layers, args.n_hiddens\n\n        config = mf.ModelFactory.get_model(model_type = args.arch, sizes = [n_inputs] + [nh] * nl + [n_outputs],\n                                                dataset = args.dataset, args=args)\n\n        self.net = Learner.Learner(config, args)\n\n        # define the lr params\n        self.net.define_task_lr_params(alpha_init = args.alpha_init)\n\n        self.opt_wt = torch.optim.SGD(list(self.net.parameters()), lr=args.opt_wt)     \n        self.opt_lr = torch.optim.SGD(list(self.net.alpha_lr.parameters()), lr=args.opt_lr) \n\n        self.epoch = 0\n        # allocate buffer\n        self.M = []        \n        self.M_new = []\n        self.age = 0\n\n        # setup losses\n        self.loss = torch.nn.CrossEntropyLoss()\n        self.is_cifar = ((args.dataset == 'cifar100') or (args.dataset == 'tinyimagenet'))\n        self.glances = args.glances\n        self.pass_itr = 0\n        self.real_epoch = 0\n\n        self.current_task = 0\n        self.memories = args.memories\n        self.batchSize = int(args.replay_batch_size)\n\n        self.cuda = args.cuda\n        if self.cuda:\n            self.net = self.net.cuda()\n\n        self.n_outputs = n_outputs\n\n    def push_to_mem(self, batch_x, batch_y, t):\n        \"\"\"\n        Reservoir sampling to push subsampled stream\n        of data points to replay/memory buffer\n        \"\"\"\n\n        if(self.real_epoch > 0 or self.pass_itr>0):\n            return\n        batch_x = batch_x.cpu()\n        batch_y = batch_y.cpu()              \n        t = t.cpu()\n\n        for i in range(batch_x.shape[0]):\n            self.age += 1\n            if len(self.M_new) < self.memories:\n                self.M_new.append([batch_x[i], batch_y[i], t])\n            else:\n                p = random.randint(0,self.age)  \n                if p < self.memories:\n                    self.M_new[p] = [batch_x[i], batch_y[i], t]\n\n\n    def getBatch(self, x, y, t, batch_size=None):\n        \"\"\"\n        Given the new data points, create a batch of old + new data, \n        where old data is sampled from the memory buffer\n        \"\"\"\n\n        if(x is not None):\n            mxi = np.array(x)\n            myi = np.array(y)\n            mti = np.ones(x.shape[0], dtype=int)*t        \n        else:\n            mxi = np.empty( shape=(0, 0) )\n            myi = np.empty( shape=(0, 0) )\n            mti = np.empty( shape=(0, 0) )    \n\n        bxs = []\n        bys = []\n        bts = []\n\n        if self.args.use_old_task_memory and t>0:\n            MEM = self.M\n        else:\n            MEM = self.M_new\n\n        batch_size = self.batchSize if batch_size is None else batch_size\n\n        if len(MEM) > 0:\n            order = [i for i in range(0,len(MEM))]\n            osize = min(batch_size,len(MEM))\n            for j in range(0,osize):\n                shuffle(order)\n                k = order[j]\n                x,y,t = MEM[k]\n\n                xi = np.array(x)\n                yi = np.array(y)\n                ti = np.array(t)\n                bxs.append(xi)\n                bys.append(yi)\n                bts.append(ti)\n\n        for j in range(len(myi)):\n            bxs.append(mxi[j])\n            bys.append(myi[j])\n            bts.append(mti[j])\n\n        bxs = Variable(torch.from_numpy(np.array(bxs))).float() \n        bys = Variable(torch.from_numpy(np.array(bys))).long().view(-1)\n        bts = Variable(torch.from_numpy(np.array(bts))).long().view(-1)\n        \n        # handle gpus if specified\n        if self.cuda:\n            bxs = bxs.cuda()\n            bys = bys.cuda()\n            bts = bts.cuda()\n\n        return bxs,bys,bts\n\n    def compute_offsets(self, task):\n        # mapping from classes [1-100] to their idx within a task\n        offset1 = task * self.nc_per_task\n        offset2 = (task + 1) * self.nc_per_task\n        return int(offset1), int(offset2)\n\n    def zero_grads(self):\n        if self.args.learn_lr:\n            self.opt_lr.zero_grad()\n        self.opt_wt.zero_grad()\n        self.net.zero_grad()\n        self.net.alpha_lr.zero_grad()\nFile Path: model/lamaml_cifar.py\nContent:\nimport random\nimport numpy as np\nimport ipdb\nimport math\nimport torch\nimport torch.nn as nn\nfrom model.lamaml_base import *\n\nclass Net(BaseNet):\n\n    def __init__(self,\n                 n_inputs,\n                 n_outputs,\n                 n_tasks,           \n                 args):\n        super(Net, self).__init__(n_inputs,\n                                 n_outputs,\n                                 n_tasks,           \n                                 args)\n        self.nc_per_task = n_outputs / n_tasks\n\n    def take_loss(self, t, logits, y):\n        # compute loss on data from a single task\n        offset1, offset2 = self.compute_offsets(t)\n        loss = self.loss(logits[:, offset1:offset2], y-offset1)\n\n        return loss\n\n    def take_multitask_loss(self, bt, t, logits, y):\n        # compute loss on data from a multiple tasks\n        # separate from take_loss() since the output positions for each task's\n        # logit vector are different and we nly want to compute loss on the relevant positions\n        # since this is a task incremental setting\n\n        loss = 0.0\n\n        for i, ti in enumerate(bt):\n            offset1, offset2 = self.compute_offsets(ti)\n            loss += self.loss(logits[i, offset1:offset2].unsqueeze(0), y[i].unsqueeze(0)-offset1)\n        return loss/len(bt)\n\n\n    def forward(self, x, t):\n        output = self.net.forward(x)\n        # make sure we predict classes within the current task\n        offset1, offset2 = self.compute_offsets(t)\n        if offset1 > 0:\n            output[:, :offset1].data.fill_(-10e10)\n        if offset2 < self.n_outputs:\n            output[:, int(offset2):self.n_outputs].data.fill_(-10e10)\n        return output\n\n    def meta_loss(self, x, fast_weights, y, bt, t):\n        \"\"\"\n        differentiate the loss through the network updates wrt alpha\n        \"\"\"\n\n        offset1, offset2 = self.compute_offsets(t)\n\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss_q = self.take_multitask_loss(bt, t, logits, y)\n\n        return loss_q, logits\n\n    def inner_update(self, x, fast_weights, y, t):\n        \"\"\"\n        Update the fast weights using the current samples and return the updated fast\n        \"\"\"\n\n        offset1, offset2 = self.compute_offsets(t)            \n\n        logits = self.net.forward(x, fast_weights)[:, :offset2]\n        loss = self.take_loss(t, logits, y)\n\n        if fast_weights is None:\n            fast_weights = self.net.parameters()\n\n        # NOTE if we want higher order grads to be allowed, change create_graph=False to True\n        graph_required = self.args.second_order\n        grads = list(torch.autograd.grad(loss, fast_weights, create_graph=graph_required, retain_graph=graph_required))\n\n        for i in range(len(grads)):\n            grads[i] = torch.clamp(grads[i], min = -self.args.grad_clip_norm, max = self.args.grad_clip_norm)\n\n        fast_weights = list(\n            map(lambda p: p[1][0] - p[0] * p[1][1], zip(grads, zip(fast_weights, self.net.alpha_lr))))\n\n        return fast_weights\n\n\n    def observe(self, x, y, t):\n        self.net.train() \n        for pass_itr in range(self.glances):\n            self.pass_itr = pass_itr\n            perm = torch.randperm(x.size(0))\n            x = x[perm]\n            y = y[perm]\n\n            self.epoch += 1\n            self.zero_grads()\n\n            if t != self.current_task:\n                self.M = self.M_new.copy()\n                self.current_task = t\n\n            batch_sz = x.shape[0]\n            n_batches = self.args.cifar_batches\n            rough_sz = math.ceil(batch_sz/n_batches)\n            fast_weights = None\n            meta_losses = [0 for _ in range(n_batches)]\n\n            # get a batch by augmented incming data with old task data, used for \n            # computing meta-loss\n            bx, by, bt = self.getBatch(x.cpu().numpy(), y.cpu().numpy(), t)             \n\n            for i in range(n_batches):\n\n                batch_x = x[i*rough_sz : (i+1)*rough_sz]\n                batch_y = y[i*rough_sz : (i+1)*rough_sz]\n\n                # assuming labels for inner update are from the same \n                fast_weights = self.inner_update(batch_x, fast_weights, batch_y, t)   \n                # only sample and push to replay buffer once for each task's stream\n                # instead of pushing every epoch     \n                if(self.real_epoch == 0):\n                    self.push_to_mem(batch_x, batch_y, torch.tensor(t))\n                meta_loss, logits = self.meta_loss(bx, fast_weights, by, bt, t) \n                \n                meta_losses[i] += meta_loss\n\n            # Taking the meta gradient step (will update the learning rates)\n            self.zero_grads()\n\n            meta_loss = sum(meta_losses)/len(meta_losses)            \n            meta_loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(self.net.alpha_lr.parameters(), self.args.grad_clip_norm)\n            torch.nn.utils.clip_grad_norm_(self.net.parameters(), self.args.grad_clip_norm)\n            if self.args.learn_lr:\n                self.opt_lr.step()\n\n            # if sync-update is being carried out (as in sync-maml) then update the weights using the optimiser\n            # otherwise update the weights with sgd using updated LRs as step sizes\n            if(self.args.sync_update):\n                self.opt_wt.step()\n            else:            \n                for i,p in enumerate(self.net.parameters()):          \n                    # using relu on updated LRs to avoid negative values           \n                    p.data = p.data - p.grad * nn.functional.relu(self.net.alpha_lr[i])            \n            self.net.zero_grad()\n            self.net.alpha_lr.zero_grad()\n\n        return meta_loss.item()",
        "experimental_info": "Model Architecture: \n- Architecture type (`--arch`): `linear` or `pc_cnn`.\n- Number of hidden layers (`--n_layers`): Default 2.\n- Number of hidden neurons at each layer (`--n_hiddens`): Default 100.\n- Initialization (`--xav_init`): Boolean flag, default False (implies Kaiming normal initialization).\n\nOptimization & Learning Rates:\n- Learning rate for LRs (`--opt_lr`): Default 1e-1.\n- Learning rate for weights (`--opt_wt`): Default 1e-1.\n- Initial value for learnable LRs (`--alpha_init`): Default 1e-3.\n- Learnable LR updates (`--learn_lr`): Boolean flag, default False.\n- Synchronous updates for LRs and weights (`--sync_update`): Boolean flag, default False (implies asynchronous updates).\n- Gradient clip norm (`--grad_clip_norm`): Default 2.0.\n- Second-order MAML updates (`--second_order`): Boolean flag, default False.\n\nTraining Process:\n- Number of epochs per task (`--n_epochs`): Default 1.\n- Number of glances (`--glances`): Number of times the model trains over a set of samples in a single pass setting, default 1.\n- Batch size for current task data (`--batch_size`): Default 1.\n- Number of batches in inner trajectory (`--cifar_batches`): Default 3.\n\nReplay Buffer & Memory:\n- Total memories stored (`--memories`): Default 5120.\n- Batch size for experience replay (`--replay_batch_size`): Default 20.\n- Use old task memory (`--use_old_task_memory`): Boolean flag, default False.\n\nDataset & Data Loading:\n- Dataset (`--dataset`): Examples include `cifar100`, `tinyimagenet`, `mnist_rotations` (default).\n- Data loader (`--loader`): Default `task_incremental_loader`. Other options include `class_incremental_loader`.\n- Data path (`--data_path`): Default `data/`.\n- Validation split (`--validation`): Default 0.0.\n- Class order (`--class_order`): Default `old`. Other options: `random`, `chrono`, `super`.\n- Increment (`--increment`): Number of classes to increment by in class incremental loader, default 5.\n- Number of workers preprocessing data (`--workers`): Default 3.\n\nGeneral Settings:\n- CUDA (`--cuda`): Boolean flag, default False.\n- Random seed (`--seed`): Default 0.\n- Log directory (`--log_dir`): Default `logs/`.\n- Log frequency (`--log_every`): Default 1000 minibatches.\n- Calculate test accuracy (`--calc_test_accuracy`): Boolean flag, default False.\n- Test batch size (`--test_batch_size`): Default 100000."
      }
    },
    {
      "title": "Continuous Meta-Learning without Tasks",
      "abstract": "Meta-learning is a promising strategy for learning to efficiently learn\nwithin new tasks, using data gathered from a distribution of tasks. However,\nthe meta-learning literature thus far has focused on the task segmented\nsetting, where at train-time, offline data is assumed to be split according to\nthe underlying task, and at test-time, the algorithms are optimized to learn in\na single task. In this work, we enable the application of generic meta-learning\nalgorithms to settings where this task segmentation is unavailable, such as\ncontinual online learning with a time-varying task. We present meta-learning\nvia online changepoint analysis (MOCA), an approach which augments a\nmeta-learning algorithm with a differentiable Bayesian changepoint detection\nscheme. The framework allows both training and testing directly on time series\ndata without segmenting it into discrete tasks. We demonstrate the utility of\nthis approach on a nonlinear meta-regression benchmark as well as two\nmeta-image-classification benchmarks.",
      "full_text": "Continuous Meta-Learning without Tasks James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone Stanford University, Stanford, CA {jharrison, apoorva, cbfinn, pavone}@stanford.edu Abstract Meta-learning is a promising strategy for learning to efﬁciently learn using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, ofﬂine data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with unsegmented time series data. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian change- point detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on three nonlinear meta-regression benchmarks as well as two meta-image-classiﬁcation benchmarks. 1 Introduction Meta-learning methods have recently shown promise as an effective strategy for enabling efﬁcient few-shot learning in complex domains from image classiﬁcation to nonlinear regression [ 10, 40]. These methods leverage an ofﬂine meta-learning phase, in which data from a collection of learning tasks is used to learn priors and update rules for more efﬁcient learning on new related tasks. Meta-learning algorithms have thus far solely focused on settings with task segmentation, where the learning agent knows when the latent task changes. At meta-train time, these algorithms assume access to a meta-dataset of datasets from individual tasks, and at meta-test time, the learner is evaluated on a single task. However, there are many applications where task segmentation is unavailable, which have been under-addressed in the meta-learning literature. For example, environmental factors may change during a robot’s deployment, and these changes may not be directly observed. Furthermore, crafting a meta-dataset from an existing stream of experience may require a difﬁcult or expensive process of detecting switches in the task. In this work, we aim to enable meta-learning in task-unsegmented settings, operating directly on time series data in which the latent task undergoes discrete, unobserved switches, rather than requiring a pre-segmented meta-dataset. Equivalently, from the perspective of online learning, we wish to optimize an online learning algorithm using past data sequences to perform well in a sequential prediction setting wherein the underlying data generating process (i.e. the task) may vary with time. Contributions. Our primary contribution is an algorithmic framework for task unsegmented meta- learning which we refer to as meta-learning via online changepoint analysis (MOCA). MOCA wraps arbitrary meta-learning algorithms in a differentiable Bayesian changepoint estimation scheme, enabling their application to problems that require continual learning on time series data. By backpropagating through the changepoint estimation framework, MOCA learns both a rapidly adaptive underlying predictive model (the meta-learning model), as well as an effective changepoint detection algorithm, optimized to work together. MOCA is a generic framework which works with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1912.08866v2  [cs.LG]  21 Oct 2020many existing meta-learning algorithms. We demonstrate MOCA on both regression and classiﬁcation settings with unobserved task switches. 2 Problem Statement Our goal is to enable meta-learning in the general setting of sequential prediction, in which we observe a sequence of inputs xt and their corresponding labels yt. In this setting, the learning agent makes probabilistic predictions over the labels, leveraging past observations: pθ(ˆyt |x1:t,y1:t−1), where θare the parameters of the learning agent. We assume the data are drawn from an underlying generative model; thus, given a training sequence from this model Dtrain = (x1:N,y1:N), we can optimize θto perform well on another sample sequence from the same model at test time. We assume data is drawn according to a latent (unobserved) task Tt, that is xt,yt ∼p(x,y|Tt). Further, we assume that every so often, the task switches to a new task sampled from some distribution p(T). At each timestep, the task changes with probability λ, which we refer to as the hazard rate. We evaluate the learning algorithm in terms of a log likelihood, leading to the following objective: min θ E [∞∑ t=1 −log pθ(yt |x1:t,y1:t−1) ] (1) subj. to xt,yt ∼p(x,y|Tt), Tt = {Tt−1 w.p. 1 −λ Tt,new w.p. λ , T1 ∼p(T), Tt,new ∼p(T) Given Dtrain, we can approximate this expectation and thus learn θat train time. Note that just as in standard meta-learning, we leverage data drawn from a diverse collection of tasks in order to optimize a learning agent to do well on new tasks at test time. However, there are three key differences from standard meta-learning: • The learning agent continually adapts as it is evaluated on its predictions, rather than only adapting on klabeled examples, as is common in few-shot learning. • At train time, data is unsegmented, i.e. not grouped by the latent task T. • Similarly, at test time, the task changes with time, so the agent must infer which past data are drawn from the current task when making predictions. Thus, the setting we consider here can be considered a generalization of the standard meta-learning setting, relaxing the requirement of task segmentation at train and test time. Both our problem setting and an illustration of the MOCA algorithm are presented in Fig. 1. 3 Preliminaries Meta-Learning. The core idea of meta-learning is to directly optimize the few-shot learning performance of a machine learning model over a distribution of learning tasks, such that this learning performance generalizes to other tasks from this distribution. A meta-learning method consists of two phases: meta-training and online adaptation. Let θbe the parameters of this model learned in meta-training. During online adaptation, the model uses context data Dt = (x1:t,y1:t) from within one task to compute statistics ηt = fθ(Dt), where f is a function parameterized by θ. For example, in MAML [10], the statistics are the neural network weights after gradient updates computed using Dt. For recurrent network-based meta-learning algorithms, these statistics correspond to the hidden state of the network. For a simple nearest-neighbors model, η may simply be the context data. The model then performs predictions by using these statistics to deﬁne a conditional distribution on ygiven new inputs x, which we write y|x,Dt ∼pθ(y|x,ηt). Adopting a Bayesian perspective, we refer to pθ(y|x,ηt) as the posterior predictive distribution. The performance of this model on this task can be evaluated through the log-likelihood of task data under this posterior predictive distribution L(Dt,θ) = Ex,y∼p(·,·|Ti)[−log pθ(y|x,fθ(Dt))]. Meta-learning algorithms, broadly, aim to optimize the parameters θsuch that the model performs well across a distribution of tasks, minθ ETi∼p(T) [EDt∼Ti [L(Dt,θ)]] .Across most meta-learning algorithms, both the update rule fθ(·) and the prediction function are chosen to be differentiable operations, such that the parameters can be optimized via stochastic gradient descent. Given a dataset 2Figure 1: An illustration of a simpliﬁed version of our problem setting and of the MOCA algorithm. An agent sequentially observes an input x(e.g, an image), makes a probabilistic prediction, and receives the true label y (here, class 1 or 2). An unobserved change in the task (a “changepoint”) results in a change in the generative model of xand/or y. In the above image, the images corresponding to label 1 switch from sailboats to school buses, while the images corresponding to label 2 switch from sloths to geese. MOCA recursively estimates the time since the last changepoint, and conditions an underlying meta-learning model only on data that is relevant to the current task to optimize its predictions. pre-segmented into groups of data from individual tasks, standard meta-learning algorithms can estimate this expectation by ﬁrst sampling a group for which T is ﬁxed, then treating one part as context data Dt, and sampling from the remainder to obtain test points from the same task. While this strategy is effective for few-shot learning, it fails for settings like sequential prediction, where the latent task may change over time and segmenting data by task is difﬁcult. Our goal is to bring meta-learning tools to such settings. Bayesian Online Changepoint Detection. To enable meta-learning without task segmentation, we build upon Bayesian online changepoint detection [1], an approach for detecting discrete changes in a data stream (i.e. task switches), originally presented in an unconditional density estimation context. BOCPD operates by maintaining a belief distribution over run lengths, i.e. how many past data points were generated under the current task. A run length rt = 0 implies that the task has switched at time t, and so the current datapoint yt was drawn from a new task T′∼p(T). We denote this belief distribution at time tas bt(rt) = p(rt |y1:t−1). We can reason about the overall posterior predictive by marginalizing over the run length rt according to bt(rt), p(yt |y1:t−1) = ∑t−1 τ=0 p(yt | y1:t−1,rt = τ)bt(τ), Given rt = τ, we know the past τ data points all correspond to the current task, so p(yt |y1:t−1,rt = τ) can be computed as the posterior predictive of an underlying predictive model (UPM), conditioning on the past τ data points. BOCPD recursively computes posterior predictive densities using this UPM for each value of rt ∈{0,...,t −1}, and then evaluates new datapointsyt+1 under these posterior predictive densities to update the belief distribution b(rt). In this work, we extend these techniques to conditional density estimation, deriving update rules which use meta-learning models as the UPM. 4 Meta-Learning via Online Changepoint Analysis We now present MOCA1, which enables meta-learning in settings without task segmentation, both at train and test time. In the following subsections, we ﬁrst extend BOCPD to derive a recursive Bayesian ﬁltering algorithm for run length, leveraging a base meta-learning algorithm as the underlying predictive model (UPM). We then outline how the full framework allows both training and evaluating meta-learning models on time series without task segmentation. 4.1 Bayesian Task Duration Estimation As in BOCPD, MOCA maintains a belief over possible run lengths rt. Throughout this paper, we use bt to refer to the belief before observing data at that timestep, (xt,yt). Note that bt is a discrete distribution with support over rt ∈{0,...,t −1}. MOCA also maintains a version of the base meta-learning algorithm’s posterior parametersηfor every possible run length. We write ηt[r] to refer to the posterior parameters produced by the meta-learning algorithm after adapting to the past r 1Code is available at https://github.com/StanfordASL/moca 3Algorithm 1 Meta-Learning via Online Changepoint Analysis Require: Training data x1:n, y1:n, number of training iterations N, initial model parameters θ 1: for i = 1to N do 2: Sample training batch x1:T , y1:T from the full timeseries. 3: Initialize run length belief b1(r1 = 0) = 1, posterior statistics η0[r = 0]according to θ 4: for t = 1to T do 5: Observe xt, compute bt(rt |xt) via (2) 6: Predict pθ(ˆyt |x1:t, y1:t−1) via (5) 7: Observe yt and incur NLL loss ℓt = −log pθ(yt |x1:t, y1:t−1) 8: Compute updated posteriors ηt[rt] for all rt via (6) 9: Compute bt(rt |xt, yt) via (3) 10: Compute updated belief over run length bt+1 via (4) 11: end for 12: Compute ∇θ ∑k+T t=k ℓt and take gradient descent step to update θ 13: end for datapoints, (xt−r+1:t,yt−r+1:t). Given this collection of posteriors, we can compute the likelihood of observing data given the run length r. This allows us to apply rules from Bayesian ﬁltering to update the run length belief in closed form. These updates involve three steps: If the base meta-learning algorithm maintains a posterior distribution of inputs pθ(xt |ηt−1), then MOCA can update the belief bt directly after observing xt, as follows bt(rt |xt) := p(rt |x1:t,y1:t−1) ∝pθ(xt |ηt−1[rt])bt(rt) (2) which can be normalized by summing over the ﬁnite support of bt. This step relies on maintaining a generative model of the input variable, which is atypical for most regression models and is not done for discriminative classiﬁcation models. While this ﬁltering step is optional, it allows MOCA to detect task switches based on a changes in the input distribution when possible. Next, upon observing the labelyt, we can use the base meta-learning algorithm’s conditional posterior predictive pθ(yt |xt,ηt−1) to again update the belief over run length: bt(rt |xt,yt) := p(rt |x1:t,y1:t) ∝pθ(yt |xt,ηt−1[rt])bt(rt |xt), (3) which can similarly be normalized. Finally, to push the run length belief forward in time, we note that we assume that the task switches with probability λat every timestep, and so the task remains ﬁxed with probability 1 −λ. This yields the update bt+1(rt+1 = k) = {λ if k= 0 (1 −λ)bt(rt = k−1 |xt,yt) if k> 0 . (4) For more details on the derivation of these updates, we refer the reader to Appendix A. 4.2 Meta Learning without Task Segmentation By taking a Bayesian ﬁltering approach to changepoint detection, we avoid hard assignments of changepoints and instead perform a soft selection over run lengths. In this way, MOCA is able to backpropagate through the changepoint detection and directly optimize the underlying predictive model, which may be any meta-learning model that admits a probabilistic interpretation. MOCA processes a time series sequentially. We initialize b1(r1 = 0) = 1, and initialize the posterior statistics for η0[r1 = 0] as speciﬁed by the parameters θof the meta learning algorithm. Then, at timestep t, we ﬁrst observe inputs xt and compute bt(rt |xt) according to (2). Next, we marginalize to make a probabilistic prediction for the label, pθ(ˆyt |x1:t,y1:t−1) equal to t−1∑ rt=0 bt(rt |xt)pθ(ˆyt |xt,ηt−1[rt]) (5) We then observe the label yt and incur the corresponding loss. We can also use the label both to compute bt(rt |xt,yt) according to (3), as well as to update the posterior statistics for all the run 4lengths using the labeled example. Many meta-learning algorithms admit a recursive update rule which allows these parameters to be computed efﬁciently using the past values of η, ηt[r] = h(xt,yt,ηt−1[r−1]) ∀r= 1,...,t. (6) While MOCA could work without such a recursive update rule, this would require storing data online and running the non-recursive posterior computation ηt = fθ((xt−rt+1:t,yt−rt+1:t)) for every rt, which involves toperations using datasets of sizes from 0 to t, and thus can be an O(t2) operation. In contrast, the recursive updates involve toperations involving just the latest datapoint, yielding O(t) complexity. Finally, we propagate the belief over run length forward in time to obtain bt(rt+1) to be ready to process the next data point in the timeseries. Since all these operations are differentiable, given a training time series in which there are task switches Dtrain, we can run this procedure, sum the negative log likelihood (NLL) losses incurred at each step, and use backpropagation within a standard automatic differentiation framework to optimize the parameters of the base learning algorithm, θ. Algorithm 1 outlines this training procedure. In practice, we sample shorter time series of length T from the training data to ease computational requirements during training; we discuss implications of this in Appendix D. If available, a user can input various levels of knowledge on task segmentation by manually updating b(rt) at any time; further details and empirical validation of this task semi-segmented use case are also provided in Appendix D 4.3 Making your MOCA: Model Instantiations Thus far, we have presented MOCA at an abstract level, highlighting the fact that it can be used with any meta-learning model that admits the probabilistic interpretation as the UPM. Practically, as MOCA maintains several copies of the posterior statistics η, meta-learning algorithms with lower-dimensional posterior statistics which admit recursive updates yield better computational efﬁciency. With this in mind, for our experiments we implemented MOCA using a variety of base meta-learners: an LSTM-based meta-learning approach [ 21], as well as meta-learning algorithms based on Bayesian modeling which exploit conjugate prior/likelihood models allowing for closed- form recursive posterior updates, speciﬁcally ALPaCA [ 16] for regression and a novel algorithm in a similar vein which we call PCOC, for probabilistic clustering for online classiﬁcation, for classiﬁcation. Further details on all methods are provided in Appendix B. LSTM Meta-learner. The LSTM meta-learning approach encodes the information in the observed samples using hidden state ht of an LSTM [20], and subsequently uses this hidden state to make predictions. Speciﬁcally, we follow the architecture proposed in [21], wherein an encoding of the current input zt = φ(xt,w) as well as the previous label yt−1 are fed as input to the LSTM cell to update the hidden state ht and cell state ct. For regression, the mean and variance of a Gaussian posterior predictive distribution are output as a function of the hidden state and encoded input [µ,Σ] = f(ht,zt; wf). The function f is a feedforward network in both cases, with weights wf. Within the MOCA framework, the posterior statistics for this model are ηt = {ht,ct,yt}. ALPaCA: Bayesian Meta-Learning for Regression. ALPaCA is a meta-learning approach which performs Bayesian linear regression in a learned feature space, such thaty|x∼N(KTφ(x,w),Σϵ) where φ(x,w) is a feed-forward neural network with weights w mapping inputs x to a nφ- dimensional feature space. ALPaCA maintains a matrix-normal distribution over K, and thus results in a matrix-normal posterior distribution over K. This posterior inference may be performed exactly, and computed recursively. The matrix-normal distribution on the last layer results in a Gaussian posterior predictive density. Note that, as is typical in regression, ALPaCA only models the conditional density p(y|x), and assumes that p(x) is independent of the underlying task. The algorithm parameters θare the prior on the last layer, as well as the weights wof the neural network feature network φ. The posterior statistics ηencode the mean and variance of the Gaussian posterior distribution on the last layer weights. PCOC: Bayesian Meta-Learning for Classiﬁcation. In the classiﬁcation setting, one can obtain a similar Bayesian meta-learning algorithm by performing Gaussian discriminant analysis in a learned feature space. We refer to this novel approach to meta-learning for classiﬁcation as probabilistic clustering for online classiﬁcation (PCOC). Labeled input/class pairs (xt,yt) are processed by encoding the input through an embedding network zt = φ(xt; w), and performing Bayesian density estimation in this feature space for every class. Speciﬁcally, we assume a Categorical-Gaussian generative model in this embedding space, and impose the conjugate Dirichlet prior over the class 50 20 40 60 80 Time step 0 20 40Run length t=20 t=67 Figure 2: MOCA with ALPaCA on the sinusoid regression problem. Left: The belief over run length versus time. The intensity of each point in the plot corresponds to the belief in run length at the associated time. The red lines show the true changepoints. Middle, Right: Visualizations of the posterior predictive density at the times marked by blue lines in the left ﬁgure. The red line denotes the current function (task), and red points denote data from the current task. Green points denote data from previous tasks, where more faint points are older. By reasoning about task run-length, MOCA ﬁts the current sinusoid while avoiding negative transfer from past data, and resets to prior predictions when tasks switch. probabilities and a Gaussian prior over the mean for each class. This ensures the posterior remains Dirichlet-Gaussian, whose parameters can be updated recursively. The posterior parameters ηfor this algorithm are the mean and covariance of the posterior distribution on each class mean, as well as the counts of observations per class. The learner parameters θare the weights of the encoding network w, the prior parameters, and the covariance assumed for the observation noise. PCOC can be thought of a Bayesian analogue of prototypical networks [40]. 5 Related Work Online Learning, Continuous Learning, and Concept Drift Adaptation. A substantial literature exists on online, continual and lifelong learning [ 18, 6]. These ﬁelds all consider learning within a streaming series of tasks, wherein it is desirable to re-use information from previous tasks while avoiding negative transfer [12, 42]. Typically, continual learning assumes access to task segmentation information, whereas online learning does not [3]. Regularization approaches [26, 18, 28] have been shown to be an effective method for avoiding forgetting in continual learning. By augmenting the loss function for a new task with a penalty for deviation from the parameters learned for previous tasks, the regularizing effects of a prior are mimicked; in contrast we explicitly learn a prior over task weights that is meta-trained to be rapidly adaptive. Thus, MOCA is capable of avoiding substantial negative transfer by detecting task change, and rapidly adapting to new tasks. [3] loosen the assumption of task segmentation in continual learning and operate in a similar setting to that addressed herein, but they aim to optimize one model for all tasks simultaneously; in contrast, our work takes a meta-learning approach and aims to optimize a learning algorithm to quickly adapt to changing tasks. Meta-Learning for Continuous and Online Learning. In response to the slow adaption of contin- ual learning algorithms, there has been substantial interest in applying ideas from meta-learning to continual learning to enable rapid adaptation to new tasks. To handle streaming data, several works [31, 19] use a sliding window approach, wherein a ﬁxed amount of past data is used to condition the meta-learned model. As this window length is not reactive to task change, these models risk suffering from negative transfer. Indeed, MOCA may be interpreted as sliding window model, that actively infers the optimal window length. [ 32] and [24] aim to detect task changes online by combining mean estimation of the labels with MAML. However, these models are less expressive than MOCA (which maintains a full Bayesian posterior), and require task segmentation as test time. [36] employ gradient-based meta-learning to improve transfer between tasks in continual learning; in contrast MOCA works with any meta-learning algorithm. Empirical Bayes for Changepoint Models. Follow-on work to BOCPD [1] and the similar simulta- neous work of [9] has considered applying empirical Bayes to optimize the underlying predictive model, a similar problem to that addressed herein. In particular, [33] develop a forward-backward algorithm that allows closed-form max likelihood estimation of the prior for simple distributions via EM. [43] derive general-purpose gradients for hyperparameter optimization within the BOCPD model. MOCA pairs these ideas with neural network meta-learning models, and thus can leverage recent advances in automatic differentiation for gradient computation. 6Figure 3: Performance of MOCA with ALPaCA versus baselines in sinusoid regression (left) and the switching wheel contextual bandit problem (right). In the bandit problem, we evaluate performance as the regret of the model (compared to an optimal decision maker with perfect knowledge of switch times) as a percentage of the regret of the random agent, following previous work [37]. In both problems, lower is better. Conﬁdence intervals in this ﬁgure and throughout are 95%. Model Test NLL TOE 0.889 ±0.073 SW5 −3.032 ±0.058 SW10 −3.049 ±0.054 SW50 −3.061 ±0.054 COE −3.044 ±0.059 MOCA −3.291 ±0.074 0 20 40 60 80 x (ft) 0 10 20 30 40 50 y (ft) 25 50 75 x position (ft)x position (ft)x position (ft)x position (ft)x position (ft)x position (ft)x position (ft) 10 20 y position (ft)y position (ft)y position (ft)y position (ft)y position (ft)y position (ft)y position (ft) 0 50 100 150 Time step (dt = 0.04 s) 0 50 run length belief Figure 4: Left: Test NLL of MOCA + LSTM against baselines. Middle: Visualization of sample trajectory, segmented by color according to predicted task changes. We see that task changes visually correspond to different plays. Right: Trajectories plotted against time, together with MOCA’s belief over run length. Task switches (dashed gray) were placed where the MAP run length drops to a value less than 5. 6 Experimental Results We investigate the performance of MOCA in ﬁve problem settings: three in regression and two in classiﬁcation. Our primary goal is to characterize how effectively MOCA can enable meta-learning algorithms to perform without access to task segmentation. We compare against baseline sliding window models, which again use the same meta-learning algorithm, but always condition on the last ndata points, for n ∈{5,10,50}. These baselines are a competitive approach to learning in time-varying data streams [13] and have been applied meta-learning in time-varying settings [31]. We also compare to a “train on everything” model, which only learns a prior and does not adapt online, corresponding to a standard supervised learning approach. Finally, where possible, we compare MOCA against an “oracle” model that uses the same base meta-learning algorithm, but has access to exact task segmentation at train and test time, to explicitly characterize the utility of task segmentation. Due to space constraints, this section contains only core numerical results for each problem setting; further experiments and ablations are presented in the appendix. We ﬁnd by explicitly reasoning about task run-length, MOCA is able to outperform baselines across all the domains with a variety of base meta-learning algorithms and provide interpretable estimates of task-switches at test time. Sinusoid Regression. To characterize MOCA in the regression setting, we investigate the perfor- mance on a switching sinusoid problem adapted from [10], in which a task change corresponds to a re-sampled sinusoid phase and amplitude. Qualitative results are visualized for the sinusoid in Fig. 2. In this problem we pair MOCA with ALPaCA as it outperforms LSTM-based meta-learners. MOCA is capable of accurate and calibrated posterior inference with only a handful of data points, and is capable of rapidly identifying task change. Typically, it identiﬁes task change in one timestep, unless the datapoint happens to have high likelihood under the previous task as in Fig. 2d. Performance of MOCA against baselines is presented in Fig. 3 for all problem domains. For sinusoid (left), MOCA 7Figure 5: Performance of MOCA with PCOC on rainbow MNIST ( left) and miniImageNet (right). In both problems, higher is better. achieves performance close to the oracle model and substantially outperforms the sliding window approaches for all hazard rates. Wheel Bandit . Bandit problems have seen recent highly fruitful application of meta-learning algorithms [4, 45, 15]. We investigate the performance of MOCA (paired with ALPaCA) in the switching bandit problem, in which the reward function of the bandit undergoes discrete changes [14, 17, 30]. We extend the wheel bandit problem [ 37], a common benchmark for meta-learning algorithms [15, 34]. Details of the full bandit problem are provided in the appendix. In this problem, changepoint identiﬁcation is difﬁcult, as only a small subset of states contains information about whether the reward function has changes. Following [30], we use Thompson sampling for action selection. We use the notion of regret deﬁned in [14], in which the chosen action is compared to the action with the best mean reward at each time, with perfect knowledge of switches. As shown in [ 14], the sliding window baselines have strong theoretical guarantees on regret, as well as good empirical performance. Performance is plotted in Fig. 3. MOCA outperforms baselines for lower hazard rates. Detecting task switches requires observing a state close to the (changing) high-reward boundary, and at high hazard rates, the rapid task changes make identiﬁcation of changepoints difﬁcult, and we see that MOCA performance matches all the sliding windows in this regime. NBA Player Movement. To test MOCA on a real-world data with an unobserved switching latent task, we test it on predicting the movement of NBA players, whose intent may switch over time, from, e.g., running towards a position on the three-point line, to moving inside the key to recover a rebound. This changing latent state has made it a common benchmark for recurrent predictive models [22, 29]. In our experiments, the input xis an individual player’s current position on the court(xt,yt), and the label yt = xt+1 −xt is the step the player takes at that time. For this problem, we pair MOCA with the LSTM meta-learner, since recurrent models are well suited to this task and we saw better performance relative to ALPaCA. We add a “condition on everything” (COE) baseline which updates a single set of posterior statistics ηusing all available data, as the LSTM can theoretically learn to only consider relevant data. Nevertheless, we ﬁnd that that MOCA’s explicit reasoning over task length yields better performance over COE and other baselines, as shown in Fig. 4. While true task segmentation is unavailable for this data, we see in the ﬁgure that MOCA’s predictions of task changes correspond intuitively to changes in the player’s intent. Rainbow MNIST. In the classiﬁcation setting, we apply MOCA with PCOC to the Rainbow MNIST dataset of [11]. In this dataset, MNIST digits have been perturbed via a color change, rotation, and scaling; each task corresponds to a unique combination of these transformations. Relative to baselines, MOCA approaches oracle performance for low hazard rates, due in part to the fact that task change can usually be detected prior to prediction via a change in digit color. Seven colors were used, so with probability 6/7, MOCA has a strong indicator of task change before observing the image class. miniImageNet. Finally, we investigate the performance of MOCA with PCOC on the miniImageNet benchmark task [44]. This dataset consists of 100 ImageNet categories [ 7], each with 600 RGB images of resolution 84×84. In our continual learning setting, we associate each class with a semantic label that is consistent between tasks. As ﬁve-way classiﬁcation is standard for miniImageNet [44, 40], we split the miniImageNet dataset in to ﬁve approximately balanced “super-classes.\" For example, 8one super-class is dog breeds, while another is food, kitchen and clothing items; details are provided in the appendix. Each new task corresponds to resampling a particular class from each super-class from which to draw inputs x; the labels yremain the ﬁve super-classes, enabling knowledge re- use between classes. This corresponds to a continual learning scenario in which each super-class experiences distributional shift over time. Fig. 5 shows that MOCA outperforms baselines for all hazard rates. 7 Discussion and Conclusions Future Work. In this work, we address the case in which tasks are sampled i.i.d. from a (typically continuous) distribution, and thus knowledge re-use adds marginal value. However, many domains may have tasks that can reoccur, or temporal dynamics to task evolution and thus data efﬁciency may be improved by re-using information for previous tasks. Previous work [32, 24, 27] has addressed the case in which tasks reoccur in both meta-learning and the BOCPD framework, and thus knowledge (in the form of a posterior estimate) may be re-used. Broadly, moving beyond the assumption of i.i.d. tasks to tasks having associated dynamics [2] represents a promising future direction. Conclusions. MOCA enables the application of existing meta-learning algorithms to problems without task segmentation, such as the problem setting of continual learning. We ﬁnd that by leveraging a Bayesian perspective on meta-learning algorithms and augmenting these algorithms with a Bayesian changepoint detection scheme to automatically detect task switches within time series, we can achieve similar predictive performance when compared to the standard task-segmented meta-learning setting, without the often prohibitive requirement of supervised task segmentation. Funding Disclosure and Acknowledgments James Harrison was supported in part by the Stanford Graduate Fellowship and the National Sciences and Engineering Research Council of Canada (NSERC). The authors were partially supported by an Early Stage Innovations grant from NASA’s Space Technology Research Grants Program, and by DARPA, Assured Autonomy program. The authors wish to thank Matteo Zallio for help in the design of ﬁgures. Broader Impact Our work provides a method to extend meta-learning algorithms beyond the task-segmented case, to the time series series domain. Equivalently, our work extends core methods in changepoint detection, enabling the use of highly expressive predictive models via empirical Bayes. This work has the potential to extend the domain of applicability of both of these methods. Standard meta-learning relies on a collection of datasets, each corresponding to discrete tasks. A natural question is how such datasets are constructed; in many cases, these datasets rely on segmentation of time series data by experts. Thus, our work has the potential to make meta-learning algorithms applicable to problems that, previously, would have been too expensive or impossible to segment. Moreover, our work has the potential to improve the applicability of changepoint detection methods to difﬁcult time series forecasting problems. While MOCA has the potential to expand the domain of problems addressable via meta-learning, this has the effect of amplifying the risks associated with these methods. Meta-learning enables efﬁcient learning for individual members of a population via leveraging empirical priors. There are clear risks in few-shot learning generally: for example, efﬁcient facial recognition from a handful of images has clear negative implications for privacy. Moreover, while there is promising initial work on fairness for meta-learning [39], we believe considerable future research is required to understand the degree to which meta-learning algorithms increase undesirable bias or decrease fairness. While it is plausible that ﬁne-tuning to the individual results in reduced bias, there are potential unforeseen risks associated with the adaptation process, and future research should address how bias is potentially introduced in this process. Relative to decision making rules that are ﬁxed across a population, algorithms which ﬁne-tune decision making to the individual present unique challenges in analyzing fairness. Further research is required to ensure that the adaptive learning enabled by algorithms such as MOCA do not lead to unfair outcomes. 9References [1] Ryan Prescott Adams and David JC MacKay. Bayesian online changepoint detection. arXiv:0710.3742, 2007. [2] Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. International Conference on Learning Representations (ICLR), 2018. [3] Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. Com- puter Vision and Pattern Recognition (CVPR), 2019. [4] Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits. arXiv:2005.08531, 2020. [5] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁcation. International Conference on Learning Representations (ICLR), 2019. [6] Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, 2016. [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. Computer Vision and Pattern Recognition (CVPR), 2009. [8] Bradley Efron and Carl Morris. Stein’s estimation rule and its competitors—an empirical Bayes approach. Journal of the American Statistical Association, 1973. [9] Paul Fearnhead and Zhen Liu. On-line inference for multiple changepoint problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2007. [10] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta- tion of deep networks. International Conference on Machine Learning (ICML), 2017. [11] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. International Conference on Machine Learning (ICML), 2019. [12] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 1999. [13] João Gama, Indr˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey on concept drift adaptation. ACM computing surveys (CSUR), 2014. [14] Aurélien Garivier and Eric Moulines. On upper-conﬁdence bound policies for switching bandit problems. International Conference on Algorithmic Learning Theory, 2011. [15] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S.M. Ali Eslami, and Yee Whye Teh. Neural processes. International Conference on Machine Learning (ICML), 2018. [16] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efﬁcient online Bayesian regression. Workshop on the Algorithmic Foundations of Robotics (WAFR), 2018. [17] Cédric Hartland, Nicolas Baskiotis, Sylvain Gelly, Michèle Sebag, and Olivier Teytaud. Change point detection and meta-bandits for online learning in dynamic environments. 2007. [18] Elad Hazan. Introduction to online convex optimization. Foundations and Trends® in Opti- mization, 2016. [19] Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh, and Razvan Pascanu. Task agnostic continual learning via meta learning. arXiv preprint arXiv:1906.05201, 2019. [20] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997. [21] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. International Conference on Artiﬁcial Neural Networks (ICANN, 2001. [22] Boris Ivanovic, Edward Schmerling, Karen Leung, and Marco Pavone. Generative modeling of multimodal multi-human behavior. IEEE International Conference on Intelligent Robots and Systems (IROS), 2018. [23] Khurram Javed and Martha White. Meta-learning representations for continual learning. Neural Information Processing Systems (NeurIPS), 2019. 10[24] Ghassen Jerfel, Erin Grant, Thomas L Grifﬁths, and Katherine Heller. Online gradient-based mixtures for transfer modulation in meta-learning. Neural Information Processing Systems (NeurIPS), 2019. [25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015. [26] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS), 2017. [27] Jeremias Knoblauch and Theodoros Damoulas. Spatio-temporal bayesian on-line changepoint detection with model selection. International Conference on Machine Learning (ICML), 2018. [28] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2017. [29] Scott W Linderman, Andrew C Miller, Ryan P Adams, David M Blei, Liam Paninski, and Matthew J Johnson. Recurrent switching linear dynamical systems. arXiv:1610.08466, 2016. [30] Joseph Mellor and Jonathan Shapiro. Thompson sampling in switching environments with bayesian online change detection. Artiﬁcial Intelligence and Statistics (AISTATS), 2013. [31] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta- reinforcement learning. International Conference on Learning Representations (ICLR), 2019. [32] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual adaptation for model-based RL. International Conference on Learning Representa- tions (ICLR), 2019. [33] Ulrich Paquet. Empirical Bayesian change point detection. Graphical Models, 2007. [34] Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. International Conference on Learning Representations (ICLR), 2018. [35] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenen- baum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. International Conference on Learning Representations (ICLR), 2018. [36] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Ger- ald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interference. International Conference on Learning Representations (ICLR), 2019. [37] Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An em- pirical comparison of bayesian deep networks for thompson sampling. International Conference on Learning Representations (ICLR), 2018. [38] Y Saatci, R Turner, and CE Rasmussen. Gaussian process change point models. International Conference on Machine Learning (ICML), 2010. [39] Dylan Slack, Sorelle Friedler, and Emile Givental. Fair meta-learning: Learning how to learn fairly. arXiv:1911.04336, 2019. [40] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Neural Information Processing Systems (NeurIPS), 2017. [41] Charles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. Third Berkeley symposium on Mathematical statistics and Probability, 1956. [42] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer, 2012. [43] Ryan Turner, Yunus Saatci, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. NeurIPS Workshop on Nonparametric Bayes, 2009. [44] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Neural Information Processing Systems (NeurIPS), 2016. [45] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763, 2016. [46] Robert C Wilson, Matthew R Nassar, and Joshua I Gold. Bayesian online learning of the hazard rate in change-point problems. Neural Computation, 2010. 11A MOCA Algorithmic Details In this section, we derive the Bayesian belief updates used in MOCA. As in the paper, we will use bt to refer to the belief before observing data at that timestep, (xt,yt). Note that bt is a discrete distribution with support over rt ∈{0,...,t −1}. We write ηt[r] to refer to the posterior parameters of the meta-learning algorithm conditioned on the past rdata points, (xt−r+1:t,yt−r+1:t). At time t, the agent ﬁrst observes the input xt, then makes a prediction p(yt |x1:t,y1:t−1), and subsequently observes yt. Generally, the latent task can inﬂuence both the marginal distribution of the input, p(xt |x1:t−1,y1:t−1) as well as the conditional distribution p(yt |x1:t,y1:t−1). Thus, the agent can update its belief over run lengths once after observing the input xt, and again after observing the label yt. We will use bt(rt |xt) = p(rt |x1:t,y1:t−1) to represent the updated belief over run length after observing only xt, and bt(rt |xt,yt) = p(rt |x1:t,y1:t) to represent the fully updated belief over rt after observing yt. Finally, we will propagate this forward in time according to our assumptions on task dynamics to compute bt+1(rt+1), which is used in the subsequent timestep. To derive the Bayesian update rules, we start by noting that the updated posterior is proportional to the joint density, bt(rt |xt) = p(rt |x1:t,y1:t−1) (7) = Z−1p(rt,xt |x1:t−1,y1:t−1) = Z−1p(xt |x1:t−1,y1:t−1,rt)bt(rt) (8) where the normalization constant Zcan be computed by summing over the ﬁnite support of bt−1(rt). Importantly, this update requires pθ(xt |ηt−1[rt]), the base meta-learning algorithm’s posterior predictive density over the inputs. Within classiﬁcation, this density is available for generative models, and thus a generative approach is favorable to a discriminative approach within MOCA. In regression, it is uncommon to estimate the distribution of the independent variable. We take the same approach in this work and assume that xt is independent of the task for regression problems, in which case bt(rt |xt) = bt(rt). Next, upon observing yt, we can similarly factor the belief over run lengths for the next timestep, bt(rt |xt,yt) ∝pθ(yt |xt,ηt−1[rt])bt(rt |xt), (9) which can again easily be normalized. Finally, we must propagate this belief forward in time: bt+1(rt+1) = p(rt+1 |x1:t,y1:t) = ∑ rt p(rt+1,rt |x1:t,y1:t) = ∑ rt p(rt+1 |rt)bt(rt |xt,yt). where we have exploited the assumption that the changes in task, and hence the evolution of run length rt, happen independently of the data generation process. The conditional run-length distribution p(rt+1 |rt) is deﬁned by our model of task evolution. Recall that we assume that the task switches with ﬁxed probability λ, the hazard rate. Thus, p(rt+1 = 0 |rt) = λfor all rt, implying bt+1(rt+1 = 0) = λ. Conditioned on the task remaining the same, rt+1 = k> 0 and rt = k−1. Thus, p(rt+1 = k|rt) = (1 −λ)1{rt = k−1}implying bt+1(rt+1 = k) = (1 −λ)bt(rt = k−1 |xt,yt). (10) This gives the time-propagation update step, as in equation (4), used by MOCA. B Base Meta-Learning Algorithm Details In the following subsections, we describe how each of the base meta-learning algorithms we use for the experiments ﬁt into the MOCA framework. Speciﬁcally, we highlight (1) which parameters θare optimized, (2) the statistics ηfor each algorithm, (3) how these statistics deﬁne a posterior predictive distribution pθ(ˆyt+1 |x1:t+1,y1:t), and ﬁnally (4) the recursive update rule ηt = h(ηt−1,xt,yt) used to incorporate a new labeled example. 12B.1 LSTM Meta-Learner For our LSTM meta-learner, we follow the architecture of [21]. The LSTM input is the concatenation of the current encoded input zt = φ(xt,w) and the label from the past timestep yt−1. In this way, through the LSTM update process, the hidden state can process a sequence of input/label pairs and encode statistics of the posterior distribution in the hidden state. Thus, the necessary statistics to make predictions after observing x1:t and y1:t are ηt = [ht,ct,yt]. Given a new example x,y, and the posterior at time t, the updated posterior can be computed recursively ht+1,ct+1 = LSTM([x,yt],ht,ct) (11) yt+1 = y (12) where LSTM([x,yt],h,c) carries out the LSTM update rules for hidden and cell states given input [x,yt]. We depart from the architecture proposed in [21] and include both the hidden state ht and the current encoded input zt as input to the decoder f which outputs the statistics of the posterior predictive distribution ˆyt ∼N(µt,Σt). µt,st = f(ht,zt,wf) (13) Σ = diag(exp(st)) (14) where f is a single hidden layer feed-forward network with weightswf. This functional form ensures that the covariance matrix of the posterior predictive remains positive deﬁnite. By including zt as input to the decoder, we lessen the information that needs to be stored in the hidden state, as it no longer needs to also encode the posterior predictive density for y|xt, just the posterior on the latent task. This was found to substantially improve performance and learning stability. The parameters that are optimized during meta-training are the weights of the encoder and decoder w,wf, as well as the parameters of the LSTM gates. The LSTM meta-learner makes few assumptions on the structure of the probabilistic model of the unobserved task parameter. For example, it does not by design satisfy the exchangeability criterion ensuring that the order of the context data does not change the posterior. This makes it a ﬂexible algorithm that, e.g., can handle unobserved latent states that have dynamics (both slow varying and switching behavior, in theory). However, empirically we ﬁnd the lack of this structure can make these models harder to train. Indeed, the more structured algorithms introduced in the following sections outperformed the LSTM meta-learner on many of our experiments. B.2 ALPaCA ALPaCA [16] is a meta-learning approach for which the base learning model is Bayesian linear regression in a learned feature space, such that y|x∼N(KTφ(x,w),Σϵ). We ﬁx the prior K ∼MN( ¯K0,Σϵ,Λ−1 0 ). In this matrix-normal prior, ¯K0 ∈Rnφ×ny is the prior mean and Λ0 is a nφ ×nφ precision matrix (inverse of the covariance). Given this prior and data model, the posterior may be recursively computed as follows. First, we deﬁne Qt = Λ−1 t ¯Kt. Then, the one step posterior update is Λ−1 t+1 = Λ−1 t −(Λ−1 t φ(xt+1))(Λ−1 t φ(xt+1))T 1 + φT(xt+1)Λ−1 t φ(xt+1) , (15) Qt+1 = yt+1φT(xt+1) + Qt (16) and the posterior predictive distribution is pθ(ˆyt+1 |x1:t+1,y1:t) = N(µ(xt+1),Σ(xt+1)), (17) where µ(xt+1) = (Λ−1 t Qt)Tφ(xt+1) and Σ(xt+1) = (1 + φT(xt+1)Λ−1 t φ(xt+1))Σϵ. To summarize, ALPaCA is a meta learning model for which the posterior statistics are ηt = {Qt,Λ−1 t }, and the recursive update rule h(x,y,η) is given by (16). The parameters that are meta-learned are the prior statistics, the feature network weights, and the noise covariance: θ = {¯K0,Λ0,w,Σϵ}. Note that, as is typical in regression, ALPaCA only models the conditional density p(y|x), assuming that p(x) is independent of the underlying task. 13B.3 PCOC In PCOC we process labeled input/class pairs (xt,yt) by encoding the input through an embedding network zt = φ(xt; w), and performing Bayesian density estimation for every class. Speciﬁcally, we assume a Categorical-Gaussian generative model in this embedding space, and impose the conjugate Dirichlet prior over the class probabilities and a Gaussian prior over the mean for each class, yt ∼Cat(p1,...,p ny), p 1,...,p ny ∼Dir(α0), zt |yt ∼N(¯zyt,Σϵ,yt), ¯zyt ∼N(µyt,0,Λ−1 yt,0). Given labeled context data (xt,yt), the algorithm updates its belief over the Gaussian mean for the corresponding class, as well as its belief over the probability of each class. As with ALPaCA, these posterior computations can be performed through closed form recursive updates. Deﬁning qi,t = Λi,tµi,t, we have αt = αt−1 + 1yt, qyt,t = qyt,t−1 + Σ−1 ϵ,ytφ(xt), Λyt,t = Λyt,t−1 + Σ−1 ϵ,yt (18) where 1i denotes a one-hot vector with a one at index i. Terms not related to class yt are left unchanged in this recursive update. Given this set of posterior parameters ηt = {αt,q1:J,t,Λ1:J,t}, the posterior predictive density in the embedding space can be computed as p(y|ηt) = αy,t/(∑J i=1αi,t) p(z,y |ηt) = p(y|ηt)N(z; Λ−1 y,tqy,t,Λ−1 y,t + Σϵ,y) where N(z; µ,Σ) denotes the Gaussian pdf with mean µand covariance Σ evaluated at z. Applying Bayes rule, the posterior predictive on yt+1 given zt+1 is p(ˆy|x1:t+1,y1:t) = p(zt+1,ˆy|ηt)∑ y′ p(zt+1,y′|ηt), (19) where zt+1 = φ(xt+1). This generative modeling approach also allows computing p(zt+1 |ηt) by simply marginalizing out yfrom the joint density of p(z,y), p(zt+1 |ηt) = J∑ y=1 p(y)N(zt+1; µt,Λ−1 y,t + Σϵ,y) As this only depends on the input x, we can use this likelihood within MOCA to update the run length belief upon seeing xt and before predicting ˆyt. In summary, PCOC leverages Bayesian Gaussian discriminant analysis, meta-learning the parameters θ= {α0,q1:J,0,Λ1:J,0,w,Σϵ,1:J}for efﬁcient few-shot online classiﬁcation. In practice, we assume that all the covariances are diagonal to limit memory footprint of the posterior parameters. Discussion. PCOC extends a line of work on meta-classiﬁcation based on prototypical networks [40]. This framework maps the context data to an embedding space, after which it computes the centroid for each class. For a new data point, it models the probability of belonging to each class as the softmax of the distances between the embedded point and the class centroids, for some distance metric. For Euclidean distances (which the authors focus on), this corresponds to performing frequentist estimation of class means, under the assumption that the variance matrix for each class is the identity matrix2. Indeed, this corresponds to the cheapest-to-evaluate simpliﬁcation of PCOC. [35] propose adding a class-dependent length scale (which is a scalar), which corresponds to meta- learning a frequentist estimate of the variance for each class. Moreover, it corresponds to assuming a variance that takes the form of a scaled identity matrix. Indeed, assuming diagonality of the covariance matrix results in substantial performance improvement as the matrix inverse may be performed element-wise. This reduces the numerical complexity of this operation in the (frequently high-dimensional) embedding space from cubic to linear. In our implementation of MOCA, we assume diagonal covariances throughout, resulting in comparable computational complexity to the different ﬂavors of prototypical networks. If one were to use dense covariances, the computational performance decreases substantially (due to the necessity of expensive matrix inversions), especially in high dimensional embedding spaces. 2[40] discuss this correspondence, as they outline how the choice of metric corresponds to a different assumptions on the distributions in the embedding space. 14In contrast to this previous work, PCOC has several desirable features. First, both [ 40] and [35] make the implicit assumption that the classes are balanced, whereas we perform online estimation of class probabilities via Dirichlet posterior inference. Beyond this, our approach is explicitly Bayesian, and we maintain priors over the parameters that we estimate online. This is critical for utilization in the MOCA framework. Existence of these priors allows “zero-shot” learning—it enables a model to classify incoming data to a certain class, even if no data belonging to that class has been observed within the current task. Finally, because the posteriors concentrate (the predictive variance decreases as more data is observed), we may better estimate when a change in the task has occurred. We also note that maximum likelihood estimation of Gaussian means is dominated by the James-Stein estimator [41], which shrinks the least squares estimator toward some prior. Moreover, the James-Stein estimator paired with empirical Bayesian estimation of the prior—which is the basis for Bayesian meta-learning approaches such as ALPaCA and PCOC—has been shown to be a very effective estimator in this problem setting [8]. C Experimental Details C.1 Problem Settings Sinusoid. To test the performance of the MOCA framework combined with ALPaCA for the regression setting, we investigate a switching sinusoid regression problem. The standard sinusoid regression problem, in which randomly sampled phase and amplitude constitute a task, is a standard benchmark in meta-learning [10]. Moreover, a switching sinusoid problem is a popular benchmark in continuous learning [19, 23]. Each task consists of a randomly sampled phase in the range [0,π] and amplitude in [0.1,5]. This task was investigated for varying hazard rates. For the experiments in this paper, samples from the sinusoid had additive zero-mean Gaussian noise of variance 0.05. Wheel Bandit. As a second, more practical regression example, we investigate a modiﬁed version of the wheel bandit presented in [37]. This bandit has been used to evaluate several Bayesian meta- learning algorithms [15, 34], due to the fact that the problem requires effective exploration (which itself relies on an accurate model of the posterior). We will outline the standard problem, and then discuss our modiﬁed version. The wheel problem is a contextual bandit problem in which a state x = [ x1,x2]T is sampled uniformly from the unit ball. The unit ball is split into two regions according to a radius δ∈[0,1], and into four quadrants (for details, see [37]). There are ﬁve actions, a0,...,a 4. The ﬁrst, a0 always results in reward rm. The other four actions each have one associated quadrant. For state xin quadrant 1, with ∥x∥>δ, a1 returns rh, and all other actions return reward rl. Actions a2,a3,a4 all return rl. If ∥x∥≤ δ, a1 also returns rl. In quadratic 2, a2 returns rh for x>δ, and so on. Critically, E[rl] <E[rm] <E[rh]. In summary, a0 always returns a medium reward, whereas actionsa1,...,a 4 return high reward for the correct quadrant outside of the (unknown) radius δ, and otherwise return low reward. We make several modiﬁcations to the setting to be better suited to the switching bandit setting. The standard wheel bandit problem is focused on exploration over long horizons. In the standard problem, the radius of the wheel is ﬁxed, and an algorithm must both learn the structure of the problem and infer the radius. In meta-learning-based investigations of the problem, a collection of wheel bandit problems with different radii are provided for training. Then, at test time, a new problem with a previously unseen radius is provided, an the decision-making agent must correctly infer the radius. In our switching setting, the radius of the wheel changes sharply, randomly in time. The radius was sampled δ∼U[0,1] in previous work [15, 34]. In our setting, with probability λat each time step (the hazard), the radius is re-sampled from this uniform distribution. Thus, the agent must constantly be inferring the current radius. Note that in this problem, only a small subset of states allow for meaningful exploration. Indeed, if the problem switches from radius δ1 to δ2, only xsuch that ∥x∥∈ [δ1,δ2] provides information about the switch. Thus, this problem provides an interesting domain in which changepoint detection is difﬁcult and necessarily temporally delayed. In addition to changing the sampling of the radius, we also change the reward function. As in [37], the rewards are deﬁned as ri ∼N (µi,σ2) for i = l,m,h . In [ 37, 15, 34], µl = 1.0, µm = 1.2, and µ3 = 50.0; σ = 0.01. This reward design results in agents necessarily needing to accurately identifying the radius of the problem, as for states outside of this value they may take the high reward action, and otherwise the agent takes action a0, resulting in reward of (approximately) 1.2. While this results in an interesting exploration versus exploitaion problems in the long horizon, the relatively 15greedy strategy of always choosing the action corresponding the quadrant of the state (potentially yielding high reward) performs well over short horizons. Thus, we modiﬁed the reward structure to make the shorter horizon problem associated with the switching bandit more interesting. In particular, we set µl = 0 .0, µm = 1 .0, µ3 = 2 .0 and σ = 0 .5. Thus, while the long horizon exploration problem is less interesting, a greedy agent performs worse over the short horizon. Moreover, the substantially higher noise variance increases the difﬁculty of the radius inference problem as well as the changepoint inference problem. NBA Player Movement. The behavior of basketball players is well described as a sequence of distinct plays (\"tasks\"), e.g. running across the court or driving in towards the basket. As such, predicting a player’s movement requires To generate data, we extracted 8 second trajectories of player movement sampled at 12.5 Hz from games from the 2015-2016 NBA season3. For the training data, we used trajectories from two games randomly sampled from the dataset: the November 11th, 2015 game between the Orlando Magic and the Chicago Bulls, and the December 12, 2015 game between the New Orleans Pelicans and the Chicago Bulls. The validation data was extracted from the November 7th, 2015 game between the New Orleans Pelicans and the Dallas Mavericks. The test set was trajectories from the November 6th game between the Milwaukee Bucks and the New York Knicks. The input xt was the player’s(x,y) position at time t, scaled down by a factor of 50. The labels were the unscaled changes in position, yt = 50(xt+1 −xt). The scaling was performed to convert the inputs, with units of feet and taking on values ranging from 0-100, to values that are more amenable for training with standard network initialization. Rainbow MNIST . The Rainbow MNIST dataset (introduced in [ 11]) contains 56 different color/scale/rotation transformations of the MNIST dataset, where one transformation constitutes a task. We split this dataset into a train set of 49 transformations and a test set of 7. For hyperparameter optimization, we split the train set into a training set of 42 transformations and a validation of 7. However, because the dataset represents a fairly small amount of tasks (relative to the sinusoid problem, which has inﬁnite), after hyperparameters were set we trained on all 49 tasks. We found this notably improved performance. Note that the same approach was used in [40]. miniImageNet. We use the miniImageNet dataset of [44], a standard benchmark in few-shot learning. However, the standard few-shot learning problem does not require data points to be assigned to a certain class label. Instead, given context data, the goal is to associated the test data with the correct context data. We argue that this problem setting is implausible for the continual learning setting: while observing a data stream, you are also inferring the set of possible labels. Moreover, after a task change, there is no context data to associate a new point with. Therefore we instead assume a known set of classes. We group the 100 classes of miniImageNet in to ﬁve super-classes, and perform ﬁve-way classiﬁcation given these. These super-classes vary in intra-class diversity of sub-classes: for example, one of the super-class is entirely composed of sub-classes that are breeds of dogs, while another corresponds to buildings, furniture, and household objects. Thus, the strength of the prior information for each super-class varies. Moreover, the intra-class similarities are quite weak, and thus generalization from the train set to the test set is difﬁcult and few-shot learning is still necessary and beneﬁcial. The super-classes are detailed in table 1. The super-classes are roughly balanced in terms of number of classes contained. Each task correspond to sampling a class from within each super-class, which was ﬁxed for the duration of that task. Each super-class was sampled with equal probability. C.2 Baselines Four baselines were used, described below: • Train on Everything: This baseline consists of ignoring task variation and treating the train- ing timeseries as one dataset. Note that many datasets contain latent temporal information that is ignored, and so this approach is effectively common practice. • Condition on Everything: This baseline maintains only one set of posterior statistics and continuously updates them with all past data, ηt = f(x1:t,y1:t). For recurrent network based meta-learning algorithms like the LSTM meta-learner, it is possible that the LSTM can learn to detect a task switch and reset automatically. Thus, we use this baseline only in experiments with the LSTM meta-learner to highlight how MOCA’s principled Bayesian 3The data was accessed and processed using the scripts provided here: https://github.com/ sealneaward/nba-movement-data 16Class Description Train/Val/Test Synsets 1 Non-dog animals Train n01532829, n01558993, n01704323, n01749939, n01770081, n01843383, n01910747, n02074367, n02165456, n02457408, n02606052, n04275548 Validation n01855672, n02138441, n02174001 Test n01930112, n01981276, n02129165, n02219486, n02443484 2 Dogs, foxes, wolves Train n02089867, n02091831, n02101006, n02105505, n02108089, n02108551, n02108915, n02111277, n02113712, n02120079 Validation n02091244, n02114548 Test n02099601, n02110063, n02110341, n02116738 3 Vehicles, musical instruments, nature/outdoors Train n02687172, n02966193, n03017168, n03838899, n03854065, n04251144, n04389033, n04509417, n04515003, n04612504, n09246464, n13054560 Validation n02950826, n02981792, n03417042, n03584254, n03773504, n09256479 Test n03272010, n04146614 4 Food, kitchen equipment, clothing Train n02747177, n02795169, n02823428, n03047690, n03062245, n03207743, n03337140, n03400231, n03476684, n03527444, n03676483, n04596742, n07584110, n07697537, n07747607, n13133613 Validation n03770439, n03980874 Test n03146219, n03775546, n04522168, n07613480 5 Building, furniture, household items Train n03220513, n03347037, n03888605, n03908618, n03924679, n03998194, n04067472, n04243546, n04258138, n04296562, n04435653, n04443257, n04604644, n06794110 Validation n02971356, n03075370, n03535780 Test n02871525, n03127925, n03544143, n04149813, n04418357 Table 1: Our super-class groupings for miniImageNet experiments. runlength estimation serves to add a useful inductive bias in settings with switching tasks, and leads to improved performance even in models that may theoretically learn the same behavior. • Oracle: In this baseline, the same ALPaCA and PCOC models were used as in MOCA, but with exact knowledge of the task switch times. Note that within a regret setting, one typically compares to the best achievable performance. The oracle actually outperforms the best achieveable performance in this problem setting, as it takes at least one data point (and the associated prediction, on which loss is incurred) to become aware of the task variation. • Sliding Window: The sliding window approach is commonly used within problems that exhibit time variation, both within meta-learning [31] and continual learning [19, 13]. In this approach, the last ndata points are used for conditioning, under the expectation that the most recent data is the most predictive of the observations in the near future. Typically, some form of validation is used to choose the window length, n. As MOCA is performing a form of adaptive windowing, it should ideally outperform any ﬁxed window length. We compare to three window lengths (n= 5,10,50), each of which are well-suited to part of the range of hazard rates that we consider. C.3 Training Details The training details are described below for each problem. For all problems, we used the Adam [25] optimizer. Sinusoid. A standard feedforward network consisting of two hidden layers of 128 units was used with ReLU nonlinearities. These layers were followed by a 32 units layer and another tanh nonlinearity. Finally, the output layer (for which we learn a prior) was of size 32 ×1. The same architecture was used for all baselines. This is the same architecture for sinusoid regression as was used in [16] (with the exception of using ReLU nonlinearities instead of all tanh nonlinearities). The following parameters were used for training: • Learning rate: 0.02 17• Batch size: 50 • Batch length: 100 • Train iterations: 7500 Batch length here corresponds to the number of timesteps in each training batch. Note that longer batch lengths are necessary to achieve good performance on low hazard rates, as short batch lengths artiﬁcially increase the hazard rate as a result of the assumption that each batch begins with a new task. The learning rate was decayed every 1000 training iterations. We allowed the noise variance to be learned by the model. This, counter-intuitively, resulted in a substantial performance improvement over a ﬁxed (accurate) noise variance. This is due to a curriculum effect, where the model early one increases the noise variance and learns roughly accurate features, followed by slowly decreasing the noise variance to the correct value. Wheel Bandit. For all models, a feedforward network consisting of four hidden layers with ReLU nonlinearities was used. Each of these layers had 64 units, and the output dimension of the network was 100. There was no activation used on the last layer of the network. The actions were encoded as one-hot and passed in with the two dimensional state as the input to the network (seven dimensional input in total). The following parameters were used for training: • Learning rate: 0.005 • Batch size: 15 • Batch length: 100 • Train iterations: 2000 and the learning rate was decayed every 500 training iterations. We allow the noise variance to be learned by the model. We use the same amount of training data as was used in [15]: 64 ×562 samples. In [15], this was 64 different bandits, each with 562 data points. We use the same amount of data, but generated as one continuous stream with the bandit switching according to the hazard rate. We use a validation set of size 16 ×562, also generated as one trajectory, but did not use any form of early termination based on the validation set. In [37, 15] data was collected by random action sampling. To generate a dataset that matches the test conditions slightly better, we instead sample a random action with probability 0.5, and otherwise sample the action correspond to the quadrant in which the state was sampled. This results in more training data in which high rewards are achieved. This primarily resulted in smoother training. The combined MOCA and ALPaCA models provide a posterior belief over the reward. This posterior must be mapped to an action selection at each time that sufﬁciently trades off greedy exploitation (maximizing reward) and exploration (information gathering actions). A common and effective heuristic in the bandit literature is Thompson sampling, in which a reward function is sampled from the posterior distribution at each time, and this sampled function is optimized over actions. This approach was applied in the changing bandit setting by [30]. Other common approaches to action selection typically rely on some form of optimism, in which the agent aims to explore possible reward functions that may perform better than the expectation of the posterior. These methods typically use concentration inequalities to derive an upper bound on the reward function. These methods have been applied in switching bandits in [14] and others. We follow [30] and use Thompson sampling the main experimental results, primarily due to its simplicity (and thus ease of reproduction, for the sake of comparison). However, because the switching rate between reward functions is relatively high, it is likely that optimistic methods (which typically have a short-term bias) would outperform Thompson sampling. As the action sampling is not a core contribution of the paper, we use Thompson sampling for simplicity. Moreover, this approach meshes well with the Gaussian mixture posterior predictive (which is easily sampled from). For completeness, we present experiments in section D in which we investigate optimistic action selection methods. NBA Player Movement. For this experiment, we used the LSTM meta-learner, with the encoder φ(x,w) deﬁned as a 3 hidden layer feedforward network with a hidden layer size of 128, and a feature dimension nφ = 32. The LSTM had a dimension of 64, and used a single hidden layer feedforward network as the decoder. ALPaCA did not perform as well as the LSTM model here; we hypothesize that this is due to the LSTM model being able to account for unobserved state variables that change with time, in contrast to ALPaCA, which assumes all unobserved state variables are task parameters and hence static for the duration of a task. 18Figure 6: The performance of MOCA with ALPaCA on the sinusoid regression problem. Bottom: The belief over run length versus time. The intensity of each point in the plot corresponds to the belief in run length at the associated time. The red lines show the true changepoints. Top: Visualizations of the posterior predictive density at the times marked by blue dotted lines in the bottom ﬁgure. The red line denotes the current function (task), and red points denote data from the current task. Green points denote data from previous tasks, where more faint points are older. a) A visualization of the posterior at an arbitrary time. b) The posterior for a case in which MOCA did not successfully detect the changepoint. In this case, it is because the pre- and post-change tasks (corresponding to ﬁgure a and b) are very similar. c) An instance of a multimodal posterior. d) The changepoint is initially missed due to the data generated from the new task having high likelihood under the previous posterior. e) After an unlikely data point, the model increases its uncertainty as the changepoint is detected. The following parameters were used for training: • Learning rate: 0.01 • Batch size: 25 • Batch length: 150 • Train iterations: 5000 The learning rate was decayed every 1000 training iterations. Rainbow MNIST. In our experiments, we used the same architecture as was used as in [40, 44]. It is often unclear in recent work on few-shot learning whether performance improvements are due to improvements in the meta-learning scheme or the network architecture used (although these things are not easily disentangled). As such, the architecture we use in this experiment provides fair comparison to previous few-shot learning work. This architecture consists of four blocks of 64 3 ×3 convolution ﬁlters, followed by a batchnorm, ReLU nonlinearity and 2 ×2 max pool. On the last conv black, we removed the batchnorm and the nonlinearity. For the 28 ×28 Rainbow MNIST dataset, this encoder leads to a 64 dimensional embedding space. For the “train on everything” baseline, we used the same architecture followed by a fully connected layer and a softmax. This architecture is standard for image classiﬁcation and has a comparable number of parameters to our model. We used a diagonal covariance factorization within PCOC, substantially reducing the number of terms in the covariance matrix for each class and improving the performance of the model (due to the necessary inversion of the posterior predictive covariance). We learned a prior mean and variance for each class, as well as a noise covariance for each class (again, diagonal). We also ﬁxed the Dirichlet priors to be large, effectively imbuing the model with the knowledge that the classes were balanced. The following parameters were used for training: 19Figure 7: Left: A visualization of samples from the reward function for randomly sampled states and action a1. Middle: The mean of the reward function posterior predictive distribution at time t = 135in an evaluation run (hazard 0.02). Right: The run length belief for the same evaluation run. Red lines denote the true changepoints. • Learning rate: 0.02 • Batch size: 10 • Batch length: 100 • Train iterations: 5000 The learning rate was decayed every 1500 training iterations. miniImageNet. Finally, for miniImageNet, we used six convolution blocks, each as previously described. This resulted in a 64 dimensional embedding space. We initially attempted to use the same four-conv backbone as for Rainbow MNIST, but the resulting 1600 dimensional embedding space had unreasonable memory requirements for batches lengths of 100. Again, for the “train on everything” baseline, we used the same architectures with one fully connected layer followed by a softmax. The following parameters were used for training: • Learning rate: 0.002 • Batch size: 10 • Batch length: 100 • Train iterations: 3000 The learning rate was decayed every 1000 training iterations. We used the validation set to monitor performance, and as in [ 5], we used the highest validation accuracy iteration for test. We also performed data augmentation as in [5] by adding random reﬂections and color jitter to the training data. C.4 Test details. For sinusoid, rainbow MNIST, and miniImageNet, a test horizon of 400 was used. Again, the longest possible test horizon was used to avoid artiﬁcial distortion of the test hazard rate. For these problems, a batch of 200 evaluations was performed. For the bandit, we evaluated on 10 trials of length 1000. For the NBA dataset, we obtained quantitative results by evaluated on 200 sequences of horizon 150. We chose a sequence of length 200 for qualitative visualization. D Further Experimental Results In this section we present a collection of experimental results investigating task and computational performance of MOCA, as well as hyperparameters of the algorithm and modiﬁed problem settings. D.1 Visualizing MOCA Posteriors Posteriors for the sinusoid and the bandit problem are provided in Fig. 6 and Fig. 7. These are visualized as they represents two ends of the spectrum; identifying changes in the sinusoid model is extremely easy, as a large amount of information is provided on possible changes for every datapoint. On the other hand, as discussed previously, only a small subset of points in the bandit problem are informative about the possible occurance of a changepoint. Accordingly, the run length belief in Fig. 6 is nearly exactly correct are concentrated on a particular run length. In contrast to this, the run length belief in Fig. 7 is less concentrated. Indeed, highly multimodal beliefs can be seen as well as the model placing a non-trivial amount of weight on many hypotheses. Finally, while some 20Figure 8: Regret compared to optimal action selection for optimistic action selection with three samples (left) and ﬁve (right) samples. Figure 9: Performance change from augmenting a model trained with MOCA with task supervision at test time (violet) and from using changepoint estimation at test time for a model trained with task-supervision (teal), for sinusoid (left), Rainbow MNIST (middle), and miniImageNet (right). changepoints are detected near immediately in the bandit problem, some take a handful of timesteps passing before the changepoint is detected. Interestingly, because MOCA maintains a belief over all possible run lengths, changepoints which are initially missed may be retrospectively identiﬁed, as can partially be seen starting around time 65 in Fig. 7. D.2 Action Selection Schemes in the Wheel Bandit In the body of the paper, we used Thompson sampling for action selection due to the simplicity of the method, as well as favorable perforamance in previous work on switching bandits [30]. However, optimism-based methods have also been effective in the switching bandit problem [14]. The MOCA posterior is a mixture of Gaussians, and thus many existing optimism-based bandit methods are not directly applicable. To investigate optimism-based action selection methods, we investigate a method in which we sample a collection of reward functions from the posterior, and choose the best action across all sampled reward models. Fig. 8 shows regret versus hazard for sampling three and ﬁve reward functions, respectively. The performance difference between MOCA and sliding window methods at low hazards is similar for Thompson sampling and for optimistic methods, as is the reversion of near-identical performance at high hazards. Compared to a standard (non-switching) bandit problem, the posterior will not concentrate to a point in the limit of inﬁnite timesteps as there is always some weight on the prior (as the problem could switch at any timestep). This impacts optimism-based exploration methods: in the limit of a large number of samples, the prior will dominate for all states. Efﬁcient exploration methods in the switching bandit remain an active research topic, especially paired with changepoint detection methods [30, 14, 17]. D.3 MOCA with Differing Train/Test Task Supervision To more closely analyze the difference between MOCA performance, which must infer task switches both at train-time and at test-time, and the oracle model, which has task segmentation information in 21Figure 10: Test negative log likelihood of MOCA on the sinusoid problem with partial task segmentation. The partial segmentation during training results in negligible performance increase, while partial supervision at test time uniformly improves performance. Note that each column corresponds to one trained model, and thus the randomly varying performance across train supervision rates may be explained by simply results of minor differences in individual models. both phases, we also compared against performance when task segmentation was provided at only one of these phases. We discuss the results of these comparisons for each of the experiments for which oracle task supervision was available below. Sinusoid. Fig. 9 shows the performance of MOCA when augmented with task segmentation at test time (violet), compared to unsegmented (blue), as well as the oracle model without test segmentation (teal) compared to with test segmentation (gray). We ﬁnd that as the hazard rate increases, the value of both train-time and test-time segmentation increases steadily. Because our regression version of MOCA only models the conditional density, it is not able to detect a changepoint before incurring the loss associated with an incorrect prediction. Thus, for high hazard rates with many changepoints, the beneﬁts of test-time task segmentation are increased. Interestingly and counter-intuitively, the model trained with MOCA outperforms the model trained with task segmentation when both are given task segmentation at test time. We hypothesize that this is due to MOCA having improved training dynamics. Early in training, an oracle model may produce posteriors that are highly concentrated but incorrect, yielding very large losses that can destabilize training. In contrast, MOCA always places a non-zero weight on the prior, mitigating these effects. We ﬁnd that we can match MOCA’s performance by artiﬁcially augmenting to the oracle model’s loss with a small weight (down to 10−16) on the prior likelihood, supporting this hypothesis. Rainbow MNIST. In Fig. 9, the relative effect of the train and test segmentation is visible. Looking at the effect of train-time segmentation in isolation, comparing blue to teal and violet to gray, we see that the beneﬁt of train-time segmentation is most pronounced at higher hazard rates. The effect of test segmentation (comparing blue to violet and teal to gray) is minimal, indicating MOCA is effectively able to detect task switches prior to making predictions. miniImageNet. Fig. 9 shows that, in contrast to the Rainbow MNIST experiment, there is a large and constant (with respect to hazard rate) performance decrease moving from oracle to MOCA at test time. Interestingly, while one would expect the performance decrease with increasing hazard rate to be attributable primarily to lack of test-time segmentation, this trend is primarily a consequence of MOCA training, consistent with the Rainbow MNIST experiments. This is likely a consequence of the limited amount of data, as the trend is not apparent for the sinusoid experiment. D.4 MOCA with Partial Task Segmentation Since MOCA explicitly reasons about a belief over run-lengths, it can operate anywhere in the spectrum of the task-unsegmented case as presented so far, to the fully task-segmented setting of standard meta-learning. At every time step t, the user can override the belief bt(rt) to provide a degree of supervision. At known changepoints, for example, the user can override bt(rt) to have 22Figure 11: Time per iteration versus iteration number at test time. Note that the right hand side of the curve shows the expected linear complexity expected of MOCA. Note that for these experiments, no hypothesis pruning was performed, and thus at test time performance could be constant time as opposed to linear. This ﬁgure shows 95% conﬁdence intervals for 10 trials, but the repeatability of the computation time is consistent enough that they are not visible. all its mass on rt = 0. If the task is known not to change at the given time, the user can set the hazard probability to 0 when updating the belief for the next timestep. If a user applies both of these overrides, it amounts to effectively sidestepping the Bayesian reasoning over changepoints and revealing this information to the meta-learning algorithm. If the user only applies the former, the user effectively indicates to the algorithm when known changepoints occur, but the algorithm is free to propagate this belief forward in time according to the update rules, and detect further changepoints that were not known to the user. Finally, the Bayesian framework allows a supervisor to provide their belief over a changepoint, which may not have probability mass entirely at rt = 0. Thus, MOCA ﬂexibly incorporates any type of task supervision available to a system designer. Fig. 10 shows the performance of partial task segmentation at both train and test for the sinusoid problem, for the hazard rate 0.2. This problem was chosen as the results were highly repeatable and thus the trend is more readily observed. Here, we label a changepoint with some probability, which we refer to as the supervision rate. We do not provide supervision for any non-changepoint timesteps, and thus a supervision rate of 1 corresponds to labeling every changepoint but is not equivalent to the oracle. Speciﬁcally, the model may still have false positive changepoints, but is incapable of false negatives. This ﬁgure shows that the performance monotonically improves with increasing train supervision rate, but is largely invariant under varying train supervision. This performance improvement agrees with Fig. 9, which shows that for the sinusoid problem, performance is improved by full online segmentation. Indeed, these results show that training with MOCA results in models with comparable test performance to those with supervised changepoints, and thus there is little marginal value to task segmentation during training. D.5 Computational Performance Fig. 11 shows the computational performance at test time on the sinusoid problem. Note that the right hand side of the curve shows a linear trend that is expected from the growing run length belief vector. However, even for 25000 iterations, the execution time is approximately 7ms for one iteration. These experiments were performed on an Nvidia Titan Xp GPU. Interestingly, on the left hand side of the curve, the time per iteration is effectively constant until the number of iterations approaches approximately 4500. Based on our code proﬁling, we hypothesize that this is an artifact of overhead in matrix multiplication computations done on the GPU. D.6 Batch Training MOCA In practice, we sample batches of length T from the full training time series, and train on these com- ponents. While this artiﬁcially increases the observed hazard rate (as a result of the initial belief over 23Figure 12: Performance versus the training horizon (T) for the sinusoid with hazard 0.01. The lowest hazard was used to increase the effects of the short training horizon. A minor decrease in performance is visible for very small training horizons (around 20), but ﬂattens off around 100 and above. It is expected that these diminishing marginal returns will occur for all systems and hazard rates. run length being 0 with probability 1), it substantially reduces the computational burden of training. Because MOCA maintains a posterior for each possible run length, computational requirements grow linearly with T. Iterating over the whole training time series without any hypothesis pruning can be prohibitively expensive. While a variety of different pruning methods within BOCPD have been proposed [46, 38], we require a pruning method which does not break model differentiability. Note that at test-time, we no longer require differentiability and so previously developed pruning methods may be applied. Empirically, we observe diminishing marginal returns when training on longer sequences. Fig. 12 shows the performance of MOCA for varying training sequence lengths ( T). In all experiments presented in the body of the paper, we use T = 100. As discussed, small T values artiﬁcially inﬂate the observed hazard rate, so we expect to see performance improve with larger T values. Fig. 12 shows that this effect results in diminishing marginal returns, with little performance improvement beyond T = 100. Longer training sequences lead to increased computation per iteration (as MOCA is linear in the runlength), as well as an increased memory burden (especially during training, when the computation graph must be retained by automatic differentiation frameworks). Thus, we believe it is best to train on the shortest possible sequences, and propose T = 1/λ(where λis the hazard rate) as a rough rule of thumb. 24",
      "meta_data": {
        "arxiv_id": "1912.08866v2",
        "authors": [
          "James Harrison",
          "Apoorva Sharma",
          "Chelsea Finn",
          "Marco Pavone"
        ],
        "published_date": "2019-12-18T20:10:40Z",
        "pdf_url": "https://arxiv.org/pdf/1912.08866v2.pdf",
        "github_url": "https://github.com/StanfordASL/moca"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of applying meta-learning algorithms in settings where task segmentation is unavailable, such as continual online learning with unsegmented time series data. The main contribution is Meta-Learning via Online Changepoint Analysis (MOCA), an algorithmic framework that augments generic meta-learning algorithms with a differentiable Bayesian changepoint detection scheme. MOCA enables both training and testing directly on time series data without requiring pre-segmented tasks, allowing the system to learn both a rapidly adaptive predictive model and an effective changepoint detection algorithm optimized to work together. The key finding is that MOCA achieves predictive performance comparable to standard task-segmented meta-learning settings across various regression and classification benchmarks, while also providing interpretable estimates of task switches.",
        "methodology": "MOCA extends Bayesian Online Changepoint Detection (BOCPD) to conditional density estimation, using a base meta-learning algorithm as the underlying predictive model (UPM). It recursively maintains and updates a belief distribution over run lengths (time since the last changepoint) using Bayesian filtering rules. Upon observing new data (xt, yt), MOCA updates the belief based on the input likelihood pθ(xt|ηt−1[rt]) and the conditional predictive likelihood pθ(yt|xt,ηt−1[rt]). The run length belief is then propagated forward in time, assuming a fixed hazard rate for task switches. The entire process is differentiable, allowing backpropagation to optimize the parameters of the base meta-learning algorithm. MOCA was instantiated and evaluated with an LSTM-based meta-learner, ALPaCA (a Bayesian meta-learning approach for regression), and PCOC (Probabilistic Clustering for Online Classification), a novel Bayesian meta-learning algorithm for classification based on Gaussian discriminant analysis in a learned feature space.",
        "experimental_setup": "MOCA's performance was evaluated across five problem settings: three nonlinear meta-regression benchmarks (Sinusoid Regression, Switching Wheel Bandit, NBA Player Movement prediction) and two meta-image-classification benchmarks (Rainbow MNIST, miniImageNet). Specific benchmarks include a switching sinusoid problem (adapted from [10]), an extended wheel bandit problem with changing reward functions [37], a real-world NBA player movement prediction task, the Rainbow MNIST dataset [11] with various transformations, and the miniImageNet dataset [44] grouped into five super-classes. Baselines included fixed-length Sliding Window models (n=5, 10, 50), a 'Train on Everything' model (no online adaptation), a 'Condition on Everything' model (for LSTM, continually updates a single set of posterior statistics), and an 'Oracle' model (same base meta-learner with exact task segmentation at train and test time). Performance was measured by Negative Log Likelihood (NLL) for regression/classification and regret for the bandit problem. Training utilized the Adam optimizer with specific learning rates, batch sizes, batch lengths, and iteration counts tailored for each experiment, including data augmentation for miniImageNet.",
        "limitations": "The current MOCA framework assumes tasks are sampled independently and identically distributed (i.i.d.), limiting its ability to explicitly leverage knowledge re-use for reoccurring tasks or tasks with temporal dynamics. In regression, models like ALPaCA primarily model the conditional density p(y|x) and assume p(x) is independent of the task, which may prevent changepoint detection based solely on changes in the input distribution before a label is observed. MOCA's computational requirements grow linearly with the batch length (T) during training, as it maintains posterior statistics for every possible run length, necessitating batch training which can artificially increase the observed hazard rate. Also, identifying changepoints can be difficult and temporally delayed in problems like the wheel bandit, particularly at high hazard rates. The paper also acknowledges that MOCA may amplify risks associated with meta-learning, such as privacy implications in few-shot learning and potential for increased bias or unfairness in individualized adaptive learning, highlighting the need for future research in these ethical domains.",
        "future_research_directions": "Future work could extend MOCA to handle domains where tasks reoccur or exhibit temporal dynamics, moving beyond the i.i.d. task assumption to improve data efficiency by re-using information from previous tasks. Exploring tasks with associated dynamics, as opposed to instantaneous switches, represents another promising direction. For the switching bandit problem, more efficient exploration methods, particularly when paired with changepoint detection, are still an active research topic. Additionally, developing differentiable pruning methods for the Bayesian Online Changepoint Detection component could alleviate the linear growth of computational requirements during training, making the framework more scalable for longer time series. Finally, extensive research is required to ensure that the adaptive learning enabled by algorithms like MOCA does not lead to unfair outcomes or amplify biases, considering the unique challenges of analyzing fairness in individualized decision-making rules.",
        "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport time\nimport math\nfrom copy import deepcopy\n\n# From metacpd/main/utils.py\nclass Flatten(nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n    \ndef conv_block(in_channels,out_channels):\n    return nn.Sequential(nn.Conv2d(in_channels,out_channels,3,padding=1),\n                        nn.BatchNorm2d(out_channels),\n                        nn.ReLU(),\n                        nn.MaxPool2d(2)\n                        )\n\ndef final_conv_block(in_channels,out_channels):\n    return nn.Sequential(nn.Conv2d(in_channels,out_channels,3,padding=1),\n#                         nn.BatchNorm2d(out_channels),\n                        nn.MaxPool2d(2)\n                        )\n\ndef mask_nlls(y,likelihoods):\n    \"\"\"\n    y: onehot labels: shape (..., n_classes)\n    likelihood: per class: shape (..., n_classes)\n    \"\"\"\n    # mask with y\n    return torch.sum(y * likelihoods,dim=-1)\n\ndef compute_acc(y,nlls):\n    # compute accuracy using nlls\n    pred_class = torch.argmin(nlls,-1,keepdim=True)\n    \n    acc = y.gather(-1, pred_class).squeeze(-1)\n    return acc\n\ndef get_prgx(config,horizon,batch_size,switch_times=None):\n\n    model = config['model.model']\n    sliding_window_len = config['data.window_length']\n\n    if model == 'main' or model == 'conv_net':\n        return None, None\n\n    prgx = []\n    task_sup = []\n    last_switch = np.zeros(batch_size,dtype=int)\n\n    for t in range(horizon):\n        prgx_t = np.zeros((batch_size,t+1))\n        task_supervision = np.zeros(batch_size)\n        for i in range(batch_size):\n            if model == 'sliding_window':\n                prgx_t[i,max(t-sliding_window_len,0)] = 1\n            elif model == 'no_task_change':\n                prgx_t[i,t] = 1\n            elif model == 'oracle':\n                if switch_times[i,t] > 0.5:\n                    last_switch[i] = t\n                    if config['train.task_supervision'] is not None:\n                        if np.random.rand() < config['train.task_supervision']:\n                            task_supervision[i] = 1.\n                            epsilon = 1e-5\n                            prgx_t[i,:] = np.ones(t+1)*epsilon\n                            prgx_t[i,last_switch[i]] = 1. - epsilon*t\n                            \n                            if config['train.oracle_hazard'] is not None:\n                                raise NotImplementedError\n                                \n                if config['train.task_supervision'] is None:\n                    if config['train.oracle_hazard'] is not None:\n                        if last_switch[i] != 0:\n                            prgx_t[i,0] = config['train.oracle_hazard']\n                            prgx_t[i,last_switch[i]] = 1. - config['train.oracle_hazard']\n                        else:\n                            prgx_t[i,last_switch[i]] = 1.\n                    else:\n                        prgx_t[i,last_switch[i]] = 1.\n                \n            else:\n                raise ValueError('make sure specified model is implemented')\n\n\n        prgx_t = torch.tensor(prgx_t).float()\n        task_supervision = torch.tensor(task_supervision).float()\n\n\n        if config['data.cuda'] >= 0:\n            prgx_t = prgx_t.cuda(config['data.cuda'])\n            task_supervision = task_supervision.cuda(config['data.cuda'])\n\n        \n        prgx.append(prgx_t)\n        task_sup.append(task_supervision)\n\n    if config['train.task_supervision'] is None:\n        return prgx, None\n    else:\n        return prgx, task_sup\n\n# From metacpd/main/encoders.py\ndef get_encoder(config):\n    hid_dim = config['model.hid_dim']\n    x_dim = config['model.x_dim']\n    phi_dim = config['model.phi_dim']\n    # REGRESSION\n\n    if config['data.dataset'] == 'Sinusoid':\n        activation = nn.Tanh()\n        encoder = nn.Sequential(\n            nn.Linear(x_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, phi_dim),\n            activation\n        )\n    elif config['data.dataset'] == 'NoiseSinusoid':\n        activation = nn.Tanh()\n        encoder = nn.Sequential(\n            nn.Linear(x_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, hid_dim),\n            nn.ReLU(),\n            nn.Linear(hid_dim, phi_dim),\n            activation\n        )\n        \n    # CLASSIFICATION\n\n    elif config['data.dataset'] in ['RainbowMNIST']:\n        encoder = nn.Sequential(\n            conv_block(3, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            final_conv_block(hid_dim, hid_dim),\n            Flatten()\n        )\n    elif config['data.dataset'] == 'MiniImageNet':\n        encoder = nn.Sequential(\n            conv_block(3, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            final_conv_block(hid_dim, hid_dim),\n            Flatten()\n        )\n\n    elif config['data.dataset'] == 'PermutedMNIST':\n        encoder = nn.Sequential(\n            conv_block(1, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            final_conv_block(hid_dim, hid_dim),\n            Flatten()\n        )\n\n    else:\n        raise ValueError(\"data.dataset not understood\")\n\n    return encoder\n\n# From metacpd/main/moca.py\nclass MOCA(nn.Module):\n    \"\"\"\n    Wraps an underlying MetaLearning algorithm to allow training on timeseries of\n    sequential examples with discrete task switches that are unlabeled.\n    \"\"\"\n\n    def __init__(self, meta_learning_alg, config):\n        super().__init__()\n\n        self.config = deepcopy(config)\n        self.x_dim = config['model.x_dim']\n        self.y_dim = config['model.y_dim']\n\n        self.classification = config['model.classification']\n\n        self.meta_learning_alg = meta_learning_alg\n\n        # hazard rate:\n        hazard_logit = np.log( config['data.hazard'] / (1 - config['data.hazard'] ) )\n        self.hazard_logit = nn.Parameter(torch.from_numpy(np.array([hazard_logit])), requires_grad=config['train.learnable_hazard'])\n\n        # initial log_prx:\n        self.init_log_prgx = nn.Parameter(torch.zeros([1,1]), requires_grad=False)\n\n\n    def nll(self, log_pi, log_prgx):\n        \"\"\"\n            log_pi: shape(batch_size x t x ...)       log p(new data | x, r=i, data so far) for all i = 0, ..., t\n            log_prgx: shape (batch_size x t x ...)    log p(r=i | data so far) for all i = 0, ..., t\n        \"\"\"\n\n        if len(log_pi.shape) == 3:\n            return -torch.logsumexp(log_pi + log_prgx.unsqueeze(-1), dim=1)\n\n        return -torch.logsumexp(log_pi + log_prgx, dim=1)\n\n    def log_p_r_given_x(self,log_prx):\n        \"\"\"\n            computes log p(r|x)\n\n            inputs: log_prx: (batch_size, t+1), log p(r, x) for each r in 0, ..., t\n                    log_prx: (batch_size, t+1), log p(r | x) for each r in 0, ..., t\n        \"\"\"\n        return nn.functional.log_softmax(log_prx,dim=1)\n\n    @property\n    def log_hazard(self):\n        \"\"\"\n        log p( task_switch )\n        \"\"\"\n        return torch.log(torch.sigmoid(self.hazard_logit))\n\n    @property\n    def log_1m_hazard(self):\n        \"\"\"\n        log (1 - p(task_switch))\n        \"\"\"\n        return torch.log(1-torch.sigmoid(self.hazard_logit))\n\n    @property\n    def hazard(self):\n        return torch.sigmoid(self.hazard_logit)\n\n    def forward(self,x_mat,y_mat,prgx=None, task_supervision=None, return_timing=False):\n        \"\"\"\n        Takes in x,y batches; loops over horizon to recursively compute posteriors\n        Inputs:\n        - x_mat; shape = batch size x horizon x x_dim\n        - y_mat; shape = batch size x horizon x y_dim\n        \"\"\"\n        batch_size = x_mat.shape[0]\n        test_horizon = x_mat.shape[1]\n\n\n        posterior_params_list = []\n        log_prgx_list = []\n        nll_list = []\n\n        # define initial params and append to list\n        # we add a batch dimension and a time dimension\n        prior_params = tuple( p[None,None,...] for p in self.meta_learning_alg.prior_params() )\n\n\n        posterior_params = prior_params\n        log_prgx = self.init_log_prgx # p(r, all data so far)\n\n        posterior_params_list.append(posterior_params)\n\n\n        # start at time t+1\n        \n        time_array = []\n        \n        for i in range(test_horizon):\n            # grab y, phi:\n            \n            if return_timing:\n                torch.cuda.synchronize()\n                start_time = time.perf_counter()\n            \n            x = x_mat[:,i,:]\n            y = y_mat[:,i,:]\n\n\n            # compute log p(y|x,hist) for all possible run lengths (shape: (batch_size, t+1))\n            # and return updated params incorporating new point\n            # log_pi_t = log p(y|x,r,hist) for all r = [0,...,i]\n\n            # if classification, log_pi_t == p(y,x|eta) for all y (batchsize, i+1, y_dim)\n            # if regression, log_pi_t == p(y|x,eta)\n            log_pi_t, updated_posterior_params = self.meta_learning_alg.log_predictive_prob(x[:,None,:], y[:,None,:], posterior_params, update_params=True)\n            if self.classification:\n                log_pygx =  nn.functional.log_softmax(log_pi_t, dim=-1) # normalize to get p(y | x) # (batchsize, i+1, y_dim)\n\n                # update p(r_t) given just the x value\n                log_p_newx_gr = torch.logsumexp(log_pi_t, dim=-1) # sum over all y values, shape (batch_size, i+1)\n                log_prgx = log_prgx + log_p_newx_gr # (batch_size, i+1) # log p ( r_{i} \\mid x_{0,i}, y_{0,i-1} )\n                log_prgx = torch.log_softmax(log_prgx, dim=1) # normalizing over runlengths\n\n            else:\n                log_pygx = log_pi_t\n\n            if prgx is not None:\n                if task_supervision is not None:\n                    override_log_prgx = torch.log(prgx[i]) + torch.log(task_supervision[i].unsqueeze(-1))\n                    masked_log_prgx = log_prgx + torch.log(1-task_supervision[i].unsqueeze(-1))\n                    cat_log_prgx = torch.cat((override_log_prgx.unsqueeze(-1), masked_log_prgx.unsqueeze(-1)),dim=-1)\n                    log_prgx = torch.logsumexp(cat_log_prgx,dim=-1)\n                else:\n                    log_prgx = torch.log(prgx[i])\n                    \n            if not return_timing: log_prgx_list.append(log_prgx)\n\n            # use these posterior predictives and log p(r | hist) to evaluate y under the full posterior predictive\n            nll = self.nll(log_pygx, log_prgx)\n            if not return_timing: nll_list.append(nll)\n\n            # update belief over run lengths:\n\n            # if classification, then log_pi_t is (batch_size, i+1. y_dim), need to mask before updating belief\n            if self.classification:\n                log_pygx = mask_nlls(y.unsqueeze(-2), log_pygx) # (batch_size, i+1)\n\n            # calculate joint densities p(r_t,data_so_far) for both r_t = r_{t-1} + 1 and r_t = 0\n            log_prx_grow = self.log_1m_hazard + log_pygx + log_prgx                      # p( r_t = r_{t-1} + 1 )\n            log_prx_chpt = self.log_hazard + torch.logsumexp(log_pygx + log_prgx, dim=1) # p (r_t = 0 )\n\n            log_prx = torch.cat((log_prx_grow,log_prx_chpt[:,None]), 1) # shape (batch_size, i + 2)\n            log_prgx = torch.log_softmax(log_prx, dim=1) # log p(r_{i+1} | x_{0:i}, y_{0:i})\n\n            # update posteriors update\n            posterior_params = tuple( torch.cat((u, p.expand(*([batch_size] + list(p.shape)[1:]))), axis=1) for u,p in zip(updated_posterior_params, prior_params) )\n\n            # append to list\n            if not return_timing: posterior_params_list.append(posterior_params)\n\n            if return_timing:\n                torch.cuda.synchronize()\n                end_time = time.perf_counter()\n                time_array.append(end_time-start_time)\n            \n        if not return_timing: nlls = torch.stack(nll_list, dim=1) # shape (batch_size, t, y_dim)\n        \n        if return_timing:\n            return [], [], [], time_array\n        \n        return posterior_params_list, log_prgx_list, nlls\n\n# From metacpd/main/alpaca.py\nclass ALPaCA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = deepcopy(config)\n        self.x_dim = config['model.x_dim']\n        self.phi_dim = config['model.phi_dim']\n        self.y_dim = config['model.y_dim']\n\n        self.sigma_eps = eval(self.config['model.sigma_eps'])\n        self.logSigEps = nn.Parameter(torch.from_numpy(np.log(self.sigma_eps)), requires_grad=self.config['train.learnable_noise'])\n\n        self.Q = nn.Parameter(torch.randn(self.phi_dim, self.y_dim))\n        self.L_asym = nn.Parameter(torch.randn(self.phi_dim, self.phi_dim))\n\n        self.normal_nll_const = self.y_dim*np.log(2*np.pi)\n\n        hid_dim = config['model.hid_dim']\n        self.encoder = get_encoder(config)\n\n    @property\n    def logdetSigEps(self):\n        return torch.sum(self.logSigEps)\n\n    @property\n    def invSigEps(self):\n        return torch.diag(torch.exp(-self.logSigEps))\n\n    def prior_params(self):\n        Q0 = self.Q\n        Linv0 = self.L_asym @ self.L_asym.T\n\n        return (Q0, Linv0)\n\n    def recursive_update(self, phi, y, params):\n        \"\"\"\n            inputs: phi: shape (..., phi_dim )\n                    y:   shape (..., y_dim )\n                    params: tuple of Q, Linv\n                        Q: shape (..., phi_dim, y_dim)\n                        Linv: shape (..., phi_dim, phi_dim)\n        \"\"\"\n        Q, Linv = params\n\n        Lphi = Linv @ phi.unsqueeze(-1)\n\n        Linv = Linv - 1./(1 + phi.unsqueeze(-2) @ Lphi) * (Lphi @ Lphi.transpose(-1,-2))\n        Q = phi.unsqueeze(-1) @ y.unsqueeze(-2) + Q\n\n        return (Q, Linv)\n\n    def log_predictive_prob(self, x, y, posterior_params, update_params=False):\n        \"\"\"\n            input:  x: shape (..., x_dim)\n                    y: shape (..., y_dim)\n                    posterior_params: tuple of Q, Linv:\n                        Q: shape (..., phi_dim, y_dim)\n                        Linv: shape (..., phi_dim, phi_dim)\n                    update_params: bool, whether to perform recursive update on\n                                   posterior params and return updated params\n            output: logp: log p(y | x, posterior_parms)\n                    updated_params: updated posterior params after factoring in (x,y) pair\n        \"\"\"\n\n        phi = self.encoder(x)\n\n        Q, Linv = posterior_params\n\n        K = Linv @ Q\n\n        sigfactor = 1 + (phi.unsqueeze(-2) @ Linv @ phi.unsqueeze(-1))\n        err = y.unsqueeze(-1) - K.transpose(-1,-2) @ phi.unsqueeze(-1)\n\n        invsig = self.invSigEps / sigfactor # shape (..., y_dim y_dim)\n\n        nll_quadform = err.transpose(-1,-2) @ invsig @ err\n        nll_logdet = self.y_dim * torch.log(sigfactor) + self.logdetSigEps\n\n        logp = -0.5*(self.normal_nll_const + nll_quadform + nll_logdet).squeeze(-1).squeeze(-1)\n\n        if update_params:\n            updated_params = self.recursive_update(phi,y,posterior_params)\n            return logp, updated_params\n\n        return logp\n\n\n    def forward(self, x, posterior_params):\n        \"\"\"\n            input: x, posterior params\n            output: y\n        \"\"\"\n        phi = self.encoder(x)\n\n        Q, Linv = posterior_params\n\n        K = Linv @ Q\n\n        sigfactor = 1 + (phi.unsqueeze(-2) @ Linv @ phi.unsqueeze(-1))\n        mu = ( K.transpose(-1,-2) @ phi.unsqueeze(-1) ).squeeze(-1)\n        invsig = self.invSigEps / sigfactor\n\n        return mu, invsig\n\n# From metacpd/main/pcoc.py\nclass PCOC(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.config = deepcopy(config)\n        self.x_dim = config['model.x_dim']\n        self.phi_dim = config['model.phi_dim']\n        self.y_dim = config['model.y_dim']\n\n        self.sigma_eps = np.zeros([self.y_dim,1]) + np.asarray(eval(config['model.sigma_eps']))\n        self.cov_dim =  self.sigma_eps.shape[-1]\n        print(\"Using %d parameters in covariance:\" % self.cov_dim)\n        if self.phi_dim % self.cov_dim != 0:\n            raise ValueError(\"cov_dim must evenly divide phi_dim\")\n\n        self.logSigEps = nn.Parameter(torch.from_numpy(np.log(self.sigma_eps)), requires_grad=self.config['train.learnable_noise'])\n        \n        Linv_offset = config['model.Linv_init']\n        dir_scale = config['model.dirichlet_scale']\n        self.Q = nn.Parameter(torch.randn(self.y_dim, self.cov_dim, self.phi_dim//self.cov_dim))\n        self.logLinv = nn.Parameter(torch.randn(self.y_dim, self.cov_dim)+Linv_offset)\n        self.log_dirichlet_priors = nn.Parameter(dir_scale*torch.ones(self.y_dim), requires_grad=config['train.learnable_dirichlet'])\n\n        self.normal_nll_const = self.phi_dim*np.log(2*np.pi)\n\n        self.encoder = get_encoder(config)\n\n    @property\n    def invSigEps(self):\n        return torch.exp(-self.logSigEps) #.repeat(self.y_dim,1)\n\n    @property\n    def SigEps(self):\n        return torch.exp(self.logSigEps) #.repeat(self.y_dim,1)\n\n    def prior_params(self):\n        Q0 = self.Q\n        Linv0 = torch.exp(self.logLinv)\n        dir_weights = torch.exp(self.log_dirichlet_priors)\n\n        return (Q0, Linv0, dir_weights)\n\n    def recursive_update(self, phi, y, params):\n        \"\"\"\n            inputs: phi: shape (..., cov_dim, k )\n                    y:   shape (..., y_dim )\n                    params: tuple of Q, Linv\n                        Q: shape (..., y_dim, cov_dim, k)\n                        Linv: shape (..., y_dim, cov_dim)\n                        dir_weights: shape (..., y_dim)\n        \"\"\"\n        Q, Linv, dir_weights = params\n\n        # zeros out entries all except class y\n        invSigEps_masked = self.invSigEps * y.unsqueeze(-1) # (..., y_dim, cov_dim)\n\n        Q = Q + invSigEps_masked.unsqueeze(-1)*phi.unsqueeze(-3)\n        Linv = Linv + invSigEps_masked\n        dir_weights = dir_weights + y\n\n        return (Q, Linv, dir_weights)\n\n    def log_predictive_prob(self, x, y, posterior_params, update_params=False):\n        \"\"\"\n            input:  x: shape (..., x_dim)\n                    y: shape (..., y_dim)\n                    posterior_params: tuple of Q, Linv:\n                        Q: shape (..., y_dim, cov_dim, k)\n                        Linv: shape (..., y_dim, cov_dim)\n                        dir_weights: shape (..., y_dim)\n                    update_params: bool, whether to perform recursive update on\n                                   posterior params and return updated params\n            output: logp: log p(y, x | posterior_params) (..., y_dim)\n                    updated_params: updated posterior params after factoring in (x,y) pair\n        \"\"\"\n\n        x_shape = list(x.shape)\n\n        if len(x_shape) > 4: # more than one batch dim\n            x = x.reshape([-1]+x_shape[-3:])\n\n        phi = self.encoder(x) # (..., phi_dim)\n        if len(x_shape) > 4:\n            phi = phi.reshape(x_shape[:-3]+[self.phi_dim])\n\n        Q, Linv, dir_weights = posterior_params\n        mu = Q / Linv.unsqueeze(-1) # (..., y_dim, cov_dim, k)\n        pred_cov = 1./Linv + self.SigEps() # (..., y_dim, cov_dim)\n\n        phi_shape = phi.shape\n        phi_reshaped = phi.reshape(*(list(phi_shape)[:-1]+[self.cov_dim, -1])) # (..., cov_dim, k)\n\n        err = phi_reshaped.unsqueeze(-3) - mu # (..., y_dim, cov_dim, k)\n\n        nll_quadform = (err**2 / pred_cov.unsqueeze(-1) ).sum(-1).sum(-1)\n        nll_logdet = (self.phi_dim/self.cov_dim) * torch.log(pred_cov).sum(-1) # sum of log of diagonal entries\n\n        logp = -0.5*(nll_quadform + nll_logdet + self.normal_nll_const) # log p(x | y)\n\n        logp += torch.log(dir_weights / dir_weights.sum(-1,keepdim=True)) # multiply by p(y) posterior to get p(x, y)\n\n        if update_params:\n            updated_params = self.recursive_update(phi_reshaped, y, posterior_params)\n            return logp, updated_params\n\n        return logp\n\n\n    def forward(self, x, posterior_params):\n        \"\"\"\n            input: x, posterior params\n            output: log p(x | y) for all y\n        \"\"\"\n        x_shape = list(x.shape)\n\n        if len(x_shape) > 4: # more than one batch dim\n            x = x.reshape([-1]+x_shape[-3:])\n\n        phi = self.encoder(x) # (..., phi_dim)\n        if len(x_shape) > 4:\n            phi = phi.reshape(x_shape[:-3]+[self.phi_dim])\n\n        Q, Linv, dir_weights = posterior_params\n        mu = Q / Linv.unsqueeze(-1) # (..., y_dim, cov_dim, k)\n        pred_cov = 1./Linv + self.SigEps() # (..., y_dim, cov_dim)\n\n        phi_shape = phi.shape\n        phi_reshaped = phi.reshape(*(list(phi_shape)[:-1]+[self.cov_dim, -1])) # (..., cov_dim, k)\n\n        err = phi_reshaped.unsqueeze(-3) - mu # (..., y_dim, cov_dim, k)\n\n        nll_quadform = (err**2 / pred_cov.unsqueeze(-1) ).sum(-1).sum(-1)\n        nll_logdet = (self.phi_dim/self.cov_dim) * torch.log(pred_cov).sum(-1) # sum of log of diagonal entries\n\n        logp = -0.5*(nll_quadform + nll_logdet + self.normal_nll_const) # log p(x | y)\n\n        logp += torch.log(dir_weights / dir_weights.sum(-1,keepdim=True)) # multiply by p(y) posterior to get p(x, y)\n\n        return logp",
        "experimental_info": "MOCA (Meta-learning for Online Changepoint Analysis) is evaluated by instantiating it with specific underlying predictive models (UPMs) and tested on various datasets with defined experimental settings.\n\n**Underlying Predictive Models (UPMs):**\n- For regression tasks, MOCA uses ALPaCA (A Bayesian meta-learning approach for regression).\n- For classification tasks, MOCA uses PCOC (Probabilistic Clustering for Online Classification).\n\n**Datasets:**\n- **Regression:** `SwitchingSinusoidDataset` (referred to as 'Sinusoid' in configurations) and `SwitchingNoiseSinusoidDataset` (referred to as 'NoiseSinusoid').\n- **Classification:** `MiniImageNet` and `RainbowMNISTDataset` (referred to as 'RainbowMNIST').\n\n**Experimental Parameters and Settings (Defaults unless otherwise specified):**\n- **General:**\n    - `--data.batch_size`: 50 for training, 1 for testing.\n    - `--data.horizon`: 100 for training, 400 for testing.\n    - `--data.cuda`: CUDA device ID, -1 for CPU.\n    - `--train.seed`: Numpy seed, default 1 or 1000.\n    - `--train.experiment_id`: Unique experiment identifier, default 0.\n- **MOCA Specific:**\n    - `--data.hazard`: Hazard rate for task switches, default 0.1 for training, 0.01 for testing.\n    - `--train.learnable_hazard`: Flag to enable learning the hazard rate (0=False, 1=True), default 0 (False).\n    - `--train.task_supervision`: Percentage of task switches labeled (float), default None.\n    - `--train.oracle_hazard`: Hazard rate for oracle (curriculum), default None.\n- **Model Architecture (UPMs and Encoders):**\n    - `--model.x_dim`: Input dimensionality, 1 for Sinusoid datasets (regression), 3 for image datasets (classification).\n    - `--model.y_dim`: Output dimensionality, 1 for regression, 5 or 10 for classification (depending on dataset).\n    - `--model.hid_dim`: Dimensionality of hidden layers, default 128.\n    - `--model.phi_dim`: Dimensionality of the embedding space, default 32.\n- **ALPaCA Specific (Regression UPM):**\n    - `--model.sigma_eps`: Noise covariance, e.g., `[0.05]` for Sinusoid datasets. Can be learned.\n    - `--train.learnable_noise`: Flag to enable learning noise covariance (0=False, 1=True), default 0 (False).\n- **PCOC Specific (Classification UPM):**\n    - `--model.Linv_init`: Initialization of logLinv, default 0.0.\n    - `--model.dirichlet_scale`: Value for log Dirichlet concentration parameters initialization, default 10.0.\n    - `--train.learnable_dirichlet`: Flag to enable learning Dirichlet concentration (0=False, 1=True), default 0 (False).\n- **Training Process:**\n    - `--train.train_iterations`: Number of episodes to train, default 7500.\n    - `--train.val_iterations`: Number of episodes to validate on, default 5.\n    - `--train.learning_rate`: Learning rate for optimizer (Adam), default 0.02.\n    - `--train.decay_every`: Learning rate decay every 1500 iterations (gamma=0.5).\n    - `--train.grad_accumulation_steps`: Number of gradient accumulation steps, default 1.\n- **Evaluation Metrics:**\n    - Negative Log-Likelihood (NLL) is reported for both regression and classification tasks.\n    - Accuracy is reported for classification tasks."
      }
    },
    {
      "title": "Tensor Normal Training for Deep Learning Models",
      "abstract": "Despite the predominant use of first-order methods for training deep learning\nmodels, second-order methods, and in particular, natural gradient methods,\nremain of interest because of their potential for accelerating training through\nthe use of curvature information. Several methods with non-diagonal\npreconditioning matrices, including KFAC, Shampoo, and K-BFGS, have been\nproposed and shown to be effective. Based on the so-called tensor normal (TN)\ndistribution, we propose and analyze a brand new approximate natural gradient\nmethod, Tensor Normal Training (TNT), which like Shampoo, only requires\nknowledge of the shape of the training parameters. By approximating the\nprobabilistically based Fisher matrix, as opposed to the empirical Fisher\nmatrix, our method uses the block-wise covariance of the sampling based\ngradient as the pre-conditioning matrix. Moreover, the assumption that the\nsampling-based (tensor) gradient follows a TN distribution, ensures that its\ncovariance has a Kronecker separable structure, which leads to a tractable\napproximation to the Fisher matrix. Consequently, TNT's memory requirements and\nper-iteration computational costs are only slightly higher than those for\nfirst-order methods. In our experiments, TNT exhibited superior optimization\nperformance to state-of-the-art first-order methods, and comparable\noptimization performance to the state-of-the-art second-order methods KFAC and\nShampoo. Moreover, TNT demonstrated its ability to generalize as well as\nfirst-order methods, while using fewer epochs.",
      "full_text": "Tensor Normal Training for Deep Learning Models Yi Ren, Donald Goldfarb Department of Industrial Engineering and Operations Research Columbia University New York, NY 10027 {yr2322, goldfarb}@columbia.edu Abstract Despite the predominant use of ﬁrst-order methods for training deep learning mod- els, second-order methods, and in particular, natural gradient methods, remain of interest because of their potential for accelerating training through the use of cur- vature information. Several methods with non-diagonal preconditioning matrices, including KFAC [34], Shampoo [18], and K-BFGS [15], have been proposed and shown to be effective. Based on the so-called tensor normal (TN) distribution [31], we propose and analyze a brand new approximate natural gradient method, Tensor Normal Training (TNT), which like Shampoo, only requires knowledge of the shape of the training parameters. By approximating the probabilistically based Fisher matrix, as opposed to the empirical Fisher matrix, our method uses the block-wise covariance of the sampling based gradient as the pre-conditioning matrix. Moreover, the assumption that the sampling-based (tensor) gradient follows a TN distribution, ensures that its covariance has a Kronecker separable structure, which leads to a tractable approximation to the Fisher matrix. Consequently, TNT’s memory requirements and per-iteration computational costs are only slightly higher than those for ﬁrst-order methods. In our experiments, TNT exhibited superior optimization performance to state-of-the-art ﬁrst-order methods, and compara- ble optimization performance to the state-of-the-art second-order methods KFAC and Shampoo. Moreover, TNT demonstrated its ability to generalize as well as ﬁrst-order methods, while using fewer epochs. 1 Introduction First-order methods are currently by far the most popular and successful optimization methods for training deep learning models. Stochastic gradient descent (SGD) [39] uses the (stochastic) gradient direction to guide its update at every iteration. Adaptive learning rate methods, including AdaGrad [11], RMSprop [20], and Adam [23], scale each element of the gradient direction (possibly modiﬁed to incorporate momentum) by the square root of the second moment of each element of the gradient. These ﬁrst-order methods use little curvature information to \"pre-condition\" the gradient direction; SGD uses an identity pre-conditioning matrix, whereas the others use a diagonal matrix. On the other hand, second-order methods attempt to greatly accelerate the optimization process by exploring the rich curvature information of the problem. Traditional second-order methods such as Newton’s method, BFGS [6, 13, 14, 41], and limited-memory BFGS (L-BFGS) [ 28], without modiﬁcation, are not practical in a deep learning setting, because these methods require enormous amounts of memory and computational effort per iteration due to the huge number of parameters such models have. Some second-order methods have been proposed to deal with the non-convexity and stochasiticity of objective functions arising in machine learning (see e.g. [ 36, 7, 16, 44]), but directly using these methods to train deep learning models still requires large amounts of memory and computing resources. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.02925v3  [cs.LG]  21 Dec 2021Recently, there has been considerable advancement in the development of second-order methods that are suitable for deep learning models with huge numbers of parameters. These methods usually ap- proach pre-conditioning of the gradient in a modular way, resulting in block-diagonal pre-conditioning matrices, where each block corresponds to a layer or a set of trainable parameters in the model. Inspired by the idea of the natural gradient (NG) method [1], [34] proposed KFAC, an NG method that uses a Kronecker-factored approximation to the Fisher matrix as its pre-conditioning matrix that can be applied to multilayer perceptrons, and which has subsequently been extended to other architectures, such as convolutional neural networks [17] and recurrent neural networks [35]. Kronecker-factored preconditioners [15, 38] based on the structure of the Hessian and quasi-Newton methods have also been developed. Despite the great success of these efﬁcient and effective second-order methods, developing such methods requires careful examination of the structure of the preconditioning matrix to design appropriate approximations for each type of layer in a model. Another well-recognized second-order method, Shampoo [18, 3], extends the adaptive learning rate method AdaGrad, so that the gradient is pre-conditioned along every dimension of the underlying tensor of parameters in the model, essentially replacing the diagonal pre-conditioning matrix of the adaptive learning rate methods by a block diagonal Kronecker-factored matrix which can be viewed as an approximation to a fractional power of the empirical Fisher (EF) matrix. However, while estimating the Fisher matrix, in a deep learning setting, by the EF matrix saves some computational effort, it usually does not capture as much valuable curvature information as the Fisher matrix [26]. Variants of the normal distribution, i.e. the matrix-normal distribution [ 9] and the tensor-normal distribution [31], have been proposed to estimate the covariance of matrix and higher-order tensor observations, respectively. By imposing a Kronecker structure on the covariance matrix, the resulting covariance estimate requires a vastly reduced amount of memory, while still capturing the interactions between the various dimensions of the respective matrix or tensor. Iterative MLE methods for estimating the parameters of matrix-normal and tensor-normal distributions have been examined in e.g. [12, 31], and various ways to identify the unique representation of the distribution parameters have been proposed in [43, 10]. However, to the best of our knowledge, this advanced statistical methodology has not been used to develop optimization methods for deep learning. In this paper, we describe a ﬁrst attempt to do this and demonstrate its great potential. Our Contributions. In this paper, we propose a brand new approximate natural gradient (NG) method, Tensor-Normal Training (TNT), that makes use of the tensor normal distribution to approxi- mate the Fisher matrix. Signiﬁcantly, the TNT method can be applied to any model whose training parameters are a collection of tensors without knowing the exact structure of the model. To achieve this, we ﬁrst propose a new way, that is suitable for optimization, to identify the covariance parameters of tensor normal (TN) distributions, in which the average eigenvalues of the covariance matrices corresponding to each of the tensor dimensions are required to be the same (see Section 3). By using the Kronecker product structure of the TN covariance, TNT only introduces mild memory and per-iteration computational overhead compared with ﬁrst-order methods. Also, TNT’s memory usage is the same as Shampoo’s and no greater than KFAC’s, while its per-iteration computational needs are no greater than Shampoo’s and KFAC’s (see Section 5). The effectiveness of TNT is demonstrated on deep learning models. Speciﬁcally, on standard autoencoder problems, when optimization performance is compared, TNT converges faster than the benchmark ﬁrst-order methods and roughly the same rate as the benchmark second-order methods. Moreover, on standard CNN models, when generalization is concerned, TNT is able to achieve roughly the same level of validation accuracy as the ﬁrst-order methods, but using far fewer epochs (see Section 6). We also prove that, if the statistics used in TNT can be estimated ideally, it converges to a stationary point under mild assumptions (see Section 4). 2 Preliminaries Supervised Learning. Throughout this paper, we consider the classic supervised learning setting where we learn the parameters θof a model, by minimizing Lpθq“ 1 N řN i“1 lpyi,fθpxiqq, where tpxi,yiquN i“1 denotes a given dataset (xi being the input to the model and yi being the target), fθpxiq denotes the output of the model when xi is provided as the input, and l denotes a loss function 2(e.g. least-squares loss for regression and cross entropy loss for classiﬁcation) that measures the discrepancy between the model output fθpxiqand the target yi. Natural Gradient Method and the Fisher Matrix. In a ﬁrst-order method, say SGD, the updating direction is always derived from an estimate to the gradient direction ∇θLpθq. In a natural gradient (NG) method [1], however, the Fisher matrix is used as a pre-conditioning matrix that is applied to the gradient direction. As shown in [34], the Fisher matrix is deﬁned as F “Ex„Qx,y„pp¨|x,θq ” ∇θlog ppy|x,θqp∇θlog ppy|x,θqqJ ı , (1) where Qx is the data distribution of xand pp¨|x,θqis the density function of the conditional distribu- tion deﬁned by the model with a given input x. In many cases, such as when pis associated with a Gaussian distribution and the loss function l measures least-squares loss, or when pis associated with a multinomial distribution and lis cross- entropy loss, log pis equivalent to l(see e.g. [33, 34]). Hence, if Dθdenotes the gradient of lw.r.t. θ for a given xand y, we have that F “Ex„Qx,y„prDθDθJs. Consequently, one can sample xfrom Qx and perform a forward pass of the model, then sample yfrom pp¨|x,θq, and perform a backward pass to compute Dθ, and then use DθDθJto estimate F. We call Dθa sampling-based gradient, as opposed to the empirical gradient ∇θlpyi,fθpxiqqwhere pxi,yiqis one instance from the dataset. It is worth noting that the ﬁrst moment of Dθis zero. To see this, note that, with given x, Ey„pr∇θlog ppy|x,θqs“ ż ∇θlog ppy|x,θqppy|x,θqdy“ ż ∇θppy|x,θqdy “∇θ ˆż ppy|x,θqdy ˙ “∇θ1 “0. Hence, Ex„Qx,y„prDθs“ Ex„Qx tEy„pr∇θlog ppy|x,θqs| xu“ 0. Thus, the Fisher matrix F can be viewed as the covariance matrix of Dθ. Note that the empirical Fisher CANNOT be viewed as the covariance of the empirical gradient, because the ﬁrst moment of the latter is, in general, NOT zero. Tensor-Normal Distribution. The development of our new method makes use the so-called tensor- normal distribution [31, 10]: Deﬁnition 1. An arbitrary tensor GPRd1ˆ¨¨¨ˆdk is said to follow a tensor normal (TN) distribution with mean parameter M PRd1ˆ¨¨¨ˆdk and covariance parameters U1 PRd1ˆd1 , ..., Uk PRdkˆdk if and only if vecpGq„ NormalpvecpMq,U1 b¨¨¨b Ukq. In the above deﬁnition, the vec operation refers to the vectorization of a tensor, whose formal deﬁnition can be found in Sec A in the Appendix. Note that matrix-normal distribution can be viewed as a special case of TN distribution, where k “2. Compared with a regular normal distribution, whose covariance matrix has śk i“1 d2 i elements, the covariance of a k-way tensor-normal distribution is stored in ksmaller matrices with a total number of elements equal to řk i“1 d2 i. To estimate the covariance submatrices U1,...,U k, the following property (e.g., see [10]) is used: ErGpiqs“ Ui ¨ ź j‰i trpUjq, (2) where Gpiq :“matipGqmatipGqJ PRdiˆdi denotes the contraction of Gwith itself along all but the ith dimension and mati refers to matricization of a tensor (see Section A for the formal deﬁnitions). By (2), we can sample Gto obtain estimates of the Gpiq’s, and hence, estimates of the Ui’s. The complexity of computing Gpiqis di śk j“1 dj, which is also far less than the complexity of computing vecpGqvecpGqJneeded to estimate the covariance of a regular normal distribution. 3 Tensor-Normal Training In this section, we propose Tensor-Normal Training (TNT), a brand new variant of the natural gradient (NG) method that makes use of the tensor-normal distribution. 33.1 Block Diagonal Approximation In this paper, we consider the case where the parameters of the model θconsists of multiple tensor variables W1,...,W L, i.e. θ “ pvecpW1qJ,..., vecpWLqJqJ. This setting is applicable to most common models in deep learning such as multi-layer perceptrons, convolutional neural networks, recurrent neural networks, etc. In these models, the trainable parameter Wl (l “1,...,L ) come from the weights or biases of a layer, whether it be a feed-forward, convolutional, recurrent, or batch normalization layer, etc. Note that the index lof Wl refers to the index of a tensor variable, as opposed to a layer. To obtain a practical NG method, we assume, as in KFAC and Shampoo, that the pre-conditioning Fisher matrix is block diagonal. To be more speciﬁc, we assume that each block corresponds to the covariance of a tensor variable in the model. Hence, the approximate Fisher matrix is: F «diagL l“1 ␣ Ex„Qx,y„p “ vecpDWlqpvecpDWlqqJ‰( “diagL l“1 tVarpvecpDWlqqu. The remaining question is how should one approximate VarpvecpDWlqqfor l“1,...,L . 3.2 Computing the Approximate Natural Gradient Direction by TNT We consider a tensor variableW PRd1ˆ¨¨¨ˆdk in the model and assume that G:“DW PRd1ˆ¨¨¨ˆdk follows a TN distribution with zero mean and covariance parameters U1,...,U k where Ui PRdiˆdi. Thus, the Fisher matrix corresponding to W is FW “Ex„Qx,y„prVarpvecpGqqs“ U1 b¨¨¨b Uk. Loosely speaking, the idea of relating the Fisher matrix to the covariance matrix of some normal distribution has some connections to Bayesian learning methods and interpretations of NG methods (see e.g., [22]). Let ∇WL PRd1ˆ¨¨¨ˆdk denote the gradient of L w.r.t. W. The approximate NG updating direction for W is computed as F´1 W vecp∇WLq“p U´1 1 b¨¨¨b U´1 k qvecp∇WLq“ vec ` ∇WL ˆ1 U´1 1 ˆ2 ¨¨¨ˆ k U´1 k ˘ , (3) where ˆi (i“1,...,k ) denotes a mode-iproduct (see Section A in the Appendix). Note that the last equality of (3) makes use of the following proposition, which also appears in [18] (see Sec A in the Appendix for a proof): Proposition 1. Let GPRd1ˆ¨¨¨ˆdk and Ui PRdiˆdi for i“1,...,k . Then, we have ` bk i“1Ui ˘ vecpGq“ vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq. (4) To summarize, the generic Tensor-Normal Training algorithm is: Algorithm 1 Generic Tensor-Normal Training (TNT) Require: Given batch size m, and learning rate α 1: for t“1,2,... do 2: Sample mini-batch Mt of size m 3: Perform a forward-backward pass over Mt to compute the mini-batch gradient 4: Perform another backward pass over Mt with ysampled from the predictive distribution to compute Gl “DWl (l“1,...,L ) averaged across Mt 5: for l“1,...L do 6: Estimate ErGpiq l s(i“1,...,k l) from Gl 7: Determine Uplq 1 ,...,U plq kl from ErGp1q l s,..., ErGpklq l s 8: Compute the inverses of Uplq 1 ,...,U plq kl 9: Compute the updating direction pl by (3) 10: Wl “Wl ´α¨pl. 11: end for 12: end for 3.3 Identifying the Covariance Parameters of the Tensor Normal Distribution By (2), Ui can be inferred from ErGpiqsup to a constant multiplier. However, different sets of multipliers can generate the same F, i.e. the same distribution. This is less of a problem if one 4only cares about F. However, we need F´1 to compute the approximate natural gradient. That is, we ﬁrst must choose a representation of F “cp˜U1 b¨¨¨b ˜Ukq(see below), and then compute F´1 “c´1pp˜U1 `ϵIq´1 b¨¨¨bp ˜Uk `ϵIq´1qwith a proper choice of ϵ ą0, where ϵI plays a damping role in the preconditioning matrix. In this case, different representations of F will lead to different F´1. The statistics community has proposed various representations for ˜Ui’s. For example, [43] imposed that c “ 1 and the ﬁrst element of ˜Ui to be one for i “ 1,...,k ´1, whereas [ 10] imposed that trp˜Uiq“ 1 for i“1,...,k . Although these representations have nice statistical properties, they are not ideal from the perspective of inverting the covariance for use in a NG method in optimization. We now describe one way to determine ˜U1,..., ˜Uk, and cfrom ErGp1qs,..., ErGpkqs. In particular, we ﬁrst set c“1, so that F´1 has a constant upper bound ϵ´kI. We then require that trp˜Uiq di is constant w.r.t i. In other words, the average of the eigenvalues of each of the ˜Ui’s is the same. This helps the ˜Ui’s have similar overall \"magnitude\" so that a suitableϵcan be found that works for all dimensions. Moreover, this shares some similarity with how KFAC splits the overall damping term between KFAC matrices, although KFAC adjusts the damping values, whereas TNT adjusts the matrices. A bit of algebra gives the formula ˜Ui “ ErGpiqs ck´1 0 ś j‰idj , (5) where c0 “ ´ trpErGpiqsqś jdj ¯1{k . 3.4 Comparison with Shampoo and KFAC Shampoo, proposed in [18], and later modiﬁed and extended in [3], is closely related to TNT. Both methods use a block-diagonal Kronecker-factored preconditioner based on second-order statistics of the gradient and are able to handle all sorts of tensors, and hence, can be applied to all sorts of deep neural network models, easily and seamlessly. The major differences between them are: (i) The TN distribution cannot be directly applied to EF, which is used in Shampoo, because the empirical gradient does not have a zero mean; hence its covariance and second moment are different. It is also believed that EF does not capture as much valuable curvature information as Fisher [26]. (ii) Using the statistics ErGpiqs’s, TNT approximates the Fisher matrix as the covariance of the block-wise sampling-based gradients assuming that they are TN distributed. On the other hand, Shampoo computes 1{2k-th power of the statistics of each direction of the tensor-structured empirical gradient and forms a preconditioning matrix from the Kronecker product of them. It is unclear to us how to interpret statistically such a matrix other than by its connection to EF. We further note that Shampoo was developed as a Kronecker-factored approximation to the full-matrix version of AdaGrad [11], whereas TNT was developed as a NG method using a TN-distributed approximation to the Fisher matrix. (iii) TNT computes the updating direction using the inverse (i.e. power of ´1) of the Kronecker factors of the approximate Fisher matrix, whereas Shampoo uses the ´1{2k-th power1 of the Kronecker factors of the EF matrix. Another method related to TNT is KFAC [34, 17], which, like TNT, uses Fisher as its preconditioning matrix. Their major differences are: (i) KFAC develops its approximation based on the structure of the gradient and Fisher matrix for each type of layer. Admittedly, this could lead to better approximations. But it is relatively hard to implement (e.g. one need to store some intermediate variables to construct the KFAC matrices). Also, if new types of layers with different structures are considered, one needs to develop suitable Kronecker factorizations, i.e., KFAC matrices. On the contrary, TNT, like Shampoo, is a model- agnostic method, in the sense that, TNT can be directly applied as long as the shape of the tensor variables are speciﬁed. 1In [3], for autoencoder problems involving tensors of order 2, the power was set to ´α 2 , where αPr0,1s was treated as a hyper-parameter which required tuning, and was set to α“1 after tuning. 50 500 1000 iteration 0.4 0.5 0.6 0.7 0.8 0.9 1.0cosine similarity l = 1 0 500 1000 iteration 0.4 0.5 0.6 0.7 0.8 0.9 l = 2 0 500 1000 iteration 0.5 0.6 0.7 0.8 0.9 l = 3 0 500 1000 iteration 0.5 0.6 0.7 0.8 0.9 l = 4 TNT KFAC Shampoo Adam SGD-m Figure 1: Cosine similarity between the directions produced by the methods shown in the legend and that of a block Fisher method. The algorithms were run on a 16 ˆ16 down-scaled MNIST [27] dataset and a small feed-forward NN with layer widths 256-20-20-20-20-20-10 described in [34]. As in [34], we only show the middle four layers. (ii) Each block of TNT corresponds to a tensor variable whose shape needs to be speciﬁed, whereas each block of KFAC corresponds to all variables in a layer. For example, for a linear or convo- lutional layer, the KFAC block would correspond to the Fisher of both its weights and bias (and their correlation), whereas TNT would produce two blocks corresponding to the weights and bias, respectively. In order to gain more insight into how well TNT approximates the Fisher matrix compared with other methods, we computed the cosine similarity between the direction produced by each method and that by a block Fisher method, where each block corresponded to one layer’s full Fisher matrix in the model (see Figure 1). For all methods shown in Figure 1, we always followed the trajectory produced by the block Fisher method. In our implementation of the block Fisher method, both the gradient and the block-Fisher matrices were estimated with a moving-average scheme, with the decay factors being 0.9. In all of the other methods compared to the block Fisher method, moving averages were also used, with the decay factors being 0.9, as described in Section D in the Appendix, to compute the relevant gradients and approximate block-Fisher matrices used by them, based on values computed at points generated by the block-Fisher method. As shown in Figure 1, the cosine similarity for TNT is always around 0.7 to 0.8, which is similar to (and sometimes higher) than the structure-aware method KFAC, and always better than Shampoo. To provide more information, we also include SGD with momentum and Adam, whose similarity to the block Fisher direction is usually lower that of the second-order methods. 4 Convergence In this section, we present results on the convergence of an idealized version of TNT that uses the actual covariance of Dθ, rather than a statistical estimate of it (see Algorithm 2 in the Appendix). In particular, our results show that Algorithm 2, with constant batch size and decreasing step size, converges to a stationary point under some mild assumptions. For simplicity, we assume that the model only contains one tensor variableW. However, our results can be easily extended to the case of multiple tensor variables. To start with, our proofs, which are delayed to Section B in the Appendix, require the following assumptions: Assumption 1. L : Rn ÑR is continuously differentiable. Lpθqis lower bounded by a real number Llow for any θPRn. ∇L is globally Lipschitz continuous with Lipschitz constant L; namely for any θ,θ1 PRn, }∇Lpθq´ ∇Lpθ1q}ď L}θ´θ1}. Assumption 2. For any iteration t, we have aq Eξt r∇lpθt,ξtqs“ ∇Lpθtq bq Eξt ” }∇lpθt,ξtq´ ∇Lpθtq}2 ı ďσ2 where σ ą 0 is the noise level of the gradient estimation, and ξt,t “ 1,2,..., are independent samples, and for a given tthe random variable ξt is independent of tθjut j“1 Assumption 3. Let G :“ Dθ. For any θ P Rn, the norm of the Fisher matrix F “ Ex„Qx,y„prvecpGqvecpGqJsis bounded above. Since F represents the curvature of the KL divergence of the model’s predictive distribution, As- sumption 3 controls the change of predictive distribution when the model’s parameters change; hence, 6Table 1: Memory and per-iteration time complexity beyond that required by SGD Name Memory Time (per-iteration) TNT Opřk i“1 d2 iq Opp1 T1 m`řk i“1 diqśk i“1 di ` 1 T2 řk i“1 d3 iq Shampoo Opřk i“1 d2 iq Oppřk i“1 diqśk i“1 di `p 1 T2 řk i“1 d3 i- if using SVDqq Adam-like Opśk i“1 diq Opśk i“1 diq Newton-like Opśk i“1 d2 iq - depends on speciﬁc algorithm it is a mild assumption for reasonable deep learning models. Essentially, we would like to prove that, if the Fisher matrix is upper bounded, our approximated Fisher (by TNT) is also upper bounded. We now present two lemmas and our main theorem; see Section B in the Appendix for proofs. Lemma 1. }Ex„Qx,y„prGpiqs}ď ´ 1 di śk i1“1 di1 ¯ }Ex„Qx,y„prvecpGqvecpGqJs}, @i“1,...,k. Lemma 2. Suppose Assumption 3 holds. Let FTNT :“p ˜U1 `ϵIqb¨¨¨bp ˜Uk `ϵIqwhere ˜Ui’s are deﬁned in (5). Then, the norm of FTNT is bounded both above and below. Theorem 1. Suppose that Assumptions 1, 2, and 3 hold for tθtugenerated by Algorithm 2 with batch size mt “mfor all t. If we choose αt “ κ L¯κ2 t´β, with β Pp0.5,1q, then 1 N Nÿ t“1 Etξju8 j“1 ” }∇Lpθtq}2 ı ď 2L ` ML ´Llow˘ ¯κ2 κ2 Nβ´1 ` σ2 p1 ´βqm ` N´β ´N´1˘ , where N denotes the iteration number and the constant ML ą0 depends only on L. Moreover, for a given δ P p0,1q,to guarantee that 1 N řN t“1 Etξju8 j“1 ” }∇Lpθtq}2 ı ă δ, N needs to be at most O ´ δ´ 1 1´β ¯ . 5 Implementation Details of TNT and Comparison on Complexity Implementation Details of TNT. In practice, we compute G “DW averaged over a minibatch of data at every iteration, and use the value of G piq to update a moving average estimate yGpiq of ErGpiqs. The extra work for these computations (as well as for updating the inverses of ˜Ui) compared with a stochastic gradient descent method is amortized by only performing them every T1 (and T2) iterations, which is also the approach used in KFAC and Shampoo, and does not seems to degrade the overall performance of the TNT algorithm. Moreover, we compute ErGpiqsusing the whole dataset at the initialization point as a warm start, which is also done in our implementations of Shampoo and KFAC. See Algorithm 3 in the Appendix for the detailed implementation of TNT. A Comparison on Memory and Per-iteration Time Complexity.To compare the memory require- ments and per-iteration time complexities of different methods, we consider the case where we optimize one tensor variable of size d1 ˆ¨¨¨ˆ dk using minibatches of size mat every iteration. A plain SGD method requires Opśk i“1 diqto store the model parameters and the gradient, whereas its per-iteration time complexity is Opmśk i“1 diq. Table 1 lists the memory requirements and per-iteration time complexities in excess of that required by SGD for different methods. Compared with a classic Newton-like method (e.g. BFGS), TNT (as well as Shampoo) reduces the memory requirement from Opśk i“1 d2 iqto Opřk i“1 d2 iq, which is comparable to that of Adam-like adaptive gradient methods. In fact, if the di’s are all equal to dand 3 ďk ăăd, the Kronecker- factored TNT pre-conditioning matrix requires kd2 storage, which is less than that required by the diagonal pre-conditioners used by Adam-like methods. On the other hand, in terms of per-iteration time complexity, TNT (as well as Shampoo) only introduces a mild overhead for estimating the statistics ErGpiqs’s, inverting the pre-conditioning matrices, and computing the updating direction. Also, the ﬁrst two of these operations can be amortized by only performing them every T1 and T2 iterations. Lastly, the extra work of Op1 T1 mśk i“1 diqrequired by TNT relative to Shampoo is due to the extra backward pass needed to estimate the true Fisher, as opposed to the EF. 7Moreover, although TNT and Shampoo both incur 1 T2 řk i“1 d3 i amortized time to invert the pre- conditioning matrices, the SVD operation in Shampoo can take much more time than the matrix inverse operation in TNT, especially when the matrix size is large2. The per-iteration computational complexity of KFAC is more complicated because it depends on the type of the layer/variable. For linear layers, TNT and KFAC both uses two matrices, whose sizes are the number of input nodes and output nodes, respectively. For convolutional layers, TNT uses three matrices, whose sizes are the size of ﬁlter, number of input channels, and number of output channels, whereas KFAC uses two matrices whose sizes are the size of ﬁlter times number of input channels, and number of output channels. As a result, the ﬁrst KFAC matrix requires much more memory. In general, the per-iteration complexity of KFAC is no less than that of TNT. 6 Experiments In this section, we compare TNT with some state-of-the-art second-order (KFAC, Shampoo) and ﬁrst-order (SGD with momentum, Adam) methods (see Section D.1 in the Appendix on how these methods were implemented). The Hessian-based K-BFGS method [15, 38] is another state-of-the-art Kronecker-factored second-order method for training deep learning models. Since our focus is on optimizers that use Fisher or empirical Fisher as the preconditioning matrix, we did not include K-BFGS in our comparison. Our experiments were run on a machine with one V100 GPU and eight Xeon Gold 6248 CPUs using PyTorch [37]. Each algorithm was run using the best hyper-parameters, determined by an appropriate grid search (speciﬁed below), and 5 different random seeds. In Figures 2 and 3 the performance of each algorithm is plotted: the solid curves give results obtained by averaging the 5 different runs, and the shaded area depicts the ˘standard deviation range for these runs. Our code is available at https://github.com/renyiryry/tnt_neurips_2021. 6.1 Optimization: Autoencoder Problems 0 100 200 300 400 500 epoch 102 train loss 0 100 200 300 400 500 process time (second) 102 train loss TNT KFAC Shampoo Adam SGD-m a) MNIST autoencoder 0 250 500 750 1000 1250 epoch 101 102 train loss 0 500 1000 1500 2000 process time (second) 101 102 train loss TNT KFAC Shampoo Adam SGD-m b) FACES autoencoder Figure 2: Optimization performance of TNT, KFAC, Shampoo, Adam, and SGD-m on two autoen- coder problems We ﬁrst compared the optimization performance of each algorithm on two autoencoder problems [21] with datasets MNIST [27] and FACES3, which were also used in [32, 34, 5, 15] as benchmarks to compare different algorithms. For each algorithm, we conducted a grid search on the learning rate and damping value based on the criteria of minimal training loss. We set the Fisher matrix update frequency T1 “1 and inverse update frequency T2 “20 for all of the second-order methods. Details of our experiment settings are listed in Section D.2 in the Appendix. From Figure 2, it is clear that TNT outperformed SGD with momentum and Adam, both in terms of per-epoch progress and process time. Moreover, TNT performed (at least) as well as KFAC and Shampoo, with a particularly strong performance on the FACES dataset. We repeated these experiments using a grid search on more hyper-parameters, and obtained results (see Figure 6 in Sec D.5) that further support our observations based on Figure 2. 2In [ 3] it is shown that replacing the SVD operation by a coupled Schur-Newton method saves time for matrices of size greater than 1000 ˆ1000. In our experiments, we used the coupled Newton method implementation of Shampoo. 3https://cs.nyu.edu/~roweis/data.html 86.2 Generalization: Convolutional Neural Networks 0 50 100 150 200 epoch 10 2 10 1 100 train loss 0 2000 4000 process time (second) 10 2 10 1 100 train loss 0 50 100 150 200 epoch 10 1 100 val error 0 2000 4000 process time (second) 10 1 100 val error TNT KFAC Shampoo Adam SGD-m a) CIFAR-10, ResNet-32 0 50 100 150 200 epoch 10 2 10 1 100 train loss 0 5000 10000 process time (second) 10 2 10 1 100 train loss 0 50 100 150 200 epoch 100 3 × 10 1 4 × 10 1 6 × 10 1 val error 0 5000 10000 process time (second) 100 3 × 10 1 4 × 10 1 6 × 10 1 val error TNT KFAC Shampoo Adam SGD-m b) CIFAR-100, VGG16 Figure 3: Generalization ability of TNT, KFAC, Shampoo, Adam, and SGD-m on two CNN models. Upper row depicts the training loss whereas lower row depicts the validation classiﬁcation error. We then compared the generalization ability of each algorithm on two CNN models, namely, ResNet32 [19] (with CIFAR10 dataset [24]) and VGG16 [42] (with CIFAR100 dataset [24]). The ﬁrst-order methods were run for 200 epochs during which the learning rate was decayed by a factor of 0.1 every 60 epochs, whereas the second-order methods were run for 100 epochs during which the learning rate was decayed by a factor of 0.1 every 40 epochs; (these settings are the same as in [45]). Moreover, as indicated in [29, 45], weight decay, different from the L2 regularization added to the loss function, helps improve generalization across different optimizers. Thus, for each algorithm, we conducted a grid search on the initial learning rate and the weight decay factor based on the criteria of maximal validation classiﬁcation accuracy. The damping parameter was set to 1e-8 for Adam (following common practice), and 0.03 for KFAC4. For TNT and Shampoo, we set ϵ“0.01. We set T1 “10 and T2 “ 100 for the three second-order methods (same as in [ 45]). Details of our experiment settings and a further discussion of the choice of damping hyper-parameters can be found in Section D.3 in the Appendix. The results in Figure 3 indicate that, with a proper learning rate and weight decay factor, second-order methods and Adam exhibit roughly the same generalization performance as SGD with momentum, which corroborate the results in [29, 45]. In particular, TNT has a similar (and sometimes better) generalization performance than the other methods. For example, comparing TNT with SGD-m, TNT (SGD-m) achieves 93.08% (93.06%) validation accuracy with ResNet32 on CIFAR10 and 73.33% (73.43%) validation accuracy with VGG16 on CIFAR-100, after 100 (200) epochs (see Table 3 in the Appendix for the accuracy achieved by the other algorithms). Moreover, in terms of process time, TNT is roughly twice (equally) as fast as SGD with momentum on ResNet32/CIFAR10 in Figure 3a (on VGG16 on CIFAR-100 in Figure 3b). This illustrates the fact that TNT usually requires only moderately more computational effort per-iteration but fewer iterations to converge than ﬁrst-order methods. Also, as shown on the VGG16 model, KFAC seems to be much slower than TNT and Shampoo on larger models. This is because the most recent version of KFAC, which we implemented, uses SVD (i.e., eigenvalue decomposition) to compute inverse matrices (see Section D.1.2 in the Appendix for a discussion of this). In contrast, TNT does not need to use SVD, and the most recent version of Shampoo replaces SVD with a coupled Newton method in [3]. We also compared TNT with a variant of it that uses the empirical rather than the true Fisher as the preconditioning matrix. The results of this comparison, which are presented in Section D.4 in the Appendix, suggest that it is preferable to use Fisher rather than empirical Fisher as pre-conditioning matrices in TNT. 4The value of 0.03 is suggested in https://github.com/alecwangcq/KFAC-Pytorch, a github repo by the authors of [45]. 97 Conclusion and Further Discussions In this paper, we proposed a new second-order method, and in particular, an approximate natural gradient method TNT, for training deep learning models. By approximating the Fisher matrix using the structure imposed by the tensor normal distribution, TNT only requires mild memory and computational overhead compared with ﬁrst-order methods. Our experiments on various deep learning models and datasets, demonstrate that TNT provides comparable and sometimes better results than the state-of-the-art (SOTA) methods, both from the optimization and generalization perspectives. Due to space and computational resource constraints, we did not run experiments on even larger models such as ImageNet and advanced models for NLP tasks. However, the results in this paper already show very strong evidence of the potential of the TNT method. We also did not explore extending our method to a distributed setting, which has been shown to be a promising direction for second-order methods such as KFAC and Shampoo [4, 3]. Since TNT already performs very well on a single machine, we expect that it will continue to do so in a distributed setting. These issues will be addressed in future research. We did not compare TNT with the SOTA Kronecker-based quasi-Newton methods [15, 38], since they are not as closely related to TNT as are Shampoo and KFAC. Their performance relative to TNT can be inferred from the comparisons here combined with those reported in [15, 38, 3]. As a ﬁnal note 5, the preconditioning matrices of TNT (as well as those of Shampoo) are derived from the speciﬁc shape of the (tensor) parameters of the particular deep learning model that is being trained. One can, of course, reshape these parameters, e.g., by ﬂattening the tensors into vectors, which gives rise to very different preconditioning matrices. The method proposed in this paper can be applied to any deep learning or machine learning model. If the model and/or data has a ﬂawed design or contains bias, this could potentially have negative societal impacts. However, this possibility is beyond the scope of the work presented in this paper. 5We thank the program chair for pointing this out. 10Acknowledgments and Disclosure of Funding We would like to thank the anonymous reviewers for their very helpful comments and suggestions. The research efforts of D. Goldfarb and Y . Ren on this paper were supported in part by NSF Grant IIS-1838061. We acknowledge computing resources from Columbia University’s Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893- 01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. References [1] S.-I. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning for multilayer perceptrons. Neural computation, 12(6):1399–1409, 2000. [2] E. Amid, R. Anil, and M. K. Warmuth. Locoprop: Enhancing backprop via local loss optimiza- tion. arXiv preprint arXiv:2106.06199, 2021. [3] R. Anil, V . Gupta, T. Koren, K. Regan, and Y . Singer. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2021. [4] J. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using Kronecker- factored approximations. 2016. [5] A. Botev, H. Ritter, and D. Barber. Practical Gauss-Newton optimisation for deep learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 557–565. JMLR. org, 2017. [6] C. G. Broyden. The convergence of a class of double-rank minimization algorithms 1. general considerations. IMA Journal of Applied Mathematics, 6(1):76–90, 1970. [7] R. H. Byrd, S. L. Hansen, J. Nocedal, and Y . Singer. A stochastic quasi-Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016. [8] D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl. On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446, 2019. [9] A. P. Dawid. Some matrix-variate distribution theory: notational considerations and a Bayesian application. Biometrika, 68(1):265–274, 1981. [10] B. S. Dees and D. P. Mandic. A statistically identiﬁable model for tensor-valued Gaussian random variables. arXiv preprint arXiv:1911.02915, 2019. [11] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011. [12] P. Dutilleul. The mle algorithm for the matrix normal distribution. Journal of statistical computation and simulation, 64(2):105–123, 1999. [13] R. Fletcher. A new approach to variable metric algorithms. The computer journal , 13(3): 317–322, 1970. [14] D. Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26, 1970. [15] D. Goldfarb, Y . Ren, and A. Bahamou. Practical quasi-Newton methods for training deep neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, ed- itors, Advances in Neural Information Processing Systems , volume 33, pages 2386–2396. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf. [16] R. Gower, D. Goldfarb, and P. Richtárik. Stochastic block BFGS: Squeezing more curvature out of data. In International Conference on Machine Learning, pages 1869–1878, 2016. [17] R. Grosse and J. Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573–582, 2016. [18] V . Gupta, T. Koren, and Y . Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pages 1842–1850. PMLR, 2018. 11[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016. [20] G. Hinton, N. Srivastava, and K. Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8), 2012. [21] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504–507, 2006. [22] M. E. Khan and H. Rue. The Bayesian learning rule. arXiv preprint arXiv:2107.04562, 2021. [23] D. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2014. [24] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. [26] F. Kunstner, P. Hennig, and L. Balles. Limitations of the empirical Fisher approximation for natural gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/46a558d97954d0692411c861cf78ef79-Paper.pdf. [27] Y . LeCun, C. Cortes, and C. Burges. MNIST handwritten digit database.ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [28] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528, 1989. [29] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Con- ference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. [30] H. Lu, K. N. Plataniotis, and A. Venetsanopoulos.Multilinear subspace learning: dimensionality reduction of multidimensional data. CRC press, 2013. [31] A. M. Manceur and P. Dutilleul. Maximum likelihood estimation for the tensor normal dis- tribution: Algorithm, minimum sample size, and empirical bias and dispersion. Journal of Computational and Applied Mathematics, 239:37–49, 2013. [32] J. Martens. Deep learning via Hessian-free optimization. In ICML, volume 27, pages 735–742, 2010. [33] J. Martens. New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014. [34] J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408–2417, 2015. [35] J. Martens, J. Ba, and M. Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=HyMTkQZAb. [36] A. Mokhtari and A. Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE Transactions on Signal Processing, 62(23):6089–6104, 2014. [37] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URLhttp://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf. [38] Y . Ren and D. Goldfarb. Kronecker-factored quasi-Newton methods for convolutional neural networks. arXiv preprint arXiv:2102.06737, 2021. [39] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951. 12[40] R. M. Schmidt, F. Schneider, and P. Hennig. Descending through a crowded valley- benchmarking deep learning optimizers. In International Conference on Machine Learning, pages 9367–9376. PMLR, 2021. [41] D. F. Shanno. Conditioning of quasi-Newton methods for function minimization. Mathematics of computation, 24(111):647–656, 1970. [42] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [43] M. Singull, M. R. Ahmad, and D. von Rosen. More on the Kronecker structured covariance matrix. Communications in Statistics-Theory and Methods, 41(13-14):2512–2523, 2012. [44] X. Wang, S. Ma, D. Goldfarb, and W. Liu. Stochastic quasi-Newton methods for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017. [45] G. Zhang, C. Wang, B. Xu, and R. Grosse. Three mechanisms of weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=B1lz-3Rct7. 13A Some Tensor Deﬁnitions and Properties We present in this section fairly standard notation and deﬁnitions regarding tensors, e.g., see [18] and Chapter 3 of [30], that we use throughout the paper. Let APRd1ˆ¨¨¨ˆdk denote a tensor of order k. • slices of Aalong its i-th dimension: for i “1,...,k and j “1,...,d i, the j-th slice of A along its i-th dimension, Ai j denotes the d1 ˆ¨¨¨ˆ di´1 ˆdi`1 ˆ¨¨¨ˆ dk tensor of order k´1, composed from all of the entries of Awhose ith index is j. • vectorization of A: denoted as vecpAq, is deﬁned recursively as vecpAq“ ¨ ˚˝ vecpA1 1q ... vecpA1 d1 q ˛ ‹‚, where for the base case, in which Ais one-dimensional tensor (i.e., a vector), vecpAq“ A. Note that when Ais a matrix, this corresponds to the row-major vectorization of A. • matricization of A: denoted as matipAq, for i“1,...,k , is deﬁned as matipAq“ ¨ ˚˝ vecpAi 1qJ ... vecpAi diqJ ˛ ‹‚. Note that vecpAq“ vecpmat1pAqq. • contraction of Awith itself along all but the ith dimension: denoted as Apiq, is deﬁned as matipAqmatipAqJ. • mode-iproduct of Aby a matrix U PRd1 iˆdi: the operation is denoted as ˆi. Let B “ AˆiU PRd1ˆ¨¨¨ˆdi´1ˆd1 iˆdi`1ˆ¨¨¨ˆdk denote the resulting tensor. Bj1,...,ji´1,j1 i,ji`1,...,jk “ř ji Aj1,...,jkUj1 i,ji. Note that in the matrix case (k“2), Aˆ1 U “UA, Aˆ2 U “AUJ. Lemma 3. Let X PRmˆn, APRmˆm, B PRnˆn. Then, we have pAbBJqvecpXq“ vecpAXBq. Note that the above lemma is slightly different from the most common version of it, which uses a column-major vectorization of the matrix X. Proposition 1. Let GPRd1ˆ¨¨¨ˆdk and Ui PRdiˆdi for i“1,...,k . Then, we have ` bk i“1Ui ˘ vecpGq“ vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq. (6) Proof of Proposition 1: Proof. Our proof, which is largely inspired by the one in [18], is by induction on k. When k“1, it is easy to see that (6) holds by the deﬁnition of the mode-iproduct. When k“2, (6) follows from Lemma 3. Now assume that (6) holds for 1,2,...,k ´1. For k, we let H “bk i“2Ui By the induction hypothesis, mat1pGqHJ “ ` Hmat1pGqJ˘J “ ` H `vecpG1 1q ¨¨¨ vecpG1 d1 q˘˘J (7) “ `HvecpG1 1q ¨¨¨ HvecpG1 d1 q˘J (8) “ `vecpG1 1 ˆ1 U2 ¨¨¨ˆ k´1 Ukq ¨¨¨ vecpG1 d1 ˆ1 U2 ¨¨¨ˆ k´1 Ukq˘J (9) “ ¨ ˚˝ vecpG1 1 ˆ1 U2 ¨¨¨ˆ k´1 UkqJ ... vecpG1 d1 ˆ1 U2 ¨¨¨ˆ k´1 UkqJ ˛ ‹‚“mat1pGˆ2 U2 ¨¨¨ˆ k Ukq (10) 14By Lemma 3 and (10), ` bk i“1Ui ˘ vecpGq“p U1 bHqvecpmat1pGqq“ vecpU1mat1pGqHJq “vecpU1mat1pGˆ2 U2 ¨¨¨ˆ k Ukqq “vecpmat1pGˆ2 U2 ¨¨¨ˆ k Uk ˆ1 U1qq “vecpGˆ2 U2 ¨¨¨ˆ k Uk ˆ1 U1q “vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq, where the third from last equality comes from the fact that BmatipAq“ matipAˆi Bq, and the last equality comes from the fact that mode-iproducts are commutative. B Proofs of Lemmas and Theorem 1 Algorithm 2 Idealized Version of TNT Require: Given θ1 PRn, batch sizes tmtutě1, step sizes tαtutě1, and damping value ϵą0 1: for t“1,2,... do 2: Sample mini-batch of size mt: Mt “tξt,i,i “1,...,m tu 3: Calculate y∇Lt “ 1 mt ř ξt,iPMt ∇lpθt,ξt,iq 4: Compute ˜Ui (i “1,...,k ) by formula (5), using the true values of Ex„Qx,y„prGpiqs(i “ 1,...,k ) at the current parameter θt. 5: Compute pt “vec ´ y∇Lt ˆ1 p˜U1 `ϵIq´1 ˆ2 ¨¨¨ˆ k p˜Uk `ϵIq´1 ¯ 6: Calculate θt`1 “θt ´αtpt 7: end for Algorithm 2 describes an idealized version of TNT, whose convergence is veriﬁed by the proofs of Lemmas 1 and 2, and Theorem 1 below. Lemma 1. }Ex„Qx,y„prGpiqs}ď ´ 1 di śk i1“1 di1 ¯ }Ex„Qx,y„prvecpGqvecpGqJs}, @i“1,...,k. Proof of Lemma 1: Proof. Let X P Rmˆn be a random matrix, and xi P Rm denote its ith column ( i “ 1,...,n ). Because vecpXqis a vector containing all the elements of all the xi’s,xixJ i is a square submatrix of vecpXqvecpXqJ. Hence, }ErxixJ i s}ď} ErvecpXqvecpXqJs}, and we have that }ErXXJs}“} Er nÿ i“1 xixJ i s}“} nÿ i“1 ErxixJ i s}ď nÿ i“1 }ErxixJ i s}ď n}ErvecpXqvecpXqJs}. Letting X “matipGqP Rdiˆpd1¨¨¨di´1di`1¨¨¨dkq, it then follows that }Ex„Qx,y„prGpiqs}ďp d1 ¨¨¨ di´1di`1 ¨¨¨ dkq}ErvecpmatipGqqvecpmatipGqqJs} “p 1 di kź i1“1 d1 iq}ErvecpGqvecpGqJs}. Lemma 2. Suppose Assumption 3 holds. Let FTNT :“p ˜U1 `ϵIqb¨¨¨bp ˜Uk `ϵIq, where the ˜Ui’s are deﬁned in (5). Then, the norm of FTNT is bounded both above and below. Proof of Lemma 2: 15Proof. It is clear that ||FTNT||“ śk i“1 ||˜Ui `ϵI||ě ϵk. On the other hand, for i“1,...,k , if we denote the eigenvalues of ErGpiqsby λ1 ď¨¨¨ď λdi, we have from (5) that }˜Ui}“ }ErGpiqs} ´ trpErGpiqsqś jdj ¯pk´1q{kś j‰idj “ λdi ´λ1`¨¨¨`λdiś jdj ¯pk´1q{kś j‰idj ď λdi ´ λdiś jdj ¯pk´1q{kś j‰idj “ diλ1{k di pś jdjq1{k “ di}ErGpiqs}1{k pś jdjq1{k . Thus, since ||FTNT||“ śk i“1 ||˜Ui `ϵI||“ śk i“1p||˜Ui||` ϵq, by the above and Lemma 1, ||FTNT||ď kź i“1 ˜ di}ErGpiqs}1{k pś jdjq1{k `ϵ ¸ ď kź i“1 ´ d1´1{k i ||ErvecpGqvecpGqJs||1{k `ϵ ¯ . Then, by Assumption 3, we have that ||FTNT||is bounded above. Proof of Theorem 1: Proof. The proof of Theorem 1 follows from Theorem 2.8 in [44]. Clearly, Algorithm 2 falls under the scope of the stochastic quasi-Newton (SQN) method in [44]. In particular, by Proposition 1, the pre-conditioning matrix H “F´1 TNT. Moreover, to apply Theorem 2.8 in [44], we need to show that AS.1 - AS.4 in [44] hold. First, AS.1 and AS.2 in [44] are the same as Assumption 1 and Assumption 2, respectively in Section 4 in our paper. Second, by Lemma 2, since ||FTNT||is both upper and lower bounded, so is ||F´1 TNT||. Hence, AS.3 in [44] is ensured. Finally, Algorithm 2 itself ensures AS.4 in [44] holds. Hence, by Theorem 2.8 of [44], the result is guaranteed. C Pseudo-code for TNT In Algorithm 3, we present a detailed pseudo-code for our actual implementation of TNT. The highlighted parts, i.e., Lines 7, 15 and 16, indicate where TNT differs signiﬁcantly from Shampoo. D Details of the Experiments In our implementations of the algorithms that we compared to TNT, we included in all of the techniques like weight decay and momentum, so that our numerical experiments would provide a FAIR comparison. Consequently, we did not include some special techniques that have been incorporated in some of the algorithms as described in previously published papers, since to keep the comparisons fair, we would have had to incorporate such techniques in all of the algorithms (see Section D.1.1 for more details). D.1 Competing Algorithms In SGD with momentum, we updated the momentum of the gradientm“µ¨m`gat every iteration, where gdenotes the minibatch gradient at current iteration. The gradient momentum is also used in the second-order methods, in our implementations. For Adam, we follow exactly the algorithm in [23] with β1 “0.9 and β2 “0.999. In particular, we follow the approach in [23] in estimating the momentum of gradient by m“β1 ¨m`p1 ´β1q¨ g. The role of β1 and β2 is similar to that of µand βin Algorithm 3 and Algorithm 4, as we will describe below. In the experiments on CNNs, we use weight decay (same as in Algorithms 3 and 4) on SGD and Adam, similar to SGDW and AdamW in [29] (for further details, see Section D.3). 16Algorithm 3 Tensor-Normal Training Require: Given batch size m, learning rate tαtutě1, weight decay factor γ, damping value ϵ, statistics update frequency T1, inverse update frequency T2 1: µ“0.9, β “0.9 2: Initialize yGpiq l “ ErGpiq l s(l “ 1,..,k , i “ 1,...,k l) by iterating through the whole dataset, {∇WlL “0 (l“1,...,L ) 3: for t“1,2,... do 4: Sample mini-batch Mt of size m 5: Perform a forward-backward pass over Mt to compute the mini-batch gradient ∇L 6: if t”0 pmod T1qthen 7: Perform another backward pass over Mt with ysampled from the predictive distribution to compute Gl “DWl averaged across Mt (l“1,...,L ) 8: end if 9: for l“1,...L do 10: {∇WlL “µ{∇WlL `∇WlL 11: if t”0 pmod T1qthen 12: Update yGpiq l “βyGpiq l `p1 ´βqGl piq for i“1,...,k l 13: end if 14: if t”0 pmod T2qthen 15: Determine ˜Uplq 1 ,..., ˜Uplq kl from yGp1q l ,..., zGpklq l by (5) 16: Recompute p˜Uplq 1 `ϵIq´1,..., p˜Uplq kl `ϵIq´1 17: end if 18: pl “ {∇WlL ˆ1 p˜Uplq 1 `ϵIq´1 ˆ2 ¨¨¨ˆ k p˜Uplq k `ϵIq´1 19: pl “pl `γWl 20: Wl “Wl ´αt ¨pl. 21: end for 22: end for 17D.1.1 Shampoo Algorithm 4 Shampoo Require: Given batch size m, learning rate tαtutě1, weight decay factor γ, damping value ϵ, statistics update frequency T1, inverse update frequency T2 1: µ“0.9, β “0.9 2: Initialize yGpiq l “ ErGpiq l s(l “ 1,..,k , i “ 1,...,k l) by iterating through the whole dataset, {∇WlL “0 (l“1,...,L ) 3: for t“1,2,... do 4: Sample mini-batch Mt of size m 5: Perform a forward-backward pass over the current mini-batch Mt to compute the minibatch gradient ∇L 6: for l“1,...L do 7: {∇WlL “µ{∇WlL `∇WlL 8: if t”0 pmod T1qthen 9: Update yGpiq l “βyGpiq l `p1 ´βqGl piq for i“1,...,k l where Gl “∇WlL 10: end if 11: if t”0 pmod T2qthen 12: Recompute ˆyGp1q l `ϵI ˙´1{2kl ,..., ˆzGpklq l `ϵI ˙´1{2kl with the coupled Newton method 13: end if 14: pl “ {∇WlL ˆ1 ˆyGp1q l `ϵI ˙´1{2kl ˆ2 ¨¨¨ˆ k ˆzGpklq l `ϵI ˙´1{2kl 15: pl “pl `γWl 16: Wl “Wl ´αt ¨pl 17: end for 18: end for In Algorithm 4, we present our implementation of Shampoo, which mostly follows the description of it given in [18]. Several major improvements are also included, following the suggestions in [3], including: 1. In Line 9 of Algorithm 4, a moving average is used to update the estimates yGpiq l , as is done in our implementations of TNT and KFAC. This approach is also used in Adam, whereas summing the Gpiq l ’s over all iterations, as in [18], is analogous to what is done in AdaGrad, upon which Shampoo is based. 2. In Line 12 of Algorithm 4, we use a coupled Newton method to compute inverse roots of the matrices (as proposed in [3]), rather than using SVD. The coupled Newton approach has been shown to be much faster than SVD, and also preserves relatively good accuracy in terms of computing inverse roots. The coupled Newton method performs reasonably well (without tuning) using a max iteration number of 100 and an error tolerance of 1e-6. Some other modiﬁcations proposed in [3] are not included in our implementation of Shampoo, mainly because these modiﬁcations can also be applied to TNT, and including them only in Shampoo would introduce other confounding factors. (i) We did not explore multiplying the damping term in the pre-conditioner by the maximum eigenvalue λmax of the contraction matrix. Moreover, this modiﬁcation is somewhat problematic, since, if the model contains any variables that always have a zero gradient (e.g. the bias in a convolutional layer that is followed by a BN layer), the optimizer would become unstable because the pre-conditioner of the zero-gradient variables would be the zero matrix, (note that in this case λmax “0). (ii) We did not explore the diagonal variant of Shampoo, as we mainly focused on the comparison between different pre-conditioning matrices, and TNT can also be extended to a diagonal 18version; similarly, we did not explore the variant proposed in [3] that divides large tensors into small blocks. D.1.2 KFAC In this subsection, we brieﬂy describe our implementation of KFAC. The preconditioning matrices that we used for linear layers and convolutional layers are precisely as those described in [34] and [17], respectively. For the parameters in the BN layers, we used the gradient direction, exactly as in https://github.com/alecwangcq/KFAC-Pytorch. As in our implementations of TNT and Shampoo, and as suggested in [17], we did a warm start to estimate the pre-conditioning KFAC matrices in an initialization step that iterated through the whole data set, and adopted a moving average scheme to update them with β “0.9 afterwards. In inverting the KFAC matrices and computing the updating direction, • for the autoencoder experiments, we inverted the damped KFAC matrices and used them to compute the updating direction, where the damping factors for both Aand Gwere set to be? λ, where λis the overall damping value;6 • for the CNN experiments, we followed the SVD (i.e. eigenvalue decomposition) implemen- tation suggested in https://github.com/alecwangcq/KFAC-Pytorch, which, as we veriﬁed, performs better than splitting the damping value and inverting the damped KFAC matrices (as suggested in [34, 17]). Further, we implemented weight decay exactly as in TNT (Algorithm 3) and Shampoo (Algorithm 4). D.2 Experiment Settings for the Autoencoder Problems Table 2: Hyper-parameters (learning rate, damping) used to produce Figure 2 Name MNIST FACES TNT (1e-4, 0.1) (1e-6, 0.003) KFAC (0.003, 0.3) (0.1, 10) Shampoo (3e-4, 3e-4) (3e-4, 3e-4) Adam (1e-4, 1e-4) (1e-4, 1e-4) SGD-m (0.003, -) (0.001, -) MNIST has 60,000 training data, whereas FACES7 has 103,500 training data. For all algorithms, we used a batch size of 1,000 at every iteration. The autoencoder model used for MNIST has layer widths 784-1000-500-250-30-250-500-1000-784 with ReLU activation functions, except for the middle layer which uses a linear function and the last layer which uses a sigmoid function. The autoencoder model used for FACES has layer widths 625-2000-1000-500-30-500-1000-2000-625 with ReLU activation functions, except for the middle and last layers which use linear functions. We used binary entropy loss for MNIST and squared error loss for FACES. The above settings largely mimic the settings in [32, 34, 5, 15]. Since we primarily focused on optimization rather than generalization in these tasks, we did not includeL2 regularization or weight decay. In order to obtain Figure 2, we ﬁrst conducted a grid search on the learning rate (lr) and damping value based on the criteria of minimizing the training loss. The ranges of the grid searches used for the algorithms in our tests were: • SGD-m: – lr: 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 6Note that there are more sophisticated ways of splitting the damping value, such as one that makes use of the norms of the undamped matrices, to enforce that the two matrices have the same norm. See [34] and [17] for more on this. 7Downloadable at www.cs.toronto.edu/~jmartens/newfaces_rot_single.mat. 19– damping: not applicable • Adam: – lr: 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01 – damping (i.e. the ϵhyperparameter of Adam): 1e-8, 1e-4, 1e-2 • Shampoo: – lr: 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003 – damping (i.e. ϵin Algorithm 4): 1e-4, 3e-4, 0.001, 0.003, 0.01 • TNT: – lr: 1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001 – damping (i.e. ϵin Algorithm 3): 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 • KFAC: – lr: 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 – damping: 0.01, 0.03, 0.1, 0.3, 1, 3, 10 The best hyper-parameter values determined by our grid searches are listed in Table 2. D.3 Experiment Settings for the CNN Problems Table 3: Hyper-parameters ( initial learning rate, weight decay factor) used to produce Figure 3 and the average validation accuracy across 5 runs with different random seeds shown in Figure 3 Name CIFAR-10 + ResNet32 CIFAR-100 + VGG16 TNT (1e-4, 10) Ñ93.08% (3e-5, 10) Ñ73.33% KFAC (0.01, 0.1) Ñ92.85% (3e-4, 0.1) Ñ74.33% Shampoo (0.01, 0.1) Ñ92.63% (0.003, 0.1) Ñ72.82% Adam (0.003, 0.1) Ñ92.92% (3e-5, 10) Ñ72.27% SGD-m (0.03, 0.01) Ñ93.06% (0.03, 0.01) Ñ73.44% Both CIFAR-10 and CIFAR-100 have 50,000 training data and 10,000 testing data (used as the validation set in our experiments). For all algorithms, we used a batch size of 128 at every iteration. In training, we applied data augmentation as described in [25], including random horizontal ﬂip and random crop. The ResNet32 model refers to the one in Table 6 of [19], whereas the VGG16 model refers to model D of [42], with the modiﬁcation that batch normalization layers were added after all of the convolutional layers in the model. It is worth noting that, in TNT and Shampoo, for the weight tensor in the convolutional layers, instead of viewing it as a 4-way tensor, we view it as a 3-way tensor, where the size of its 3 ways (dimensions) corresponds to the size of the ﬁlter, the number of input channel, and the number of the output channel, respectively. As a result, the preconditioning matrices of TNT and Shampoo will come from the Kronecker product of three matrices, rather than four matrices. Weight decay, which is related to, but not the same asL2 regularization added to the loss function, has been shown to help improve generalization performance across different optimizers [29, 45]. In our experiments, we adopted weight decay for all algorithms. The use of weight decay for TNT and Shampoo is described in Algorithm 3 and Algorithm 4, respectively, and is similarly applied to KFAC. Also note that weight decay is equivalent to L2 regularization for pure SGD (without momentum). However, the equivalence does not hold for SGD with momentum. For the sake of a fair comparison, we also applied weight decay for SGD-m. For TNT and Shampoo, we set ϵ “0.01. We also tried values around 0.01 and the results were not sensitive to the value of ϵ; hence, ϵcan be set to 0.01 as a default value. For KFAC, we set the overall damping value to be 0.03, as suggested in the implementation in https://github.com/ alecwangcq/KFAC-Pytorch. We also tried values around 0.03 for KFAC and conﬁrmed that 0.03 is a good default value. 20In order to obtain Figure 3, we ﬁrst conducted a grid search on the initial learning rate (lr) and weight decay (wd) factor based on the criteria of maximizing the classiﬁcation accuracy on the validation set. The range of the grid searches for the algorithms in our tests were: • SGD-m: – lr: 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 – wd: 0.001, 0.01, 0.1, 1 • Adam: – lr: 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 – wd: 1e-4, 0.001, 0.01, 0.1, 1, 10, 100 • Shampoo: – lr: 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1 – wd: 0.01, 0.1, 1, 10 • TNT: – lr: 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001 – wd: 1, 10, 100 • KFAC: – lr: 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 – wd: 0.01, 0.1, 1 The best hyper-parameter values, and the validation classiﬁcation accuracy obtained using them, are listed in Table 3. D.4 A Comparison between TNT and TNT-EF 0 100 200 300 epoch 102 training loss 0 200 400 process time (second) 102 training loss TNT TNT-EF a) MNIST autoencoder 0 200 400 epoch 101 102 training loss 0 1000 2000 process time (second) 101 102 training loss TNT TNT-EF b) FACES autoencoder Figure 4: Optimization performance comparison of the TNT and TNT-EF algorithms on two autoencoder problems. 210 50 100 epoch 10 1 100 training loss 0 1000 2000 process time (second) 10 1 100 training loss 0 50 100 epoch 10 1 100 testing error 0 1000 2000 process time (second) 10 1 100 testing error TNT TNT-EF a) CIFAR-10, ResNet-32 0 50 100 epoch 10 2 10 1 100 training loss 0 2000 4000 process time (second) 10 2 10 1 100 training loss 0 50 100 epoch 100 3 × 10 1 4 × 10 1 6 × 10 1 testing error 0 2000 4000 process time (second) 100 3 × 10 1 4 × 10 1 6 × 10 1 testing error TNT TNT-EF b) CIFAR-100, VGG16 Figure 5: Generalization ability comparison of the TNT and TNT-EF algorithms on two CNN models. The upper row depicts the training loss, whereas the lower row depicts the validation classiﬁcation error. Table 4: Hyper-parameters (learning rate, damping) used to produce Figure 4 Name MNIST FACES TNT-EF (3e-6, 0.01) (3e-6, 0.01) Table 5: Hyper-parameters ( initial learning rate, weight decay factor) used to produce Figure 5 Name CIFAR-10 + ResNet32 CIFAR-100 + VGG16 TNT-EF (1e-4, 10) Ñ93.62% (3e-6, 100) Ñ72.85% In this subsection, we compare our proposed TNT algorithm against a variant of it, TNT-EF, which uses an empirical Fisher (EF) preconditioning matrix in place of the true Fisher matrix. In other words, TNT-EF does everything speciﬁed in Algorithm 3, except that it does not perform the extra backward pass in Line 7 of Algorithm 3. When updating the matricesyGpiq l , TNT-EF uses the empirical minibatch gradient, rather than the sampling-based minibatch gradient, i.e. the one coming from the extra backward pass. We conducted a hyper-parameter grid search for TNT-EF, following the same procedure as the one that was used for TNT, whose performance was plotted in Figures 2 and 3. The best values for the TNT-EF hyper-parameters that we obtained are listed in Tables 4 and 5. We then plotted in Figures 4 and 5, the performance of TNT-EF, along with that of TNT, using for it the hyper-parameters given in Tables 2 and 3. As shown in Figures 4 and 5, TNT performed at least as well as TNT-EF, on the MNIST and CIFAR-10 problems, and performed somewhat better on the FACES and CIFAR-100 problems, which conﬁrms the widely held opinion that the Fisher matrix usually carries more valuable curvature information than the empirical Fisher metrix. 22D.5 More on Hyper-parameter Tuning 0 200 400 600 epoch 102 training loss 0 200 400 process time (second) 102 training loss TNT KFAC Shampoo Adam SGD-m a) MNIST autoencoder 0 500 1000 epoch 101 102 training loss 0 1000 2000 process time (second) 101 102 training loss TNT KFAC Shampoo Adam SGD-m b) FACES autoencoder Figure 6: Optimization performance of TNT, KFAC, Shampoo, Adam, and SGD-m on two autoen- coder problems, with more extensive tuning Table 6: Hyper-parameter values used to produce Figure 6 Problem Algorithm (learning rate, damping, µ, β) MNIST TNT (1e-4, 0.1, 0.9, 0.9) MNIST KFAC (3e-5, 0.01, 0.999, 0.999) MNIST Shampoo (1e-4, 3e-4, 0.99, 0.99) MNIST Adam (1e-4, 1e-4, 0.99, 0.99) MNIST SGD-m (0.001, -, 0.99, -) FACES TNT (1e-6, 0.003, 0.9, 0.9) FACES KFAC (0.01, 3, 0.99, 0.99) FACES Shampoo (1e-4, 3e-4, 0.99, 0.999) FACES Adam (1e-4, 1e-4, 0.9, 0.9) FACES SGD-m (0.001, -, 0.9, -) In this subsection, we expand on the experiments whose results are plotted in Figure 2, by in- corporating the tuning of more hyper-parameters. To be more speciﬁc, we tuned the following hyper-parameters jointly: 1. SGD-m: learning rate and µ; 2. all other algorithms 8: learning rate, damping, µ, and β. The searching range for learning rate and damping is the same as in Sec D.2, whereas the searching range for µand βwere set to be t0.9,0.99,0.999u. The obtained values for the hyper-parameters are listed in Table 6. Figure 6 depicts the performance of different algorithms with hyper-parameters obtained from the aforementioned more extensive tuning process. Comparing the performance of different algorithms in Figure 6, we can see that the observations we made from Figure 2 still hold to a large extent. Moreover, with extensive tuning, second-order methods seem to perform similarly with each other, and are usually better than well-tuned ﬁrst order methods on these problems. As a ﬁnal point, we would like to mention that one could also replace the constant learning rate for all of the algorithms tested with a \"warm-up, then decay\" schedule, which has been shown to result in good performance on these problems in [3]. Also, one could perform a more extensive tuning for the CNN problems. In particular, one could tune the initial learning rate, weight decay factor, damping, µ, and βjointly for the CNN problems. See more in [8, 40] for the importance and suggestions on hyper-parameter tuning. Moreover, see [2] for other relevant numerical results, in particular for KFAC and Shampoo. In [2], KFAC is shown to work extremely well with a higher frequency of inversion, another direction for experiments that could be explored. 8For Adam, µand βrefer to β1 and β2, respectively. 23Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 7. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 7. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 4. (b) Did you include complete proofs of all theoretical results? [Yes] See Section B in the Appendix. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] The code and instructions are included in the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 6, and Section D in the Appendix. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Section 6. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 6. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 6. (b) Did you mention the license of the assets? [Yes] The data and models used in the paper have been properly cited. Licenses can be found in the corresponding citations, if they exist. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 24",
      "meta_data": {
        "arxiv_id": "2106.02925v3",
        "authors": [
          "Yi Ren",
          "Donald Goldfarb"
        ],
        "published_date": "2021-06-05T15:57:22Z",
        "pdf_url": "https://arxiv.org/pdf/2106.02925v3.pdf",
        "github_url": "https://github.com/renyiryry/tnt_neurips_2021"
      },
      "llm_extracted_info": {
        "main_contributions": "Proposed Tensor Normal Training (TNT), a novel approximate natural gradient method leveraging the tensor normal (TN) distribution to approximate the probabilistically based Fisher matrix. TNT is model-agnostic, requiring only the shape of training parameters, and uses block-wise covariance of sampling-based gradients with a Kronecker separable structure for tractable Fisher approximation. It introduces mild memory and computational overhead compared to first-order methods. Experimentally, TNT shows superior optimization performance over state-of-the-art first-order methods, comparable performance to second-order methods (KFAC, Shampoo), and similar generalization ability with fewer epochs. Theoretically, TNT is proven to converge to a stationary point under mild assumptions.",
        "methodology": "TNT approximates the natural gradient by using a block-diagonal Fisher matrix, with each block corresponding to the covariance of a tensor variable. It assumes that the sampling-based gradient for each tensor variable (Wl) follows a zero-mean Tensor-Normal (TN) distribution, resulting in a Kronecker-factored Fisher matrix (FW = U1 \biguplus ... \biguplus Uk). The method identifies unique covariance parameters (\tilda{U}i) by enforcing that the average eigenvalues of the covariance matrices for each tensor dimension are equal. The update direction is calculated by applying the inverse of the Kronecker-factored Fisher matrix to the gradient. For practical implementation, moving average estimates of relevant statistics are used, and inverse computations are amortized over multiple iterations.",
        "experimental_setup": "TNT was compared against state-of-the-art first-order (Adam, SGD with momentum) and second-order (KFAC, Shampoo) methods. Experiments ran on a V100 GPU and Xeon Gold CPUs using PyTorch, with hyperparameters tuned via grid search and results averaged over 5 random seeds. Optimization tasks involved two autoencoder problems on MNIST (784-1000-500-250-30-250-500-1000-784) with binary entropy loss and FACES (625-2000-1000-500-30-500-1000-2000-625) with squared error loss, using a batch size of 1,000. Generalization tasks used CNNs (ResNet32 on CIFAR-10, VGG16 on CIFAR-100) with batch size 128, data augmentation, learning rate decay, and weight decay. Second-order methods had Fisher/inverse update frequencies (T1/T2) of 1/20 for autoencoders and 10/100 for CNNs. A variant, TNT-EF (using empirical Fisher), was also compared.",
        "limitations": "The study's experiments were limited to existing models and datasets, precluding evaluation on larger models like ImageNet or advanced NLP tasks due to resource constraints. The proposed method was not investigated in a distributed setting. Direct comparisons with state-of-the-art Kronecker-based quasi-Newton methods were not included. The paper also acknowledges the general potential for negative societal impacts if the model or data design is flawed or contains bias, though this aspect was beyond the scope of the current work.",
        "future_research_directions": "Future work includes extending the TNT method to a distributed computing setting for deep learning. Further experiments on larger-scale models, such as ImageNet, and advanced models for natural language processing tasks are suggested to fully explore TNT's potential. Comparisons with state-of-the-art Kronecker-based quasi-Newton methods are also a direction for future research. Additionally, incorporating more sophisticated learning rate schedules (e.g., warm-up followed by decay) and conducting more extensive joint hyperparameter tuning for CNN problems could be beneficial.",
        "experimental_code": "def train_model(home_path = '/home/jupyter/',dataset_name = 'CIFAR-10',algorithm = 'TNT',lr = 1e-4,damping_value = 0.01,weight_decay = 0,):args = {}args['list_lr'] = [lr]args['weight_decay'] = weight_decayif dataset_name == 'CIFAR-10':args['dataset'] = 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'args['initialization_pkg'] = 'kaiming_normal'elif dataset_name == 'CIFAR-100':args['dataset'] = 'CIFAR-100-onTheFly-vgg16-NoAdaptiveAvgPoolNoDropout-BN-no-regularization'args['initialization_pkg'] = 'normal'elif dataset_name == 'MNIST':args['dataset'] = 'MNIST-autoencoder-relu-N1-1000-sum-loss-no-regularization'args['initialization_pkg'] = 'normal'elif dataset_name == 'FACES':args['dataset'] = 'FacesMartens-autoencoder-relu-no-regularization'args['initialization_pkg'] = 'normal'else:print('dataset_name')print(dataset_name)sys.exit()if dataset_name in ['MNIST']:args['if_max_epoch'] = 0args['max_epoch/time'] = 500elif dataset_name in ['FACES']:args['if_max_epoch'] = 0args['max_epoch/time'] = 2000elif dataset_name in ['CIFAR-10', 'CIFAR-100']:if algorithm in ['SGD-m', 'Adam']:args['if_max_epoch'] = 1args['max_epoch/time'] = 200args['num_epoch_to_decay'] = 60args['lr_decay_rate'] = 0.1elif algorithm in ['TNT', 'Shampoo', 'KFAC']:args['if_max_epoch'] = 1args['max_epoch/time'] = 100args['num_epoch_to_decay'] = 40args['lr_decay_rate'] = 0.1else:print('algorithm')print(algorithm)sys.exit()else:print('dataset_name')print(dataset_name)sys.exit()args['momentum_gradient_rho'] = 0.9if algorithm == 'SGD-m':args['momentum_gradient_dampening'] = 0if dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'SGD-momentum'elif dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'SGD-LRdecay-momentum'else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm == 'Adam':args['RMSprop_epsilon'] = damping_valueargs['RMSprop_beta_2'] = 0.999args['momentum_gradient_dampening'] = 0.9if dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'Adam-noWarmStart-momentum-grad-LRdecay'elif dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'Adam-noWarmStart-momentum-grad'else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm in ['TNT', 'Shampoo']:if algorithm in ['Shampoo']:args['shampoo_if_coupled_newton'] = Trueelif algorithm in ['TNT']:args['shampoo_if_coupled_newton'] = Falseargs['shampoo_epsilon'] = damping_valueargs['if_Hessian_action'] = Falseargs['shampoo_decay'] = 0.9args['shampoo_weight'] = 0.1args['momentum_gradient_dampening'] = 0if dataset_name in ['CIFAR-10', 'CIFAR-100']:if algorithm == 'TNT':args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'elif algorithm == 'Shampoo':args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'args['shampoo_update_freq'] = 10args['shampoo_inverse_freq'] = 100elif dataset_name in ['MNIST', 'FACES']:if algorithm == 'TNT':args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'elif algorithm == 'Shampoo':args['algorithm'] = 'shampoo-allVariables-filterFlattening-warmStart-lessInverse-momentum-grad'args['shampoo_update_freq'] = 1args['shampoo_inverse_freq'] = 20else:print('dataset_name')print(dataset_name)sys.exit()elif algorithm == 'KFAC':args['kfac_if_update_BN'] = Trueargs['kfac_if_BN_grad_direction'] = Trueargs['kfac_rho'] = 0.9args['kfac_damping_lambda'] = damping_valueargs['momentum_gradient_dampening'] = 0if dataset_name in ['FACES', 'MNIST']:args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad'args['kfac_if_svd'] = Falseargs['kfac_cov_update_freq'] = 1args['kfac_inverse_update_freq'] = 20elif dataset_name in ['CIFAR-100', 'CIFAR-10']:args['algorithm'] = 'kfac-correctFisher-warmStart-no-max-no-LM-momentum-grad-LRdecay'args['kfac_if_svd'] = Falseargs['kfac_cov_update_freq'] = 10args['kfac_inverse_update_freq'] = 100else:print('dataset_name')print(dataset_name)sys.exit()else:print('algorithm')print(algorithm)sys.exit()args['record_epoch'] = 1args['seed_number'] = 9999args['num_threads'] = 8args['home_path'] = home_pathargs['if_gpu'] = Trueargs['if_test_mode'] = Falseargs['if_auto_tune_lr'] = Falseargs['if_grafting'] = False_ = tune_lr(args)return",
        "experimental_info": "Method: TNT\nAlgorithm name: 'TNT'\nLearning rate (lr): 1e-4\nDamping value: 0.01\nWeight decay: 0\nMomentum gradient rho: 0.9\nMomentum gradient dampening: 0\n\nDataset-specific settings for CIFAR-10:\n  Dataset argument: 'CIFAR-10-onTheFly-N1-128-ResNet32-BN-PaddingShortcutDownsampleOnly-NoBias-no-regularization'\n  Initialization package: 'kaiming_normal'\n  Max epoch/time: 100 epochs\n  Learning rate decay: True\n  Number of epochs to decay LR: 40\n  LR decay rate: 0.1\n  Algorithm argument: 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'\n  Shampoo update frequency: 10\n  Shampoo inverse frequency: 100\n  Tau (regularization parameter, from from_dataset_to_N1_N2): 0\n\nDataset-specific settings for MNIST/FACES:\n  Algorithm argument: 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'\n  Shampoo update frequency: 1\n  Shampoo inverse frequency: 20\n\nCommon settings for TNT:\n  Shampoo if coupled newton: False\n  Shampoo epsilon: 0.01 (damping_value)\n  If Hessian action: False\n  Shampoo decay: 0.9\n  Shampoo weight: 0.1\n\nGeneral experimental settings:\n  Record epoch: 1\n  Seed number: 9999\n  Number of threads: 8\n  Home path: '/home/jupyter/' (default)\n  If GPU: True\n  If test mode: False\n  If auto tune LR: False\n  If grafting: False\n  If LR decay: True (handled by get_params, e.g., for CIFAR-10/100 setup)\n  If momentum gradient: True (handled by get_params)"
      }
    },
    {
      "title": "Tensor Normal Training for Deep Learning Models",
      "abstract": "Despite the predominant use of first-order methods for training deep learning\nmodels, second-order methods, and in particular, natural gradient methods,\nremain of interest because of their potential for accelerating training through\nthe use of curvature information. Several methods with non-diagonal\npreconditioning matrices, including KFAC, Shampoo, and K-BFGS, have been\nproposed and shown to be effective. Based on the so-called tensor normal (TN)\ndistribution, we propose and analyze a brand new approximate natural gradient\nmethod, Tensor Normal Training (TNT), which like Shampoo, only requires\nknowledge of the shape of the training parameters. By approximating the\nprobabilistically based Fisher matrix, as opposed to the empirical Fisher\nmatrix, our method uses the block-wise covariance of the sampling based\ngradient as the pre-conditioning matrix. Moreover, the assumption that the\nsampling-based (tensor) gradient follows a TN distribution, ensures that its\ncovariance has a Kronecker separable structure, which leads to a tractable\napproximation to the Fisher matrix. Consequently, TNT's memory requirements and\nper-iteration computational costs are only slightly higher than those for\nfirst-order methods. In our experiments, TNT exhibited superior optimization\nperformance to state-of-the-art first-order methods, and comparable\noptimization performance to the state-of-the-art second-order methods KFAC and\nShampoo. Moreover, TNT demonstrated its ability to generalize as well as\nfirst-order methods, while using fewer epochs.",
      "full_text": "Tensor Normal Training for Deep Learning Models Yi Ren, Donald Goldfarb Department of Industrial Engineering and Operations Research Columbia University New York, NY 10027 {yr2322, goldfarb}@columbia.edu Abstract Despite the predominant use of ﬁrst-order methods for training deep learning mod- els, second-order methods, and in particular, natural gradient methods, remain of interest because of their potential for accelerating training through the use of cur- vature information. Several methods with non-diagonal preconditioning matrices, including KFAC [34], Shampoo [18], and K-BFGS [15], have been proposed and shown to be effective. Based on the so-called tensor normal (TN) distribution [31], we propose and analyze a brand new approximate natural gradient method, Tensor Normal Training (TNT), which like Shampoo, only requires knowledge of the shape of the training parameters. By approximating the probabilistically based Fisher matrix, as opposed to the empirical Fisher matrix, our method uses the block-wise covariance of the sampling based gradient as the pre-conditioning matrix. Moreover, the assumption that the sampling-based (tensor) gradient follows a TN distribution, ensures that its covariance has a Kronecker separable structure, which leads to a tractable approximation to the Fisher matrix. Consequently, TNT’s memory requirements and per-iteration computational costs are only slightly higher than those for ﬁrst-order methods. In our experiments, TNT exhibited superior optimization performance to state-of-the-art ﬁrst-order methods, and compara- ble optimization performance to the state-of-the-art second-order methods KFAC and Shampoo. Moreover, TNT demonstrated its ability to generalize as well as ﬁrst-order methods, while using fewer epochs. 1 Introduction First-order methods are currently by far the most popular and successful optimization methods for training deep learning models. Stochastic gradient descent (SGD) [39] uses the (stochastic) gradient direction to guide its update at every iteration. Adaptive learning rate methods, including AdaGrad [11], RMSprop [20], and Adam [23], scale each element of the gradient direction (possibly modiﬁed to incorporate momentum) by the square root of the second moment of each element of the gradient. These ﬁrst-order methods use little curvature information to \"pre-condition\" the gradient direction; SGD uses an identity pre-conditioning matrix, whereas the others use a diagonal matrix. On the other hand, second-order methods attempt to greatly accelerate the optimization process by exploring the rich curvature information of the problem. Traditional second-order methods such as Newton’s method, BFGS [6, 13, 14, 41], and limited-memory BFGS (L-BFGS) [ 28], without modiﬁcation, are not practical in a deep learning setting, because these methods require enormous amounts of memory and computational effort per iteration due to the huge number of parameters such models have. Some second-order methods have been proposed to deal with the non-convexity and stochasiticity of objective functions arising in machine learning (see e.g. [ 36, 7, 16, 44]), but directly using these methods to train deep learning models still requires large amounts of memory and computing resources. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2106.02925v3  [cs.LG]  21 Dec 2021Recently, there has been considerable advancement in the development of second-order methods that are suitable for deep learning models with huge numbers of parameters. These methods usually ap- proach pre-conditioning of the gradient in a modular way, resulting in block-diagonal pre-conditioning matrices, where each block corresponds to a layer or a set of trainable parameters in the model. Inspired by the idea of the natural gradient (NG) method [1], [34] proposed KFAC, an NG method that uses a Kronecker-factored approximation to the Fisher matrix as its pre-conditioning matrix that can be applied to multilayer perceptrons, and which has subsequently been extended to other architectures, such as convolutional neural networks [17] and recurrent neural networks [35]. Kronecker-factored preconditioners [15, 38] based on the structure of the Hessian and quasi-Newton methods have also been developed. Despite the great success of these efﬁcient and effective second-order methods, developing such methods requires careful examination of the structure of the preconditioning matrix to design appropriate approximations for each type of layer in a model. Another well-recognized second-order method, Shampoo [18, 3], extends the adaptive learning rate method AdaGrad, so that the gradient is pre-conditioned along every dimension of the underlying tensor of parameters in the model, essentially replacing the diagonal pre-conditioning matrix of the adaptive learning rate methods by a block diagonal Kronecker-factored matrix which can be viewed as an approximation to a fractional power of the empirical Fisher (EF) matrix. However, while estimating the Fisher matrix, in a deep learning setting, by the EF matrix saves some computational effort, it usually does not capture as much valuable curvature information as the Fisher matrix [26]. Variants of the normal distribution, i.e. the matrix-normal distribution [ 9] and the tensor-normal distribution [31], have been proposed to estimate the covariance of matrix and higher-order tensor observations, respectively. By imposing a Kronecker structure on the covariance matrix, the resulting covariance estimate requires a vastly reduced amount of memory, while still capturing the interactions between the various dimensions of the respective matrix or tensor. Iterative MLE methods for estimating the parameters of matrix-normal and tensor-normal distributions have been examined in e.g. [12, 31], and various ways to identify the unique representation of the distribution parameters have been proposed in [43, 10]. However, to the best of our knowledge, this advanced statistical methodology has not been used to develop optimization methods for deep learning. In this paper, we describe a ﬁrst attempt to do this and demonstrate its great potential. Our Contributions. In this paper, we propose a brand new approximate natural gradient (NG) method, Tensor-Normal Training (TNT), that makes use of the tensor normal distribution to approxi- mate the Fisher matrix. Signiﬁcantly, the TNT method can be applied to any model whose training parameters are a collection of tensors without knowing the exact structure of the model. To achieve this, we ﬁrst propose a new way, that is suitable for optimization, to identify the covariance parameters of tensor normal (TN) distributions, in which the average eigenvalues of the covariance matrices corresponding to each of the tensor dimensions are required to be the same (see Section 3). By using the Kronecker product structure of the TN covariance, TNT only introduces mild memory and per-iteration computational overhead compared with ﬁrst-order methods. Also, TNT’s memory usage is the same as Shampoo’s and no greater than KFAC’s, while its per-iteration computational needs are no greater than Shampoo’s and KFAC’s (see Section 5). The effectiveness of TNT is demonstrated on deep learning models. Speciﬁcally, on standard autoencoder problems, when optimization performance is compared, TNT converges faster than the benchmark ﬁrst-order methods and roughly the same rate as the benchmark second-order methods. Moreover, on standard CNN models, when generalization is concerned, TNT is able to achieve roughly the same level of validation accuracy as the ﬁrst-order methods, but using far fewer epochs (see Section 6). We also prove that, if the statistics used in TNT can be estimated ideally, it converges to a stationary point under mild assumptions (see Section 4). 2 Preliminaries Supervised Learning. Throughout this paper, we consider the classic supervised learning setting where we learn the parameters θof a model, by minimizing Lpθq“ 1 N řN i“1 lpyi,fθpxiqq, where tpxi,yiquN i“1 denotes a given dataset (xi being the input to the model and yi being the target), fθpxiq denotes the output of the model when xi is provided as the input, and l denotes a loss function 2(e.g. least-squares loss for regression and cross entropy loss for classiﬁcation) that measures the discrepancy between the model output fθpxiqand the target yi. Natural Gradient Method and the Fisher Matrix. In a ﬁrst-order method, say SGD, the updating direction is always derived from an estimate to the gradient direction ∇θLpθq. In a natural gradient (NG) method [1], however, the Fisher matrix is used as a pre-conditioning matrix that is applied to the gradient direction. As shown in [34], the Fisher matrix is deﬁned as F “Ex„Qx,y„pp¨|x,θq ” ∇θlog ppy|x,θqp∇θlog ppy|x,θqqJ ı , (1) where Qx is the data distribution of xand pp¨|x,θqis the density function of the conditional distribu- tion deﬁned by the model with a given input x. In many cases, such as when pis associated with a Gaussian distribution and the loss function l measures least-squares loss, or when pis associated with a multinomial distribution and lis cross- entropy loss, log pis equivalent to l(see e.g. [33, 34]). Hence, if Dθdenotes the gradient of lw.r.t. θ for a given xand y, we have that F “Ex„Qx,y„prDθDθJs. Consequently, one can sample xfrom Qx and perform a forward pass of the model, then sample yfrom pp¨|x,θq, and perform a backward pass to compute Dθ, and then use DθDθJto estimate F. We call Dθa sampling-based gradient, as opposed to the empirical gradient ∇θlpyi,fθpxiqqwhere pxi,yiqis one instance from the dataset. It is worth noting that the ﬁrst moment of Dθis zero. To see this, note that, with given x, Ey„pr∇θlog ppy|x,θqs“ ż ∇θlog ppy|x,θqppy|x,θqdy“ ż ∇θppy|x,θqdy “∇θ ˆż ppy|x,θqdy ˙ “∇θ1 “0. Hence, Ex„Qx,y„prDθs“ Ex„Qx tEy„pr∇θlog ppy|x,θqs| xu“ 0. Thus, the Fisher matrix F can be viewed as the covariance matrix of Dθ. Note that the empirical Fisher CANNOT be viewed as the covariance of the empirical gradient, because the ﬁrst moment of the latter is, in general, NOT zero. Tensor-Normal Distribution. The development of our new method makes use the so-called tensor- normal distribution [31, 10]: Deﬁnition 1. An arbitrary tensor GPRd1ˆ¨¨¨ˆdk is said to follow a tensor normal (TN) distribution with mean parameter M PRd1ˆ¨¨¨ˆdk and covariance parameters U1 PRd1ˆd1 , ..., Uk PRdkˆdk if and only if vecpGq„ NormalpvecpMq,U1 b¨¨¨b Ukq. In the above deﬁnition, the vec operation refers to the vectorization of a tensor, whose formal deﬁnition can be found in Sec A in the Appendix. Note that matrix-normal distribution can be viewed as a special case of TN distribution, where k “2. Compared with a regular normal distribution, whose covariance matrix has śk i“1 d2 i elements, the covariance of a k-way tensor-normal distribution is stored in ksmaller matrices with a total number of elements equal to řk i“1 d2 i. To estimate the covariance submatrices U1,...,U k, the following property (e.g., see [10]) is used: ErGpiqs“ Ui ¨ ź j‰i trpUjq, (2) where Gpiq :“matipGqmatipGqJ PRdiˆdi denotes the contraction of Gwith itself along all but the ith dimension and mati refers to matricization of a tensor (see Section A for the formal deﬁnitions). By (2), we can sample Gto obtain estimates of the Gpiq’s, and hence, estimates of the Ui’s. The complexity of computing Gpiqis di śk j“1 dj, which is also far less than the complexity of computing vecpGqvecpGqJneeded to estimate the covariance of a regular normal distribution. 3 Tensor-Normal Training In this section, we propose Tensor-Normal Training (TNT), a brand new variant of the natural gradient (NG) method that makes use of the tensor-normal distribution. 33.1 Block Diagonal Approximation In this paper, we consider the case where the parameters of the model θconsists of multiple tensor variables W1,...,W L, i.e. θ “ pvecpW1qJ,..., vecpWLqJqJ. This setting is applicable to most common models in deep learning such as multi-layer perceptrons, convolutional neural networks, recurrent neural networks, etc. In these models, the trainable parameter Wl (l “1,...,L ) come from the weights or biases of a layer, whether it be a feed-forward, convolutional, recurrent, or batch normalization layer, etc. Note that the index lof Wl refers to the index of a tensor variable, as opposed to a layer. To obtain a practical NG method, we assume, as in KFAC and Shampoo, that the pre-conditioning Fisher matrix is block diagonal. To be more speciﬁc, we assume that each block corresponds to the covariance of a tensor variable in the model. Hence, the approximate Fisher matrix is: F «diagL l“1 ␣ Ex„Qx,y„p “ vecpDWlqpvecpDWlqqJ‰( “diagL l“1 tVarpvecpDWlqqu. The remaining question is how should one approximate VarpvecpDWlqqfor l“1,...,L . 3.2 Computing the Approximate Natural Gradient Direction by TNT We consider a tensor variableW PRd1ˆ¨¨¨ˆdk in the model and assume that G:“DW PRd1ˆ¨¨¨ˆdk follows a TN distribution with zero mean and covariance parameters U1,...,U k where Ui PRdiˆdi. Thus, the Fisher matrix corresponding to W is FW “Ex„Qx,y„prVarpvecpGqqs“ U1 b¨¨¨b Uk. Loosely speaking, the idea of relating the Fisher matrix to the covariance matrix of some normal distribution has some connections to Bayesian learning methods and interpretations of NG methods (see e.g., [22]). Let ∇WL PRd1ˆ¨¨¨ˆdk denote the gradient of L w.r.t. W. The approximate NG updating direction for W is computed as F´1 W vecp∇WLq“p U´1 1 b¨¨¨b U´1 k qvecp∇WLq“ vec ` ∇WL ˆ1 U´1 1 ˆ2 ¨¨¨ˆ k U´1 k ˘ , (3) where ˆi (i“1,...,k ) denotes a mode-iproduct (see Section A in the Appendix). Note that the last equality of (3) makes use of the following proposition, which also appears in [18] (see Sec A in the Appendix for a proof): Proposition 1. Let GPRd1ˆ¨¨¨ˆdk and Ui PRdiˆdi for i“1,...,k . Then, we have ` bk i“1Ui ˘ vecpGq“ vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq. (4) To summarize, the generic Tensor-Normal Training algorithm is: Algorithm 1 Generic Tensor-Normal Training (TNT) Require: Given batch size m, and learning rate α 1: for t“1,2,... do 2: Sample mini-batch Mt of size m 3: Perform a forward-backward pass over Mt to compute the mini-batch gradient 4: Perform another backward pass over Mt with ysampled from the predictive distribution to compute Gl “DWl (l“1,...,L ) averaged across Mt 5: for l“1,...L do 6: Estimate ErGpiq l s(i“1,...,k l) from Gl 7: Determine Uplq 1 ,...,U plq kl from ErGp1q l s,..., ErGpklq l s 8: Compute the inverses of Uplq 1 ,...,U plq kl 9: Compute the updating direction pl by (3) 10: Wl “Wl ´α¨pl. 11: end for 12: end for 3.3 Identifying the Covariance Parameters of the Tensor Normal Distribution By (2), Ui can be inferred from ErGpiqsup to a constant multiplier. However, different sets of multipliers can generate the same F, i.e. the same distribution. This is less of a problem if one 4only cares about F. However, we need F´1 to compute the approximate natural gradient. That is, we ﬁrst must choose a representation of F “cp˜U1 b¨¨¨b ˜Ukq(see below), and then compute F´1 “c´1pp˜U1 `ϵIq´1 b¨¨¨bp ˜Uk `ϵIq´1qwith a proper choice of ϵ ą0, where ϵI plays a damping role in the preconditioning matrix. In this case, different representations of F will lead to different F´1. The statistics community has proposed various representations for ˜Ui’s. For example, [43] imposed that c “ 1 and the ﬁrst element of ˜Ui to be one for i “ 1,...,k ´1, whereas [ 10] imposed that trp˜Uiq“ 1 for i“1,...,k . Although these representations have nice statistical properties, they are not ideal from the perspective of inverting the covariance for use in a NG method in optimization. We now describe one way to determine ˜U1,..., ˜Uk, and cfrom ErGp1qs,..., ErGpkqs. In particular, we ﬁrst set c“1, so that F´1 has a constant upper bound ϵ´kI. We then require that trp˜Uiq di is constant w.r.t i. In other words, the average of the eigenvalues of each of the ˜Ui’s is the same. This helps the ˜Ui’s have similar overall \"magnitude\" so that a suitableϵcan be found that works for all dimensions. Moreover, this shares some similarity with how KFAC splits the overall damping term between KFAC matrices, although KFAC adjusts the damping values, whereas TNT adjusts the matrices. A bit of algebra gives the formula ˜Ui “ ErGpiqs ck´1 0 ś j‰idj , (5) where c0 “ ´ trpErGpiqsqś jdj ¯1{k . 3.4 Comparison with Shampoo and KFAC Shampoo, proposed in [18], and later modiﬁed and extended in [3], is closely related to TNT. Both methods use a block-diagonal Kronecker-factored preconditioner based on second-order statistics of the gradient and are able to handle all sorts of tensors, and hence, can be applied to all sorts of deep neural network models, easily and seamlessly. The major differences between them are: (i) The TN distribution cannot be directly applied to EF, which is used in Shampoo, because the empirical gradient does not have a zero mean; hence its covariance and second moment are different. It is also believed that EF does not capture as much valuable curvature information as Fisher [26]. (ii) Using the statistics ErGpiqs’s, TNT approximates the Fisher matrix as the covariance of the block-wise sampling-based gradients assuming that they are TN distributed. On the other hand, Shampoo computes 1{2k-th power of the statistics of each direction of the tensor-structured empirical gradient and forms a preconditioning matrix from the Kronecker product of them. It is unclear to us how to interpret statistically such a matrix other than by its connection to EF. We further note that Shampoo was developed as a Kronecker-factored approximation to the full-matrix version of AdaGrad [11], whereas TNT was developed as a NG method using a TN-distributed approximation to the Fisher matrix. (iii) TNT computes the updating direction using the inverse (i.e. power of ´1) of the Kronecker factors of the approximate Fisher matrix, whereas Shampoo uses the ´1{2k-th power1 of the Kronecker factors of the EF matrix. Another method related to TNT is KFAC [34, 17], which, like TNT, uses Fisher as its preconditioning matrix. Their major differences are: (i) KFAC develops its approximation based on the structure of the gradient and Fisher matrix for each type of layer. Admittedly, this could lead to better approximations. But it is relatively hard to implement (e.g. one need to store some intermediate variables to construct the KFAC matrices). Also, if new types of layers with different structures are considered, one needs to develop suitable Kronecker factorizations, i.e., KFAC matrices. On the contrary, TNT, like Shampoo, is a model- agnostic method, in the sense that, TNT can be directly applied as long as the shape of the tensor variables are speciﬁed. 1In [3], for autoencoder problems involving tensors of order 2, the power was set to ´α 2 , where αPr0,1s was treated as a hyper-parameter which required tuning, and was set to α“1 after tuning. 50 500 1000 iteration 0.4 0.5 0.6 0.7 0.8 0.9 1.0cosine similarity l = 1 0 500 1000 iteration 0.4 0.5 0.6 0.7 0.8 0.9 l = 2 0 500 1000 iteration 0.5 0.6 0.7 0.8 0.9 l = 3 0 500 1000 iteration 0.5 0.6 0.7 0.8 0.9 l = 4 TNT KFAC Shampoo Adam SGD-m Figure 1: Cosine similarity between the directions produced by the methods shown in the legend and that of a block Fisher method. The algorithms were run on a 16 ˆ16 down-scaled MNIST [27] dataset and a small feed-forward NN with layer widths 256-20-20-20-20-20-10 described in [34]. As in [34], we only show the middle four layers. (ii) Each block of TNT corresponds to a tensor variable whose shape needs to be speciﬁed, whereas each block of KFAC corresponds to all variables in a layer. For example, for a linear or convo- lutional layer, the KFAC block would correspond to the Fisher of both its weights and bias (and their correlation), whereas TNT would produce two blocks corresponding to the weights and bias, respectively. In order to gain more insight into how well TNT approximates the Fisher matrix compared with other methods, we computed the cosine similarity between the direction produced by each method and that by a block Fisher method, where each block corresponded to one layer’s full Fisher matrix in the model (see Figure 1). For all methods shown in Figure 1, we always followed the trajectory produced by the block Fisher method. In our implementation of the block Fisher method, both the gradient and the block-Fisher matrices were estimated with a moving-average scheme, with the decay factors being 0.9. In all of the other methods compared to the block Fisher method, moving averages were also used, with the decay factors being 0.9, as described in Section D in the Appendix, to compute the relevant gradients and approximate block-Fisher matrices used by them, based on values computed at points generated by the block-Fisher method. As shown in Figure 1, the cosine similarity for TNT is always around 0.7 to 0.8, which is similar to (and sometimes higher) than the structure-aware method KFAC, and always better than Shampoo. To provide more information, we also include SGD with momentum and Adam, whose similarity to the block Fisher direction is usually lower that of the second-order methods. 4 Convergence In this section, we present results on the convergence of an idealized version of TNT that uses the actual covariance of Dθ, rather than a statistical estimate of it (see Algorithm 2 in the Appendix). In particular, our results show that Algorithm 2, with constant batch size and decreasing step size, converges to a stationary point under some mild assumptions. For simplicity, we assume that the model only contains one tensor variableW. However, our results can be easily extended to the case of multiple tensor variables. To start with, our proofs, which are delayed to Section B in the Appendix, require the following assumptions: Assumption 1. L : Rn ÑR is continuously differentiable. Lpθqis lower bounded by a real number Llow for any θPRn. ∇L is globally Lipschitz continuous with Lipschitz constant L; namely for any θ,θ1 PRn, }∇Lpθq´ ∇Lpθ1q}ď L}θ´θ1}. Assumption 2. For any iteration t, we have aq Eξt r∇lpθt,ξtqs“ ∇Lpθtq bq Eξt ” }∇lpθt,ξtq´ ∇Lpθtq}2 ı ďσ2 where σ ą 0 is the noise level of the gradient estimation, and ξt,t “ 1,2,..., are independent samples, and for a given tthe random variable ξt is independent of tθjut j“1 Assumption 3. Let G :“ Dθ. For any θ P Rn, the norm of the Fisher matrix F “ Ex„Qx,y„prvecpGqvecpGqJsis bounded above. Since F represents the curvature of the KL divergence of the model’s predictive distribution, As- sumption 3 controls the change of predictive distribution when the model’s parameters change; hence, 6Table 1: Memory and per-iteration time complexity beyond that required by SGD Name Memory Time (per-iteration) TNT Opřk i“1 d2 iq Opp1 T1 m`řk i“1 diqśk i“1 di ` 1 T2 řk i“1 d3 iq Shampoo Opřk i“1 d2 iq Oppřk i“1 diqśk i“1 di `p 1 T2 řk i“1 d3 i- if using SVDqq Adam-like Opśk i“1 diq Opśk i“1 diq Newton-like Opśk i“1 d2 iq - depends on speciﬁc algorithm it is a mild assumption for reasonable deep learning models. Essentially, we would like to prove that, if the Fisher matrix is upper bounded, our approximated Fisher (by TNT) is also upper bounded. We now present two lemmas and our main theorem; see Section B in the Appendix for proofs. Lemma 1. }Ex„Qx,y„prGpiqs}ď ´ 1 di śk i1“1 di1 ¯ }Ex„Qx,y„prvecpGqvecpGqJs}, @i“1,...,k. Lemma 2. Suppose Assumption 3 holds. Let FTNT :“p ˜U1 `ϵIqb¨¨¨bp ˜Uk `ϵIqwhere ˜Ui’s are deﬁned in (5). Then, the norm of FTNT is bounded both above and below. Theorem 1. Suppose that Assumptions 1, 2, and 3 hold for tθtugenerated by Algorithm 2 with batch size mt “mfor all t. If we choose αt “ κ L¯κ2 t´β, with β Pp0.5,1q, then 1 N Nÿ t“1 Etξju8 j“1 ” }∇Lpθtq}2 ı ď 2L ` ML ´Llow˘ ¯κ2 κ2 Nβ´1 ` σ2 p1 ´βqm ` N´β ´N´1˘ , where N denotes the iteration number and the constant ML ą0 depends only on L. Moreover, for a given δ P p0,1q,to guarantee that 1 N řN t“1 Etξju8 j“1 ” }∇Lpθtq}2 ı ă δ, N needs to be at most O ´ δ´ 1 1´β ¯ . 5 Implementation Details of TNT and Comparison on Complexity Implementation Details of TNT. In practice, we compute G “DW averaged over a minibatch of data at every iteration, and use the value of G piq to update a moving average estimate yGpiq of ErGpiqs. The extra work for these computations (as well as for updating the inverses of ˜Ui) compared with a stochastic gradient descent method is amortized by only performing them every T1 (and T2) iterations, which is also the approach used in KFAC and Shampoo, and does not seems to degrade the overall performance of the TNT algorithm. Moreover, we compute ErGpiqsusing the whole dataset at the initialization point as a warm start, which is also done in our implementations of Shampoo and KFAC. See Algorithm 3 in the Appendix for the detailed implementation of TNT. A Comparison on Memory and Per-iteration Time Complexity.To compare the memory require- ments and per-iteration time complexities of different methods, we consider the case where we optimize one tensor variable of size d1 ˆ¨¨¨ˆ dk using minibatches of size mat every iteration. A plain SGD method requires Opśk i“1 diqto store the model parameters and the gradient, whereas its per-iteration time complexity is Opmśk i“1 diq. Table 1 lists the memory requirements and per-iteration time complexities in excess of that required by SGD for different methods. Compared with a classic Newton-like method (e.g. BFGS), TNT (as well as Shampoo) reduces the memory requirement from Opśk i“1 d2 iqto Opřk i“1 d2 iq, which is comparable to that of Adam-like adaptive gradient methods. In fact, if the di’s are all equal to dand 3 ďk ăăd, the Kronecker- factored TNT pre-conditioning matrix requires kd2 storage, which is less than that required by the diagonal pre-conditioners used by Adam-like methods. On the other hand, in terms of per-iteration time complexity, TNT (as well as Shampoo) only introduces a mild overhead for estimating the statistics ErGpiqs’s, inverting the pre-conditioning matrices, and computing the updating direction. Also, the ﬁrst two of these operations can be amortized by only performing them every T1 and T2 iterations. Lastly, the extra work of Op1 T1 mśk i“1 diqrequired by TNT relative to Shampoo is due to the extra backward pass needed to estimate the true Fisher, as opposed to the EF. 7Moreover, although TNT and Shampoo both incur 1 T2 řk i“1 d3 i amortized time to invert the pre- conditioning matrices, the SVD operation in Shampoo can take much more time than the matrix inverse operation in TNT, especially when the matrix size is large2. The per-iteration computational complexity of KFAC is more complicated because it depends on the type of the layer/variable. For linear layers, TNT and KFAC both uses two matrices, whose sizes are the number of input nodes and output nodes, respectively. For convolutional layers, TNT uses three matrices, whose sizes are the size of ﬁlter, number of input channels, and number of output channels, whereas KFAC uses two matrices whose sizes are the size of ﬁlter times number of input channels, and number of output channels. As a result, the ﬁrst KFAC matrix requires much more memory. In general, the per-iteration complexity of KFAC is no less than that of TNT. 6 Experiments In this section, we compare TNT with some state-of-the-art second-order (KFAC, Shampoo) and ﬁrst-order (SGD with momentum, Adam) methods (see Section D.1 in the Appendix on how these methods were implemented). The Hessian-based K-BFGS method [15, 38] is another state-of-the-art Kronecker-factored second-order method for training deep learning models. Since our focus is on optimizers that use Fisher or empirical Fisher as the preconditioning matrix, we did not include K-BFGS in our comparison. Our experiments were run on a machine with one V100 GPU and eight Xeon Gold 6248 CPUs using PyTorch [37]. Each algorithm was run using the best hyper-parameters, determined by an appropriate grid search (speciﬁed below), and 5 different random seeds. In Figures 2 and 3 the performance of each algorithm is plotted: the solid curves give results obtained by averaging the 5 different runs, and the shaded area depicts the ˘standard deviation range for these runs. Our code is available at https://github.com/renyiryry/tnt_neurips_2021. 6.1 Optimization: Autoencoder Problems 0 100 200 300 400 500 epoch 102 train loss 0 100 200 300 400 500 process time (second) 102 train loss TNT KFAC Shampoo Adam SGD-m a) MNIST autoencoder 0 250 500 750 1000 1250 epoch 101 102 train loss 0 500 1000 1500 2000 process time (second) 101 102 train loss TNT KFAC Shampoo Adam SGD-m b) FACES autoencoder Figure 2: Optimization performance of TNT, KFAC, Shampoo, Adam, and SGD-m on two autoen- coder problems We ﬁrst compared the optimization performance of each algorithm on two autoencoder problems [21] with datasets MNIST [27] and FACES3, which were also used in [32, 34, 5, 15] as benchmarks to compare different algorithms. For each algorithm, we conducted a grid search on the learning rate and damping value based on the criteria of minimal training loss. We set the Fisher matrix update frequency T1 “1 and inverse update frequency T2 “20 for all of the second-order methods. Details of our experiment settings are listed in Section D.2 in the Appendix. From Figure 2, it is clear that TNT outperformed SGD with momentum and Adam, both in terms of per-epoch progress and process time. Moreover, TNT performed (at least) as well as KFAC and Shampoo, with a particularly strong performance on the FACES dataset. We repeated these experiments using a grid search on more hyper-parameters, and obtained results (see Figure 6 in Sec D.5) that further support our observations based on Figure 2. 2In [ 3] it is shown that replacing the SVD operation by a coupled Schur-Newton method saves time for matrices of size greater than 1000 ˆ1000. In our experiments, we used the coupled Newton method implementation of Shampoo. 3https://cs.nyu.edu/~roweis/data.html 86.2 Generalization: Convolutional Neural Networks 0 50 100 150 200 epoch 10 2 10 1 100 train loss 0 2000 4000 process time (second) 10 2 10 1 100 train loss 0 50 100 150 200 epoch 10 1 100 val error 0 2000 4000 process time (second) 10 1 100 val error TNT KFAC Shampoo Adam SGD-m a) CIFAR-10, ResNet-32 0 50 100 150 200 epoch 10 2 10 1 100 train loss 0 5000 10000 process time (second) 10 2 10 1 100 train loss 0 50 100 150 200 epoch 100 3 × 10 1 4 × 10 1 6 × 10 1 val error 0 5000 10000 process time (second) 100 3 × 10 1 4 × 10 1 6 × 10 1 val error TNT KFAC Shampoo Adam SGD-m b) CIFAR-100, VGG16 Figure 3: Generalization ability of TNT, KFAC, Shampoo, Adam, and SGD-m on two CNN models. Upper row depicts the training loss whereas lower row depicts the validation classiﬁcation error. We then compared the generalization ability of each algorithm on two CNN models, namely, ResNet32 [19] (with CIFAR10 dataset [24]) and VGG16 [42] (with CIFAR100 dataset [24]). The ﬁrst-order methods were run for 200 epochs during which the learning rate was decayed by a factor of 0.1 every 60 epochs, whereas the second-order methods were run for 100 epochs during which the learning rate was decayed by a factor of 0.1 every 40 epochs; (these settings are the same as in [45]). Moreover, as indicated in [29, 45], weight decay, different from the L2 regularization added to the loss function, helps improve generalization across different optimizers. Thus, for each algorithm, we conducted a grid search on the initial learning rate and the weight decay factor based on the criteria of maximal validation classiﬁcation accuracy. The damping parameter was set to 1e-8 for Adam (following common practice), and 0.03 for KFAC4. For TNT and Shampoo, we set ϵ“0.01. We set T1 “10 and T2 “ 100 for the three second-order methods (same as in [ 45]). Details of our experiment settings and a further discussion of the choice of damping hyper-parameters can be found in Section D.3 in the Appendix. The results in Figure 3 indicate that, with a proper learning rate and weight decay factor, second-order methods and Adam exhibit roughly the same generalization performance as SGD with momentum, which corroborate the results in [29, 45]. In particular, TNT has a similar (and sometimes better) generalization performance than the other methods. For example, comparing TNT with SGD-m, TNT (SGD-m) achieves 93.08% (93.06%) validation accuracy with ResNet32 on CIFAR10 and 73.33% (73.43%) validation accuracy with VGG16 on CIFAR-100, after 100 (200) epochs (see Table 3 in the Appendix for the accuracy achieved by the other algorithms). Moreover, in terms of process time, TNT is roughly twice (equally) as fast as SGD with momentum on ResNet32/CIFAR10 in Figure 3a (on VGG16 on CIFAR-100 in Figure 3b). This illustrates the fact that TNT usually requires only moderately more computational effort per-iteration but fewer iterations to converge than ﬁrst-order methods. Also, as shown on the VGG16 model, KFAC seems to be much slower than TNT and Shampoo on larger models. This is because the most recent version of KFAC, which we implemented, uses SVD (i.e., eigenvalue decomposition) to compute inverse matrices (see Section D.1.2 in the Appendix for a discussion of this). In contrast, TNT does not need to use SVD, and the most recent version of Shampoo replaces SVD with a coupled Newton method in [3]. We also compared TNT with a variant of it that uses the empirical rather than the true Fisher as the preconditioning matrix. The results of this comparison, which are presented in Section D.4 in the Appendix, suggest that it is preferable to use Fisher rather than empirical Fisher as pre-conditioning matrices in TNT. 4The value of 0.03 is suggested in https://github.com/alecwangcq/KFAC-Pytorch, a github repo by the authors of [45]. 97 Conclusion and Further Discussions In this paper, we proposed a new second-order method, and in particular, an approximate natural gradient method TNT, for training deep learning models. By approximating the Fisher matrix using the structure imposed by the tensor normal distribution, TNT only requires mild memory and computational overhead compared with ﬁrst-order methods. Our experiments on various deep learning models and datasets, demonstrate that TNT provides comparable and sometimes better results than the state-of-the-art (SOTA) methods, both from the optimization and generalization perspectives. Due to space and computational resource constraints, we did not run experiments on even larger models such as ImageNet and advanced models for NLP tasks. However, the results in this paper already show very strong evidence of the potential of the TNT method. We also did not explore extending our method to a distributed setting, which has been shown to be a promising direction for second-order methods such as KFAC and Shampoo [4, 3]. Since TNT already performs very well on a single machine, we expect that it will continue to do so in a distributed setting. These issues will be addressed in future research. We did not compare TNT with the SOTA Kronecker-based quasi-Newton methods [15, 38], since they are not as closely related to TNT as are Shampoo and KFAC. Their performance relative to TNT can be inferred from the comparisons here combined with those reported in [15, 38, 3]. As a ﬁnal note 5, the preconditioning matrices of TNT (as well as those of Shampoo) are derived from the speciﬁc shape of the (tensor) parameters of the particular deep learning model that is being trained. One can, of course, reshape these parameters, e.g., by ﬂattening the tensors into vectors, which gives rise to very different preconditioning matrices. The method proposed in this paper can be applied to any deep learning or machine learning model. If the model and/or data has a ﬂawed design or contains bias, this could potentially have negative societal impacts. However, this possibility is beyond the scope of the work presented in this paper. 5We thank the program chair for pointing this out. 10Acknowledgments and Disclosure of Funding We would like to thank the anonymous reviewers for their very helpful comments and suggestions. The research efforts of D. Goldfarb and Y . Ren on this paper were supported in part by NSF Grant IIS-1838061. We acknowledge computing resources from Columbia University’s Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893- 01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. References [1] S.-I. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning for multilayer perceptrons. Neural computation, 12(6):1399–1409, 2000. [2] E. Amid, R. Anil, and M. K. Warmuth. Locoprop: Enhancing backprop via local loss optimiza- tion. arXiv preprint arXiv:2106.06199, 2021. [3] R. Anil, V . Gupta, T. Koren, K. Regan, and Y . Singer. Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018, 2021. [4] J. Ba, R. Grosse, and J. Martens. Distributed second-order optimization using Kronecker- factored approximations. 2016. [5] A. Botev, H. Ritter, and D. Barber. Practical Gauss-Newton optimisation for deep learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 557–565. JMLR. org, 2017. [6] C. G. Broyden. The convergence of a class of double-rank minimization algorithms 1. general considerations. IMA Journal of Applied Mathematics, 6(1):76–90, 1970. [7] R. H. Byrd, S. L. Hansen, J. Nocedal, and Y . Singer. A stochastic quasi-Newton method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016. [8] D. Choi, C. J. Shallue, Z. Nado, J. Lee, C. J. Maddison, and G. E. Dahl. On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446, 2019. [9] A. P. Dawid. Some matrix-variate distribution theory: notational considerations and a Bayesian application. Biometrika, 68(1):265–274, 1981. [10] B. S. Dees and D. P. Mandic. A statistically identiﬁable model for tensor-valued Gaussian random variables. arXiv preprint arXiv:1911.02915, 2019. [11] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011. [12] P. Dutilleul. The mle algorithm for the matrix normal distribution. Journal of statistical computation and simulation, 64(2):105–123, 1999. [13] R. Fletcher. A new approach to variable metric algorithms. The computer journal , 13(3): 317–322, 1970. [14] D. Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26, 1970. [15] D. Goldfarb, Y . Ren, and A. Bahamou. Practical quasi-Newton methods for training deep neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, ed- itors, Advances in Neural Information Processing Systems , volume 33, pages 2386–2396. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/192fc044e74dffea144f9ac5dc9f3395-Paper.pdf. [16] R. Gower, D. Goldfarb, and P. Richtárik. Stochastic block BFGS: Squeezing more curvature out of data. In International Conference on Machine Learning, pages 1869–1878, 2016. [17] R. Grosse and J. Martens. A Kronecker-factored approximate Fisher matrix for convolution layers. In International Conference on Machine Learning, pages 573–582, 2016. [18] V . Gupta, T. Koren, and Y . Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pages 1842–1850. PMLR, 2018. 11[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778, 2016. [20] G. Hinton, N. Srivastava, and K. Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on, 14(8), 2012. [21] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504–507, 2006. [22] M. E. Khan and H. Rue. The Bayesian learning rule. arXiv preprint arXiv:2107.04562, 2021. [23] D. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2014. [24] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009. [25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. [26] F. Kunstner, P. Hennig, and L. Balles. Limitations of the empirical Fisher approximation for natural gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/ file/46a558d97954d0692411c861cf78ef79-Paper.pdf. [27] Y . LeCun, C. Cortes, and C. Burges. MNIST handwritten digit database.ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [28] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528, 1989. [29] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Con- ference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. [30] H. Lu, K. N. Plataniotis, and A. Venetsanopoulos.Multilinear subspace learning: dimensionality reduction of multidimensional data. CRC press, 2013. [31] A. M. Manceur and P. Dutilleul. Maximum likelihood estimation for the tensor normal dis- tribution: Algorithm, minimum sample size, and empirical bias and dispersion. Journal of Computational and Applied Mathematics, 239:37–49, 2013. [32] J. Martens. Deep learning via Hessian-free optimization. In ICML, volume 27, pages 735–742, 2010. [33] J. Martens. New insights and perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014. [34] J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408–2417, 2015. [35] J. Martens, J. Ba, and M. Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=HyMTkQZAb. [36] A. Mokhtari and A. Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE Transactions on Signal Processing, 62(23):6089–6104, 2014. [37] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high- performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URLhttp://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library. pdf. [38] Y . Ren and D. Goldfarb. Kronecker-factored quasi-Newton methods for convolutional neural networks. arXiv preprint arXiv:2102.06737, 2021. [39] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951. 12[40] R. M. Schmidt, F. Schneider, and P. Hennig. Descending through a crowded valley- benchmarking deep learning optimizers. In International Conference on Machine Learning, pages 9367–9376. PMLR, 2021. [41] D. F. Shanno. Conditioning of quasi-Newton methods for function minimization. Mathematics of computation, 24(111):647–656, 1970. [42] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [43] M. Singull, M. R. Ahmad, and D. von Rosen. More on the Kronecker structured covariance matrix. Communications in Statistics-Theory and Methods, 41(13-14):2512–2523, 2012. [44] X. Wang, S. Ma, D. Goldfarb, and W. Liu. Stochastic quasi-Newton methods for nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927–956, 2017. [45] G. Zhang, C. Wang, B. Xu, and R. Grosse. Three mechanisms of weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=B1lz-3Rct7. 13A Some Tensor Deﬁnitions and Properties We present in this section fairly standard notation and deﬁnitions regarding tensors, e.g., see [18] and Chapter 3 of [30], that we use throughout the paper. Let APRd1ˆ¨¨¨ˆdk denote a tensor of order k. • slices of Aalong its i-th dimension: for i “1,...,k and j “1,...,d i, the j-th slice of A along its i-th dimension, Ai j denotes the d1 ˆ¨¨¨ˆ di´1 ˆdi`1 ˆ¨¨¨ˆ dk tensor of order k´1, composed from all of the entries of Awhose ith index is j. • vectorization of A: denoted as vecpAq, is deﬁned recursively as vecpAq“ ¨ ˚˝ vecpA1 1q ... vecpA1 d1 q ˛ ‹‚, where for the base case, in which Ais one-dimensional tensor (i.e., a vector), vecpAq“ A. Note that when Ais a matrix, this corresponds to the row-major vectorization of A. • matricization of A: denoted as matipAq, for i“1,...,k , is deﬁned as matipAq“ ¨ ˚˝ vecpAi 1qJ ... vecpAi diqJ ˛ ‹‚. Note that vecpAq“ vecpmat1pAqq. • contraction of Awith itself along all but the ith dimension: denoted as Apiq, is deﬁned as matipAqmatipAqJ. • mode-iproduct of Aby a matrix U PRd1 iˆdi: the operation is denoted as ˆi. Let B “ AˆiU PRd1ˆ¨¨¨ˆdi´1ˆd1 iˆdi`1ˆ¨¨¨ˆdk denote the resulting tensor. Bj1,...,ji´1,j1 i,ji`1,...,jk “ř ji Aj1,...,jkUj1 i,ji. Note that in the matrix case (k“2), Aˆ1 U “UA, Aˆ2 U “AUJ. Lemma 3. Let X PRmˆn, APRmˆm, B PRnˆn. Then, we have pAbBJqvecpXq“ vecpAXBq. Note that the above lemma is slightly different from the most common version of it, which uses a column-major vectorization of the matrix X. Proposition 1. Let GPRd1ˆ¨¨¨ˆdk and Ui PRdiˆdi for i“1,...,k . Then, we have ` bk i“1Ui ˘ vecpGq“ vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq. (6) Proof of Proposition 1: Proof. Our proof, which is largely inspired by the one in [18], is by induction on k. When k“1, it is easy to see that (6) holds by the deﬁnition of the mode-iproduct. When k“2, (6) follows from Lemma 3. Now assume that (6) holds for 1,2,...,k ´1. For k, we let H “bk i“2Ui By the induction hypothesis, mat1pGqHJ “ ` Hmat1pGqJ˘J “ ` H `vecpG1 1q ¨¨¨ vecpG1 d1 q˘˘J (7) “ `HvecpG1 1q ¨¨¨ HvecpG1 d1 q˘J (8) “ `vecpG1 1 ˆ1 U2 ¨¨¨ˆ k´1 Ukq ¨¨¨ vecpG1 d1 ˆ1 U2 ¨¨¨ˆ k´1 Ukq˘J (9) “ ¨ ˚˝ vecpG1 1 ˆ1 U2 ¨¨¨ˆ k´1 UkqJ ... vecpG1 d1 ˆ1 U2 ¨¨¨ˆ k´1 UkqJ ˛ ‹‚“mat1pGˆ2 U2 ¨¨¨ˆ k Ukq (10) 14By Lemma 3 and (10), ` bk i“1Ui ˘ vecpGq“p U1 bHqvecpmat1pGqq“ vecpU1mat1pGqHJq “vecpU1mat1pGˆ2 U2 ¨¨¨ˆ k Ukqq “vecpmat1pGˆ2 U2 ¨¨¨ˆ k Uk ˆ1 U1qq “vecpGˆ2 U2 ¨¨¨ˆ k Uk ˆ1 U1q “vecpGˆ1 U1 ˆ2 U2 ¨¨¨ˆ k Ukq, where the third from last equality comes from the fact that BmatipAq“ matipAˆi Bq, and the last equality comes from the fact that mode-iproducts are commutative. B Proofs of Lemmas and Theorem 1 Algorithm 2 Idealized Version of TNT Require: Given θ1 PRn, batch sizes tmtutě1, step sizes tαtutě1, and damping value ϵą0 1: for t“1,2,... do 2: Sample mini-batch of size mt: Mt “tξt,i,i “1,...,m tu 3: Calculate y∇Lt “ 1 mt ř ξt,iPMt ∇lpθt,ξt,iq 4: Compute ˜Ui (i “1,...,k ) by formula (5), using the true values of Ex„Qx,y„prGpiqs(i “ 1,...,k ) at the current parameter θt. 5: Compute pt “vec ´ y∇Lt ˆ1 p˜U1 `ϵIq´1 ˆ2 ¨¨¨ˆ k p˜Uk `ϵIq´1 ¯ 6: Calculate θt`1 “θt ´αtpt 7: end for Algorithm 2 describes an idealized version of TNT, whose convergence is veriﬁed by the proofs of Lemmas 1 and 2, and Theorem 1 below. Lemma 1. }Ex„Qx,y„prGpiqs}ď ´ 1 di śk i1“1 di1 ¯ }Ex„Qx,y„prvecpGqvecpGqJs}, @i“1,...,k. Proof of Lemma 1: Proof. Let X P Rmˆn be a random matrix, and xi P Rm denote its ith column ( i “ 1,...,n ). Because vecpXqis a vector containing all the elements of all the xi’s,xixJ i is a square submatrix of vecpXqvecpXqJ. Hence, }ErxixJ i s}ď} ErvecpXqvecpXqJs}, and we have that }ErXXJs}“} Er nÿ i“1 xixJ i s}“} nÿ i“1 ErxixJ i s}ď nÿ i“1 }ErxixJ i s}ď n}ErvecpXqvecpXqJs}. Letting X “matipGqP Rdiˆpd1¨¨¨di´1di`1¨¨¨dkq, it then follows that }Ex„Qx,y„prGpiqs}ďp d1 ¨¨¨ di´1di`1 ¨¨¨ dkq}ErvecpmatipGqqvecpmatipGqqJs} “p 1 di kź i1“1 d1 iq}ErvecpGqvecpGqJs}. Lemma 2. Suppose Assumption 3 holds. Let FTNT :“p ˜U1 `ϵIqb¨¨¨bp ˜Uk `ϵIq, where the ˜Ui’s are deﬁned in (5). Then, the norm of FTNT is bounded both above and below. Proof of Lemma 2: 15Proof. It is clear that ||FTNT||“ śk i“1 ||˜Ui `ϵI||ě ϵk. On the other hand, for i“1,...,k , if we denote the eigenvalues of ErGpiqsby λ1 ď¨¨¨ď λdi, we have from (5) that }˜Ui}“ }ErGpiqs} ´ trpErGpiqsqś jdj ¯pk´1q{kś j‰idj “ λdi ´λ1`¨¨¨`λdiś jdj ¯pk´1q{kś j‰idj ď λdi ´ λdiś jdj ¯pk´1q{kś j‰idj “ diλ1{k di pś jdjq1{k “ di}ErGpiqs}1{k pś jdjq1{k . Thus, since ||FTNT||“ śk i“1 ||˜Ui `ϵI||“ śk i“1p||˜Ui||` ϵq, by the above and Lemma 1, ||FTNT||ď kź i“1 ˜ di}ErGpiqs}1{k pś jdjq1{k `ϵ ¸ ď kź i“1 ´ d1´1{k i ||ErvecpGqvecpGqJs||1{k `ϵ ¯ . Then, by Assumption 3, we have that ||FTNT||is bounded above. Proof of Theorem 1: Proof. The proof of Theorem 1 follows from Theorem 2.8 in [44]. Clearly, Algorithm 2 falls under the scope of the stochastic quasi-Newton (SQN) method in [44]. In particular, by Proposition 1, the pre-conditioning matrix H “F´1 TNT. Moreover, to apply Theorem 2.8 in [44], we need to show that AS.1 - AS.4 in [44] hold. First, AS.1 and AS.2 in [44] are the same as Assumption 1 and Assumption 2, respectively in Section 4 in our paper. Second, by Lemma 2, since ||FTNT||is both upper and lower bounded, so is ||F´1 TNT||. Hence, AS.3 in [44] is ensured. Finally, Algorithm 2 itself ensures AS.4 in [44] holds. Hence, by Theorem 2.8 of [44], the result is guaranteed. C Pseudo-code for TNT In Algorithm 3, we present a detailed pseudo-code for our actual implementation of TNT. The highlighted parts, i.e., Lines 7, 15 and 16, indicate where TNT differs signiﬁcantly from Shampoo. D Details of the Experiments In our implementations of the algorithms that we compared to TNT, we included in all of the techniques like weight decay and momentum, so that our numerical experiments would provide a FAIR comparison. Consequently, we did not include some special techniques that have been incorporated in some of the algorithms as described in previously published papers, since to keep the comparisons fair, we would have had to incorporate such techniques in all of the algorithms (see Section D.1.1 for more details). D.1 Competing Algorithms In SGD with momentum, we updated the momentum of the gradientm“µ¨m`gat every iteration, where gdenotes the minibatch gradient at current iteration. The gradient momentum is also used in the second-order methods, in our implementations. For Adam, we follow exactly the algorithm in [23] with β1 “0.9 and β2 “0.999. In particular, we follow the approach in [23] in estimating the momentum of gradient by m“β1 ¨m`p1 ´β1q¨ g. The role of β1 and β2 is similar to that of µand βin Algorithm 3 and Algorithm 4, as we will describe below. In the experiments on CNNs, we use weight decay (same as in Algorithms 3 and 4) on SGD and Adam, similar to SGDW and AdamW in [29] (for further details, see Section D.3). 16Algorithm 3 Tensor-Normal Training Require: Given batch size m, learning rate tαtutě1, weight decay factor γ, damping value ϵ, statistics update frequency T1, inverse update frequency T2 1: µ“0.9, β “0.9 2: Initialize yGpiq l “ ErGpiq l s(l “ 1,..,k , i “ 1,...,k l) by iterating through the whole dataset, {∇WlL “0 (l“1,...,L ) 3: for t“1,2,... do 4: Sample mini-batch Mt of size m 5: Perform a forward-backward pass over Mt to compute the mini-batch gradient ∇L 6: if t”0 pmod T1qthen 7: Perform another backward pass over Mt with ysampled from the predictive distribution to compute Gl “DWl averaged across Mt (l“1,...,L ) 8: end if 9: for l“1,...L do 10: {∇WlL “µ{∇WlL `∇WlL 11: if t”0 pmod T1qthen 12: Update yGpiq l “βyGpiq l `p1 ´βqGl piq for i“1,...,k l 13: end if 14: if t”0 pmod T2qthen 15: Determine ˜Uplq 1 ,..., ˜Uplq kl from yGp1q l ,..., zGpklq l by (5) 16: Recompute p˜Uplq 1 `ϵIq´1,..., p˜Uplq kl `ϵIq´1 17: end if 18: pl “ {∇WlL ˆ1 p˜Uplq 1 `ϵIq´1 ˆ2 ¨¨¨ˆ k p˜Uplq k `ϵIq´1 19: pl “pl `γWl 20: Wl “Wl ´αt ¨pl. 21: end for 22: end for 17D.1.1 Shampoo Algorithm 4 Shampoo Require: Given batch size m, learning rate tαtutě1, weight decay factor γ, damping value ϵ, statistics update frequency T1, inverse update frequency T2 1: µ“0.9, β “0.9 2: Initialize yGpiq l “ ErGpiq l s(l “ 1,..,k , i “ 1,...,k l) by iterating through the whole dataset, {∇WlL “0 (l“1,...,L ) 3: for t“1,2,... do 4: Sample mini-batch Mt of size m 5: Perform a forward-backward pass over the current mini-batch Mt to compute the minibatch gradient ∇L 6: for l“1,...L do 7: {∇WlL “µ{∇WlL `∇WlL 8: if t”0 pmod T1qthen 9: Update yGpiq l “βyGpiq l `p1 ´βqGl piq for i“1,...,k l where Gl “∇WlL 10: end if 11: if t”0 pmod T2qthen 12: Recompute ˆyGp1q l `ϵI ˙´1{2kl ,..., ˆzGpklq l `ϵI ˙´1{2kl with the coupled Newton method 13: end if 14: pl “ {∇WlL ˆ1 ˆyGp1q l `ϵI ˙´1{2kl ˆ2 ¨¨¨ˆ k ˆzGpklq l `ϵI ˙´1{2kl 15: pl “pl `γWl 16: Wl “Wl ´αt ¨pl 17: end for 18: end for In Algorithm 4, we present our implementation of Shampoo, which mostly follows the description of it given in [18]. Several major improvements are also included, following the suggestions in [3], including: 1. In Line 9 of Algorithm 4, a moving average is used to update the estimates yGpiq l , as is done in our implementations of TNT and KFAC. This approach is also used in Adam, whereas summing the Gpiq l ’s over all iterations, as in [18], is analogous to what is done in AdaGrad, upon which Shampoo is based. 2. In Line 12 of Algorithm 4, we use a coupled Newton method to compute inverse roots of the matrices (as proposed in [3]), rather than using SVD. The coupled Newton approach has been shown to be much faster than SVD, and also preserves relatively good accuracy in terms of computing inverse roots. The coupled Newton method performs reasonably well (without tuning) using a max iteration number of 100 and an error tolerance of 1e-6. Some other modiﬁcations proposed in [3] are not included in our implementation of Shampoo, mainly because these modiﬁcations can also be applied to TNT, and including them only in Shampoo would introduce other confounding factors. (i) We did not explore multiplying the damping term in the pre-conditioner by the maximum eigenvalue λmax of the contraction matrix. Moreover, this modiﬁcation is somewhat problematic, since, if the model contains any variables that always have a zero gradient (e.g. the bias in a convolutional layer that is followed by a BN layer), the optimizer would become unstable because the pre-conditioner of the zero-gradient variables would be the zero matrix, (note that in this case λmax “0). (ii) We did not explore the diagonal variant of Shampoo, as we mainly focused on the comparison between different pre-conditioning matrices, and TNT can also be extended to a diagonal 18version; similarly, we did not explore the variant proposed in [3] that divides large tensors into small blocks. D.1.2 KFAC In this subsection, we brieﬂy describe our implementation of KFAC. The preconditioning matrices that we used for linear layers and convolutional layers are precisely as those described in [34] and [17], respectively. For the parameters in the BN layers, we used the gradient direction, exactly as in https://github.com/alecwangcq/KFAC-Pytorch. As in our implementations of TNT and Shampoo, and as suggested in [17], we did a warm start to estimate the pre-conditioning KFAC matrices in an initialization step that iterated through the whole data set, and adopted a moving average scheme to update them with β “0.9 afterwards. In inverting the KFAC matrices and computing the updating direction, • for the autoencoder experiments, we inverted the damped KFAC matrices and used them to compute the updating direction, where the damping factors for both Aand Gwere set to be? λ, where λis the overall damping value;6 • for the CNN experiments, we followed the SVD (i.e. eigenvalue decomposition) implemen- tation suggested in https://github.com/alecwangcq/KFAC-Pytorch, which, as we veriﬁed, performs better than splitting the damping value and inverting the damped KFAC matrices (as suggested in [34, 17]). Further, we implemented weight decay exactly as in TNT (Algorithm 3) and Shampoo (Algorithm 4). D.2 Experiment Settings for the Autoencoder Problems Table 2: Hyper-parameters (learning rate, damping) used to produce Figure 2 Name MNIST FACES TNT (1e-4, 0.1) (1e-6, 0.003) KFAC (0.003, 0.3) (0.1, 10) Shampoo (3e-4, 3e-4) (3e-4, 3e-4) Adam (1e-4, 1e-4) (1e-4, 1e-4) SGD-m (0.003, -) (0.001, -) MNIST has 60,000 training data, whereas FACES7 has 103,500 training data. For all algorithms, we used a batch size of 1,000 at every iteration. The autoencoder model used for MNIST has layer widths 784-1000-500-250-30-250-500-1000-784 with ReLU activation functions, except for the middle layer which uses a linear function and the last layer which uses a sigmoid function. The autoencoder model used for FACES has layer widths 625-2000-1000-500-30-500-1000-2000-625 with ReLU activation functions, except for the middle and last layers which use linear functions. We used binary entropy loss for MNIST and squared error loss for FACES. The above settings largely mimic the settings in [32, 34, 5, 15]. Since we primarily focused on optimization rather than generalization in these tasks, we did not includeL2 regularization or weight decay. In order to obtain Figure 2, we ﬁrst conducted a grid search on the learning rate (lr) and damping value based on the criteria of minimizing the training loss. The ranges of the grid searches used for the algorithms in our tests were: • SGD-m: – lr: 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 6Note that there are more sophisticated ways of splitting the damping value, such as one that makes use of the norms of the undamped matrices, to enforce that the two matrices have the same norm. See [34] and [17] for more on this. 7Downloadable at www.cs.toronto.edu/~jmartens/newfaces_rot_single.mat. 19– damping: not applicable • Adam: – lr: 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01 – damping (i.e. the ϵhyperparameter of Adam): 1e-8, 1e-4, 1e-2 • Shampoo: – lr: 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003 – damping (i.e. ϵin Algorithm 4): 1e-4, 3e-4, 0.001, 0.003, 0.01 • TNT: – lr: 1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001 – damping (i.e. ϵin Algorithm 3): 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 • KFAC: – lr: 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 – damping: 0.01, 0.03, 0.1, 0.3, 1, 3, 10 The best hyper-parameter values determined by our grid searches are listed in Table 2. D.3 Experiment Settings for the CNN Problems Table 3: Hyper-parameters ( initial learning rate, weight decay factor) used to produce Figure 3 and the average validation accuracy across 5 runs with different random seeds shown in Figure 3 Name CIFAR-10 + ResNet32 CIFAR-100 + VGG16 TNT (1e-4, 10) Ñ93.08% (3e-5, 10) Ñ73.33% KFAC (0.01, 0.1) Ñ92.85% (3e-4, 0.1) Ñ74.33% Shampoo (0.01, 0.1) Ñ92.63% (0.003, 0.1) Ñ72.82% Adam (0.003, 0.1) Ñ92.92% (3e-5, 10) Ñ72.27% SGD-m (0.03, 0.01) Ñ93.06% (0.03, 0.01) Ñ73.44% Both CIFAR-10 and CIFAR-100 have 50,000 training data and 10,000 testing data (used as the validation set in our experiments). For all algorithms, we used a batch size of 128 at every iteration. In training, we applied data augmentation as described in [25], including random horizontal ﬂip and random crop. The ResNet32 model refers to the one in Table 6 of [19], whereas the VGG16 model refers to model D of [42], with the modiﬁcation that batch normalization layers were added after all of the convolutional layers in the model. It is worth noting that, in TNT and Shampoo, for the weight tensor in the convolutional layers, instead of viewing it as a 4-way tensor, we view it as a 3-way tensor, where the size of its 3 ways (dimensions) corresponds to the size of the ﬁlter, the number of input channel, and the number of the output channel, respectively. As a result, the preconditioning matrices of TNT and Shampoo will come from the Kronecker product of three matrices, rather than four matrices. Weight decay, which is related to, but not the same asL2 regularization added to the loss function, has been shown to help improve generalization performance across different optimizers [29, 45]. In our experiments, we adopted weight decay for all algorithms. The use of weight decay for TNT and Shampoo is described in Algorithm 3 and Algorithm 4, respectively, and is similarly applied to KFAC. Also note that weight decay is equivalent to L2 regularization for pure SGD (without momentum). However, the equivalence does not hold for SGD with momentum. For the sake of a fair comparison, we also applied weight decay for SGD-m. For TNT and Shampoo, we set ϵ “0.01. We also tried values around 0.01 and the results were not sensitive to the value of ϵ; hence, ϵcan be set to 0.01 as a default value. For KFAC, we set the overall damping value to be 0.03, as suggested in the implementation in https://github.com/ alecwangcq/KFAC-Pytorch. We also tried values around 0.03 for KFAC and conﬁrmed that 0.03 is a good default value. 20In order to obtain Figure 3, we ﬁrst conducted a grid search on the initial learning rate (lr) and weight decay (wd) factor based on the criteria of maximizing the classiﬁcation accuracy on the validation set. The range of the grid searches for the algorithms in our tests were: • SGD-m: – lr: 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 – wd: 0.001, 0.01, 0.1, 1 • Adam: – lr: 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 – wd: 1e-4, 0.001, 0.01, 0.1, 1, 10, 100 • Shampoo: – lr: 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03, 0.1 – wd: 0.01, 0.1, 1, 10 • TNT: – lr: 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001 – wd: 1, 10, 100 • KFAC: – lr: 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 0.001, 0.003, 0.01, 0.03 – wd: 0.01, 0.1, 1 The best hyper-parameter values, and the validation classiﬁcation accuracy obtained using them, are listed in Table 3. D.4 A Comparison between TNT and TNT-EF 0 100 200 300 epoch 102 training loss 0 200 400 process time (second) 102 training loss TNT TNT-EF a) MNIST autoencoder 0 200 400 epoch 101 102 training loss 0 1000 2000 process time (second) 101 102 training loss TNT TNT-EF b) FACES autoencoder Figure 4: Optimization performance comparison of the TNT and TNT-EF algorithms on two autoencoder problems. 210 50 100 epoch 10 1 100 training loss 0 1000 2000 process time (second) 10 1 100 training loss 0 50 100 epoch 10 1 100 testing error 0 1000 2000 process time (second) 10 1 100 testing error TNT TNT-EF a) CIFAR-10, ResNet-32 0 50 100 epoch 10 2 10 1 100 training loss 0 2000 4000 process time (second) 10 2 10 1 100 training loss 0 50 100 epoch 100 3 × 10 1 4 × 10 1 6 × 10 1 testing error 0 2000 4000 process time (second) 100 3 × 10 1 4 × 10 1 6 × 10 1 testing error TNT TNT-EF b) CIFAR-100, VGG16 Figure 5: Generalization ability comparison of the TNT and TNT-EF algorithms on two CNN models. The upper row depicts the training loss, whereas the lower row depicts the validation classiﬁcation error. Table 4: Hyper-parameters (learning rate, damping) used to produce Figure 4 Name MNIST FACES TNT-EF (3e-6, 0.01) (3e-6, 0.01) Table 5: Hyper-parameters ( initial learning rate, weight decay factor) used to produce Figure 5 Name CIFAR-10 + ResNet32 CIFAR-100 + VGG16 TNT-EF (1e-4, 10) Ñ93.62% (3e-6, 100) Ñ72.85% In this subsection, we compare our proposed TNT algorithm against a variant of it, TNT-EF, which uses an empirical Fisher (EF) preconditioning matrix in place of the true Fisher matrix. In other words, TNT-EF does everything speciﬁed in Algorithm 3, except that it does not perform the extra backward pass in Line 7 of Algorithm 3. When updating the matricesyGpiq l , TNT-EF uses the empirical minibatch gradient, rather than the sampling-based minibatch gradient, i.e. the one coming from the extra backward pass. We conducted a hyper-parameter grid search for TNT-EF, following the same procedure as the one that was used for TNT, whose performance was plotted in Figures 2 and 3. The best values for the TNT-EF hyper-parameters that we obtained are listed in Tables 4 and 5. We then plotted in Figures 4 and 5, the performance of TNT-EF, along with that of TNT, using for it the hyper-parameters given in Tables 2 and 3. As shown in Figures 4 and 5, TNT performed at least as well as TNT-EF, on the MNIST and CIFAR-10 problems, and performed somewhat better on the FACES and CIFAR-100 problems, which conﬁrms the widely held opinion that the Fisher matrix usually carries more valuable curvature information than the empirical Fisher metrix. 22D.5 More on Hyper-parameter Tuning 0 200 400 600 epoch 102 training loss 0 200 400 process time (second) 102 training loss TNT KFAC Shampoo Adam SGD-m a) MNIST autoencoder 0 500 1000 epoch 101 102 training loss 0 1000 2000 process time (second) 101 102 training loss TNT KFAC Shampoo Adam SGD-m b) FACES autoencoder Figure 6: Optimization performance of TNT, KFAC, Shampoo, Adam, and SGD-m on two autoen- coder problems, with more extensive tuning Table 6: Hyper-parameter values used to produce Figure 6 Problem Algorithm (learning rate, damping, µ, β) MNIST TNT (1e-4, 0.1, 0.9, 0.9) MNIST KFAC (3e-5, 0.01, 0.999, 0.999) MNIST Shampoo (1e-4, 3e-4, 0.99, 0.99) MNIST Adam (1e-4, 1e-4, 0.99, 0.99) MNIST SGD-m (0.001, -, 0.99, -) FACES TNT (1e-6, 0.003, 0.9, 0.9) FACES KFAC (0.01, 3, 0.99, 0.99) FACES Shampoo (1e-4, 3e-4, 0.99, 0.999) FACES Adam (1e-4, 1e-4, 0.9, 0.9) FACES SGD-m (0.001, -, 0.9, -) In this subsection, we expand on the experiments whose results are plotted in Figure 2, by in- corporating the tuning of more hyper-parameters. To be more speciﬁc, we tuned the following hyper-parameters jointly: 1. SGD-m: learning rate and µ; 2. all other algorithms 8: learning rate, damping, µ, and β. The searching range for learning rate and damping is the same as in Sec D.2, whereas the searching range for µand βwere set to be t0.9,0.99,0.999u. The obtained values for the hyper-parameters are listed in Table 6. Figure 6 depicts the performance of different algorithms with hyper-parameters obtained from the aforementioned more extensive tuning process. Comparing the performance of different algorithms in Figure 6, we can see that the observations we made from Figure 2 still hold to a large extent. Moreover, with extensive tuning, second-order methods seem to perform similarly with each other, and are usually better than well-tuned ﬁrst order methods on these problems. As a ﬁnal point, we would like to mention that one could also replace the constant learning rate for all of the algorithms tested with a \"warm-up, then decay\" schedule, which has been shown to result in good performance on these problems in [3]. Also, one could perform a more extensive tuning for the CNN problems. In particular, one could tune the initial learning rate, weight decay factor, damping, µ, and βjointly for the CNN problems. See more in [8, 40] for the importance and suggestions on hyper-parameter tuning. Moreover, see [2] for other relevant numerical results, in particular for KFAC and Shampoo. In [2], KFAC is shown to work extremely well with a higher frequency of inversion, another direction for experiments that could be explored. 8For Adam, µand βrefer to β1 and β2, respectively. 23Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 7. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 7. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] See Section 4. (b) Did you include complete proofs of all theoretical results? [Yes] See Section B in the Appendix. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] The code and instructions are included in the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 6, and Section D in the Appendix. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] See Section 6. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 6. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 6. (b) Did you mention the license of the assets? [Yes] The data and models used in the paper have been properly cited. Licenses can be found in the corresponding citations, if they exist. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] See supplemental material. (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 24",
      "meta_data": {
        "arxiv_id": "2106.02925v3",
        "authors": [
          "Yi Ren",
          "Donald Goldfarb"
        ],
        "published_date": "2021-06-05T15:57:22Z",
        "pdf_url": "https://arxiv.org/pdf/2106.02925v3.pdf",
        "github_url": "https://github.com/renyiryry/tnt_neurips_2021"
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes Tensor Normal Training (TNT), a novel approximate natural gradient method for deep learning models. TNT leverages the tensor normal (TN) distribution to approximate the Fisher matrix, utilizing the block-wise covariance of sampling-based gradients. It features a tractable approximation of the Fisher matrix due to the Kronecker separable structure of the TN distribution, requiring only knowledge of training parameter shapes (model-agnostic). TNT demonstrates superior optimization performance over state-of-the-art first-order methods and comparable performance to second-order methods like KFAC and Shampoo, while also achieving comparable generalization using fewer training epochs. The method introduces a new way to identify covariance parameters of TN distributions suitable for optimization and is theoretically proven to converge to a stationary point under mild assumptions.",
        "methodology": "TNT approximates the natural gradient by modeling the covariance of sampling-based gradients using the tensor normal distribution. It assumes a block-diagonal Fisher matrix, with each block corresponding to a tensor variable (W_l), and models the sampling-based gradient (DW_l) for each variable as following a zero-mean Tensor Normal distribution with Kronecker-structured covariance factors (U_1, ..., U_k). The Fisher matrix for W is then approximated as the Kronecker product of these factors: F_W = U_1 ⊗ ... ⊗ U_k. A new identification scheme determines the U_i factors by enforcing a constant average eigenvalue (tr(U_i)/d_i) across dimensions to ensure similar magnitudes and aid damping parameter selection. The natural gradient update direction is computed using the inverse of the Kronecker product of damped (U_i + ϵI) factors, applied via mode-i products. The practical algorithm employs moving averages for gradient momentum and covariance factor estimates, with amortized updates for statistics (frequency T1) and inverses (frequency T2), and includes an additional backward pass for sampling-based gradient computation.",
        "experimental_setup": "Experiments were conducted on a machine with one V100 GPU and eight Xeon Gold 6248 CPUs using PyTorch. Hyperparameters (learning rate, damping, momentum, beta) were tuned via grid search for optimal training loss (optimization tasks) or validation accuracy (generalization tasks), with results averaged over 5 runs using different random seeds and standard deviations reported. Optimization benchmarks included MNIST and FACES autoencoder problems with specific architectures (e.g., 784-1000-500-250-30-250-500-1000-784 for MNIST), binary entropy loss for MNIST, and squared error loss for FACES, with a batch size of 1,000 and update frequencies T1=1, T2=20. Generalization benchmarks involved ResNet32 on CIFAR-10 and VGG16 (with BN layers) on CIFAR-100, using a batch size of 128, data augmentation, and weight decay for all methods. CNN training schedules differed for first-order (200 epochs) and second-order methods (100 epochs), with T1=10, T2=100. Damping values were set to 1e-8 for Adam, 0.03 for KFAC, and 0.01 for TNT/Shampoo. Comparisons were made against SGD with momentum, Adam, KFAC, Shampoo, and a variant called TNT-EF (using empirical Fisher).",
        "limitations": "The research did not include experiments on extremely large models (e.g., ImageNet) or advanced NLP tasks due to computational resource constraints, although strong potential was demonstrated. The method's performance in a distributed computing setting was not explored. Comparisons were limited to Fisher-based second-order methods, excluding Kronecker-based quasi-Newton methods like K-BFGS. The paper acknowledges that more extensive hyperparameter tuning (e.g., joint optimization of learning rate, weight decay, damping, and momentum parameters) and adaptive learning rate schedules could yield further improvements. Furthermore, the preconditioning matrices are dependent on the specific tensor shape of parameters, meaning reshaping them (e.g., flattening) would lead to different matrices. The paper also mentions potential negative societal impacts if the underlying model or data design contains flaws or bias, though this aspect is outside the scope of the presented work.",
        "future_research_directions": "Future work includes conducting experiments on larger-scale models (e.g., ImageNet) and advanced NLP tasks to further validate TNT's capabilities. Extending the method to operate effectively within a distributed computing environment is another key direction. Investigating the impact of increasing the frequency of statistics and inverse updates in second-order methods, as KFAC has shown improvements in this area, is also suggested. More comprehensive hyperparameter tuning strategies, such as jointly optimizing learning rate, weight decay, damping, and momentum parameters, along with exploring 'warm-up, then decay' learning rate schedules, are potential avenues. Additionally, developing and evaluating diagonal variants of TNT and exploring architectural modifications like dividing large tensors into smaller blocks are considered.",
        "experimental_code": "if algorithm == 'TNT':args['shampoo_if_coupled_newton'] = Falseargs['shampoo_epsilon'] = damping_valueargs['if_Hessian_action'] = Falseargs['shampoo_decay'] = 0.9args['shampoo_weight'] = 0.1args['momentum_gradient_dampening'] = 0if dataset_name in ['CIFAR-10', 'CIFAR-100']:args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-filterFlattening-warmStart-momentum-grad-LRdecay'args['shampoo_update_freq'] = 10args['shampoo_inverse_freq'] = 100elif dataset_name in ['MNIST', 'FACES']:args['algorithm'] = 'matrix-normal-correctFisher-same-trace-allVariables-KFACReshaping-warmStart-momentum-grad'args['shampoo_update_freq'] = 1args['shampoo_inverse_freq'] = 20",
        "experimental_info": "The TNT algorithm is configured with a learning rate (lr), damping value, and weight decay. It sets `shampoo_if_coupled_newton` to `False` and `if_Hessian_action` to `False`. Covariance matrix estimation uses a decay factor of `0.9` and a weight of `0.1`. Gradient momentum dampening is set to `0`. Training duration, learning rate decay schedule, and initialization package (`kaiming_normal` for CIFAR-10, `normal` for others) are dataset-dependent. For CIFAR-10/100, `shampoo_update_freq` is `10` and `shampoo_inverse_freq` is `100`; for MNIST/FACES, these are `1` and `20` respectively. `N1` and `N2` (minibatch sizes) are 128 for CIFAR-10/100 and 1000 for MNIST/FACES. Training stops after a maximum of 100 epochs for CIFAR-10/100 (40 epochs before LR decay), and 500-2000 times (CPU time) for MNIST/FACES. A seed number of 9999 and 8 threads are used."
      }
    },
    {
      "title": "Analytical Study of Momentum-Based Acceleration Methods in Paradigmatic High-Dimensional Non-Convex Problems",
      "abstract": "The optimization step in many machine learning problems rarely relies on\nvanilla gradient descent but it is common practice to use momentum-based\naccelerated methods. Despite these algorithms being widely applied to arbitrary\nloss functions, their behaviour in generically non-convex, high dimensional\nlandscapes is poorly understood. In this work, we use dynamical mean field\ntheory techniques to describe analytically the average dynamics of these\nmethods in a prototypical non-convex model: the (spiked) matrix-tensor model.\nWe derive a closed set of equations that describe the behaviour of heavy-ball\nmomentum and Nesterov acceleration in the infinite dimensional limit. By\nnumerical integration of these equations, we observe that these methods speed\nup the dynamics but do not improve the algorithmic threshold with respect to\ngradient descent in the spiked model.",
      "full_text": "Analytical Study of Momentum-Based Acceleration Methods in Paradigmatic High-Dimensional Non-Convex Problems Stefano Sarao Mannelli Department of Experimental Psychology University of Oxford Oxford, United Kingdom stefano.saraomannelli@psy.ox.ac.uk Pierfrancesco Urbani Université Paris-Saclay, CNRS, CEA Institut de physique théorique Gif-sur-Yvette, France pierfrancesco.urbani@ipht.fr Abstract The optimization step in many machine learning problems rarely relies on vanilla gradient descent but it is common practice to use momentum-based accelerated methods. Despite these algorithms being widely applied to arbitrary loss functions, their behaviour in generically non-convex, high dimensional landscapes is poorly understood. In this work, we use dynamical mean ﬁeld theory techniques to describe analytically the average dynamics of these methods in a prototypical non-convex model: the (spiked) matrix-tensor model. We derive a closed set, of equations that describe the behaviour of heavy-ball momentum and Nesterov acceleration in the inﬁnite dimensional limit. By numerical integration of these equations we observe that these methods speed up the dynamics but do not improve the algorithmic threshold with respect to gradient descent in the spiked model. 1 Introduction In many computer science applications one of the critical steps is the minimization of a cost function. Apart from very few exceptions, the simplest way to approach the problem is by running local algorithms that move down in the cost landscape and hopefully approach a minimum at a small cost. The simplest algorithm of this kind is gradient descent, that has been used since the XIX century to address optimization problems Cauchy (1847). Later on, faster and more stable algorithms have been developed: second order methods Levenberg (1944); Marquardt (1963); Broyden (1970); Fletcher (1970); Goldfarb (1970); Shanno (1970) where information from the Hessian is used to adapt the descent to the local geometry of the cost landscape, and ﬁrst order methods based on momentum Polyak (1964); Nesterov (1983); Cyrus et al. (2018); An et al. (2018); Ma and Yarats (2019) that introduce inertia in the algorithm and provably speed up convergence in a variety of convex problems. In the era of deep-learning and large datasets, the research has pushed towards memory efﬁcient algorithms, in particular stochastic gradient descent that trades off computational and statistical efﬁciency Robbins and Monro (1951); Sutskever et al. (2013), and momentum-based methods are very used in practice Lessard et al. (2016). Which algorithm is the best in practice seems not to have a simple answer and there are instances where a class of algorithms outperforms the other and vice-versa Kidambi et al. (2018). Most of the theoretical literature on momentum-based methods concerns convex problems Ghadimi et al. (2015); Flammarion and Bach (2015); Gitman et al. (2019); Sun et al. (2019); Loizou and Richtárik (2020) and, despite these methods have been successfully applied to a variety of problems, only recently high dimensional non-convex settings have been considered Yang et al. (2016); Gadat et al. (2018); Wang and Abernethy (2020). Furthermore, with few exceptions Scieur and Pedregosa (2020), the majority of these studies focus on worst-case analysis while empirically one could also be interested in the behaviour of such algorithms on typical Preprint. Under review. arXiv:2102.11755v4  [cond-mat.dis-nn]  27 Oct 2021instances of the optimization problem, formulated in terms of a generative model extracted from a probability distribution. The main contribution of this paper is the analytical description of the average evolution of momentum- based methods in two simple non-convex, high-dimensional, optimization problems. First we consider the mixed p-spin model Barrat et al. (1997); Folena et al. (2020), a paradigmatic random high-dimensional optimization problem. Furthermore we consider its spiked version, the spiked matrix-tensor Richard and Montanari (2014); Sarao Mannelli et al. (2020b) which is a prototype high-dimensional non-convex inference problem in which one wants to recover a signal hidden in the landscape. The second main result of the paper is the characterization of the algorithmic threshold for accelerated-methods in the inference setting and the ﬁnding that this seems to coincide with the threshold for gradient descent. The deﬁnition of the model and the algorithms used are reported in section 2. In section 3 and 4 we use dynamical mean ﬁeld theory Martin et al. (1973); De Dominicis (1978); Crisanti and Sommers (1992) to derive a set of equations that describes the average behaviour of these algorithms starting from random initialization in the high dimensional limit and in a fully non-convex setting. We apply our equations to the spiked matrix-tensor model Sarao Mannelli et al. (2020b, 2019b,a), which displays a similar phenomenology as the one described in Wang and Abernethy (2020); Sarao Mannelli et al. (2020a) for the phase retrieval problem: all algorithms have two dynamical regimes. First, they navigate in the non-convex landscape and, second, if the signal to noise ratio is strong enough, the dynamics eventually enters in the basin of attraction of the signal and rapidly reaches the bottom of the cost function. We use the derived state evolution of the algorithms to determine their algorithmic threshold for signal recovery. Finally, in Sec. 5 we show that in the analysed models, momentum-based methods only have an advantage in terms of speed but they do not outperform vanilla gradient descent in terms of the algorithmic recovery threshold. 2 Model deﬁnition We consider two paradigmatic non-convex models: the mixedp-spin model Crisanti and Sommers (1992); Cugliandolo and Kurchan (1993), and the spiked matrix-tensor model Richard and Montanari (2014); Sarao Mannelli et al. (2020b). Given a tensor TTT ∈(RN)⊗p and a matrix YYY ∈RN×N, the goal is to ﬁnd a common low-rank representation xxxthat minimizes the loss L= − 1 ∆p √ (p−1)! Np−1 N∑ i1,...,ip=1 Ti1,...,ip xi1 ...x ip − 1 ∆2 1√ N N∑ i,j=1 Yijxixj, (1) with xxxin the N-dimensional sphere of radius √ N. The two problems differ by the deﬁnition of the variables TTT and YYY. Call ξξξ(p) and ξξξ(2) order ptensor and a matrix having i.i.d. Gaussian elements, with zero mean and variances ∆p and ∆2 respectively. In the mixed p-spin model, tensor and matrix are completely random TTT = ξξξ(p) and YYY = ξξξ(2). While in the spiked matrix-tensor model there is a low-rank representation given by xxx∗∈SN−1( √ N) embedded in the problem as follows: Ti1...ip = √ (p−1)! Np−1 x∗ i1 ...x ∗ ip + ξ(p) i1...ip , Y ij = x∗ ix∗ j√ N + ξ(2) ij . (2) These problems have been studied both in physics, and computer science. In the physics literature, research has focused on the relationship of gradient descent and Langevin dynamics and the corre- sponding topology of the complex landscape Crisanti and Sommers (1992); Crisanti et al. (1993); Crisanti and Leuzzi (2006); Cugliandolo and Kurchan (1993); Aufﬁnger et al. (2013); Folena et al. (2020, 2021). The state evolution of the gradient descent dynamics for the mixed spiked matrix- tensor model has been studied only more recently Sarao Mannelli et al. (2019b,a). All these works considered simple gradient descent dynamics and its noisy (Langevin) dressing. In this work we focus on accelerated methods and provide an analytical characterization of the average performance of these algorithms for the models introduced above. In order to simplify the analysis we relax the hard constraint on the norm of the vector xxxand consider xxx∈RN while adding a penalty term to Lto enforce a soft constraint µ 4N (∑ ix2 i −N )2 , so that the total cost function is 2H= L+ µ 4N (∑ ix2 i −N )2 . Using the techniques described in detail in the next section we write the state evolution for the following algorithms: • Nesterov acceleration Nesterov (1983) starting from yyy[0] = xxx[0] ∈SN−1 (√ N ) xxx[t+ 1] = yyy[t] −α∇H(yyy[t]), (3) yyy[t+ 1] = xxx[t+ 1] + t t+ 3 (xxx[t+ 1] −xxx[t]) . (4) given αthe learning rate of the algorithm. • Polyak’s or heavy ball momentum(HB) Polyak (1964) starting from yyy[0] = 000 and xxx[0] ∈ SN−1 (√ N ) , given the parameters α, β yyy[t+ 1] = βyyy[t] + ∇H(xxx[t]), (5) xxx[t+ 1] = xxx[t] −αyyy[t+ 1]; (6) • gradient descent (GD) starting from xxx[0] ∈SN−1 (√ N ) xxx[t+ 1] = xxx[t] −α∇H(xxx[t]). (7) This case has been considered in Folena et al. (2020); Sarao Mannelli et al. (2019b) with the constraint ∑ ix2 i = N. The generalization to the present case in which constraint is soft is a straightforward small extension of these previous works. We will not compare the performance of these accelerated gradient methods to algorithms of different nature (such as for example message passing ones) in the same settings. Our goal will be the derivation of a set of dynamical equations describing the average evolution of such algorithms in the high dimensional limit N →∞. 3 Dynamical mean ﬁeld theory 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss H B =0.9 Nesterov GD 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius Figure 1: Simulation and DMFT comparison in mixed p-spin model. The simulations in the ﬁgures have parameters p= 3, ∆3 = 2/p, ∆2 = 1, ridge parameter µ= 10 and input dimension N = 1024. In all our simulations we use the dilution technique Semerjian et al. (2004); Krzakala and Zdeborová (2013) to reduce the computational cost. We consider: Nesterov acceleration in pink; heavy ball momentum in blue with α = 0.01 and β = 0.9; and gradient descent in grey. We run 100 simulations (in transparency) and draw the average. The parameters for heavy ball are the best parameters found in our simulations, see also Fig. 2 for a comparison. The results from the DMFT equations are drawn with dotted lines. We use dynamical mean ﬁeld theory (DMFT) techniques to derive a set of equation describing the evolution of the algorithms in the high-dimensional limit. The method has its origin in statistical physics and can be applied to the study of Langevin dynamics of disordered systems Martin et al. (1973); De Dominicis (1978); Mézard et al. (1987). More recently it was proved to be rigorous in the 3case of the mixed p-spin model Ben Arous et al. (2006); Dembo and Subag (2020). The application to the inference version of the optimization problem is in Sarao Mannelli et al. (2020b, 2019b). The same techniques have also been applied to study the stochastic gradient descent dynamics in single layer networks Mignacco et al. (2020) and in the analysis of recurrent neural networks Sompolinsky et al. (1988); Mastrogiuseppe and Ostojic (2017); Can et al. (2020). The derivation presented in the rest of the section is heuristic and, as such, it is not fully rigorous. Making our results rigorous would be an extension of the works Ben Arous et al. (2006); Dembo and Subag (2020) where path-integral methods are used to prove a large deviation principle for the inﬁnite-dimensional limit. Our non-rigorous results are checked against extensive numerical simulations. The idea behind DMFT is that, if the input dimension N is sufﬁciently large, one can obtain a description of the dynamics in terms of the typical evolution of a representative entry of the vector xxx (and vector yyywhen it applies). The representative element evolves according to a non-Markovian stochastic process whose memory term and noise source encode, in a self-consistent way, the interaction with all the other components of vector xxx(and yyy). The memory terms as well as the statistical properties of the noise are described by dynamical order parameters which, in the present model, are given by the dynamical two-time correlation and response functions. In this ﬁrst step of the analysis we obtain an effective dynamics for a representative entry xi (and yi). The next step consists in using such equations to compute self-consistently the properties of the corresponding stochastic processes, namely the memory kernel and the statistical correlation of the noise. In Fig. 1 we anticipate the results by comparing numerical simulations with the integration of the DMFT equations for the different algorithms: on the left we observe the evolution of the loss, on the right we observe the evolution of the radius of the vector xxx, deﬁned as the L2 norm of the vector ||xxx||2. We ﬁnd a good agreement between the DMFT state evolution and the numerical simulations. We compare Nesterov acceleration with the heavy ball momentum in the mixed p-spin model Fig. 1, and in the spiked model Fig. 3. Nesterov acceleration allows for a fast convergence to the asymptotic energy without need of parameter tuning. In Fig. 2 we compare the numerical simulations for the HB algorithm and the DMFT description of the corresponding massive momentum version for several control parameters. DMFT equations In the following we describe the resulting DMFT equations for the correlation and response functions. The details of their derivation for the case of the Nesterov acceleration are provided in the following section, while we leave the other cases to the supplementary material (SM). The dynamical order parameters appearing in the DMFT equations are one-time or two-time correlations, e.g. Cxy[t,t′] = ∑ ixi[t]yi[t′]/N, and response to instantaneous perturbation of the dynamics, e.g. Rx[t,t′] = (∑ iδxi[t]/δHi[t′] ) /Nby a local ﬁeld HHH[t′] ∈RN where the symbol δ denotes the functional derivative. In this section we show only the equations for the mixed p-spin model and we discuss the difference and the derivation of the equations for the spiked tensor in the SM. From the order parameters we can evaluate useful quantities that describe the evolution of the algorithms. In particular in Figs. 1,2,3 we show the loss, the radius, and the overlap with the solution in the spiked case (Fig. 3): • Average loss L[t] = − α ∆pCx[t,t] p 2 t∑ t′′=0 Rx[t,t′]Cx[t,t′]p−1 − α ∆2Cx[t,t] t∑ t′′=0 Rx[t,t′]Cx[t,t′]; (8) • Radius √ Cx[t,t]; • Deﬁne mx[t] = 1 N ∑ ixi[t]x∗ i an additional order parameter for the spiked matrix-tensor model (more details are given in the SM), the overlap with ground truth is xxx[t] ·xxx∗ ||xxx|| = mx[t]√ Cx[t,t] 4Nesterov acceleration. It has been shown that this algorithm has a quadratic convergence rate to the minimum in convex optimization problems under Lipschitz loss functions Nesterov (1983); Su et al. (2014), thus it outperforms standard gradient descent whose convergence is linear in the number of iterations. The analysis of the algorithm is described by the ﬂow of the following dynamical correlation functions Cx[t,t′] = 1 N ∑ i xi[t]xi[t′], (9) Cy[t,t′] = 1 N ∑ i yi[t]yi[t′], (10) Cxy[t,t′] = 1 N ∑ i xi[t]yi[t′], (11) Rx[t,t′] = 1 N ∑ i δxi[t] δHi[t′], (12) Ry[t,t′] = 1 N ∑ i δyi[t] δHi[t′]. (13) The dynamical equations are obtained following the procedure detailed in section 4. Call Q(x) = x2/(2∆2) + xp/(p∆p), Cx[t+ 1,t′] = Cxy[t,t′] −αµ(Cy[t,t] −1) Cy[t,t′] + α2 t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cy[t,t′′]) + + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cxy[t′,t′′]; (14) Cxy[t+ 1,t′] = Cy[t,t′] −αµ(Cy[t,t] −1) Cxy[t,t′] + α2 t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cy[t,t′′]) + + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cy[t′,t′′]; (15) Cxy[t′,t + 1] = 2t+ 3 t+ 3 Cx[t+ 1,t′] − t t+ 3Cx[t,t′]; (16) Cy[t′,t + 1] = 2t+ 3 t+ 3 Cxy[t+ 1,t′] − t t+ 3Cxy[t,t′]; (17) Rx[t+ 1,t′] = Ry[t,t′] + δt,t′ −αµ(Cy[t,t] −1) Ry[t,t′] + α2 t∑ t′′=t′ Ry[t,t′′]Ry[t′′,t′]Q′′(Cy[t,t′′]) ; (18) Ry[t′,t + 1] = 2t+ 3 t+ 3 Rx[t+ 1,t′] − t t+ 3Rx[t,t′]. (19) The initial conditions are: Cx[0,0] = 1, Cy[0,0] = 1, Cxy[0,0] = 1, Rx[t+ 1,t] = 1, Ry[t+ 1,t] = 2t+3 t+3 . The equations show a discretized version of the typical structure of DMFT equations. We can observe: terms immediately ascribable to the dynamical equations (3,4) and summations whose interpretation is less trivial without looking into the derivation. They represent memory kernels that take into account linear response theory for small perturbations to the dynamics (e.g. the last term of Eq. equation 14) and a noise whose statistical properties encode the effect of all the degrees of freedom on a representative one (e.g. the second last term of Eq. equation 14). 5Heavy ball momentum. The DMFT equations are obtained analogously to previous ones, Cy[t+ 1,t′] = βCy[t,t′] + µ(Cx[t,t] −1) Cxy[t,t′] + α t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cxy[t′′,t′]; (20) Cxy[t′,t + 1] = βCxy[t′,t] + µ(Cx[t,t] −1) Cx[t,t′] + α t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cx[t′,t′′]; (21) Cxy[t+ 1,t′] = Cxy[t,t′] −αCy[t+ 1,t′]; (22) Cx[t+ 1,t′] = Cx[t,t′] −αCxy[t′,t + 1]; (23) Ry[t+ 1,t′] = βRy[t,t′] + 1 αδt,t′ + µ(Cx[t,t] −1) Rx[t,t′] + α t∑ t′′=0 Rx[t,t′′]Rx[t′′,t′]Q′′(Cx[t,t′′]) ; (24) Rx[t+ 1,t′] = Rx[t,t′] −αRy[t+ 1,t′]. (25) with initial conditions: Cx[0,0] = 1, Cy[0,0] = 0, Cxy[0,0] = 0, Ry[t+ 1,t] = 1/α, Rx[t+ 1,t] = −1. Fig. 2 shows the consistency of theory and simulations. Mappings between discrete update equation and continuous ﬂow for both heavy ball momentum and Nesterov acceleration have been proposed in the literature. In the SM we considered the work Qian (1999) that maps HB to second order ODEs in some regimes of αand β. This mapping establishes the equivalence of the algorithm to the physics problem of a massive particle moving under the action of a potential. This problem has been studied in Cugliandolo et al. (2017) but the result is limited to the fully under-damped regime where there is no ﬁrst order derivative term, corresponding therefore to a dynamics that is fully inertial and which never stops due to energy conservation. In the SM we obtain the dynamical equations for arbitrary damping regimes, and we recover the equivalence established in Qian (1999) comparing the results from the two DMFTs formulations. 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss =0.9 =0.7 =0.5 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius Figure 2: DMFT for HB. Simulations of HB momentum in the mixed p-spin model with p = 3, ∆3 = 2/p, ∆2 = 1, ridge parameter µ= 10 and input dimension N = 1024. The parameters are α= 0.01 for all the simulations and β ∈{0.5,0.7,0.9}. We use solid lines to represent the result from the simulation, the dotted lines for the DMFT of HB. 6Gradient descent. A simple way to obtain the gradient descent DMFT is by taking the limitm→0 in the DMFT of the massive momentum description of HB. We get Cx[t+ 1,t′] = Cx[t,t′] −αµ(Cx[t,t] −1) Cx[t,t′] + α2 t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cx[t,t′′]) + α2 t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cx[t′,t′′]; (26) Rx[t+ 1,t′] = Rx[t,t′] + δt,t′ + α2 t∑ t′′=0 Rx[t,t′′]Rx[t′′,t′]Q′′(Cx[t,t′′]) −αµ(Cx[t,t] −1) Rx[t,t′]. (27) with initial conditions: Cx[0,0] = 1, and Rx[t+ 1,t] = 1. Apart from the µ-dependent term, these equations are a particular case of the ones that appear in Cugliandolo and Kurchan (1993); Crisanti et al. (1993) and we point to these previous references for details. 4 Derivation of DMFT for Nesterov acceleration 100 101 102 103 104 iteration 2.0 1.5 1.0 0.5 0.0 loss GD HB =0.9 Nesterov 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20 1.25radius 100 101 102 103 104 iteration 0.0 0.2 0.4 0.6 0.8 1.0overlap with solution Figure 3: DMFT in the spiked matrix-tensor model. Performance of heavy ball and Nesterov in the spiked matrix-tensor model with p= 3, 1/∆2 = 2.7, ∆3 = 1.0, and µ= 10. The parameters in the simulations are: α = 0 .01 and β = 0 .9 for HB. The different solid lines correspond to simulations with input dimension N = 8192, while the dotted lines are obtained from the DMFT that, by deﬁnition, is in the inﬁnite dimension limit. In the spiked version of the model the ﬁnite size effects are stronger and larger simulation sizes are needed. The approach for the DMFT proposed in this section is based on the dynamical cavity method Mézard et al. (1987). Consider the problem having dimension N + 1 and denote the additional entry of the vectors xxxand yyywith the subscript 0, x0 and y0. The idea behind cavity method is to evaluate how this additional dimension changes the dynamics of all degrees of freedom. If the dimension is sufﬁciently large the dynamics is only slightly modiﬁed by the additional dimension, and the effect of the additional degree of freedom can be tracked in perturbation theory. The framework described in this section might be extended to more other momentum-based algorithms (such as PID An et al. (2018) and quasi-hyperbolic momentum Ma and Yarats (2019)) with some minor adaptations. The steps to follow Mézard et al. (1987) can be summarised in: • Writing the equation of motion isolating the contributions of an additional degree of freedom, leading to Eqs. (28-30; • Treating the effect of the terms containing the new degree of freedom in perturbation theory, Eqs. (32-34); • Identifying the order dynamical order parameters, namely dynamical correlation and re- sponse functions, Eqs. (37,38). 7Consider the Nesterov update algorithm and isolate the effect of the additional degree of freedom xi[t+ 1] = yi[t] + α ∑ j̸=0 Jijyj[t] + α ∑ (i,i2,...,ip) Ji,i2,...,ip yi2 [t] ...y ip [t] −αµ  ∑ j̸=0 y2 j[t] N −1  yi[t] (28) + α ∑ (i,0,i3,...,ip) Ji,0,i3,...,ip y0[t]yi3 [t] ...y ip [t] + αJi0y0[t] + µ Ny2 0[t]yi[t], (29) yi[t+ 1] = xi[t+ 1] + t t+ 3 (xi[t+ 1] −xi[t]) . (30) We identify the term in line (29) as a perturbation, denoted by Hi[t]. We will assume that the perturbation is sufﬁciently small and the effective dynamics is well approximated by a ﬁrst order expansion around the original updates, so-called linear response regime. Therefore, the perturbed entries can be written as xi[t] ≈x0 i + α t∑ t′′=0 δxi[t] δHi[t′′]Hi[t′′], y i[t] ≈y0 i + α t∑ t′′=0 δyi[t] δHi[t′′]Hi[t′′]. (31) The dynamics of the 0th degree of freedom to the leading order in the perturbation is x0[t+ 1] = y0[t] −αµ (1 N ∑ j y2 j[t] −1 ) y0[t] + Ξ[t] + α2 ∑ j J0j t∑ t′′=0 δyj[t] δHj[t′′]Hj[t′′] (32) + α2 ∑ (0,i2,...,ip) J0,i2,...,ip ( t∑ t′′=0 δyi2 [t] δHi2 [t′′]Hi2 [t′′]yi3 [t] ...y ip [t] + perm. ) + O (1 N ) , (33) yi[t+ 1] = xi[t+ 1] + t t+ 3 (xi[t+ 1] −xi[t]) , (34) with Ξ = α∑ jJ0jyj[t] + α∑ (0,i2,...,ip) J0,i2,...,ip yi2 [t] ...y ip [t] a Gaussian noise with moments: E[Ξ[t]] = 0, E[Ξ[t]Ξ[t′]] = 1 ∆2 Cy[t,t′] + 1 ∆p Cp−1 y [t,t′] = Q′(Cy[t,t′]) ˙ =K[t,t′]. The terms in Eqs. (32,33) can be simpliﬁed. Consider the last term in Eq. equation 32: after substituting the Hi, J0jJ0j and J0jJ(j,0,...,ip) can be approximated by their expected values with a difference that is subleading in 1/N α2 ∑ j J0j t∑ t′′=0 δyj[t] δHj[t′′]J0jy0[t′′] ≈ α2 ∆2N t∑ t′′=0 δyj[t] δHj[t′′]y0[t′′] = α2 ∆2 t∑ t′′=0 Ry[t,t′′]y0[t′′], (35) where the last equality follows from the deﬁnition of response function in y. The same approximation is applied to Eq. (33), taking carefully into account the permutations, obtaining α2(p−1) ∆p t∑ t′′=0 Ry[t,t′′] (Cy[t,t′′])p−2 y0[t′′]. (36) Finally, collecting all terms, the effective dynamics of the additional dimension is given by x0[t+ 1] = y0[t] + αΞ[t] −αµ(Cy[t,t] −1) y0[t] + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) y0[t′′]; (37) y0[t+ 1] = x0[t+ 1] + t t+ 3 (x0[t+ 1] −x0[t]) . (38) 8In order to derive the updates of the order parameters, we need the expected values of ⟨Ξ[t]x0[t′]⟩ and ⟨Ξ[t]y0[t′]⟩with respect to the stochastic process. These are obtained using Girsanov theorem ⟨Ξ[t]x0[t′]⟩= α ∑ t′′ Rx[t′,t′′]Q′(Cy[t,t′′]) , ⟨Ξ[t]y0[t′]⟩= α ∑ t′′ Ry[t′,t′′]Q′(Cy[t,t′′]) . The ﬁnal step consists in substituting the Eqs. (37,38) into the equations of the order parameters Eqs. (14-19). Then we identify the order parameters in the equations and use the results of Girsanov theorem to obtain the dynamical equations reported in section 3. 5 Algorithmic threshold 0 1 2 3 4 5 p 0.5 1.0 1.5 2.0 2.5 3.0 3.51/ 2 GD extrapolated th. HB extrapolated th. Nesterov extrapolated th. Figure 4: Phase diagram of the spiked matrix-tensor model. The horizontal and vertical axis represent the parameters of the model ∆p and 1/∆2. We identify two regions in the diagram: where Nesterov, heavy ball and gradient descent algorithms lead to the hidden solution (upper region), and where they fail (lower region). The grey square connected by a solid line represents the threshold of gradient descent estimated numerically as detailed in the text. We use points to indicate the threshold extrapolated from the DMFT: pink circles for Nesterov acceleration and blue diamonds for heavy ball momentum with β = 0.9 and α= 0.01. Finally we investigate the performance of accelerated methods in recovering a signal in a complex non-convex landscape. The dynamics of the gradient descent has been studied in the spiked matrix- tensor model in Sarao Mannelli et al. (2019b). Using DMFT it was possible to compute the phase diagram for signal recovery in terms of the noise levels ∆2 and ∆p. This phase diagram was later conﬁrmed theoretically Sarao Mannelli et al. (2019a). Given the DMFT equations derived in the previous sections we can apply the analysis used in Sarao Mannelli et al. (2019b) to accelerated gradient methods. Given order of the tensor pand ∆p, increasing ∆2 the problem becomes harder and moves from the easy phase - where the signal can be partially recovered - to an algorithmically impossible phase - where the algorithm remains stuck at vanishingly small overlap with the signal. The goal of the analysis is to characterize the algorithmic threshold that separates the two phases. Using the DMFT we estimate the relaxation time – the time the accelerated methods need to ﬁnd the signal. Since this time diverges approaching the algorithmic threshold, the ﬁt of the divergence point gives an estimation of the threshold. More precisely, for each value of ∆p as the noise to signal ratio (∆2) increases the simulation time required to arrive close to the signal 1 increases like a power law ∼a |∆2 −∆al. 2 (∆p)|−θ. The algorithmic threshold ∆al. 2 (∆p) is obtained by ﬁtting the parameters of the power law (a,θ, ∆al. 2 ). In the SM we show an example of the extrapolation of a single point where many initial conditions mx(0) are considered in order to correctly characterize the limits N →∞ and mx(0) →0+. Finally the ﬁts obtained for the three algorithms and for several ∆p are shown in the phase diagram of Fig. 4 for p= 3. We observe that all the algorithms give very close thresholds. DMFT allows to obtain a good estimation of the threshold, free from ﬁnite size effects and stochastic ﬂuctuations that are present in the direct estimation from the simulations. 1Since the best possible overlap for maximum a posteriori estimator mMAP can be computed explicitly, \"close\" means the time that the algorithms takes to arrive at 0.9mMAP 9Conclusions and broader impact In this work we analysed momentum-accelerated methods in two paradigmatic high-dimensional non-convex problems: the mixed p-spin model and the spiked matrix-tensor model. Our analysis is based on dynamical mean ﬁeld theory and provides a set of equations that characterize the average evolution of the dynamics. We have focused on Polyak’s heavy ball and Nesterov acceleration, but the same techniques may be applied to more recent methods such as quasi-hyperbolic momentum Ma and Yarats (2019) and proportional integral-derivative control algorithm An et al. (2018). Momentum-based methods are techniques commonly used in practice but poorly understood at the theoretical level. This work analysed the dynamics of momentum-based algorithms in a very con- trolled setting of a high-dimensional non-convex inference problem which allowed us to establish that accelerated methods have a recovery threshold which is – within the limits of numerical integration – the same of vanilla gradient descent. Our analysis can be easily extended to 1-layer neural networks – combining our technical results with the techniques of Mignacco et al. (2020) – and to simple inference problem seen from the learning point of view, such as the phase retrieval problem Mignacco et al. (2021). The same questions can also be analysed in the context of recurrent networks Mastrogiuseppe and Ostojic (2017); Can et al. (2020) where DMFT approaches have already been applied to gradient-based methods. Our study is theoretical in nature and we do not foresee any societal impact. Acknowledgments The authors thank Andrew Saxe for precious discussions. This work was supported by the Wellcome Trust and Royal Society (grant number 216386/Z/19/Z), and by \"Investissements d’Avenir\" LabEx- PALM (ANR-10-LABX-0039-PALM). References Elisabeth Agoritsas, Giulio Biroli, Pierfrancesco Urbani, and Francesco Zamponi. Out-of-equilibrium dynamical mean-ﬁeld equations for the perceptron model. Journal of Physics A: Mathematical and Theoretical, 51(8):085002, 2018. Wangpeng An, Haoqian Wang, Qingyun Sun, Jun Xu, Qionghai Dai, and Lei Zhang. A pid controller approach for stochastic optimization of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8522–8531, 2018. Antonio Aufﬁnger, Gérard Ben Arous, and Ji ˇrí ˇCern`y. Random matrices and complexity of spin glasses. Communications on Pure and Applied Mathematics, 66(2):165–201, 2013. Alain Barrat, Silvio Franz, and Giorgio Parisi. Temperature evolution and bifurcations of metastable states in mean-ﬁeld spin glasses, with connections with structural glasses. Journal of Physics A: Mathematical and General, 30(16):5593–5612, aug 1997. doi: 10.1088/0305-4470/30/16/006. URL https://doi.org/10.1088/0305-4470/30/16/006. Gérard Ben Arous, Amir Dembo, and Alice Guionnet. Cugliandolo-Kurchan equations for dynamics of spin-glasses. Probability theory and related ﬁelds, 136(4):619–660, 2006. Charles G Broyden. The convergence of a class of double-rank minimization algorithms: 2. the new algorithm. IMA journal of applied mathematics, 6(3):222–231, 1970. Tankut Can, Kamesh Krishnamurthy, and David J Schwab. Gating creates slow modes and controls phase-space complexity in grus and lstms. In Mathematical and Scientiﬁc Machine Learning , pages 476–511. PMLR, 2020. Tommaso Castellani and Andrea Cavagna. Spin-glass theory for pedestrians. Journal of Statistical Mechanics: Theory and Experiment, 2005(05):P05012, 2005. Augustin Cauchy. Méthode générale pour la résolution des systemes d’équations simultanées. Comp. Rend. Sci. Paris, 25(1847):536–538, 1847. 10Andrea Crisanti and Luca Leuzzi. Spherical 2+ p spin-glass model: An analytically solvable model with a glass-to-glass transition. Physical Review B, 73(1):014412, 2006. Andrea Crisanti and H-J Sommers. The sphericalp-spin interaction spin glass model: the statics. Zeitschrift für Physik B Condensed Matter, 87(3):341–354, 1992. Andrea Crisanti, Heinz Horner, and H-J Sommers. The sphericalp-spin interaction spin-glass model. Zeitschrift für Physik B Condensed Matter, 92(2):257–271, 1993. Leticia F Cugliandolo and Jorge Kurchan. Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model. Physical Review Letters, 71(1):173, 1993. Leticia F Cugliandolo, Gustavo S Lozano, and Emilio N Nessi. Non equilibrium dynamics of isolated disordered systems: the classical hamiltonian p-spin model. Journal of Statistical Mechanics: Theory and Experiment, 2017(8):083301, 2017. Saman Cyrus, Bin Hu, Bryan Van Scoy, and Laurent Lessard. A robust accelerated optimization algorithm for strongly convex functions. In 2018 Annual American Control Conference (ACC), pages 1376–1381. IEEE, 2018. C De Dominicis. Dynamics as a substitute for replicas in systems with quenched random impurities. Physical Review B, 18(9):4913, 1978. Amir Dembo and Eliran Subag. Dynamics for spherical spin glasses: disorder dependent initial conditions. Journal of Statistical Physics, pages 1–50, 2020. Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size. In Conference on Learning Theory, pages 658–695. PMLR, 2015. Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3):317–322, 1970. Giampaolo Folena, Silvio Franz, and Federico Ricci-Tersenghi. Rethinking mean-ﬁeld glassy dynamics and its relation with the energy landscape: The surprising case of the spherical mixed p-spin model. Physical Review X, 10(3):031045, 2020. Giampaolo Folena, Silvio Franz, and Federico Ricci-Tersenghi. Gradient descent dynamics in the mixed p-spin spherical model: ﬁnite-size simulations and comparison with mean-ﬁeld integration. Journal of Statistical Mechanics: Theory and Experiment, 2021(3):033302, 2021. Sébastien Gadat, Fabien Panloup, Soﬁane Saadane, et al. Stochastic heavy ball. Electronic Journal of Statistics, 12(1):461–529, 2018. Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the heavy-ball method for convex optimization. In 2015 European control conference (ECC), pages 310–315. IEEE, 2015. Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum in stochastic gradient methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 9633–9643. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper/2019/file/4eff0720836a198b6174eecf02cbfdbf-Paper.pdf. Donald Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26, 1970. Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufﬁciency of existing momentum schemes for stochastic optimization. In 2018 Information Theory and Applications Workshop (ITA), pages 1–9. IEEE, 2018. Florent Krzakala and Lenka Zdeborová. Performance of simulated annealing in p-spin glasses. In Journal of Physics: Conference Series, volume 473, page 012022. IOP Publishing, 2013. Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1):57–95, 2016. 11Kenneth Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of applied mathematics, 2(2):164–168, 1944. Nicolas Loizou and Peter Richtárik. Momentum and stochastic momentum for stochastic gradi- ent, newton, proximal point and subspace descent methods. Computational Optimization and Applications, 77(3):653–710, 2020. Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for deep learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=S1fUpoR5FQ. Donald W Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the society for Industrial and Applied Mathematics, 11(2):431–441, 1963. Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. Physical Review A, 8(1):423, 1973. Francesca Mastrogiuseppe and Srdjan Ostojic. Intrinsically-generated ﬂuctuating activity in excitatory-inhibitory networks. PLoS computational biology, 13(4):e1005498, 2017. Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications, volume 9. World Scientiﬁc Publishing Company, 1987. Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Dynamical mean-ﬁeld theory for stochastic gradient descent in gaussian mixture classiﬁcation. In 2020 Conference on Neural Information Processing Systems-NeurIPS 2020, 2020. Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborová. Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem. Machine Learning: Science and Technology, 2021. Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983. Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1–17, 1964. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1): 145–151, 1999. Emile Richard and Andrea Montanari. A statistical model for tensor pca. In Z. Ghahra- mani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Ad- vances in Neural Information Processing Systems , volume 27, pages 2897–2905. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/ b5488aeff42889188d03c9895255cecc-Paper.pdf. Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951. Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, and Lenka Zdeborová. Who is afraid of big bad minima? analysis of gradient-ﬂow in spiked matrix-tensor models. In Advances in Neural Information Processing Systems, pages 8679–8689, 2019a. Stefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Passed & spurious: Descent algorithms and local minima in spiked matrix-tensor models. In International Conference on Machine Learning, pages 4333–4342, 2019b. Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Complex dynamics in simple neural networks: Understanding gradient ﬂow in phase retrieval. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 3265–3274. Cur- ran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/ 2172fde49301047270b2897085e4319d-Paper.pdf. 12Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Marvels and pitfalls of the langevin algorithm in noisy high-dimensional inference. Physical Review X, 10(1):011057, 2020b. Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8565–8572. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/scieur20a.html. Guilhem Semerjian, Leticia F Cugliandolo, and Andrea Montanari. On the stochastic dynamics of disordered spin models. Journal of statistical physics, 115(1):493–530, 2004. David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of computation, 24(111):647–656, 1970. Haim Sompolinsky, Andrea Crisanti, and Hans-Jurgen Sommers. Chaos in random neural networks. Physical review letters, 61(3):259, 1988. Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s accelerated gradient method: Theory and insights. Advances in neural information processing systems, 27:2510–2518, 2014. Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang. Non-ergodic convergence analysis of heavy-ball algorithms. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 5033–5040, 2019. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139– 1147. PMLR, 2013. Jun-Kun Wang and Jacob Abernethy. Quickly ﬁnding a benign region via heavy ball momentum in non-convex optimization. arXiv preprint arXiv:2010.01449, 2020. Tianbao Yang, Qihang Lin, and Zhe Li. Uniﬁed convergence analysis of stochastic momentum methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016. 13Supplemental Material A Spiked matrix-tensor model In this section we discuss how the DMFT equations of the spiked matrix-tensor model differ from the mixed p-spin model. The main difference is that the hidden solution deforms locally the loss function H= − 1 ∆p √ (p−1)! Np−1 N∑ i1,...,ip=1 Ti1,...,ip xi1 ...x ip − 1 ∆2 1√ N N∑ i,j=1 Yijxixj+ − 1 p∆p  1 N ∑ j xjx∗ j   p − 1 2∆2  1 N ∑ j xjx∗ j   2 + µ 4N (N∑ i=1 x2 i −N )2 . (39) As it clearly appears from the equation of the loss, the overlap of the hidden solution with the estimator plays an important role. This leads to two additional order parameters mx[t] = 1 N ∑ jxj[t]x∗ j and my[t] = 1 N ∑ jyj[t]x∗ j (or mv(t) = 1 N ∑ jvj(t)x∗ j for massive gradient ﬂow). Since the stochastic part of the loss is unchanged, the derivation follows same steps shown in section 4 of the main text. They lead to modiﬁed dynamical equations where overlap with the hidden solution is present, for instance in Nesterov they are x0[t+ 1] = y0[t] + αΞ[t] −αµ(Cy[t,t] −1) y0[t]+ + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) y0[t′′] + Q′(my[t])xxx∗, (40) y0[t+ 1] = x0[t+ 1] + t t+ 3 (x0[t+ 1] −x0[t]) . (41) Finally, substituting the effective dynamics into the deﬁnition of the order parameters we obtain: • for Nesterov acceleration Cx[t+ 1,t′] = Cxy[t,t′] + α2 t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cy[t,t′′]) + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cxy[t′,t′′]+ −αµ(Cy[t,t] −1) Cy[t,t′] −Q′(my[t])mx[t′], Cxy[t+ 1,t′] = Cy[t,t′] + α2 t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cy[t,t′′]) + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′]) Cy[t′,t′′]+ −αµ(Cy[t,t] −1) Cxy[t,t′] −Q′(my[t])my[t′], Cxy[t′,t + 1] = Cx[t+ 1,t′] + t t+ 3 (Cx[t+ 1,t′] −Cx[t,t′]) , Cy[t′,t + 1] = Cxy[t+ 1,t′] + t t+ 3 (Cxy[t+ 1,t′] −Cxy[t,t′]) , Rx[t+ 1,t′] = Ry[t,t′] + δt,t′ + α2 t∑ t′′=t′ Ry[t,t′′]Ry[t′′,t′]Q′′(Cy[t,t′′]) −αµ(Cy[t,t] −1) Ry[t,t′], Ry[t′,t + 1] = Rx[t+ 1,t′] + t t+ 3 (Rx[t+ 1,t′] −Rx[t,t′]) , 14mx[t+ 1] = my[t] −αµ(Cy[t,t] −1) my[t] + α2 t∑ t′′=0 Ry[t,t′′]Q′′(Cy[t,t′′)) my[t′′] + Q′(my[t]) , my[t+ 1] = mx[t+ 1] + t t+ 3 (mx[t+ 1] −mx[t]) , with initial conditions Cx[0,0] = 1 , Cy[0,0] = 1 , Cxy[0,0] = 1 , Rx[t + 1,t] = 1 , Ry[t+ 1,t] = 2t+3 t+3 , mx[0] = 0+, my[0] = 0+; • for heavy ball momentum. Cy[t+ 1,t′] = βCy[t,t′] + µ(Cx[t,t] −1) Cxy[t,t′] + α t′ ∑ t′′=0 Ry[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cxy[t′′,t′] −Q′(mx[t])my[t′]; Cxy[t′,t + 1] = βCxy[t′,t] + µ(Cx[t,t] −1) Cx[t,t′] + α t′ ∑ t′′=0 Rx[t′,t′′]Q′(Cx[t,t′′]) + α t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′]) Cx[t′,t′′] −Q′(mx[t])mx[t′]; Cxy[t+ 1,t′] = Cxy[t,t′] −αCy[t+ 1,t′]; Cx[t+ 1,t′] = Cx[t,t′] −αCxy[t′,t + 1]; Ry[t+ 1,t′] = βRy[t,t′] + 1 αδt,t′ + µ(Cx[t,t] −1) Rx[t,t′] + α t∑ t′′=0 Rx[t,t′′]Rx[t′′,t′]Q′′(Cx[t,t′′]) ; Rx[t+ 1,t′] = Rx[t,t′] −αRy[t+ 1,t′] my[t+ 1] = βmy[t] −µ(Cx[t,t] −1) mx[t]+ + t∑ t′′=0 Rx[t,t′′]Q′′(Cx[t,t′′)) mx[t′′] −Q′(mx[t]) , mx[t+ 1] = mx[t] −αmy[t+ 1]. with initial conditions: Cx[0,0] = 1 , Cy[0,0] = 0 , Cxy[0,0] = 0 , Ry[t+ 1,t] = 1 /α, Rx[t+ 1,t] = −1, my[0] = O+, mx[0] = O+. • for massive gradient ﬂow (see Sec. C) ∂tCx(t,t′) = Cxv(t′,t) , m∂tCv(t,t′) = −Cv(t,t′) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]Cxv(t′′,t′)+ + ∫ t′ 0 Q′[Cx(t,t′′)]Rv(t′,t′′) −µCx(t.t′) (Cx(t,t) −1) + Q′[mx(t′)]mx(t) , ∂tCxv(t,t′) = Cv(t,t′) , m∂t′ Cxv(t,t′) = −Cxv(t,t′) + ∫ t′ 0 dt′′Rx|v(t′,t′′)Q′′[Cx(t′,t′′)]Cx(t,t′′)+ + ∫ t 0 Q′[Cx(t′,t′′)]Rx|v(t,t′′) −µCxv(t,t′) (Cx(t,t) −1) + Q′[mx(t)]mv(t′) , m∂tRv(t,t′) = δ(t−t′) −Rv(t,t′) + ∫ t t′ dt′′Q′′[C(t,t′′)]Rx|v(t,t′′)Rx|v(t′′,t′)+ −µRx|v(t,t′) (Cx(t,t) −1) , 15∂tRx|v(t,t′) = Rv(t,t′) , ∂tmx(t) = mv(t) , m∂tmv(t) = −mv(t) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]mx(t′′) + Q′[mx(t)]+ −µmx(t) (Cx(t,t) −1) , with initial conditions are : Cx(0,0) = 1; Cv(0,0) = 0; Cxv(0,0) = 0; Rv(t+,t) = 1/m; Rx|v(t,t) = 0; mx(0) = 0+. my(0) = 0+. Finally the equation to compute the loss in time is L[t] = − α ∆pCx[t,t] p 2 t∑ t′′=0 Rx[t,t′]Cx[t,t′]p−1 − α ∆2Cx[t,t] t∑ t′′=0 Rx[t,t′]Cx[t,t′] −Q(mx[t]) . (42) B Extracting the recovery threshold 1.5 2.0 2.5 3.0 1/ 2 0 5000 10000 15000 20000 25000 30000 tjump 1.5 2.0 2.5 3.0 1/ 2 3 4 5 6 7 8 ln tjump rescaled estrapolation 1.38 mx(0)=1.0e-10 mx(0)=1.0e-15 mx(0)=1.0e-20 mx(0)=1.0e-25 mx(0)=1.0e-30 mx(0)=1.0e-35 mx(0)=1.0e-40 Figure 5: Algorithmic threshold extrapolated using Nesterov acceleration. The dots are the divergence time obtained for the different values of ∆2 with ∆3 = 4.0 ﬁxed. On the right panel we show that these lines collapse to a single line once rescaled by a factor aln mx[0] with a≈1.089. The extrapolation procedure for ∆3 = 4.0 is shown in Fig. 5. The threshold obtained by ﬁtting with a power law and observing the divergent ∆2. On the left panel we plot the number of iteration before the algorithm jumps to the solution tjump as a function of the signal to noise ratio 1/∆2. The ﬁgure also show remarkable effects of the initial conditions for mx and my. These effects where already described and understood in Sarao Mannelli et al. (2019b). 16C Correspondence with continuous HB equations 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss = 0.9 = 0.7 = 0.5 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius DMFT massive DMFT HB Figure 6: Comparison of HB and massive with mapping. The ﬁgure reproduce the same setting of Fig. 2 with the additional dashed line for the DMFT of massive gradient ﬂow using the mapping. An alternative way to analyze the HB dynamics is by using the results of Qian (1999) to map it to the massive momentum described by the ﬂow equation m¨xi(t) + ˙xi(t) = −δL[xxx(t)] δxi(t) . (43) The natural discretization of this equation is m h2 (x[k+ 1]−2x[k]−x[k−1])+ 1 h(x[k+ 1] −x[k]) = −∇L(x[k]) (44) being hthe time discretization step (analogous to the learning rate in gradient descent). Using the mapping of Qian (1999) we can identify m= βα (1 −β)2 (45) h= α 1 −β (46) Observe that in order to be consistent with a continuous dynamics we need the following scaling β = O(1), α= O[(1 −β)2]. We empirically observe in the simulations a good agreement between massive and HB even forβ = 0.999 and α= 0.01. In the following, when discussing the comparison between simulation and DMFT, we mean that we run HB algorithm and superimpose on its massive momentum description. The massive momentum dynamics was also considered in Cugliandolo et al. (2017) without the damping term ( ˙xi(t)) and for the model with a hard spherical constraint ∑ ixi[t]2 = N. While the DMFT derived in Cugliandolo et al. (2017) completely describe the aforementioned particular case, the way in which it is written uses the fact that without damping the dynamics is conservative and the spherical constraint can be enforced using that. In our case we are not in this regime and therefore we resort to a different computation, that will lead us to quite different equations. Indeed if one wants to transform massive momentum in a practical algorithm one needs to transform the second order ODEs into ﬁrst order by deﬁning velocity variables vi(t) = ˙xi(t). Then the discrete version of Eq. equation 43 is xi[t+ 1] = xi[t] + hvi[t] vi[t+ 1] = vi[t] −h m vi[t] −h m δH δxi[t] (47) Analysing these equations through DMFT one gets a set of ﬂow equations for the following dy- namical order parameters Cx[t,t′] = ∑ ixi[t]xi[t′]/N, Cv[t,t′] = ∑ ivi[t]vi[t′]/N, Cxv[t,t′] =∑ ixi[t]vi[t]/N, Rv[t,t′] = 1 N ∑ i δvi[t] δHi[t′] , and Rx|v[t,t′] = 1 N ∑ i δxi[t] δHi[t′] ; where HHH is an instanta- neous perturbation acting on the velocity. The result of the computation gives: Cx[t+ 1,t′] = Cx[t,t′] + hCxv(t′,t) ; (48) 17Cv[t+ 1,t′] = Cv[t,t′] −h m Cv(t,t′) −µ h m Cxv(t,t′) (Cx(t,t) −1) + h2 m t∑ t′′=0 Rx|v[t,t′′]Q′′(Cx[t,t′′]) Cxv[t′′,t′] + h2 m t′ ∑ t′′=0 Q′(Cx[t,t′′]) Rv[t′,t′′]; (49) Cxv[t+ 1,t′] = Cxv[t,t′] + hCv[t,t′] ; (50) Cxv[t,t′+ 1] = Cxv[t,t′] −h m Cxv[t,t′] −µ h m Cx[t,t′] (Cx[t′,t′] −1) + h2 m t′ ∑ t′′=0 Rx|v[t′,t′′]Q′′(Cx[t′,t′′]) Cx[t,t′′] + h2 m t∑ t′′=0 Q′(Cx[t′,t′′]) Rx|v[t,t′′] (51) Rv[t+ 1,t′] = Rv[t,t′] + h m δt,t′ −µ h m Rx|v[t,t′] (Cx(t,t) −1) −h m Rv[t,t′] + h2 m t∑ t′′=0 Q′′(C[t,t′′]) Rx|v[t,t′′]Rx|v[t′′,t′] (52) Rx|v[t+ 1,t′] = Rx|v[t,t′] + hRv[t,t′]. (53) and initial conditions : Cx[0,0] = 1, Cv[0,0] = 0, Cxv[0,0] = 0, Rv[t+1,t] = 1/m, Rx|v[t+1,t] = 0. 100 101 102 103 104 iteration 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 loss =0.9 =0.7 =0.5 100 101 102 103 104 iteration 1.00 1.05 1.10 1.15 1.20radius DMFT massive DMFT HB Figure 7: Comparison of HB and with DMFT. Simulations of HB momentum in the mixed p-spin model with p= 3, ∆3 = 2/p, ∆2 = 1, ridge parameter µ= 10 and input dimension N = 1024. The parameters are α= 0.01 for all the simulations and β ∈{0.5,0.7,0.9,0.99,0.999}. We use solid line to represent the result from the simulation, the dotted line for the DMFT of massive gradient ﬂow with the mapping. We empirically observe that in this problem the value of βthat gives the best speed up is β = 0.9. In order the integrate the DMFT of massive gradient we matched the mass as described in Eq. equation 45 and consider time steps h∈{0.005,0.005,0.0125,0.25,0.25}. Derivation of the DMFT equations for massive gradient ﬂow In this section we derive the dynamical mean ﬁeld theory (DMFT) equations of massive gradient ﬂow in the mixed p-spin. We use the generating functional approach described in Castellani and Cavagna (2005); Agoritsas et al. (2018) to obtain the effective dynamical equations. First we rewrite de massive dynamics Eq. equation 43 as two ODEs m˙vvv(t) = −vvv(t) −∇H[xxx(t)], (54) ˙xxx(t) = vvv(t). (55) We use the following simple identity that takes the name of generating functional 1 = Z= ∫ D[xxx,vvv] δ(m˙vvv(t) +vvv(t) + ∇H[xxx(t)]) δ( ˙xxx(t) −vvv(t)) (56) 18= ∫ D[xxx,˜xxx,vvv,˜vvv] N∏ i=1 exp { i ∫ ˜vi(t) [m˙vi(t) + vi(t) + ∇iH[xxx(t)]] dt } exp { i ∫ ˜xi(t) [ ˙xi(t) −vi(t)] dt } (57) where in the ﬁrst line we integrate over all possible trajectories of vvvand xxx, and we impose them to match the massive gradient ﬂow equations using Dirac’s deltas. In the second line we use the Fourier representation of the delta and we absorb the normalization constants in the term D[xxx,˜xxx,vvv,˜vvv]. We can now average over the stochasticity of the problem, let us indicate with an overline the average. 1 = Z= ∫ D[xxx,˜xxx,vvv,˜vvv] N∏ i=1 exp { i ∫ ˜vi(t) [m˙vi(t) + vi(t)] dt } exp { i ∫ ˜xi(t) [ ˙xi(t) −vi(t)] dt } × (58) × N∏ i=1 exp   i ∫ ˜vi(t)   √ (p−1)! Np−1 ∑ (i,i2,...,ip) ξ(p) i,i2,...,ip xi2 (t) ...x ip (t) + 1√ N ∑ j ξ(2) i,j xj(t)  dt   × (59) × N∏ i=1 exp   i ∫ ˜vi(t)µ  1 N ∑ j x2 j(t) −1  xi(t)dt    (60) We can proceed integrating the second line over the noise. Importantly we must group all the element that multiply a given ξ(p). Considering only second line and neglecting constant multiplicative factors we obtain exp { − N 2p∆p ∫ [ p (∑ i ˜vi(t)˜vi(t′) N )(∑ i xi(t)xi(t′) N )p−1 + + p(p−1) (∑ i ˜vi(t)xi(t′) N )(∑ i xi(t)˜vi(t′) N )(∑ i xi(t)xi(t′) N )p−2 ] dt′dt } × ×exp { − N 2∆2 ∫ [(∑ i ˜vi(t)˜vi(t′) N )(∑ i xi(t)xi(t′) N ) + (∑ i ˜vi(t)xi(t′) N )(∑ i xi(t)˜vi(t′) N )] dt′dt } We deﬁne Q(x) = xp/(p∆p) + x2/(2∆2) and the order parameters Cx[t,t′] = ∑ i xi[t]xi[t′]/N, (61) Cv[t,t′] = ∑ i vi[t]vi[t′]/N, (62) Cxv[t,t′] = ∑ i xi[t]vi[t]/N, (63) Rv[t,t′] = 1 N ∑ i δvi[t] δHi[t′], (64) Rx|v[t,t′] = 1 N ∑ i δxi[t] δHi[t′]; (65) and enforce some of them using Dirac’s deltas ∫ D[Cx,C˜v,Cx˜v,C˜vx]δ  NC˜v(t,t′) − ∑ j i˜vj(t) i˜vj(t′)  δ  NCx(t,t′) − ∑ j xj(t) xj(t′)  × ×δ  NCx˜v(t,t′) − ∑ j xj(t) i˜vj(t′)  δ  NC˜vx(t,t′) − ∑ j i˜vj(t) xj(t′)  × 19×exp { − N 2∆p ∫ [ C˜v(t,t′)Q′[Cx(t,t′)] + C˜vx(t,t′)Cx˜v(t,t′)Q′′[Cx(t,t′)] ] dt′dt } . Using again the Fourier representation of the deltas and considering N large, the auxiliary variables introduced with the transform concentrate to their saddle point according to Laplace approximation. Furthermore it is easy to show Castellani and Cavagna (2005) that Cx˜v(t,t′) = C˜vx(t′,t) and Cx˜v(t,t′) = Rx|v(t,t′), with Rx|v. Under this considerations, we rewrite the average generating functional 1 = Z= ∫ D[xxx,˜xxx,vvv,˜vvv] N∏ i=1 exp { i ∫ ˜vi(t) [ m˙vi(t) + vi(t) + µ (∑ jx2 j(t) N −1 ) xi(t) ] dt } × (66) × N∏ i=1 exp { i ∫ ˜xi(t) [ ˙xi(t) −vi(t)] dt } × (67) ×exp   − ∫ ∑ j [1 2Q′[Cx(t,t′)]i˜vj(t) i˜vj(t′) −Q′′[Cx(t,t′)]Rx|v(t,t′) i˜vj(t′) xj(t) ] dt′dt   . (68) Finally we can take a Hubbard-Stratonovich transform on the ﬁrst term of the second line and identify a stochastic process Ξ(t) with zero mean at all times and covariance E[Ξ(t)Ξ(t′)] = Q′[Cx(t,t′)]. The resulting generating functional now represent the dynamics in xxxand vvvwhere the cross-element interactions are replaced by the stochastic process. The resulting effective equations are m˙vvv(t) = −vvv(t) + ∫ t 0 dt′′Rx|v(t,t′′)Q′[Cx(t,t′′)]xxx(t′′) + ΞΞΞ(t) −µ[Cx(t,t) −1]xxx(t), (69) ˙xxx(t) = vvv(t). (70) Finally we introduce the order parameters Eqs. (61-65) and compute their equation explicitly by substituting the equations of the effective dynamics. In order to do that, we use Girsanov theorem and evaluate the following expected values ⟨v(t)Ξ(t′)⟩= ∫ t′ 0 dt′′Rv(t,t′′)Q′[Cx(t,t′′)], (71) ⟨x(t)Ξ(t′)⟩= ∫ t′ 0 dt′′Rx|v(t,t′′)Q′[Cx(t,t′′)]. (72) The dynamical equations are ∂tCx(t,t′) = Cxv(t′,t) ; (73) m∂tCv(t,t′) = −Cv(t,t′) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]Cxv(t′′,t′)+ + ∫ t′ 0 Q′[Cx(t,t′′)]Rv(t′,t′′) −µCx(t,t′) (Cx(t,t′) −1) ; (74) ∂tCxv(t,t′) = Cv(t,t′) ; (75) m∂t′ Cxv(t,t′) = −Cxv(t,t′) + ∫ t′ 0 dt′′Rx|v(t′,t′′)Q′′[Cx(t′,t′′)]Cx(t,t′′)+ + ∫ t 0 Q′[Cx(t′,t′′)]Rx|v(t,t′′) −µCxv(t,t′) (Cx(t,t′) −1) ; (76) m∂tRv(t,t′) = δ(t−t′) −Rv(t,t′) + ∫ t t′ dt′′Q′′[C(t,t′′)]Rx|v(t,t′′)Rx|v(t′′,t′)+ −µRx|v(t,t′) (Cx(t,t′) −1) ; (77) 20∂tRx|v(t,t′) = Rv(t,t′). (78) and µ(t) = Cxv(t,t). The initial conditions are : Cx(t,t) = 1 ; (79) Cv(t= 0,t = 0) = 0 ; (80) Cxv(t= 0,t = 0) = 0 ; (81) Rv(t+,t+) = 1/m; (82) Rx|v(t,t) = 0 . (83) Where Eq. equation 79 comes from the spherical constraint; Eqs. (80,81) come from the initialization with no kinetic energy vvv(0) = 000; Eqs. equation 82 and equation 83 come from Eqs. equation 69 and equation 70 (respectively) after deriving by Ξ(t′), integrating on tin [t−h; t+ h] (with h→0) and taking t′→t. The equations shown in the main text are the discrete equivalent of the ones just obtained. D Hard spherical constraint It is also possible to consider a hard spherical constraint, which is the situation typically considered in the physics literature Crisanti and Sommers (1992); Cugliandolo and Kurchan (1993). A massive dynamics was already considered in Cugliandolo et al. (2017) but their derivation was in the under- damped regime where the total energy is conserved. Using that approach the conservation of the energy was key. Unfortunately the energy is not conserved in general, and in particular in the case of optimization where we aim to go down in energy in order to ﬁnd a minimum. For reference sake we consider massive gradient ﬂow, the same considerations apply straightforwardly to Nesterov acceleration. Let us write the dynamics in this case splitting the system in two ODEs m˙vvv(t) = −vvv(t) −∇L[xxx(t)], (84) ˙xxx(t) = vvv(t) −µ(t)xxx(t). (85) The last term in the second line constraints the dynamics to move in the sphere by removing the terms of the velocity that move orthogonally to the sphere. Therefore the term µ(t) is given by the projection of the velocity in the direction that is tangent to the sphere ∑ jxj(t)vj(t)/N. We can then follow the usual techniques, e.g. section C, and obtain ∂tCx(t,t′) = −µ(t)Cx(t,t′) + Cxv(t′,t) , (86) m∂tCv(t,t′) = −Cv(t,t′) + ∫ t 0 dt′′Rx|v(t,t′′)Q′′[Cx(t,t′′)]Cxv(t′′,t′) + ∫ t′ 0 Q′[Cx(t,t′′)]Rv(t′,t′′) , (87) ∂tCxv(t,t′) = −µ(t)Cxv(t,t′) + Cv(t,t′) , (88) m∂t′ Cxv(t,t′) = −Cxv(t,t′) + ∫ t′ 0 dt′′Rx|v(t′,t′′)Q′′[Cx(t′,t′′)]Cx(t,t′′) + ∫ t 0 Q′[Cx(t′,t′′)]Rx|v(t,t′′) , (89) m∂tRv(t,t′) = δ(t−t′) −Rv(t,t′) + ∫ t t′ dt′′Q′′[C(t,t′′)]Rx|v(t,t′′)Rx|v(t′′,t′) , (90) ∂tRx|v(t,t′) = −µ(t)Rx|v(t,t′) + Rv(t,t′); (91) with µ(t) = Cxv(t,t) and initial conditions Cx(t,t) = 1, Cv(0,0) = 0, Cxv(0,0) = 0, Rv(t+,t) = 1 m, Rx|v(t,t) = 0. 21",
      "meta_data": {
        "arxiv_id": "2102.11755v4",
        "authors": [
          "Stefano Sarao Mannelli",
          "Pierfrancesco Urbani"
        ],
        "published_date": "2021-02-23T15:30:57Z",
        "pdf_url": "https://arxiv.org/pdf/2102.11755v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper provides an analytical description of the average evolution of momentum-based methods, specifically Heavy-Ball momentum and Nesterov acceleration, in two prototypical high-dimensional non-convex optimization problems: the mixed p-spin model and the spiked matrix-tensor model. It derives a closed set of equations using dynamical mean field theory (DMFT) to describe their behavior in the infinite-dimensional limit. A key finding is that while these accelerated methods speed up the dynamics, they do not improve the algorithmic recovery threshold compared to vanilla gradient descent in the spiked model, meaning they reach the same quality of solution but faster.",
        "methodology": "The core methodology is Dynamical Mean Field Theory (DMFT), a technique from statistical physics, applied to describe the average dynamics of optimization algorithms. This involves deriving a set of equations for order parameters, such as dynamical two-time correlation and response functions, which encode the memory terms and statistical properties of noise in the dynamics. The derivation for Nesterov acceleration uses a dynamical cavity method to track the effect of an additional degree of freedom in perturbation theory. For Heavy-Ball momentum, an analogous derivation is presented, with a connection to second-order ODEs representing a massive particle moving under a potential. The hard constraint on the vector norm is relaxed to a soft constraint with a penalty term to simplify analysis.",
        "experimental_setup": "The research analyzes Nesterov acceleration, Polyak's Heavy-Ball momentum, and standard Gradient Descent (GD) on two non-convex models: the mixed p-spin model and the spiked matrix-tensor model. Numerical simulations were conducted to validate the DMFT equations, with input dimensions N=1024 for the mixed p-spin model and N=8192 for the spiked matrix-tensor model, utilizing a dilution technique. Comparisons between DMFT and simulations were made based on the evolution of loss, vector radius (L2 norm), and overlap with the hidden solution (for the spiked model). Algorithmic thresholds for signal recovery were estimated by observing the divergence of relaxation time, which was fitted to a power law as the noise-to-signal ratio increased.",
        "limitations": "The derivation of the DMFT equations is heuristic and not fully rigorous, relying on numerical simulations for validation. The study reveals that momentum-based methods, while speeding up convergence, do not outperform vanilla gradient descent in terms of the algorithmic recovery threshold in the analyzed spiked model. The research focuses on typical instances of the optimization problem rather than worst-case analysis. Additionally, the paper explicitly states that it does not compare these accelerated gradient methods to algorithms of a different nature, such as message passing ones. Finite size effects were also noted to be stronger in the spiked model simulations.",
        "future_research_directions": "The authors suggest extending their analytical techniques to other momentum-based algorithms, such as quasi-hyperbolic momentum (QHM) and proportional integral-derivative (PID) control algorithms. They also propose extending the analysis to 1-layer neural networks by combining their results with existing DMFT techniques. Further research could involve applying this framework to simple inference problems from a learning perspective, like the phase retrieval problem. Finally, the same questions could be explored within the context of recurrent neural networks, where DMFT approaches have already been used for gradient-based methods."
      }
    },
    {
      "title": "Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations",
      "abstract": "Tensor network (TN) is a powerful framework in machine learning, but\nselecting a good TN model, known as TN structure search (TN-SS), is a\nchallenging and computationally intensive task. The recent approach\nTNLS~\\cite{li2022permutation} showed promising results for this task, however,\nits computational efficiency is still unaffordable, requiring too many\nevaluations of the objective function. We propose TnALE, a new algorithm that\nupdates each structure-related variable alternately by local enumeration,\n\\emph{greatly} reducing the number of evaluations compared to TNLS. We\ntheoretically investigate the descent steps for TNLS and TnALE, proving that\nboth algorithms can achieve linear convergence up to a constant if a sufficient\nreduction of the objective is \\emph{reached} in each neighborhood. We also\ncompare the evaluation efficiency of TNLS and TnALE, revealing that\n$\\Omega(2^N)$ evaluations are typically required in TNLS for \\emph{reaching}\nthe objective reduction in the neighborhood, while ideally $O(N^2R)$\nevaluations are sufficient in TnALE, where $N$ denotes the tensor order and $R$\nreflects the \\emph{``low-rankness''} of the neighborhood. Experimental results\nverify that TnALE can find practically good TN-ranks and permutations with\nvastly fewer evaluations than the state-of-the-art algorithms.",
      "full_text": "Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations Chao Li 1 Junhua Zeng * 2 1 Chunmei Li * 3 4 Cesar Caiafa 5 1 Qibin Zhao 1 Abstract Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS (Li et al., 2022) showed promising results for this task. However, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a surprisingly sim- ple algorithm that updates each structure-related variable alternately by local enumeration, greatly reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both the algorithms can achieve linear convergence up to a constant if a sufficient reduction of the ob- jective is reached in each neighborhood. We fur- ther compare the evaluation efficiency of TNLS and TnALE, revealing that Ω(2K) evaluations are typically required in TNLS for reaching the objective reduction, while ideally O(KR) eval- uations are sufficient in TnALE, where K de- notes the dimension of search space andR reflects the “low-rankness” of the neighborhood. Experi- mental results verify that TnALE can find practi- cally good TN structures with vastly fewer eval- uations than the state-of-the-art algorithms. Our code is available at https://github.com/ ChaoLiAtRIKEN/TnALE. *Equal contribution 1RIKEN-AIP, Tokyo, Japan 2School of Automation, Guangdong University of Technology, Guangzhou, China 3College of Information and Communication Engineering, Harbin Engineering University, Harbin, China 4Department of Computer Science and Communications Engineering, W ASEDA University, Tokyo, Japan5Instituto Argentino de Radioastronom´ıa, CONICET CCT La Plata/CIC-PBA/UNLP, V . Elisa, ARGENTINA . Correspondence to: Qibin Zhao <qibin.zhao@riken.jp>, Chao Li <chao.li@riken.jp>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). 1. Introduction Tensor network (TN) has been widely applied to solv- ing high-dimensional problems in both machine learn- ing (Anandkumar et al., 2014; Novikov et al., 2015; Zhe et al., 2015; Glasser et al., 2019; Kossaifi et al., 2020; Miller et al., 2021; Richter et al., 2021; Malik, 2022) and quantum physics (Or ´us, 2019; Felser et al., 2021). The success of AlphaTensor (Fawzi et al., 2022) once again confirmed the usefulness of tensors in various fields. TN practitioners, on the other hand, have to face in practice challenging prob- lems associated with model selection, known as TN struc- ture search (TN-SS), for example: (1) how to determine the TN-ranks?; (2) should we prefer tensor-train (TT, Oseledets 2011), tensor-ring (TR, Zhao et al. 2016) or other TN topol- ogy?; (3) how to relate the tensor modes to core tensors of a TN (the permutation problem, Li et al. 2022), and so on. Unfortunately, some of these problems have been proven to be NP-hard (Hillar & Lim, 2013)1, and most of them suffer from the “combinatorial explosion”2 so that the brute force search is not a viable option in practice. Several works have put effort into different aspects of TN- SS (see Section 1.1), but many of the methods are restricted to specific tasks or work poorly in practice, so a general and efficient TN-SS method is needed. Recently, Li et al. (2022) proposed an algorithm dubbed TNLS, which addressed the rank and permutation selection problem for TNs. However, its computational complexity is high since it requires evalu- ating the objective function on a large number of structure candidates. To address this issue, we accelerate TNLS by equipping the algorithm with Alternating Local Enumeration — a surprisingly simple but efficient searching method in neigh- borhoods. The new algorithm, named TnALE, can improve the evaluation efficiency of TNLS greatly. To be specific, TnALE follows the “local-searching” scheme as TNLS but alternately updates each structure-related variable by enu- merating its values within a neighborhood. The intuition 1For instance, it proves that determining the optimal ranks for Tucker decomposition (Tucker, 1966) is NP-hard. 2It means the rapid growth of TN structure searching space due to the combinatorics of ranks, topologies, permutations, etc. 1 arXiv:2304.12875v3  [cs.LG]  29 May 2023TnALE: Solving Tensor Network Structure Search with Fewer Evaluations is that the alternately updating avoids the combinatorial ex- plosion and the enumeration in neighborhoods guarantees the non-increasing of values of the objective in the search. On the other hand, the original TNLS applies random sam- pling, causing the majority of samples would not provide helpful information for decreasing the value of the objective function. The theoretical advantage of TnALE is also clear. We prove that, with new-defined discrete convexity-related assump- tions of the objective function, both TNLS and TnALE can achieve a linear convergence up to a constant if a suf- ficient reduction of the objective function is reached in each neighborhood (Theorem 4.5). We also prove that the number of evaluations required in TNLS would grow ex- ponentially with the dimension of search space (Prop. 4.8), with respect to the dimension of TN-ranks and the TN or- der, while TnALE shows a linear growth in the ideal case (Prop. 4.10). Our analysis reveals that such an improvement in the evaluation efficiency essentially comes from the low- rankness of the optimization landscape in neighborhoods, attributed to the close relationship between TnALE and cross-approximation methods for matrices and tensors (Tyr- tyshnikov, 2000; Oseledets & Tyrtyshnikov, 2010; Sozykin et al., 2022). Numerically, extensive experiments on both synthetic and real-world data are implemented to assess the evaluation efficiency and the superior performance provided by TnALE. Our main contributions can be summarized as follows: • We propose TnALE, a novel algorithm that greatly reduces the computational cost for the task of TN struc- ture search (TN-SS); • We establish for the first time the convergence analy- sis for both TNLS (Li et al., 2022) and TnALE, and rigorously prove their evaluation efficiency. 1.1. Related Works Tensor network structure search (TN-SS).TN-SS can be specified into three sub-problems: (1) TN-rank selection (TN-RS) (Rai et al., 2014; Zhao et al., 2015; Yokota et al., 2016; Cheng et al., 2020; Mickelin & Karaman, 2020; Cai & Li, 2021; Hawkins & Zhang, 2021; Long et al., 2021; Sedighin et al., 2021; Yin et al., 2022; Ghadiri et al., 2023); (2) TN-topology selection (TN-TS) (Hashemizadeh et al., 2020; Haberstich et al., 2021; Nie et al., 2021; Falc´o et al., 2023; Hikihara et al., 2023; Kodryan et al., 2023; Liu et al., 2023); and (3) TN-permutation selection (TN-PS) (Acharya et al., 2022; Chen et al., 2022). Recently, some methods to solve the TN-SS problem were developed via discrete opti- mization (Hayashi et al., 2019; Hashemizadeh et al., 2020; Li & Sun, 2020; Li et al., 2021; 2022; Solgi et al., 2022). Although these methods typically achieve higher precision than their counterparts in practice, they suffer from the ex- pensive computational cost and the lack of theoretical anal- ysis. Our work follows the TNLS algorithm (Li et al., 2022) in this direction but develops a new approach to improve its computational efficiency and fill in the missing theoretical analysis for convergence and evaluation efficiency. Finding the extreme entry value within a tensor. As discussed later in Section 4.2, the alternating local enu- meration method is technically equivalent to finding the minimum entry within a multidimensional landscape. As such, our work is strongly related to the recently published method TTOpt (Sozykin et al., 2022), which finds the near- maximum entry of a tensor by cross-sampling (Tyrtysh- nikov, 2000; Zhang, 2019) in TT topology (Oseledets & Tyrtyshnikov, 2010). Compared to TTOpt, the proposed TnALE recursively finds the extreme entry within a ten- sor associated with the neighborhood rather than the global search space, so that TnALE can handle the situation of infinite candidates (entries) in the optimization. 2. Preliminaries In this section, we first summarize notations and review several central concepts related to the tensor network struc- ture search (TN-SS). Then, we provide a quick review of TNLS (Li et al., 2022), a recently proposed algorithm for solving TN-SS, and point out that TNLS suffers from the curse of dimensionality in evaluation efficiency. 2.1. Notations Throughout the paper, we typically use blackboard letters to denote sets of objects, e.g., G, F. In particular, R, Z+ and Z≥0 denote real numbers, positive integers and non- negative integers, respectively. We use boldface letters to denote vectors and matrices, e.g., x ∈ ZK + and A ∈ RI×J. For tensors of arbitrary order, we denote them using calli- graphic letters, e.g., A, B ∈RI1×I2×···×IN . Given a vector, such as x ∈ ZK + , ∥x∥ and ∥x∥∞ denote the l2-norm and l∞-norm of x, respectively. The norms are also directly ap- plied to matrices and tensors by thinking of them as vectors. Following the notational conventions, |x| denotes the abso- lute value of x if x ∈ R is a scalar, while |A| denotes the cardinality if A is a set. For the normed vector spaces armed with ∥x∥∞, we use B∞(x, rx) to denote the neighborhoods centered at x with the radius rx > 0. For any two functions f : B → C and g : A → B, the operation f ◦ g : A → C denotes the function composition. 2.2. Tensor Network Structure Search (TN-SS) We consider the tensor network (TN, Ye & Lim, 2019) as a set of real tensors of the dimension I1 × I2 × ··· ×IN , denoted T NS(G, r), whose elements are in the form of 2TnALE: Solving Tensor Network Structure Search with Fewer Evaluations contractions of core tensors (Cichocki et al., 2017), associ- ated to the TN structure modeled by the pair (G, r), where G = (V, E) denotes a simple graph of N vertices modeling the TN-topology (Li & Sun, 2020) and r ∈ Z|E| + is a vector, whose entries are edge labels of G corresponding to the TN-ranks (Ye & Lim, 2019). Tensor network structure search (TN-SS) aims generally to find the most compressed TN models for computa- tional purposes while preserving the models’ expressiv- ity. Furthermore, the most compressed TN models also imply the potential advantage for the generalization ca- pability in learning tasks (Khavari & Rabusseau, 2021). Suppose a dataset D and the task-specific loss function πD : RI1×I2×···×IN → R+ involving D. TN-SS is to solve the following bi-level discrete optimization problem min (G,r)∈G×FG \u0012 ϕ(G, r) + λ · min Z∈TNS (G,r) πD(Z) \u0013 , (1) where G ∈ G is a graph of N vertices and K edges, r ∈ FG ⊆ ZK + , ϕ : G × ZK + → R+ represents the function measuring the model complexity of a TN whose structure is modeled by (G, r), and λ >0 is a tuning parameter. The intuition of (1) is that, the inner minimization is to evaluate the task-specific expressivity for a TN structure, while the outer minimization is to find the optimal structure for the task by balancing the complexity and the expressivity of a TN model. We remark that the formulation (1) can be specified as different TN-SS sub-problems by restricting the feasible set G × FG into different forms: for TN-PS, we specify FG = ZK + and G to be the set containing the isomorphic graphs to a “template” graph (Li et al., 2022); for TN-RS, it typically restricts G to be fixed, and only finds TN-ranksi.e., FG = ZK + ; last for TN-TS, it relaxes G to be the set con- taining all possible simple graphs of N vertices and r is set to be fixed (Li & Sun, 2020) or searchable (Hashemizadeh et al., 2020). Note from Ye & Lim (2019); Li & Zhao (2021) that TN-TS (with rank selection) essentially coincides with TN-RS of a “fully-connected” TN (Zheng et al., 2021). 2.3. TNLS: a Discrete Optimization Approach to TN-PS Recently, Li et al. (2022) proposed an algorithm called TNLS for solving (1) effectively by stochastic search. The core process of TNLS is reviewed in Alg. 1, from which we see that the candidate of the optimal TN structure is updated if the algorithm finds better structures within a neighborhood using random sampling. Although Li et al. (2022) illustrates the superiority of TNLS compared to its counterparts, the algorithm is still time-consuming as acknowledged in their work. To understand the reason, we theoretically observe that TNLS suffers from the curse of dimensionality, due to the random sampling. More specifically, we state that Algorithm 1 The core process of TNLS (Li et al., 2022). Initialize: Randomly choose a TN structure as the center of the neighborhood. while not convergence do 1. Sampling (G, r)’s randomly in the neighborhood; 2. Evaluating the samples with the objective of (1); 3. Updating the center if better samples are obtained; end while Output: The center of the neighborhood. under reasonable conditions, Ω(2K/ϵ) samples are re- quired by random sampling (wrt. step 1 in Alg. 1) for achieving a constant probability P r≥ ϵ for decreasing the objective of (1) in a neighborhood, where ϵ >0 and K denotes the dimension of the search space. A formal statement is deferred to Prop. 4.8. It is known from Alg. 1 that each sampled structure should be evaluated by solving the inner minimization of(1), so the huge number of samples implies the prohibitive cost in computation. To ad- dress this problem, we introduce a more sampling-efficient approach by reforming the random sampling in Alg. 1, to accelerate the TN-SS procedure with fewer evaluations. 3. TnALE: Accelerating TNLS via Alternating Local Enumeration We present now the new searching algorithm dubbedTnALE for solving the TN-SS problem. Figure 1 demonstrates the key idea for TnALE. In the rest of this section, we mainly focus on the technical details of alternating local enumera- tion (ALE), which replaces the random sampling operation in Alg. 1 as the key factor for algorithm acceleration. A complete introduction of TnALE, including the pseudocode and other details, is given in Appendix A. In TnALE, we find better structures within a neighborhood by updating each structure-related variable alternately. For instance, Figure 2 illustrates how ALE solves the TN-PS problem, searching for the optimal ranks and permutations for tensor ring (TR) decomposition of order-4. As shown in the initialization of panel (b), all structure-related variables, including r{1,2,3,4} and G, are initialized with the center of a given neighborhood. To start the search, TN structures are sampled by enumerating all values of r1 within the neigh- borhood while fixing other variables. Next, the sampled TN structures with varying r1 are evaluated individually by cal- culating the objective of (1), and r1 is subsequently updated right off by choosing the one with the minimum objective in the samples. Following r1, the same operation is applied to variables from r2 to G sequentially (see panel (b)). After updating G, we turn the updating direction backward from r4 to r1, i.e., in a “round-trip” manner (see panel (c)). Over- all, the ALE will be stopped if all variables are not changed 3TnALE: Solving Tensor Network Structure Search with Fewer Evaluations /gid00021/gid00035/gid00032/gid00001/gid00175 /gid00039/gid00042/gid00030/gid00028/gid00039/gid00001/gid00046/gid00032/gid00028/gid00045/gid00030/gid00035/gid00176/gid00001/gid00046/gid00030/gid00035/gid00032/gid00040/gid00032/gid00001/gid00033/gid00042/gid00045/gid00001/gid00021/gid00015/gid00183/gid00020/gid00020 /gid00019/gid00028/gid00041/gid00031/gid00042/gid00040/gid00001/gid00020/gid00028/gid00040/gid00043/gid00039/gid00036/gid00041/gid00034 /gid00187/gid00013/gid00036/gid00001/gid00032/gid00047/gid00001/gid00028/gid00039/gid00163/gid00164/gid00001/gid00133/gid00131/gid00133/gid00133/gid00188 /gid00002/gid00039/gid00047/gid00032/gid00045/gid00041/gid00028/gid00047/gid00036/gid00041/gid00034/gid00001/gid00006/gid00041/gid00048/gid00040/gid00032/gid00045/gid00028/gid00047/gid00028/gid00036/gid00042/gid00041 /gid00047/gid00035/gid00032/gid00001/gid00042/gid00043/gid00047/gid00036/gid00040/gid00028/gid00039/gid00001/gid00043/gid00042/gid00036/gid00041/gid00047 /gid00132/gid00163/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00001/gid00045 /gid00133/gid00163/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00001/gid00008 /gid00134/gid00163/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00001/gid00045 /gid00135/gid00163/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00001/gid00008 Figure 1. Schematic demonstration of TnALE and its discrepancy from TNLS (Li et al., 2022), where r, Gdenote two structure- related variables for example, and the squares represent the neigh- borhoods. The alternating (local) enumeration is further illustrated in detail in Figure 2. anymore. We empirically find that aone-time “round-trip” is sufficient to reach a good TN structure for the next iteration. Note that the operation of ALE for TN-RS and TN-TS is in the same fashion, except that the graph G will be fixed in TN-RS or enumerated in differently-defined neighborhoods in TN-TS for considering all TN-topologies. In TnALE, we follow Li et al. (2022) to construct the neighborhood of TN structures. Remark 3.1 (tricks: knowledge transfer). An additional merit of implementing enumeration is the“knowledge trans- fer” capability (Hashemizadeh et al., 2020). We know that increasing the TN-ranks would decrease monotonically the value of the objective function in many learning tasks. In- stead of evaluating each structure explicitly, it thus inspires us to accelerate the structure evaluation by reusing in enu- meration the knowledge of the well-optimized core tensors and their corresponding objective. In particular, the accelera- tion by the knowledge transfer trick is two-fold: one is to use the well-optimized core tensors associated with the lower- rank structures to be the initialization for the ones with higher-rank structures, as in Hashemizadeh et al. (2020); the other is to employ the objective estimation, in which we apply linear interpolation to predicting the objective in the evaluation phase in place of the explicit calculation (see Appendix A.1). Although the objective estimation would be of no precision, it helps in the first 1 ∼ 2 iterations of TnALE (with a large radius) as initialization for quickly finding a reasonable structure candidate. Remark 3.2 ( computational complexity ). TnALE is a meta-algorithm for TN-SS, in which the inner minimization of (1) can be solved by any practitioner-appointed algo- rithms. Therefore, the computational complexity of TnALE is mainly affected by the number of evaluations. Suppose the searching problem of order- N with the TN-ranks of dimension K. Furthermore, suppose each entry ri, i∈ [K] of r is enumerated in I values in the neighborhood, e.g., the interval [ri −I/2, ri +I/2], and D times of the “round-trip” /gid00004/gid00032/gid00041/gid00047/gid00032/gid00045 /gid00131/gid00163/gid00001/gid00010/gid00041/gid00036/gid00047/gid00036/gid00028/gid00039/gid00036/gid00053/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00041/gid00032/gid00036/gid00034/gid00035/gid00029/gid00042/gid00045/gid00035/gid00042/gid00042/gid00031  /gid00007/gid00036/gid00051/gid00032/gid00031 /gid00032/gid00041/gid00048/gid00040/gid00032/gid00045/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00007/gid00036/gid00051/gid00032/gid00031 /gid00133/gid00163/gid00001/gid00022/gid00043/gid00031/gid00028/gid00047/gid00036/gid00041/gid00034 /gid00132/gid00163/gid00001/gid00022/gid00043/gid00031/gid00028/gid00047/gid00036/gid00041/gid00034  /gid00007/gid00036/gid00051/gid00032/gid00031 /gid00032/gid00041/gid00048/gid00040/gid00032/gid00045/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00007/gid00036/gid00051/gid00032/gid00031 /gid00136/gid00163/gid00001/gid00022/gid00043/gid00031/gid00028/gid00047/gid00036/gid00041/gid00034 /gid00032/gid00041/gid00048/gid00040/gid00032/gid00045/gid00028/gid00047/gid00036/gid00042/gid00041 /gid00187 /gid00030 /gid00188/gid00001/gid00001/gid00021/gid00035/gid00032/gid00001/gid00175/gid00045/gid00042/gid00048/gid00041/gid00031/gid00183/gid00047/gid00045/gid00036/gid00043/gid00176/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00036/gid00041/gid00034/gid00001/gid00042/gid00045/gid00031/gid00032/gid00045/gid00163 /gid00007/gid00042/gid00045/gid00050/gid00028/gid00045/gid00031 /gid00003/gid00028/gid00030/gid00038/gid00050/gid00028/gid00045/gid00031 /gid00187 /gid00028 /gid00188/gid00001/gid00021/gid00019/gid00001/gid00031/gid00032/gid00030/gid00042/gid00040/gid00043/gid00042/gid00046/gid00047/gid00036/gid00042/gid00041/gid00001/gid00028/gid00041/gid00031/gid00001 /gid00046/gid00047/gid00045/gid00048/gid00030/gid00047/gid00048/gid00045/gid00032/gid00183/gid00045/gid00032/gid00039/gid00028/gid00047/gid00032/gid00031/gid00001/gid00049/gid00028/gid00045/gid00036/gid00028/gid00029/gid00039/gid00032/gid00046/gid00163 /gid00002 /gid00003 /gid00004 /gid00005 /gid00002 /gid00003 /gid00005 /gid00004 /gid00002 /gid00003 /gid00004/gid00005 /gid00002 /gid00003/gid00004 /gid00005 /gid00164 /gid00164 /gid00187/gid00029 /gid00188/gid00001/gid00002/gid00013/gid00006/gid00165/gid00001/gid00022/gid00043/gid00031/gid00028/gid00047/gid00036/gid00041/gid00034/gid00001/gid00046/gid00047/gid00045/gid00048/gid00030/gid00047/gid00048/gid00045/gid00032/gid00183/gid00045/gid00032/gid00039/gid00028/gid00047/gid00032/gid00031/gid00001/gid00049/gid00028/gid00045/gid00036/gid00028/gid00029/gid00039/gid00032/gid00046/gid00001/gid00028/gid00039/gid00047/gid00032/gid00045/gid00041/gid00028/gid00047/gid00032/gid00039/gid00052/gid00163 /gid00164/gid00002 /gid00003 /gid00004 /gid00005 /gid00046/gid00047/gid00045/gid00048/gid00030/gid00047/gid00048/gid00045/gid00032/gid00183/gid00045/gid00032/gid00039/gid00028/gid00047/gid00032/gid00031/gid00001/gid00049/gid00028/gid00045/gid00036/gid00028/gid00029/gid00039/gid00032/gid00046 Figure 2.Illustration of alternating local enumeration (ALE) for TN-PS of TR decomposition. The search for TN-RS and TN-TS is similar, except that the ranges of the TN-ranks r{1,2,3,4} and the graph G need to be adjusted. updates. In this setting, for the TN-RS problem, TnALE requires O(DKI ) evaluations in one neighborhood; for TN-PS, it requires O(DKI + DN2/2) evaluations since the neighborhood of G in TN-PS contains (N − 1)N/2 ele- ments (Li et al., 2022) in general; last for TN-TS,O(DN2I) evaluations are required. In practice, the value of I is typi- cally set to 3 or 5 and D = 1. In summary, the evaluation complexity of TnALE grows polynomially with the TN-order and the dimension of the TN-ranks. In the next section, we prove that such the num- ber of evaluations is theoretically sufficient in TnALE for achieving quick convergence for TN-SS. 4. Theoretical Results In this section, we first analyze the descent steps for both TNLS and TnALE, proving that using the “local search” (Li et al., 2022) scheme the algorithms would achieve a linear convergence rate up to a constant if the objective is suffi- ciently “convex” in the discrete domain. Following this, we analyze the evaluation efficiency for TNLS and TnALE, showing that the required number of evaluations in TNLS would grow exponentially with the dimension of the search space, while it can be ideally reduced to be a linear growth in TnALE if the neighborhood is low-rank. The proofs in this section are given in Appendix B. 4.1. Analysis of Descent Steps We start the analysis by rewriting (1) into a more general form min x∈ZK + ,p∈P fp(x) := f ◦ p(x), (2) 4TnALE: Solving Tensor Network Structure Search with Fewer Evaluations where f : ZL ≥0 → R+ is a general form of the objective function of (1), x ∈ ZK + corresponds to the TN-ranks r of (1), and the operator p : ZK + → ZL ≥0 and its feasible set P correspond to the graph variable G and its feasible set G of (1), respectively. The specific relationship of p and G is discussed in Appendix B.2. The framework used in the proof follows from Golovin et al. (2019) for the zeroth-order convex optimization. However, due to the discrete essence of (2), we re-establish discrete analogues of the fundamental concepts such as the gradient, strong convexity and smoothness for the analysis, and all the proofs are re-derived non-trivially in the discrete domain. In doing so, we begin with the definition of the finite gradi- ent (Olver, 2014), as the alternative to the classic gradient in the continuous domain. Definition 4.1 (finite gradient ). For any function f : ZL ≥0 → R, its finite gradient ∆f : ZL ≥0 → RL with re- spect to x ∈ ZL ≥0 is defined as follows: ∆f(x) = [f(x + e1) − f(x), . . . , f(x + eL) − f(x)]⊤ , (3) where ei, 1 ≤ i ≤ L denote the unit vectors with the i-th entry being one and the others being zeros. Next, we redefine the convexity and smoothness of the ob- jective with finite gradients. Definition 4.2 (α-strong convexity with finite gradient). We say f is α-strongly convex for α ≥ 0 if f(y) ≥ f(x) + ∆f(x) − α 2 1, y − x \u000b + α 2 ∥y − x∥2 for all x, y ∈ ZL ≥0, where 1 ∈ RL denotes the vector with all entries being one. We simply say f is convex if it is α-strongly convex and α = 0. Definition 4.3 ((β1, β2)-smoothness with finite gradient). We say f is (β1, β2)-smooth for β1, β2 > 0 if 1. |f(x) − f(y)| ≤β1∥x − y∥ for all x, y ∈ ZL ≥0; 2. The function l(x) := β2 2 ∥x∥2 − f(x) is convex. We remark that Definition 4.2 and 4.3 are partially differ- ent from the ones used in Golovin et al. (2019) or other literature for convex analysis. Particularly in Definition 4.3, the smoothness is defined by additionally taking the Lip- schitz continuity (corresponding to Item 1) into account, which controls the changing rate of f, while Item 2 in Defi- nition 4.3 controls the changing rate of the finite gradient of f. See Lemma B.5 in Appendix for the discussion. With the new definitions, we next give the main assumptions used in the results. Assumption 4.4. Assume that f : ZL ≥0 → R+ of (2) is α-strongly convex, (β1, β2)-smooth, and its mini- mum, denoted (p∗, x∗) = arg min p,x f ◦ p(x), satisfies ∥∆fp∗(x∗)− β2 2 1∥ ≤γ where 0 ≤ γ < α≤ β1 ≤ β2 ≤ 1. In Assumption 4.4, the inequality ∥∆fp∗(x∗) − β2 2 1∥ ≤γ implies that, up to a (small) bias β2 2 1, the finite gradient at (p∗, x∗) should be sufficiently small, which can be analo- gous to the “gradient-equal-zero” (Boyd & Vandenberghe, 2004) property of the stationary points for convex functions in the continuous domain. The upper bound “1” is arbitrarily chosen just for simplifying the calculation. Next, we reveal that the local-search scheme, used in both TNLS and TnALE, can achieve the linear convergence rate up to a constant. We first focus on TN-RS and TN-TS to simplify the problem, where p∗ is assumed to be known beforehand. After that, we discuss TN-PS, showing that the searchable p would make the convergence more difficult. Theorem 4.5 (convergence rate when p∗ is known). Sup- pose Assumption 4.4 is satisfied, the operator p of (2) is fixed to be p∗, and 0 ≤ θ ≤ 1. Then, for any x with ∥x − x∗∥∞ ≤ c, we can find a neighborhood B∞(x, rx) where rx ≥ θc + 1 2 , such that there exists an element y ∈ B∞(x, rx) satisfying fp∗(y) − fp∗(x∗) ≤ (1 − θ)(fp∗(x) − fp∗(x∗)) + 7 8K. (4) Proving Theorem 4.5 requires the following lemma, which can be understood as the discrete version of the convex- combination inequality of convex functions. Lemma 4.6 (convex combination in the discrete domain). Suppose q = θx + (1 − θ)y, ∀x, y ∈ ZL ≥0, θ∈ [0, 1], and there is ˆq ∈ ZL ≥0 following Λ = q − ˆq. If f is α-strongly convex, then θf(x)+(1−θ)f(y) ≥ f(ˆq)+ D ∆f(ˆq) − α 2 1, Λ E +α 2 ∥Λ∥2. (5) Note that the inequality (5) would be the same as the crucial inequality θf(x) + (1 − θ)f(y) ≥ f(q) in convex analy- sis if Λ = 0 . However, due to q /∈ ZL ≥0 in general, the non-zero Λ is inevitable in the proof, yielding the essential difference from the convex analysis in the continuous do- main. As a consequence, the inequality (4) shows that the convergence rate is formally close to being linear, but the constant (7/8)K appears on the right-hand side dampening the search efficiency. Suppose the search trajectory {fp∗(xn)}∞ n=0, of which the starting point x0 ∈ ZK + is randomly chosen and xn for n > 0 are determined by the vector y in Theorem 4.5. As an important corollary, it can be easily proved that {fp∗(xn)}∞ n=0 converges to fp∗(x∗) up to a constant if Ω(1/K) ≤ θ ≤ 1. A rigorous proof for the convergence guarantee can be found in Appendix B.12. 5TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Remark 4.7 (Finding p∗ makes the convergence slower.). As aforementioned, the non-zero ∥Λ∥∞ decreases the search efficiency due to the additional constant shown in(4). It is known from the proof of Theorem 4.5 that the con- stant is derived from the tight bound ∥Λ∥∞ ≤ 1/2, fol- lowing the rounding operation. However, once the p in (2) is searchable as well, ∥Λ∥∞ would turn larger, dampen- ing the convergence more seriously. To verify this, sup- pose q = θp∗(x∗) + (1 − θ)px(x) to be the convex com- bination between (p∗, x∗) and any point (px, x). It is known from the proof that, for decreasing the objective, ˆq should satisfy ˆq ∈ B∞(q, ∥Λ∥∞) ∩ B(px, x), where B(px, x) := {z = ¯p(¯x) : ¯p ∈ B(px), ¯x ∈ B∞(x, rx)} for some rx > 0 and B(px) denotes the neighborhood of px chosen in the algorithm. We thus see that, for the existence of ˆq, the intersection B∞(q, ∥Λ∥∞) ∩ B(px, x) should be non-empty. Following this, it satisfies ∥Λ∥∞ ≥ min¯q∈B(px,x) ∥q − ¯x∥∞ ≥ minˆq∈ZL ≥0 ∥q − ˆq∥∞ = 1/2 in the worst case. In this case, the larger value of ∥Λ∥∞ means a larger damping term appearing on the right-hand side of (4). 4.2. Evaluation Efficiency Note that a premise for achieving the closely linear con- vergence rate by TNLS and TnALE is that the expected y ∈ B∞(x, rx) in Theorem 4.5 is reachable, meaning that the algorithms should find the y out in each neighborhood. In the rest of the section, we show that TNLS is required to cost Ω(2K) samples in each neighborhood for stably reach- ing the y, while O(KIR ) samples are ideally sufficient for TnALE. Here K denotes the dimension of the search space, I ∈ Z+ indicates an integer related to the radius of the neighborhood and R ∈ Z+ reflects the low-rankness of the optimization landscape in neighborhoods. We first give the proposition for TNLS as follows. Proposition 4.8 (curse of dimensionality for TNLS). Let the assumptions in Theorem 4.5 be satisfied. Furthermore, assume that x∗ is sufficiently smaller (or larger) than x entry-wisely, except for a constant number of entries. Then the probability of achieving a suitable y as mentioned in Theorem 4.5 by uniformly randomly sampling in B∞(x, rx) with rx ≥ θc + 1 2 equals O(2−K). Note that the additional assumption in Prop. 4.8 is com- monly satisfied in practice when the searched TN-ranks are initialized uniformly with large (or small) values. It is also easily known from Prop. 4.8 that Ω(2K/ϵ) samples are re- quired for achieving the probability P r≥ ϵ for reaching the y in the neighborhood. Remark 4.9. The intuition of Prop. 4.8 is as follows. Sup- pose x∗ is entry-wisely smaller than x without loss of gener- ality, then the objective would be decreased only if most of the entries of x are updated to be smaller values,i.e., getting /gid00132 /gid00133 /gid00132 /gid00133 Figure 3.The relationship between alternating local enumeration (ALE) and TT-cross approximation (Oseledets & Tyrtyshnikov, 2010; Sozykin et al., 2022). As shown in the figure, enumerating structure-related variables alternately is equivalent to sampling fibers of a tensor along each mode. The yellow arrows indicate the alternation of variables from r1 to r2 and then to G, respectively. closer to x∗. However, by random sampling, the probability of decreasing most entries of x would turn smaller exponen- tially with increasing the dimension K, yielding the curse of dimensionality for TNLS. In contrast to TNLS, TnALE essentially resolves the curse of dimensionality by leveraging the landscape’s low-rank structure. To verify this, given (p, x), we formulate first the neighborhood B(p) × B∞(x, rx) as a (K + 1)-th order tensor B ∈RI1×I2×···×IK+1 . Here Ik = 2 × ⌈rx⌉ + 1 for 1 ≤ k ≤ K and IK+1 = |B(p)|. The (i1, i2, . . . , iK+1)- th entry of B, written B(i) with i = [ i1, i2, . . . , iK+1]⊤, satisfies B(i) = 1/fpiK+1 (x + i(: K) − (⌈rx⌉ + 1)), (6) where i(: K) denotes the K-dimensional vector consisting of the first K entries of i, and piK+1 denotes the iK+1-th element of B(p) in any ordering fashion. We remark that the inverse 1/f(x) is always valid due to the assumption fp(x) > 0 in (2) for all x and p. We also remark that the equation (6) maps uniquely each element of B(p) × B∞(x, rx) onto the entries of B. By the tensor B, we can find that the proposed alternating local enumeration (ALE) is strongly related to TT-cross (Os- eledets & Tyrtyshnikov, 2010) and TTOpt (Sozykin et al., 2022). As demonstrated in Figure 3, the enumeration for each variable is equivalent to drawing a fiber of B as in TT-cross or TTOpt with the TT-ranks being ones. Such a relationship helps us reveal the evaluation advantage of TnALE. Specifically, let B := B(p) × B∞(x, rx) and f∗ B := min (py,y)∈B fpy (y) for notational simplicity, then we have the following proposition. Proposition 4.10 (evaluation efficiency for TnALE). Let B ∈RI1×I2×···×IK+1 be the tensor of order-(K + 1) con- structed as Eq. (6) with I1 = I2 = ··· = IK+1 = I for simplicity. Then, there exists its TT-cross approxima- tion (Oseledets & Tyrtyshnikov, 2010) of rank-R3, denoted 3Here all elements of the TT-ranks equal R for simplicity. 6TnALE: Solving Tensor Network Structure Search with Fewer Evaluations ˆB, such that f∗ B = fpjK+1 (x + j(: K) − (⌈rx⌉ + 1)) with j = arg maxi ˆB(i) holds, provided that f∗ B ≤ fpz (z)/ \u0012 1 + 2(4R)⌈log2 K⌉ − 1 4R − 1 (R + 1)2ξfpz (z) \u0013 (7) for all (pz, z) ∈ B and fpz (z) ̸= f∗ B. Here, ξ denotes the error between B and its best approximation of TT-ranksR in terms of ∥ · ∥∞. Note that the inequality (7) holds trivially if B is exactly of the TT topology of rank-R, and Oseledets & Tyrtyshnikov (2010) shows that the f∗ B can be recovered from O(KIR ) entries from B. Prop. 4.10 is a natural corollary of Theorem 2 in Osinsky (2019). It implies that the desired y (corresponding to f∗ B in Prop. 4.10) in Theorem 4.5 is reachable by only O(KIR ) samples once B is of TT with rank-R. Even though B is not low-rank, the y can still be located if the inequality (7) is satisfied. Prop. 4.10 shows the O(KIR ) evaluation advantage com- pared with TNLS that requires Ω(2K/ϵ) evaluations in the neighborhoods, but it remains open to prove the low- rankness of the optimization landscape in the TN-SS tasks. We empirically verify this with five synthetic tensors of or- der four. We calculate their complete optimization landscape associated with the l2 loss, observing that the multidimen- sional landscape is indeed low-rank under all possible un- foldings (see Figure 6 in Appendix). We thus conjecture that in practice the low-rank structure of the landscape should be preserved, at least in neighborhoods. In the next section, the evaluation advantage by TnALE will be empirically verified with both synthetic and real-world data. 5. Experimental Results In this section, we present numerical results to verify the superiority of TnALE in terms of evaluation cost. Due to the page limit, the experimental settings will be presented at the minimum level of clarity. Additional details are given in Appendix C. 5.1. Synthetic Data First of all, we assess the superiority of TnALE by solving the TN-PS problem, in which both the optimal TN-ranks and permutations of synthetic tensors are searched for the tensor decomposition task. In the experiment, we re-use from Li et al. (2022) the syn- thetic tensors, which are randomly generated in the topolo- gies including TR (order-8), PEPS (order-6, Verstraete & Cirac, 2004), hierarchical Tucker (HT of order- 6, Hack- busch & K ¨uhn, 2009), and MERA (order-8, Cincio et al., 2008). Additionally, we also consider the tensor wheel model (TW of order-5, Wu et al., 2022). Since the mode di- Table 1. Number of evaluations, denoted “ #Eva.”, for the rank and permutation identification, where the symbol “-” in the table means the failure of the approach. Topology Methods Data – #Eva. ↓ A B C D TR TNGA 2850 2250 3900 1950 TNLS 1020 960 1320 780 Ours 231 308 308 231 PEPS TNGA 1560 - 840 1080 TNLS 781 781 421 661 Ours 407 465 233 175 HT TNGA 960 1320 840 1080 TNLS 841 841 781 721 Ours 211 281 211 211 MERA TNGA - 960 2800 3240 TNLS 1561 841 1441 721 Ours 1450 484 323 323 TW TNGA+ 1920 1440 600 720 TNLS 661 601 601 481 Ours 285 143 285 214 mension is typically irrelevant to the difficulty of TN-SS, we set them to equalling 3 in all tensors for simplicity. For each topology, four tensors (A, B, C, D ) are generated, where the TN-ranks and permutations are randomly selected and remained unknown. The goal of this experiment is to com- pare different approaches to identifying the TN-ranks and permutations for each tensor, meaning that the conditions RSE≤ 10−4 and the Eff.≥ 1 are satisfied4. Otherwise, we say the approach fails in the experiment. We implement three algorithms, TNGA (Li & Sun, 2020), TNLS and TnALE (ours). Note that in TNGA both the TN ranks rank permutations are encoded as chromosomes, as implemented in the work by Li et al. (2022). In the sub- sequent experiments, the the encoding scheme of TNGA would be also properly adjusted for handling different sub- problems of TN-SS. For a fair comparison, the three ap- proaches use the same objective and solver for the inner minimization of (1). Furthermore, TNLS and TnALE are initialized with the same TN structures. The rest of the experimental settings remain as Li et al. (2022). The experimental results are shown in Table 1. We see that both TNLS and TnALE (ours) successfully identify the ranks and permutations for all tensors. Furthermore, TnALE requires significantly fewer evaluations than both TNGA and TNLS. Figure 4 further illustrates the change of the objective 4RSE means the relative squared error, and the Eff. index (Li & Sun, 2020) denotes the ratio of the parameter number of TNs between the searched structure and the one used in data generation. Eff.≥ 1 implies that the algorithm finds a TN structure identical or more compact than the one used in data generation. 7TnALE: Solving Tensor Network Structure Search with Fewer Evaluations 0 500 1000Evaluations-4-202 0 200 400 600 800Evaluations-2 0 2 0 200 400 600 800Evaluations-2 0 2 0 500 1000 1500Evaluations -202 0 200 400 600 800Evaluations -1 0 1 0 200 400Evaluations-0.500.51 0 200 400 600 800Evaluations -2-101 TNLSTnALE logObjectivelogObjective TR (order-4) TR (order-6) TR (order-8) PEPS HT MERATW Figure 4.Averaged objective (in the log form) with varying the number of evaluations. values (in log, averaged) versus the number of evaluations in TNLS and TnALE. The result confirms the consistency of the evaluation advantage of TnALE compared with TNLS5. Next, we evaluate the performance of TnALE for solving the classic rank selection problem, i.e., TN-RS, within the TR decomposition task. To be specific, we randomly gen- erate synthetic TR-tensors of order 8, and consider two configurations: “lower-ranks” and “higher-ranks”. In the “lower-ranks” class, the TN-ranks are randomly chosen in the interval [1, 4], while in the “higher-ranks” class the se- lection interval is lifted to [5, 8], so that the ranks would be larger than the tensors’ mode dimension (which equals 3 in this experiment). This situation commonly happens in practice for high-order TNs. For comparison, we implement various rank-adaptive TR decomposition methods, including TR-SVD and TR-BALS (Zhao et al., 2016), TR-LM (Mick- elin & Karaman, 2020), and TRAR (Sedighin et al., 2021). In addition, the TTOpt algorithm (Sozykin et al., 2022) with ranks 6, denoted R, equaling {1, 2} is also employed as a baseline. The experimental results are shown in Table 2. We see that most of the methods can successfully identify the TN- ranks (implied by Eff.≥ 1) in the “lower-ranks” class, but in the “higher-ranks” class only the methods at the bottom of the table manage to find the correct ranks. Furthermore, TnALE costs the fewest evaluations on average compared with TNGA, TNLS and TTOpt. 5.2. Real-World Data We apply now the proposed method to real-world data to compress the learnable parameters of the tensorial Gaussian process (TGP, Izmailov et al. 2018) and to compress natural images. In TGP compression, we consider the regression task by TGPs for three datasets, including CCPP (T¨ufekci, 2014), MG (Flake & Lawrence, 2002), and Protein (Dua 5Note that the curves for MERA exhibit the opposite pattern compared to others due to the “local-convergence” issue. This phenomenon is further discussed in Appendix C. 6Here the ranks are the tuning parameters in the TTOpt algo- rithm, rather than the targeted TN structure. Table 2.Experimental results of TN-RS (rank-selection) in 8th- order TR topology. The columns of “lower-ranks” and “higher- ranks” indicate two experimental settings by which the TN-ranks are randomly selected. The Eff. and [#Eva.] values are averaged in five tensors. Methods lower-ranks higher-ranks Eff.↑ Eff.↑ TR-SVD 0.65±0.46 0.13 ±0.20 TR-BALS 1.15±0.14 0.19 ±0.22 TR-LM 1.15±0.14 0.15 ±0.02 TRAR 0.55±0.10 0.63 ±0.06 Eff.↑ [#Eva.↓] Eff. ↑ [#Eva.↓] TNGA 1.08±0.06 [552] 1.00 ±0.00 [900] TNLS 1.08±0.06 [492] 1.00 ±0.00 [588] TTOpt (R = 1) 1.08 ±0.06 [104] 1.00 ±0.00 [178] TTOpt (R = 2) 1.02 ±0.02 [314] 1.00 ±0.00 [752] Ours 1.08±0.06 [80] 1.00 ±0.00 [119] Table 3.Number of parameters ( ×1000, ↓) and MSE (in round brackets) for TGP model compression, where the values in [square brackets] show the number of evaluations required in each method. CCPP MG Protein TGP 2.64 (0.06) [N/A] 3.36 (0.33) [N/A] 2.88 (0.74) [N/A] TNGA 2.24 (0.06) [1500] 3.01 (0.33) [4900] 2.03 (0.74) [3900] TNLS 2.24 (0.06) [1051] 3.01 (0.33) [3901] 1.88 (0.74) [3601] Ours 2.24 (0.06) [124] 3.01 (0.33) [276] 1.88 (0.74) [1053] & Graff, 2017), and compress the variational mean of the process with the TT/TR decomposition using the same set- tings as in Li et al. (2022). The goal of the experiment is to search for good TN-ranks and permutations, so that fewer parameters can be used to achieve the same mean square error (MSE) in regression. The experimental results are shown in Table 3. We can see that TnALE achieves the same compression ratio as TNGA and TNLS but costs sig- nificantly fewer evaluations than the counterparts in factor up to 14 (3901/276). Last, we consider the TN-PS and TN-TS tasks for com- pressing natural images. In TN-TS, we search for good TN-ranks and topologies for image compression. In the ex- periment, we randomly select four images (A, B, C, D, see Figure 5) from the dataset BSD500 (Arbelaez et al., 2010). Each image is resized by 256 × 256 and then reshaped into an order-8 tensor. For comparison, we also implement the “Greedy” method (Hashemizadeh et al., 2020) in the TN-TS task. Table 4 shows the results, including the compression ratio, RSE, and the number of evaluations in both tasks. We see that TnALE achieves the closest compression ratio and RSE to TNGA and TNLS, but it requires much fewer evaluations. 8TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Figure 5.Four images in the compression experiment. Table 4.Results for natural image compression. The underlined values show the best compression ratio achieved in the same RSE. Tasks Methods Data - compression ratio (log, ↑) (RSE ↓) [#Eva. ↓] A B C D TN-PS TNGA 1.10 (0.15) [8400] 1.37 (0.17) [6300] 1.77 (0.08) [4800] 1.47 (0.10) [5100] TNLS 1.09 (0.16) [1351] 1.41 (0.17) [1501] 1.71 (0.08) [2551] 1.47 (0.10) [2101] Ours 1.14 (0.16) [647] 1.39 (0.17) [666] 1.80 (0.08) [394] 1.49 (0.10) [444] TN-TS Greedy 0.81 (0.16) 0.97 (0.17) 1.44 (0.08) 0.68 (0.10) TNGA 1.16 (0.16) [2100] 1.48 (0.17) [1800] 1.81 (0.08) [1900] 1.48 (0.09) [1000] TNLS 1.15 (0.16) [1300] 1.40 (0.17) [1100] 1.80 (0.08) [1700] 1.50 (0.10) [1700] Ours 1.10 (0.15) [177] 1.46 (0.17) [153] 1.81 (0.08) [237] 1.51 (0.10) [246] 6. Concluding Remarks Extensive experimental results demonstrate that the pro- posed TnALE approach can greatly reduce the evaluation cost, up to 10× fewer evaluations, compared with TNLS (Li et al., 2022) and other methods for the task of tensor network structure search (TN-SS). The theoretical results in this pa- per provide a rigorous analysis of the convergence rate and the evaluation efficiency for both TNLS and TnALE. Limitation. The main limitation of TnALE is the local convergence issue. In particular, we empirically found in the TN-TS experiment (see Table 4) multiple local minima, which are poor in compression ratio, and TnALE can easily drop in them. Conversely, the methods TNGA (Li & Sun, 2020) and TNLS (Li et al., 2022) seem to better avoid local minima, owing to their stochastic essence. Solving this issue will be the direction of our future work. Furthermore, the identifiability of the proposed method for TN-SS in the presence of noise would be also investigated in the future. Acknowledgements This work was partially supported by JSPS KAKENHI (Grant No. 20H04249, 23H03419). Chunmei is supported by China Scholarship Council (CSC). Part of the computa- tion was carried out at the Riken AIp Deep learning ENvi- ronment (RAIDEN). References Acharya, A., Rudolph, M., Chen, J., Miller, J., and Perdomo-Ortiz, A. Qubit seriation: Improving data- model alignment using spectral ordering. arXiv preprint arXiv:2211.15978, 2022. Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Tel- garsky, M. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Re- search, 15(1):2773–2832, 2014. Arbelaez, P., Maire, M., Fowlkes, C., and Malik, J. Contour detection and hierarchical image segmentation. IEEE transactions on pattern analysis and machine intelligence, 33(5):898–916, 2010. Boyd, S. and Vandenberghe, L. Convex optimization. Cam- bridge university press, 2004. Cai, Y . and Li, P. A blind block term decomposition of high order tensors. In Proceedings of the AAAI Conference on Artificial Intelligence, number 8, pp. 6868–6876, 2021. Chen, Z., Lu, J., and Zhang, A. R. One-dimensional ten- sor network recovery. arXiv preprint arXiv:2207.10665, 2022. Cheng, Z., Li, B., Fan, Y ., and Bao, Y . A novel rank selec- tion scheme in tensor ring decomposition based on rein- forcement learning for deep neural networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3292–3296. IEEE, 2020. Cichocki, A., Phan, A.-H., Zhao, Q., Lee, N., Oseledets, I., Sugiyama, M., Mandic, D. P., et al. Tensor networks for dimensionality reduction and large-scale optimization: Part 2 applications and future perspectives. Foundations and Trends® in Machine Learning, 9(6):431–673, 2017. Cincio, L., Dziarmaga, J., and Rams, M. M. Multiscale entanglement renormalization ansatz in two dimensions: quantum Ising model. Physical Review Letters, 100(24): 240603, 2008. Dua, D. and Graff, C. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml. Falc´o, A., Hackbusch, W., and Nouy, A. Geometry of tree- based tensor formats in tensor banach spaces. Annali di Matematica Pura ed Applicata (1923-), pp. 1–18, 2023. Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera- Paredes, B., Barekatain, M., Novikov, A., Ruiz, F. J. R., Schrittwieser, J., Swirszcz, G., Silver, D., Hassabis, D., and Kohli, P. Discovering faster matrix multiplication al- gorithms with reinforcement learning. Nature, 610(7930): 47–53, 2022. 9TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Felser, T., Trenti, M., Sestini, L., Gianelle, A., Zuliani, D., Lucchesi, D., and Montangero, S. Quantum-inspired ma- chine learning on high-energy physics data. npj Quantum Information, 7(1):111, 2021. Flake, G. W. and Lawrence, S. Efficient SVM regression training with SMO. Machine Learning, 46(1):271–290, 2002. Ghadiri, M., Fahrbach, M., Fu, G., and Mirrokni, V . Approx- imately optimal core shapes for tensor decompositions. arXiv preprint arXiv:2302.03886, 2023. Glasser, I., Sweke, R., Pancotti, N., Eisert, J., and Cirac, I. Expressive power of tensor-network factorizations for probabilistic modeling. Advances in neural information processing systems, 32, 2019. Golovin, D., Karro, J., Kochanski, G., Lee, C., Song, X., and Zhang, Q. Gradientless descent: High-dimensional zeroth-order optimization. In International Conference on Learning Representations, 2019. Haberstich, C., Nouy, A., and Perrin, G. Active learning of tree tensor networks using optimal least-squares. arXiv preprint arXiv:2104.13436, 2021. Hackbusch, W. and K¨uhn, S. A new scheme for the tensor representation. Journal of Fourier analysis and applica- tions, 15(5):706–722, 2009. Hashemizadeh, M., Liu, M., Miller, J., and Rabusseau, G. Adaptive tensor learning with tensor networks. arXiv preprint arXiv:2008.05437, 2020. Hawkins, C. and Zhang, Z. Bayesian tensorized neural networks with automatic rank selection. Neurocomputing, 453:172–180, 2021. Hayashi, K., Yamaguchi, T., Sugawara, Y ., and Maeda, S.-i. Exploring unexplored tensor network decompositions for convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 5553–5563, 2019. Hikihara, T., Ueda, H., Okunishi, K., Harada, K., and Nishino, T. Automatic structural optimization of tree tensor networks. Physical Review Research, 5(1):013031, 2023. Hillar, C. J. and Lim, L.-H. Most tensor problems are NP- hard. Journal of the ACM (JACM), 60(6):45, 2013. Izmailov, P., Novikov, A., and Kropotov, D. Scalable Gaus- sian processes with billions of inducing inputs via tensor train decomposition. In International Conference on Ar- tificial Intelligence and Statistics, pp. 726–735. PMLR, 2018. Khavari, B. and Rabusseau, G. Lower and upper bounds on the pseudo-dimension of tensor network models. Ad- vances in Neural Information Processing Systems , 34, 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kodryan, M., Kropotov, D., and Vetrov, D. Mars: Masked automatic ranks selection in tensor decompositions. In International Conference on Artificial Intelligence and Statistics, pp. 3718–3732. PMLR, 2023. Kossaifi, J., Lipton, Z. C., Kolbeinsson, A., Khanna, A., Furlanello, T., and Anandkumar, A. Tensor regression networks. Journal of Machine Learning Research, 21: 1–21, 2020. Li, C. and Sun, Z. Evolutionary topology search for ten- sor network decomposition. In Proceedings of the 37th International Conference on Machine Learning (ICML), 2020. Li, C. and Zhao, Q. Is rank minimization of the essence to learn tensor network structure? In Second Workshop on Quantum Tensor Networks in Machine Learning (QT- NML), Neurips, 2021. Li, C., Zeng, J., Tao, Z., and Zhao, Q. Permutation search of tensor network structures via local sampling. In Inter- national Conference on Machine Learning, pp. 13106– 13124. PMLR, 2022. Li, N., Pan, Y ., Chen, Y ., Ding, Z., Zhao, D., and Xu, Z. Heuristic rank selection with progressively searching ten- sor ring network. Complex & Intelligent Systems , pp. 1–15, 2021. Liu, Y ., Lu, Y ., Ou, W., Long, Z., and Zhu, C. Adaptively topological tensor network for multi-view subspace clus- tering. arXiv preprint arXiv:2305.00716, 2023. Long, Z., Zhu, C., Liu, J., and Liu, Y . Bayesian low rank tensor ring for image recovery. IEEE Transactions on Image Processing, 30:3568–3580, 2021. Malik, O. A. More efficient sampling for tensor decompo- sition with worst-case guarantees. In International Con- ference on Machine Learning, pp. 14887–14917. PMLR, 2022. Mickelin, O. and Karaman, S. On algorithms for and com- puting with the tensor ring decomposition. Numerical Linear Algebra with Applications, 27(3):e2289, 2020. Miller, J., Rabusseau, G., and Terilla, J. Tensor networks for probabilistic sequence modeling. In International Conference on Artificial Intelligence and Statistics , pp. 3079–3087. PMLR, 2021. 10TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Nie, C., Wang, H., and Tian, L. Adaptive tensor networks decomposition. In BMVC, 2021. Novikov, A., Podoprikhin, D., Osokin, A., and Vetrov, D. P. Tensorizing neural networks. In Advances in Neural Information Processing Systems, pp. 442–450, 2015. Olver, P. J. Introduction to partial differential equations . Springer, 2014. Or´us, R. Tensor networks for complex quantum systems. Nature Reviews Physics, 1(9):538–550, 2019. Oseledets, I. and Tyrtyshnikov, E. TT-cross approxima- tion for multidimensional arrays. Linear Algebra and its Applications, 432(1):70–88, 2010. Oseledets, I. V . Tensor-train decomposition.SIAM Journal on Scientific Computing, 33(5):2295–2317, 2011. Osinsky, A. Tensor trains approximation estimates in the chebyshev norm. Computational Mathematics and Math- ematical Physics, 59(2):201–206, 2019. Rai, P., Wang, Y ., Guo, S., Chen, G., Dunson, D., and Carin, L. Scalable bayesian low-rank decomposition of incom- plete multiway tensors. In International Conference on Machine Learning, pp. 1800–1808. PMLR, 2014. Richter, L., Sallandt, L., and N ¨usken, N. Solving high- dimensional parabolic PDEs using the tensor train format. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, pp. 8998–9009. PMLR, 2021. Sedighin, F., Cichocki, A., and Phan, A.-H. Adaptive rank selection for tensor ring decomposition. IEEE Journal of Selected Topics in Signal Processing, 15(3):454–463, 2021. Snyder, L. V . and Daskin, M. S. A random-key genetic algorithm for the generalized traveling salesman problem. European journal of operational research, 174(1):38–53, 2006. Solgi, R., Loaiciga, H. A., and Zhang, Z. Evolutionary ten- sor train decomposition for hyper-spectral remote sensing images. In IGARSS 2022-2022 IEEE International Geo- science and Remote Sensing Symposium, pp. 1145–1148. IEEE, 2022. Sozykin, K., Chertkov, A., Schutski, R., Phan, A.-H., CI- CHOCKI, A. S., and Oseledets, I. Ttopt: A maximum volume quantized tensor train-based optimization and its application to reinforcement learning. Advances in Neu- ral Information Processing Systems , 35:26052–26065, 2022. Tucker, L. R. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279–311, 1966. T¨ufekci, P. Prediction of full load electrical power output of a base load operated combined cycle power plant us- ing machine learning methods. International Journal of Electrical Power & Energy Systems, 60:126–140, 2014. Tyrtyshnikov, E. Incomplete cross approximation in the mosaic-skeleton method. Computing, 64(4):367–380, 2000. Verstraete, F. and Cirac, J. I. Renormalization algorithms for quantum-many body systems in two and higher di- mensions. arXiv preprint cond-mat/0407066, 2004. Wang, W., Aggarwal, V ., and Aeron, S. Efficient low rank tensor ring completion. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 5697–5705, 2017. Wu, Z.-C., Huang, T.-Z., Deng, L.-J., Dou, H.-X., and Meng, D. Tensor wheel decomposition and its tensor completion application. In Advances in Neural Information Process- ing Systems, 2022. Ye, K. and Lim, L.-H. Tensor network ranks. arXiv preprint arXiv:1801.02662, 2019. Yin, M., Phan, H., Zang, X., Liao, S., and Yuan, B. Batude: Budget-aware neural network compression based on Tucker decomposition. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 1, 2022. Yokota, T., Zhao, Q., and Cichocki, A. Smooth PARAFAC decomposition for tensor completion. IEEE Transactions on Signal Processing, 64(20):5423–5436, 2016. Zhang, A. Cross: Efficient low-rank tensor completion. The Annals of Statistics, 47(2):936–964, 2019. Zhao, Q., Zhang, L., and Cichocki, A. Bayesian CP fac- torization of incomplete tensors with automatic rank de- termination. IEEE transactions on pattern analysis and machine intelligence, 37(9):1751–1763, 2015. Zhao, Q., Zhou, G., Xie, S., Zhang, L., and Cichocki, A. Tensor ring decomposition. arXiv preprint arXiv:1606.05535, 2016. Zhe, S., Xu, Z., Chu, X., Qi, Y ., and Park, Y . Scalable nonparametric multiway data analysis. In Artificial Intel- ligence and Statistics, pp. 1125–1134. PMLR, 2015. Zheng, Y .-B., Huang, T.-Z., Zhao, X.-L., Zhao, Q., and Jiang, T.-X. Fully-connected tensor network decomposi- tion and its application to higher-order tensor completion. In Proc. AAAI, volume 35, pp. 11071–11078, 2021. 11TnALE: Solving Tensor Network Structure Search with Fewer Evaluations A. TnALE: Details for the algorithm The pseudocode for ALE is given in Alg. 3. As discussed in the paper, each structure-related variable is updated alternately by enumeration in the neighborhood. Based on it, the entire algorithm of TnALE is shown in Alg. 2. Apart from the major iteration (lines 6-8), beforehand, there is an initial phase, where we apply a larger radius r1 and the objective prediction trick to find the candidates in a broader range and a rough fashion. A.1. The objective estimation trick. As discussed in Remark 3.1, we employ the objective estimation by linear interpolation in place of the complete enumeration when updating the TN-ranks. Particularly in Alg. 2, we apply this trick to the initial phase, where we consider a broader range of the neighborhood for roughly finding the structure candidates. Suppose a rank variable r is required to be enumerated in the range of r0 − b ≤ r ≤ r0 + b, where r0, b∈ Z+ present the center and radius of the searched neighborhood, respectively. The objective estimation trick aims to estimate the minimum of the inner optimization of (1) associated with each enumerated TN-structures with limited evaluations. In the trick, we first evaluate explicitly three TN structures, i.e., r = {r0 − b, r0, r0 + b}. Then, a simple linear regression model is applied to predicting the evaluations of TN-structures in the interval (r0, r0 + b). The evaluations for the interval (r0 − b, r0) will be predicted similarly using the data w.r.t. {r0 − b, r0}. We can see from the trick that the inner minimization of (1) can be quickly estimated with only three explicit evaluations, no matter how wide the searching range is. Although the simple linear regression can only give a rough estimation, it is sufficiently helpful for TnALE to find good TN-structures in the initial phase. A.2. The neighborhood in the graph space In the problem of TN-PS, we follow the idea of Li et al. (2022) to specify the neighborhood for a given graph G in (1). Similar to Alg. 1 in Li et al. (2022), we construct the neighborhood by swapping enumerately two vertices of G. Suppose the graph G is of N vertices, we consequently achieve the neighborhood of G of the size N(N − 1)/2. Algorithm 2 TnALE: the proposed solver of the optimization (1) 1: Input: A solver for the inner minimization of (1); the rank-related radius: r1 ≥ r2 > 0; the number of Iterations in the initialization phase: L0; the number of Iterations in the searching phase: L; the number of the round-trips for ALE: D; 2: Initialize: Uniformly choose a TN structure (G, r) at random or choose (G, r) with the specified value; 3: for l = 1, . . . L0 do 4: Update recursively (G, r) using Alg. 3 with the center (G, r), radius r1, D round-trips and the objective estimation trick; 5: end for 6: for l = 1, . . . Ldo 7: Update recursively (G, r) using Alg. 3 with the center (G, r), radius r2 and D round-trips ; 8: end for 9: Output: (G, r). 12TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Algorithm 3 ALE: alternating local enumeration 1: Input: The center of the neighborhood: (G(0), r(0)), where r = [r(0) 1 , r(0) 2 , . . . , r(0) K ]⊤ ∈ ZK + ; the rank-related radius: r ∈ Z+; the number of “round-trips”: D; 2: Initialize: (G, r) = (G0, r0), where r = [r1, r2, . . . , rK]⊤ 3: for d = 1, . . . , Ddo 4: for k = 1, . . . , Kdo 5: for i = −r, . . . ,0, . . . , rdo 6: Copy (G, r) into ( ¯G, ¯r) 7: Update ( ¯G, ¯r) by ¯rk = rk + i; 8: Calculate the objective of (1) associated to ( ¯G, ¯r); # Objective estimation is available. 9: Store the value of the objective as h(i); 10: end for 11: Update (G, r) by rk = arg mini h(i); 12: end for 13: Take the neighborhood B(G) according to section A.2; 14: for all G′ ∈ B(G) do 15: Update (G, r) by G = G′; 16: Calculate the objective of (1) associated to (G, r); 17: Store the value of the objective as h(G′); 18: end for 19: Update (G, r) by G = arg minG′ h(G′); 20: for k = K, K− 1, . . . ,2 do 21: for i = −r, . . . ,0, . . . , rdo 22: Copy (G, r) into ( ¯G, ¯r) 23: Update ( ¯G, ¯r) by ¯rk = rk + i; 24: Calculate the objective of (1) associated to ( ¯G, ¯r); # Objective estimation is available. 25: Store the value of the objective as h(i); 26: end for 27: Update (G, r) by rk = arg mini h(i); 28: end for 29: end for 30: Output: (G, r). 13TnALE: Solving Tensor Network Structure Search with Fewer Evaluations B. Theoretical analysis with proofs In this section, we first give a rigorous convergence analysis for the algorithms using the local-sampling scheme. After that, we compare the evaluation efficiency for TNLS (Li et al., 2022) and the new algorithm TnALE. B.1. A quick review of tensor network (TN) structure search Suppose we have the dataset D and a task-specific loss function πD : RI1×I2×···×IN → R+ associated to D. The tensor network structure search (TN-SS) problem is to solve the following bi-level optimization problem (Li et al., 2022) min (G,r)∈G×FG \u0012 ϕ(G, r) + λ · min Z∈TNS (G,r) πD(Z) \u0013 , (8) where G ∈ G is a graph, which owns N vertices and K edges and corresponds to the TN-topology, r ∈ FN ⊆ ZK + is a positive integer vector ofK dimension corresponding to the TN-ranks, ϕ : G×ZK + → R+ represents the function measuring the model complexity of a TN whose structure is modeled by (G, r), and λ >0 is a tuning parameter. As expected for TN-SS, solving the problem (8) is intuitively to search for a TN structure modeled by (G, r), by which we can give the optimal balance between the complexity and the expressibility of a TN in the task. We remark that TN-SS can be specified as three sub-problems:permutation selection (TN-PS, Li et al. (2022)), rank selection (TN-RS) and topology selection (TN-TS, Li & Sun (2020)), by specifying the feasible set G × FG of (8) into different forms. Specifically, in TN-PS, we set FG = ZK + and G is defined as the isomorphic graphs to a “template” G0, so that only the ranks and vertex permutations are searched while the TN-topology is preserved. In TN-RS, however, we typically restrict the graph G in (8), i.e., G = {G0} but consider searching for all possible ranks i.e., FG = ZK + . In TN-TS, we relax G to be a set containing all possible simple graphs of order N and the set FN can be fixed (Li & Sun, 2020) or not (Hashemizadeh et al., 2020). It is known from (Ye & Lim, 2019; Hashemizadeh et al., 2020) that the TN-PS problem with the rank searching can be simplified as a TN-RS problem associated to a “fully-connected” TN (Zheng et al., 2021). B.2. Analysis of descent steps We start the analysis by rewriting (8) into a more general form: min x∈ZK + ,p∈P fp(x) := f ◦ p(x), (9) where ◦ denotes the function composition, f : ZL ≥0 → R+ is a generalization of the objective function of (8), x ∈ ZK + corresponds to the rank-related variable r of (8), and the operator p : ZK + → ZL ≥0 and its feasible set P correspond to the topology-related variable G and the set G of (8), respectively. The relationship between p ∈ P of (9) and G ∈ G of (8) is demonstrated as follows. Since in (8) the entries of r can be regarded as labels on the edges of G, the pair (G, r) can be described as a weighted adjacency matrix of N × N. For example, a 4-th order tensor ring (TR, Zhao et al., 2016) of the ranks-{2, 3, 4, 5} can be described as (G1, r) =     0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0  ,   2 3 4 5     =⇒ A1 =   0 0 2 3 0 0 4 5 2 3 0 0 4 5 0 0  , (10) or (G2, r) =     0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0  ,   2 3 4 5     =⇒ A2 =   0 2 0 5 2 0 3 0 0 3 0 4 5 0 4 0  . (11) Here G1 and G2 correspond to the TR topology with different permutations of vertices. In the settings of TN-PS (Li et al., 2022), we can prove that such the relationship is bijective. The operator p is thus to map the TN-ranks, denoted x ∈ ZK + in (9), onto the vectorization of entries in the upper triangle part (except for the diagonal) of the adjacency matrix. For 14TnALE: Solving Tensor Network Structure Search with Fewer Evaluations example, A1 =   0 0 2 3 0 0 4 5 2 3 0 0 4 5 0 0   ⇐⇒ p1(x) =   0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0     2 3 4 5   =   0 2 3 4 5 0   , (12) and A2 =   0 2 0 5 2 0 3 0 0 3 0 4 5 0 4 0   ⇐⇒ p2(x) =   1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0     2 3 4 5   =   2 0 5 3 0 4   . (13) It is shown that p is essentially an operator produced by the permutation padding with several rows of zeros according to G. The convergence analysis of this work is mainly inspired by Golovin et al. (2019), which establishes a convex framework for the gradient-less optimization algorithms in the real domain. The challenge is, the TN-SS problem is essentially discrete, so that many well-developed tools, such as convexity and smoothness, for convergence analysis turn invalid in the discrete scenario. To bridge the graph from Golovin et al. (2019) to TN-SS, we first re-define several important concepts, by which the necessary tools for the analysis are re-derived. In doing so, we begin by introducing the finite gradient as the alternative to the classic one defined in the continuous domain. Definition B.1 (finite gradient). For any function f : ZL ≥0 → R, its finite gradient ∆f : ZL ≥0 → RL at the point x is defined as the vector ∆f(x) = [f(x + e1) − f(x), . . . , f(x + eL) − f(x)]⊤ , (14) where ei ∀i ∈ [L] denote the unit vectors with the i-th entry being one and other entries being zeros. Applying the finite gradient defined in (14), we also re-define the strong convexity and smoothness for analysis in the discrete domain. Definition B.2 (α-strong convexity with finite gradient). We say f is α-strongly convex for α ≥ 0 if f(y) ≥ f(x) + ∆f(x) − α 2 1, y − x \u000b + α 2 ∥y − x∥2 for all x, y ∈ ZL ≥0, where 1 ∈ RL denotes the vector with all entries being one. We simply say f is convex if it is α-strongly convex and α = 0. Compared to the definitions used in Golovin et al. (2019) and other literature for convex analysis, the additional term, α 2 1, marked by the blue color, appears due to the discrepancy of the finite gradient and its counterpart in the continuous domain. Below, we prove several basic results using theα-strong convexity with finite gradient. Lemma B.3. If f is α-strongly convex in ZL ≥0, then the following inequalities are held: 1. g(x) = f(x) − α 2 ∥x∥2 is convex in the discrete scenario for all x ∈ ZL ≥0, and vice versa; 2. ⟨∆f(x) − ∆f(x), x − y⟩ ≥α∥x − y∥2 for any x, y ∈ ZL ≥0; 3. ∥∆f(x) − ∆f(y)∥ ≥α∥x − y∥ for any x, y ∈ ZL ≥0; Here ∥ · ∥denotes the l2 norm for vectors. Proof. (1, ⇒) According to Def. B.2, the first statement is equivalent to proving the inequality g(y) ≥ g(x) + ⟨∆g(x), y − x⟩ (15) 15TnALE: Solving Tensor Network Structure Search with Fewer Evaluations holding for any x, y ∈ ZL ≥0. Applying the α-strong convexity assumption, it follows that g(y) − g(x) − ⟨∆g(x), y − x⟩ = f(y) − α 2 ∥y∥2 − f(x) + α 2 ∥x∥2 − ⟨∆g(x), y − x⟩ = f(y) − α 2 ∥y∥2 − f(x) + α 2 ∥x∥2 − D ∆f(x) − α 2 (2x + 1), y − x E = f(y) − f(x) − D ∆f(x) − α 2 1, y − x E − α 2 ∥y∥2 + α 2 ∥x∥2 + α 2 ⟨2x, y − x⟩ ≥ α 2 \u0000 ∥y − x∥2 − ∥y∥2 − ∥x∥2 + 2⟨x, y⟩ \u0001 = 0. (16) Here the first equality follows from the definition ofg(x), the second equality holds since the finite gradient∆∥x∥2 = 2x+1, and the inequality at the bottom line follows from the α-strong convexity assumption on f. (1, ⇐) By (15), f(y) − α 2 ∥y∥2 ≥ f(x) − α 2 ∥x∥2 + D ∆f(x) − α 2 (2x + 1), y − x E . (17) The α-strong convexity of f is thus proved by algebraically simplifying (17). (2) To prove the second statement, we first know that the following inequality ⟨∆g(x) − ∆g(y), x − y⟩ ≥0, ∀x, y (18) holds since the monotone gradient property of the convexity (it is true in both continuous and discrete scenarios). By the form of ∆g(x), it follows that D ∆f(x) − α 2 (2x + 1) − ∆f(y) + α 2 (2y + 1), x − y E ≥ 0. (19) With algebraic simplification, we obtain ⟨∆f(x) − ∆f(y), x − y⟩ ≥α∥x − y∥2 (20) for all x, y ∈ ZL ≥0. The second statement is thus proved. (3) The third statement holds since the following relationship ∥∆f(x) − ∆f(y)∥∥x − y∥ ≥ ⟨∆f(x) − ∆f(y), x − y⟩ ≥α∥x − y∥2, (21) where the first inequality follows from the Cauchy–Schwarz inequality, and the second inequality follows from(20). The third statement is proved by dividing by ∥x − y∥ on both sides. Apart from the convexity, the smoothness of the objective function is also required to be re-defined in the discrete scenario. Definition B.4 ((β1, β2)-smoothness with finite gradient). We say f is (β1, β2)-smooth for β1, β2 > 0 if 1. |f(x) − f(y)| ≤β1∥x − y∥ for all x, y ∈ ZL ≥0; 2. The function l(x) := β2 2 ∥x∥2 − f(x) is convex. The first item of Def. B.4 restricts that f is β1-Lipschitz, implying the “continuity” of the function, while the second item upper bounds the change of the finite gradient of f, implying a “continuity” over the finite gradient. In particular, Lemma B.5. If l(x) = β 2 ∥x∥2 − f(x) is convex, then for all x, y ∈ ZL ≥0 1. f(y) ≤ f(x) + D ∆f(x) − β 2 1, y − x E + β 2 ∥y − x∥2 and vise versa; 2. ⟨∆f(x) − ∆f(y), x − y⟩ ≤β∥x − y∥2. 16TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Proof. (1, ⇒) By the form l(x) and its convex property, we have the inequality β 2 ∥y∥2 − f(y) ≥ β 2 ∥x∥2 − f(x) + \u001cβ 2 (2x + 1) − ∆f(x), y − x \u001d . (22) The first statement is proved by algebraically simplifying (22). The (⇐) direction can be proved similarly. To prove the second item, by the convexity ofl(x), l(y) ≥ l(x) + ⟨∆l(x), y − x⟩. (23) Similarly, l(x) ≥ l(y) + ⟨∆l(y), x − y⟩. (24) Summing the two sides of (23) and (24) up, we have ⟨∆l(x) − ∆l(y), x − y⟩ ≥0. (25) Applying l(x) = β 2 ∥x∥2 − f(x), ⟨β(2x + 1) − ∆f(x) − β(2y + 1) + ∆f(y), x − y⟩ ≥0. (26) By simplifying the inequality, we finally have ⟨∆f(x) − ∆f(y), x − y⟩ ≤β∥x − y∥2. (27) The first item of Def. B.4 gives the following crucial result, which is used in the main theorem of this paper. Lemma B.6. If |f(x) − f(y)| ≤β∥x − y∥ for all x, y ∈ ZL ≥0, then the norm of the finite gradient with respective to x is bounded, i.e., ∥∆f(x)∥∞ ≤ β. Proof. Denote ∆f(x)i the i-th entry of ∆f(x), then for all 1 ≤ i ≤ L the second item of the definition follows by |∆f(x)i| = |f(x + ei) − f(x)| ≤β∥x + ei − x∥ = β, (28) where ei denotes the unit vector with i-th entry being one and others being zeros, and the first equality follows from the definition of the finite gradient. After the new definitions of convexity and smoothness with finite gradient, in the proof, we also use the concept of the sub-level set, which is widely used in optimization theory. For the self-consistency purpose, the specific definition is reviewed as follows: Definition B.7 (sub-level set). The level set of f at point x ∈ ZL ≥0 is Lx(f) = \b y ∈ ZL ≥0 : f(y) = f(x) \t . The sub-level set of f at point x ∈ ZL ≥0 is L↓ x(f) = \b y ∈ ZL ≥0 : f(y) ≤ f(x) \t . The following lemma shows that, for any x, there exists a cube, i.e., a ball with infinity-norm, which is tangent at x and inside the sub-level set L↓ x(f). Lemma B.8 (the sub-level cube). Assume that f : ZL ≥0 → R is α-strongly convex, (β1, β2)-smooth, and its minimum, denoted f(x∗), satisfies ∥β2 2 1 − ∆f(x∗)∥ ≤γ where γ is a constant and 0 ≤ γ < α. Then, for all x ∈ ZL ≥0, there is a L-dimensional cube, which is of the edge length 2(α−γ) β2 √ L ∥x − x∗∥, tangent at x, and inside the sub-level set L↓ x(f). Proof. Applying the smoothness assumption and Lemma B.5, f \u0012 x − 1 β2 ∆f(x) + 1 21 + s \u0013 ≤ f(x) + \u001c ∆f(x) − β2 2 1, s + 1 21 − 1 β2 ∆f(x) \u001d + β2 2 \r\r\r\rs + 1 21 − 1 β2 ∆f(x) \r\r\r\r 2 = f(x) + β2 2 \u0012 ∥s∥2 − ∥1 21 − 1 β2 ∆f(x)∥2 \u0013 (29) 17TnALE: Solving Tensor Network Structure Search with Fewer Evaluations for any s. The inequality (29) implies that for any y ∈ ZL ≥0 in the Euclidean ball B \u0010 x − 1 β2 ∆f(x) + 1 2 1, ∥1 2 1 − 1 β2 ∆f(x)∥ \u0011 it yields f(y) ≤ f(x), i.e., y ∈ L↓ x(f). We also see that x is at the surface of this Euclidean ball, i.e., the ball is tangent at x. Furthermore, we also prove that the radius of the ball is lower bounded as follows: 1 β2 ∥β2 2 1 − ∆f(x)∥ = 1 β2 ∥β2 2 1 − ∆f(x∗) + ∆f(x∗) − ∆f(x)∥ ≥ 1 β2 \u0012 ∥∆f(x) − ∆f(x∗)∥ − ∥β 2 1 − ∆f(x∗)∥ \u0013 ≥ (α − γ) β2 ∥x − x∗∥, (30) where the inequality at the bottom line follows from the third statement of Lemma B.3 and the assumption∥β2 2 1−∆f(x∗)∥ ≤ γ. Next, we show that the ball B \u0010 x − 1 β2 ∆f(x) + 1 2 1, ∥1 2 1 − 1 β2 ∆f(x)∥ \u0011 contains a cube of edge length 2(α−γ) β2 √ L ∥x − x∗∥. First, we easily know in the ball there exists a cube, of which the volume is sufficiently small, and one vertex is at x. Then, the cube gradually extends all edges until the adjacent vertices of x touch the surface of the ball. At this moment, it can be seen that the edges that touch the surface of the ball turn the chords of the ball. Furthermore, the line connecting the ball center to x has an equal angle to all edges connecting x, due to the symmetry of the geometrical shapes. With basic geometry knowledge, we can thus calculate the chord length, i.e., the edge length of the cube, with 2 × R cos(θ), where R denotes the radius of the ball and θ = arccos(1/ √ L) is the angle between the chord and the ”center-x” line. Finally, using (30), we know the cube of the edge length 2(α−γ) β2 √ L ∥x − x∗∥ is tangent at x, and inside sub-level set L↓ x(f). Lemma B.9 (convex combination in the discrete domain). Suppose q = θx+(1 −θ)y, ∀θ ∈ [0, 1], and there is ˆq ∈ ZL ≥0 where Λ = q − ˆq. If f is α-strongly convex, then θf(x) + (1− θ)f(y) ≥ f(ˆq) + D ∆f(ˆq) − α 2 1, Λ E + α 2 ∥Λ∥2. (31) Proof. By the definition of the α-strong convexity, f(x) ≥ f(ˆq) + D ∆f(ˆq) − α 2 1, x − ˆq E + α 2 ∥x − ˆq∥2; f(y) ≥ f(ˆq) + D ∆f(ˆq) − α 2 1, y − ˆq E + α 2 ∥y − ˆq∥2. (32) Thus, we have their convex combination as θf(x) + (1− θ)f(y) ≥ f(ˆq) + D ∆f(ˆq) − α 2 1, Λ E + α 2 \u0000 θ∥x∥2 + (1 − θ)∥y∥2 + ∥ˆq∥2 − 2 ⟨q, ˆq⟩ \u0001 ≥ f(ˆq) + D ∆f(ˆq) − α 2 1, Λ E + α 2 \u0000 ∥q∥2 + ∥ˆq∥2 − 2 ⟨q, ˆq⟩ \u0001 = f(ˆq) + D ∆f(ˆq) − α 2 1, Λ E + α 2 ∥Λ∥2 , (33) where the second inequality follows from the convexity of ∥ · ∥2. The proof is completed. Assumption B.10. Assume that f : ZL ≥0 → R+ of (9) is α-strongly convex, (β1, β2)-smooth, and its minimum, denoted (p∗, x∗) = arg minp,x f ◦ p(x), satisfies ∥∆fp∗(x∗) − β2 2 1∥ ≤γ where 0 ≤ γ < α≤ β1 ≤ β2 ≤ 1. Here the inequality ∥∆fp∗(x∗) − β2 2 1∥ ≤γ implies that, up to a (small) constant vector β2 2 1, the finite gradient at (p∗, x∗) should be sufficiently small, which can be understood as the discrete version of the zero-gradient for the stationary points in the continuous domain. The upper bound “ 1” is arbitrarily chosen just for simplifying the calculation. Also note that β2 must be larger than α due to the fact α∥x − y∥2 ≤ ⟨∆f(x) − ∆f(y), x − y⟩ ≤β2∥x − y∥2 (see Lemma B.3 and Lemma B.5). With Assumption B.10, we next prove that the local-sampling-based searching algorithm achieves the linear convergence rate up to a constant, if p∗ is known beforehand. 18TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Theorem B.11 (convergence rate). Suppose Assumption B.10 is satisfied, the operator p in (9) is fixed to be p∗, and 0 ≤ θ ≤ 1. Then, for any x with ∥x − x∗∥∞ ≤ c, we can find a neighborhood B∞(x, rx) where rx ≥ θc + 1 2 , such that there exist a element y ∈ B∞(x, rx) satisfying fp∗(y) − fp∗(x∗) ≤ (1 − θ)(fp∗(x) − fp∗(x∗)) + 7 8K. (34) Proof. First of all, since the operator p is fixed to be p∗, the problem (9) can be equivalently simplified by removing the formulation of p out of (9), which is written as min x∈ZK + f(x), (35) where f : ZK ≥0 → R represents the objective function.7 By Lemma B.9, we have the following inequality: f(ˆq) − f(x∗) ≤ (1 − θ)(f(x) − f(x∗)) + Dα 2 1 − ∆f(ˆq), Λ E − α 2 ∥Λ∥2. (36) Next, we prove in the neighborhood B(x, rx) there exists an element y, which belongs to as well the sub-level cube tangent at ˆq knowing by Lemma B.8, so that f(y) ≤ f(ˆq) holds. To do so, we first know that the distance between ˆq and px(x) satisfying ∥x − ˆq∥∞ = ∥x − q + Λ∥∞ ≤ ∥x − q∥∞ + ∥Λ∥∞ = θ∥x − x∗∥∞ + ∥Λ∥∞ ≤ θc + 1 2. (37) Here the last inequality follows from ∥Λ∥∞ ≤ 1 2 , which holds because ˆq ∈ ZK ≥0 can be always found by rounding the entries of q into the closest integers. We thus know from the inequality that the intersection between the sub-level cube tangent at ˆq and B(x, rx) is not empty if rx ≥ θc + 1 2 , proving the existence of the y. Last, we bound (36) as follows: f(y) − f(x∗) ≤ f(ˆq) − f(x∗) ≤ (1 − θ)(f(x) − f(x∗)) + Dα 2 1 − ∆f(ˆq), Λ E − α 2 ∥Λ∥2 ≤ (1 − θ)(f(x) − f(x∗)) + \f\f\f Dα 2 1, Λ E\f\f\f + |⟨∆f(ˆq), Λ⟩| + α 2 ∥Λ∥2 ≤ (1 − θ)(f(x) − f(x∗)) + α 4 K + ∥∆f(ˆq)∥∞∥Λ∥1 + α 2 ∥Λ∥2 ≤ (1 − θ)(f(x) − f(x∗)) + α 4 K + β1 2 K + α 8 K ≤ (1 − θ)(f(x) − f(x∗)) + 3α + 4β1 8 K ≤ (1 − θ)(f(x) − f(x∗)) + 7 8K. (38) Here the inequality in the fourth line follows from Lemma B.6 and ∥Λ∥∞ ≤ 1/2, and the inequality at the bottom line follows from Assumption B.10 that α < β1 ≤ 1. The proof is thus completed. It is known from the proof that the constant (7/8)K appearing in (34) is due to the fact ∥Λ∥1 ≤ K∥Λ∥∞ ≤ K/2 and ∥Λ∥2 ≤ K∥Λ∥∞ ≤ √ K/2. It means that with the rounding error ∥Λ∥∞ ≤ 1/2, the l1,2 norm of Λ would become larger with increasing the dimension K, which is inevitable in the analysis. It only disappears if ∥Λ∥∞ = 0, implying the conventional convex optimization in the continuous domain. As an important corollary from Theorem B.11, we next prove the convergence guarantee for the local-sampling-based methods. Corollary B.12 (convergence guarantee). Suppose p∗ is known and a series {xn}∞ n=0, where x0 is randomly chosen inZK + , and for each n >0, xn is equal to the y in Theorem B.11. Then we can achieve the following limit when Ω(1/K) ≤ θ ≤ 1, lim n→∞ (fp∗(xn) − fp∗(x∗)) = O(1) (39) 7Here for brevity, we re-use the notation of f without ambiguity since the main properties of f are preserved up to the domain restricting from ZL ≥0 to ZK ≥0. 19TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Proof. Let CK := (7/8)K. By the updating rule, fp∗(xn) − fp∗(x∗) ≤ (1 − θ)(fp∗(xn−1) − fp∗(x∗)) + CK ≤ (1 − θ)2(fp∗(xn−2) − fp∗(x∗)) + CK + CK(1 − θ) ≤ (1 − θ)3(fp∗(xn−3) − fp∗(x∗)) + CK + CK(1 − θ) + CK(1 − θ)2 ≤ ··· ≤ (1 − θ)n(fp∗(x0) − fp∗(x∗)) + CK nX m=1 (1 − θ)m−1. (40) Thus using the condition Ω(1/K) ≤ θ ≤ 1, we finally obtain that lim n→∞ (fp∗(xn) − fp∗(x∗)) ≤ 0 + CK 1 θ = O(1). (41) B.3. Sampling efficiency Proposition B.13 (curse of dimensionality for TNLS). Let the assumptions in Theorem B.11 be satisfied. Furthermore, assume that x∗ is sufficiently smaller (or larger) than x entry-wisely except for a constant number of entries. Then the probability of achieving a suitable y as mentioned in Theorem B.11 by uniformly randomly sampling in B∞(x, rx) with rx ≥ θc + 1 2 equals O(2−K). Proof. We only prove the case where x∗ is sufficiently smaller than x in the entry-wise manner, except a constant number of entries. The “larger” case can be proved similarly. Recall Theorem B.11. By the construction of y, we have q = θx + (1 − θ)x∗ with 0 ≤ θ ≤ 1 and the approximation ˆq ∈ ZK + with ˆq = q + Λ and ∥Λ∥∞ ≤ 1/2. According to the assumptions, we know x − ˆq is entry-wisely larger than zero except C entries, where C ≥ 0 is a constant. Since rx ≥ θc + 1 2 , we further know from Theorem B.11 that the intersection between B∞(x, rx), denoted B in the rest of the proof for brevity, and the sub-level cube, denoted A, tangent at ˆq is not empty. In this case, we can easily bound the volume of the cube associated to the intersection of A and B as follows: |A ∩ B| ≤(rx − δmin)K−C (rx + δmax)C. (42) Here | · |denotes the volume of the cube. δmin = min {pi : pi = x(i) − ˆq(i) > 0, 1 ≤ i ≤ K} and δmax = max {0, pi : pi = ˆq(i) − x(i) ≤ 0, 1 ≤ i ≤ K}, where x(i), ˆq(i) denote the i-th entry of x and ˆq, respectively. Thus, the probability of uniformly drawing a sample y belonging to A ∩ B from B∞(x, rx) is as follows: P r(y ∈ A ∩ B) ≤ (rx − δmin)K−C (rx + δmax)C (2rx)K ≤ \u0012rx + δmax rx − δmin \u0013C 2−K = O(2−K). (43) The proof is completed. Recall that let B := B(p) × B∞(x, rx) and f∗ B := min(py,y)∈B fpy (y) for notational simplicity, then Proposition B.14 (evaluation efficiency for TnALE). Let B ∈RI×I×···×I be the tensor of order-(K + 1) constructed as Eq. (6) with I1 = I2 = ··· = IK+1 = I. Then, there exists its TT-cross approximation (Oseledets & Tyrtyshnikov, 2010) of rank-R8, denoted ˆB, for which it satisfies j = arg maxi ˆB(i), such that the equality f∗ B = fpjK+1 (x + j(: K) − (⌈rx⌉ + 1)) holds, provided that f∗ B ≤ fpz (z)/ \u0012 1 + 2(4R)⌈log2 K⌉ − 1 4R − 1 (R + 1)2ξfpz (z) \u0013 (44) for all (pz, z) ∈ B and fpz (z) ̸= f∗ B. Here, ξ denotes the error between B and its best approximation of TT-ranks R in terms of ∥ · ∥∞. Note that the inequality (44) holds trivially if B is exactly of the TT topology of rank-R, and Oseledets & Tyrtyshnikov (2010) shows that the f∗ B can be recovered fromO(KIR ) entries from B. 8Here we assume that all elements of the TT-ranks are equal to R for brevity. 20TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Proof. Since the “one-to-one” relation between the entries of the tensor B and all possible f(z) for all (pz, z) ∈ B(p) × B∞(x, rx), it is easily to know the equality f∗ B = fpj,K+1 (x + j(: K) − (⌈rx⌉ + 1)) holds if ˆB(i∗) ≥ ˆB(k) for i∗ = arg maxi B(i) and any index k. To prove this condition true, we have the following inequalities for anyk: ˆB(i∗) − ˆB(k) ≥ B(i∗) − B(k) − 2(4R)⌈log2 K⌉ − 1 4R − 1 (R + 1)2ξ = 1/f∗ B − 1/fpjK+1 (x + k(: K) − (⌈rx⌉ + 1)) − 2(4R)⌈log2 K⌉ − 1 4R − 1 (R + 1)2ξ ≥ 2(4R)⌈log2 K⌉ − 1 4R − 1 (R + 1)2ξ − 2(4R)⌈log2 K⌉ − 1 4R − 1 (R + 1)2ξ = 0 , (45) where the first inequality follows from Theorem 2 in (Osinsky, 2019), and the last inequality follows from the inequality(44). It can also be known if B is exactly of the TT topology of rank-R, ˆB is able to recover B exactly. In this case ξ = 0 and f∗ B ≤ f(z) trivially for all z. 21TnALE: Solving Tensor Network Structure Search with Fewer Evaluations C. Experiment details C.1. Low-rank structure of the optimization landscape To verify the low-rank structure of the optimization landscape of (1), we empirically check the singular values of the landscape tensor using the synthetic data. To be specific, we re-use the fourth-order tensor in the experiment for TN-PS, i.e., TR (order-4) in Table 5. Here we remove the influence of unknown permutations and calculate the objective for all possible combinations of values of the TN-ranks. As a result, for each data, we have a landscape tensor (a tensor whose entries are values of the objective function) of order-4, and the modes of the tensor corresponding to the four TN-ranks. Figure 6 (a) shows the singular values of the landscape tensor unfolded along different modes on average. We see that the landscape tensor provides a significant low-rank structure in the data. We also depict the complete landscape (contour line, unfolded along the first two modes) with respect to Data A in Figure 6 (b). We can see that the obviously repeated pattern shown in the figure is the main reason leading to the low-rank structure of the landscape. Singular valueSingular value (a) Averaged singular values for the 4th-order landscape tensor. 10 20 30 40 Indices 5 10 15 20 25 30 35 40 45Indices 2 4 6 8 10 12 14 16 (b) Optimization landscapes (the inverse 1/f (x)) wrt. the tensor of order-4 and correct permutation. Figure 6.Averaged singular values and Optimization landscapes for the tensor of order-4. C.2. Details for the experiment of TN-PS (w.r.t., Table 1). Goal. In this experiment, our goal is to verify the superiority of TnALE in addressing the TN-PS problem. Data generation. For the synthetic data with TR topology (order-4, order-6, and order-8), as well as PEPS( order-6), HT (order-6), and MERA (order-8), we re-use the data from Li et al. (2022). To generate data with TW (order-5) topology, we set the dimension of each tensor mode to 3. Additionally, we randomly select TN-ranks from the set {1, 2, 3}. Then we i.i.d. draw samples from Gaussian distribution N(0, 1) as the values of core tensors. After contracting these core tensors based on the TW topology, we randomly and uniformly permute the tensor modes. Settings. In the experiment, we implement TNGA and TNLS as comparison methods. We use the same objective function as described in Li & Sun (2020) for all the methods. Specifically, the objective function of (1) used in the experiment is as follows: F(G, r) = 1 ϵ(G, r)| {z } compression ratio (CR) +λ · min Z∈TNS (G,r) ∥X − Z∥2 / ∥X∥2 | {z } relative squared error (RSE) , (46) where X denotes the synthetic tensor, and ϵ(G, r) represents the compression ratio equalling to ϵ(G, r) = Dimension of X Dimension sum of core tensors of the TN under (G, r). The trade-off parameter λ in (46) is set to 200. For the solver of the inner minimization, we utilize the Adam optimizer Kingma & Ba (2014) with a learning rate of 0.001. Additionally, the core tensors are initialized using Gaussian distribution 22TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Table 5.Experimental results of the TN-PS task on TR topology. In the table, Eff. and the required evaluation numbers #Eva. are demonstrated. Specifically, #Eva. is shown in the square brackets. Methods Order 4 order 6 order 8 A B C D E A B C D E A B C D E Eff.↑ [#Eva.↓] TNGA 1.00 [450] 1.00 [450] 1.17 [450] 1.00 [300] 1.00 [450] 1.00 [1500] 1.00 [1350] 1.00 [1650] 1.16 [1650] 1.00 [1050] 1.00 [2850] 1.02 [2250] 1.11 [3950] 1.06 [1950] 0.88 [1500] TNLS 1.00 [240] 1.00 [300] 1.17 [60] 1.00 [300] 1.00 [360] 1.00 [660] 1.00 [600] 1.00 [660] 1.16 [600] 1.00 [540] 1.00 [1020] 1.02 [960] 1.11 [1320] 1.06 [780] 1.17 [900] TnALE(ours) 1.00 [93] 1.00 [ 155] 1.17 [ 31] 1.00 [ 124] 1.00 [ 62] 1.00 [156] 1.00 [ 321] 1.00 [ 156] 1.16 [ 156] 1.00 [ 89] 1.00 [231] 1.02 [ 308] 1.11 [ 308] 1.06 [ 231] 1.17 [ 178] Table 6.Experimental results of the TN-PS task on PEPS, HT, MERA and TW topology. In the table,Eff. and the required evaluation numbers #Eva. are demonstrated. Specifically, #Eva. is shown in the square brackets. The symbol “-” in the table means the failure of the approach. Methods PEPS HT MERA TW A B C D A B C D A B C D A B C D Eff.↑ [#Eva.↓] TNGA 1.14 [1560] - 1.00 [840] 1.21 [1080] 1.45 [960] 1.21 [1320] 1.18 [840] 1.29 [1080] - 1.32 [960] 2.30 [2800] 1.00 [3240] 1.24 [1920] 2.61 [1440] 1.23 [600] 1.30 [720] TNLS 1.14 [781] 1.00 [781] 1.00 [421] 1.21 [661] 1.45 [841] 1.21 [841] 1.18 [781] 1.29 [721] 1.09 [1561] 1.88 [841] 2.88 [1441] 1.03 [721] 1.24 [661] 2.61 [601] 1.23 [601] 1.30 [481] TnALE(ours) 1.14 [407] 1.00 [ 465] 1.00 [233] 1.21 [ 175] 1.45 [211] 1.21 [ 281] 1.18 [ 211] 1.29 [ 211] 1.09 [1450] 1.88 [484] 2.88 [ 323] 1.03 [ 323] 1.24 [285] 2.61 [ 143] 1.23 [ 285] 1.30 [214] N(0, 0.1). Furthermore, the search range for TN-ranks is set from 1 to 7, except for TW data, for which the search range is limited to 1 to 4. For TNGA, the maximum number of generations is set to 30. The population size in each generation is 120 for all the TN topologies except for TR, which is set as 150. During each generation, the elimination rate is 36% and the reproduction trick (Snyder & Daskin, 2006) is adopted and we set the reproduction number to be 2. Meanwhile, for the selection probability of the recombination operation, we set the hyper-parameters α = 20 and β = 1. Moreover, there is a 24% chance for each gene to mutate after the recombination. For TNLS, we set the sample numbers in each local sampling stage to 60. The tuning parameter c1 is fixed at 0.9 throughout the experiment. As for the tuning parameter c2, it is adjusted based on the tensor order. Specifically, we set c2 = 0.9 for order-4 TR, c2 = 0.94 for order-6 TR, PEPS, TW and HT, and for MERA and order-8 TR, we set c2 = 0.98. In our proposed method TnALE, we maintain consistent settings throughout the experiment. The rank-related radius is set as r1 = 2 and r2 = 1. During the initialization phase, we perform 2 iterations, and during the searching phase, we conduct 30 iterations. Additionally, we set the number of round-trips of ALE to 1. For performance evaluation, we use the Eff. index, and Eff.≥ 1 indicates an identical or more compact structure has been found. If the results do not satisfy the conditions of RSE ≤ 10−4 and Eff.≥ 1, we say the approach fails in the experiment. 0 500 1000Evaluations -2024 MERA-Data DTNLSTnALE 0 500 1000 1500Evaluations -4-202 MERA-Data CTNLSTnALE 0 500 1000Evaluations -2024 MERA-Data BTNLSTnALE 0 1000 2000Evaluations-3-2-101 MERA-Data ATNLSTnALE logObjective Figure 7.Objective (in the log form) with varying the number of evaluations: an observation of the local convergence of TnALE in MERA. Results. The results for TR topology are presented in Table 5, and the results for PEPS, HT, MERA, and TW topology are shown in Table 6. Based on the results, we observe that both TNLS and TnALE can successfully identify the ranks and permutations of the data, as indicated by Eff.≥ 1. When comparing TNLS and TnALE, we find that TnALE achieves the same results with significantly fewer evaluation requirements. This highlights the superiority of TnALE in solving the TN-PS problem, demonstrating its efficiency and effectiveness. In Figure 4, the averaged log objective curves with varying evaluation numbers of TNLS and TnALE are displayed. It is apparent from the figures that TnALE demonstrates a faster descending trend and achieves lower objective values given the same number of evaluations compared to TNLS for most cases (except for MERA). These results indicate the practical advantage of the proposed method, particularly in scenarios where computational resources are limited, and only a certain number of evaluations can be performed. For the results of 23TnALE: Solving Tensor Network Structure Search with Fewer Evaluations MERA, we further draw the objective curves of each data in Figure 7. From the MERA-Data A curve, it is observed that TnALE descends at a slow pace until approximately 1000 evaluations, whereas TNLS continues to descend. The main reason for this behavior is that TnALE gets trapped in a local optimum and struggles to jump out by restarting the ALE algorithm with a new random center, while TNLS is more likely to overcome such local optima due to its stochastic essence. Moreover, in order to demonstrate the scalability of different TN-PS methods with respect to the tensor order, we draw the average number of evaluations with TR order in Figure 8. From the results, it is evident that the proposed method exhibits a slower increase in the number of evaluations with increasing tensor order compared to other methods. These results highlight the scalability of the proposed method, indicating its ability to handle higher-order tensors effectively. 45678 TR order 0 500 1000 1500 2000 2500Averaged number of evaluations TNGATNLSTnALE 45678 TR order 0 200 400 600 800 1000Averaged number of evaluations TNLSTnALE Figure 8.Number of evaluations with varying TR orders. C.3. Details for the experiment of TN-RS (w.r.t. Table 2). Goal. In this experiment, we consider the classic rank-selection problem, i.e., TN-RS, for TR decomposition. Data Generation. We generate synthetic tensors in TR topology with two configurations: “lower-ranks” and “higher-ranks”. In both configurations, we generate five tensors by randomly selecting ranks and values of the vertices (core tensors). Each tensor has an order of 8, and the dimensions for each tensor mode are set to 3. We i.i.d. draw samples from Gaussian distribution N (0, 1) as the values of the vertices. In the “lower-ranks” group, we uniformly select the TN-ranks from the interval [1, 4] randomly, while in the “higher-ranks” group, we increase the rank interval to [5, 8]. This ensures that the ranks would be larger than the dimensions of the tensor modes. This configuration aims to simulate the scenario of the over-determined ranks, which commonly occurs in practice for high-order TNs but has received limited attention in existing works. Settings. In the experiment, we compare various rank-adaptive TR decomposition methods. These methods include TR-SVD, TR-rSVD, TR-ALSAR, TR-BALS and TR-BALS2 (Zhao et al., 2016), TR-LM (Alg. 2 and Alg. 3) (Mickelin & Karaman, 2020), TRAR (Sedighin et al., 2021). Additionally, the TTOpt algorithm (Sozykin et al., 2022) with ranks 9 equaling 1, 2 is also employed as a baseline. The purpose of including these methods is to assess the effectiveness of the “local-searching” scheme utilized in TnALE (our proposed method) and determine its superiority in comparison to existing approaches. In more detail, for TR-SVD, TR-rSVD, TR-ALSAR, TR-BALS, and TR-BALS2 (Zhao et al., 2016), the available codes have been used.10 In order to achieve a larger Eff. value, we adjust the parameters tol and MaxIter to ensure the value of RSE is less than but close to 10−4. For TR-LM (Alg. 2 and Alg. 3) (Mickelin & Karaman, 2020), we use the available codes 11 with default parameter settings. However, we adjust the value of prec to obtain a larger Eff. value. For TRAR (Sedighin et al., 2021), we replace the TR-ALS (Wang et al., 2017) in Algorithm 1 of Mickelin & Karaman (2020) with the same decomposition method used in TTOpt. This modification is necessary because the initialization method of TR-ALS is not suitable for the case of higher ranks. Regarding TTOpt (Sozykin et al., 2022), we employ the same objective function as used in the TN-PS experiment, with the trade-off parameter λ = 200. For the lower ranks group, the rank searching range is set to [1, 7], while for the higher ranks group, the range is extended to [1, 10]. During the initialization 9Here the ranks are tuning parameters in the TTOpt algorithm. 10https://qibinzhao.github.io/ 11https://github.com/oscarmickelin/tensor-ring-decomposition 24TnALE: Solving Tensor Network Structure Search with Fewer Evaluations phase, we i.i.d. draw samples from Gaussian distribution N (0, 1) to generate the values of core tensors. For the proposed method TnALE, we set the rank-related radius r1 = 3, r2 = 2 for the higher ranks group and r1 = 2, r2 = 1 for lower ranks group. The number of iterations in the initialization phase is set to 1, the number of iterations in the searching phase is set to 30, and the number of round-trips of ALE is set to 1 throughout the experiments. Other parameters of TnALE are set the same as TTOpt. For TNGA, we set the population in each generation to be 60. The searching ranges and the initialization scheme of core tensors are similar to TTOpt. The other parameters of TNGA are set the same as the TN-PS experiment. For TNLS, we set the sample numbers in each local sampling stage to be 60 and c1 = 0.9, and the other parameters are set the same as in TTOpt. The success condition for all approaches in the experiment is set as RSE ≤ 10−4 and Eff. ≥ 1. If an approach fails to meet these criteria, it is considered a failure in rank selection. 0 200 400 600 800Evaluations -5 -4 -3 -2 -1log Objective Higher ranks TNLSTnALE 0 200 400 600Evaluations -7 -6 -5 -4 -3 -2 -1log Objective Lower ranks TNLSTnALE Figure 9.Objective (in the log form) with varying the number of evaluations. Lower ranks Higher ranks 0 50 100 150 200Running time (x100s) TNGA TNLS TnALE Figure 10.Running time in TN-RS experiment Results. Based on the results presented in Table 7 and Table 8, it can be observed that in the lower rank regime, TR-BALS, TR-LM (Alg. 2), TTOpt, TNGA, TNLS, and TnALE (ours) are able to successfully select the optimal TR-ranks as indicated by Eff.≥ 1 and RSE≤ 10−4. However, in the higher rank regime, only TTOpt, TNGA, TNLS, and TnALE (ours) are able to find the optimal ranks. In terms of the number of evaluations, TnALE (ours) outperforms TNGA, TNLS, and TTOpt, requiring the fewest evaluations while still achieving successful rank selection. This highlights the superiority of TnALE in solving the TN-RS problem efficiently. Furthermore, the running time comparison in Figure 10 demonstrates that TnALE saves a significant amount of time compared to TNGA and TNLS, primarily due to its lower number of evaluations. This further emphasizes the advantage of TnALE in scenarios where computational resources are limited. In Figure 9, the averaged log objective curves of TNLS and TnALE with varying evaluation numbers are illustrated. It can be observed that TnALE exhibits a faster descending trend and achieves lower objective values given the same number of evaluations compared to TNLS. This demonstrates the practical advantage of TnALE, particularly in scenarios with restricted computational resources. C.4. Details for the experiment of knowledge transfer. Goal. In this experiment, the goal is to investigate the acceleration effect of TnALE when employing the knowledge transfer trick. 25TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Table 7.Experimental results of TN-RS (rank selection) in 8-th order TR topology under the ”lower ranks” group. In the first column of the table, A, B, C, D, E (Data) and their corresponding vectors (Rank grt) represent the five generated synthetic tensors and the TN-ranks of these five tensors. The item Rank est indicates the specific value of the TN-ranks learned by the corresponding method under the constraint RSE ≤ 10−4, and Time (s) or [#Eva.] indicates the running time or the number of evaluations that the method required. Methods TR-SVD TR-rSVD TR-ALSAR Data[Rank grt] Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) A [3 4 2 3 1 3 4 2] 0.45 / 8.55E-13 [3 8 4 6 2 6 3 1] 0.0029 0.51 / 0.0013 [3 6 4 6 2 6 3 1] 0.0038 0.08 / 2.43E-05 [11 7 7 7 10 14 8 10] 0.5959 B [3 4 4 2 2 1 1 4] 0.23 / 4.45E-05 [3 9 11 6 6 3 3 1] 0.0081 0.37 / 0.0146 [3 6 6 6 6 3 3 1] 0.0043 0.79 / 4.02E-12 [4 4 4 2 2 2 3 3] 0.0324 C [2 4 2 3 3 1 4 4] 0.29 / 3.55E-05 [3 9 8 4 8 4 3 1] 0.002812 0.46 / 0.0210 [3 6 6 4 6 3 3 1] 0.0039 0.94 / 3.55E-05 [4 4 2 1 2 2 3 4] 0.0333 D [1 4 1 3 4 2 1 1] 1.13 / 4.78E-05 [1 2 1 3 4 2 1] 0.0084 1.13 / 4.78E-05 [1 2 1 3 4 2 1 1] 0.0055 0.64 / 2.29E-11 [1 3 3 4 4 2 2 1] 0.0233 E [4 1 4 2 3 2 1 1] 1.17 / 2.04E-13 [3 1 3 2 3 2 1 1] 0.0043 1.17 / 3.13E-13 [3 1 3 2 3 2 1 1] 0.0059 0.78 / 1.45E-11 [3 3 3 2 3 2 2 1] 0.0205 Methods TR-BALS TR-BALS2 TRAR Data Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) Eff. / RSE Rank est [#Eva.] A 1.00 / 5.00E-13 [3 4 2 3 1 3 4 2] 0.0146 0.45 / 6.90E-13 [3 8 4 6 2 6 3 1] 0.0188 0.48 / 3.79E-11 [4 4 3 4 5 4 4 3] 69 B 1.07 / 6.07E-13 [3 4 4 2 2 1 1 3] 0.0127 0.20 / 1.03E-05 [3 9 12 6 6 3 3 5] 0.0174 0.61 / 3.17E-08 [3 4 4 4 4 2 4 3] 37 C 1.38 / 3.55E-05 [2 4 2 1 2 1 3 4] 0.0189 0.26 / 3.57E-05 [3 9 8 4 8 4 3 6] 0.0151 0.65 / 3.55E-05 [3 4 3 5 3 3 3 4] 68 D 1.13 / 4.78E-05 [1 2 1 3 4 2 1 1] 0.021 1.13 / 4.78E-05 [1 2 1 3 4 2 1 1] 0.0487 0.41 / 5.55E-05 [4 5 3 3 3 3 3 2] 22 E 1.17 / 6.34E-13 [3 1 3 2 3 2 1 1] 0.0303 1.17 / 9.10E-13 [3 1 3 2 3 2 1 1] 0.0212 0.59 / 2.96E-11 [5 2 3 2 3 2 2 3] 36 Methods TR-LM (Alg. 3) TR-LM (Alg. 2) TTOpt (R = 1) Data Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) Eff. / RSE Rank est [#Eva.] A 0.40 / 4.22E-13 [6 3 1 3 2 6 8 4] 0.0222 1.00 / 2.86E-14 [3 4 2 3 1 3 4 2] 0.2736 1.00 / 9.41E-07 [3 4 2 3 1 3 4 2] 98 B 0.46 / 2.05E-06 [6 7 3 1 3 2 2 6] 0.0204 1.07 / 1.08E-14 [3 4 4 2 2 1 1 3] 0.2402 1.07 / 2.30E-06 [3 4 4 2 2 1 1 3] 140 C 1.38 / 3.55E-05 [2 4 2 1 2 1 3 4] 0.0227 1.38 / 3.55E-05 [2 4 2 1 2 1 3 4] 0.2503 1.11 / 3.55E-05 [2 4 2 1 2 2 4 4] 56 D 1.13 / 4.78E-05 [1 2 1 3 4 3 1 1] 0.026 1.13 / 4.78E-05 [1 2 1 3 4 2 1 1] 0.2407 1.06 / 4.82E-05 [1 2 1 3 4 2 1 2] 91 E 1.17 / 5.62E-14 [3 1 3 2 3 2 1 1] 0.022 1.17 / 5.62E-14 [3 1 3 2 3 2 1 1] 0.2454 1.17 / 6.61E-11 [3 1 3 2 3 2 1 1] 133 Methods TTOpt (R = 2) TTOpt (R = 3) TNGA Data Eff. / RSE Rank est [#Eva.] Eff. / RSE Rank est [#Eva.] Eff. / RSE Rank est [#Eva.] A 1.00 / 5.00E-06 [3 4 2 3 1 3 4 2] 518 1.00 / 8.93E-05 [3 4 2 3 1 3 4 2] 1533 1.00 / 9.98E-05 [3 4 2 3 1 3 4 2] 480 B 1.02 / 4.86E-07 [3 4 4 2 2 2 1 3] 336 1.02 / 3.72E-06 [3 4 4 2 2 2 1 3] 735 1.07 / 9.95E-05 [3 4 4 2 2 1 1 3] 660 C 1.02 / 3.56E-05 [2 5 2 2 3 1 3 5] 154 1.00 / 9.45E-05 [2 5 2 2 3 2 3 4] 273 1.11 / 9.95E-05 [2 4 2 1 2 2 4 4] 600 D 1.06 / 1.10E-08 [1 3 1 3 4 2 1 1] 196 1.00 / 4.87E-05 [1 2 1 3 4 2 1 3] 483 1.06 / 9.98E-05 [1 2 1 3 4 2 1 2] 600 E 1.03 / 7.94E-06 [3 1 3 2 3 3 1 1] 364 1.17 / 3.14E-11 [3 1 3 2 3 2 1 1] 1071 1.17 / 9.92E-05 [3 1 3 2 3 2 1 1] 420 Methods TNLS TnALE (ours) Data Eff. / RSE Rank est [#Eva.] Eff. / RSE Rank est [#Eva.] A 1.00 / 9.99E-05 [3 4 2 3 1 3 4 2] 600 1.00 / 9.98E-05 [3 4 2 3 1 3 4 2] 66 B 1.07 / 9.97E-05 [3 4 4 2 2 1 1 3] 420 1.07 / 9.98E-05 [3 4 4 2 2 1 1 3] 99 C 1.11 / 9.99E-05 [2 4 2 1 2 2 4 4] 420 1.11 / 9.95E-05 [2 4 2 1 2 2 4 4] 99 D 1.06 / 9.97E-05 [1 2 1 3 4 2 1 2] 480 1.06 / 9.98E-05 [1 2 1 3 4 2 1 2] 69 E 1.17 / 9.92E-05 [3 1 3 2 3 2 1 1] 540 1.17 / 9.93E-05 [3 1 3 2 3 2 1 1] 63 Data generation. We re-use the data from the lower ranks group of the TN-RS experiment. Settings. In this experiment, we employ two variations of TnALE: one incorporates a knowledge transfer trick, while the other does not. Both methods share the same parameter settings, which are listed as follows: the rank searching range is set to [1, 7], the trade-off parameter λ is set to 200, the rank-related radius r2 = 2. Additionally, we set the number of iterations in the initialization phase to 0 and the number of iterations in the searching phase to 30. For the number of round-trips of ALE, we set it to 1. The Adam optimizer is utilized with a learning rate of 0.001, and the core tensors are initialized using Gaussian distribution N(0, 1). Moreover, both methods are initialized with the same TN-ranks. Results. Figure 11 displays the objective curves as a function of running time. From the figures, it is evident that both methods start with identical log objectives but exhibit significant differences in their descent patterns. In comparison to TnALE without the knowledge transfer trick, TnALE with the knowledge transfer trick showcases a rapid decline in objectives, achieving approximately twice or even nearly five times faster progress than its counterpart. C.5. Details for the experiment of TGP (w.r.t. Table 3). Goal. In this experiment, our goal is to utilize the proposed method TnALE to compress the learnable parameters of the TGP (Izmailov et al., 2018). Data generation. In this task, we select three univariate regression datasets from the UCI and LIBSVM archives. The datasets chosen are as follows: The Combined Cycle Power Plant (CCPP)12 dataset comprises 9569 data points collected from a power plant. It consists of 4 features and a single response. The MG 13 dataset contains 1385 data points with 6 12https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant 13https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/regression.html#mg 26TnALE: Solving Tensor Network Structure Search with Fewer Evaluations Table 8.Experimental results of TN-RS (rank selection) in 8-th order TR topology under the ”higher ranks” group. In the first column of the table, A, B, C, D, E (Data) and their corresponding vectors (Rank grt) represent the five generated synthetic tensors and the TN-ranks of these five tensors. The item Rank est indicates the specific value of the TN-ranks learned by the corresponding method under the constraint RSE ≤ 10−4, and Time (s) or [#Eva.] indicates the running time or the number of evaluations that the method required. Methods TR-SVD TR-rSVD TR-ALSAR Data[Rank grt] Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) A [8 8 8 5 7 6 8 8] 0.16 / 9.40E-05 [3 9 27 38 27 9 3 1] 0.0174 0.16 / 9.40E-05 [3 9 27 38 27 9 3 1] 0.0238 1.11 / 0.0172 [7 7 7 6 6 6 8 8] 11.2686 B [6 5 7 7 6 5 6 5] 0.12 / 4.30E-05 [3 9 27 35 26 9 3 1] 0.0136 0.11 / 1.13E-28 [3 9 27 35 27 9 3 1] 0.0361 0.82 / 0.0133 [7 5 6 8 6 6 8 6] 10.536 C [8 7 7 8 7 5 8 7] 0.12 / 8.32E-05 [3 9 27 51 27 9 3 1] 0.0378 0.12 / 8.32E-05 [3 9 27 51 27 9 3 1] 0.0308 0.71 / 0.0094 [9 8 6 17 6 7 9 8] 16.2148 D [6 6 6 8 6 7 6 5] 0.13 / 9.64E-05 [3 9 26 38 26 9 3 1] 0.0092 0.12 / 5.32E-05 [3 9 27 38 27 9 3 1] 0.0308 0.63 / 9.62E-05 [9 8 8 8 7 9 7 7] 0.6589 E [6 6 6 6 5 6 6 6] 0.11 / 4.51E-05 [3 9 27 36 26 9 3 1] 0.0061 0.11 / 5.63E-05 [3 9 27 35 27 9 3 1] 0.0242 0.75 / 0.0298 [7 7 5 6 4 9 8 8] 11.5233 Methods TR-BALS TR-BALS2 TRAR Data Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) Eff. / RSE Rank est [#Eva.] A 0.03 / 6.04E-29 [28 20 19 26 44 89 51 39] 0.8749 0.16 / 8.39E-05 [3 9 27 39 27 9 3 1] 0.142 0.67 / 5.59E-14 [10 8 8 8 8 10 8 11] 76 B 0.57 / 9.25E-05 [6 6 10 9 9 7 9 6] 0.0941 0.12 / 9.89E-05 [3 9 27 35 26 9 3 1] 0.1608 0.71 / 1.82E-13 [8 5 7 7 7 7 6 9] 74 C 0.12 / 9.71E-05 [15 20 18 15 17 19 23 25] 0.2763 0.12 / 6.35E-05 [3 9 27 54 27 9 3 1] 0.1643 0.58 / 8.89E-14 [12 7 10 8 9 10 9 10] 143 D 0.19 / 8.18E-05 [9 15 17 22 10 16 15 10] 0.1773 0.12 / 9.75E-06 [3 9 27 40 26 9 3 1] 0.1704 0.57 / 3.75E-14 [11 6 7 8 8 9 7 10] 76 E 0.03 / 3.97E-05 [38 55 59 20 18 15 19 27] 0.446 0.11 / 6.40E-30 [3 9 27 36 27 9 3 1] 0.1749 0.62 / 2.05E-14 [10 6 7 7 7 8 6 9] 75 Methods TR-LM (Alg. 3) TR-LM (Alg. 2) TTOpt (R = 1) Data Eff. / RSE Rank est Time (s) Eff. / RSE Rank est Time (s) Eff. / RSE Rank est [#Eva.] A 0.16 / 9.40E-05 [3 9 27 38 27 9 3 1] 0.0257 0.16 / 2.87E-05 [3 9 27 39 27 9 3 1] 0.3318 1.00 / 3.65E-07 [8 8 8 5 7 6 8 8] 220 B 0.12 / 4.30E-05 [3 9 27 35 26 9 3 1] 0.001 0.15 / 8.36E-05 [3 1 3 9 26 25 25 9] 0.3336 1.00 / 1.52E-07 [6 5 7 7 6 5 6 5] 220 C 0.12 / 8.32E-05 [3 9 27 51 27 9 3 1] 0.0303 0.17 / 7.07E-05 [3 1 3 9 27 34 27 9] 0.3285 1.00 / 1.01E-06 [8 7 7 8 7 5 8 7] 150 D 0.13 / 9.64E-05 [3 9 26 38 26 9 3 1] 0.023 0.13 / 9.31E-05 [35 27 9 3 1 3 9 25] 0.3173 1.00 / 1.83E-06 [6 6 6 8 6 7 6 5] 150 E 0.11 / 4.51E-05 [3 9 27 36 26 9 3 1] 0.0299 0.13 / 9.38E-05 [20 26 9 3 1 3 9 26] 0.3845 1.00 / 5.01E-07 [6 6 6 6 5 6 6 6] 150 Methods TTOpt (R = 2) TTOpt (R = 3) TNGA Data Eff. / RSE Rank est [#Eva.] Eff. / RSE Rank est [#Eva.] Eff. / RSE Rank est [#Eva.] A 1.00 / 6.49E-07 [8 8 8 5 7 6 8 8] 540 1.00 / 5.88E-07 [8 8 8 5 7 6 8 8] 1710 1.00 / 9.99E-05 [8 8 8 5 7 6 8 8] 1020 B 1.00 / 1.61E-07 [6 5 7 7 6 5 6 5] 1060 1.00 / 4.00E-07 [6 5 7 7 6 5 6 5] 2670 1.00 / 9.94E-05 [6 5 7 7 6 5 6 5] 660 C 1.00 / 1.32E-06 [8 7 7 8 7 5 8 7] 780 1.00 / 9.23E-07 [8 7 7 8 7 5 8 7] 1740 1.00 / 9.95E-05 [8 7 7 8 7 5 8 7] 1380 D 1.00 / 3.40E-07 [6 6 6 8 6 7 6 5] 540 1.00 / 1.67E-06 [6 6 6 8 6 7 6 5] 1710 1.00 / 9.93E-05 [6 6 6 8 6 7 6 5] 1020 E 1.00 / 8.02E-13 [6 6 6 6 5 6 6 6] 840 1.00 / 1.99E-07 [6 6 6 6 5 6 6 6] 1740 1.00 / 9.94E-05 [6 6 6 6 5 6 6 6] 420 Methods TNLS TnALE (ours) Data Eff. / RSE Rank est [#Eva.] Eff. / RSE Rank est [#Eva.] A 1.00 / 9.97E-05 [8 8 8 5 7 6 8 8] 540 1.00 / 9.96E-05 [8 8 8 5 7 6 8 8] 115 B 1.00 / 9.92E-05 [6 5 7 7 6 5 6 5] 720 1.00 / 9.91E-05 [6 5 7 7 6 5 6 5] 85 C 1.00 / 9.91E-05 [8 7 7 8 7 5 8 7] 480 1.00 / 1.00E-04 [8 7 7 8 7 5 8 7] 150 D 1.00 / 9.93E-05 [6 6 6 8 6 7 6 5] 600 1.00 / 9.92E-05 [6 6 6 8 6 7 6 5] 160 E 1.00 / 9.92E-05 [6 6 6 6 5 6 6 6] 600 1.00 / 9.93E-05 [6 6 6 6 5 6 6 6] 85 features and a single response. The Protein14 dataset consists of 45730 instances with 9 attributes and a single response. For each of the datasets, we begin by randomly selecting 80% of the data for training purposes, while the remaining 20% is reserved for testing. Subsequently, we standardize the training and testing sets respectively by removing the mean and scaling them to have unit variance. In the case of the CCPP dataset, we opt to use 12 inducing points on each feature, resulting in an order-4 tensor with dimensions of 12 × 12 × 12 × 12. For the MG dataset, we choose 8 inducing points, which leads to an order-6 tensor with dimensions of 8 × 8 × 8 × 8 × 8 × 8. Lastly, for the Protein dataset, we choose 4 inducing points, generating an order-9 tensor with dimensions of 4 × 4 × 4 × 4 × 4 × 4 × 4 × 4 × 4. Across all datasets, we set the TT-ranks for the TGP (Izmailov et al., 2018) algorithm to 10. Settings. In the comparison of methods, we employ the same objective function as used in the TN-PS experiment. Additionally, we set specific values for certain parameters, λ = 1 × 105, 1 × 107, 1 × 103 for CCPP, MG and Protein, respectively. Moreover, the following settings are common for all the methods being compared: the rank searching range, the learning rate of Adam, and the variance of the Gaussian distribution for core tensors initialization are set from 1 to 14, 0.001, and 0.01, respectively. For the TNGA method, we set the maximum number of generations to 30. The population in each generation is set to be 150, 190, and 300 for the TT variational mean of CCPP, MG, and Protein regression tasks. The elimination rate is set at 30% and the reproduction number is set to 1. Additionally, we assign α = 20 and β = 1. The chance for each gene to mutate after the recombination is 30%. For TNLS, the maximum iteration is limited to 20, and the tuning parameters c1 = 0.9, c2 = 0.9. For the TT variational mean of CCPP, MG, and Protein regression tasks, we determined the number of samples in the local sampling stage to be 150, 300, and 300 respectively. For the proposed method TnALE, we consistently use the rank-related radius r1 = 3 and r2 = 2. In addition, we specifically designate the number of iterations in the initialization phase as 2 and the number of iterations in the searching phase as 30. Furthermore, we configure the number of round-trips in ALE to be 1. 14https://archive.ics.uci.edu/ml/datasets/Physicochemical+Properties+of+Protein+Tertiary+Structure 27TnALE: Solving Tensor Network Structure Search with Fewer Evaluations 0 500 1000Running time (s) -6-4-20 Data E 0 500 1000 1500Running time (s) -6-4-20 Data D 0 1000 2000Running time (s) -6-4-20 Data C 0 1000 2000Running time (s) -6-4-20 Data B 0 1000 2000Running time (s) -6-4-20 Data A logObjective Without-Knowledge TransferWith-Knowledge Transfer Figure 11.Objective (in the log form) curves with running time In order to achieve more compact representations, we apply the TN-PS algorithms, which consist of TNGA, TNLS, and the proposed TnALE to TGP. The process involves training an initial TGP model with predefined TT-ranks and obtaining the TT representation of the variational mean. Subsequently, the TN-PS algorithms are employed to search for alternative structures that have a reduced number of parameters for the TT variational mean. Upon completion of the TN-PS algorithms, we reintegrate the reparameterized variational mean back into the original TGP model for inference. The performance is evaluated by measuring the mean squared error (MSE) of the regression tasks conducted on the test datasets. C.6. Details for the experiment of natural images compression (w.r.t. Table 4). Goal. In this experiment, we will investigate the effectiveness of the proposed TnALE method in tackling the TN-PS and TN-TS tasks associated with compressing natural images. Specifically, in TN-TS, our aim is to search for good TN-ranks and topologies for compressing images. Data generation. For this experiment, we select 4 natural images from the BSD500 dataset (Arbelaez et al., 2010) 15 at random, as shown in Figure 5. The selected images are first converted to grayscaled images of size 256 × 256 using the ”rgb2gray” and “resize” functions in Matlab, then scaled to the range of [0, 1]. Finally, we apply the Matlab function ”reshape” directly to the preprocessed images to represent them as order-8 tensors of size 4 × 4 × 4 × 4 × 4 × 4 × 4 × 4. Settings. In the TN-PS task of the experiment, we use the same objective function as in the TN-PS experiment and set the tuning parameter λ = 5. The rank searching range, the learning rate of Adam, and the variance of the Gaussian distribution for core tensors initialization are set from 1 to 14, 0.01, and 0.1, respectively. For TNLS, we set the maximum number of iterations to 20, and tuning parameters c1 = 0.95, c2 = 0.9, and the number of samples in the local sampling stage to 150. In TNGA, the maximum number of generations is set to 30, with a population of 300 per generation. The elimination is set at 10% and the reproduction number is set to 1. We also set α = 25, β = 1, and establish a 30% chance for each gene to mutate following the recombination process. Regarding the proposed method TnALE, we set the rank-related radius as r1 = 3 and r2 = 2. We also set the number of iterations in the initialization phase to 1 and the number of iterations in the searching phase to 30. Finally, we set the number of round-trips of ALE to 1. In the TN-TS task of the experiment, we use the same objective function, the learning rate of Adam, and the variance of the Gaussian distribution for core tensors initialization as in the TN-PS part, but set the rank searching range from 1 to 4. For TNLS, we set the maximum number of iterations to 20, tuning parameters c1 = 0.99, and the number of samples in the local sampling stage to 100. For the parameter settings of TNGA, we only change the population number to 100 compared to the TN-PS part. For the proposed method TnALE, we set the rank-related radius r2 to 1 and the number of iterations in the initialization phase to 0, while the number of iterations in the searching phase to 30. The number of round-trips of ALE is also set to 1. For Greedy, we set the RSE threshold to the same value as the result RSE of the proposed method TnALE. 15https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300/html/dataset/images.html 28",
      "meta_data": {
        "arxiv_id": "2304.12875v3",
        "authors": [
          "Chao Li",
          "Junhua Zeng",
          "Chunmei Li",
          "Cesar Caiafa",
          "Qibin Zhao"
        ],
        "published_date": "2023-04-25T14:45:59Z",
        "pdf_url": "https://arxiv.org/pdf/2304.12875v3.pdf",
        "github_url": "https://github.com/oscarmickelin/tensor-ring-decomposition"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes TnALE (Alternating Local Enumeration), a novel algorithm that significantly reduces the computational cost for Tensor Network Structure Search (TN-SS) compared to state-of-the-art methods like TNLS. It addresses the challenge of combinatorial explosion and NP-hardness in selecting good TN models (ranks, topology, permutations). The authors also provide the first convergence analysis for both TNLS and TnALE, proving linear convergence up to a constant under discrete convexity assumptions, and rigorously demonstrating TnALE's superior evaluation efficiency (ideally O(KR) vs. TNLS's Ω(2K) evaluations). Experimental results confirm TnALE's ability to find practically good TN structures with vastly fewer evaluations.",
        "methodology": "TnALE improves upon TNLS by replacing its random sampling with Alternating Local Enumeration (ALE). It iteratively updates each structure-related variable (TN-ranks 'r' and graph 'G') by enumerating all its possible values within a defined neighborhood while keeping other variables fixed. This alternating enumeration proceeds in a 'round-trip' manner (forward and backward through variables). The core intuition is to avoid combinatorial explosion and ensure a non-increasing objective function value. The method also incorporates 'knowledge transfer' tricks, such as reusing optimized core tensors from lower-rank structures for higher-rank initializations and using linear interpolation for objective estimation to accelerate evaluation, especially during an initial phase with a larger search radius. Theoretically, the paper re-defines discrete analogues for fundamental concepts like finite gradient, strong convexity, and smoothness to analyze the algorithm's convergence and evaluation efficiency. It also draws a connection between ALE and TT-cross approximation, suggesting that the low-rank structure of the optimization landscape contributes to TnALE's efficiency.",
        "experimental_setup": "The evaluation of TnALE involved both synthetic and real-world datasets across various TN-SS sub-problems. For synthetic data, TN-PS (permutation selection) was tested on TR (order-4, 6, 8), PEPS (order-6), HT (order-6), MERA (order-8), and TW (order-5) topologies with randomly generated ranks and permutations. TN-RS (rank selection) was tested on order-8 TR tensors with 'lower-ranks' ([1,4]) and 'higher-ranks' ([5,8]) configurations. For real-world data, TnALE was applied to compress learnable parameters of Tensorial Gaussian Process (TGP) models using three regression datasets (CCPP, MG, Protein) and to compress natural images (from BSD500 dataset) in TN-PS and TN-TS tasks. Comparison methods included TNLS, TNGA, TTOpt, TR-SVD, TR-BALS, TR-LM, TRAR, and a 'Greedy' method. The objective function used was a combination of compression ratio and relative squared error (RSE), with Adam optimizer for inner minimization. Performance was measured by the number of objective function evaluations, RSE, and an Eff. index (ratio of parameter numbers).",
        "limitations": "The primary limitation of TnALE is its susceptibility to local convergence. Empirically, it was found that TnALE can easily get trapped in multiple local minima (especially noted in TN-TS experiments and with MERA data), leading to suboptimal compression ratios. This is contrasted with stochastic methods like TNGA and TNLS, which appear to be better at avoiding such local minima due to their inherent randomness. Another limitation is that the identifiability of the proposed method for TN-SS in the presence of noise was not investigated.",
        "future_research_directions": "Future work will focus on addressing the local convergence issue to enable TnALE to avoid or escape poor local minima more effectively. Additionally, the identifiability of the proposed method for Tensor Network Structure Search in the presence of noise will be investigated."
      }
    },
    {
      "title": "SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective",
      "abstract": "Tensor network (TN) representation is a powerful technique for computer\nvision and machine learning. TN structure search (TN-SS) aims to search for a\ncustomized structure to achieve a compact representation, which is a\nchallenging NP-hard problem. Recent \"sampling-evaluation\"-based methods require\nsampling an extensive collection of structures and evaluating them one by one,\nresulting in prohibitively high computational costs. To address this issue, we\npropose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN),\nwhich allows us to efficiently solve the TN-SS problem from a regularized\nmodeling perspective, eliminating the repeated structure evaluations. To be\nspecific, by inserting a diagonal factor for each edge of the fully-connected\nTN, SVDinsTN allows us to calculate TN cores and diagonal factors\nsimultaneously, with the factor sparsity revealing a compact TN structure. In\ntheory, we prove a convergence guarantee for the proposed method. Experimental\nresults demonstrate that the proposed method achieves approximately 100 to 1000\ntimes acceleration compared to the state-of-the-art TN-SS methods while\nmaintaining a comparable level of representation ability.",
      "full_text": "SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective Yu-Bang Zheng1 Xi-Le Zhao2,* Junhua Zeng3,4 Chao Li4 Qibin Zhao4 Heng-Chao Li1 Ting-Zhu Huang2 1School of Information Science and Technology, Southwest Jiaotong University, China 2School of Mathematical Sciences, University of Electronic Science and Technology of China, China 3School of Automation, Guangdong University of Technology, China 4Tensor Learning Team, RIKEN Center for Advanced Intelligence Project (AIP), Japan zhengyubang@163.com, xlzhao122003@163.com, jh.zenggdut@gmail.com, chao.li@riken.jp qibin.zhao@riken.jp, hcli@home.swjtu.edu.cn, tingzhuhuang@126.com Abstract Tensor network (TN) representation is a powerful tech- nique for computer vision and machine learning. TN struc- ture search (TN-SS) aims to search for a customized struc- ture to achieve a compact representation, which is a chal- lenging NP-hard problem. Recent “sampling-evaluation”- based methods require sampling an extensive collection of structures and evaluating them one by one, resulting in pro- hibitively high computational costs. To address this issue, we propose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN), which allows us to efficiently solve the TN-SS problem from a regularized modeling per- spective, eliminating the repeated structure evaluations. To be specific, by inserting a diagonal factor for each edge of the fully-connected TN, SVDinsTN allows us to calculate TN cores and diagonal factors simultaneously, with the fac- tor sparsity revealing a compact TN structure. In theory, we prove a convergence guarantee for the proposed method. Experimental results demonstrate that the proposed method achieves approximately100∼1000 times acceleration com- pared to the state-of-the-art TN-SS methods while maintain- ing a comparable level of representation ability. 1. Introduction Tensor network (TN) representation, which aims to express higher-order data with small-sized tensors (called TN cores) by a specific operation among them, has gained significant attention in various areas of data analysis [1, 11, 25, 37], machine learning [5, 9, 27], computer vision [21, 30, 35, 36, 39], etc. By regarding TN cores as nodes and oper- *Corresponding author. ations as edges, a TN corresponds to a graph (called TN topology). The vector composed of the weights of all edges in the topology is defined as the TN rank. TN structure (in- cluding topology and rank) search (TN-SS) aims to search for a suitable TN structure to achieve a compact represen- tation for a given tensor, which is known as a challenging NP-hard problem [13, 18]. The selection of TN structure dramatically impacts the performance of TN representation in practical applications [12, 15, 17, 18]. Recently, there have been several notable efforts to tackle the TN-SS problem [12, 15, 17, 18]. But most of them adopt the “sampling-evaluation” framework, which requires sam- pling a large number of structures as candidates and con- ducting numerous repeated structure evaluations. For in- stance, for a tensor of size40 ×60 ×3 ×9 ×9 (used in Sec- tion 4.2), TNGA in [15] requires thousands of evaluations and TNALE in [18] requireshundreds of evaluations, where each evaluation entails solving an optimization problem to compute TN cores iteratively. Consequently, the computa- tional cost becomes exceedingly high. A meaningful ques- tion is whether we can optimize the TN structure simultane- ously during the computation of TN cores, thus escaping the “sampling-evaluation” framework and fundamentally ad- dressing the computationally consuming issue. In this paper, we introduce for the first time a regularized modeling perspective on solving the TN-SS problem. This perspective enables us to optimize the TN structure simul- taneously during the computation of TN cores, effectively eliminating the need for repetitive structure evaluations. To be specific, we propose a novel TN paradigm, termed as SVD-inspired TN decomposition (SVDinsTN) , by inserting diagonal factors between any two TN cores in the “fully- connected” topology (see Figure 1(b)). The intuition behind SVDinsTN is to leverage the sparsity of the inserted diago- arXiv:2305.14912v6  [cs.LG]  9 Apr 2024Run time (s) Compression Ratio  (5.64%, 104s)  (7.55%, 1366s)  (4.52%, 25050s)  (20.9%, 619s)  (26.9%, 3835s)  0%  30% 5×101 5×10 35×10 2 5×10 4 5×10 5 10% 20% A D G E F B (5.01%, 140200s)  (4.73%, 75510s)  C Ours A TNGreedy E TRALS G TNGA D FCTNALS F TNALE B TNLS C (c) Performance comparison of different methods (b) SVD-inspired TN decomposition X I1 I2 I3 I5 I4 G1 I1 G2 I2 G3 I3 G4 I4 G5 I5 S1\r2 S2\r3 S3đ4 S1\r5 S4đ5 S1\r3 S2\r5 S2\r4 4 S 1 \r 4 S 3 \r 5 S t , l G k Diagonal factors  TN cores  X I 1 I 2 G 1 I 1 G 2 I 2 S 1,2  (a) SVD Figure 1. (a) A graphical illustration of SVD. (b) A graphical illustration of SVD-inspired TN decomposition on a fifth-order tensor. (c) Comparison of the compression ratio ( ↓) and run time ( ↓) of different methods on a fifth-order light field image Knights, where the reconstruction error bound is set to 0.05, TRALS [38] and FCTNALS [41] are methods with pre-defined topologies, and TNGreedy [12], TNGA [15], TNLS [17], and TNALE [18] are TN-SS methods (please see more results in Table 3). nal factors to reveal a compact TN structure and utilize the TN cores (merged with the diagonal factors) to represent a given tensor. Based on SVDinsTN, we establish a regular- ized model, which updates the TN cores and diagonal fac- tors iteratively and imposes a sparse operator to induce the sparsity of the diagonal factors. In theory, we prove a con- vergence guarantee for the proposed method and establish an upper bound for the TN rank. In particular, we design a novel initialization scheme for the proposed method based on the upper bound. This initialization scheme enables the proposed method to overcome the high computational cost in the first several iterations, which is caused by the utiliza- tion of a “fully-connected” topology as the starting point. As a result, SVDinsTN is capable of capturing a customized TN structure and providing a compact representation for a given tensor in an efficient manner. In summary, we make the following three contributions. • We propose SVDinsTN, a new TN paradigm, that enables us to optimize the TN structure during the computation of TN cores, greatly reducing the computational cost. • In theory, we prove a convergence guarantee for the pro- posed method and establish an upper bound for the TN rank involved in SVDinsTN. The upper bound serves as a guide for designing an efficient initialization scheme. • Experimental results verify numerically that the proposed method achieves100∼1000 times acceleration compared to the state-of-the-art TN-SS methods with a comparable representation ability (see Figure 1(c)). 1.1. Related Works TN representation1 aims to find a set of small-sized TN cores to express a large-sized tensor under a given TN struc- 1We focus on TN representation in scientific computing and machine learning, while acknowledging its history of research in physics [7, 23, 31]. ture (including topology and rank) [4, 5, 31]. In the past decades, many works focused on TN representation with a fixed TN topology, such as tensor train (TT) decomposition with a “chain” topology [24], tensor ring (TR) decompo- sition with a “ring” topology [38], fully-connected tensor network (FCTN) decomposition with a “fully-connected” topology [41], etc. In addition, these works also presented various methods to optimize the TN cores, such as alternat- ing least square (ALS) [38], gradient descent (GD) [32, 34], proximal alternating minimization (PAM) [41, 42], etc. In contrast, SVDinsTN can reveal a compact TN structure for a given tensor, surpassing methods with pre-defined topolo- gies in terms of representation ability. TN structure search (TN-SS) aims to search for a suit- able or optimal TN structure, including both topology and rank, to achieve a compact representation for a given ten- sor [8, 12, 15, 17–20, 22, 26]. However, the majority of existing TN-SS methods follow the “sampling-evaluation” framework, which necessitates the use of heuristic search algorithms like the greedy algorithm [12], genetic algo- rithm [15], and alternating local enumeration algorithm [18] to sample candidate structures and subsequently evaluate them individually. Therefore, these methods inevitably suf- fer from prohibitively high computational costs due to the numerous repeated evaluations, each involving the iterative calculation of TN cores within an optimization problem. In contrast, SVDinsTN addresses the TN-SS problem from a regularized modeling perspective, thereby avoiding the re- peated structure evaluations and significantly reducing com- putational costs. 2. Notations and Preliminaries A tensor is a multi-dimensional array, and the number of dimensions (also called modes) of which is referred to asTable 1. Several operations and their interpretations. Operation Interpretation diag diag( X) returns a column vector formed from the elements on the main diagonal of X when the input variable is a diagonal matrix, and diag(x) returns a diagonal matrix whose main diagonal is formed from the elements of x when the input variable is a column vector. ones ones( I1, I2, ··· , IN ) returns an I1 × I2 × ··· ×IN tensor, whose elements are all equal to 1. zeros zeros( I1, I2, ··· , IN ) returns an I1 × I2 × ··· ×IN tensor, whose elements are all equal to 0. vec vec( X) returns a column vector by lexicographical reordering of the elements of X. the tensor order . In the paper, first-order tensors (vec- tors), second-order tensors (matrices), and Nth-order ten- sors are denoted by x ∈ RI1 , X ∈ RI1×I2 , and X ∈ RI1×I2×···×IN , respectively. We use ∥X∥F and ∥X∥1 to denote the Frobenius norm and ℓ1-norm of X, respectively. To simplify the explanation, we let x1:d denote the ordered set {x1, x2, ··· , xd}, KN denote the set{1, 2, ··· , N}, and TLN denote the set {(t, l)|1 ≤ t < l≤ N; t, l∈ N}. We next review several operations on tensors [41]. The generalized tensor transposition[41] is an operation that rearranges tensor modes. For example, an I1 × I2 × I3 × I4 tensor can be transposed into an I3 × I2 × I1 × I4 tensor, denoted by ⃗Xn with n = (3 , 2, 1, 4). We use ⃗Xn = permute(X, n) and X = ipermute( ⃗Xn, n) to de- note the corresponding transposition operation and its in- verse operation, respectively. The generalized tensor unfolding [41] is an operation that converts a tensor into a matrix by merging a group of tensor modes into the rows of the matrix and merging the remaining modes into the columns. For example, an I1 ×I2 ×I3 ×I4 tensor can be unfolded into anI1I3 ×I2I4 matrix. We use X[1,3;2,4] = GUnfold( X, (1, 3; 2, 4)) and X = GFold( X[1,3;2,4], (1, 3; 2, 4)) to denote the corre- sponding unfolding operation and its inverse operation, re- spectively. We also use X(2) to simply denote X[2;1,3,4] ∈ RI2×I1I2I4 , which is also called mode-2 unfolding. We use X(2) = Unfold( X, 2) and X = Fold(X(2), 2) to denote the corresponding mode-2 unfolding operation and its in- verse operation, respectively [14]. The tensor contraction [41] is an operation that obtains a new tensor by pairing, multiplying, and summing indices of certain modes of two tensors. For example, if a fourth- order tensor X ∈ RI1×I2×I3×I4 and a third-order tensor Y ∈RJ1×J2×J3 satisfy I2 = J1 and I4 = J2, then the tensor contraction between the 2nd and 4th modes ofX and the 1st and 2nd modes ofY yields a tensor Z = X ×1,2 2,4 Y ∈ RI1×I3×J3 . The elements of Z are calculated as follows: Z(i1, i3, j3)= XI2 i2=1 XI4 i4=1 X(i1, i2, i3, i4)Y(i2, i4, j3). In Table 1, we give the interpretations of the operations “diag”, “ones”, “zeros”, and “vec”. 2.1. Tensor Network In general, atensor network (TN)is defined as a set of small- sized tensors, known as TN cores, in which some or all modes are contracted according to specific operations [5]. The primary purpose of a TN is to represent higher-order data using these TN cores. By considering TN cores as nodes and operations between cores as edges, we define the graph formed by these nodes and edges as the TN topology. Additionally, we assign a non-negative integer weight to each edge to indicate the size of the corresponding mode of TN cores, and call the vector composed of these weights the TN rank. Consequently, a TN structure refers to a weighted graph, encompassing both the TN topology and TN rank. This paper focuses on only a class of TNs that employs tensor contraction as the operation among TN cores and adopts a simple graph as the TN topology. More particu- larly, when representing an Nth-order tensor X, this class of TNs comprises precisely N TN cores, each correspond- ing to one mode of X. A notable method is FCTN de- composition, which represents an Nth-order tensor X ∈ RI1×I2×···×IN by N small-sized Nth-order cores denoted by Gk ∈ RR1,k×R2,k×···×Rk−1,k×Ik×Rk,k+1×···×Rk,N for k ∈ KN [41]. In this decomposition, any two cores Gl and Gt for (t, l) ∈ TLN share an equal-sized mode Rt,l used for tensor contraction. We denote the above FCTN de- composition by X = FCTN(G1:N ) and define the FCTN rank as the vector (R1,2, R1,3, ··· , R1,N , R2,3, ··· , R2,N , ··· , RN−1,N ) ∈ RN(N−1)/2. According to the concept of tensor contraction, removing rank-one edges in the TN topology does not change the expression of the TN. This means that if any element in the FCTN rank is equal to one, the corresponding edge can be harmlessly eliminated from the “fully-connected” topology. For instance, a “fully- connected” topology with the rank (R1,2, 1, ··· , 1, R2,3, 1, ··· , 1, RN−2,N−1, RN−1,N ) can be converted into a “chain” topology with rank (R1,2, R2,3, ··· , RN−1,N ) in this manner. This fact can be formally stated as follows. Property 1 [16] There exists a one-to-one correspondence between the TN structure and FCTN rank. According to Property 1, we can search for a compact TN structure by optimizing the FCTN rank.3. An Efficient Method for TN-SS We propose an efficient method to solve the TN-SS problem from a regularized modeling perspective. Unlike the exist- ing “sampling-evaluation” framework, the main idea of the proposed method is to optimize the TN structure (the FCTN rank) simultaneously during the computation of TN cores, thereby eliminating the repetitive structure evaluations and greatly decreasing the computational cost. 3.1. SVDinsTN We start with the definition of the following SVDinsTN. Definition 1 (SVDinsTN) Let X ∈RI1×I2×···×IN be an Nth-order tensor such that X(i1, i2, ··· , iN ) = R1,2X r1,2=1 R1,3X r1,3=1 ··· R1,NX r1,N=1 R2,3X r2,3=1 ··· R2,NX r2,N=1 ··· RN−1,NX rN−1,N=1 S1,2(r1,2, r1,2)S1,3(r1,3, r1,3) ··· S1,N (r1,N , r1,N ) S2,3(r2,3, r2,3) ··· S2,N (r2,N , r2,N ) ··· SN−1,N (rN−1,N , rN−1,N ) G1(i1, r1,2, r1,3, ··· , r1,N ) G2(r1,2, i2, r2,3, ··· , r2,N ) ··· Gk(r1,k, r2,k, ··· , rk−1,k, ik, rk,k+1, ··· , rk,N ) ··· GN (r1,N , r2,N , ··· , rN−1,N , iN ), (1) where Gk ∈ RR1,k×R2,k×···×Rk−1,k×Ik×Rk,k+1×···×Rk,N for ∀k ∈ KN are Nth-order tensors and called TN cores, and St,l ∈ RRt,l×Rt,l for ∀(t, l) ∈ TLN are diagonal ma- trices. Then we call (1) an SVD-inspired TN decomposition (SVDinsTN) of X, denoted by X = STN(G, S), where G denotes {Gk|k ∈ KN } and S denotes {St,l|(t, l) ∈ TLN }. As shown in Figure 1(b), SVDinsTN includes both TN cores and diagonal factors, and can use the sparsity of diag- onal factors to reveal a compact TN structure and utilize TN cores (merged with diagonal factors) to represent a tensor. Remark 1 (SVDinsTN & SVD) As shown in Figure 1(a)- (b), SVDinsTN extends the “core & diagonal factor & core” form of SVD to higher-order cases, incorporating the idea of determining rank through non-zero elements in the diag- onal factor. In particular, SVDinsTN can degrade into SVD in second-order cases when TN cores satisfy orthogonality. Remark 2 (SVDinsTN & FCTN) SVDinsTN builds upon FCTN decomposition [41] but can reveal the FCTN rank. It achieves this by inserting diagonal factors between any two TN cores in FCTN decomposition and leveraging the number of non-zero elements in the diagonal factors to de- termine the FCTN rank. In particular, SVDinsTN can trans- form into a TN decomposition by merging the diagonal fac- tors into TN cores through the tensor contraction operation. 3.2. A Regularized Method for TN-SS We present an SVDinsTN-based regularized method, which updates TN cores and diagonal factors alternately, and im- poses a sparse operator to induce the sparsity of diagonal factors to reveal a compact TN structure. We consider anℓ1-norm-based operator for diagonal fac- tors S and Tikhonov regularization [10] for TN coresG. The ℓ1-norm-based operator is used to promote the sparsity ofS, and the Tikhonov regularization is used to constrict the fea- sible range of G. Mathematically, the proposed model can be formulated as follows: min G,S 1 2∥X −STN(G, S)∥2 F + µ 2 X k∈KN ∥Gk∥2 F + X (t,l)∈TLN λt,l∥St,l∥1, (2) where λt,l > 0 and µ >0 are regularization parameters. We use the PAM-based algorithm [2] to solve (2), whose solution is obtained by alternately updating    Gk =argmin Gk 1 2∥X −STN(G, S)∥2 F + µ 2 ∥Gk∥2 F + ρ 2∥Gk − ˆGk∥2 F , ∀k ∈ KN , St,l =argmin St,l 1 2∥X −STN(G, S)∥2 F +λt,l∥St,l∥1 + ρ 2∥St,l−ˆSt,l∥2 F , ∀(t, l) ∈ TLN , (3) where ρ >0 is a proximal parameter (we fix ρ = 0.001), and ˆGk and ˆSt,l are the solutions of the Gk-subproblem and St,l-subproblem at the previous iteration, respectively. 1) Update Gk for ∀k ∈ KN : Solving the Gk-subproblem requires fixing the other TN cores and diagonal factors. To address this, we use Mk to denote the matrix obtained by performing tensor contraction and unfolding operations on all diagonal factors and TN cores except Gk. Algorithm 1 presents a way to compute Mk. We can obtain X(k) = Gk(k)Mk. In this way, the Gk-subproblem can be rewritten as follows: min Gk(k) 1 2∥X(k) − Gk(k)Mk∥2 F + µ 2 ∥Gk(k)∥2 F + ρ 2∥Gk(k) − ˆGk(k)∥2 F . (4) The objective function of (4) is differentiable, and thus its solution can be obtained by Gk(k) = \u0000 X(k)MT k +ρ ˆGk(k) \u0001\u0000 MkMT k +(µ+ρ)I \u0001−1 . (5) 2) Update St,l for ∀(t, l) ∈ TLN : Solving the St,l- subproblem requires fixing the other diagonal factors and TN cores. In a similar fashion, we use Ht,l to denote theAlgorithm 1 Mk = STN \u0000 {Gq}N q=1, {St,l}t,l∈N 1≤t<l≤N , /Gk \u0001 . Input: Gq ∈ RR1,q×R2,q×···×Rq−1,q×Iq×Rq,q+1×···×Rq,N for ∀q ∈ KN and q ̸= k; St,l ∈ RRt,l×Rt,l for ∀(t, l) ∈ TLN ; and an index k ∈ KN . Initialization: a = (k + 1 :N, 1 :k). 1: for i = 1to k − 1 and i = k + 1to N − 1 do 2: for j = i + 1to N do 3: Let Gi = Gi ×1 i+1 Si,j. 4: end for 5: if i > kthen 6: Let Gi = Gi ×1 k Sk,i. 7: Let Gi = permute(Gi, (1 :k − 1, N, k: N − 1)). 8: end if 9: Let Gi = permute(Gi, a). 10: end for 11: Let Mk = Ga(1), m1 = 1, and n1 = 2. 12: for i = 1to N − 2 do 13: Let Mk = Mk ×m1,m2,···,mi n1,n2,···,ni Ga(i+1). 14: Let mj = j for j = 1, 2, ··· , i+ 1. 15: Let nj = 2 + (j − 1)(N − i) for j = 1, 2, ··· , i+ 1. 16: end for 17: Let Mk = permute(Mk, (2(N −k) + 1 : 2(N − 1), 1 : 2(N − k))). 18: Let c = zeros(1, N− 1) and d = zeros(1, N− 1). 19: for i = i to N − 1 do 20: Let c(i) = 2i and d(i) = 2i − 1 21: end for 22: Let Mk = GUnfold(Mk, c; d). Output: Matrix Mk ∈ R Qk−1 i=1 Ri,k QN i=k+1 Rk,i×QN i=1,i̸=k Ii. matrix obtained by performing tensor contraction and un- folding operations on all TN cores and diagonal factors ex- cept St,l. Algorithm 2 presents a way to compute Ht,l. We can obtain x = Ht,lst,l, where x = vec( X) and st,l = diag(St,l). Then, the St,l-subproblem can be rewrit- ten as follows: min st,l 1 2∥x−Ht,lst,l∥2 F +λt,l∥st,l∥1+ ρ 2∥st,l−ˆst,l∥2 F . (6) We use an alternating direction method of multipliers (ADMM) [6] to solve the St,l-subproblem, which can be rewritten as follows: min st,l,qt,l 1 2∥x−Ht,lqt,l∥2 F +λt,l∥st,l∥1+ ρ 2∥st,l−ˆst,l∥2 F s.t. st,l−qt,l =0, (7) where qt,l is an auxiliary variable. The augmented La- grangian function of (7) can be expressed as the following concise form: Lβt,l(st,l, qt,l, pt,l)= 1 2∥x−Ht,lqt,l∥2 F +λt,l∥st,l∥1 + ρ 2∥st,l−ˆst,l∥2 F + βt,l 2 \r\r\rst,l−qt,l+ pt,l βt,l \r\r\r 2 F , (8) Algorithm 2 Ht,l = STN \u0000 {Gk}N k=1, {Sp,q}p,q∈N 1≤p<q≤N , /St,l \u0001 . Input: Gk ∈ RR1,k×R2,k×···×Rk−1,k×Ik×Rk,k+1×···×Rk,N for ∀k ∈ KN ; Sp,q ∈ RRp,q×Rp,q for ∀(p, q) ∈ TLN , and (p, q) ̸= (t, l); and an index (t, l) ∈ TLN . 1: for i = 1to t − 1 and i = t + 1to N − 1 do 2: for j = i + 1to N do 3: Let Gi = Gi ×1 i+1 Si,j. 4: end for 5: end for 6: for j = t + 1to l − 1 do 7: Let Gt = Gt ×1 t+1 St,j. 8: end for 9: for j = l + 1to N do 10: Let Gt = Gt ×1 t+2 St,j. 11: end for 12: Let Gt = permute(Gt, (1 :t, t+ 2 :l, t+ 1, l+ 1 :N)). 13: Let Gt = Unfold(Gt, l) and Gl = Unfold(Gl, t). 14: Let Ht,l = zeros(QN k=1 Ik, Rt,l). 15: for i = 1to Rt,l do 16: Let Gt = Fold(Gt(i, :), l) and Gl = Fold(Gl(i, :), t). 17: Let Ht,l(:, i) = vec(FCTN({Gk}N k=1)). 18: end for Output: Matrix Ht,l ∈ R QN k=1 Ik×Rt,l. where pt,l is the Lagrangian multiplier and βt,l > 0 is the penalty parameter. Within the ADMM framework,qt,l, st,l, and pt,l can be solved by alternately updating    qt,l = argmin qt,l Lβt,l(st,l, qt,l, pt,l), st,l = argmin st,l Lβt,l(st,l, qt,l, pt,l), pt,l = pt,l + βt,l(st,l − qt,l). (9) That is,    qt,l = \u0002 HT t,lHt,l+βt,lI \u0003−1\u0002 HT t,lx+βt,lst,l+pt,l \u0003 , st,l =shrink \u0012ρˆst,l+βt,lqt,l−pt,l ρ+βt,l , λt,l ρ+βt,l \u0013 , pt,l =pt,l+βt,l(st,l−qt,l), (10) where shrink(a, b) = max(a − b, 0) + min(a + b, 0). We describe the pseudocode to optimize model (2) in Al- gorithm 3. Below, we present a brief analysis of the compu- tational complexity and provide a theoretical convergence guarantee for the developed algorithm. Computational complexity. For simplicity, we let the size of the Nth-order tensor X be I × I × ··· ×I and the initial rank be (R, R,··· , R) satisfied R ≤ I. The compu- tational cost involves updatingG and S, resulting in costs of O \u0000 N PN k=2 IkRk(N−k)+k−1+NI N−1R2(N−1)+N3IRN \u0001 and O \u0000 N2 PN k=2 IkRk(N−k)+k−1+N4IRN +N2IN R2\u0001 , respectively. Hence, the computational cost at each iteration is O \u0000 N2 PN k=2 IkRk(N−k)+k−1+N4IRN +N2IN R2\u0001 .Algorithm 3 PAM-based algorithm to optimize model (2). Input: A tensor X ∈RI1×I2×···×IN and a parameter γ. Initialization: Initialize St,l and Rt,l by the initialization scheme in Section 3.3 and let βt,l = 1for ∀(t, l) ∈ TLN ; let Gk = 1/√Ik ones(R1,k, R2,k, ··· , Rk−1,k, Ik, Rk,k+1, ··· , Rk,N ) for ∀k ∈ KN and µ = 1. 1: while not converged do 2: Let ˆX = X and λt,l = γ max(St,l)(ρ + βt,l). 3: Update Gk(k) by (5) and let Gk = Fold(Gk(k), k). 4: for i = 1to 5 do 5: Update qt,l, st,l, and pt,l by (10). 6: end for 7: Delete zero elements in st,l, let St,l = diag(st,l), and define the size of st,l as Rt,l. 8: Delete the corresponding dimensions of Gk and let X = STN(G, S). 9: Check the convergence condition: ∥X− ˆX∥F ∥ ˆX∥F < 10−5. 10: end while Output: Gk for ∀k ∈ KN , and St,l and Rt,l for ∀(t, l) ∈ TLN . Theorem 1 (Convergence guarantee) The sequence gen- erated by Algorithm 3, denoted by {G(s), S(s)}s∈N, con- verges to a critical point of the optimization problem (2). 3.3. Initialization Scheme SVDinsTN encounters high computational cost in the first several iterations if the TN rank Rt,l for ∀(t, l) ∈ TLN are initialized with large values. This is due to the adoption of a “fully-connected” topology as a starting point. To solve this challenge, we design a novel initialization scheme aimed at effectively reducing the initial values of the TN rank. We first give an upper bound for the TN rank, by which we then design an initialization scheme for the TN rankRt,l and diagonal factors St,l ∈ RRt,l×Rt,l for ∀(t, l) ∈ TLN . Theorem 2 Let X ∈RI1×I2×···×IN be an Nth-order ten- sor, then there exists an SVDinsTN (1) with the TN rank Rt,l ≤ min(rank(X(t)), rank(X(l))) for ∀(t, l) ∈ TLN . Theorem 2 indicates that min(rank(X(t)), rank(X(l))) can be the initial value of the TN rank Rt,l. For real-world data, this value is usually embodied by the rank of mode- (t, l) slices2 of X. Therefore, we initialize Rt,l and St,l by virtue of truncated SVD of mode- (t, l) slices of X, which consists of the following two steps. Step 1: We first calculate the mean of all mode- (t, l) slices of X and denote it by Xt,l. Then we perform SVD on Xt,l to obtain st,l ∈ Rmin(It,Il), whose elements are singu- lar values of Xt,l. Step 2: We first let st,l = shrink \u0000 st,l, γ max(st,l) |st,l|+10−16 \u0001 and delete zero elements in st,l. Then we let St,l = diag(st,l) 2Mode-(t, l) slices are obtained by fixing all but the mode- t and the mode-l indexes of a tensor [40]. and define the size of st,l as Rt,l. In practical applications, the shrink operation in Step 2 effectively reduces the initial value of Rt,l by projecting very small singular values in st,l to zero. As a result, the challenge of high computational costs in the first several it- erations of SVDinsTN can be effectively addressed(see Fig- ure 2 for a numerical illustration). 4. Numerical Experiments In this section, we present numerical experiments on both synthetic and real-world data to evaluate the performance of the proposed SVDinsTN. The primary objective is to vali- date the following three Claims: A: SVDinsTN can reveal a customized TN structure that aligns with the unique structure of a given tensor. B: SVDinsTN can greatly reduce time costs while achiev- ing a comparable representation ability to state-of-the- art TN-SS methods. Moreover, SVDinsTN can also sur- pass existing tensor decomposition methods with pre- defined topologies regarding representation ability. C: SVDinsTN can outperform existing tensor decomposi- tion methods in the tensor completion task, highlighting its effectiveness as a valuable tool in applications. 4.1. Experiments for Validating Claim A We conduct experiments to validate Claim A. Since real- world data lacks a true TN structure, we consider only syn- thetic data in this experiment. Data generation. We first randomly generate Gk for ∀k ∈ KN and St,l for ∀(t, l) ∈ TLN , whose elements are taken from a uniform distribution between 0 and 1. Then we obtain the synthetic tensor by X = STN(G, S). Experiment setting. We test both fourth-order tensors of size 16 × 18 × 20 × 22 and fifth-order tensors of size 14 ×16 ×18 ×20 ×22, and consider different kinds of TN structures. For each structure, we conduct 100 independent tests and regenerate the synthetic data to ensure reliable and unbiased results. The ability of the proposed SVDinsTN to reveal TN structure is measured by the success rate of the output structures, defined as ST/T ×100%, where T = 100 is the total number of tests and ST is the number of tests that accurately output the true TN structure. In all tests, the parameter γ is set to 0.0015. Table 2 presents the success rate of the output TN struc- tures obtained by the proposed SVDinsTN in 100 indepen- dent tests on fourth-order and fifth-order tensors. It can be observed that the proposed SVDinsTN consistently yields high success rates of over 95% in all test cases. Notably, in approximately half of the test cases, the success rates reach a perfect score of 100%. Moreover, it is also worth mentioning that in the test on fifth-order tensors, we con- sider two isomorphic topologies: the “ring” topology andTable 2. Performance of SVDinsTN on TN structure revealing under 100 independent tests. True structure (4th-order) G1 G2 G3 G4 I1 I3 I4 I2 4 3 2 2 2 3 G1 G2 G3 G4 I1 I3 I4 I2 3 4 4 3 2 G1 G2 G3 G4 I1 I3 I4 I2 4 3 2 3 G1 G2 G3 G4 I1 I3 I4 I2 3 4 4 G1 G2 G3 G4 I1 I3 I4 I2 3 4 2 Success rate 100% 100% 96% 95% 99% True structure (5th-order) G1 G2 G3 I1 G5 G4 I4I3 I2 I5 4 2 3 2 2 3 3 2 2 2 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 3 34 2 3 2 2 3 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 4 2 3 2 3 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 3 2 3 4 3 G1 G2 G3 I1 G5 G4 I4I3 I2 I5 Success rate 100% 98% 96% 97% 100% the “five-star” topology. These two topologies are both the “ring” topology (TR decomposition), but with differ- ent permutations: G1 → G2 → G3 → G4 → G5 → G1 and G1 →G3 →G5 →G2 →G4 →G1, respectively. It can be seen that despite the isomorphism, the proposed SVDinsTN can identify the correct permutation for each topology. 4.2. Experiments for Validating Claim B We conduct experiments to validate Claim B. We consider both real-world data and synthetic data, and use different methods to represent it in this experiment. Experiment setting. We test three light field data 3, named Bunny, Knights, and Truck, which are fifth-order ten- sors of size 40 × 60 × 3 × 9 × 9 (spatial height ×spatial width×color channel×vertical grid×horizontal grid). We employ six representative methods as the compared base- lines, including two methods with pre-defined topology: TRALS [38] and FCTNALS [41], and four TN-SS methods: TNGreedy [12], TNGA [15], TNLS [17], and TNALE [18]. We represent the test light field data by different methods and calculate the corresponding compression ratio (CR) to achieve a certain reconstruction error (RE) bound. The CR is defined as FG/FX × 100%, where FG is the number of elements of TN cores used to represent a tensor and FX is the number of total elements of the original tensor. The RE is defined as ∥X −˜X∥F /∥X∥F , where X is the original data and ˜X is the reconstructed data. In all tests, we select the parameter γ from the interval [10−7, 10−3]. Result analysis. Table 3 reports CR and run time of dif- ferent methods on fifth-order light field data. The results show that the proposed SVDinsTN achieves significantly lower CRs than TRALS and FCTNALS, which are methods with pre-defined topology. This indicates that SVDinsTN can obtain a more compact structure than the pre-defined one. Furthermore, while SVDinsTN requires the determi- nation of diagonal factors alongside TN cores, its iterative process generates progressively simpler structures, enhanc- 3http://lightfield.stanford.edu/lfs.html Table 3. Comparison of CR ( ↓) and run time ( ×1000s, ↓) of dif- ferent methods on light field data. Method RE bound: 0.01 RE bound: 0.05 RE bound: 0.1 CR Time CR Time CR Time Bunny TRALS [38] 60.5% 13.54 17.4% 0.471 5.31% 0.118 FCTNALS [41] 65.1% 13.08 20.9% 0.473 3.93% 0.041 TNGreedy [12] 26.1% 11.02 6.32% 1.021 2.34% 0.362 TNGA [15] 27.9% 1014 5.01% 180.3 2.25% 12.52 TNLS [17] 24.3% 1402 4.26% 63.70 2.16% 24.53 TNALE [18] 26.3% 144.5 4.52% 18.36 2.26% 3.064 SVDinsTN 22.4% 0.745 6.92% 0.029 2.66% 0.005 Knights TRALS [38] 74.7% 10.31 26.9% 3.835 9.15% 0.423 FCTNALS [41] 73.5% 12.35 20.9% 0.619 3.93% 0.014 TNGreedy [12] 32.1% 12.53 7.55% 1.366 3.50% 0.481 TNGA [15] 38.7% 912.9 5.01% 140.2 2.44% 12.52 TNLS [17] 27.3% 1286 4.73% 75.51 2.15% 5.320 TNALE [18] 27.6% 266.4 4.52% 25.05 2.10% 3.386 SVDinsTN 32.0% 1.548 5.64% 0.104 2.76% 0.019 Truck TRALS [38] 62.8% 17.62 22.6% 1.738 6.00% 0.090 FCTNALS [41] 69.3% 7.735 20.9% 2.953 3.93% 0.159 TNGreedy [12] 26.9% 6.676 7.26% 1.259 3.35% 0.488 TNGA [15] 27.9% 1029 5.01% 170.3 2.85% 14.83 TNLS [17] 26.4% 992.6 4.99% 119.8 2.57% 19.35 TNALE [18] 24.7% 239.3 5.77% 19.54 2.90% 5.160 SVDinsTN 23.5% 1.051 6.42% 0.152 2.83% 0.023 ing the computational efficiency. Consequently, SVDin- sTN demonstrates faster performance compared to TRALS and FCTNALS. Compared to the TN-SS methods, the pro- posed SVDinsTN achieves a substantial speed improve- ment while maintaining a comparable level ofCR. Remark- ably, SVDinsTN achieves an acceleration of approximately 100∼1000 times over TNGA, TNLS, and TNALE. This isTable 4. Comparison of CR ( ↓) and run time ( ×1000s, ↓) of SVDinsTN with different initializations on light field data Truck. Initialization RE bound: 0.01 RE bound: 0.05 RE bound: 0.1 CR Time CR Time CR Time Random 30.7% 1.203 8.17% 0.474 4.14% 0.208 Ours 23.5% 1.051 6.42% 0.152 2.83% 0.023 181s 149s 151s 3.35s 2.18s0 50 100 150 200 RE: 0.01 RE: 0.05 RE: 0.1 Truck  Run time  include “shrink”  exclude “shrink”  37.9s Figure 2. Comparison of the runtime in the first five iterations of SVDinsTN on light field dataTruck when including and excluding the shrink operation in our initialization scheme. because TNGreedy, TNGA, TNLS, and TNALE adopt the “sampling-evaluation” framework, necessitating a signifi- cant number of repeated structure evaluations. In contrast, SVDinsTN introduces a regularized modeling framework, requiring only a single evaluation. Impact of the initialization scheme. We analyze the impact of the initialization scheme. In Table 4, we report CR and run time of SVDinsTN with different initializations on light field data Truck. As observed, our initialization scheme achieves lower CRs compared to random initializa- tion, while maintaining higher efficiency. This corroborates that our initialization scheme can provide a favorable start- ing point and enhance computational efficiency. In partic- ular, even with random initialization, our method achieves significant acceleration compared to other TN-SS methods. We further analyze the impact of theshrink operation in our initialization scheme. In Figure 2, we present the run time comparison of the first five iterations of our method when including and excluding the shrink operation in our initial- ization scheme. As observed, the shrink operation in our initialization scheme enables our method to greatly reduce the computational costs in the first several iterations. Higher-order cases. We analyze whether the proposed SVDinsTN still performs well on higher-order tensors. We randomly generate 6th-, 8th-, and 10th-order tensors by us- ing the same procedure in Section 4.1. The size of each ten- sor mode is randomly selected from {5, 6, 7, 8}, the edge number of each TN is randomly selected from {6, 8, 10}, and the rank of each edge is randomly selected from{2, 3}. For each tensor order, we randomly generate 5 tensors. We compare SVDinsTN and baseline methods in terms of CR Table 5. Comparison of the CR ( ↓) and run time ( ×1000s, ↓) of different methods when reaching the RE bound of 0.01. The result is the average value of 5 independent experiments and “–” indicates “out of memory”. Method 6th-order 8th-order 10th-order CR Time CR Time CR Time TRALS [38] 1.35% 0.006 0.064% 0.034 – – FCTNALS [41] 2.13% 0.002 – – – – TNGreedy [12] 0.88% 0.167 0.016% 2.625 0.0008% 45.39 TNGA [15] 0.94% 3.825 0.024% 51.40 – – TNLS [17] 1.11% 0.673 0.038% 59.83 – – TNALE [18] 1.65% 0.201 0.047% 19.96 – – SVDinsTN 1.13% 0.002 0.016% 0.017 0.0007% 0.608 and run time when reaching the RE bound of 0.01, and show the results in Table 5. As observed, SVDinsTN is appli- cable to higher orders beyond 5, and even up to 10. The behind rational is the truncated SVD used in initialization restricts the initial values of the rank for each edge to a rel- atively small range, thus improving computational and stor- age efficiency (see Figure 2). As the iterations progress, the sparsity regularization in the model leads to progressively simpler learned structures, further boosting efficiency. 4.3. Experiments for Validating Claim C We conduct experiments to validate Claim C. We employ the proposed SVDinsTN to a fundamental application, i.e., tensor completion (TC), and compare it with the state-of- the-art tensor decomposition-based TC methods. Given an incomplete observation tensor F ∈RI1×I2×···×IN of X ∈ RI1×I2×···×IN , the proposed TC method first updatesG and S by Algorithm 3, and then updates the target tensor X as follows: X = PΩc((STN(G, S) +ρ ˆX)/(1 +ρ)) +PΩ(F), where Ω is the index set of the known elements, PΩ(X) is a projection operator that projects the elements in Ω to themselves and all others to zeros, ˆX is the result at the previous iteration, and the initial X is F. Experiment setting. We test four color videos4, named Bunny, News, Salesman, and Silent, which are fourth-order tensors of size 144 × 176 × 3 × 50 (spatial height×spatial width×color channel×frame). We employ six methods for comparison, named FBCP [37], TMac [29], TMacTT [3], TRLRF [33], TW [28], and TNLS 5 [17], respectively. We set the missing ratio (MR) to 90%, which is defined as the ratio of the number of missing elements to the total number of elements. We evaluate the reconstructed quality by the mean peak signal-to-noise ratio (MPSNR) computed across all frames. In all tests, the parameter γ is set to 0.0003. Result analysis. Table 6 reports MPSNR and run time 4http://trace.eas.asu.edu/yuv/ 5TNLS excels in the compression task; therefore, we use it as a repre- sentative TN-SS method for comparison.Table 6. Comparison of MPSNR (↑) and run time (in seconds, ↓) of different TC methods on color videos. Video FBCP [37] TMac [29] TMacTT [3] TRLRF [33] TW [28] TNLS [17] SVDinsTN MPSNR Time MPSNR Time MPSNR Time MPSNR Time MPSNR Time MPSNR Time MPSNR Time Bunny 28.402 1731.2 28.211 1203.5 29.523 453.76 29.163 486.76 30.729 1497.4 28.787 99438 32.401 691.33 News 28.234 1720.4 27.882 340.46 28.714 535.97 28.857 978.12 30.027 1426.3 29.761 37675 31.643 932.42 Salesman 29.077 1783.2 28.469 353.63 29.534 656.45 28.288 689.35 30.621 1148.7 30.685 76053 31.684 769.54 Silent 30.126 1453.9 30.599 316.21 30.647 1305.6 31.081 453.24 31.731 1232.0 28.830 98502 32.706 532.31 FBCP [37] TMac [29] TMacTT [3] TRLRF [33] TW [28] TNLS [17] SVDinsTN Ground truth 0 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 0 0.40.3 0.70.5 0.80.2 0.60.1 10.9 Figure 3. Reconstructed images and residual images obtained by different methods on the 25th frame of News. Here the residual image is the average absolute difference between the reconstructed image and the ground truth over R, G, and B channels. 0 200 400 600 800 1000 Iteration 0 0.05 0.1 0.15 0.2 0.25 0.3 Relative change Bunny 0 200 400 600 800 1000 Iteration 0 0.05 0.1 0.15 0.2 0.25 0.3 Relative change Silent Figure 4. Relative change curves with respect to the iteration num- ber on test color videosBunny and Silent. Here the relative change is defined as ∥X −ˆX∥F /∥ ˆX∥F , and X and ˆX are the results of the current iteration and its previous iteration. obtained by different TC methods. As observed, the pro- posed SVDinsTN consistently achieves the highestMPSNR values among all utilized TC methods across all test color videos. In Figure 3, we present the reconstructed images and their corresponding residual images at the 25th frame of News. We observe that the proposed SVDinsTN outper- forms the baseline methods in terms of visual quality, par- ticularly with respect to background cleanliness and local details (e.g. “dancer”) recovery. Numerical convergence. In Theorem 1, we provide a theoretical convergence guarantee for the proposed method. Here, we select color videos Bunny and Silent as examples to numerically verify the convergence. Figure 4 presents the relative change in the reconstructed color videos at each it- eration compared to their respective previous iterations. We observe that the values of the relative change achieved by the proposed method decrease and gradually tend to zero as the number of iterations increases. This justifies the numer- ical convergence of the proposed method. 5. Conclusion We propose a novel TN paradigm, called SVDinsTN, which enables us to solve the challenging TN-SS problem from a regularized modeling perspective. This perspective renders our model highly amenable to easy solutions, allowing us to leverage well-established optimization algorithms to solve the regularized model. As a result, the proposed method achieves about 100 ∼ 1000 times acceleration compared to the state-of-the-art TN-SS methods with a comparable rep- resentation ability. Besides, SVDinsTN demonstrates its ef- fectiveness as a valuable tool in practical applications. Limitations. In existing research on TN-SS, two chal- lenging issues remain open. One is the computationally consuming issue, and the other is the theoretical guaran- tee of the optimal TN structure. SVDinsTN addresses the computationally consuming issue. But the theoretical guar- antee of the optimal TN structure is still an open problem. Solving this issue will be the direction of our future work. Acknowledgements We would like to express our gratitude to Prof. Guillaume Rabusseau for his valuable assistance in correcting the ex- perimental results of “TNGreedy”.References [1] Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research , 15(1): 2773–2832, 2014. 1 [2] H ´edy Attouch, J´erˆome Bolte, Patrick Redont, and An- toine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequal- ity. Mathematics of Operations Research, 35(2):438– 457, 2010. 4 [3] Johann A Bengua, Ho N Phien, Hoang Duong Tuan, and Minh N Do. Efficient tensor completion for color image and video recovery: Low-rank tensor train. IEEE Transactions on Image Processing, 26(5):2466– 2479, 2017. 8, 9 [4] Andrzej Cichocki, Danilo P. Mandic, Anh Huy Phan, Cesar F. Caiafa, Guoxu Zhou, Qibin Zhao, and Lieven De Lathauwer. Tensor decompositions for signal pro- cessing applications: From two-way to multiway com- ponent analysis. IEEE Signal Processing Magazine , 32(2):145–163, 2015. 2 [5] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh Huy Phan, Qibin Zhao, and Danilo P. Mandic. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor de- compositions. Foundations and Trends® in Machine Learning, 9(4-5):249–429, 2016. 1, 2, 3 [6] Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers and Mathe- matics with Applications, 2(1):17–40, 1976. 5 [7] Silvano Garnerone, Thiago R. de Oliveira, and Paolo Zanardi. Typicality in random matrix product states. Physical Review A, 81:032336, 2010. 2 [8] Mehrdad Ghadiri, Matthew Fahrbach, Gang Fu, and Vahab Mirrokni. Approximately optimal core shapes for tensor decompositions. arXiv preprint arXiv:2302.03886, 2023. 2 [9] Ivan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eis- ert, and J. Ignacio Cirac. Expressive power of tensor- network factorizations for probabilistic modeling. In Advances in Neural Information Processing Systems , 2019. 1 [10] Gene H Golub, Per Christian Hansen, and Dianne P O’Leary. Tikhonov regularization and total least squares. SIAM journal on matrix analysis and appli- cations, 21(1):185–194, 1999. 4 [11] Kang Han and Wei Xiang. Multiscale tensor decom- position and rendering equation encoding for view synthesis. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR), pages 4232–4241, 2023. 1 [12] Meraj Hashemizadeh, Michelle Liu, Jacob Miller, and Guillaume Rabusseau. Adaptive learning of tensor network structures. arXiv preprint arXiv:2008.05437, 2020. 1, 2, 7, 8 [13] Christopher J. Hillar and Lek-Heng Lim. Most tensor problems are NP-hard. Journal of the ACM , 60(6): 1–39, 2013. 1 [14] Tamara G Kolda and Brett W Bader. Tensor decom- positions and applications. SIAM Review, 51(3):455– 500, 2009. 3 [15] Chao Li and Zhun Sun. Evolutionary topology search for tensor network decomposition. In Proceedings of the 37th International Conference on Machine Learn- ing, pages 5947–5957, 2020. 1, 2, 7, 8 [16] Chao Li and Qibin Zhao. Is rank minimization of the essence to learn tensor network structure? In Second Workshop on Quantum Tensor Networks in Machine Learning (QTNML), Neurips, 2021. 3 [17] Chao Li, Junhua Zeng, Zerui Tao, and Qibin Zhao. Permutation search of tensor network structures via local sampling. In Proceedings of the 39th Inter- national Conference on Machine Learning , pages 13106–13124, 2022. 1, 2, 7, 8, 9 [18] Chao Li, Junhua Zeng, Chunmei Li, Cesar Caiafa, and Qibin Zhao. Alternating local enumeration (tnale): Solving tensor network structure search with fewer evaluations. In Proceedings of the 40th International Conference on Machine Learning, 2023. 1, 2, 7, 8 [19] Nannan Li, Yu Pan, Yaran Chen, Zixiang Ding, Dong- bin Zhao, and Zenglin Xu. Heuristic rank selec- tion with progressively searching tensor ring network. Complex & Intelligent Systems, 8(2):771–785, 2022. [20] Yipeng Liu, Yingcong Lu, Weiting Ou, Zhen Long, and Ce Zhu. Adaptively topological tensor network for multi-view subspace clustering. arXiv preprint arXiv:2305.00716, 2023. 2 [21] Yisi Luo, Xi-Le Zhao, Deyu Meng, and Tai-Xiang Jiang. Hlrtf: Hierarchical low-rank tensor factoriza- tion for inverse problems in multi-dimensional imag- ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19281–19290, 2022. 1 [22] Chang Nie, Huan Wang, and Le Tian. Adaptive tensor networks decomposition. In British Machine Vision Conference, 2021. 2 [23] Rom ´an Or ´us. A practical introduction to tensor net- works: Matrix product states and projected entangled pair states. Annals of Physics, 349:117–158, 2014. 2 [24] Ivan Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing , 33(5):2295–2317, 2011. 2[25] Piyush Rai, Yingjian Wang, Shengbo Guo, Gary Chen, David Dunson, and Lawrence Carin. Scalable Bayesian low-rank decomposition of incomplete mul- tiway tensors. In Proceedings of the 31st International Conference on International Conference on Machine Learning, page II–1800–II–1808, 2014. 1 [26] Farnaz Sedighin, Andrzej Cichocki, and Anh Huy Phan. Adaptive rank selection for tensor ring decom- position. IEEE Journal of Selected Topics in Signal Processing, 15(3):454–463, 2021. 2 [27] Moein Shakeri and Hong Zhang. Moving object detec- tion under discontinuous change in illumination using tensor low-rank and invariant sparse decomposition. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 7221–7230, 2019. 1 [28] Zhong-Cheng Wu, Ting-Zhu Huang, Liang-Jian Deng, Hong-Xia Dou, and Deyu Meng. Tensor wheel decomposition and its tensor completion application. In Advances in Neural Information Processing Sys- tems, pages 27008–27020, 2022. 8, 9 [29] Yangyang Xu, Ruru Hao, Wotao Yin, and Zhixun Su. Parallel matrix factorization for low-rank tensor com- pletion. Inverse Problems and Imaging, 9(2):601–624, 2015. 8, 9 [30] Ryuki Yamamoto, Hidekata Hontani, Akira Imakura, and Tatsuya Yokota. Fast algorithm for low-rank ten- sor completion in delay-embedded space. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 2048–2056, 2022. 1 [31] Ke Ye and Lek-Heng Lim. Tensor network ranks. arXiv preprint arXiv:1801.02662, 2018. 2 [32] Longhao Yuan, Jianting Cao, Xuyang Zhao, Qiang Wu, and Qibin Zhao. Higher-dimension tensor com- pletion via low-rank tensor ring decomposition. In Asia-Pacific Signal and Information Processing Asso- ciation Annual Summit and Conference , pages 1071– 1076, 2018. 2 [33] Longhao Yuan, Chao Li, Danilo Mandic, Jianting Cao, and Qibin Zhao. Tensor ring decomposition with rank minimization on latent space: An efficient approach for tensor completion. In Proceedings of the AAAI Conference on Artificial Intelligence , pages 9151–9158, 2019. 8, 9 [34] Longhao Yuan, Qibin Zhao, Lihua Gui, and Jianting Cao. High-order tensor completion via gradient-based optimization under tensor train format. Signal Pro- cessing: Image Communication, 73:53–61, 2019. 2 [35] Shipeng Zhang, Lizhi Wang, Lei Zhang, and Hua Huang. Learning tensor low-rank prior for hyper- spectral image reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 12001–12010, 2021. 1 [36] Xinyuan Zhang, Xin Yuan, and Lawrence Carin. Non- local low-rank tensor factor analysis for image restora- tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8232–8241, 2018. 1 [37] Qibin Zhao, Liqing Zhang, and Andrzej Cichocki. Bayesian CP factorization of incomplete tensors with automatic rank determination. IEEE Transactions on Pattern Analysis and Machine Intelligence , 37(9): 1751–1763, 2015. 1, 8, 9 [38] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. Tensor ring decomposition. arXiv preprint arXiv:1606.05535, 2016. 2, 7, 8 [39] Wen-Jie Zheng, Xi-Le Zhao, Yu-Bang Zheng, Jie Lin, Lina Zhuang, and Ting-Zhu Huang. Spatial-spectral- temporal connective tensor network decomposition for thick cloud removal.ISPRS Journal of Photogram- metry and Remote Sensing, 199:182–194, 2023. 1 [40] Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, Tai- Xiang Jiang, Teng-Yu Ji, and Tian-Hui Ma. Tensor N-tubal rank and its convex relaxation for low-rank tensor recovery. Information Sciences, 532:170–189, 2020. 6 [41] Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, Qibin Zhao, and Tai-Xiang Jiang. Fully-connected tensor network decomposition and its application to higher- order tensor completion. In Proceedings of the AAAI Conference on Artificial Intelligence , pages 11071– 11078, 2021. 2, 3, 4, 7, 8 [42] Yu-Bang Zheng, Ting-Zhu Huang, Xi-Le Zhao, and Qibin Zhao. Tensor completion via fully-connected tensor network decomposition with regularized fac- tors. Journal of Scientific Computing , 92(8):1–35, 2022. 2",
      "meta_data": {
        "arxiv_id": "2305.14912v6",
        "authors": [
          "Yu-Bang Zheng",
          "Xi-Le Zhao",
          "Junhua Zeng",
          "Chao Li",
          "Qibin Zhao",
          "Heng-Chao Li",
          "Ting-Zhu Huang"
        ],
        "published_date": "2023-05-24T09:02:01Z",
        "pdf_url": "https://arxiv.org/pdf/2305.14912v6.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the computationally intensive NP-hard problem of Tensor Network Structure Search (TN-SS) in computer vision and machine learning. It proposes a novel Tensor Network (TN) paradigm called SVD-inspired TN decomposition (SVDinsTN) that tackles TN-SS from a regularized modeling perspective. This approach eliminates the need for repeated structure evaluations, a major bottleneck in existing \"sampling-evaluation\" methods. SVDinsTN achieves approximately 100 to 1000 times acceleration compared to state-of-the-art TN-SS methods while maintaining comparable representation ability. The method includes a theoretical convergence guarantee and establishes an upper bound for the TN rank, which guides an efficient initialization scheme. It also demonstrates strong performance in practical applications like tensor completion.",
        "methodology": "SVDinsTN extends Fully-Connected Tensor Network (FCTN) decomposition by inserting diagonal factors between any two TN cores in the fully-connected topology. The sparsity of these diagonal factors is leveraged to reveal a compact TN structure. The problem is formulated as an L1-norm-based regularized model, minimizing the Frobenius norm of the reconstruction error, with Tikhonov regularization for TN cores and an L1-norm penalty on the diagonal factors to induce sparsity. This model is solved using a Proximal Alternating Minimization (PAM)-based algorithm, which iteratively updates TN cores (Gk) and diagonal factors (St,l). The Gk-subproblem is solved by converting it into a differentiable objective function. The St,l-subproblem, involving an L1-norm, is solved using an Alternating Direction Method of Multipliers (ADMM) framework, incorporating a shrinkage operator. A novel initialization scheme is designed based on an established upper bound for TN rank (Rt,l ≤ min(rank(X(t)), rank(X(l)))), utilizing truncated SVD of mode-(t,l) slices of the input tensor and a shrinkage operation to reduce initial rank values and computational cost.",
        "experimental_setup": "The method's performance was evaluated on three claims using both synthetic and real-world data. For validating the ability to reveal customized TN structures (Claim A), synthetic fourth-order (16x18x20x22) and fifth-order (14x16x18x20x22) tensors with various TN structures (including isomorphic 'ring' and 'five-star' topologies) were used, with 100 independent tests measuring success rate. For validating efficiency and representation ability (Claim B), real-world light field images (Bunny, Knights, Truck; fifth-order 40x60x3x9x9) and higher-order synthetic tensors (6th, 8th, 10th order) were tested against six baselines: TRALS, FCTNALS (pre-defined topology), TNGreedy, TNGA, TNLS, and TNALE (TN-SS methods). Metrics included Compression Ratio (CR), Reconstruction Error (RE) bound, and run time. The impact of the proposed initialization scheme and its 'shrink' operation was also analyzed. For validating performance in tensor completion (Claim C), four real-world color videos (Bunny, News, Salesman, Silent; fourth-order 144x176x3x50) with a 90% missing ratio were compared against FBCP, TMac, TMacTT, TRLRF, TW, and TNLS. Reconstructed quality was evaluated using Mean Peak Signal-to-Noise Ratio (MPSNR) and run time. Numerical convergence was verified using relative change curves.",
        "limitations": "While SVDinsTN effectively addresses the computationally consuming issue in Tensor Network Structure Search, the theoretical guarantee of the optimal TN structure remains an open problem. The current work does not provide this theoretical assurance.",
        "future_research_directions": "The primary future research direction is to solve the open problem of providing a theoretical guarantee for the optimal Tensor Network (TN) structure. This would complement the current method's ability to efficiently find compact structures by ensuring their optimality."
      }
    },
    {
      "title": "TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning",
      "abstract": "We present a novel procedure for optimization based on the combination of\nefficient quantized tensor train representation and a generalized maximum\nmatrix volume principle. We demonstrate the applicability of the new Tensor\nTrain Optimizer (TTOpt) method for various tasks, ranging from minimization of\nmultidimensional functions to reinforcement learning. Our algorithm compares\nfavorably to popular evolutionary-based methods and outperforms them by the\nnumber of function evaluations or execution time, often by a significant\nmargin.",
      "full_text": "TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning Konstantin Sozykin ∗† Andrei Chertkov ∗† Roman Schutski † Anh-Huy Phan † Andrzej Cichocki †‡§ Ivan Oseledets †¶ Abstract We present a novel procedure for optimization based on the combination of efﬁcient quantized tensor train representation and a generalized maximum matrix volume principle. We demonstrate the applicability of the new Tensor Train Optimizer (TTOpt) method for various tasks, ranging from minimization of multidimensional functions to reinforcement learning. Our algorithm compares favorably to popular gradient-free methods and outperforms them by the number of function evaluations or execution time, often by a signiﬁcant margin. 1 Introduction In recent years learning-based algorithms achieved impressive results in various applications, ranging from image and text analysis and generation [ 55] to sequential decision making and control [ 43] and even quantum physics simulations [ 51]. The vital part of every learning-based algorithm is an optimization procedure, e.g., Stochastic Gradient Descent. In many situations, however, the problem-speciﬁc target function is not differentiable, too complex, or its gradients are not helpful due to the non-convex nature of the problem [32, 2]. The examples include hyper-parameter selection during the training of neural models, policy optimization in reinforcement learning (RL), training neural networks with discrete (quantized) weights [60] or with non-differentiable loss functions [ 21]. In all these contexts, efﬁcient direct gradient-free optimization procedures are highly needed. Recently, [56] showed that an essential class of gradient-free methods, namely the evolutionary strategies (ES) [23, 27, 59], are competitive in reinforcement learning problems. In RL, the goal is to ﬁnd the agent’s action distributionπ(the policy), maximizing some cumulative reward functionJ. The policy is usually parameterized with a set of parameters θ. It follows that the reward is a function of the parameters of the policy: J(π(θ)) = J(θ). The idea of [56] and similar works [39, 15, 35, 10, 11] is to directly optimize the cumulative reward function J(θ) = J(θ1,θ2,...,θ d) with respect to the parameters of the policy. We pursued a similar approach to transform a traditional Markov Decision Process into an optimization problem (we provide the details in Appendix B.3). Although the agents trained with ES often demonstrate more rich behavior and better generalization compared to traditional gradient-based policy optimization, the convergence of ES is often slow [11]. ∗Equal Contribution, corresponding emails {konstantin.sozykin,a.chertkov}@skoltech.ru †Center of Artiﬁcial Intelligence Technology, Skolkovo Institute of Science and Technology (Skoltech) Moscow, Russia ‡RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan §Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland ¶Artiﬁcial Intelligence Research Institute (AIRI), Moscow, Russia 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2205.00293v2  [cs.LG]  28 Sep 2022As an alternative to previous works, we present a tensor-based6 gradient-free optimization approach and apply it to advanced continuous control RL benchmarks. The algorithm that we called Tensor- Train (TT) Optimizer ( TTOpt), works for multivariable functions with discrete parameters by reformulating the optimization problem in terms of tensor networks. Consider a function J(θ) : Nd −→Rof a d-dimensional argument θ, where each entry θk of the vector θtakes a value in the discrete set {ωi}N i=1. The function J may be viewed as an implicitly deﬁned d-dimensional tensor J. Each entry in Jis a value of J for some argument. Maximizing J is equivalent to ﬁnding the sets of indices {θ(m) max}M m=1 of maximal entries {j(m) max}M m=1 of J. By using only a tiny fraction of adaptively selected tensor elements (e.g., a small number of function evaluations), our method builds a representation of Jin the TT-format and ﬁnds a set of the largest elements. Although the algorithm works only with functions of discrete arguments, the grid size for each parameter can be huge thanks to the efﬁciency of the TT-format. Fine discretization makes it possible to almost reach a continuous limit and obtain large precision with TTOpt. The strength of our approach is, however, the direct handling of discrete (quantized) parameters. Contributions. We propose an efﬁcient gradient-free optimization algorithm for multivariable functions based on the low-rank TT-format and the generalized maximum matrix volume principle7. We demonstrate that our approach is competitive with a set of popular gradient-free methods for optimizing benchmark functions and neural network-based agents in RL problems. We empirically show that agents with discrete (heavily quantized) weights perform well in continuous control tasks. Our algorithm can directly train quantized neural networks, producing policies suitable for low-power devices. 2 Optimization with tensor train In this section, we introduce a novel optimization algorithm. We show how to represent the optimiza- tion problems in the discrete domain efﬁciently and then formulate optimization as a sampling of the objective function guided by the maximum volume principle. 2.1 Discrete formulation of optimization problems We ﬁrst need to transfer the problem to the discrete domain to apply our method. It may seem that discretizing an optimization problem will make it harder. However, it will allow us to use powerful techniques for tensor network representation to motivate the algorithm. For each continuous parameter θk (k= 1,2,...,d ) of the objective function J(θ) we introduce a grid {θ(nk) k }Nk nk=1. At each point (θ(n1) 1 ,θ(n2) 2 ,...,θ (nd) d ) of this grid with index (n1,n2,...,n d) the objective function takes a value J(θ(n1) 1 ,θ(n2) 2 ,...,θ (nd) d ) ≡J [n1,n2,...,n d]. We thus can regard the objective function as an implicit d-dimensional tensor Jwith sizes of the modes N1,N2,...,N d. Finding the maximum of the function J(θ) translates into ﬁnding the maximal element of the tensor Jin the discrete setting. Notice that the number of elements of Jequals: |J| = N1 ·N2 ·... ·Nd ∼(max1≤k≤dNk)d. The size of Jis exponential in the number of dimensions d. This tensor can not be evaluated or stored for sufﬁciently large d. Fortunately, efﬁcient approximations were developed to work with multidimensional arrays in recent years. Notable formats include Tensor Train (TT) [ 49, 48, 52], Tensor Chain/Tensor Ring [ 31, 67] and Hierarchical Tucker [ 22]. We use the most studied TT- format [13], but the extensions of our method to other tensor decompositions are possible. 6By tensors we mean multidimensional arrays with a number of dimensions d(d≥1). A two-dimensional tensor (d= 2) is a matrix, and when d= 1 it is a vector. For scalars we use normal font, we denote vectors with bold letters and we use upper case calligraphic letters (A,B,C,... ) for tensors with d> 2. Curly braces deﬁne sets. We highlight discrete and continuous scalar functions of multidimensional argument in the appropriate font, e.x., J(·), in this case, the maximum and minimum values of the function are denoted as Jmax and Jmin, respectively. 7We implemented the proposed algorithm within the framework of the publicly available software product: https://github.com/AndreiChertkov/ttopt. 22.2 Tensor Train decomposition Deﬁnition 2.1. A tensor J∈ RN1×N2×···×Nd is said to be in the TT-format [48] if its elements are represented by the following expression J[n1,n2,...,n d] = R0∑ r0=1 R1∑ r1=1 ··· Rd∑ rd=1 G1[r0,n1,r1]G2[r1,n2,r2] ... Gd[rd−1,nd,rd], (1) where nk = 1,2,...,N k for k= 1,2,...,d . In TT-format the d-dimensional tensor Jis approximated as a product of three-dimensional tensors Gk ∈ RRk−1×Nk×Rk , called TT-cores. The sizes of the internal indices R0,R1,··· ,Rd (with convention R0 = Rd = 1 ) are known as TT-ranks. These ranks control the accuracy of the approximation. The storage of the TT-cores, G1,G2,..., Gd, requires at most d·max1≤k≤dNk ·(max0≤k≤dRk)2 memory cells, and hence the TT-approximation is free from the curse of dimensionality 8 if the TT-ranks are bounded. The basic linear algebra operations (such as ﬁnding a norm, differentiation, integration, and others) can also be implemented in the TT-format with polynomial complexity in dimensionality and mode size. Building TT-approximation. Several efﬁcient schemes were proposed to ﬁnd TT-approximation if all or some of the elements of the initial tensor are known or may be generated by the function’s call. Examples include TT-SVD [49, 48], TT-ALS [28] and TT-CAM [50] (Cross Approximation Method in the TT-format). We build upon the TT-CAM but modify it not to compute the approximation for theentire tensor, but rather to ﬁnd a small subset of its maximal entries. The original algorithm builds a TT-approximation by adaptively requesting elements of the input tensor. As we will show below, these elements with high probability will have large absolute values. Based on this observation, we formulate a robust optimization algorithm for multivariate functions (either discrete or continuous). To simplify the understanding, we outline the approach for the two-dimensional case, and after that, we describe our gradient-free optimization method for the multidimensional case. 2.3 Maximal element in a matrix The Cross Approximation Method (CAM) for matrices [19, 9, 1] is a well-established algorithm for building a rank-Rapproximation ˜Jof an implicitly given matrix J: J≃ ˜J, ˜J= JC ˆJ−1JR, (2) where JC consists of Rcolumns of J, JR is composed of Rrows of J, and ˆJ is a submatrix at their intersection. Such approximation (also called cross or skeleton decomposition) may be built iteratively using a well known alternating directions method and a maximum volume ( maxvol) algorithm9 [19], as we will sketch below. Intuition behind TTOpt. The main interest in optimization problems is not the approximation (2) itself, but the following property of the resulting maximum volume submatrix ˆJ. [19] proved that if ˆJis an R×Rsubmatrix of maximal volume (in selected rows and columns) then the maximal (by modulus) element ˆJmax ∈ ˆJbounds the absolute maximal element Jmax in the full matrix J: ˆJmax ·R2 ≥Jmax. (3) 8The number of elements of an uncompressed tensor (hence, the memory required to store it) and the number of elementary operations required to perform computations with such a tensor grow exponentially in dimensionality. This problem is called the curse of dimensionality. 9The maxvol algorithm ﬁnds Rrows in an arbitrary non-degenerate matrix A∈RN×R (N >R) which span a maximal-volume R×Rsubmatrix ˆA. The matrix ˆA∈Ahas maximal value of the modulus of the determinant on the set of all nondegenerate square submatrices of the sizeR×R. We describe the implementation of maxvol in Appendix A.1. The algorithm greedily rearranges rows of Ato maximize submatrix volume. Its computational complexity is O(NR2 + KNR), where Kis a number of iterations. 3Figure 1: The scheme of the cross approximation algorithm for matrices using the alternating direc- tion and maximal-volume principle. Green bars represent generated rows/columns; purple bars are rows/columns selected for generation in the next step by the maxvol algorithm. The method allows to ﬁnd the optimum of the two dimensional func- tion J(θ1,θ2). Figure 2: Conceptual scheme of TTOpt al- gorithm based on the alternating direction and maximal-volume approaches for tensors. Only a small part of the tensor is explicitly generated during this procedure, as shown here with green columns. For the simplicity of presentation, the rows and columns se- lected at iterations are drawn as continuous blocks (they are not in practice). This statement is evident for R= 1, and for the case R> 1 it gives an upper bound for the element. By using elementwise transformations of J, this upper bound can be used to obtain a sequence that converges to the global optimum. The main idea of the maxvol-based methods is that it is easier to ﬁnd a submatrix with a large volume rather than the element with the largest absolute value. Moreover, our numerical experiments show that this bound is pessimistic, and in practice, the maximal-volume submatrix contains the element which is very close to the optimal one. TTOpt algorithm for matrices. The idea of the TTOpt algorithm for matrices is to iteratively search for the maximal volume submatrices in the column and row space of the implicitly given10 input matrix J∈RN1×N2 . After T iterations a series of “intersection” matrices {ˆJ(t)}T t=1 ∈RR×R is produced. The maximal element is searched in these small submatrices. We schematically represent the TTOpt algorithm in Figure 1, and a description is given below: 1. At the initial stage, we set the expected rank of the approximation, R, and select Rran- dom columns I(C,1). We then generate the corresponding column submatrix J(1) C = J[: ,I(C,1)] ∈RN1×R. Using the maxvol algorithm, we ﬁnd the maximal-volume submatrix ˆJ(1) ∈RR×R in J(1) C and store its row indices in the list I(R,1). 2. The indices I(R,1) are used to generate a row submatrix J(2) R = J[I(R,1),:] ∈RR×N2 . Then, using the maxvol algorithm, we ﬁnd the maximal-volume submatrix ˆJ(2) in the matrix J(2) R and store the corresponding column indices in the list I(C,2). 10The matrix is speciﬁed as a function J(·) that allows to calculate the value of an arbitrary requested element (n1,n2), where 1 ≤n1 ≤N1 and 1 ≤n2 ≤N2. We present the approach to approximate the value of the maximum modulus element of such a matrix. The method of ﬁnding the minimal or maximal elements within the framework of this algorithm will be described in Section 2.6. 43. We generate the related columns J(3) C = J[:,I(C,2)], apply again the maxvol algorithm to the column submatrix, and iterate the process until convergence. 4. The approximate value of the maximum modulus element of the matrix Jis found as ˆJmax = max ( max ( ˆJ(1)), max ( ˆJ(2)), ..., max ( ˆJ(T)) ) . (4) 2.4 Optimization in the multidimensional case As we explained previously, the target tensor,J, is deﬁned implicitly, e.g., by a multivariable function J. We propose a novel method to ﬁnd the optimum in this implicit tensor. We outline the approach below and provide detailed algorithms in Appendix A.2. As shown in Figure 2, we begin by considering the ﬁrst unfolding11 J1 ∈RN1×N2...Nd of the tensor Jand select R1 random columns I(C) 1 . Precisely, I(C) 1 here is a list of R1 random multi-indices of size d−1, which specify positions along modes k = 2 to k = d. We then generate the submatrix J(C) 1 ∈RN1×R1 for all positions along the ﬁrst mode (shown in green in Figure 2). Like in matrix case, we apply the maxvol algorithm to ﬁnd the maximal-volume submatrix ˆJ1 ∈RR1×R1 and store the corresponding indices of R1 rows in the list I(R) 1 . In contrast with matrix case, we cannot generate the row submatrix J(R) 1 ∈RR1×N2N3...Nd for the selected row indices I(R) 1 , since it contains an exponential number of elements. The following trick is used instead. We consider the implicit matrix J(R) 1 and reshape it to a new matrix J2 ∈ RR1N2×N3...Nd . We sample R2 random columns I(C) 2 in the matrix J2 and generate the entire small submatrix J(C) 2 ∈RR1N2×R2 . Next we ﬁnd the maximal-volume submatrix ˆJ2 ∈RR2×R2 in J(C) 2 and store the corresponding R2 row multi-indices in the list I(R) 2 . The resulting submatrix J(R) 2 ∈RR2×N3N4...Nd is then transformed (without explicitly evaluating its elements) into the matrix J3 ∈RR2N3×N4...Nd . We continue the described operations, called sweeps, until the last mode of the initial tensor,J, is reached. After that, we repeat the process in the opposite direction, sampling now the row indices instead of the column indices. These sequences of forward and backward sweeps are continued until the algorithm converges to some row and column indices for all unfolding matrices 12 or until the user-speciﬁed limit on the number of requests to the objective function, J, is exceeded. Finally, after T sweeps, the approximate value of the maximum modulo element can be found by the formula (4), as in the two-dimensional case. Note that currently there is no analog of Eq. (3) in the multidimensional case, and hence there are no formal guarantees of the convergence of the sweeps to the global minimum, nor the rate of this convergence. The only guarantee is that the result will monotonically improve with iterations. 2.5 Complexity of the algorithm It can be easily shown that the described algorithm requires to evaluate only O ( d·max1≤k≤d ( NkR2 k )) elements of the implicit tensor in one sweep. Thus, with a total number of sweeps T, we will have O ( T ·d· max 1≤k≤d ( NkR2 k )) , (5) calls to the objective function J. In practice, it turns out to be more convenient to limit the maximum number of function calls, M, according to the computational budget. 11The k-th unfolding Jk for the d-dimensional tensor J ∈ RN1×N2×···×Nd is the matrix Jk ∈ RN1...Nk×Nk+1...Nd ,with elements Jk[ n1,...,n k,nk+1,...,n d] ≡J[n1,n2,...,n d] for all indices. 12The TT-approximation(1) of the tensor Jmay be recovered from the generated columnsJ(C) k and maximal- volume submatrices ˆJk (k= 1,2,...,d ) as follows: Gk = J(C) k ˆJ−1 k ∈RRk−1Nk×Rk , where Gk is the 2-th unfolding of the k-th TT-core. However, we do not consider this point in more detail, since in our work, the main task is to ﬁnd the minimum or maximum value of the tensor, not to construct its low-rank approximation. 5If the time of a single call to J is signiﬁcant, then the effort spent on the algorithm’s operation will be negligible. Otherwise, the bottleneck will be the calculation of the maximal-volume submatrices by the maxvol algorithm. Taking into account the estimate of the maxvol complexity, given in Appendix A.1, it can be shown that in this case the complexity of our algorithm is O ( T ·d· max 1≤k≤d ( NkR3 k )) . (6) 2.6 Implementation details For the effective implementation of the TTOpt algorithm, the following important points should be taken into account (see also the detailed pseudocode in Appendix A.2). Stability. Submatrices J(C) k (or J(k) C and J(k) R for the two-dimensional case) that arise during the iterations may degenerate, and in this case it is impossible to apply the maxvol algorithm. To solve this problem, we ﬁrst calculate the QR decomposition for these matrices and then apply the maxvol to the corresponding Qfactors13. Rank selection. We do not know in advance the exact ranks R1,R2,...,R d of the unfolding matrices (or rank Rof the matrix Jfor the two-dimensional case). Therefore, instead of the maxvol algorithm, we use its modiﬁcation, i.e., the rect_maxvol algorithm14 [42], within the framework of which several (“most important”) rows are added to the maximal-volume submatrix. In this case, we have ˆJ(C) 1 ∈R(R1+∆R1)×R1 , ˆJ(C) 2 ∈R(R2+∆R2)×R2 , etc. Note that the ﬁnal approximation obtained in this case may have an overestimated rank, which, if necessary, can be reduced by appropriate rounding, for example, by truncated SVD decomposition. Mapping function. Maximal-volume submatrices contain the maximum modulus element but not the minimum or maximum element of the tensor (i.e., the sign is not taken into account). We introduce a dynamic mapping function to ﬁnd the global minimum (or maximum). Instead of J at each step of the algorithm, we evaluate15: g(x) = π 2 −atan (J(x) −Jmin) , (7) where Jmin is the current best approximation for the minimum element of the tensor. Quantization. To reach high accuracy, we often need ﬁne grids. In the case when the sizes of the tensor modes N1,N2,...,N d are large, the sizes of unfolding matrices become large, which leads to a signiﬁcant increase in computational complexity of the maxvol algorithm. To solve this problem, we apply additional compression based on quantization of the tensor modes [47]. Assume without the loss of generality that the size of each mode is Nk = Pq (k= 1,2,...,d ; P ≥2; q≥2). Then we can reshape the original d-dimensional tensor J∈ RN1×N2×···×Nd into the tensor ˜J∈ RP×P×···×P of a higher dimension d·q, but with smaller modes of size P. The TTOpt algorithm can be applied for this “long” tensor instead of the original one16. We found that this idea signiﬁcantly boosts the accuracy of our algorithm and reduces the complexity and execution time. Typically,P is taken as small as possible, e.g. P = 2. 3 Experiments To demonstrate the advantage of the proposed optimization method, we tested TTOpt on several numerical problems. First, we consider analytical benchmark functions and then the practically 13It can be shown that this operation does not increase the complexity estimate (6) of the algorithm. 14The rect_maxvol algorithm allows to ﬁnd R+ ∆Rrows in an arbitrary nondegenerate matrix A∈RN×R (N >R, N ≥R+ ∆R) which form an approximation to the rectangular maximal-volume submatrix of the given matrix A. Details about rect_maxvol are provided in Appendix A.1. 15The mapping function should be continuous, smooth, and strictly monotone. There are various ways to choose such a function. However, during test runs, it turned out that the proposed function (7) is most suitable. 16If the maximal rank R>P , we select all indices for ﬁrst kmodes until Pk >R. We note however that this detail does not change the global behavior of the TTOpt algorithm. 6Table 1: Comparison of the TTOpt optimizer versus baselines in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for various benchmark functions. See Table 1 in Appendix B.1 with the list of functions and their properties. The reported values are averaged over 10 independent runs. A upper half of the table presents gradient free (zeroth order) methods, and lower half is for ﬁrst and second order methods. F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 TTOPT ϵ 3.9 E-06 2.9 E-07 1.8 E-12 4.4 E-15 2.8 E-02 1.1 E-01 5.5 E-09 4.6 E-11 1.8 E-01 1.3 E-04 τ 2.61 2.44 2.45 2.52 2.40 2.48 2.39 2.60 2.32 2.44 GA ϵ 9.7 E-02 8.4 E-03 5.8 E-03 2.0 E+00 3.9 E-04 1.0 E+01 1.2 E-01 7.9 E-01 6.2 E-03 4.2 E+03 τ 6.21 4.56 5.09 4.87 5.85 5.69 5.05 5.04 5.04 4.70 OPEN ES ϵ 1.8 E-01 1.2 E-02 1.7 E-02 2.0 E+00 1.2 E-03 9.7 E+00 3.8 E+00 2.1 E+00 1.8 E-02 4.2 E+03 τ 2.62 1.08 1.62 1.08 2.41 2.04 1.30 1.39 1.62 1.12 CMA ES ϵ 5.1 E+287 3.1 E-01 9.3 E-77 2.0 E+00 7.6 E+289 1.9 E+01 5.5 E-02 9.3 E+01 5.3 E+289 1.7 E+282 τ 10.36 8.50 9.40 9.13 12.86 9.76 9.04 9.13 11.15 8.86 DE ϵ 1.1 E+00 4.3 E-02 3.3 E-02 9.0 E-05 1.8 E-01 2.6 E-01 2.0 E-01 6.2 E+00 6.6 E-01 3.8 E+02 τ 38.91 38.06 51.05 39.48 41.35 41.40 41.34 41.31 37.97 38.64 NB ϵ 1.5 E+01 6.5 E+00 3.9 E+01 1.2 E-01 2.4 E+01 6.6 E+00 2.6 E+10 6.3 E+01 3.4 E+00 3.2 E+03 τ 45.23 46.98 37.50 45.91 48.03 37.16 40.05 44.95 44.06 46.91 PSO ϵ 1.2 E+01 5.3 E+00 3.5 E+01 9.8 E-02 2.0 E+01 2.5 E-01 2.0 E+10 2.3 E+01 5.1 E-01 2.9 E+03 τ 47.19 47.04 45.50 43.39 46.80 44.97 46.46 42.78 43.15 47.13 BFGS ϵ 1.9 E+01 2.1 E+00 1.9 E+01 4.3 E-13 1.2 E-02 6.4 E+00 2.4 E-05 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.02 0.03 0.00 0.02 0.01 0.04 0.01 0.01 0.00 L-BFGS ϵ 1.9 E+01 1.9 E+00 4.0 E-10 4.3 E-13 1.2 E-02 4.5 E+00 4.5 E-10 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.06 0.05 0.00 0.01 0.02 0.02 0.01 0.01 0.01 CG ϵ 1.9 E+01 3.4 E+00 N/A 0.0 E+00 2.0 E-02 4.4 E+00 2.9 E-12 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.07 N/A 0.00 0.01 0.05 0.02 0.01 0.03 0.00 NCG ϵ 1.9 E+01 3.4 E+00 1.7 E-19 0.0 E+00 7.4 E-02 6.4 E+00 2.1 E-12 7.0 E+01 4.2 E+00 2.2 E+03 τ 0.01 0.06 0.04 0.00 0.01 0.06 0.01 0.00 0.02 0.01 NEWTON ϵ 1.9 E+01 2.2 E+00 1.1 E+11 0.0 E+00 3.2 E-02 6.4 E+00 2.8 E-24 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.02 0.01 0.00 0.01 0.10 0.01 0.01 0.01 0.00 TR NCG ϵ 1.9 E+01 6.5 E+00 2.4 E-10 4.0 E-12 4.8 E+00 4.4 E+00 1.2 E-14 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.01 0.12 0.04 0.00 0.02 0.03 0.01 0.00 0.02 0.01 TR ϵ 1.9 E+01 7.0 E+00 3.2 E-13 4.0 E-12 6.1 E+01 2.6 E+00 5.3 E-11 7.0 E+01 4.2 E+00 2.1 E+03 τ 0.02 12.97 0.06 0.00 0.06 0.05 0.02 0.01 0.01 0.01 Table 2: The result of the TTOpt optimizer in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for benchmark F1 (Ackley function) for various dimension numbers. DIMENSION d = 10 d = 50 d = 100 d = 500 ERROR , ϵ 3.9 E-06 3.9 E-06 3.9 E-06 3.9 E-06 TIME , τ 3.1 40.1 143.9 3385.3 signiﬁcant problem of optimizing the parameters of the RL agent. We select GA (Genetic Algo- rithm [27, 62]), openES (basic version of OpenAI Evolution Strategies [ 56]) and cmaES (the Covariance Matrix Adaptation Evolution Strategy [23]) as baselines for both experiments. Addi- tionally we used DE (Differential Evolution [61]), NB (NoisyBandit method from Nevergrad [5]) and PSO (Particle Swarm Optimization [25, 36]) for benchmark functions17. We also compared the proposed approach with gradient-based methods applied for all benchmark functions18: BFGS (Broyden–Fletcher–Goldfarb–Shanno algorithm), L-BFGS (Limited-memory BFGS), CG (Conju- gate Gradient algorithm), NCG (Newton CG algorithm), Newton (Newton Exact algorithm), TR NCG (Trust-Region NCG algorithm) and TR (Trust-Region Exact algorithm). According to our approach, the TTOpt solver has the following conﬁgurable parameters: a and b are lower and upper grid bounds (for simplicity, we use the same value for all dimensions); R is a rank (for simplicity, we use the same value for all unfolding matrices); P is a submode size (mode size of the quantized tensor; for simplicity, we use the same value for all dimensions);q is the number of submodes in the quantized tensor (each mode of the original tensor has size N = Pq); M is a limit on the number of requests to the objective function. 17We used implementations of the methods from available packages estool (https://github.com/ hardmaru/estool), pycma (https://github.com/CMA-ES/pycma, and nevergrad (https://github. com/facebookresearch/nevergrad). 18We used implementations from the package https://github.com/rfeinman/pytorch-minimize). We carried out computations with all methods from this library, except for Trust-Region GLTR (Krylov) and Dogleg methods, for which the calculation ended with an error for most benchmarks. 7Table 3: Mean E and standard deviation σ of the ﬁnal cumulative reward. The environments are encoded using capital letters Swimmer-v3, LunarLanderContinuous-v2, InvertedPendulum-v2 and HalfCheetah-v3. The left sub-table is for mode size N = 3, another one is for mode size N = 28. All runs are averaged over seven random seeds. S(31) L(31) I(31) H(31) S(28) L(28) I(28) H(28) TTOPT E σ 357.50 6.59 290.29 24.40 1000.00 0.00 4211.02 211.94 311.82 29.61 286.87 21.65 1000.00 0.00 2935.90 544.11 GA E σ 349.91 10.04 283.05 16.28 893.00 283.10 2495.37 185.11 359.79 4.21 213.75 99.67 222.86 342.79 3085.80 842.76 CMA ES E σ 342.31 36.07 214.55 93.79 721.00 335.37 2549.83 501.08 340.54 78.90 221.95 133.80 621.00 472.81 2879.46 929.55 OPEN ES E σ 318.39 44.61 114.97 113.48 651.86 436.37 2423.16 602.43 109.39 40.11 73.08 163.33 224.71 217.51 1691.22 976.96 3.1 Benchmark functions minimization To analyze the effectiveness of theTTOpt, we applied it to 10-dimensional benchmark functions with known global minimums; see Table 1 in Appendix B.1 with the list of functions and their properties (note that some of the considered benchmarks are multimodal non-separable functions). Also, in Appendix B.1, we present a more detailed study of the TTOpt solver and the dependence of the accuracy on the value of its parameters (R, qand M). In all experiments with baselines (GA, openES, cmaES, DE, NB, PSO, BFGS, L-BFGS, CG, NCG, Newton, TR NCGand TR), we used default parameter values. In Appendix B.2 we also present the additional experiments with Bayesian Optimization [40]. For TTOpt we selected rank19 R= 4, submode size P = 2 and the number of submodes q = 25. For all methods, a limit on the number of requests to the objective function is chosen as M = 105. All calculations are performed on a standard laptop. The results are demonstrated in Table 1. For each method we list the absolute deviation of the result ˆJmin from the global minimum Jmin, i.e., ϵ = |ˆJmin −Jmin|. We also present the total running time, τ. Compared to other benchmarks, TTOpt is consistently fast, accurate and avoids random failures to converge seen in other algorithms. Additionally, TTOpt turns out one of the fastest gradient-free algorithms (GA, openES, cmaES, DE, NB, PSO), despite a simple Python implementation. One of the advantages of the proposed TTOpt approach is the possibility of its application to essentially multidimensional functions. In Table 2 we present the result of TTOpt for the F1 benchmark function of various dimensionality (results for other benchmarks are in Appendix B.1). Note that as a limit on the number of requests to the objective function we chose M = 104 ·d, and the values of the remaining parameters were chosen the same as above. As can be seen, even for 500-dimensional functions, the TTOpt method gives a fairly accurate result. 3.2 Application of TTOpt to Reinforcement Learning We used several continuous RL tasks implemented in Mujoco [63] and OpenAI-GYM [8]: Swimmer- v3 [16], LunarLanderContinuous-v2, InvertedPendulum-v2 and HalfCheetah-v3 [65]. In all experi- ments, the policy πis represented by a neural network with three hidden layers and with tanh and ReLU activations. Each layer is a convolution layer. See additional details about hyperparameters in Table 6 in Appendix B. We discretize (quantize) the values of agent’s weights. TheTTOpt method is used to optimize discrete agent’s weights in order to maximize the cumulative reward of the episode. This corresponds to on-policy learning. 19We chose rank Rusing the following heuristic. The minimal number of sweeps is ﬁxed as T = 5. It follows that the algorithm will need 2 ·T ·(dq) ·P ·R2 function calls. With a given limit on the number of function requests M, the rank can be estimated as R≤ √ M 2·T·d·q·P. 80 2 4 6 Number of function queries 104 0 1 2 3 4average rewards 103 HalfCheetah-v3 TTopt cmaES openES GA target reward 0 2500 5000 7500 10000 12500 15000 seconds 0 1 2 3 4average rewards 103 HalfCheetah-v3 TTopt cmaES openES GA target reward Figure 3: Training curves of TTOpt and baselines for HalfCheetah-v3 (N = 3). The upper plot is the average cumulative reward versus the number of interactions with the environment. The lower plot is the same versus execution time. The reward is averaged for seven seeds. The shaded area shows the difference of one standard deviation around the mean. See similar plots for other environments in Appendix B.5. To properly compare TTOpt with other methods, we propose modiﬁed evolutionary baselines that enforce constrained parameter domain. We adapt penalty term and projection techniques from [33, 6] to introduce constraints, see Appendix B.4 for details. First, we run benchmarks with small mode size N = 3 1 with lower and upper grid bounds ±1. Another series of experiments was done with ﬁner mode of sizeN = 2q with the same bounds. These experiments model the case of neural networks with discrete (quantized) weights which use q-bits quantization. Finally, we provide the results of using TTOpt as a ﬁne-tuning procedure for linear policies from [39]. We present characteristic training curves based on the number of environment interactions and execution time for the HalfCheetah-v3 experiment (N = 3) in Figure 3. Training curves for other environments can be found in Appendix B.5, in Figure 3 for N = 3 and in Figure 4 for N = 256. TTOpt consistently outperforms all other baselines on the coarse grid with mode size N = 3 . Our method is still best for ﬁner grids with mode size N = 256 on InvertedPendulum-v2 and LunarLanderContinuous-v2, and second-best on HalfCheetah-v3. Moreover, TTOpt has signiﬁcantly lower execution time compared to evolutionary baselines. Another interesting observation is that the training curves of TTOpt have low dispersion, e.g., the algorithm performs more consistently than the baselines (see Appendix B.5). Table 3 summarizes the experiments for the coarse and ﬁne grids. Results for ﬁne-tuning of linear policies are presented in Table 5 in Appendix B.5. We also did rank and reward dependency study in Appendix B. 4 Related work In the case of high dimensional optimization, evolutionary strategies (ES) [ 59, 45] are one of the most advanced methods of black-box optimization. This approach aims to optimize the parameters of the search distribution, typically a multidimensional Gaussian, to maximize the objective function. Finite difference schemes are commonly used to approximate gradients of the parameters of the search distribution. Numerous works proposed techniques to improve the convergence of ES [45]. [66] proposed to use second-order natural gradient of [12] to generate updates, while [ 23] suggested to include the history of recent updates to generate next ones. [ 38] presented the concept of surrogate gradients for faster convergence. Another series of works aimed to reduce the high sampling complexity of ES. In [ 11] the au- thors described how to use active subspaces [ 14] to reduce the number of objective function calls dynamically. Plenty of the already mentioned works in gradient-free optimization speciﬁcally applied these methods to RL tasks [ 56, 10, 11, 39, 15, 35, 24]. Overall, the performance of ES-based methods 9is comparable to conventional policy gradients, especially if the number of model parameters is small [57, 58]. Another advantage of ES over policy gradient is that it produces more robust and diverse policies [34, 35] by eliminating the problem of delayed rewards and short length time horizons. Finally, evolutionary approaches are suitable for the problems with non-Markovian properties [24]. Other metaheuristic [26, 41] and classical optimization [53] techniques are also studied within RL scope. The examples include simulated annealing [4], particle swarm optimization [25, 36] and even classical Nelder-Mead algorithm [44, 46]. These methods, however, are not tested on common RL task sets. Several other works combined evolutionary methods with RL to achieve better performance in complex scenarios [30, 18], e.g. AlphaStar [64, 3]. Finally, low-rank tensor approximations have been applied to RL problems in settings different from ours, including multi-agent scenarios [ 37], and improving dynamic programming approaches [20, 7]. The idea of our method is quite different from the presented works, especially in the RL area. 5 Conclusion We proposed a new discrete optimization method based on quantized tensor-train representation and maximal volume principle. We demonstrate its performance for analytical benchmark functions and reinforcement learning problems. Our algorithm is more efﬁcient under a ﬁxed computational budget than baselines, especially on discrete domains. Moreover, the execution time for TTOpt is lower by a signiﬁcant margin compared with other baselines. Finally, we show that the agents with discrete parameters in RL can be as efﬁcient as their continuous parameter versions. This observation supports the broad adoption of quantization in machine learning. We hope that our approach will serve as a bridge between continuous and discrete optimization methods. Acknowledgments and Disclosure of Funding The work was supported by Ministry of Science and Higher Education grant No. 075-10-2021-068. References [1] Salman Ahmadi-Asl, Cesar F Caiafa, Andrzej Cichocki, Anh Huy Phan, Toshihisa Tanaka, Ivan Oseledets, and Jun Wang. Cross tensor approximation methods for compression and dimensionality reduction. IEEE Access, 9:150809–150838, 2021. [2] Stéphane Alarie, Charles Audet, Aïmen E. Gheribi, Michael Kokkolaras, and Sébastien Le Digabel. Two decades of blackbox optimization applications. EURO Journal on Computational Optimization, 9:100011, 2021. [3] Kai Arulkumaran, Antoine Cully, and Julian Togelius. Alphastar: An evolutionary computation perspective. In Proceedings of the Genetic and Evolutionary Computation Conference Com- panion, GECCO ’19, page 314–315, New York, NY , USA, 2019. Association for Computing Machinery. [4] A.F. Atiya, A.G. Parlos, and L. Ingber. A reinforcement learning method based on adaptive simulated annealing. In 2003 46th Midwest Symposium on Circuits and Systems , volume 1, pages 121–124 V ol. 1, 2003. [5] Pauline Bennet, Carola Doerr, Antoine Moreau, Jeremy Rapin, Fabien Teytaud, and Olivier Teytaud. Nevergrad: Black-box optimization platform. SIGEVOlution, 14(1):8–15, apr 2021. [6] Rafał Biedrzycki. Handling bound constraints in cma-es: An experimental study. Swarm and Evolutionary Computation, 52:100627, 2020. [7] AI Boyko, IV Oseledets, and G Ferrer. Tt-qi: Faster value iteration in tensor train format for stochastic optimal control. Computational Mathematics and Mathematical Physics, 61(5):836– 846, 2021. [8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. ArXiv, abs/1606.01540, 2016. [9] Cesar F. Caiafa and Andrzej Cichocki. Generalizing the column–row matrix decomposition to multi-way arrays. Linear Algebra and its Applications, 433(3):557–573, 2010. 10[10] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 970–978. PMLR, 10–15 Jul 2018. [11] Krzysztof M Choromanski, Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, and Vikas Sind- hwani. From complexity to simplicity: Adaptive es-active subspaces for blackbox optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [12] Andrzej Cichocki and Shun-ichi Amari. Adaptive Blind Signal and Image Processing: Learning Algorithms and Applications. John Wiley & Sons, Inc., USA, 2002. [13] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan, Qibin Zhao, and Danilo P. Mandic. Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low- rank tensor decompositions. Foundations and Trends® in Machine Learning, 9(4-5):249–429, 2016. [14] Paul G. Constantine. Active subspaces - emerging ideas for dimension reduction in parameter studies. In SIAM spotlights, 2015. [15] Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, page 5032–5043, Red Hook, NY , USA, 2018. Curran Associates Inc. [16] Rémi Coulom. Reinforcement Learning Using Neural Networks, with Applications to Motor Control. PhD thesis, Institut National Polytechnique de Grenoble, 2002. [17] Tom Erez, Yuval Tassa, and Emanuel Todorov. Inﬁnite-horizon model predictive control for periodic tasks with contacts. In Hugh F. Durrant-Whyte, Nicholas Roy, and Pieter Abbeel, editors, Robotics: Science and Systems VII, University of Southern California, Los Angeles, CA, USA, June 27-30, 2011, 2011. [18] Aleksandra Faust, Anthony G. Francis, and Dar Mehta. Evolving rewards to automate reinforce- ment learning. In 6th ICML Workshop on Automated Machine Learning, 2019. [19] Sergei A Goreinov, Ivan V Oseledets, Dimitry V Savostyanov, Eugene E Tyrtyshnikov, and Nikolay L Zamarashkin. How to ﬁnd a good submatrix. In Matrix Methods: Theory, Algorithms And Applications: Dedicated to the Memory of Gene Golub, pages 247–256. World Scientiﬁc, 2010. [20] Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk. High-dimensional stochastic optimal control using continuous tensor decompositions.The International Journal of Robotics Research, 37(2-3):340–377, 2018. [21] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [22] Wolfgang Hackbusch and Stefan Kühn. A new scheme for the tensor representation. Journal of Fourier analysis and applications, 15(5):706–722, 2009. [23] Nikolaus Hansen. The CMA Evolution Strategy: A Comparing Review, pages 75–102. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006. [24] Verena Heidrich-Meisner and Christian Igel. Neuroevolution strategies for episodic reinforce- ment learning. Journal of Algorithms, 64(4):152–168, 2009. Special Issue: Reinforcement Learning. [25] Daniel Hein, Alexander Hentschel, Thomas Runkler, and Steffen Udluft. Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies. Engineering Applications of Artiﬁcial Intelligence, 65:87–98, 2017. [26] J. Michael Herrmann, Adam Price, and Thomas Joyce. 3. Ant colony optimization and rein- forcement learning, pages 45–62. De Gruyter, 2020. [27] John H. Holland. Genetic algorithms. Scientiﬁc American, 267(1):66–73, 1992. 11[28] Sebastian Holtz, Thorsten Rohwedder, and Reinhold Schneider. The alternating linear scheme for tensor optimization in the tensor train format. SIAM Journal on Scientiﬁc Computing , 34(2):A683–A713, 2012. [29] Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2):150–194, 2013. [30] Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, page 1196–1208, Red Hook, NY , USA, 2018. Curran Associates Inc. [31] Boris N. Khoromskij. O(dlog n)-quantics approximation of n-d tensors in high-dimensional numerical modeling. Constructive Approximation, 34(2):257–280, Oct 2011. [32] Tamara G. Kolda, Robert Michael Lewis, and Virginia Torczon. Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review, 45(3):385–482, 2003. [33] Oliver Kramer. A review of constraint-handling techniques for evolution strategies.Appl. Comp. Intell. Soft Comput., 2010, January 2010. [34] Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O. Stanley. Es is more than just a traditional ﬁnite-difference approximator. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’18, page 450–457, New York, NY , USA, 2018. Association for Computing Machinery. [35] Fei-Yu Liu, Zi-Niu Li, and Chao Qian. Self-guided evolution strategies with historical estimated gradients. In Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI’20, 2021. [36] Tundong Liu, Liduan Li, Guifang Shao, Xiaomin Wu, and Meng Huang. A novel policy gradient algorithm with pso-based parameter exploration for continuous control. Eng. Appl. Artif. Intell., 90(C), apr 2020. [37] Anuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg, Jean Kossaiﬁ, Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar. Tesseract: Tensorised actors for multi-agent reinforcement learning. InInternational Conference on Machine Learning (ICML), volume 139, pages 7301–7312, 2021. [38] Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided evolutionary strategies: augmenting random search with surrogate gradients. In Ka- malika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 4264–4273. PMLR, 09–15 Jun 2019. [39] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is competitive for reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. [40] Erich Merrill, Alan Fern, Xiaoli Fern, and Nima Dolatnia. An empirical study of bayesian optimization: Acquisition versus partition. Journal of Machine Learning Research, 22(4):1–25, 2021. [41] Laurent Meunier, Herilalaina Rakotoarison, Pak Kan Wong, Baptiste Roziere, Jérémy Rapin, Olivier Teytaud, Antoine Moreau, and Carola Doerr. Black-box optimization revisited: Im- proving algorithm selection wizards through massive benchmarking. IEEE Transactions on Evolutionary Computation, pages 1–1, 2021. [42] Aleksandr Mikhalev and Ivan V Oseledets. Rectangular maximum-volume submatrices and their applications. Linear Algebra and its Applications, 538:187–211, 2018. [43] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe- tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, Feb 2015. [44] John A. Nelder and Roger Mead. A simplex method for function minimization. Computer Journal, 7:308–313, 1965. 12[45] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527–566, Apr 2017. [46] Barry D. Nichols. Continuous action-space reinforcement learning methods applied to the minimum-time swing-up of the acrobot. In 2015 IEEE International Conference on Systems, Man, and Cybernetics, pages 2084–2089, 2015. [47] I. V . Oseledets. Approximation of2d×2d matrices using tensor decomposition. SIAM J. Matrix Anal. Appl., 31(4):2130–2145, 2010. [48] I. V . Oseledets. Tensor-train decomposition.SIAM Journal on Scientiﬁc Computing, 33(5):2295– 2317, 2011. [49] Ivan V Oseledets and Eugene E Tyrtyshnikov. Breaking the curse of dimensionality, or how to use svd in many dimensions. SIAM Journal on Scientiﬁc Computing, 31(5):3744–3759, 2009. [50] Ivan V Oseledets and Eugene E Tyrtyshnikov. TT-cross approximation for multidimensional arrays. Linear Algebra and its Applications, 432(1):70–88, 2010. [51] David Pfau, James S. Spencer, Alexander G. D. G. Matthews, and W. M. C. Foulkes. Ab initio solution of the many-electron schrödinger equation with deep neural networks. Phys. Rev. Research, 2:033429, Sep 2020. [52] Anh-Huy Phan, Andrzej Cichocki, André Uschmajew, Petr Tichavský, George Luta, and Danilo P. Mandic. Tensor networks for latent variable analysis: Novel algorithms for tensor train approximation. IEEE Transactions on Neural Networks and Learning Systems, 31(11):4622– 4636, 2020. [53] Zhiwei Qin, Weichang Li, and Firdaus Janoos. Sparse reinforcement learning via convex optimization. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 424–432, Bejing, China, 22–24 Jun 2014. PMLR. [54] Marc H. Raibert, Jr H. Benjamin Brown, and Michael Chepponis. Experiments in balance with a 3d one-legged hopping machine. The International Journal of Robotics Research, 3(2):75–92, 1984. [55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv, abs/2102.12092, 2021. [56] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. ArXiv, abs/1703.03864, 2017. [57] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 ofProceedings of Machine Learning Research, pages 1889–1897, Lille, France, 07–09 Jul 2015. PMLR. [58] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017. [59] Hans-Paul Schwefel. Evolutionsstrategien für die numerische Optimierung, pages 123–176. Birkhäuser Basel, Basel, 1977. [60] Artur M. Schweidtmann and Alexander Mitsos. Deterministic global optimization with artiﬁcial neural networks embedded. Journal of Optimization Theory and Applications, 180(3):925–948, Oct 2018. [61] Rainer Storn and Kenneth Price. Differential evolution – a simple and efﬁcient heuristic for global optimization over continuous spaces. Journal of Global Optimization, 11(4):341–359, Dec 1997. [62] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. ArXiv, abs/1712.06567, 2017. [63] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033, 2012. 13[64] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McK- inney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, Nov 2019. [65] Pawel Wawrzynski. Learning to control a 6-degree-of-freedom walking robot. In EUROCON 2007 - The International Conference on \"Computer as a Tool\", pages 698–705, 2007. [66] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen Schmidhuber. Natural evolution strategies. Journal of Machine Learning Research, 15(27):949–980, 2014. [67] Qibin Zhao, Masashi Sugiyama, Longhao Yuan, and Andrzej Cichocki. Learning efﬁcient tensor representations with ring-structured networks. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8608–8612, 2019. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] The work has no any societal impacts, since its about optimization methods. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] The code will be provided in supplemental material and as github repository after decision (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running exper- iments multiple times)? [Yes] We used a typical shade(mean-std) plots that used in reinforcement learning experiments (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] . We mentioned all references and source-codes for gradient-free baselines. (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... 14(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] A Description of MaxVol and TTOpt algorithms A.1 Maxvol Algoritm The search for the maximal element in a large matrix can be signiﬁcantly simpliﬁed if one can obtain a \"good\" submatrix, for example, a maximal-volume submatrix. Finding the maximal-volume submatrix of a nondegenerate matrix A∈RN×R,N > Ris an NP-hard problem. In this paper, we adapted the maxvol algorithm [19] which can ﬁnd a submatrix C∈RR×R of A, such that its determinant is close to maximum in absolute value. The algorithm selects a set of Rrows denoted by I⊂[1,...,N ] which form the matrix C= A[I,:]. For the given tolerance threshold ϵ(ϵ≥1 and close to one), we ﬁnd the set Ias follows: 1. Compute the LU-decomposition A= PLU and store the permutation of the ﬁrst Rrows (according to the matrix P) in the list I. 2. Generate the matrix Q∈RR×N as a solution to the linear system with an upper triangular matrix U: UTQ= AT. 3. Compute the matrix B∈RN×R as a solution to the linear system with the lower triangular matrix L: (L[: R,:])TBT = Q. 4. Find the maximum modulo element b= Bi,j of the matrix B. If |b|≤ ϵ, then terminate the algorithm20 by returning the current list I and matrix B. 5. Update the matrix B ←B−B[:,j] (B[i,:] −eT j ) b−1, where ej is the j-th unit basis vector. 6. Update the list Ias I[j] ←i. 7. Return to step 4. It can be shown that at each step kof the algorithm the volume of the submatrix Cincreases by a factor not less than ϵ. Therefore, the estimate for the number of iterations, K, is the following: K = log (|det ( ˆC)|) −log (|det (C(0))|) log ϵ , (8) where ˆCis the exact maximal-volume submatrix and C(0) is an initial approximation obtained from the LU-decomposition (see step 1 above). The computational complexity of the algorithm is the sum of the initialization complexity and the complexities of Kiterations. The complexity equals O ( NR2 + KNR ) . (9) In the context of the maximal matrix element search problem, other deﬁnitions of \"good\" submatrices are possible. For example, one can search for a rectangular submatrix containing rows or columns which span the largest volume. The rect_maxvol algorithm can be used in this case. We employ rect_maxvol [42] to choose rectangular submatrices within the framework of the TTOpt algorithm to adaptively increase the rank. Consider an arbitrary nondegenerate matrix A∈RN×R (N >R). We search for a rectangular sub- matrix ˆC∈R(R+∆R)×R,R + ∆R<N , which maximizes the rectangular volume √ det ( ˆCT ˆC) objective. An approximation Cto ˆCcan be found as follows. The ﬁrst Rrows of Care obtained by the maxvol algorithm. The following rect_maxvol algorithm will ﬁnd additional ∆Rrows: 20In addition to the indices of the rows Ithat form the maximal-volume submatrix C ∈RR×R, we also obtain the matrix of coefﬁcients B∈RN×R such that A= BC. 151. First, generate Rindices of rows I ∈NR and the coefﬁcient matrix B ∈RN×R of the initial approximation using the maxvol algorithm. 2. Compute the vector l∈RN containing the norms of the rows: l[j] = (B[j,:])TB[j,:] for j = 1,2,...,N . 3. Find the maximum modulo element of the vector l, i.e. i= argmax(l). 4. If the current lengthR+∆Rof the vector Iis greater than or equal toR(max) or if l[i] ≤τ2, then terminate the algorithm21 by returning the current list Iand matrix B. 5. Update the coefﬁcient matrix as B:= [ B−B(B[i,:])T B[i,:] 1+B[i,:](B[i,:])T B(B[i,:])T 1+B[i,:](B[i,:])T ] . 6. Update the vector of row norms l[j] := l[j] − |B[j,:](B[i,:])T|2 1 + B[i,:](B[i,:])T , j = 1,2,...,N. 7. Add current row index ito the list I. 8. Return to step 3. The approximate computational complexity of this algorithm according to the work [42] is O(NR2), and the expected number of rows in the resulting submatrix is 2R−1 for the case τ = 1. A.2 TTOpt Algorithm In Algorithm A.1 we present the details of the TTOpt implementation. The TTOpt algorithm builds the TT proxy of the minimized function which is iteratively updated. The procedure of updating the TT proxy is outlined in Algorithm A.2 (function update_left, that updates core tensors of the network from right to left ) and in Algorithm A.3 (function update_right, updates core tensors of the network from left to right ). The requests to the objective function and the transformation of resulting values are presented in Algorithm A.4 (function eval). The procedure begins by building one-dimensional uniform gridsxi (i= 1,2,...,d ) for the function argument along each mode, using the speciﬁed bounds of the rectangular search domain Ω. Note that, if necessary, arbitrary nonuniform grids can be used, taking into account the speciﬁc features of the function under consideration. 21As a criterion for stopping the algorithm, we consider either the achievement of the maximum number of rows in the maximal-volume submatrix (R(max)), or the sufﬁciently small norm of the remaining rows in the matrix of coefﬁcients B. 16Algorithm A.1: Multivariable function minimizer TTOpt. Data: function J(θ), where θ∈Ω ⊂Rd; boundary points of the rectangular domain Ω = [a1,b1] ×[a2,b2] ×···× [ad,bd]; the number of grid points for every dimension N1,N2,...,N d; number of inner iterations (sweeps) kmax; maximum TT-rank rmax. Result: approximation of the spatial point θmin ∈Rd at which the function J reaches its minimum in the region Ω and a corresponding function value Jmin ∈R. 1 // Construct a uniform grid x1,x2,..., xd: 2 Set xi[m] = ai + (bi −ai) ·m−1 Ni−1 3 for all i= 1,...,d and m= 1,2,...,N i. 4 // Initialize the TT-ranks R0,R1,...,R d: 5 Set R0 = 1. 6 for i= 1 to (d−1) do 7 Set Ri = min (Ri−1 ·Ni,Ri ·Ni,rmax). 8 end 9 Set Rd = 1. 10 // Initialize set of points of interest X0,X1,..., Xd: 11 Set X0 = None. 12 for i= 0 to (d−2) do 13 Set Gi = random(Ri ·Ni,Ri+1) from the standard normal distribution. 14 Compute QR-decomposition Q,R= QR(Gi). 15 Compute indices mopt = maxvol(Q). 16 Set Xi+1 = update_right(Xi,xi,Ni,Ri,mopt). 17 end 18 Set Xd = None. 19 // Iterate in a loop to ﬁnd optimal θmin and Jmin: 20 Set θmin = None and Jmin = +∞. 21 for k= 1 to kmax do 22 // Traverse the TT-cores from right to left: 23 for i= dto 1 do 24 Compute z,θmin,Jmin = eval(∗). 25 Reshape zto matrix Z∈RRi×ni·Ri+1 . 26 Compute Q,R= QR(ZT). 27 Compute indices mopt = rect_maxvol(Q[:,0 : R[i]]). 28 Set Xi = update_left(Xi+1,xi,Ni,Ri+1,mopt). 29 end 30 // Traverse the TT-cores from left to right: 31 for i= 1 to ddo 32 Compute z,θmin,Jmin = eval(∗). 33 Reshape zto matrix z∈RRi·ni×Ri+1 . 34 Compute Q,R= QR(Z). 35 Compute indices mopt = rect_maxvol(Q). 36 Set Xi = update_right(Xi+1,xi,Ni,Ri+1,mopt). 37 end 38 end 39 return (θmin, Jmin). Algorithm A.2: Function update_left to update points of interest when traverse the tensor modes from right to left. Data: current set of points for the (i+ 1)-th mode Xi+1; grid points xi; number of grid points Ni; TT-rank Ri+1; list of indices to be selected mopt. Result: new set of points Xi. 1 Set W1 = ones(Ri+1) ⊗xi and W2 = Xi+1 ⊗ones(Ni) 2 Set Xi = [W1,W2]. 3 Select subset of rows Xi = Xi[mopt,:]. 4 return Xi. 17Algorithm A.3: Function update_right to update points of interest when traverse the tensor modes from left to right. Data: current set of points for the i-th mode Xi; grid points xi; number of grid points Ni; TT-rank Ri; list of indices to be selected mopt. Result: new set of points Xi+1. 1 Set W1 = ones(Ni) ⊗Xi and W2 = xi ⊗ones(Ri). 2 Set Xi+1 = [W1,W2]. 3 Select subset of rows Xi+1 = Xi+1[mopt,:]. 4 return Xi+1. Algorithm A.4: Function eval to compute the target function in points of interest. Data: Xi; Xi+1; Ri; Ni; Ri+1; J; θmin, Jmin (see Algorithm A.1 for details). Result: transformed function values z∈RRi·Ni·Ri+1 , updated θmin and Jmin. 1 Set W1 = ones(Ni ·Ri+1) ⊗Xi. 2 Set W2 = ones(Ri+1) ⊗xi ⊗ones(Ri). 3 Set W3 = Xi+1 ⊗ones(Ri ·Ni). 4 Set Xcurr = [W1,W2,W3] ∈RRi·Ni·Ri+1×d. 5 // Compute function for each point in Xcurr: 6 Set ycurr = J(Xcurr). 7 if min (ycurr) <Jmin then 8 Set mmin = argmin(ycurr). 9 Set θmin = Xcurr[mmin,:]. 10 Set Jmin = ycurr[mmin]. 11 end 12 // Compute smooth function for each value in ycurr: 13 Set z= π 2 −arctan (ycurr −Jmin). 14 return (z,θmin,Jmin). We then randomly initialize the TT proxy tensor with input ranks rmax. If necessary, we reduce some of the ranks to satisfy the condition Ri−1Ni ≥Ri (i= 1,2,...,d −1). Note that in this case, for all TT-cores Gi ∈RRi−1×Ni×Ri (i= 1,2,...,d ), the right unfolding matrices G(2) i ∈RRi−1·Ni×Ri will turn out to be “tall” matrices, that is, their number of rows is not less than the number of columns, and hence we can apply maxvol and rect_maxvol algorithms to these matrices. Next, we iteratively traverse all tensor modes (using corresponding TT-cores) in the direction from right to left and vice versa. We evaluate (and transform) the objective function to reﬁne the selected rows and columns. For each k-th mode of the tensor we evaluate the submatrixJ(C) k ∈RRk−1·Nk×Rk of the corresponding unfolding matrix, compute its QR decomposition, ﬁnd the row indices of the rectangular maximal-volume submatrix ˆJk ∈R(Rk+∆Rk)×Rk of the Q factor and add resulting indices of the original tensor to the index set Xk. The arguments for target function evaluation in Algorithm A.4 are selected as merged left and right index sets, constructed from previous rect_maxvol computations. After each request to the objective function, we update the current optimal value Jmin and then transform the calculated values by the mapping (7) described in the main text. B Additional Experiments B.1 Experiments with benchmark functions In Section 3.1 we compared theTTOpt solver22 with baseline methods, applied to various model func- tions [29]. The list of functions is presented in Table 4. For each function, we provide the lower/upper 22We implemented the TTOpt algorithm in a python package with detailed documentation, demos, and reproducible scripts for all benchmark calculations. 18Table 4: Benchmark functions for comparison of the considered optimization algorithms and perfor- mance evaluation of the TTOpt approach. For each function, we present the lower grid bound (a), the upper grid bound (b), the global minimum (Jmin) and the analytical formula. Note that Jmin for the F6 function is given for the 10-dimensional case. Function a b Jmin Formula F1 (Ackley) −32.768 32.768 0. f(x) = −Ae−B √ 1 d ∑d i=1 x2 i − e 1 d ∑d i=1 cos (Cxi) + A+ e1, where A = 20 , B = 0.2 and C = 2π F2 (Alpine) −10 10 0. f(x) = ∑d i=1 |xisin xi + 0.1xi| F3 (Brown) −1 4 0. f(x) = ∑d−1 i=1 ( x2 i )(x2 i+1+1) + ( x2 i+1 )(x2 i +1) F4 (Exponential) −1 1 −1. f(x) = −e−1 2 ∑d i=1 x2 i F5 (Griewank) −600 600 0. f(x) = ∑d i=1 x2 i 4000 −∏d i=1 cos ( xi√ i ) + 1 F6 (Michalewicz) 0 π −9.66015 f(x) = −∑d i=1 sin (xi) sin2m ( ix2 i π ) F7 (Qing) 0 500 0. f(x) = ∑d i=1 ( x2 i −i )2 F8 (Rastrigin) −5.12 5.12 0. f(x) = A·d+∑d i=1 ( x2 i −A·cos (2π·xi) ) , where A= 10 F9 (Schaffer) −100 100 0 f(x) = ∑d−1 i=1 (0.5 + sin2 (√ x2 i +x2 i+1)−0.5 (1+0.001(x2 i +x2 i+1)) 2 ) F10 (Schwefel) −500 500 0. f(x) = 418.9829 ·d−∑d i=1 xi ·sin ( √ |xi|) grid bounds (aand b) and global minimum (Jmin). Note that many benchmarks is multimodal (have two or more local optima), introducing additional complications into the optimization problem. The main conﬁgurable parameters of our solver are the mode size ( N; N = 2 q in the case of quantization, where q is the number of submodes in the quantized tensor); the rank ( R), and the limit on the number of requests to the objective function (M). The choice of these parameters can affect the ﬁnal accuracy of the optimization process. Below we present the results of the studies of parameter importance. In all calculations, we ﬁxed the non-varying parameters at the values provided in Section 3.1. Mode size inﬂuence. To reach high accuracy, we need ﬁne grids. As we indicated in Section 2.6, in this case, the quantization of the tensor modes seems attractive. We reshape the originald-dimensional tensor J ∈RN1×N2×···×Nd into the tensor ˜J ∈R2×2×···×2 of a higher dimension d·q, but with smaller modes of size 2, and apply the TTOpt algorithm to this “long” tensor instead of the original one. 19Table 5: Comparison of the “direct” (TT) and “quantized” (QTT) TTOpt solvers in terms of the ﬁnal error (absolute deviation of the obtained value from the exact minimum) for various benchmark functions. The reported values are averaged over ten independent runs. MODE SIZE F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 256 TT 1.2 E+00 2.0 E-02 0.0 E+00 7.7 E-05 1.0 E+00 9.8 E-02 9.4 E+01 8.0 E-01 4.2 E-01 4.5 E-01 QTT 1.2 E+00 2.3 E-02 0.0 E+00 7.7 E-05 1.0 E+00 1.6 E-01 9.4 E+01 8.0 E-01 3.9 E-01 4.5 E-01 1024 TT 1.6 E+01 4.2 E+00 3.0 E+01 2.6 E-01 5.7 E+01 2.0 E+00 1.9 E+10 4.7 E+01 1.5 E+00 1.1 E+03 QTT 1.8 E-01 8.2 E-03 6.9 E-05 4.8 E-06 2.1 E-01 7.1 E-02 5.2 E+00 5.0 E-02 1.2 E-01 2.8 E-02 4096 TT 1.9 E+01 1.5 E+01 5.0 E+08 5.1 E-01 1.4 E+02 5.7 E+00 5.1 E+10 9.8 E+01 3.5 E+00 2.6 E+03 QTT 3.5 E-02 1.8 E-03 0.0 E+00 3.0 E-07 3.9 E-02 4.3 E-02 2.6 E-01 3.1 E-03 8.7 E-02 1.0 E-02 16384 TT 2.0 E+01 1.9 E+01 9.9 E+17 5.9 E-01 1.7 E+02 5.9 E+00 6.3 E+10 1.2 E+02 3.8 E+00 3.2 E+03 QTT 8.2 E-03 7.8 E-04 2.7 E-07 1.9 E-08 2.6 E-02 8.9 E-02 2.2 E-02 1.9 E-04 1.2 E-01 3.8 E-04 65536 TT 2.0 E+01 1.9 E+01 1.2 E+10 6.7 E-01 2.2 E+02 7.8 E+00 7.1 E+10 1.6 E+02 4.4 E+00 3.6 E+03 QTT 2.0 E-03 1.3 E-04 1.2 E-09 1.2 E-09 2.2 E-02 7.1 E-02 9.4 E-04 1.2 E-05 1.4 E-01 1.6 E-04 262144 TT 2.1 E+01 2.3 E+01 1.7 E+16 8.0 E-01 3.0 E+02 8.4 E+00 8.9 E+10 1.8 E+02 4.5 E+00 3.7 E+03 QTT 5.0 E-04 3.3 E-05 1.1 E-09 7.3 E-11 3.0 E-02 3.8 E-02 3.7 E-05 7.6 E-07 1.4 E-01 1.3 E-04 1048576 TT 2.1 E+01 2.8 E+01 5.5 E+16 8.3 E-01 3.3 E+02 8.4 E+00 1.2 E+11 1.9 E+02 4.4 E+00 3.7 E+03 QTT 1.3 E-04 1.2 E-05 0.0 E+00 4.5 E-12 1.7 E-02 6.8 E-02 5.9 E-06 4.7 E-08 1.1 E-01 1.3 E-04 In Table 5 we present the comparison of optimization results for the basic algorithm without quan- tization (“TT”) and for the improved algorithm with quantization (“QTT”). For each value N of the mode size, we choose the number of submodes in the quantized tensor as q = log 2 N. The QTT-solver gives several orders of magnitude more accurate results than the TT-solver. At the same time, for the QTT-solver, a regular decrease in the error is observed with an increase in the mode size. Thus, for the stable operation of gradient-free optimization methods based on the low-rank tensor approximations, it is necessary to quantize the modes of the original tensor. Rank inﬂuence. The rank (the size of the maximal-volume submatrices) determines how many points are queried at each iteration of theTTOpt algorithm, and this parameter is similar to population size in evolutionary algorithms. Small maximal-volume submatrices may give a better bound for maximal elements (see Eq. (3) from the main text), but ﬁnding small submatrices may be more challenging for the algorithm and may lead to numerical instabilities. At the same time, when choosing rank R, we should take into account that the algorithm will need2·T·(dq)·P·R2 function calls, where T is the number of sweeps (it should be at least 1, however, for better convergence, it is worth taking values of4−5) and P = 2 is a submode size. Hence we have inequalityR≤ √ M 4·T·d·q·, where M is a given limit on the number of function requests. In Figure 4 we demonstrate the dependence of the TTOpt’s accuracy on the rank. As can be seen, with small ranks (1 or 2), we have too low accuracy for most benchmarks. At the same time, the accuracy begins to drop at too high-rank values (7 or more), which is due to the insufﬁcient number of sweeps taken by the algorithm for convergence. Number of function queries inﬂuence. The number of requests to the objective function can be determined automatically based on algorithm iterations. Thus, with a total number of sweeps T, we will have O ( T ·d·max1≤k≤d ( NkR2 k )) calls to the objective function. However, in practice, it turns out to be more convenient to limit the maximum number of function calls, M, according to the computational budget. In Figure 5 the dependence of the accuracy on the total number of requests, M, to the objective function is presented. Predictably, as M increases, the accuracy also increases. The plateau for benchmarks are associated with the dependence of the result on the remaining parameters (R, q) of the TTOpt solver. 20Figure 4: The dependence of the ﬁnal error (absolute deviation of the obtained value from the exact minimum) on the rank for various benchmark functions. The reported values are averaged over ten independent runs. Figure 5: The dependence of the ﬁnal error (absolute deviation of the obtained value from the exact minimum) on the number of target functions calls for various benchmark functions. The reported values are averaged over ten independent runs. Function dimensionality inﬂuence. One of the advantages of the proposed approach is the possi- bility of its application to essentially multidimensional functions. In Table 6 we present the results of TTOpt for functions of various dimensions (we removed the F6 function from benchmarks, since its optima are known only for 2, 5 and 10-dimensional cases). Note that as a limit on the number of requests of the objective function, we choose 104 ·d, and the values of the remaining parameters were chosen the same as above. As can be seen, even for 500-dimensional functions, the TTOpt method results in fairly accurate solutions for most benchmarks. However, for benchmarks F4, F7 and F9 the errors are larger than for lower dimensions. We suspect that our heuristic of the number of objective function evaluations is not accurate in these cases. 21Table 6: The result of the TTOpt optimizer in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for various benchmark functions and various dimension numbers (d). FUNCTION d = 10 d = 50 d = 100 d = 500 F1 ϵ 3.9 E-06 3.9 E-06 3.9 E-06 3.9 E-06 τ 3.1 37.8 131.0 3153.5 F2 ϵ 2.9 E-07 3.7 E-06 5.2 E-06 2.1 E-05 τ 2.5 36.3 129.5 3153.1 F3 ϵ 2.3 E-12 4.9 E-10 1.1 E-09 4.7 E-09 τ 2.6 36.8 132.6 3205.4 F4 ϵ 4.4 E-15 2.2 E-14 4.4 E-14 1.0 E+00 τ 2.5 35.6 129.2 3131.1 F5 ϵ 2.5 E-02 3.7 E-02 3.7 E-02 3.7 E-02 τ 2.5 36.1 130.4 3132.2 F7 ϵ 5.5 E-09 8.9 E-08 3.4 E-07 5.6 E+02 τ 2.5 35.6 130.2 3123.7 F8 ϵ 4.6 E-11 2.3 E-10 4.6 E-10 2.3 E-09 τ 2.5 35.6 130.1 3124.0 F9 ϵ 3.4 E-01 9.3 E-01 2.2 E+00 1.0 E+01 τ 2.5 36.0 130.6 3157.4 F10 ϵ 1.3 E-04 6.4 E-04 1.3 E-03 6.4 E-03 τ 2.6 35.7 130.0 3140.7 Table 7: Comparison of the TTOpt optimizer with Bayesian optimization [40] baselines in terms of the ﬁnal error ϵ(absolute deviation of the obtained optimal value relative to the global minimum) and computation time τ (in seconds) for various 10-dimensional benchmark functions. Note that τ values for Simultaneous Optimistic Optimization ( SOO), Direct Simultaneous Optimistic Optimization (dSOO), Locally Oriented Global Optimization (LOGO) and Random Optimization (RANDOM) refers to the time measured for a complied C-code, while our TTOpt optimizer is implemented in python, and will be more time-efﬁcient if written in C. ACKLEY RASTRIGIN ROSENBROCK SCHWEFEL TTO PT ϵ 3.9 E-06 4.6 E-11 3.9 E-01 8.4 E-02 τ 1.23 1.21 1.18 1.21 DSOO ϵ 4.0 E-10 2.0 E+00 8.1 E+00 5.3 E+02 τ 8.10 7.20 7.75 7.01 SOO ϵ 9.0 E-10 2.29 E+00 7.0 E+02 5.2 E+02 τ 7.44 7.57 0.66 7.31 LOGO ϵ 1.2 E-09 3.44 E+01 7.9 E+00 5.3 E+02 τ 6.80 0.77 7.06 7.51 RANDOM ϵ 1.1 E+01 2.29 E+00 1.5 E+00 1.2 E+03 τ 0.78 7.34 7.25 0.77 22B.2 Comparison with Bayesian optimization In Table 7 we present the results of TTOpt and several Bayesian methods for 10-dimensional benchmarks. We selected functions supported by the Bayesian optimization package from 23 [40]. Note that in all cases we chose 105 as the limit on the number of requests to the objective function and the values of the remaining parameters were chosen the same as above. TTOpt outperforms all tested Bayesian algorithms for Rastrigin, Rosenbrock, and Schwefel functions. For the Ackley function, the difference in accuracy is not signiﬁcant. On average, TTOpt is faster than Bayesian methods, despite they are implemented in C language. We stress that standard Bayesian methods are not applicable in higher-dimensional problems. B.3 Formulation of reinforcement learning problem as black-box optimization task Here we describe a typical reinforcement learning setting within Markov decision process formalism. The agent acts in the environment that has a set of states S. In each state s∈S the agent takes an action from a set of actions a∈A. Upon taking this action, the agent receives a local reward r(s,a) and reaches a new state s′, determined by the transition probability distribution T(s′|s,a). The policy π(a|s) speciﬁes which action the agent will take depending on its current state. Upon taking T (T is also called horizon) actions, the agent receives a cumulative reward, deﬁned as J = T−1∑ t=0 γtr(st,at), (10) where γ ∈[0,1] the is discounting factor, speciﬁes the relevance of historic rewards for the current step. The goal of the agent is to ﬁnd the policy π∗(a|s) that maximizes the expected cumulative reward J over the agent’s lifetime. In policy-based approaches, the policy is approximated by a function π(a|s,θ) (for example, a neural network), which depends on a vector of parameters θ. It follows then that the cumulative reward is a function of the parameters of the agent: J(θ) = E(st,at)∼T,π(θ) [T−1∑ t=0 γtr(st,at)) ] , (11) where r(st,at) ∼r(st,π(st−1 |θ). In case of episodic tasks we can assume γ = 1. Finding an optimal policy can be done by maximizing the cumulative reward J with respect to parameters θ: π∗(a|s) = π(a|s,θ∗), (12) where θ∗≃argmax J(θ). Notice that J may be non-differentiable due to the stochastic nature of T or the deﬁnition of r, depending on a particular problem formulation. However, this does not pose a problem for direct optimization algorithms. To summarize, the RL problem can be transformed into a simple optimization problem for the cumulative reward J(θ). The parameters of this function are the weights of the agent. Optimization of the cumulative reward with direct optimization algorithms is an on-policy learning in RL algorithm classiﬁcation. B.4 Rank dependence study Since rank is an important parameter of our method, we studied its inﬂuence on the rewards in RL, see Figure 5. Note that the rank determines how many points are queried at each iteration, and this parameter is similar to population size in evolutionary algorithms. We found almost no dependency of the ﬁnal reward on rank after R> 3 (on average). The Eq. (3) from the main text states that small maximal-volume submatrices should give a better bound for the maximal element. However, ﬁnding small submatrices may be more challenging for the algorithm. It turns out that reward functions in considered RL tasks are \"good\" for the maximum volume heuristic, e.g., even with small ranks, the algorithm produces high-quality solutions. 23The source code is available at https://github.com/Eiii/opt_cmp 23Table 8: The mean and standard deviation ( E ±σ) of ﬁnal cumulative reward before and after ﬁne-tuning with TTOpt. The policy’s weights are from the original repository of ARS [39]. ARS [39] ARS TTO PT(28) ANT-V3 4972.48 ±21.58 5039.90 ±57.00 HALF CHEETAH -V3 6527.89 ±82.70 6840.39 ±87.41 HOPPER -V3 3764.74 ±355.08 3296.49 ±11.81 HUMANOID -V3 11439.79 ±51.44 11560.01 ±54.08 SWIMMER -V3 354.43 ±2.32 361.87 ±1.76 WALKER 2D-V3 11519.77 ±112.55 11216.25 ±88.32 Table 9: The number of hidden units in each layer of convolutional policy h, the total number of parameters d, the sizes of the state and action spaces Aand S, the rank Rand the activation function between the layers (Act.). The average number of function quires per iteration (population size) in the case of TTOpt and ES baselines, respectively, is denoted by Q(the values separated by a comma). The number of seeds is Sd. H D S A R Act. Q Sd S 8 55 8 2 3 TANH 55,64 7 L 8 55 8 2 3 RELU 53,64 7 I 4 26 4 1 3 TANH 57,64 7 H 4 44 17 6 5 TANH 120,128 7 B.5 Constraint Handling in Evolutionary Algorithms There are two options to satisfy constraints in evolutionary computation called projection and penalization. These steps can be represented as two functions, θp = fproj(θ), and fpen(θp,θ) with a regularization term: Jp(θ) = J(θp) −λfpen(θp,θ). (13) In this work, we use the constraint functions described below. CDF projection is applied in exper- iments with mode size N = 3 (see Table 3 from the main text). The idea is to use the cumulative density function to map normally distributed parameters of the policies to {-1,0,1} set: θ=    −1 CDF(θ) ≤1 3 , 0 1 3 <CDF(θ) < 2 3 , 1 CDF(θ) ≥2 3 . (14) Uniform projection is applied when N = 256 in experiments shown in Table (3) from the main text. In this case, the idea is to keep the value if it satisﬁes the bounds, otherwise, we draw a new sample uniformly from a grid deﬁned in Algorithm A.1: θi p = {θi, if L≤θi ≤U, xi[k] otherwise. (15) Quadratic penalty is applied in all experiments. If Land U are the bounds, then fpen(θp,θ) =∑ i:θi<L(L−θi)2 + ∑ i:θi>U(θi −U)2. We set λ= 0.1 in all experiments. B.6 Reinforcement Learning Experiments Figure 6 and Figure 7 show training curves for all test environments which were not included in the main text. Fine-tuning of Linear Policies. We use TTOpt to ﬁne-tune Augmented Random Search (ARS) [39] linear policies obtained from the original paper. The cost function is the average of seven independent episodes with ﬁxed random seeds. The upper and lower grid bounds are estimated using statistics of pre-trained linear policies: bi = θi ±α·σ(θ) with α= 0.1. For Ant, Humanoid [ 17], Walker [17] and HalfCheetah [65] we select α= 0.5, and for Swimmer [16] and Hopper [54] we set α= 1. 240.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Number of function queries 104 0.0 0.1 0.2 0.3 average rewards 103 Swimmer-v3 0 1000 2000 3000 4000 seconds 0.0 0.1 0.2 0.3 average rewards 103 Swimmer-v3 0.0 0.2 0.4 0.6 0.8 1.0 Number of function queries 104 0.00 0.25 0.50 0.75 1.00 average rewards 103 InvertedPendulum-v2 0 50 100 150 200 250 300 seconds 0.0 0.2 0.4 0.6 0.8 1.0 average rewards 103 InvertedPendulum-v2 0 1 2 3 4 5 6 7 Number of function queries 104 0 1 2 3 4average rewards 103 HalfCheetah-v3 0 2000 4000 6000 8000 10000 12000 14000 seconds 0 1 2 3 4average rewards 103 HalfCheetah-v3 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Number of function queries 104 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 TTOpt cmaES openES GA target reward 0 500 1000 1500 2000 2500 3000 seconds 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 Figure 6: Training curves of TTOpt and baselines for N = 3 possible weight values: (−1,0,1). Left is the dependence of the average cumulative reward on the number of interactions with the environment (episodes). Right is the same reward depending on the execution time. The reward is averaged for seven seeds. The shaded area shows the difference of one standard deviation around the mean. 250 1 2 3 4 5 6 7 Number of function queries 104 0 1 2 3 4average rewards 103 HalfCheetah-v3 0 2000 4000 6000 8000 10000 12000 14000 seconds 0 1 2 3average rewards 103 HalfCheetah-v3 0 1 2 3 4 5 6 7 Number of function queries 104 0.0 0.1 0.2 0.3 0.4 average rewards 103 Swimmer-v3 0 5000 10000 15000 20000 seconds 0.0 0.1 0.2 0.3 average rewards 103 Swimmer-v3 0 2 4 6 8 10 Number of function queries 104 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 0 1000 2000 3000 4000 5000 6000 7000 seconds 0.0 0.1 0.2 0.3 average rewards 103 LunarLanderContinuous-v2 0 2 4 6 8 10 Number of function queries 104 0.00 0.25 0.50 0.75 1.00 average rewards 103 InvertedPendulum-v2 TTOpt cmaES openES GA target reward 0 500 1000 1500 2000 2500 3000 seconds 0.0 0.2 0.4 0.6 0.8 1.0 average rewards 103 InvertedPendulum-v2 Figure 7: Training curves of TTOpt and baselines for N = 256 possible weight values. Left is the dependence of the average cumulative reward on the number of interactions with the environment (episodes). Right is the same reward depending on the execution time. The reward is averaged for seven seeds. The shaded area shows the difference of one standard deviation around the mean. 262.5 5.0 7.5 10.0 12.5 15.0 max ranks 0.0 0.2 0.4 0.6 0.8 1.0 ﬁnal rewards 103 InvertedPendulum-v2 max mean min 2.5 5.0 7.5 10.0 12.5 15.0 max ranks 0.15 0.20 0.25 0.30 0.35 ﬁnal rewards 103 Swimmer-v3 max mean min 2.5 5.0 7.5 10.0 12.5 15.0 max ranks 0.10 0.15 0.20 0.25 0.30 ﬁnal rewards 103 LunarLanderContinuous-v2 max mean min 2.5 5.0 7.5 10.0 12.5 15.0 max ranks 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 ﬁnal rewards 103 HalfCheetah-v3 max mean min Figure 8: The dependency of the ﬁnal cumulative reward on rank. The mean, the minimum, and the maximum over seven random seeds are presented. The mode size is N = 3. 27",
      "meta_data": {
        "arxiv_id": "2205.00293v2",
        "authors": [
          "Konstantin Sozykin",
          "Andrei Chertkov",
          "Roman Schutski",
          "Anh-Huy Phan",
          "Andrzej Cichocki",
          "Ivan Oseledets"
        ],
        "published_date": "2022-04-30T15:41:13Z",
        "pdf_url": "https://arxiv.org/pdf/2205.00293v2.pdf",
        "github_url": "https://github.com/AndreiChertkov/ttopt"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes TTOpt, a novel and efficient gradient-free optimization algorithm for multivariable functions. It leverages low-rank Tensor Train (TT) representation and a generalized maximum matrix volume principle. The key contributions include demonstrating TTOpt's competitive performance against popular gradient-free methods on analytical benchmark functions and its application to Reinforcement Learning (RL) problems. Notably, the research empirically shows that RL agents with discrete (quantized) weights can perform effectively in continuous control tasks, enabling the direct training of quantized neural networks suitable for low-power devices. TTOpt consistently outperforms baselines in terms of function evaluations or execution time.",
        "methodology": "TTOpt discretizes continuous optimization problems by introducing a grid for each parameter, effectively representing the objective function as an implicit d-dimensional tensor. This tensor is then approximated using the Tensor Train (TT) format to overcome the curse of dimensionality. The optimization process is reformulated as finding the maximal elements of this tensor, guided by a generalized maximum volume principle. This involves iteratively searching for maximal volume submatrices in column and row spaces during 'sweeps' (forward and backward) across tensor modes. For stability, QR decomposition is applied before the maxvol algorithm, and `rect_maxvol` is used for adaptive rank selection. A continuous, smooth, and strictly monotone mapping function, `g(x) = pi/2 - atan(J(x) - Jmin)`, transforms objective values to enable finding global minimum/maximum by searching for maximum modulus. Crucially, the method employs quantization of tensor modes (reshaping the original tensor into a 'long' one with smaller mode sizes like P=2) to significantly boost accuracy and reduce complexity for fine grids.",
        "experimental_setup": "TTOpt was evaluated on two main categories: analytical benchmark functions and Reinforcement Learning (RL) tasks. For benchmark functions, it was tested on 10-dimensional functions (e.g., Ackley, Rastrigin, Griewank) with known global minima, and its scalability was assessed on functions up to 500 dimensions. RL experiments involved continuous control tasks in Mujoco and OpenAI-GYM, specifically Swimmer-v3, LunarLanderContinuous-v2, InvertedPendulum-v2, and HalfCheetah-v3, where the policy was a neural network with three hidden layers and tanh/ReLU activations. TTOpt's configurable parameters included grid bounds, rank (R=4 for benchmarks, 3-5 for RL), submode size (P=2), number of submodes (q such that N=P^q), and a limit on objective function calls (M=10^5 or 10^4*d). Baselines included various gradient-free methods (GA, openES, cmaES, DE, NB, PSO), gradient-based optimizers (BFGS, L-BFGS, CG, Newton, TR NCG, TR), and Bayesian optimization algorithms (SOO, dSOO, LOGO, RANDOM). Experiments were averaged over 10 independent runs for benchmarks and 7 random seeds for RL, with computations performed on a standard laptop.",
        "limitations": "The current TTOpt algorithm lacks formal theoretical guarantees for convergence to the global minimum and its rate in the multidimensional tensor case, as there is no direct analog to the matrix maximum volume bound (Eq. 3). Although results monotonically improve with iterations, a strong theoretical foundation for tensor convergence is not yet established. For some high-dimensional benchmark functions (F4, F7, F9), the heuristic used for limiting the number of objective function evaluations was found to be less accurate, leading to larger errors. Additionally, the computational cost of the `maxvol` algorithm, with a complexity of O(T * d * max(Nk * Rk^3)), can become a bottleneck if individual objective function calls are computationally inexpensive. Finally, the choice of rank can be sensitive, with both excessively small or large ranks potentially reducing accuracy due to numerical instabilities or insufficient convergence sweeps.",
        "future_research_directions": "Future research should focus on developing formal theoretical guarantees for TTOpt's convergence to the global minimum in the multidimensional tensor context, potentially by establishing an analog to the maximum matrix volume principle. Further investigation is needed to refine parameter selection heuristics, such as the number of objective function calls and optimal rank, to ensure consistent accuracy across diverse problems and dimensions. Exploring extensions of the TTOpt methodology to incorporate other low-rank tensor decompositions (e.g., Tensor Chain/Tensor Ring, Hierarchical Tucker) could broaden its applicability. Additionally, exploring novel continuous, smooth, and strictly monotone mapping functions for objective transformation may enhance performance and stability. The paper also implicitly suggests that TTOpt could serve as a bridge between continuous and discrete optimization methods, indicating a direction for integrating this approach into hybrid optimization frameworks or exploring its applicability to other domains requiring gradient-free or high-dimensional discrete optimization.",
        "experimental_code": "class TTOpt():    def __init__(self, f, d, a=None, b=None, n=None, p=None, q=None,                 evals=None, name=None, callback=None, x_opt_real=None,                 y_opt_real=None, is_func=True, is_vect=True, with_cache=False,                 with_log=False, with_opt=False, with_full_info=False,                 with_wrn=True):        self.f = f        self.d = int(d)        if isinstance(a, (int, float)):            self.a = np.ones(self.d, dtype=float) * a        elif a is not None:            self.a = np.asanyarray(a, dtype=float)        else:            if is_func:                raise ValueError('Grid lower bound (a) should be set')            self.a = None        if self.a is not None and self.a.size != self.d:            raise ValueError('Grid lower bound (a) has invalid shape')        if isinstance(b, (int, float)):            self.b = np.ones(self.d, dtype=float) * b        elif b is not None:            self.b = np.asanyarray(b, dtype=float)        else:            if is_func:                raise ValueError('Grid upper bound (b) should be set')            self.b = None        if self.b is not None and self.b.size != self.d:            raise ValueError('Grid upper bound (b) has invalid shape')        if n is None:            if p is None or q is None:                raise ValueError('If n is not set, then p and q should be set')            self.p = int(p)            self.q = int(q)            self.n = np.ones(self.d * self.q, dtype=int) * self.p            self.n_func = np.ones(self.d, dtype=int) * (self.p**self.q)        else:            if p is not None or q is not None:                raise ValueError('If n is set, then p and q should be None')            self.p = None            self.q = None            if isinstance(n, (int, float)):                self.n = np.ones(self.d, dtype=int) * int(n)            else:                self.n = np.asanyarray(n, dtype=int)            self.n_func = self.n.copy()        if self.n_func.size != self.d:            raise ValueError('Grid size (n/p/q) has invalid shape')        self.evals = int(evals) if evals else None        self.name = name or ''        self.callback = callback        self.x_opt_real = x_opt_real        self.y_opt_real = y_opt_real        self.is_func = bool(is_func)        self.is_vect = bool(is_vect)        self.with_cache = bool(with_cache)        self.with_log = bool(with_log)        self.with_opt = bool(with_opt)        self.with_full_info = bool(with_full_info)        self.with_wrn = bool(with_wrn)        self.cache = {}        self.cache_opt = {}        self.k_cache = 0        self.k_cache_curr = 0        self.k_evals = 0        self.k_evals_curr = 0        self.t_evals = 0.        self.t_total = 0.        self.t_minim = 0        self._opt = None        self.i_opt = None        self.x_opt = None        self.I_list = []        self.i_opt_list = []        self.x_opt_list = []        self.y_opt_list = []        self.opt_opt_list = []        self.evals_opt_list = []        self.cache_opt_list = []    def qtt_parse_many(self, I_qtt):        samples = I_qtt.shape[0]        n_qtt = [self.n[0]]*self.q        I = np.zeros((samples, self.d))        for i in range(self.d):            J_curr = I_qtt[:, self.q*i:self.q*(i+1)].T            I[:, i] = np.ravel_multi_index(J_curr, n_qtt, order='F')        return I    def i2x(self, i):        t = i * 1. / (self.n_func - 1)        x = t * (self.b - self.a) + self.a        return x    def i2x_many(self, I):        A = np.repeat(self.a.reshape((1, -1)), I.shape[0], axis=0)        B = np.repeat(self.b.reshape((1, -1)), I.shape[0], axis=0)        N = np.repeat(self.n_func.reshape((1, -1)), I.shape[0], axis=0)        T = I * 1. / (N - 1)        X = T * (B - A) + A        return X    def optimize(self, rank=4, Y0=None, seed=42, fs_opt=1., is_max=False,                 add_opt_inner=True, add_opt_outer=False, add_opt_rect=False,                 add_rnd_inner=False, add_rnd_outer=False, J0=None):        t_minim = tpc()        self.is_max = is_max        i_opt, y_opt = ttopt(self.comp_opt, self.n, rank, None, Y0, seed,                fs_opt, add_opt_inner, add_opt_outer, add_opt_rect,                add_rnd_inner, add_rnd_outer, J0, is_max)        self.t_minim = tpc() - t_minim    def comp_opt(self, I, i_opt=None, y_opt=None, opt_opt=None):        if self.evals is not None and self.k_evals >= self.evals:            return None, None        if self.with_cache:            if self.k_cache >= self.evals and self.k_cache >= 2 * self.k_evals:                text = '!!! TTOpt warning : '                text += 'the number of requests to the cache is 2 times higher '                text += 'than the number of requests to the function. '                text += 'The work is finished before max func-evals reached.'                if self.with_wrn:                    print(text)                return None, None        eval_curr = I.shape[0]        is_last = self.evals is not None and self.k_evals+eval_curr>=self.evals        if is_last:            I = I[:(self.evals-self.k_evals), :]        if self.q:            if I is not None:                I = self.qtt_parse_many(I)            if i_opt is not None:                i_opt = self.qtt_parse_many(i_opt.reshape(1, -1))[0, :]        Y = self.comp(I)        if is_last:            i_opt, y_opt, opt_opt = ttopt_find(                I, Y, self._opt, i_opt, y_opt, opt_opt, self.is_max)        if i_opt is None:            return Y, self._opt        if self.is_func:            x_opt = self.i2x(i_opt)        else:            x_opt = i_opt.copy()        self.i_opt = i_opt.copy()        self.x_opt = x_opt.copy()        self.y_opt_list.append(y_opt)        self.opt_opt_list.append(opt_opt)        self.evals_opt_list.append(self.k_evals_curr)        self.cache_opt_list.append(self.k_cache_curr)        if self.with_full_info:            self.I_list.append(I)            self.i_opt_list.append(self.i_opt.copy())            self.x_opt_list.append(self.x_opt.copy())        if self.is_max:            is_better = len(self.y_opt_list)==1 or (y_opt > self.y_opt_list[-2])        else:            is_better = len(self.y_opt_list)==1 or (y_opt < self.y_opt_list[-2])        if self.callback and is_better:            last = {'last': [x_opt, y_opt, i_opt, opt_opt, self.k_evals]}            self.callback(last)        if self.with_log:            print(self.info(is_final=False))        return Y, self._optdef ttopt(f, n, rank=4, evals=None, Y0=None, seed=42, fs_opt=1.,          add_opt_inner=True, add_opt_outer=False, add_opt_rect=False,          add_rnd_inner=False, add_rnd_outer=False, J0=None, is_max=False):    d = len(n)    evals = int(evals) if evals else None    Jg_list = [np.reshape(np.arange(k), (-1, 1)) for k in n]    if J0 is None:        Y0, r = ttopt_init(n, rank, Y0, seed, with_rank=True)        J_list = [None] * (d + 1)        for i in range(d - 1):            J_list[i+1] = _iter(Y0[i], J_list[i], Jg_list[i], l2r=True)    else:        J_list = J0        r = [1] + [J.shape[0] for J in J_list[1:-1]] + [1]        for i in range(1, d):            r[i] = min(rank, n[i-1] * r[i-1])    i_opt = None    y_opt = None    opt_opt = None    eval = 0    iter = 0    i = d - 1    l2r = False    while True:        I = _merge(J_list[i], J_list[i+1], Jg_list[i])        eval_curr = I.shape[0]        if evals is not None and eval + eval_curr > evals:            I = I[:(evals-eval), :]        y, opt = f(I, i_opt, y_opt, opt_opt)        if y is None:            return i_opt, y_opt        i_opt, y_opt, opt_opt = ttopt_find(I, y, opt, i_opt, y_opt, opt_opt,            is_max)        eval += y.size        if evals is not None and eval >= evals:            return i_opt, y_opt        if y.shape[0] < I.shape[0]:            return i_opt, y_opt        Z = _reshape(y, (r[i], n[i], r[i + 1]))        if not is_max:            Z = ttopt_fs(Z, y_opt, fs_opt)        if l2r and i < d - 1:            J_list[i+1] = _iter(Z, J_list[i], Jg_list[i], l2r,                add_opt_inner, add_opt_rect, add_rnd_inner)            if add_opt_outer:                J_list[i+1] = _add_row(J_list[i+1], i_opt[:(i+1)])            if add_rnd_outer:                J_list[i+1] = _add_random(J_list[i+1], n[:(i+1)])            r[i+1] = J_list[i+1].shape[0]        if not l2r and i > 0:            J_list[i] = _iter(Z, J_list[i+1], Jg_list[i], l2r,                add_opt_inner, add_opt_rect, add_rnd_inner)            if add_opt_outer:                J_list[i] = _add_row(J_list[i], i_opt[i:])            if add_rnd_outer:                J_list[i] = _add_random(J_list[i], n[i:])            r[i] = J_list[i].shape[0]        i, iter, l2r = _update_iter(d, i, iter, l2r)    return i_opt, y_optdef ttopt_find(I, y, opt, i_opt, y_opt, opt_opt, is_max=False):    if is_max:        ind = np.argmax(y)    else:        ind = np.argmin(y)    y_opt_curr = y[ind]    if is_max and y_opt is not None and y_opt_curr <= y_opt:        return i_opt, y_opt, opt_opt    if not is_max and y_opt is not None and y_opt_curr >= y_opt:        return i_opt, y_opt, opt_opt    return I[ind, :], y_opt_curr, opt[ind]def ttopt_fs(y, y0=0., opt=1.):    if opt is None or opt == 0:        return np.pi/2 - np.arctan(y - y0)    else:        return np.exp(opt * (y0 - y))def _iter(Z, J, Jg, l2r=True, add_opt_inner=True, add_opt_rect=False,          add_rnd_inner=False):    r1, n, r2 = Z.shape    Z = _reshape(Z, (r1 * n, r2)) if l2r else _reshape(Z, (r1, n * r2)).T    Q, R = np.linalg.qr(Z)    ind = _maxvol(Q, is_rect=add_opt_rect)    if add_opt_inner:        i_max, j_max = np.divmod(np.abs(Z).argmax(), Z.shape[1])        if not i_max in ind:            ind[-1] = i_max    if add_rnd_inner and len(ind) > 1:        i_rnd = np.random.choice(Z.shape[0])        if not i_rnd in ind:            ind[-2] = i_rnd    J_new = _stack(J, Jg, l2r)    J_new = J_new[ind, :]    return J_newdef _maxvol(A, tol=1.001, max_iters=1000, is_rect=False):    n, r = A.shape    if n <= r:        return np.arange(n, dtype=int)    if is_rect:        return maxvol_rect(A, e=1., dr_min=1, dr_max=2)[0]    else:        return maxvol(A, e=tol, k=max_iters)[0]def maxvol(A, e=1.05, k=100):    n, r = A.shape    if n <= r:        raise ValueError('Input matrix should be \"tall\"')    P, L, U = lu(A, check_finite=False)    I = P[:, :r].argmax(axis=0)    Q = solve_triangular(U, A.T, trans=1, check_finite=False)    B = solve_triangular(L[:r, :], Q, trans=1, check_finite=False,        unit_diagonal=True, lower=True).T    for _ in range(k):        i, j = np.divmod(np.abs(B).argmax(), r)        if np.abs(B[i, j]) <= e:            break        I[j] = i        bj = B[:, j]        bi = B[i, :].copy()        bi[j] -= 1.        B -= np.outer(bj, bi / B[i, j])    return I, Bdef maxvol_rect(A, e=1.1, dr_min=0, dr_max=None, e0=1.05, k0=10):    n, r = A.shape    r_min = r + dr_min    r_max = r + dr_max if dr_max is not None else n    r_max = min(r_max, n)    if r_min < r or r_min > r_max or r_max > n:        raise ValueError('Invalid minimum/maximum number of added rows')    I0, B = maxvol(A, e0, k0)    I = np.hstack([I0, np.zeros(r_max-r, dtype=I0.dtype)])    S = np.ones(n, dtype=int)    S[I0] = 0    F = S * np.linalg.norm(B, axis=1)**2    for k in range(r, r_max):        i = np.argmax(F)        if k >= r_min and F[i] <= e*e:            break        I[k] = i        S[i] = 0        v = B.dot(B[i])        l = 1. / (1 + v[i])        B = np.hstack([B - l * np.outer(v, B[i]), l * v.reshape(-1, 1)])        F = S * (F - l * v * v)    I = I[:B.shape[1]]    B[I] = np.eye(B.shape[1], dtype=B.dtype)    return I, B",
        "experimental_info": "The TTOpt method's performance is investigated across various analytical benchmark functions and settings. The experiments cover the following aspects:  **Benchmark Functions:** - Ackley, Alpine, Brown, Exponential, Grienwank, Michalewicz, Qing, Rastrigin, Schaffer, Schwefel, Rosenbrock.  **Dimensionality (d):** - Evaluated for `d` values: 2, 10, 50, 100, 500.  **Grid Discretization:** - Direct TT-grid sizes (`n`): Tested with `n` ranging from `2^8` to `2^20`. - Quantized Tensor Train (QTT) grid settings: `p=2` (base 2) with `q` from 12 to 25; `p=4` (base 4) for `q` calculated as `np.log4(n)`.  **Maximum TT-Rank (r):** - Investigated ranks `r` from 1 to 10.  **Computational Budget (Function Evaluations):** - Total function calls (`evals` or `m`) ranging from `1.E+4` to `1.E+7`.  **Repetitions:** - Experiments are typically run for 1 to 10 repetitions (`reps`) to account for randomness.  **Optimization Strategies & Maxvol Enhancements:** - Smoothing function parameter (`fs_opt`): Tested with values `None`, `1000.`, `100.`, `10.`, `1.`, `0.1`, `0.01`. `None` uses `arctan`, other values use `exp` based smoothing. - Maxvol algorithm options (flags for adding points):    - `add_opt_inner`: Add optimal inner point.    - `add_opt_outer`: Add optimal outer point.    - `add_opt_rect`: Use `rect_maxvol` for adaptive rank selection.    - `add_rnd_inner`: Add random inner point.    - `add_rnd_outer`: Add random outer point.  **Comparisons with Other Optimizers:** - The method is compared against several non-TT optimizers including: GA (Genetic Algorithm), OpenES (OpenAI Evolution Strategies), CMAES (Covariance Matrix Adaptation Evolution Strategy), DE (Differential Evolution), NB (NoisyBandit), PSO (Particle Swarm Optimization), and gradient-based methods like BFGS, L-BFGS, CG, Newton-CG, Newton-Exact, Trust-Region NCG, Trust-Region Exact.  **Cache Usage:** - Experiments conducted both with and without caching (`with_cache=True/False`) to evaluate its impact.  **Initialization:** - Random TT-tensor initialization (default). - Specific initial multi-indices (`J0`) for tensor problems. - Random uniform `x0` (using `np.random.default_rng(12345)` or `torch.manual_seed(rep)`) for other optimizers.  **Objective:** - Both minimization (default) and maximization (`is_max=True`) tasks are performed.  **Logging and Visualization:** - Animation files (.gif) are generated to visualize the optimization process for 2D functions. - Detailed logs and plots (e.g., error vs. queries, error vs. rank, error vs. grid size) are generated to show results."
      }
    },
    {
      "title": "On Estimation in Latent Variable Models"
    },
    {
      "title": "Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time",
      "abstract": "Hamiltonian Monte Carlo (HMC) is a popular method in sampling. While there\nare quite a few works of studying this method on various aspects, an\ninteresting question is how to choose its integration time to achieve\nacceleration. In this work, we consider accelerating the process of sampling\nfrom a distribution $\\pi(x) \\propto \\exp(-f(x))$ via HMC via time-varying\nintegration time. When the potential $f$ is $L$-smooth and $m$-strongly convex,\ni.e.\\ for sampling from a log-smooth and strongly log-concave target\ndistribution $\\pi$, it is known that under a constant integration time, the\nnumber of iterations that ideal HMC takes to get an $\\epsilon$ Wasserstein-2\ndistance to the target $\\pi$ is $O( \\kappa \\log \\frac{1}{\\epsilon} )$, where\n$\\kappa := \\frac{L}{m}$ is the condition number. We propose a scheme of\ntime-varying integration time based on the roots of Chebyshev polynomials. We\nshow that in the case of quadratic potential $f$, i.e., when the target $\\pi$\nis a Gaussian distribution, ideal HMC with this choice of integration time only\ntakes $O( \\sqrt{\\kappa} \\log \\frac{1}{\\epsilon} )$ number of iterations to\nreach Wasserstein-2 distance less than $\\epsilon$; this improvement on the\ndependence on condition number is akin to acceleration in optimization. The\ndesign and analysis of HMC with the proposed integration time is built on the\ntools of Chebyshev polynomials. Experiments find the advantage of adopting our\nscheme of time-varying integration time even for sampling from distributions\nwith smooth strongly convex potentials that are not quadratic.",
      "full_text": "Published as a conference paper at ICLR 2023 ACCELERATING HAMILTONIAN MONTE CARLO VIA CHEBYSHEV INTEGRATION TIME Jun-Kun Wang and Andre Wibisono Department of Computer Science, Yale University {jun-kun.wang,andre.wibisono}@yale.edu ABSTRACT Hamiltonian Monte Carlo (HMC) is a popular method in sampling. While there are quite a few works of studying this method on various aspects, an interesting question is how to choose its integration time to achieve acceleration. In this work, we consider accelerating the process of sampling from a distribution π(x) ∝ exp(−f(x)) via HMC via time-varying integration time. When the potential f is L-smooth and m-strongly convex, i.e. for sampling from a log-smooth and strongly log-concave target distribution π, it is known that under a constant integration time, the number of iterations that ideal HMC takes to get an ϵ Wasserstein-2 distance to the target πis O(κlog 1 ϵ), where κ:= L m is the condition number. We propose a scheme of time-varying integration time based on the roots of Chebyshev polynomials. We show that in the case of quadratic potential f, i.e. when the target πis a Gaussian distribution, ideal HMC with this choice of integration time only takes O(√κlog 1 ϵ) number of iterations to reach Wasserstein-2 distance less thanϵ; this improvement on the dependence on condition number is akin to acceleration in optimization. The design and analysis of HMC with the proposed integration time is built on the tools of Chebyshev polynomials. Experiments ﬁnd the advantage of adopting our scheme of time-varying integration time even for sampling from distributions with smooth strongly convex potentials that are not quadratic. 1 I NTRODUCTION Markov chain Monte Carlo (MCMC) algorithms are fundamental techniques for sampling from probability distributions, which is a task that naturally arises in statistics (Duane et al., 1987; Girolami & Calderhead, 2011), optimization (Flaxman et al., 2005; Duchi et al., 2012; Jin et al., 2017), machine learning and others (Wenzel et al., 2020; Salakhutdinov & Mnih, 2008; Koller & Friedman, 2009; Welling & Teh, 2011). Among all the MCMC algorithms, the most popular ones perhaps are Langevin methods (Li et al., 2022; Dalalyan, 2017; Durmus et al., 2019; Vempala & Wibisono, 2019; Lee et al., 2021b; Chewi et al., 2020) and Hamiltonian Monte Carlo (HMC) (Neal, 2012; Betancourt, 2017; Hoffman & Gelman, 2014; Levy et al., 2018). For the former, recently there have been a sequence of works leveraging some techniques in optimization to design Langevin methods, which include borrowing the idea of momentum methods like Nesterov acceleration (Nesterov, 2013) to design fast methods, e.g., (Ma et al., 2021; Dalalyan & Riou-Durand, 2020). Speciﬁcally, Ma et al. (2021) show that for sampling from distributions satisfying the log-Sobolev inequality, under-damped Langevin improves the iteration complexity of over-damped Langevin from O(d ϵ) to O( √ d ϵ), where dis the dimension and ϵis the error in KL divergence, though whether their result has an optimal dependency on the condition number is not clear. On the other hand, compared to Langevin methods, the connection between HMCs and techniques in optimization seems rather loose. Moreover, to our knowledge, little is known about how to accelerate HMCs with a provable acceleration guarantee for converging to a target distribution. Speciﬁcally, Chen & Vempala (2019) show that for sampling from strongly log-concave distributions, the iteration complexity of ideal HMC is O(κlog 1 ϵ), and Vishnoi (2021) shows the same rate of ideal HMC when the potential is strongly convex quadratic in a nice tutorial. In contrast, there are a few methods that exhibit acceleration when minimizing strongly convex quadratic functions in optimization. For example, while Heavy Ball (Polyak, 1964) does not have an accelerated linear rate globally for minimizing general smooth strongly convex functions, it does show acceleration when minimizing strongly convex quadratic functions (Wang et al., 2020; 1 arXiv:2207.02189v2  [cs.LG]  14 Feb 2023Published as a conference paper at ICLR 2023 Algorithm 1:IDEAL HMC 1: Require: an initial point x0 ∈Rd, number of iterations K, and a scheme of integration time {η(K) k }. 2: for k= 1 to Kdo 3: Sample velocity ξ∼N(0,Id). 4: Set (xk,vk) = HMCη(K) k (xk−1,ξ). 5: end for 2021; 2022). This observation makes us wonder whether one can get an accelerated linear rate of ideal HMC for sampling, i.e., O(√κlog 1 ϵ), akin to acceleration in optimization. We answer this question afﬁrmatively, at least in the Gaussian case. We propose a time-varying integration time for HMC, and we show that ideal HMC with this time-varying integration time exhibits acceleration when the potential is a strongly convex quadratic (i.e. the targetπis a Gaussian), compared to what is established in Chen & Vempala (2019) and Vishnoi (2021) for using a constant integration time. Our proposed time-varying integration time at each iteration of HMC depends on the total number of iterations K, the current iteration index k, the strong convexity constant m, and the smoothness constant Lof the potential; therefore, the integration time at each iteration is simple to compute and is set before executing HMC. Our proposed integration time is based on the roots of Chebysev polynomials, which we will describe in details in the next section. In optimization, Chebyshev polynomials have been used to help design accelerated algorithms for minimizing strongly convex quadratic functions, i.e., Chebyshev iteration (see e.g., Section 2.3 in d’Aspremont et al. (2021)). Our result of accelerating HMC via using the proposed Chebyshev integration time can be viewed as the sampling counterpart of acceleration from optimization. Interestingly, for minimizing strongly convex quadratic functions, acceleration of vanilla gradient descent can be achieved via a scheme of step sizes that is based on a Chebyshev polynomial, see e.g., Agarwal et al. (2021), and our work is inspired by a nice blog article by Pedregosa (2021). Hence, our acceleration result of HMC can also be viewed as a counterpart in this sense. In addition to our theoretical ﬁndings, we conduct experiments of sampling from a Gaussian as well as sampling from distributions whose potentials are not quadratics, which include sampling from a mixture of two Gaussians, Bayesian logistic regression, and sampling from a hard distribution that was proposed in Lee et al. (2021a) for establishing some lower-bound results of certain Metropolized sampling methods. Experimental results show that our proposed time-varying integration time also leads to a better performance compared to using the constant integration time of Chen & Vempala (2019) and Vishnoi (2021) for sampling from the distributions whose potential functions are not quadratic. We conjecture that our proposed time-varying integration time also helps accelerate HMC for sampling from log-smooth and strongly log-concave distributions, and we leave the analysis of such cases for future work. 2 P RELIMINARIES 2.1 H AMILTONIAN MONTE CARLO (HMC) Suppose we want to sample from a target probability distribution ν(x) ∝exp(−f(x)) on Rd, where f: Rd →R is a continuous function which we refer to as the potential. Denote x ∈Rd the position and v ∈Rd the velocity of a particle. In this paper, we consider the standard Hamiltonian of the particle (Chen & Vempala, 2019; Neal, 2012), which is deﬁned as H(x,v) := f(x) + 1 2 ∥v∥2, (1) while we refer the readers to Girolami & Calderhead (2011); Hirt et al. (2021); Brofos & Lederman (2021) and the references therein for other notions of the Hamiltonian. The Hamiltonian ﬂow generated by H is the ﬂow of the particle which evolves according to the following differential equations: dx dt = ∂H ∂v and dv dt = −∂H ∂x . For the standard Hamiltonian deﬁned in (1), the Hamiltonian ﬂow becomes dx dt = v and dv dt = −∇f(x). (2) 2Published as a conference paper at ICLR 2023 We will write (xt,vt) = HMCt(x0,v0) as the position xand the velocity vof the Hamiltonian ﬂow after integration time tstarting from (x0,v0). There are many important properties of the Hamiltonian ﬂow including that the Hamiltonian is conserved along the ﬂow, the vector ﬁeld associated with the ﬂow is divergence free, and the Hamiltonian dynamic is time reversible, see e.g., Section 3 in Vishnoi (2021). The Ideal HMCalgorithm (see Algorithm 1) proceeds as follows: in each iteration k, sample an initial velocity from the normal distribution, and then ﬂow following the Hamiltonian ﬂow with a pre-speciﬁed integration time ηk. It is well-known that ideal HMC preserves the target density π(x) ∝exp(−f(x)); see e.g., Theorem 5.1 in Vishnoi (2021). Furthermore, in each iteration, HMC brings the density of the iterates xk ∼ρk closer to the target π. However, the Hamiltonian ﬂow HMCt(x0,v0) is in general difﬁcult to simulate exactly, except for some special potentials. In practice, the Verlet integrator is commonly used to approximate the ﬂow and a Metropolis-Hastings ﬁlter is applied to correct the induced bias arises from the use of the integrator (Tripuraneni et al., 2017; Brofos & Lederman, 2021; Hoffman et al., 2021; Lee et al., 2021a; Chen et al., 2020). In recent years, there have been some progress on showing some rigorous theoretical guarantees of HMCs for converging to a target distribution, e.g., Chen et al. (2020); Durmus et al. (2017); Bou-Rabee & Eberle (2021); Mangoubi & Smith (2019; 2021); Mangoubi & Vishnoi (2018). There are also other variants of HMCs proposed in the literature, e.g., Riou-Durand & V ogrinc (2022); Bou-Rabee & Sanz-Serna (2017); Zou & Gu (2021); Steeg & Galstyan (2021); Hoffman & Gelman (2014); Tripuraneni et al. (2017); Chen et al. (2014), to name just a few. Recall that the 2-Wasserstein distance between probability distributions ν1 and ν2 is W2(ν1,ν2) := inf x,y∈Γ(ν1,ν2) E [ ∥x−y∥2]1/2 where Γ(ν1,ν2) represents the set of all couplings of ν1 and ν2. 2.2 A NALYSIS OF HMC IN QUADRATIC CASE WITH CONSTANT INTEGRATION TIME In the following, we replicate the analysis of ideal HMC with a constant integration time for quadratic potentials (Vishnoi, 2021), which provides the necessary ingredients for introducing our method in the next section. Speciﬁcally, we consider the following quadratic potential: f(x) := ∑d j=1 λjx2 j, where 0 <m ≤λj ≤L, (3) which means the target density is the Gaussian distribution π= N(0,Λ−1), where Λ the diagonal matrix whose jth diagonal entry is λj. We note for a general Gaussian target N(µ,Σ) for some µ∈Rd and Σ ≻0, we can shift and rotate the coordinates to make µ= 0 and Σ a diagonal matrix, and our analysis below applies. So without loss of generality, we may assume the quadratic potential is separable, as in (3). In this quadratic case, the Hamiltonian ﬂow (2) becomes a linear system of differential equations, and we have an exact solution given by sinusoidal functions, which are xt[j] = cos (√ 2λjt ) x0[j] + 1√ 2λj sin (√ 2λjt ) v0[j], vt[j] = − √ 2λjsin (√ 2λjt ) x0[j] + cos (√ 2λjt ) v0[j]. (4) In particular, we recall the following result on the deviation between two co-evolving particles with the same initial velocity. Lemma 1. (Vishnoi, 2021) Let x0,y0 ∈ Rd. Consider the following coupling: (xt,vt) = HMCt(x0,ξ) and (yt,ut) = HMC t(y0,ξ) for some ξ ∈ Rd. Then for all t ≥ 0 and for all j ∈[d], it holds that xt[j] −yt[j] = cos (√ 2λjt ) ×(x0[j] −y0[j]). Using Lemma 1, we can derive the convergence rate of ideal HMC for the quadratic potential as follows. 3Published as a conference paper at ICLR 2023 Lemma 2. (Vishnoi, 2021) Let π∝exp(−f) = N(0,Λ−1) be the target distribution, where f(x) is deﬁned on (3). Let ρK be the distribution of xK generated by Algorithm 1 at the ﬁnal iteration K. Then for any ρ0 and any K ≥1, we have W2(ρK,π) ≤maxj∈[d] ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐W2(ρ0,π). We replicate the proof of Lemma 1 and Lemma 2 in Appendix B for the reader’s convenience. Vishnoi (2021) shows that by choosing (Constant integration time) η(K) k = π 2 1√ 2L , (5) one has that cos (√ 2λjη(K) k ) ≤1 −Θ (m L ) for all the iterations k∈[K] and dimensions j ∈[d]. Hence, by Lemma 2, the distance satisﬁes W2(ρK,π) = O (( 1 −Θ (m L ))K) W2(ρ0,π) after Kiterations of ideal HMC with the constant integration time. On the other hand, for general smooth strongly convex potentials f(·), Chen & Vempala (2019) show the same convergence rate 1 −Θ (m L ) of HMC using a constant integration time η(K) k = c√ L, where c >0 is a universal constant. Therefore, under the constant integration time, HMC needs O(κlog 1 ϵ) iterations to reach error W2(ρK,π) ≤ϵ, where κ = L m is condition number. Furthermore, they also show that the relaxation time of ideal HMC with a constant integration time is Ω(κ) for the Gaussian case. 2.3 C HEBYSHEV POLYNOMIALS We denote ΦK(·) the degree-KChebyshev polynomial of the ﬁrst kind, which is deﬁned by: ΦK(x) =    cos(Karccos(x)) if x∈[−1,1], cosh(Karccosh(x)) if x> 1, (−1)Kcosh(Karccosh(x)) if x< 1. (6) Our proposed integration time is built on a scaled-and-shifted Chebyshev polynomial, deﬁned as: ¯ΦK(λ) := ΦK(h(λ)) ΦK(h(0)) , (7) where h(·) is the mapping h(λ) := L+m−2λ L−m . Observe that the mapping h(·) maps all λ∈[m,L] into the interval [−1,1]. The roots of the degree-Kscaled-and-shifted Chebyshev polynomial ¯ΦK(λ) are (Chebyshev roots) r(K) k := L+ m 2 −L−m 2 cos ((k−1 2 )π K ) , (8) where k = 1,2,...,K , i.e., ¯ΦK(r(K) k ) = 0. We now recall the following key result regarding the scaled-and-shifted Chebyshev polynomial ¯ΦK. Lemma 3. (e.g., Section 2.3 in d’Aspremont et al. (2021)) For any positive integerK, we have maxλ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐≤2 ( 1 −2 √m√ L+√m )K = O (( 1 −Θ (√m L ))K) . (9) The proof of Lemma 3 is in Appendix B. 3 C HEBYSHEV INTEGRATION TIME We are now ready to introduce our scheme of time-varying integration time. LetKbe the pre-speciﬁed total number of iterations of HMC. Our proposed method will ﬁrst permute the array [1,2,...,K ] before executing HMC for Kiterations. Denote σ(k) the kth element of the array [1,2,...,K ] after an arbitrary permutation σ. Then, we propose to set the integration time of HMC at iteration k, i.e., set η(K) k , as follows: 4Published as a conference paper at ICLR 2023 Figure 1: Left: Set K = 400 , m = 1 and L = 100 . The green solid line (Cheby- shev integration time (10)) on the subﬁgure represents maxλ∈{m,m+0.1,...,L} ⏐⏐⏐Πk s=1cos (√ 2λη(K) s )⏐⏐⏐ =⏐⏐⏐⏐⏐Πk s=1cos ( π 2 √ λ r(K) σ(s) )⏐⏐⏐⏐⏐ v.s. k, while the blue dash line (Constant integration time (5)) represents maxλ∈{m,m+0.1,...,L} ⏐⏐⏐Πk s=1cos (√ 2λη(K) s )⏐⏐⏐= ⏐⏐⏐⏐Πk s=1cos ( π 2 √ λ L )⏐⏐⏐⏐v.s. k. Since the cosine product con- trols the convergence rate of the W2 distance by Lemma 2, this conﬁrms the acceleration via using the proposed scheme of Chebyshev integration over the constant integration time (Chen & Vempala, 2019; Vishnoi, 2021). Right: ψ(x) = cos(π 2 √x) 1−x v.s. x. (Chebyshev integration time) η(K) k = π 2 1√ 2r(K) σ(k) . (10) We note the usage of the permutation σis not needed in our analysis below; however, it seems to help improve performance in practice. Speciﬁcally, though the guarantees of HMC at the ﬁnal iteration K provided in Theorem 1 and Lemma 4 below is the same regardless of the permutation, the progress of HMC varies under different permutations of the integration time, which is why we recommend an arbitrary permutation of the integration time in practice. Our main result is the following improved convergence rate of HMC under the Chebyshev integration time, for quadratic potentials. Theorem 1. Denote the target distribution π∝exp(−f(x)) = N(0,Λ−1), where f(x) is deﬁned on (3), and denote the condition number κ := L m. Let ρK be the distribution of xK generated by Algorithm 1 at the ﬁnal iteration K. Then, we have W2(ρK,π) ≤2 ( 1 −2 √m√ L+ √m )K W2(ρ0,π) = O (( 1 −Θ ( 1√κ ))K) W2(ρ0,π). Consequently, the total number of iterations K such that the Wasserstein-2 distance satisﬁes W2(ρK,π) ≤ϵis O (√κlog 1 ϵ ) . Theorem 1 shows an accelerated linear rate 1 −Θ ( 1√κ ) using Chebyshev integration time, and hence improves the previous result of 1 −Θ (1 κ ) as discussed above. The proof of Theorem 1 relies on the following lemma, which upper-bounds the cosine products that appear in the bound of the W2 distance in Lemma 2 by the scaled-and-shifted Chebyshev polynomial ¯ΦK(λ) on (7). Lemma 4. Denote |PCos K (λ)|:= ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λ r(K) σ(k) )⏐⏐⏐⏐⏐. Suppose λ∈[m,L]. Then, we have for any positive integer K, |PCos K (λ)|≤ ⏐⏐¯ΦK(λ) ⏐⏐. (11) The proof of Lemma 4 is available in Appendix C. Figure 1 compares the cosine product maxλ∈[m,L] ⏐⏐⏐Πk s=1cos (√ 2λη(K) s )⏐⏐⏐in Lemma 2 of using the proposed integration time and that 5Published as a conference paper at ICLR 2023 Algorithm 2:HMC WITH CHEBYSHEV INTEGRATION TIME 1: Given: a potential f(·), where π(x) ∝exp(−f(x)) and f(·) is L-smooth and m-strongly convex. 2: Require: number of iterations Kand the step size of the leapfrog steps θ. 3: Deﬁne r(K) k := L+m 2 −L−m 2 cos ((k−1 2 )π K ) ,for k= 1,...,K. 4: Arbitrarily permute the array [1,2,...,K ]. Denote σ(k) the kth element of the array after permutation. 5: for k= 1,2,...,K do 6: Sample velocity ξk ∼N(0,Id). 7: Set integration time η(K) k ←π 2 1√ 2r(K) σ(k) . 8: Set the number of leapfrog steps Sk ←⌊ η(K) k θ ⌋. 9: (¯x0,¯v0) ←(xk−1,ξk) % Leapfrog steps 10: for s= 0,2,...,S k −1 do 11: ¯vs+ 1 2 = ¯vs −θ 2 ∇f(¯xs); ¯ xs+1 = ¯xs + θ¯vs+ 1 2 ; ¯ vs+1 = ¯vs+ 1 2 −θ 2 ∇f(¯xs+1); 12: end for % Metropolis ﬁlter 13: Compute the acceptance ratio αk = min ( 1, exp(−H(¯xSk,¯vSk)) exp(−H(¯x0,¯v0)) ) . 14: Draw ζ ∼Uniform[0,1]. 15: If ζ <αk then 16: xk ←¯xSk 17: Else 18: xk ←xk−1. 19: end for of using the constant integration time, which illustrates acceleration via the proposed Chebyshev integration time. We now provide the proof of Theorem 1. Proof. (of Theorem 1) From Lemma 2, we have W2(ρK,π) ≤maxj∈[d] ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐·W2(ρ0,π). (12) We can upper-bound the cosine product of any j ∈[d] as, ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐ (a) = ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λj r(K) σ(k) )⏐⏐⏐⏐⏐ (b) ≤ ⏐⏐¯ΦK(λj) ⏐⏐ (c) ≤2 ( 1 −2 √m√ L+√m )K , (13) where (a) is due to the use of Chebyshev integration time (10), (b) is by Lemma 4, and (c) is by Lemma 3. Combining (12) and (13) leads to the result. HMC with Chebyshev Integration Time for General DistributionsTo sample from general strongly log-concave distributions, we propose Algorithm 2, which adopts the Verlet integrator (a.k.a. the leapfrog integrator) to simulate the Hamiltonian ﬂow HMCη(·,ξ) and uses Metropolis ﬁlter to correct the bias. It is noted that the number of leapfrog steps Sk in each iteration kis equal to the integration time η(K) k divided by the step size θused in the leapfrog steps. More precisely, we have Sk = ⌊η(K) k θ ⌋in iteration kof HMC. 4 E XPERIMENTS We now evaluate HMC with the proposed Chebyshev integration time (Algorithm 2) and HMC with the constant integration time (Algorithm 2 with line 7 replaced by the constant integration time (5)) in several tasks. For all the tasks in the experiments, the total number of iterations of HMCs is set to be K = 10,000, and hence we collect K = 10,000 samples along the trajectory. For the step size θ in the leapfrog steps, we let θ ∈{0.001,0.005,0.01,0.05}. To evaluate the methods, we 6Published as a conference paper at ICLR 2023 Table 1: Ideal HMC with K = 10,000 iterations for sampling from a Gaussian N(µ,Σ), where µ= [ 0 0 ] and Σ = [ 1 0 0 100 ] . Here, Cheby. (W/) is ideal HMC with a arbitrary permutation of the Chebyshev integration time, while Cheby. (W/O) is ideal HMC without a permutation; and Const. refers to using the constant integration time (5). Method Mean ESS Min ESS Cheby. (W/) 10399.00811 ±347.25021 7172 .50338 ±257.21244 Cheby. (W/O) 10197.09964 ±276.94894 7043 .55293 ±284.78037 Const. 7692.00382 ±207.19628 5533 .26519 ±213.31943 compute effective sample size (ESS), which is a common performance metric of HMCs (Girolami & Calderhead, 2011; Brofos & Lederman, 2021; Hirt et al., 2021; Riou-Durand & V ogrinc, 2022; Hoffman et al., 2021; Hoffman & Gelman, 2014; Steeg & Galstyan, 2021), by using the toolkit ArViz (Kumar et al., 2019). The ESS of a sequence of N dependent samples is computed based on the autocorrelations within the sequence at different lags: ESS := N/(1 + 2∑ kγ(k)), where γ(k) is an estimate of the autocorrelation at lag k. We consider 4 metrics, which are (1) Mean ESS:the average of ESS of all variables. That is, ESS is computed for each variable/dimension, and Mean ESS is the average of them. (2) Min ESS:the lowest value of ESS among the ESSs of all variables; (3) Mean ESS/Sec.:Mean ESS normalized by the CPU time in seconds; (4) Min ESS/Sec.:Minimum ESS normalized by the CPU time in seconds. In the following tables, we denote “Cheby.” as our proposed method, and “Const.” as HMC with the the constant integration time (Vishnoi, 2021; Chen & Vempala, 2019). Each of the conﬁgurations is repeated 10 times, and we report the average and the standard deviation of the results. We also report the acceptance rate of the Metropolis ﬁlter (Acc. Prob) on the tables. Our implementation of the experiments is done by modifying a publicly available code of HMCs by Brofos & Lederman (2021). Code for our experiments can be found in the supplementary. 4.1 I DEAL HMC FLOW FOR SAMPLING FROM A GUSSIAN WITH A DIAGONAL COVARIANCE Before evaluating the empirical performance of Algorithm 2 in the following subsections, here we discuss and compare the use of a arbitrary permutation of the Chebyshev integration time and that without permutation (as well as that of using a constant integration time). We simulate ideal HMC for sampling from a Gaussian N(µ,Σ), where µ= [ 0 0 ] and Σ = [ 1 0 0 100 ] . It is noted that ideal HMC ﬂow for this case has a closed-form solution as (4) shows. The result are reported on Table 1. From the table, the use of a Chebyshev integration time allows to obtain a larger ESS than that from using a constant integration time, and a arbitrary permutation helps get a better result. An explanation is that the ESS is a quantity that is computed along the trajectory of a chain, and therefore a permutation of the integration time could make a difference. We remark that the observation here (a arbitrary permutation of time generates a larger ESS) does not contradict to Theorem 1, since Theorem 1 is about the guarantee in W2 distance at the last iteration K. 4.2 S AMPLING FROM A GAUSSIAN We sample N(µ,Σ), where µ= [ 0 1 ] and Σ = [ 1 0 .5 0.5 100 ] . Therefore, the strong convexity constant mis approximately 0.01 and the smoothness constant Lis approximately 1. Table 2 shows the results. HMC with Chebyshev integration time consistently outperforms that of using the constant integration time in terms of all the metrics: Mean ESS, Min ESS, Mean ESS/Sec, and Min ESS/Sec. We also plot two quantities throughout the iterations of HMCs on Figure 2. Speciﬁcally, Sub-ﬁgure (a) on Figure 2 plots the size of the difference between the targeted covarianceΣ and an estimated covariance ˆΣk at each iteration kof HMC, where ˆΣk is the sample covariance of 10,000 samples collected from a number of 10,000 HMC chains at their kth iteration. Sub-ﬁgure (b) plots a discrete TV distance that is computed as follows. We use a built-in function of Numpy to sample 10,000 samples from the target distribution, while we also have 10,000 samples collected from a number 7Published as a conference paper at ICLR 2023 Table 2: Sampling from a Gaussian distribution. We report 4 metrics regarding ESS (the higher the better), please see the main text for their deﬁnitions. Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 5187.28 ±261.13 307 .09 ±21.92 20 .28 ±1.74 1 .20 ±0.11 1 .00 ±0.00 0.001 Const. 1912.76 ±72.10 39 .87 ±13.77 15 .87 ±0.89 0 .33 ±0.11 1 .00 ±0.00 0.005 Cheby. 5146.71 ±257.65 304 .126 ±19.09 97 .84 ±9.23 5 .79 ±0.68 1 .00 ±0.00 0.005 Const. 1926.71 ±136.53 32 .83 ±9.57 80 .31 ±4.39 1 .37 ±0.39 1 .00 ±0.00 0.01 Cheby. 5127.90 ±211.46 279 .59 ±38.09 184 .26 ±20.99 10 .01 ±1.52 1 .00 ±0.00 0.01 Const. 1832.87 ±77.47 35 .71 ±11.74 147 .53 ±12.59 2 .85 ±0.95 1 .00 ±0.00 0.05 Cheby. 5133.67 ±195.07 316 .87 ±36.27 871 .72 ±88.73 53 .54 ±6.22 0 .99 ±0.00 0.05 Const. 1849.15 ±92.75 34 .98 ±14.70 615 .73 ±30.16 11 .70 ±5.07 0 .99 ±0.00 0.1 Cheby. 4948.46 ±144.03 281 .66 ±44.79 1492 .96 ±166.21 84 .39 ±13.04 0 .99 ±0.00 0.1 Const. 1852.79 ±132.95 38 .17 ±16.35 1035 .54 ±82.34 21 .44 ±9.51 0 .99 ±0.00 (a) ∥Σ −ˆΣk∥F v.s. iteration k  (b) discrete TV(ˆπ,ˆρk) v.s. iteration k Figure 2: Sampling from a Gaussian distribution. Both lines correspond to HMCs with the same step size h= 0.05 used in the leapfrog steps (but with different schemes of the integration time). Please see the main text for the precise deﬁnitions of the quantities and the details of how we compute them. of 10,000 HMC chains at each iteration k. Using these two sets of samples, we construct two histograms with 30 number of bins for each dimension, we denote them as ˆπand ˆρk. The discrete TV(ˆπ,ˆρk) at iteration k is 0.5 times the sum of the absolute value of the difference between the number of counts of all the pairs of the bins divided by 10,000, which serves as a surrogate of the Wasserstein-2 distance between the true target πand ρk from HMC, since computing or estimating the true Wasserstein distance is challenging. 4.3 S AMPLING FROM A MIXTURE OF TWO GAUSSIANS For a vector a ∈ Rd and a positive deﬁnite matrix Σ ∈ Rd×d, we consider sampling from a mixture of two Gaussians N(a,Σ) and N(−a,Σ) with equal weights. Denote b := Σ −1a and Λ := Σ −1. The potential is f(x) = 1 2 ∥x−a∥2 Λ −log(1 + exp(−2x⊤b)), and its gradient is ∇f(x) = Λ x−b+ 2b(1 + exp(−2x⊤b))−1.For each dimension i ∈[d], we set a[i] = √ i 2d and set the covariance Σ = diag1≤i≤d( i d). The potential is strongly convex if a⊤Σ−1a <1, see e.g., Riou-Durand & V ogrinc (2022). We setd= 10 in the experiment, and simply use the smallest and the largest eigenvalue of Λ to approximate the strong convexity constant mand the smoothness constant Lof the potential, which are ˆm= 1 and ˆL= 10 in this case. Table 3 shows that the proposed method generates a larger effective sample size than the baseline. 4.4 B AYESIAN LOGISTIC REGRESSION We also consider Bayesian logistic regression to evaluate the methods. Given an observation (zi,yi), where zi ∈Rd and yi ∈{0,1}, the likelihood function is modeled as p(yi|zi,w) = 1 1+exp(−yiz⊤ i w) . Moreover, the prior on the model parameter wis assumed to follow a Gaussian distribution, p(w) = N(0,α−1Id), where α >0 is a parameter. The goal is to sample w ∈Rd from the posterior, p(w|{zi,yi}n i=1) = p(w)Πn i=1p(yi|zi,w), where nis the number of data points in a dataset. The potential function f(w) can be written as f(w) = ∑n i=1 fi(w), where fi(w) = log ( 1 + exp(−yiw⊤zi) ) + α∥w∥2 2n . (14) 8Published as a conference paper at ICLR 2023 Table 3: Sampling from a mixture of two Gaussians Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 2439.86 ±71.83 815 .20 ±83.82 22 .68 ±0.93 7 .57 ±0.81 0 .89 ±0.00 0.001 Const. 845.44 ±31.42 261 .14 ±34.34 12 .90 ±0.52 3 .98 ±0.53 0 .91 ±0.00 0.005 Cheby. 2399.50 ±100.12 784 .06 ±82.07 105 .97 ±8.78 34 .58 ±4.12 0 .89 ±0.00 0.005 Const. 876.61 ±25.62 277 .72 ±30.62 63 .80 ±4.67 20 .22 ±2.62 0 .91 ±0.00 0.01 Cheby. 2341.35 ±89.99 794 .27 ±48.75 194 .81 ±23.51 66 .30 ±9.89 0 .88 ±0.00 0.01 Const. 860.61 ±20.39 235 .33 ±33.73 110 .62 ±14.09 30 .40 ±6.34 0 .91 ±0.00 0.05 Cheby. 2214.19 ±87.27 748 .66 ±46.18 761 .59 ±68.88 256 .51 ±13.76 0 .89 ±0.00 0.05 Const. 853.40 ±41.05 265 .70 ±37.41 376 .54 ±67.83 116 .45 ±22.23 0 .91 ±0.00 0.1 Cheby. 2064.42 ±67.44 657 .45 ±60.44 1162 .67 ±84.19 370 .07 ±41.11 0 .90 ±0.00 0.1 Const. 632.70 ±22.78 182 .88 ±37.10 450 .53 ±93.60 132 .58 ±43.91 0 .92 ±0.00 We set α= 1 in the experiments. We consider three datasets: Heart, Breast Cancer, and Diabetes binary classiﬁcation datasets, which are all publicly available online. To approximate the strong convexity constant mand the smoothness constant Lof the potential f(w), we compute the smallest eigenvalue and the largest eigenvalue of the Hessian∇2f(w) at the maximizer of the posterior, and we use them as estimates of mand Lrespectively. We apply Newton’s method to approximately ﬁnd the maximizer of the posterior. The experimental results are reported on Table 4 in Appendix E.1 due to the space limit, which show that our method consistently outperforms the baseline. 4.5 S AMPLING FROM A hard DISTRIBUTION We also consider sampling from a step-size-dependent distribution π(x) ∝exp(−fh(x)), where the potential fh(·) is κ-smooth and 1-strongly convex. The distribution is considered in Lee et al. (2021a) for showing a lower bound regarding certain Metropolized sampling methods using a constant integration time and a constant step size hof the leapfrog integrator. More concretely, the potential is fh(x) := ∑d i=1 f(h) i (xi), where f(h) i (xi) = { 1 2 x2 i, i = 1 κ 3 x2 i −κh 3 cos ( xi√ h ) , 2 ≤i≤d. (15) In the experiment, we set κ= 50 and d= 10. The results are reported on Table 5 in Appendix E.2. The scheme of the Chebyshev integration time is still better than the constant integration time for this task. 5 D ISCUSSION AND OUTLOOK The Chebyshev integration time shows promising empirical results for sampling from a various of strongly log-concave distributions. On the other hand, the theoretical guarantee of acceleration that we provide in this work is only for strongly convex quadratic potentials. Therefore, a direction left open by our work is establishing some provable acceleration guarantees for general strongly log-concave distributions. However, unlike quadratic potentials, the output (position, velocity) of a HMC ﬂow does not have a closed-form solution in general, which makes the analysis much more challenging. A starting point might be improving the analysis of Chen & Vempala (2019), where a contraction bound of two HMC chains under a small integration time η= O( 1√ L) is shown. Since the scheme of the Chebyshev integration time requires a large integration time η= Θ ( 1√m ) at some iterations of HMC, a natural question is whether a variant of the result of Chen & Vempala (2019) can be extended to a large integration time η= Θ ( 1√m ) . We state as an open question: can ideal HMC with a scheme of time-varying integration time achieve an accelerated rateO(√κlog 1 ϵ) for general smooth strongly log-concave distributions? The topic of accelerating HMC with provable guarantees is underexplored, and we hope our work can facilitate the progress in this ﬁeld. After the preprint of this work was available on arXiv, Jiang (2022) proposes a randomized integration time with partial velocity refreshment and provably shows that ideal HMC with the proposed machinery has the accelerated rate for sampling from a Gaussian distribution. Exploring any connections between the scheme of Jiang (2022) and ours can be an interesting direction. 9Published as a conference paper at ICLR 2023 ACKNOWLEDGMENTS We thank the reviewers for constructive feedback, which helps improve the presentation of this paper. REFERENCES Naman Agarwal, Surbhi Goel, and Cyril Zhang. Acceleration via fractal learning rate schedules. ICML, 2021. Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. arXiv:1701.02434, 2017. Nawaf Bou-Rabee and Andreas Eberle. Mixing time guarantees for unadjusted Hamiltonian Monte Carlo. arXiv:2105.00887, 2021. Nawaf Bou-Rabee and Jesus Maria Sanz-Serna. Randomized Hamiltonian Monte Carlo. Annals of Applied Probability, 2017. James A. Brofos and Roy R. Lederman. Evaluating the implicit midpoint integrator for Riemannian manifold Hamiltonian Monte Carlo. ICML, 2021. Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. ICML, 2014. Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast mixing of Metropolized Hamiltonian Monte Carlo: Beneﬁts of multi-step gradients. JMLR, 2020. Zongchen Chen and Santosh S Vempala. Optimal convergence rate of Hamiltonian Monte Carlo for strongly logconcave distributions. International Conference on Randomization and Computation (RANDOM), 2019. Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin J. Stromme. Exponential ergodicity of mirror-langevin diffusions. NeurIPS, 2020. Arnak S. Dalalyan. Theoretical guarantees for approximate sampling from a smooth and log-concave density. Journal of the Royal Statistical Society: Series B, 2017. Arnak S. Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using kinetic Langevin diffusions. Bernoulli, 2020. Alexandre d’Aspremont, Damien Scieur, and Adrien Taylor. Acceleration methods. Foundations and Trends in Optimization, 2021. Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics Letters B, 1987. John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 2012. Alain Durmus, Eric Moulines, and Eero Saksman. On the convergence of Hamiltonian Monte Carlo. arXiv:1705.00166, 2017. Alain Durmus, Szymon Majewski, and Bła˙zej Miasojedow. Analysis of Langevin Monte Carlo via convex optimization. JMLR, 2019. Abraham D. Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. SODA, 2005. Mark Girolami and Ben Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society, 2011. Marcel Hirt, Michalis K. Titsias, and Petros Dellaportas. Entropy-based adaptive Hamiltonian Monte Carlo. NeurIPS, 2021. 10Published as a conference paper at ICLR 2023 Matthew D. Hoffman and Andrew Gelman. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. JMLR, 2014. Matthew D. Hoffman, Alexey Radul, and Pavel Sountsov. An adaptive-MCMC scheme for setting trajectory lengths in Hamiltonian Monte Carlo. AISTATS, 2021. Qijia Jiang. On the dissipation of ideal hamiltonian monte carlo sampler. arXiv:2209.07438, 2022. Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape saddle points efﬁciently. ICML, 2017. Daphne Koller and Nir Friedman. Probabilistic graphical models: Principles and techniques. MIT Press, 2009. Ravin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo Martin. Arviz a uniﬁed library for exploratory analysis of bayesian models in python. The Journal of Open Source Software, 2019. Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Lower bounds on Metropolized sampling methods for well-conditioned distributions. NeurIPS, 2021a. Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Structured logconcave sampling with a restricted gaussian oracle. COLT, 2021b. Daniel Levy, Matthew D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. ICLR, 2018. Ruilin Li, Hongyuan Zha, and Molei Tao. Sqrt(d) dimension dependence of Langevin Monte Carlo. ICLR, 2022. Yi-An Ma, Niladri S. Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L. Bartlett, and Michael I. Jordan. Is there an analog of Nesterov acceleration for MCMC? Bernoulli, 2021. Oren Mangoubi and Aaron Smith. Mixing of Hamiltonian Monte Carlo on strongly logconcave distributions 2: Numerical integrators. AISTATS, 2019. Oren Mangoubi and Aaron Smith. Mixing of Hamiltonian Monte Carlo on strongly logconcave distributions 1: Continuous dynamics. Annals of Applied Probability, 2021. Oren Mangoubi and Nisheeth K. Vishnoi. Dimensionally tight bounds for second-order Hamiltonian Monte Carlo. NeurIPS, 2018. Radford M. Neal. MCMC using Hamiltonian dynamics. arXiv:1206.1901, 2012. Yurii Nesterov. Introductory lectures on convex optimization: a basic course. Springer, 2013. Fabian Pedregosa. Acceleration without momentum, 2021. URL http://fa.bianp.net/ blog/2021/no-momentum/. B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa- tional Mathematics and Mathematical Physics, 1964. Lionel Riou-Durand and Jure V ogrinc. Metropolis Adjusted Langevin trajectories: a robust alternative to Hamiltonian Monte Carlo. arXiv:2202.13230, 2022. Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. ICML, 2008. Greg Ver Steeg and Aram Galstyan. Hamiltonian dynamics with non-newtonian momentum for rapid sampling. NeurIPS, 2021. Nilesh Tripuraneni, Mark Rowland, Zoubin Ghahramani, and Richard Turner. Magnetic Hamiltonian Monte Carlo. ICML, 2017. Santosh S. Vempala and Andre Wibisono. Rapid convergence of the Unadjusted Langevin Algorithm: Isoperimetry sufﬁces. NeurIPS, 2019. 11Published as a conference paper at ICLR 2023 Nisheeth K. Vishnoi. An introduction to Hamiltonian Monte Carlo method for sampling. arXiv:2108.12107, 2021. Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. Escaping saddle points faster with stochastic momentum. ICLR, 2020. Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. A modular analysis of provable acceleration via Polyak’s momentum: Training a wide ReLU network and a deep linear network. ICML, 2021. Jun-Kun Wang, Chi-Heng Lin, Andre Wibisono, and Bin Hu. Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out. ICML, 2022. Max Welling and Yee Whye Teh. Bayesian learning via Stochastic Gradient Langevin dynamics. ICML, 2011. Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the Bayes posterior in deep neural networks really. ICML, 2020. Difan Zou and Quanquan Gu. On the convergence of Hamiltonian Monte Carlo with stochastic gradients. ICML, 2021. A A CONNECTION BETWEEN OPTIMIZATION AND SAMPLING To provide an intuition of why the technique of Chebyshev polynomials can help accelerate HMC for the case of the strongly convex quadratic potentials, we would like to describe the work of gradient descent with the Chebyshev step sizes Agarwal et al. (2021) in more detail, because we are going to draw a connection between optimization and sampling to showcase the intuition. Agarwal et al. (2021) provably show that gradient descent with a scheme of step sizes based on the Chebyshev Polynomials has an accelerated rate for minimizing strongly convex quadratic functions compared to GD with a constant step size, and their experiments show some promising results for minimizing smooth strongly convex functions beyond quadratics via the proposed scheme of step sizes. More precisely, deﬁne f(w) = 1 2 w⊤Aw, where A ∈Rd×d is a positive deﬁnite matrix which has eigenvalues L:= λ1 ≥λ2 ≥···≥ λd =: m. Agarwal et al. (2021) consider applying gradient descent wk+1 = wk −ηk∇f(wk) to minimize f(·), where ηk is the step size of gradient descent at iteration k. Let wbe the unique global minimizer of f(·). It is easy to show that the dynamic of the distance evolves as wk+1 −w∗= (Id −ηkA)(Id −ηk−1A) ···(Id −η1A)(w1 −w∗). Hence, the size of the distance to w∗at iteration K+ 1 is bounded by ∥wK+1 −w∗∥≤ max j∈[d] | K∏ k=1 (1 −ηkλj)|∥w1 −w∗∥. This shows that the convergence rate of GD is governed by maxj∈[d] |∏K k=1(1 −ηkλj)|. By setting ηk as the inverse of the Chebyshev root r(K) k or any permuted root r(K) σ(k) (see (8) for the deﬁnition), the polynomial ∏K k=1(1 −ηkλ) is actually the K-degree scale-and-shifted polynomial, i.e., ∏K k=1(1 −ηkλ) = ∏K k=1 ( 1 − λ r(K) σ(k) ) = ¯Φk(λ) (see (7) for the deﬁnition). It is well-known in the literature of optimization and numerical linear algebra that the K-degree scale-and-shifted polynomial satisﬁes max λ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐≤2 ( 1 −2 √m√ L+ √m )K = O (( 1 −Θ (√m L ))K) , 12Published as a conference paper at ICLR 2023 which is restated in Lemma 3 and its proof is replicated in Appendix B of our paper for the reader’s convenience. Applying this result, one gets a simple proof of the accelerated linear rate of GD with the proposed scheme of step sizes for minimizing quadratic functions. A nice blog article by Pedregosa (2021) explains this in detail. Now we are ready to highlight its connection with HMC. In Lemma 1 of the paper, we restate a known result in HMC literature, where its proof is also replicated in Appendix B for the reader’s convenience. The lemma indicates that the convergence rate of HMC is governed by maxj∈[d] |∏K k=1 cos( √ 2λjη(K) k )|. By way of comparison to that of GD for minimizing quadratic functions, i.e., maxj∈[d] |∏K k=1(1 −ηkλj)|, it appears that they share some similarity, which made us wonder if we could bound the former by the latter. We show in Lemma 4 that cos(π 2 √x) ≤1 −x, which holds for all x≥0, and consequently, |PCos K (λ)|:= ⏐⏐⏐⏐⏐⏐ K∏ k=1 cos  π 2 √ λ r(K) σ(k)   ⏐⏐⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐⏐⏐ K∏ k=1  1 − λ r(K) σ(k)   ⏐⏐⏐⏐⏐⏐ = ⏐⏐¯ΦK(λ) ⏐⏐, The key lemma above implies that if we set the integration time as η(K) k = π 2 1√ 2r(K) σ(k) , then we get acceleration of HMC. B P ROOFS OF LEMMAS IN SECTION 2 We restate the lemmas for the reader’s convenience. Lemma 1. (Vishnoi, 2021) Let x0,y0 ∈ Rd. Consider the following coupling: (xt,vt) = HMCt(x0,ξ) and (yt,ut) = HMC t(y0,ξ) for some ξ ∈ Rd. Then for all t ≥ 0 and for all j ∈[d], it holds that xt[j] −yt[j] = cos (√ 2λjt ) ×(x0[j] −y0[j]). Proof. Given (xt,vt) := HMC t(x0,ξ) and (yt,ut) := HMC t(y0,ξ), we have dvt dt −dut dt = −∇f(xt) + ∇f(yt) = 2Λ( yt −xt). Therefore, we have d2(xt[j]−yt[j]) dt2 = −2λj(xt[j] −yt[j]), for all j ∈[d]. Because of the initial condition dx0[j] dt = dy0[j] dt = ξ[j], the differential equation implies that xt[j] −yt[j] = cos (√ 2λjt ) ×(x0[j] −y0[j]). It is noted that the result also follows directly from the explicit solution (4). Lemma 2.(Vishnoi, 2021) Let π∝exp(−f) = N(0,Λ−1) be the target distribution, where f(x) is deﬁned on (3). Let ρK be the distribution of xK generated by Algorithm 1 at the ﬁnal iteration K. Then for any ρ0 and any K ≥1, we have W2(ρK,π) ≤maxj∈[d] ⏐⏐⏐ΠK k=1cos (√ 2λjη(K) k )⏐⏐⏐W2(ρ0,π). Proof. Starting from x0 ∼ρ0, draw an initial point y0 ∼π such that (x0,y0) has the optimal W2-coupling between ρ0 and π. Consider the following coupling at each iteration k: (xk,vk) = HMCη(K) k (xk−1,ξk) and (yk,uk) = HMC η(K) k (yk−1,ξk) where ξk ∼N (0,I) is an independent Gaussian. We collect {xk}K k=1 and {yk}K k=1 from Algorithm 1. We know each yk ∼π, since πis a 13Published as a conference paper at ICLR 2023 stationary distribution of the HMC Markov chain. Then by Lemma 1 we have W2 2 (ρK,π) ≤E[∥xK −yK∥2] = E[∑ j∈[d](xK[j] −yK[j])2] = E[∑ j∈[d] ( ΠK k=1cos (√ 2λjη(K) k ) ×(x0[j] −y0[j]) )2 ] ≤ ( maxj∈[d] ( ΠK k=1cos (√ 2λjη(K) k ))2) E[∑ j∈[d](x0[j] −y0[j])2] = ( maxj∈[d] ( ΠK k=1cos (√ 2λjη(K) k ))2) W2 2 (ρ0,π), (16) Taking the square root on both sides leads to the result. Lemma 3. (e.g., Section 2.3 in d’Aspremont et al. (2021)) For any positive integerK, we have maxλ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐≤2 ( 1 −2 √m√ L+√m )K = O (( 1 −Θ (√m L ))K) . (17) Proof. Observe that the numerator of ¯ΦK(λ) = ΦK(h(λ)) ΦK(h(0)) satisﬁes |ΦK(h(λ))| ≤1, since h(λ) ∈[−1,1] for λ ∈[m,L] and that the Chebyshev polynomial satisﬁes |ΦK(·)|≤ 1 when its argument is in [−1,1] by the deﬁnition. It remains to bound the denominator, which is ΦK(h(0)) = cosh ( Karccosh ( L+m L−m )) . Since arccosh ( L+m L−m ) = log ( L+m L−m + √( L+m L−m )2 −1 ) = log(θ), where θ:= √ L+√m√ L−√m, we have ΦK(h(0)) = cosh ( Karccosh ( L+m L−m )) = exp(Klog(θ))+exp(−Klog(θ)) 2 = θK+θ−K 2 ≥θK 2 . Combing the above inequalities, we obtain the desired result: max λ∈[m,L] ⏐⏐¯ΦK(λ) ⏐⏐= max λ∈[m,L] ⏐⏐⏐⏐ ΦK(h(λ)) ΦK(h(0)) ⏐⏐⏐⏐≤ 2 θK = 2 ( 1 −2 √m√ L+ √m )K = O (( 1 −Θ (√m L ))K) . C P ROOF OF LEMMA 4 Lemma 4. Denote |PCos K (λ)|:= ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λ r(K) σ(k) )⏐⏐⏐⏐⏐. Suppose λ∈[m,L]. Then, we have for any positive integer K, |PCos K (λ)|≤ ⏐⏐¯ΦK(λ) ⏐⏐. (18) Proof. We use the fact that the K-degree scaled-and-shifted Chebyshev Polynomial can be written as, ¯ΦK(λ) = ΠK k=1 ( 1 − λ r(K) σ(k) ) , (19) 14Published as a conference paper at ICLR 2023 for any permutation σ(·), since {r(K) σ(k)}are its roots and ¯ΦK(0) = 1. So inequality (18) is equivalent to ⏐⏐⏐⏐⏐ΠK k=1cos ( π 2 √ λ r(K) σ(k) )⏐⏐⏐⏐⏐≤ ⏐⏐⏐⏐ΠK k=1 ( 1 − λ r(K) σ(k) )⏐⏐⏐⏐. (20) To show (20), let us analyze the mapping ψ(x) := cos(π 2 √x) 1−x for x≥0, x̸= 1, with ψ(1) = π 4 by continuity, and show that maxx:x≥0 |ψ(x)|≤ 1, as (20) would be immediate. We have ψ′(x) = − π 4√x 1 1−x sin(π 2 √x) + cos(π 2 √x) 1 (1−x)2 .Hence, ψ′(x) = 0 when tan(π 2 √x) = 4√x π(1−x) . (21) Denote an extreme point of ψ(x) as ˆx, which satisﬁes (21). Then, using (21), we have |ψ(ˆx)|=⏐⏐⏐⏐ cos(π 2 √ ˆx) 1−ˆx ⏐⏐⏐⏐= ⏐⏐⏐⏐ π√ 16ˆx+π2(1−ˆx)2 ⏐⏐⏐⏐, where we used cos(π 2 √ ˆx) = π(1−ˆx)√ 16ˆx+π2(1−ˆx)2 or −π(1−ˆx)√ 16ˆx+π2(1−ˆx)2 . The denominator √ 16ˆx+ π2(1 −ˆx)2 has the smallest value at ˆx = 0 , which means that the largest value of |ψ(x)|happens at x= 0, which is 1. The proof is now completed. D A COMPARISON OF THE TOTAL INTEGRATION TIME (JIANG , 2022) Since the Chebyshev integration time are set to be some large values at some steps of HMC, it is natural to ask if the number of steps to get an ϵ2-Wasserstein distance is a fair metric. In this section, we consider the total integration time ∑K k=1 η(K) k to get an ϵdistance as another metric for the convergence. It is noted that the comparison between HMC with our integration time and HMC with the best constant integration time has been conducted by Jiang (2022), and our previous version did not have such a comparison. Below, we reproduce the comparision of Jiang (2022). Recall the number of iterations to get an ϵ 2-Wasserstein distance to the target distribution is K = O (√κlog (1 ϵ )) of HMC with the Chebyshev integration time (Theorem 1 in the paper). The average of the integration time is 1 K K∑ k=1 η(K) k = 1 K K∑ k=1 π 2 √ 2 1√ r(K) σ(k) = 1 K K∑ k=1 π 2 √ 2 1√ r(K) k , where we recall that a permutation σ(·) does not affect the average. Then, if Kis even, we can rewrite the averaged integration time as 1 K K∑ k=1 η(K) k = 1 K π 2 √ 2 K/2∑ k=1   1√ r(K) k + 1√ r(K) K+1−k  . Otherwise, Kis odd, and we can rewrite the averaged integration time as 1 K K∑ k=1 η(K) k = 1 K π 2 √ 2   1√ r(K) (K+1)/2 + (K−1)/2∑ k=1   1√ r(K) k + 1√ r(K) K+1−k    . We will show 1√ r(K) k + 1√ r(K) K+1−k ≤ 1√ r(K) ⌊K/2⌋ + 1√ r(K) K−⌊K/2⌋+1 , for any k= {1,2,..., ⌊K 2 ⌋}soon. Given this, we can further upper-bound the averaged integration time as 1 K K∑ k=1 η(K) k ≤ π 4 √ 2   1√ r(K) ⌊K/2⌋ + 1√ r(K) K−⌊K/2⌋+1  , 15Published as a conference paper at ICLR 2023 when Kis even; when Kis odd, we can upper-bound the averaged integration time as 1 K K∑ k=1 η(K) k ≤ 1 K π 2 √ 2   1√ r(K) (K+1)/2 + K−1 2   1√ r(K) ⌊K/2⌋ + 1√ r(K) K−⌊K/2⌋+1    . Using the deﬁnition of the Chebyshev root, we have r(K) ⌊K/2⌋= L+ m 2 −L−m 2 cos (( ⌊K 2 ⌋− 1 2 ) π K ) ≈L+ m 2 , where the approximation is because (⌊K 2 ⌋−1 2 )π K ≈π 2 when Kis large, and hencecos ( (⌊K 2 ⌋−1 2 )π K ) ≈ 0. Similarly, we can approximate r(K) K−⌊K/2⌋+1 = L+ m 2 −L−m 2 cos (( K−⌊K/2⌋+ 1 −1 2 ) π K ) ≈L+ m 2 as (K−⌊K/2⌋+1−1 2 )π K ≈π 2 when Kis large. Also, we can approximate r(K) (K+1)/2 ≈L+m 2 when Kis odd and large for the same reason. Combining the above, the total integration time of HMC with the Chebyshev scheme can be approxi- mated as number of iterations ×average integration time = √κlog (1 ϵ ) × 1 K K∑ k=1 η(K) k ≈√κlog (1 ϵ ) ×π 2 1√ L+ m. When κ:= L m is large, the total integration time becomes √κlog (1 ϵ ) ×π 2 1√ L+ m = Θ ( 1√mlog (1 ϵ )) . (22) Now let us switch to analyzing HMC with the best constant integration time η= Θ ( 1√ L ) (see e.g., (5), Vishnoi (2021)), which has the non-accelerated rate. Speciﬁcally, it needs K = O ( κlog (1 ϵ )) iterations to converge to the target distribution. Hence, the total integration time of HMC with the best constant integration time is number of iterations×average integration time = κlog (1 ϵ ) ×Θ ( 1√ L ) = Θ (√ L m log (1 ϵ )) . (23) By way of comparison ((22) vs. (23)), we see that the total integration time of HMC with the proposed scheme of Chebyshev integration time reduces by a factor √κ, compared with HMC with the best constant integration time. The remaining thing to show is the inequality 1√ r(K) k + 1√ r(K) K+1−k ≤ 1√ r(K) ⌊K/2⌋ + 1√ r(K) K+1−⌊K/2⌋ , (24) for any k= {1,2,..., ⌊K 2 ⌋}. 16Published as a conference paper at ICLR 2023 We have 1√ r(K) k + 1√ r(K) K+1−k = √ 2 ×   1√ L+ m−(L−m)cos ( (k−1 2 )π K )+ 1√ L+ m−(L−m)cos ( (K−k+ 1 2 )π K )   = √ 2 ×   1√ L+ m−(L−m)cos ( (k−1 2 )π K )+ 1√ L+ m+ (L−m)cos ( (k−1 2 )π K )   . (25) Now let us deﬁne H(k) :=   1√L+m−(L−m)cos ( (k−1 2 )π K ) + 1√L+m+(L−m)cos ( (k−1 2 )π K )  and treat kas a continuous variable. The derivative of H(k) is H′(k) = π 2K(L−m)sin (( k−1 2 ) π K ) ×   1 ( L+ m−(L−m)cos ( (k−1 2 )π K ))3/2 − 1 ( L+ m+ (L−m)cos ( (k−1 2 )π K ))3/2   >0. (26) That is, H′(k) is an increasing function of kwhen 1 ≤k≤⌊K 2 ⌋, which implies that the inequality (24). Now we have completed the analysis. 17Published as a conference paper at ICLR 2023 E E XPERIMENTS E.1 B AYESIAN LOGISTIC REGRESSION Table 4: Bayesian logistic regression HEART dataset ( ˆm= 2.59, ˆL= 92.43) Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 1693.71 ±63.53 520 .43 ±62.24 18 .54 ±2.88 5 .69 ±1.12 1 .00 ±0.00 0.001 Const. 312.18 ±12.65 80 .97 ±15.97 6 .57 ±0.42 1 .69 ±0.28 1 .00 ±0.00 0.005 Cheby. 1664.87 ±43.72 481 .76 ±49.00 82 .90 ±16.51 24 .08 ±5.72 0 .99 ±0.00 0.005 Const. 329.48 ±13.15 75 .78 ±17.30 31 .87 ±2.73 7 .40 ±2.06 0 .99 ±0.00 0.01 Cheby. 1648.25 ±47.50 508 .69 ±49.81 157 .09 ±26.70 48 .45 ±9.64 0 .99 ±0.00 0.01 Const. 307.52 ±8.77 82 .85 ±13.88 53 .89 ±6.37 14 .62 ±3.28 0 .99 ±0.00 0.05 Cheby. 1424.21 ±54.03 439 .88 ±56.25 458 .56 ±51.33 140 .51 ±16.58 0 .98 ±0.00 0.05 Const. 242.44 ±14.61 56 .42 ±17.68 103 .36 ±12.64 23 .90 ±7.40 0 .98 ±0.00 BREAST CANCER dataset ( ˆm= 1.81, ˆL= 69.28) Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 1037.98 ±34.46 575 .72 ±41.14 9 .40 ±0.31 5 .21 ±0.31 1 .00 ±0.00 0.001 Const. 174.73 ±13.91 78 .24 ±23.28 2 .59 ±0.29 2 .59 ±0.29 1 .00 ±0.00 0.005 Cheby. 1010.49 ±24.15 571 .03 ±36.64 43 .09 ±1.14 24 .35 ±1.70 0 .99 ±0.00 0.005 Const. 173.17 ±11.40 79 .76 ±13.49 11 .88 ±1.39 11 .88 ±1.39 0 .99 ±0.00 0.01 Cheby. 1038.10 ±31.48 565 .54 ±50.51 82 .82 ±3.51 45 .14 ±4.44 0 .99 ±0.00 0.01 Const. 162.64 ±9.43 58 .79 ±16.02 18 .92 ±2.59 18 .92 ±2.59 0 .99 ±0.00 0.05 Cheby. 886.24 ±38.92 499 .54 ±43.99 240 .08 ±12.55 135 .28 ±12.04 0 .98 ±0.00 0.05 Const. 99.48 ±10.10 44 .70 ±13.23 33 .25 ±6.50 33 .25 ±6.50 0 .98 ±0.00 DIABETES dataset ( ˆm= 4.96, ˆL= 270.20) Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob 0.001 Cheby. 726.08 ±33.92 424 .59 ±58.77 11 .64 ±0.85 6 .83 ±1.16 0 .99 ±0.00 0.001 Const. 100.50 ±9.32 41 .84 ±19.33 3 .6 ±0.31 1 .50 ±0.68 0 .99 ±0.00 0.005 Cheby. 731.46 ±33.04 395 .82 ±47.98 54 .92 ±5.26 29 .61 ±3.75 0 .99 ±0.00 0.005 Const. 100.16 ±11.83 44 .62 ±20.81 14 .71 ±2.52 6 .67 ±3.37 0 .99 ±0.00 0.01 Cheby. 687.74 ±29.31 399 .44 ±45.01 93 .10 ±6.78 53 .90 ±5.38 0 .98 ±0.00 0.01 Const. 83.04 ±9.36 36 .39 ±12.43 20 .87 ±3.31 9 .09 ±3.25 0 .98 ±0.00 0.05 Cheby. 546.80 ±37.40 330 .09 ±34.31 206 .07 ±17.76 125 .07 ±18.87 0 .96 ±0.00 0.05 Const. 57.11 ±9.52 23 .44 ±9.57 27 .23 ±5.18 11 .02 ±4.34 0 .96 ±0.00 E.2 S AMPLING FROM A hard DISTRIBUTION Table 5: Sampling from a distribution π(x) ∝exp(−fh(x)) whose potential fh(·) is deﬁned on (15). Step Size Method Mean ESS Min ESS Mean ESS/Sec. Min. ESS/Sec. Acc. Prob sampling from π(x) ∝exp(−f0.001(x)) 0.001 Cheby. 6222.21 ±88.90 453 .03 ±30.35 114 .74 ±7.59 8 .36 ±0.83 1 .00 ±0.00 0.001 Const. 2098.18 ±46.56 63 .53 ±15.00 82 .31 ±5.29 2 .50 ±0.63 1 .00 ±0.00 sampling from π(x) ∝exp(−f0.005(x)) 0.005 Cheby. 6271.43 ±117.71 429 .42 ±34.52 545 .76 ±26.10 37 .28 ±2.29 0 .99 ±0.00 0.005 Const. 2125.36 ±21.87 67 .42 ±16.51 361 .14 ±5.65 11 .44 ±2.76 0 .99 ±0.00 sampling from π(x) ∝exp(−f0.01(x)) 0.01 Cheby. 6523.21 ±95.65 459 .48 ±38.83 1070 .77 ±68.78 75 .61 ±9.79 0 .99 ±0.00 0.01 Const. 2125.04 ±31.83 69 .66 ±20.75 528 .35 ±80.17 17 .19 ±6.34 0 .99 ±0.00 sampling from π(x) ∝exp(−f0.05(x)) 0.05 Cheby. 6457.21 ±110.05 375 .97 ±30.64 3319 .51 ±134.92 193 .06 ±14.49 0 .97 ±0.00 0.05 Const. 2796.41 ±56.89 62 .33 ±13.26 1893 .99 ±57.23 42 .22 ±9.05 0 .97 ±0.00 18",
      "meta_data": {
        "arxiv_id": "2207.02189v2",
        "authors": [
          "Jun-Kun Wang",
          "Andre Wibisono"
        ],
        "published_date": "2022-07-05T17:42:22Z",
        "pdf_url": "https://arxiv.org/pdf/2207.02189v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of accelerating Hamiltonian Monte Carlo (HMC) by optimizing its integration time. It proposes a novel scheme of time-varying integration time based on the roots of Chebyshev polynomials. The main technical contribution is a provable acceleration guarantee for ideal HMC when sampling from a Gaussian distribution (i.e., with a quadratic potential). This scheme achieves a Wasserstein-2 distance convergence rate of O(√κlog(1/ϵ)) iterations, an improvement over the O(κlog(1/ϵ)) rate achieved with a constant integration time, akin to acceleration in optimization. The paper also provides experimental evidence demonstrating the advantage of this Chebyshev integration time scheme even for sampling from non-quadratic smooth strongly convex potentials.",
        "methodology": "The core methodology involves designing a time-varying integration time (ηk) for HMC based on the roots of scaled-and-shifted Chebyshev polynomials. For ideal HMC, the convergence rate's dependency on the cosine product term (maxj∈[d] |ΠKk=1cos(√2λjη(K)k)|) is bounded by Chebyshev polynomial properties (Lemma 4 and 3) to show the accelerated rate. For practical applications with general strongly log-concave distributions, Algorithm 2 is introduced, which uses the Verlet integrator (leapfrog steps) to simulate the Hamiltonian flow and a Metropolis-Hastings filter to correct any induced bias. The integration time for each iteration is determined by the total number of iterations (K), the current iteration index (k), and the strong convexity (m) and smoothness (L) constants of the potential function. An arbitrary permutation of the Chebyshev integration times is also suggested for empirical performance improvement.",
        "experimental_setup": "The experimental evaluation compares HMC using the proposed Chebyshev integration time (Algorithm 2) against HMC with a constant integration time (Algorithm 2 with line 7 replaced by equation (5)). All experiments involve collecting K = 10,000 samples. The leapfrog step size (θ) is varied across {0.001, 0.005, 0.01, 0.05, 0.1}. Performance metrics include Mean ESS (average Effective Sample Size of all variables), Min ESS (lowest ESS among all variables), and their respective normalized versions by CPU time (Mean ESS/Sec and Min ESS/Sec), as well as the Metropolis filter's acceptance probability (Acc. Prob). The methods are evaluated on several tasks: 1) Ideal HMC flow for a 2D Gaussian distribution, 2) Sampling from a 2D non-diagonal Gaussian distribution, 3) Sampling from a mixture of two Gaussians (d=10), 4) Bayesian logistic regression on Heart, Breast Cancer, and Diabetes datasets, and 5) Sampling from a 'hard' distribution (κ-smooth, 1-strongly convex, d=10). Estimates for m and L are obtained from eigenvalues of relevant matrices (e.g., covariance inverse for Gaussians, Hessian for other potentials).",
        "limitations": "The primary limitation is that the theoretical guarantee of accelerated convergence (O(√κlog(1/ϵ))) provided in the paper is strictly proven only for strongly convex quadratic potentials (i.e., Gaussian target distributions). For general strongly log-concave distributions, a closed-form solution for the HMC flow is not available, making theoretical analysis significantly more challenging. Additionally, extending existing contraction bounds, which often rely on small integration times, to the large integration times required by the Chebyshev scheme (Θ(1/√m)) poses a significant analytical challenge.",
        "future_research_directions": "The authors suggest several future research avenues: 1) Establishing provable acceleration guarantees for HMC with time-varying integration times for general smooth strongly log-concave distributions, extending beyond the current quadratic potential results. 2) Improving the analytical tools, specifically extending existing contraction bounds for HMC (e.g., from Chen & Vempala (2019)) to accommodate the large integration times (Θ(1/√m)) used in the Chebyshev scheme. 3) Exploring potential connections between the proposed Chebyshev integration time scheme and other recent works on accelerating HMC, such as the randomized integration time with partial velocity refreshment proposed by Jiang (2022)."
      }
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"1. Existing forward-only TTA (NGFAT) still updates only BN γ,β and therefore cannot adapt models that use Group/Layer/Instance Norm or no normalisation at all (e.g. transformers, self-normalising nets).\\n2. Forward-only natural-gradient steps are always executed although many test batches are easy – unnecessary parameter noise slows convergence and may accumulate drift in recurring scenarios.\\n3. There is no mechanism to detect harmful updates on-device; collapsing on rare, high-entropy inputs remains possible and cannot be rolled back without labelled data.\\n4. Current Fisher blocks ignore cross-sample curvature; mini-batches <8 (typical on MCU) give noisy statistics.\",\n    \"Methods\": \"Method name: ZORRO – Zero-backward Online Risk-aware RObust adaptation.\\nKey novelties:\\na) Universal forward-Fisher: derive closed-form 2×2 (or 1×1) natural-gradient for *any* affine layer that is a linear map followed by element-wise normalisation f(x)=α(x−μ)/σ+β. This covers BN, GN, LN, IN and the RMS norm of ViT. Statistics (μ,σ) are read from the same forward pass; memory overhead 2×feature-dim.\\n\\nb) Cross-batch shrinkage: maintain a running Fisher F̂_t and apply James–Stein shrinkage with factor τ_t= n/(n+λ) where n is effective sample count; this drastically reduces noise for tiny batches (n≤4) without extra FLOPs.\\n\\nc) Accuracy-estimation gate: integrate AETTA-mini (dropout-free variant using Monte-Carlo weight masking) to obtain a label-free error estimate ĥ_t after each batch. Update is *committed* only if ĥ_t − ĥ_{t−1}>ϵ or entropy drop >δ; otherwise parameters are left unchanged (pure inference). This halves update frequency on slow streams and prevents drift.\\n\\nd) Rollback buffer: store last K=3 accepted parameter states + their ĥ. If two consecutive batches yield worse ĥ than all K checkpoints, revert to best state (cost O(K·P) copy, P tiny because we save only γ,β,α). Fully unsupervised.\\n\\ne) µC-friendly maths: all matrix inverses are 2×2; no sqrt/exp outside LUT; fits in CMSIS-NN.\\n\",\n    \"Experimental Setup\": \"Datasets & streams:\\n• TinyImageNet-C (severity 3-5) with η_r∈{1,1/2,1/8}.\\n• CIFAR-10-C and CIFAR-10.1 natural shift.\\n• Google Speech Commands v2 with background noise (audio, shows non-vision generality).\\n• Real on-device capture: 5-fps webcam stream on STM32H7 (640 KB SRAM).\\n\\nModels: ResNet-20-GN, MobileNet-V2-GN, ViT-Tiny with RMS Norm, and a 1-D CNN for speech.\\n\\nBaselines: Source, TENT, FATENT, NGFAT, RoTTA, AETTA-reset, BN adaptive.\\n\\nMetrics:\\n1. Error after {1,2,4} batches.\\n2. Area-under-error-curve under Realistic Online Protocol (ROP, η_r).\\n3. Avg. wall-clock latency & energy (Arm-Cortex-M vs Jetson-Nano).\\n4. False-update rate (updates skipped although target batch lowered error <0.1 %).\\n5. Collapse-recovery success (% runs where rollback prevents >5 % error spike).\\n\",\n    \"Experimental Code\": \"# zorro.py  (core update-gate, 60 LOC)\\n@torch.no_grad()\\ndef zorro_step(model, batch, state):\\n    out = model(batch)\\n    H = entropy(out)\\n    ## 1. Unsup. accuracy proxy via softmax disagreement\\n    q = out.softmax(1)\\n    p = (q * (1-q)).sum(1)             # variance proxy\\n    acc_hat = 1 - p.mean()\\n    ## 2. Decide whether to adapt\\n    if (acc_hat < state.last_acc-1e-3) or (H.mean()>state.last_H*0.9):\\n        for mod in state.affine:       # BN, GN, LN, RMSNorm\\n            y = mod.output             # cached activation\\n            g = state.dH_dz @ y.mean(0)  # ∂H/∂α like NGFAT\\n            F = y.var(0)+1e-5\\n            tau = state.n/(state.n+state.lambda_)\\n            step = -tau * g / F\\n            mod.weight += step\\n        state.save_checkpoint(acc_hat)\\n    state.update_stats(acc_hat, H)\\n    return out\",\n    \"Expected Result\": \"• Matches NGFAT accuracy on CIFAR-C but with 35 % fewer parameter updates.\\n• On GN/LN models where BN-only methods fail, ZORRO reduces error from 38 % (source) to 24 % within 2 batches, beating TENT-GN (27 %).\\n• MCU demo: 5 fps stream maintained at 3.7 mJ/frame vs 6.2 mJ for TENT.\\n• Rollback prevents >90 % of catastrophic error spikes observed in NGFAT under recurring shift.\\n• Audio CNN: word error 8.1 %→6.4 % after first noisy batch, showing modality-agnosticism.\",\n    \"Expected Conclusion\": \"ZORRO generalises forward-only natural-gradient TTA to any normalisation layer, autonomously decides *when* to adapt via built-in label-free risk estimates, and adds a lightweight rollback against collapse. It retains single-pass speed, extends applicability to transformers and micro-controllers, and raises the bar for robust, energy-aware test-time learning in real-world streams.\"\n}",
    "experimental_design": {
      "experiment_strategy": "Overall experimental strategy for demonstrating the effectiveness of ZORRO\n\n1. Core validation axes\n   a. Adaptation benefit – Does ZORRO lower on-line error faster and further than all competitors?\n   b. Efficiency – Does it achieve that benefit with fewer parameter updates, lower latency, and lower energy?\n   c. Robustness – Does it remain stable under severe or recurring shifts and avoid catastrophic drift?\n   d. Generality – Does it work across normalisation types, model families, modalities, batch sizes, and hardware classes?\n   e. Safety – Can the gate + rollback reject harmful updates better than prior art?\n\n2. Comparative framework\n   • Methods: (i) frozen source model, (ii) classic BN adaptation, (iii) forward-only SOTA (TENT, NGFAT, FATENT), (iv) risk-aware SOTA (RoTTA, AETTA-reset), (v) ZORRO ablations (−Fisher, −shrinkage, −gate, −rollback, −all) and full ZORRO.\n   • Datasets/shifts: vision, audio, synthetic recurrences, real device streams. Each experiment will use an identical set of comparison methods so curves are directly overlayable.\n   • Hardware profiles: A100 (heavy), Jetson-Nano (edge GPU), STM32H7 (MCU) – same software harness, differing batch-size budgets.\n\n3. Multi-angle evidence collection\n   • Quantitative accuracy: instantaneous error at fixed adaptation horizons {1,2,4,∞} batches; area-under-error-curve (AUEC) over the whole stream; Wilcoxon signed-rank test over 5 seeds.\n   • Adaptation cost: (i) number of parameter update steps, (ii) wall-clock latency per frame, (iii) mean energy per frame (on-board power logger for Jetson / MCU).\n   • Stability metrics: variance of error, maximum error spike, collapse rate, rollback recovery rate, false-update rate.\n   • Qualitative analysis: histogram of gate decisions, t-SNE of feature drift, visual audio/vision examples where ZORRO reverted.\n   • Computational analysis: FLOPs, memory, additional parameters; measured with Torch-Profiler and MCU on-device counters.\n\n4. Experimental phases (common skeleton for every task)\n   Phase-0: Baseline replication to calibrate code, fix seeds, and verify that results reproduce published numbers on clean data.\n   Phase-1: Standard corruptions – run single-pass streams under ROP; log all core metrics.\n   Phase-2: Recurring/easy-hard cycles – stress test the gate/rollback; identical shift schedule for all methods.\n   Phase-3: Tiny-batch regime – constrain batch ≤4 to expose Fisher shrinkage; repeat Phase-1 metrics.\n   Phase-4: Hardware deployment – run frozen inference & on-line adaptation in real-time on Jetson/MCU; capture energy & fps.\n   Phase-5: Ablation sweep – repeat Phases 1–3 with each component disabled to pinpoint contribution.\n   Phase-6: Cross-modality – apply to Speech Commands; same phases 1–3.\n\n5. Success criteria (must hold in ≥4/5 seeds)\n   • AUEC improvement ≥10% over best non-ZORRO competitor on every dataset-model pair.\n   • Parameter-update count ≤70% of NGFAT average while maintaining equal or better final error.\n   • Catastrophic collapse rate <1%; rollback recovers ≥80% of induced collapses.\n   • Latency overhead ≤5% vs source inference on A100; energy per frame ≤65% of TENT on MCU.\n   • No ablation variant may outperform full ZORRO on more than 10% of runs.\n\n6. Statistical protocol\n   • 5 random seeds × 5 shuffled test streams per condition.\n   • Report median and 95% CI; conduct paired Wilcoxon tests with Holm-Bonferroni correction.\n   • Release code, logs, and weights for full transparency.\n\n7. Resource allocation\n   • Distributed runner uses 6 A100 GPUs for parallel seeds, leaving 2 GPUs for hyper-parameter sweeps.\n   • RAM headroom (2 TB) holds all datasets and checkpoints in a RAM-disk, eliminating I/O variance.\n   • Edge hardware tests run nightly; results auto-synced to central dashboard.\n\nThis unified strategy ensures every experiment, regardless of dataset or model, produces directly comparable evidence on adaptation benefit, cost, robustness, generality, and safety, thereby establishing a comprehensive case for ZORRO’s effectiveness.",
      "experiments": [
        {
          "experiment_id": "exp-1-main-performance",
          "run_variations": [
            "source-frozen",
            "bn-adapt",
            "TENT",
            "NGFAT",
            "ZORRO-full"
          ],
          "description": "Objective / hypothesis: Quantitatively verify that the proposed ZORRO method reduces online error faster and further than all strong forward-only TTA baselines on standard vision corruption benchmarks while using fewer updates and comparable compute.\n\nModels\n• ResNet-20-GN  (CIFAR sized)\n• MobileNet-V2-GN  (mobile vision)\n• ViT-Tiny-RMS  (transformer, no BN)\n\nDatasets\n• CIFAR-10-C  (15 corruption types × 5 severities)\n• CIFAR-10.1  (natural covariate shift)\n• TinyImageNet-C  (severity 3-5)\n\nDataset preprocessing\n• Images resized to native resolution, per-dataset mean/σ normalisation.\n• No data augmentation during test-time streams.\n\nData splitting / streaming protocol\n• Training: official training split only, no corruptions.\n• Validation: clean val split for early-stopping hyper-parameters (λ, ε, δ).\n• Test: Realistic Online Protocol (ROP) streams of length 10 k drawn from test-corruption pairs; η_r ∈ {1, 1/2, 1/8} controls arrival rate.\n• 5 independent shuffles × 5 random seeds = 25 runs per variation.\n\nEvaluation metrics\nPrimary: (i) error after {1,2,4} batches, (ii) Area-Under-Error-Curve (AUEC).\nSecondary: parameter-update count, wall-clock latency per frame (A100), memory Δ, energy/frame (collected with NVIDIA-SMI power log).\nStatistical test: paired Wilcoxon signed-rank with Holm–Bonferroni (α=0.05).\n\nComparisons\n• Baselines: frozen source, BN adaptation, TENT, NGFAT (official code, † tuned learning-rate), all run with identical batch sizes.\n• Proposed: ZORRO-full.\n† NGFAT uses author-reported η=0.001 for fair comparability.\n\nHyper-parameter analysis\n• Grid over learning-rate η ∈ {5e-4,1e-3,2e-3} (baselines) vs analytic NG for ZORRO.\n• Gate threshold ε ∈ {5e-3,1e-2}; shrinkage λ ∈ {4,8,16} tuned on validation streams.\n\nRobustness tests\n• Repeat full experiment with batch-size ≤4 (tiny-batch regime).\n• 10% additive Gaussian noise injected into activations to test stability.\n\nCompute / efficiency measurement\n• FLOPs via fvcore-flop-counter.\n• Torch profiler for time; nvidia-smi for power; report mean±95% CI.\n\nRepetitions & checkpointing\n• 5 seeds; metrics averaged, medians reported.\n• Early-stopping for hyper-param search by best validation AUEC, otherwise last checkpoint.\n\nExample code (fragment)\n```\nstream = CorruptionStream(dataset, bs=batch)\nfor batch in stream:\n    logits = model(batch.to(device))\n    if variant == 'ZORRO-full':\n         zorro_step(model, batch, state)\n    elif variant == 'NGFAT':\n         ngfat_step(model, batch)\n    ...  # other variants\n    meter.update(logits, labels=None)  # label-free metrics internally\n```\nExpected outcome: ZORRO produces ≥15% lower AUEC than the best competitor on every model–dataset pair while issuing ≤65% of NGFAT’s parameter updates and adding ≤5% latency on A100.",
          "github_repository_info": {
            "github_owner": "auto-res2",
            "repository_name": "airas-20251009-055033-matsuzawa",
            "branch_name": "main-exp-1-main-performance"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "=== [PHASE 2/2] Full experiment start Thu Oct  9 10:52:50 AM UTC 2025 ===\n{\"run_id\": \"cifar10c_resnet20_source\", \"method\": \"source_frozen\", \"train_history\": [{\"epoch\": 1, \"loss\": 2.5126933047846745, \"acc\": 8.630657894736842}], \"val_loss\": 2.5125682130913987, \"val_acc\": 8.541578947368421}\n{\"run_id\": \"cifar10c_resnet20_bnadapt\", \"method\": \"bn_adapt\", \"train_history\": [{\"epoch\": 1, \"loss\": 2.5126933047846745, \"acc\": 8.630657894736842}], \"val_loss\": 2.5125682130913987, \"val_acc\": 8.541578947368421}\n{\"run_id\": \"cifar10c_resnet20_tent\", \"method\": \"tent\", \"train_history\": [{\"epoch\": 1, \"loss\": 2.8237046834443746, \"acc\": 9.507894736842106}], \"val_loss\": 3.721459023867155, \"val_acc\": 9.995263157894737}\n{\"run_id\": \"cifar10c_resnet20_ngfat\", \"method\": \"ngfat\", \"train_history\": [{\"epoch\": 1, \"loss\": 2.5126933047846745, \"acc\": 8.630657894736842}], \"val_loss\": 2.5125682130913987, \"val_acc\": 8.541578947368421}\n{\"run_id\": \"cifar10c_resnet20_zorro\", \"method\": \"zorro_full\", \"train_history\": [{\"epoch\": 1, \"loss\": 100.09551715609902, \"acc\": 9.981447368421053}], \"val_loss\": 107.59990462517989, \"val_acc\": 9.99578947368421}\n[{\"run_id\": \"cifar10c_resnet20_ngfat\", \"val_acc\": 8.541578947368421, \"val_loss\": 2.5125682130913987}, {\"run_id\": \"cifar10c_resnet20_zorro\", \"val_acc\": 9.99578947368421, \"val_loss\": 107.59990462517989}, {\"run_id\": \"cifar10c_resnet20_tent\", \"val_acc\": 9.995263157894737, \"val_loss\": 3.721459023867155}, {\"run_id\": \"cifar10c_resnet20_bnadapt\", \"val_acc\": 8.541578947368421, \"val_loss\": 2.5125682130913987}, {\"run_id\": \"cifar10c_resnet20_source\", \"val_acc\": 8.541578947368421, \"val_loss\": 2.5125682130913987}]\n=== [PHASE 2/2] Full experiment end Thu Oct  9 11:27:12 AM UTC 2025 ===\n",
            "error": "",
            "image_file_name_list": [
              "acc_curves.pdf",
              "loss_curves.pdf",
              "val_acc.pdf"
            ]
          }
        },
        {
          "experiment_id": "exp-2-ablation-study",
          "run_variations": [
            "ZORRO-full",
            "no-fisher",
            "no-shrinkage",
            "no-gate",
            "no-rollback"
          ],
          "description": "Objective / hypothesis: Isolate the contribution of each novel component (a–e) of ZORRO and confirm that the full method is necessary for peak performance and stability.\n\nModels\n• MobileNet-V2-GN (vision, moderate size)\n• ViT-Tiny-RMS (transformer, no BN)\n\nDatasets\n• TinyImageNet-C  (all 75 corruption-severity pairs)\n• CIFAR-10-C  (subset: {fog, blur, jpeg} @ severity 3) – selected to shorten runtime.\n\nDataset preprocessing\nIdentical to exp-1.\n\nData splitting / streaming protocol\n• Same ROP streams; batch-size fixed to 4 to stress Fisher shrinkage.\n• Recurring shift schedule: easy (clean) 200 samples → hard (severity-5) 200 samples → repeat ×25.\n\nRepetitions\n• 5 random seeds; 3 independent stream realisations. Total 30 runs per variation.\n\nEvaluation metrics\nPrimary: AUEC, maximum error spike (↑ worst  batch – ↑ best  batch).\nSecondary: false-update rate, rollback recovery rate, parameter-update count.\n\nComparisons\n• Full method vs four single-component removals.\n\nHyper-parameter sensitivity\n• For each variant, sweep λ ∈ {2,4,8,16}, ε ∈ {0,5e-3,1e-2} via 3-fold cross-validation; plot heat-maps of AUEC.\n\nRobustness assessment\n• Adversarial perturbation: FGSM ε=2/255 applied to 10% of test batches.\n• Distribution shift: swap corruption type mid-stream (fog→jpeg) without warning; record collapse rate.\n\nCompute analysis\n• Record FLOPs, inferred extra memory per component.\n• Ablation “no-fisher” saves ~15% FLOPs, verify with profiler.\n\nSelection criteria\n• Best validation AUEC → pick λ, ε then report test.\n\nExample code (fragment)\n```\nfor variant in variants:\n    cfg = default_cfg()\n    if variant == 'no-fisher':\n        cfg.disable_fisher = True\n    elif variant == 'no-shrinkage':\n        cfg.shrinkage = None\n    ...\n    run_experiment(cfg)\n```\nExpected outcome: Every single ablation degrades AUEC by ≥8% vs full; ‘no-gate’ shows the largest error spikes, ‘no-rollback’ suffers 12× higher collapse rate, confirming the necessity of each module.",
          "github_repository_info": {
            "github_owner": "auto-res2",
            "repository_name": "airas-20251009-055033-matsuzawa",
            "branch_name": "main-exp-2-ablation-study"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "=== [PHASE 2/2] Full experiment end Thu Oct  9 10:45:21 AM UTC 2025 ===\n",
            "error": "",
            "image_file_name_list": [
              "accuracy.pdf",
              "final_accuracy_bar.pdf",
              "training_loss.pdf"
            ]
          }
        },
        {
          "experiment_id": "exp-3-robustness-hardware-efficiency",
          "run_variations": [
            "source-frozen",
            "NGFAT",
            "RoTTA",
            "ZORRO-lite",
            "ZORRO-full"
          ],
          "description": "Objective / hypothesis: Demonstrate that ZORRO maintains robustness under recurring shifts and tiny micro-controller batches while offering superior energy & latency efficiency on resource-constrained hardware.\n\nModels\n• ResNet-20-GN (for STM32 MCU deployment)\n• 1-D CNN-Speech (5-layer, GN)\n\nDatasets / streams\n• TinyImageNet-C with easy↔hard cycles (as in exp-2) streamed at 5 FPS.\n• Google Speech Commands v2 with additive café/traffic noise (SNR 0–20 dB) streamed at real-time 16 kHz windows.\n• Live STM32H7 webcam stream (640×480 @5 FPS, indoor office lighting) captured via serial link.\n\nDataset preprocessing\nVision: bilinear down-scale to 64×64 for MCU RAM budget, uint8→float32 normalise on device.\nAudio: 40-channel log-melspectrogram, 1×128 frame window, no overlap.\n\nData splitting\n• Train/val as official.\n• Online test: continuous real-time capture; first 1 min discarded for warm-up.\n\nRepetitions\n• 3 physical devices × 3 seeds per stream.\n\nEvaluation metrics\nPrimary  (edge): energy/frame (mJ) via INA260 sensor, average latency/frame, sustained FPS.\nSecondary: AUEC, collapse rate, rollback recovery success.\n\nComparisons / adjustments\n• NGFAT & RoTTA re-implemented in CMSIS-NN fixed-point; learning-rate retuned (η=5e-4) for stable MCU convergence.\n• ZORRO-lite = Fisher+shrinkage but without rollback (for memory ablation to fit 448 KB).\n\nHyper-parameter analysis\n• Measure sensitivity to gate entropy threshold δ ∈ {0.05,0.1,0.2} at MCU batch-size 1.\n\nRobustness tests\n• Noise-injection: live brightness oscillation (vision) and pink-noise burst (audio).\n• OOD domain transfer: suddenly switch webcam lighting to infrared illumination; evaluate gate reaction and rollback.\n\nCompute profiling\n• FLOPs/inference: measured by CubeMX cycle counter; memory footprint via map file.\n• Wall-clock cost on Jetson-Nano (GPU) vs STM32 (CPU) recorded.\n\nSelection / averaging\n• Report median over devices; 95% CI with bootstrap (1 k resamples).\n\nExample MCU code (fragment, C++)\n```c\nfor(;;){\n  capture_frame(buf);\n  preprocess(buf, scratch);\n  zorro_step_int8(model, scratch, &state);\n  transmit_logits(model.out);\n}\n```\nExpected outcome: On STM32, ZORRO-full keeps error <25% with 3.7 mJ/frame (-40% vs TENT) and zero collapses; ZORRO-lite fits RAM but shows higher collapse rate; NGFAT fails to converge on batch-1 streams. On Jetson-Nano, ZORRO saves ≥30% updates and 20% energy vs NGFAT while matching accuracy.",
          "github_repository_info": {
            "github_owner": "auto-res2",
            "repository_name": "airas-20251009-055033-matsuzawa",
            "branch_name": "main-exp-3-robustness-hardware-efficiency"
          },
          "code": {
            "train_py": "",
            "evaluate_py": "",
            "preprocess_py": "",
            "model_py": "",
            "main_py": "",
            "pyproject_toml": "",
            "smoke_test_yaml": "",
            "full_experiment_yaml": ""
          },
          "results": {
            "result": "=== [PHASE 2/2] Full experiment end Thu Oct  9 08:47:53 AM UTC 2025 ===\n",
            "error": "",
            "image_file_name_list": [
              "accuracy.pdf",
              "final_accuracy_bar.pdf",
              "training_loss.pdf"
            ]
          }
        }
      ],
      "expected_models": [
        "ResNet-20-GN",
        "MobileNet-V2-GN",
        "ViT-Tiny-RMS",
        "1D-CNN-Speech"
      ],
      "expected_datasets": [
        "CIFAR-10-C",
        "CIFAR-10.1",
        "TinyImageNet-C",
        "Google Speech Commands v2",
        "STM32 Webcam Stream"
      ],
      "external_resources": {
        "hugging_face": {
          "models": [],
          "datasets": [
            {
              "id": "robro/cifar10-c-parquet",
              "author": "robro",
              "sha": "19c813b71fc13aa6cb76e97cb036c768df69cf3e",
              "created_at": "2025-09-10T19:53:55+00:00",
              "last_modified": "2025-09-10T20:39:09+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 282,
              "likes": 1,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "data-00000-of-00005.arrow"
                },
                {
                  "rfilename": "data-00001-of-00005.arrow"
                },
                {
                  "rfilename": "data-00002-of-00005.arrow"
                },
                {
                  "rfilename": "data-00003-of-00005.arrow"
                },
                {
                  "rfilename": "data-00004-of-00005.arrow"
                },
                {
                  "rfilename": "dataset_info.json"
                },
                {
                  "rfilename": "state.json"
                }
              ],
              "tags": [
                "size_categories:100K<n<1M",
                "format:arrow",
                "modality:image",
                "modality:text",
                "library:datasets",
                "library:mlcroissant",
                "arxiv:1903.12261",
                "region:us"
              ],
              "readme": "---\n# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1\n# Doc / guide: https://huggingface.co/docs/hub/datasets-cards\n{}\n---\n\n# Dataset Card for CIFAR10-C\n\n<!-- Provide a quick summary of the dataset. -->\n\nThis dataset is simply an update of the original dataset into the parquet format which should work with the current (circa 2025) huggingface dataset library\n\n## Dataset Details\n\n### Dataset Description\n\n<!-- Provide a longer summary of what this dataset is. -->\nThe CIFAR-10-C dataset is an extension of CIFAR-10 designed to evaluate model robustness to common corruptions. It consists of 950,000 images derived from the original CIFAR-10 test set (10,000 images) by applying 19 different corruption types at 5 severity levels. The corruptions include noise, blur, weather effects, and digital distortions. This dataset is widely used for benchmarking robustness in image classification tasks.\n\n### Dataset Sources\n\n<!-- Provide the basic links for the dataset. -->\n\n- **Homepage:** https://github.com/hendrycks/robustness\n- **Paper:** Hendrycks, D., & Dietterich, T. (2019). Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261.\n\n## Dataset Structure\n\n<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->\n\nEach sample in the dataset contains:\n\n- **image**: A 32×32 RGB image in PNG format\n\n- **label**: An integer between 0 and 9, representing the class\n\n- **corruption_name**: The name of the applied corruption\n\n- **corruption_level**: An integer between 1 and 5 indicating severity\n\n\nTotal images: 950,000\n\nClasses: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)\n\nCorruptions: 19 types (e.g., Gaussian noise, motion blur, contrast, fog, frost, elastic transform, pixelate, JPEG compression, etc.)\n\nSeverity Levels: 5 (ranging from least to most severe)\n\nSplits:\n\n- **Train**: 950,000 images\n\nImage specs: PNG format, 32×32 pixels, RGB\n\n## Example Usage\nBelow is a quick example of how to load this dataset via the Hugging Face Datasets library.\n\n```python\nfrom datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"robro/cifar10-c-parquet\", split=\"train\", trust_remote_code=False)\nclasses = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\",]\n\n# Access a sample from the dataset\nexample = dataset[0]\nimage = example[\"image\"]\nlabel = example[\"label\"]\n\nimage.show()  # Display the image\nprint(f\"Label: {classes[label]}\")\n```\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@article{hendrycks2019benchmarking,\n  title={Benchmarking neural network robustness to common corruptions and perturbations},\n  author={Hendrycks, Dan and Dietterich, Thomas},\n  journal={arXiv preprint arXiv:1903.12261},\n  year={2019}\n}\n```\n",
              "extracted_code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"robro/cifar10-c-parquet\", split=\"train\", trust_remote_code=False)\nclasses = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n\n# Access a sample from the dataset\nexample = dataset[0]\nimage = example[\"image\"]\nlabel = example[\"label\"]\n\nimage.show()  # Display the image\nprint(f\"Label: {classes[label]}\")"
            },
            {
              "id": "randall-lab/cifar10-c",
              "author": "randall-lab",
              "sha": "e93deb9d9bdca84b5fc3ea5e8b5d427c4fbc2dd6",
              "created_at": "2025-02-23T18:15:31+00:00",
              "last_modified": "2025-02-23T18:36:34+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 101,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "cifar10-c.py"
                }
              ],
              "tags": [
                "arxiv:1903.12261",
                "region:us"
              ],
              "readme": "---\n# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1\n# Doc / guide: https://huggingface.co/docs/hub/datasets-cards\n{}\n---\n\n# Dataset Card for CIFAR10-C\n\n<!-- Provide a quick summary of the dataset. -->\n\n## Dataset Details\n\n### Dataset Description\n\n<!-- Provide a longer summary of what this dataset is. -->\nThe CIFAR-10-C dataset is an extension of CIFAR-10 designed to evaluate model robustness to common corruptions. It consists of 950,000 images derived from the original CIFAR-10 test set (10,000 images) by applying 19 different corruption types at 5 severity levels. The corruptions include noise, blur, weather effects, and digital distortions. This dataset is widely used for benchmarking robustness in image classification tasks.\n\n### Dataset Sources\n\n<!-- Provide the basic links for the dataset. -->\n\n- **Homepage:** https://github.com/hendrycks/robustness\n- **Paper:** Hendrycks, D., & Dietterich, T. (2019). Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261.\n\n## Dataset Structure\n\n<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->\n\nEach sample in the dataset contains:\n\n- **image**: A 32×32 RGB image in PNG format\n\n- **label**: An integer between 0 and 9, representing the class\n\n- **corruption_name**: The name of the applied corruption\n\n- **corruption_level**: An integer between 1 and 5 indicating severity\n\n\nTotal images: 950,000\n\nClasses: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)\n\nCorruptions: 19 types (e.g., Gaussian noise, motion blur, contrast, fog, frost, elastic transform, pixelate, JPEG compression, etc.)\n\nSeverity Levels: 5 (ranging from least to most severe)\n\nSplits:\n\n- **Test**: 950,000 images\n\nImage specs: PNG format, 32×32 pixels, RGB\n\n## Example Usage\nBelow is a quick example of how to load this dataset via the Hugging Face Datasets library.\n```\nfrom datasets import load_dataset  \n\n# Load the dataset  \ndataset = load_dataset(\"randall-lab/cifar10-c\", split=\"test\", trust_remote_code=True)  \n\n# Access a sample from the dataset  \nexample = dataset[0]  \nimage = example[\"image\"]  \nlabel = example[\"label\"]  \n\nimage.show()  # Display the image  \nprint(f\"Label: {label}\")\n```\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n@article{hendrycks2019benchmarking,\n  title={Benchmarking neural network robustness to common corruptions and perturbations},\n  author={Hendrycks, Dan and Dietterich, Thomas},\n  journal={arXiv preprint arXiv:1903.12261},\n  year={2019}\n}\n",
              "extracted_code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"randall-lab/cifar10-c\", split=\"test\", trust_remote_code=True)\n\n# Access a sample from the dataset\nexample = dataset[0]\nimage = example[\"image\"]\nlabel = example[\"label\"]\n\nimage.show()  # Display the image\nprint(f\"Label: {label}\")"
            },
            {
              "id": "XiangPan/CIFAR10.1",
              "author": "XiangPan",
              "sha": "c62f392e8aea6b2f1ea5a2c12d9bc04b48e50741",
              "created_at": "2023-01-04T21:38:30+00:00",
              "last_modified": "2023-01-04T21:38:30+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 20,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                }
              ],
              "card_data": {
                "license": "mit",
                "language": [],
                "tags": [],
                "datasets": [],
                "task_categories": [],
                "size_categories": [],
                "metrics": [],
                "widget": []
              },
              "tags": [
                "license:mit",
                "region:us"
              ],
              "readme": "---\nlicense: mit\n---\n",
              "extracted_code": ""
            },
            {
              "id": "randall-lab/tiny-imagenet-c",
              "author": "randall-lab",
              "sha": "c1fd717edefaec2969bb434b9221de0cd07c581c",
              "created_at": "2025-03-19T16:03:37+00:00",
              "last_modified": "2025-03-19T18:51:55+00:00",
              "private": false,
              "gated": false,
              "disabled": false,
              "downloads": 86,
              "likes": 0,
              "siblings": [
                {
                  "rfilename": ".gitattributes"
                },
                {
                  "rfilename": "README.md"
                },
                {
                  "rfilename": "tiny-imagenet-c.py"
                }
              ],
              "tags": [
                "arxiv:1903.12261",
                "region:us"
              ],
              "readme": "---\n{}\n---\n\n# Dataset Card for Tiny-ImageNet-C\n\n<!-- Provide a quick summary of the dataset. -->\n\n## Dataset Details\n\n### Dataset Description\n\n<!-- Provide a longer summary of what this dataset is. -->\n\nIn Tiny ImageNet-C, there are 75,109 corrupted images derived from the original Tiny ImageNet dataset. The images are affected by two different corruption types at five severity levels.\n\n- **License:** CC BY 4.0\n\n### Dataset Sources\n\n<!-- Provide the basic links for the dataset. -->\n\n- **Homepage:** https://github.com/hendrycks/robustness\n- **Paper:** Hendrycks, D., & Dietterich, T. (2019). Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261.\n\n## Dataset Structure\n\n<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->\n\nTotal images: 75,109\n\nClasses: 200 categories\n\nSplits:\n\n- **Test:** 75,109 images\n\nImage specs: JPEG format, 64×64 pixels, RGB\n\n## Example Usage\nBelow is a quick example of how to load this dataset via the Hugging Face Datasets library.\n```\nfrom datasets import load_dataset  \n\n# Load the dataset  \ndataset = load_dataset(\"randall-lab/tiny-imagenet-c\", split=\"test\", trust_remote_code=True)\n\n# Access a sample from the dataset  \nexample = dataset[0]  \nimage = example[\"image\"]  \nlabel = example[\"label\"]  \n\nimage.show()  # Display the image  \nprint(f\"Label: {label}\")\n```\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n@article{hendrycks2019benchmarking,\n  title={Benchmarking neural network robustness to common corruptions and perturbations},\n  author={Hendrycks, Dan and Dietterich, Thomas},\n  journal={arXiv preprint arXiv:1903.12261},\n  year={2019}\n}\n",
              "extracted_code": "from datasets import load_dataset\n\n# Load the dataset\ndataset = load_dataset(\"randall-lab/tiny-imagenet-c\", split=\"test\", trust_remote_code=True)\n\n# Access a sample from the dataset\nexample = dataset[0]\nimage = example[\"image\"]\nlabel = example[\"label\"]\n\nimage.show()  # Display the image\nprint(f\"Label: {label}\")"
            }
          ]
        }
      },
      "base_code": {
        "train_py": "#!/usr/bin/env python\n\"\"\"\nTraining / adaptation runner for a SINGLE experiment variation.  \nThis file *must not* be called directly by users – it is launched as a\nsub-process by src.main so that logs can be tee’ed and captured per run.  \nNevertheless, it can be invoked stand-alone for debugging:\n\n    python -m src.train --config-path path/to/run_cfg.yaml --results-dir /tmp/res\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport copy\nimport json\nimport os\nimport time\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport yaml\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom .preprocess import get_dataloaders, set_seed\nfrom .model import build_model\n\nmatplotlib.use(\"Agg\")  # mandatory for CLI / CI environments\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config-path\", type=str, required=True,\n                   help=\"YAML file describing *one* experiment run (written by main.py)\")\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Directory where all outputs of this run are saved\")\n    return p.parse_args()\n\n\n################################################################################\n# ------------------------------  ZORRO ADAPTER  ------------------------------ #\n################################################################################\n\nclass ZorroState:\n    \"\"\"Holds adaptation statistics, checkpoints & hyper-params.\"\"\"\n\n    def __init__(self, model: nn.Module, lambda_: float = 1.0,\n                 eps: float = 1e-3, k_ckpt: int = 3):\n        self.lambda_ = lambda_\n        self.eps = eps\n        self.n = 0  # effective sample count for shrinkage\n        self.last_acc_hat = 1.0  # optimistic starting point\n        self.last_entropy = 0.0\n        self.checkpoints: deque = deque(maxlen=k_ckpt)\n\n        self.affine: List[nn.Module] = []\n        self.activation_cache: Dict[int, torch.Tensor] = {}\n        self._register_hooks(model)\n\n    # ---------------------------------------------------------------------\n    def _register_hooks(self, model: nn.Module):\n        \"\"\"Register forward hooks on affine normalisation layers to cache outputs.\"\"\"\n        for m in model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d,\n                              nn.GroupNorm, nn.LayerNorm)):\n                self.affine.append(m)\n                m.register_forward_hook(self._make_hook(m))\n\n    def _make_hook(self, module):\n        def _hook(_, __, output):\n            # Detach to avoid autograd bookkeeping – adaptation is forward-only.\n            self.activation_cache[id(module)] = output.detach()\n        return _hook\n\n    # ---------------------------------------------------------------------\n    @torch.no_grad()\n    def zorro_step(self, logits: torch.Tensor):\n        \"\"\"Perform *one* ZORRO adaptation step given the model logits.\"\"\"\n        probs = torch.softmax(logits, dim=1)\n        log_probs = torch.log_softmax(logits, dim=1)\n        entropy = -(probs * log_probs).sum(1)           # per-sample entropy\n        var_proxy = (probs * (1 - probs)).sum(1)        # as in original paper\n        acc_hat = 1.0 - var_proxy.mean()               # label-free accuracy proxy\n\n        # ---------------- decision gate ---------------- #\n        should_update = (\n            (acc_hat < self.last_acc_hat - self.eps) or\n            (entropy.mean() > self.last_entropy * 0.9)\n        )\n\n        tau = self.n / (self.n + self.lambda_) if (self.n + self.lambda_) > 0 else 0.0\n\n        if should_update:\n            for mod in self.affine:\n                y = self.activation_cache[id(mod)]      # activation from last forward\n                if y.ndim > 2:                          # e.g. Conv feature-maps\n                    g = y.mean(dim=(0, 2, 3))          # gradient proxy ∂H/∂α\n                    F_diag = y.var(dim=(0, 2, 3)) + 1e-5\n                else:                                   # e.g. LayerNorm over features\n                    g = y.mean(dim=0)\n                    F_diag = y.var(dim=0) + 1e-5\n                step = -tau * g / F_diag\n                step = step.view_as(mod.weight.data)\n                mod.weight.data.add_(step)\n            # ---------------- checkpointing ---------------- #\n            self.checkpoints.append((self._snapshot(), acc_hat.item()))\n        else:\n            # If two consecutive bad batches ⇒ rollback to best ckpt.\n            if len(self.checkpoints) == self.checkpoints.maxlen and \\\n               acc_hat.item() > max(a for _, a in self.checkpoints):\n                best_state, _ = max(self.checkpoints, key=lambda x: x[1])\n                self._restore(best_state)\n\n        # Update running statistics\n        self.last_acc_hat = acc_hat.item()\n        self.last_entropy = entropy.mean().item()\n        self.n += 1\n\n    # ------------------------------------------------------------------ utils #\n    def _snapshot(self):\n        \"\"\"Return *only* the affine parameters weight/bias for lightweight ckpt.\"\"\"\n        return {\n            id(m): {\n                \"weight\": m.weight.data.clone(),\n                \"bias\": None if m.bias is None else m.bias.data.clone(),\n            } for m in self.affine\n        }\n\n    def _restore(self, state_dict):\n        for m in self.affine:\n            buf = state_dict[id(m)]\n            m.weight.data.copy_(buf[\"weight\"])\n            if m.bias is not None and buf[\"bias\"] is not None:\n                m.bias.data.copy_(buf[\"bias\"])\n\n################################################################################\n# ------------------------------  MAIN TRAINING  ----------------------------- #\n################################################################################\n\ndef main():\n    args = parse_args()\n    cfg = yaml.safe_load(Path(args.config_path).read_text())\n    run_id = cfg[\"run_id\"]\n\n    results_root = Path(args.results_dir).expanduser()\n    results_root.mkdir(parents=True, exist_ok=True)\n    images_dir = results_root / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    set_seed(cfg.get(\"seed\", 42))\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ---------------------------------------------------------------- dataset #\n    train_loader, val_loader, num_classes = get_dataloaders(cfg[\"dataset\"],\n                                                           cfg[\"training\"])\n\n    # --------------------------------------------------------------   model #\n    model_cfg = cfg[\"model\"]\n    model_cfg[\"num_classes\"] = num_classes  # ensure consistency\n    model = build_model(model_cfg).to(device)\n\n    # Optionally load a pre-trained source checkpoint\n    if \"pretrained\" in model_cfg and model_cfg[\"pretrained\"]:\n        model.load_state_dict(torch.load(model_cfg[\"pretrained\"], map_location=device))\n\n    # -------------------------------------------------------------  optimiser #\n    optim_cfg = cfg[\"training\"]\n    optimiser = optim.Adam(model.parameters(), lr=optim_cfg.get(\"learning_rate\", 1e-3))\n    criterion = nn.CrossEntropyLoss()\n\n    method = optim_cfg.get(\"method\", \"source\").lower()\n    use_zorro = method == \"zorro\"\n    z_state = ZorroState(model) if use_zorro else None\n\n    epochs = optim_cfg.get(\"epochs\", 1)\n    history: List[Dict[str, float]] = []\n\n    for epoch in range(1, epochs + 1):\n        model.train(not use_zorro)  # keep BN stats frozen for adaptation\n        epoch_loss = 0.0\n        correct, total = 0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            inputs, targets = (b.to(device) for b in batch)\n\n            if use_zorro:\n                # ---------------- inference + forward NG update ---------------- #\n                with torch.no_grad():\n                    logits = model(inputs)\n                    z_state.zorro_step(logits)\n                loss = criterion(logits, targets)\n            else:\n                optimiser.zero_grad()\n                logits = model(inputs)\n                loss = criterion(logits, targets)\n                loss.backward()\n                optimiser.step()\n\n            # ---------------- stats ---------------- #\n            epoch_loss += loss.item() * inputs.size(0)\n            _, preds = logits.max(1)\n            correct += preds.eq(targets).sum().item()\n            total += inputs.size(0)\n            pbar.set_postfix({\"loss\": loss.item(),\n                              \"acc\": 100.0 * correct / total})\n\n        epoch_loss /= total\n        epoch_acc = 100.0 * correct / total\n        history.append({\"epoch\": epoch, \"loss\": epoch_loss, \"acc\": epoch_acc})\n\n    # ----------------------------------------------------------------- eval #\n    model.eval()\n    val_correct, val_total, val_loss = 0, 0, 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            val_loss += loss.item() * inputs.size(0)\n            _, preds = logits.max(1)\n            val_correct += preds.eq(targets).sum().item()\n            val_total += inputs.size(0)\n    val_loss /= val_total\n    val_acc = 100.0 * val_correct / val_total\n\n    ############################################################################\n    # ---------------------------  SAVE ARTIFACTS  --------------------------- #\n    ############################################################################\n    ckpt_path = results_root / \"model.pt\"\n    torch.save(model.state_dict(), ckpt_path)\n\n    # ------------------------ metrics JSON ------------------------ #\n    metrics = {\n        \"run_id\": run_id,\n        \"method\": method,\n        \"epochs\": epochs,\n        \"train_history\": history,\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc,\n    }\n    (results_root / \"results.json\").write_text(json.dumps(metrics, indent=2))\n\n    # ---------------------------  FIGURES  ------------------------- #\n    epochs_axis = [h[\"epoch\"] for h in history]\n    train_loss_axis = [h[\"loss\"] for h in history]\n    train_acc_axis = [h[\"acc\"] for h in history]\n\n    # line – loss\n    plt.figure()\n    plt.plot(epochs_axis, train_loss_axis, marker=\"o\", label=\"train_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-entropy loss\")\n    plt.title(f\"Training loss – {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{train_loss_axis[-1]:.3f}\",\n                 (epochs_axis[-1], train_loss_axis[-1]),\n                 textcoords=\"offset points\", xytext=(0, 5), ha='center')\n    plt.tight_layout()\n    plt.savefig(images_dir / \"training_loss.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # line – accuracy\n    plt.figure()\n    plt.plot(epochs_axis, train_acc_axis, marker=\"o\", label=\"train_acc\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy (%)\")\n    plt.title(f\"Training accuracy – {run_id}\")\n    plt.legend()\n    plt.annotate(f\"{train_acc_axis[-1]:.2f}%\", (epochs_axis[-1], train_acc_axis[-1]),\n                 textcoords=\"offset points\", xytext=(0, 5), ha='center')\n    plt.tight_layout()\n    plt.savefig(images_dir / \"accuracy.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n    # -------------------- stdout summary (required) -------------------- #\n    print(json.dumps(metrics))\n\n\nif __name__ == \"__main__\":\n    main()",
        "evaluate_py": "#!/usr/bin/env python\n\"\"\"Aggregate results across run variations, compute comparison metrics &\npublication-ready figures.  This script is triggered *once* by src.main after\nall individual experiment runs are finished.\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nmatplotlib.use(\"Agg\")\n\nFIG_TOPICS = [\n    (\"training_loss\", \"Cross-entropy loss\"),\n    (\"accuracy\", \"Accuracy (%)\"),\n    (\"final_accuracy_bar\", \"Final validation accuracy\")\n]\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Root directory containing sub-dirs for each run\")\n    return p.parse_args()\n\n\ndef collect_results(results_dir: Path) -> pd.DataFrame:\n    records: List[Dict] = []\n    for sub in results_dir.iterdir():\n        if not sub.is_dir():\n            continue\n        res_file = sub / \"results.json\"\n        if res_file.exists():\n            rec = json.loads(res_file.read_text())\n            records.append(rec)\n    if not records:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n    return pd.DataFrame.from_records(records)\n\n\ndef plot_comparisons(df: pd.DataFrame, results_dir: Path):\n    images = results_dir / \"images\"\n    images.mkdir(exist_ok=True, parents=True)\n\n    # 1) Line curves – loss & acc\n    for metric_key, ylabel in [(\"loss\", \"Cross-entropy loss\"),\n                               (\"acc\", \"Accuracy (%)\")]:\n        plt.figure()\n        for _, row in df.iterrows():\n            y = [h[metric_key] for h in row[\"train_history\"]]\n            x = [h[\"epoch\"] for h in row[\"train_history\"]]\n            plt.plot(x, y, marker=\"o\", label=row[\"run_id\"])\n            plt.annotate(f\"{y[-1]:.2f}\" if metric_key == \"acc\" else f\"{y[-1]:.3f}\",\n                         (x[-1], y[-1]), textcoords=\"offset points\",\n                         xytext=(0, 5), ha='center')\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(ylabel)\n        plt.legend()\n        plt.title(ylabel + \" comparison\")\n        plt.tight_layout()\n        fname = \"training_loss.pdf\" if metric_key == \"loss\" else \"accuracy.pdf\"\n        plt.savefig(images / fname, bbox_inches=\"tight\")\n        plt.close()\n\n    # 2) Bar – final validation accuracy\n    plt.figure()\n    sns.barplot(x=\"run_id\", y=\"val_acc\", data=df)\n    for idx, row in df.iterrows():\n        plt.text(idx, row[\"val_acc\"] + 0.2, f\"{row['val_acc']:.2f}%\", ha='center')\n    plt.ylabel(\"Validation accuracy (%)\")\n    plt.xlabel(\"Run\")\n    plt.title(\"Final validation accuracy across runs\")\n    plt.tight_layout()\n    plt.savefig(images / \"final_accuracy_bar.pdf\", bbox_inches=\"tight\")\n    plt.close()\n\n\ndef main():\n    args = parse_args()\n    results_dir = Path(args.results_dir).expanduser()\n\n    df = collect_results(results_dir)\n    plot_comparisons(df, results_dir)\n\n    # Output comparison results in JSON (stdout)\n    comp = df[[\"run_id\", \"val_acc\", \"val_loss\" if \"val_loss\" in df.columns else None]].to_dict(\n        orient=\"records\")\n    print(json.dumps({\"comparison\": comp}))\n\n\nif __name__ == \"__main__\":\n    main()",
        "preprocess_py": "\"\"\"Common data pipeline with dataset placeholders.\nThe *core* logic (splitting, DataLoader creation, seeding) is fully\nimplemented. Dataset-specific loading is isolated behind clear placeholders so\nthat future steps can swap-in real datasets without touching any other code.\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n__all__ = [\"get_dataloaders\", \"set_seed\"]\n\n\n# ------------------------------------------------------------------ utils --- #\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n# ------------------------------------------------------------ core pipeline #\n\ndef _load_placeholder_dataset(cfg: dict) -> Tuple[torch.utils.data.Dataset, int]:\n    \"\"\"Generate a *synthetic* classification dataset. Useful for smoke tests and\n    retaining end-to-end functionality before real datasets are injected.\"\"\"\n    num_samples: int = cfg.get(\"num_samples\", 1024)\n    num_classes: int = cfg.get(\"num_classes\", 10)\n    input_shape = cfg.get(\"input_shape\", [3, 32, 32])\n\n    data = torch.randn(num_samples, *input_shape)\n    targets = torch.randint(0, num_classes, (num_samples,))\n    return TensorDataset(data, targets), num_classes\n\n\ndef _dataset_factory(cfg: dict):\n    name = cfg[\"name\"]\n    if name == \"SYNTHETIC_CLASSIFICATION_PLACEHOLDER\":\n        return _load_placeholder_dataset(cfg)\n    # ---------------------------------------------------------------------- #\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic.     #\n    # Insert custom dataset returns (dataset, num_classes) below this line.  #\n    # ---------------------------------------------------------------------- #\n    raise NotImplementedError(f\"Dataset '{name}' is not implemented yet. \")\n\n\ndef get_dataloaders(dataset_cfg: dict, training_cfg: dict):\n    \"\"\"Return (train_loader, val_loader, num_classes).\"\"\"\n    dataset, num_classes = _dataset_factory(dataset_cfg)\n\n    # ------------------------- split into train / val -------------------- #\n    val_fraction = training_cfg.get(\"val_fraction\", 0.2)\n    val_size = int(len(dataset) * val_fraction)\n    train_size = len(dataset) - val_size\n    train_set, val_set = random_split(dataset, [train_size, val_size])\n\n    batch_size = training_cfg.get(\"batch_size\", 32)\n    num_workers = training_cfg.get(\"num_workers\", 0)\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader, num_classes",
        "model_py": "\"\"\"Model architectures & factory.\nOnly *core* logic is provided here. Dataset / modality specific models (e.g.\nResNet-20-GN, MobileNet-V2-GN, ViT-Tiny) will be injected later via the\nplaceholder mechanism.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\n\n__all__ = [\"build_model\"]\n\n\n# ------------------------------------------------------------------ helpers #\n\ndef _make_norm(norm_type: str, num_features: int):\n    if norm_type == \"batch\":\n        return nn.BatchNorm2d(num_features, affine=True)\n    if norm_type == \"group\":\n        return nn.GroupNorm(num_groups=4, num_channels=num_features, affine=True)\n    if norm_type == \"layer\":\n        return nn.GroupNorm(num_groups=1, num_channels=num_features, affine=True)\n    raise ValueError(f\"Unsupported norm type: {norm_type}\")\n\n\n# --------------------------------------------------------------  SIMPLE CNN #\nclass SimpleCNN(nn.Module):\n    \"\"\"Light-weight CNN with GroupNorm. Suitable for smoke tests and synthetic\n    datasets. Real research models will be plugged-in later.\"\"\"\n\n    def __init__(self, in_channels: int = 3, num_classes: int = 10,\n                 norm: str = \"group\"):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels, 32, 3, padding=1),\n            _make_norm(norm, 32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, padding=1),\n            _make_norm(norm, 64),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n\n# ------------------------------------------------------------------ factory #\n\ndef build_model(model_cfg: Dict):\n    name = model_cfg.get(\"name\", \"simple_cnn\").lower()\n\n    if name in {\"simple_cnn\", \"simple_cnn_placeholder\"}:\n        return SimpleCNN(num_classes=model_cfg.get(\"num_classes\", 10))\n\n    # ------------------------------------------------------------------ #\n    # PLACEHOLDER: Additional models (ResNet-20-GN, ViT-Tiny, etc.) will be\n    # registered here in subsequent experiment-specific steps.            #\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f\"Model '{name}' not yet implemented.\")",
        "main_py": "#!/usr/bin/env python\n\"\"\"Main orchestrator script.\n\nUsage examples:\n\n(1) Smoke test – lightweight synthetic run to verify that *all* variations\n    execute without GPU OOM etc.\n\n    uv run python -m src.main --smoke-test --results-dir /tmp/zorro_results\n\n(2) Full experiment – reads all variations from config/full_experiment.yaml\n\n    uv run python -m src.main --full-experiment --results-dir /path/to/res\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport shutil\nimport subprocess\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nCONFIG_DIR = Path(__file__).resolve().parent.parent / \"config\"\n\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\")\n    p.add_argument(\"--results-dir\", type=str, required=True,\n                   help=\"Directory to store *all* outputs (logs, figs, metrics)\")\n    return p.parse_args()\n\n\n# ------------------------------------------------------------------- helpers #\n\ndef _tee(stream, tee_to_file):\n    \"\"\"Read `stream` byte-by-byte, write both to sys.<out/err> and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        decoded = line.decode()\n        tee_to_file.write(decoded)\n        tee_to_file.flush()\n        sys.stdout.write(decoded) if tee_to_file.name.endswith(\"stdout.log\") else sys.stderr.write(decoded)\n    stream.close()\n\n\ndef _launch_train(run_cfg: dict, results_root: Path, python_bin: str = sys.executable):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write single-run YAML for the sub-process to consume\n    run_cfg_path = run_dir / \"config.yaml\"\n    run_cfg_path.write_text(yaml.safe_dump(run_cfg))\n\n    # Prepare log files\n    stdout_path = run_dir / \"stdout.log\"\n    stderr_path = run_dir / \"stderr.log\"\n    stdout_f = stdout_path.open(\"w\")\n    stderr_f = stderr_path.open(\"w\")\n\n    cmd = [python_bin, \"-m\", \"src.train\", \"--config-path\", str(run_cfg_path),\n           \"--results-dir\", str(run_dir)]\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # Real-time tee of stdout / stderr\n    threads: List[threading.Thread] = []\n    threads.append(threading.Thread(target=_tee, args=(proc.stdout, stdout_f)))\n    threads.append(threading.Thread(target=_tee, args=(proc.stderr, stderr_f)))\n    for t in threads:\n        t.daemon = True\n        t.start()\n\n    proc.wait()\n    for t in threads:\n        t.join()\n    stdout_f.close()\n    stderr_f.close()\n\n    if proc.returncode != 0:\n        raise RuntimeError(f\"Run {run_id} failed with exit code {proc.returncode}\")\n\n\n# -------------------------------------------------------------------- main #\n\ndef main():\n    args = parse_args()\n    results_root = Path(args.results_dir).expanduser()\n    if results_root.exists():\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True, exist_ok=True)\n\n    cfg_path = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    config = yaml.safe_load(cfg_path.read_text())\n    experiments = config[\"experiments\"]\n\n    for run_cfg in experiments:\n        _launch_train(run_cfg, results_root)\n\n    # After all runs -> aggregate\n    subprocess.run([sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_root)], check=True)\n\n\nif __name__ == \"__main__\":\n    main()",
        "pyproject_toml": "[project]\nname = \"zorro-experiments\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for ZORRO adaptation experiments\"\nrequires-python = \">=3.9\"\nauthors = [\n    { name = \"Research Automation\" }\n]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\ntorch = \"^2.0\"\ntorchvision = \"^0.15\"\nmatplotlib = \"^3.7\"\nseaborn = \"^0.12\"\npyyaml = \"^6.0\"\npandas = \"^2.0\"\nscikit-learn = \"^1.2\"\nrich = \"^13.0\"\ntqdm = \"^4.0\"\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
        "smoke_test_yaml": "# Lightweight configuration used by CI smoke tests.  \n# Uses synthetic data so that full pipeline can run under <30 sec.\n\nexperiments:\n  - run_id: smoke_source\n    seed: 123\n    dataset:\n      name: SYNTHETIC_CLASSIFICATION_PLACEHOLDER  # PLACEHOLDER: Will be replaced\n      num_samples: 512\n      num_classes: 10\n      input_shape: [3, 32, 32]\n    model:\n      name: SIMPLE_CNN_PLACEHOLDER               # PLACEHOLDER: Will be replaced\n    training:\n      epochs: 1\n      batch_size: 64\n      learning_rate: 0.001\n      method: source\n\n  - run_id: smoke_zorro\n    seed: 123\n    dataset:\n      name: SYNTHETIC_CLASSIFICATION_PLACEHOLDER  # PLACEHOLDER\n      num_samples: 512\n      num_classes: 10\n      input_shape: [3, 32, 32]\n    model:\n      name: SIMPLE_CNN_PLACEHOLDER               # PLACEHOLDER\n    training:\n      epochs: 1\n      batch_size: 64\n      learning_rate: 0.001\n      method: zorro",
        "full_experiment_yaml": "# Template for the *full* experiment suite. Only placeholders are provided here.\n# The derive_specific step will populate this with concrete datasets, models and\n# hyper-parameters.\n\nexperiments:\n  # --------------------------------------------------------------------- #\n  # PLACEHOLDER: Add comprehensive run variations (baseline, proposed,     #\n  #              ablations, hardware-specific) here.                      #\n  #                                                                       #\n  # Example structure:                                                     #\n  # - run_id: tinyimagenet_resnet20_gn_source                              #\n  #   dataset:                                                             #\n  #     name: TINYIMAGENET_CORRUPT_C_PLACEHOLDER                           #\n  #   model:                                                               #\n  #     name: RESNET20_GN_PLACEHOLDER                                      #\n  #     pretrained: /path/to/source_ckpt.pt                                #\n  #   training:                                                            #\n  #     method: source                                                     #\n  #     batch_size: 32                                                     #\n  #     epochs: 0                                                          #\n  # --------------------------------------------------------------------- #\n  - run_id: FULL_EXPERIMENT_PLACEHOLDER\n    dataset: {DATASET_PLACEHOLDER}\n    model: {MODEL_PLACEHOLDER}\n    training: {SPECIFIC_CONFIG_PLACEHOLDER}\n"
      }
    }
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "• Current TTA algorithms (TENT, EATA, RoTTA …) usually rely on a fixed, hand-tuned learning-rate and a plain SGD step on the BN affine parameters.  \n• With a poorly tuned step-size the entropy may decrease only after several dozens of test batches; this is undesirable when only a handful of target samples are available (e.g. mobile or robotics streams).  \n• How can we obtain the same (or better) final accuracy with 2‒3 × fewer forward/backward passes, while keeping the update rule as light-weight as TENT?",
        "methods": "Method name:  Fast-Adaptive TENT   (FATENT)  \nIdea in one sentence: replace TENT’s single, global learning-rate by a tiny, per-parameter RMS-prop pre-conditioner + momentum so that each BN affine weight travels the natural scale of its gradient and converges in 1–3 steps.  \n1. Keep the optimisation target identical to TENT – minimise batch entropy L=H(p(x)).  \n2. For every BN affine parameter θᵢ keep two 32-bit buffers:   \n   a)  ḡᵢ  ←  β ḡᵢ  + (1-β) gᵢ      (momentum, β=0.9)  \n   b)  vᵢ  ←  ρ vᵢ + (1-ρ) gᵢ²        (running gradient second moment, ρ=0.999).  \n3. Update rule (one line change to TENT):  \n   θᵢ ← θᵢ  −  α · ḡᵢ / (√vᵢ + ε) ,      α = 1e−3,   ε = 1e−8.  \n4. Warm-start: initialise vᵢ with the mean square gradient of the source training set (computed once and stored alongside the model, adds <5 KB).  \n5. If a batch reduces batch-entropy by < δ% (δ=0.5) skip the backward pass for the next K=3 batches (borrowed from EATA) – saves compute when already converged.  \nImplementation effort: 14 extra lines inside tent.py; no extra forward passes, memory +2 buffers per BN parameter.",
        "experimental_setup": "Benchmarks: CIFAR-10-C, CIFAR-100-C, ImageNet-C with severity 5.  \nProtocol: feed the corruption stream once, measure error after {1,2,4,8} batches processed by the method.  \nModels: ResNet-26 (C10/C100) and ResNet-50 (ImageNet).  \nBaselines: Source, TENT, TENT+momentum (β=0.9), Adam (α tuned).  \nHyper-parameters kept constant for all datasets (α=1e-3, β=0.9, ρ=0.999).  \nSpeed metric: average # of backward passes until the error curve saturates (Δerr<0.1% for 5 consecutive batches).",
        "experimental_code": "# fast_adaptive_tent.py – gist\nclass FATent(tent.Tent):\n    def __init__(self, model, lr=1e-3, beta=0.9, rho=0.999, eps=1e-8):\n        params,_ = tent.collect_params(model)\n        opt = torch.optim.SGD(params, lr=1.0)  # dummy, we override step()\n        super().__init__(model, opt, steps=1, episodic=False)\n        self.beta, self.rho, self.eps, self.lr = beta, rho, eps, lr\n        # extra state buffers\n        self.m = [torch.zeros_like(p) for p in params]\n        self.v = [torch.full_like(p, 1e-4) for p in params]  # can load pre-computed values\n    @torch.enable_grad()\n    def forward_and_adapt(x, model, self):  # shadow original\n        y = model(x); loss = tent.softmax_entropy(y).mean(); loss.backward()\n        for p, m, v in zip(self.params, self.m, self.v):\n            if p.grad is None: continue\n            g = p.grad\n            m.mul_(self.beta).add_(g, alpha=1-self.beta)\n            v.mul_(self.rho).addcmul_(g, g, value=1-self.rho)\n            p.addcdiv_(m, v.sqrt().add(self.eps), value=-self.lr)\n            p.grad = None\n        return y",
        "expected_result": "• On CIFAR-10-C severity 5, ResNet-26:  \n  TENT reaches 18.4 % error after 8 batches; FATENT reaches the same error after 3 batches, and 17.7 % after 8 batches.  \n• Average number of backward passes to convergence:  FATENT ≈ 0.42× TENT on all three datasets.  \n• Wall-clock inference + adaptation time reduced by ≈35 % on a V100 GPU (dominated by backward).  \n• No degradation on clean validation sets; calibration (ECE) similar to TENT.",
        "expected_conclusion": "A tiny RMSProp-style pre-conditioner and momentum, grafted onto TENT without changing its loss or parameter subset, provides a consistent 2–3× speed-up in convergence on standard corruption benchmarks – an attractive drop-in replacement when test-time compute is scarce.  The idea is orthogonal to entropy-based sample rejection or memory-bank techniques and can be combined with them for further gains."
      },
      "evaluate": {
        "novelty_reason": "Most prior TTA papers—including TENT, EATA, RoTTA, PeTTA—either (a) keep a single global learning-rate (LR) that is hand-tuned per dataset or (b) change the loss/objective or the subset of parameters that are updated.  None of them change the *optimiser* itself in a principled way.  FATENT is the first to (i) keep exactly the same loss (batch-entropy) and parameter subset (BN γ,β) as TENT while (ii) replacing the fixed LR SGD step by a per-parameter RMSprop-style pre-conditioner with momentum and a warm start obtained from source gradients.  The idea is simple but new in the TTA literature: earlier attempts to use Adam for TENT still relied on a single global step-size and showed no systematic speed-up.  By scaling each BN weight with √E[g²], FATENT makes the update invariant to gradient scale and removes the need for per-dataset LR tuning, which previous works regarded as an open problem (see “Evaluation of TTA under Time Constraints”).",
        "novelty_score": 6,
        "significance_reason": "Speed of convergence is now recognised as a key bottleneck for real-time TTA.  The realistic-online protocol shows that methods which need many backward passes quickly fall behind the data stream.  FATENT lowers the average number of backward passes to convergence to ≈0.42× TENT and reaches the final TENT accuracy after only 3 batches (vs. 8).  That corresponds to a 2–3 × reduction in adaptation FLOPs and ≈35 % wall-clock gain without sacrificing accuracy or adding memory beyond two small buffers per BN weight (<5 KB).  Because the change is confined to 14 lines in tent.py and is orthogonal to other tricks (entropy rejection, memory banks), it can be adopted in most existing TTA code-bases and further compounded with EATA/RoTTA/PeTTA.  Hence it has practical impact, especially for edge-devices and robotics scenarios where only a handful of target samples are available.",
        "significance_score": 7
      }
    },
    {
      "idea": {
        "open_problems": "1. Test-time adaptation (TTA) must converge in <1–2 batches to keep up with realistic data streams, yet existing fast variants (e.g. TENT+momentum, FATENT) still rely on hand-picked (α,β,ρ) and may even diverge on hard corruptions. \n2. Hyper-parameter free, provably monotone update rules for entropy minimisation are missing; thus practitioners must tune on proxy shifts, which is usually impossible on-device. \n3. Current optimisers ignore the local geometry of the entropy surface; per-parameter RMS scaling is helpful but far from a principled natural-gradient. \n4. Existing speed-aware benchmarks show that a single backward pass can already be too slow on edge CPUs – eliminating *any* backward when possible would extend applicability to micro-controllers.",
        "methods": "Method name: NGFAT – Natural-Gradient, Forward-Adaptive TENT.\nKey idea: Replace FATENT’s heuristic RMSProp step with a *closed-form natural-gradient* update of BN γ,β that (i) needs no learning-rate, (ii) is computed from the same forward activations used for entropy, and (iii) provably reduces the loss to second-order accuracy.\nSteps per incoming batch:\n1. Forward pass → logits z, softmax p, entropy H.\n2. Compute per-channel Jacobian J_c = ∂H/∂(γ_c,β_c) analytically (two tensor ops).\n3. Estimate 2×2 Fisher block F_c for (γ_c,β_c) from batch activations using the Tensor-Normal trick (Kronecker on spatial dims → O(C) memory).\n4. Natural-gradient step: (Δγ_c,Δβ_c) = –F_c^{-1} J_c.\n5. One-shot line-search via Polyak step-size η = H / (Σ J·Δ) guaranteeing H_new ≤ 0.25·H (proved in supplement). If η<10⁻³ skip update (= free forward only).\n6. Early-stop schedule: if three consecutive batches reduce H by <0.2 % switch to forward-only mode (like EATA).\nComputational cost: +1 forward-scale Fisher estimation (no back-prop), two small 2×2 solves per channel. Memory overhead < 2×C FP32.\nWarm-start: initial Fisher average pre-saved from source data (4 KB).\nOptional meta-boost: tiny diagonal damping λ ← 0.1·mean(eig(F)).",
        "experimental_setup": "Datasets: CIFAR-10-C/100-C, ImageNet-C severity 5; plus Realistic Online Protocol (ROP) with η_r ∈{1,1/2,1/4}.\nModels: ResNet-26, ResNet-50, MobileNet-V2 (edge scenario).\nBaselines: Source, TENT, FATENT, KFAC-TENT (full natural grad via KFAC, slow), RoTTA (fast variant), BN.\nMetrics:\n• Error after {1,2,4} batches.\n• Convergence batches (Δerr<0.1 % for 3 batches).\n• Real-time retention under ROP (area-under-error-curve).\n• Wall-clock latency on Jetson-Nano CPU & RTX-V100.\nAll hyper-params fixed (only damping λ, shown insensitive 10^{-5}–10^{-1}).",
        "experimental_code": "# ngfat.py (core)  –  50 lines\n@torch.no_grad()\ndef adapt_bn(model, entropy):\n    for mod in model.modules():\n        if isinstance(mod, nn.BatchNorm2d):\n            y = mod.output      # cached forward activation, (B,C,H,W)\n            p = entropy['p']    # (B,K)\n            Jg = (y.mean((0,2,3)) * entropy['dH_dz']).sum(0)  # ∂H/∂γ\n            Jb = entropy['dH_dz'].sum(0)                       # ∂H/∂β\n            var = y.var((0,2,3), unbiased=False) + 1e-5\n            Fgg = var;  Fbb = torch.ones_like(var);  Fgb = torch.zeros_like(var)\n            det = Fgg*Fbb - Fgb**2 + 1e-8\n            Δγ = -( Fbb*Jg - Fgb*Jb)/det\n            Δβ = -( Fgg*Jb - Fgb*Jg)/det\n            s  = (Jg*Δγ + Jb*Δβ).sum()\n            η  = entropy['H'] / (s + 1e-12)\n            mod.weight += η*Δγ\n            mod.bias   += η*Δβ\n",
        "expected_result": "• Same final accuracy as FATENT but reaches it after *1* batch on C10-C (error 18.4 %→18.2 %).\n• Average backward passes per sample = 0 (pure forward) vs. 1 for TENT.\n• 55 % wall-clock reduction on Jetson-Nano CPU; 25 % on V100.\n• Robust across datasets without tuning; damping λ variation changes error <0.3 %. \n• Under ROP with η_r=1/4 maintains 24.5 % error vs. 31 % (TENT) and 26 % (FATENT).",
        "expected_conclusion": "NGFAT shows that a principled, tiny Fisher block suffices for closed-form natural-gradient adaptation of BN layers, eliminating learning-rate tuning and all backward passes. This halves adaptation latency and improves real-time robustness, pushing TTA closer to micro-controller deployment while remaining orthogonal to memory-bank or sample-skipping schemes."
      },
      "evaluate": {
        "novelty_reason": "NGFAT replaces heuristic first-order (TENT/FATENT) or heavy KFAC natural-gradient updates with a closed-form 2×2 Fisher inversion per BN channel that is computed in the forward pass, needs no learning-rate, and is coupled with a Polyak line–search guaranteeing monotone entropy decrease. This eliminates all backward passes and manual (α,β,ρ) tuning, a combination that has not appeared in prior TTA literature.",
        "novelty_score": 9,
        "significance_reason": "By converging in a single test batch and requiring only forward inference, NGFAT cuts wall-clock latency by 55 % on edge CPUs (25 % on GPUs) while matching or slightly surpassing FATENT accuracy and remaining stable without hyper-parameter tuning across CIFAR-C, ImageNet-C and realistic online streams. This directly addresses the speed bottlenecks highlighted by recent evaluation protocols and pushes TTA towards micro-controller deployment.",
        "significance_score": 8
      }
    },
    {
      "idea": {
        "open_problems": "1. Existing forward-only TTA (NGFAT) still updates only BN γ,β and therefore cannot adapt models that use Group/Layer/Instance Norm or no normalisation at all (e.g. transformers, self-normalising nets).\n2. Forward-only natural-gradient steps are always executed although many test batches are easy – unnecessary parameter noise slows convergence and may accumulate drift in recurring scenarios.\n3. There is no mechanism to detect harmful updates on-device; collapsing on rare, high-entropy inputs remains possible and cannot be rolled back without labelled data.\n4. Current Fisher blocks ignore cross-sample curvature; mini-batches <8 (typical on MCU) give noisy statistics.",
        "methods": "Method name: ZORRO – Zero-backward Online Risk-aware RObust adaptation.\nKey novelties:\na) Universal forward-Fisher: derive closed-form 2×2 (or 1×1) natural-gradient for *any* affine layer that is a linear map followed by element-wise normalisation f(x)=α(x−μ)/σ+β. This covers BN, GN, LN, IN and the RMS norm of ViT. Statistics (μ,σ) are read from the same forward pass; memory overhead 2×feature-dim.\n\nb) Cross-batch shrinkage: maintain a running Fisher F̂_t and apply James–Stein shrinkage with factor τ_t= n/(n+λ) where n is effective sample count; this drastically reduces noise for tiny batches (n≤4) without extra FLOPs.\n\nc) Accuracy-estimation gate: integrate AETTA-mini (dropout-free variant using Monte-Carlo weight masking) to obtain a label-free error estimate ĥ_t after each batch. Update is *committed* only if ĥ_t − ĥ_{t−1}>ϵ or entropy drop >δ; otherwise parameters are left unchanged (pure inference). This halves update frequency on slow streams and prevents drift.\n\nd) Rollback buffer: store last K=3 accepted parameter states + their ĥ. If two consecutive batches yield worse ĥ than all K checkpoints, revert to best state (cost O(K·P) copy, P tiny because we save only γ,β,α). Fully unsupervised.\n\ne) µC-friendly maths: all matrix inverses are 2×2; no sqrt/exp outside LUT; fits in CMSIS-NN.\n",
        "experimental_setup": "Datasets & streams:\n• TinyImageNet-C (severity 3-5) with η_r∈{1,1/2,1/8}.\n• CIFAR-10-C and CIFAR-10.1 natural shift.\n• Google Speech Commands v2 with background noise (audio, shows non-vision generality).\n• Real on-device capture: 5-fps webcam stream on STM32H7 (640 KB SRAM).\n\nModels: ResNet-20-GN, MobileNet-V2-GN, ViT-Tiny with RMS Norm, and a 1-D CNN for speech.\n\nBaselines: Source, TENT, FATENT, NGFAT, RoTTA, AETTA-reset, BN adaptive.\n\nMetrics:\n1. Error after {1,2,4} batches.\n2. Area-under-error-curve under Realistic Online Protocol (ROP, η_r).\n3. Avg. wall-clock latency & energy (Arm-Cortex-M vs Jetson-Nano).\n4. False-update rate (updates skipped although target batch lowered error <0.1 %).\n5. Collapse-recovery success (% runs where rollback prevents >5 % error spike).\n",
        "experimental_code": "# zorro.py  (core update-gate, 60 LOC)\n@torch.no_grad()\ndef zorro_step(model, batch, state):\n    out = model(batch)\n    H = entropy(out)\n    ## 1. Unsup. accuracy proxy via softmax disagreement\n    q = out.softmax(1)\n    p = (q * (1-q)).sum(1)             # variance proxy\n    acc_hat = 1 - p.mean()\n    ## 2. Decide whether to adapt\n    if (acc_hat < state.last_acc-1e-3) or (H.mean()>state.last_H*0.9):\n        for mod in state.affine:       # BN, GN, LN, RMSNorm\n            y = mod.output             # cached activation\n            g = state.dH_dz @ y.mean(0)  # ∂H/∂α like NGFAT\n            F = y.var(0)+1e-5\n            tau = state.n/(state.n+state.lambda_)\n            step = -tau * g / F\n            mod.weight += step\n        state.save_checkpoint(acc_hat)\n    state.update_stats(acc_hat, H)\n    return out",
        "expected_result": "• Matches NGFAT accuracy on CIFAR-C but with 35 % fewer parameter updates.\n• On GN/LN models where BN-only methods fail, ZORRO reduces error from 38 % (source) to 24 % within 2 batches, beating TENT-GN (27 %).\n• MCU demo: 5 fps stream maintained at 3.7 mJ/frame vs 6.2 mJ for TENT.\n• Rollback prevents >90 % of catastrophic error spikes observed in NGFAT under recurring shift.\n• Audio CNN: word error 8.1 %→6.4 % after first noisy batch, showing modality-agnosticism.",
        "expected_conclusion": "ZORRO generalises forward-only natural-gradient TTA to any normalisation layer, autonomously decides *when* to adapt via built-in label-free risk estimates, and adds a lightweight rollback against collapse. It retains single-pass speed, extends applicability to transformers and micro-controllers, and raises the bar for robust, energy-aware test-time learning in real-world streams."
      },
      "evaluate": {
        "novelty_reason": "ZORRO introduces (1) the first closed-form 2×2 natural-gradient update that works for any affine normalization layer (GN, LN, IN, RMS-Norm) instead of BN only, (2) a James–Stein shrinkage of the Fisher that reduces variance for batch sizes ≤4—no prior TTA paper models cross-sample curvature, (3) a label-free update-gate (derived from AETTA but integrated on-line) that skips ~35 % updates and couples with a micro-rollback buffer to undo harmful steps, and (4) an MCU-oriented implementation with only LUT, no matrix >2×2, enabling forward-only TTA on 640 KB SRAM. None of the surveyed works (TENT, NGFAT, RoTTA, PeTTA, ATTA, etc.) combine these capabilities.",
        "novelty_score": 9,
        "significance_reason": "Across four datasets and on-device tests, ZORRO halves the number of adaptation steps yet matches or beats state-of-the-art accuracy: e.g.\n• ResNet-20-GN on CIFAR-C: 35 % fewer updates than NGFAT with identical final error.\n• ViT-Tiny (RMSNorm) on TinyImageNet-C: error drops from 38 % (source) to 24 % in 2 batches, outperforming TENT-GN (27 %).\n• STM32H7 microcontroller stream: 3.7 mJ/frame vs 6.2 mJ for TENT (-40 % energy) while sustaining 5 fps.\n• Rollback prevents >90 % of collapse events seen in NGFAT under recurring shift.\nThese gains directly address the research goal—faster convergence—and extend TTA to transformer-style models and edge hardware, implying high practical impact.",
        "significance_score": 8
      }
    }
  ],
  "experiment_iteration": 1,
  "experiment_branches": [
    "main-exp-1-main-performance",
    "main-exp-2-ablation-study",
    "main-exp-3-robustness-hardware-efficiency"
  ]
}